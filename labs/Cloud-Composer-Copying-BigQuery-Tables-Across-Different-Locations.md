---
id: 1418
name: 'Cloud Composer: Copying BigQuery Tables Across Different Locations'
type: Lab
url: https://www.cloudskillsboost.google/catalog_lab/1418
---

# [Cloud Composer: Copying BigQuery Tables Across Different Locations](https://www.cloudskillsboost.google/catalog_lab/1418)

In this lab, you create and run an Apache Airflow workflow in Cloud Composer to export tables from a BigQuery dataset located in Cloud Storage buckets in the US to buckets in Europe, and then import those tables to a BigQuery dataset in Europe.

## Step 1: GSP283

## Step 2: Overview

## Step 3: Setup and requirements

## Step 4: Task 1. Create a Cloud Composer environment

## Step 5: Task 2. Create Cloud Storage buckets

## Step 6: Task 3. Create the BigQuery destination dataset

## Step 7: Task 4. Airflow and core concepts, a brief introduction

## Step 8: Task 5. Define the workflow

## Step 9: Task 6. View environment information

## Step 10: Task 7. Create a variable for the DAGs Cloud Storage bucket

## Step 11: Task 8. Set Airflow variables

## Step 12: Task 9. Upload the DAG and dependencies to Cloud Storage

## Step 13: Task 10. Explore the Airflow UI

## Step 14: Task 11. Validate the results

## Step 15: Delete the Cloud Composer environment

## Step 16: Congratulations!

## Step 17: Next steps
