---
id: 989
name: 'Responsible AI for Developers: Interpretability & Transparency'
type: Course
url: https://www.cloudskillsboost.google/course_templates/989
date_published: 2024-12-04
topics:
  - Artificial Intelligence
  - Machine Learning
  - Responsible AI
---

# [Responsible AI for Developers: Interpretability & Transparency](https://www.cloudskillsboost.google/course_templates/989)

**Description:**

This course introduces concepts of AI interpretability and transparency. It discusses the importance of AI transparency for developers and engineers. It explores practical methods and tools to help achieve interpretability and transparency in both data and AI models. 

**Objectives:**

* Define interpretability and transparency as it relates to AI
* Describe the importance of interpretability and transparency in AI
* Explore the tools and techniques used to achieve interpretability and transparency in AI

## Course Introduction

This module introduces the course structure and objectives.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/989/video/515713)

* [YouTube: Course Introduction](https://www.youtube.com/watch?v=_ixhNoSv-gs)

Hello, welcome to the Course, Responsible AI for Developers, Interpretability and Transparency. This course introduces concepts of AI interpretability and transparency. It also explores practical methods and tools so that you can learn how to implement AI interpretability and transparency best practices using Google Cloud products and open source tools. In this course, you will learn to define what is interpretability and transparency as it relates to AI, describe the importance of interpretability and transparency in AI, and explore the tools and techniques used to achieve interpretability and transparency in AI. Let's begin the course.

## AI Interpretability & Transparency

This module focuses on AI interpretability and transparency. It provides various techniques and tools to help achieve interpretability and transparency in both data and AI models. 

### Video - [Overview of interpretability and transparency](https://www.cloudskillsboost.google/course_templates/989/video/515714)

* [YouTube: Overview of interpretability and transparency](https://www.youtube.com/watch?v=y6MRp8R58sg)

Welcome to AI Interpretability and Transparency. This module consists of eight lessons. Today you will learn to explain what interpretability and transparency mean in machine learning. Identify various interpretability techniques used to understand model behaviors. Discuss the various interpretability techniques that can be applied to models. Describe tools you can use to apply interpretability techniques to your machine learning models describe toolkits you can use to ensure data and model transparency. Let's start with an overview of interpretability and transparency. Interpretability and transparency relate to the fourth Google's AI principle, be accountable to people. Now. Its worth noting that principles often intersect and interpretability and transparency are also essential to the second principle because interpretation is the key to mitigating unfair biases in AI. Interpretability aims to understand machine learning model behaviors in many ways. A machine learning model is a function that converts inputs to outputs. Let's say that in this image classification example, our machine learning model receives an image and output classification result of a husky dog. It seems good, but why did our model say that it was an image of a husky dog? What kind of characteristics are used to judge an image of a husky dog? How can we understand the reasoning behind it? As we progress through this module, we explore multiple techniques to investigate these questions. It should be noted that in some cases, the term explainability is used instead of interpretability. Other times, the two terms have slightly different definitions, with interpretability meaning a deeper and wider understanding of models and holistic systems, and explainability is used for more specific and technical methods. In this course, we define and use interpretability and explainability as having similar definitions. Now, when it comes to transparency, it's very important to obtain trust from stakeholders, including users. Usually, machine learning applications involve multiple modules such as data collection system, data processing pipeline, machine learning, model evaluation system serving systems, and so on. Since each module is dependent on each other, it's always important to make documentation for each system and communicate based on that information. Google offers some useful tools and frameworks for this purpose, which we will discuss in more depth later on. At a high level, there are three stakeholder groups that benefit from interpretability and transparency as it relates to AI systems. For engineers, the focus is more on interpretable ML techniques to model understanding and improve performance. More complex models can be difficult to debug, understand and control interpretability and transparency. Methods help with this. For users, the focus is on trust. For model consumers. Users might not care about internal mode, but are very interested in understanding the impact of model predictions. Explainable systems build trust with these end users that show how model decisions are reliable and equitable. For regulators, the focus is on ensuring that model decisions are in compliance with laws and do not amplify undesirable bias from underlying datasets. Interpretability explanations provide auditable metadata. This allows regulators to trace unexpected predictions back to their inputs to inform corrective actions. Although interpretability is an active area of research, the task is not easy because interpretability issues apply to humans and AI systems. After all, it's not always easy for a person to provide a satisfactory explanation of their own decisions. Understanding complex AI models, such as deep neural networks, can be challenging, even for machine learning experts. There is typically a trade off between complexity and interpretability of models. Understanding and testing AI systems offers new challenges compared to traditional software. Traditional software is essentially a series of if then rules that, through some chasing, can be interpreted and debugged. With AI systems that not only have code but also data and models, it is much harder to pinpoint one specific bug that leads to a faulty decision.

### Video - [Overview of interpretability techniques](https://www.cloudskillsboost.google/course_templates/989/video/515715)

* [YouTube: Overview of interpretability techniques](https://www.youtube.com/watch?v=xPXTNpnyfEk)

>> Lets talk about the role interpretability plays in understanding model behaviors. Here is a classification model of a husky dog. Now the machine learning or ML model provides a smart prediction, but how do we understand the reasoning behind it? If our model is very simple like a linear regression model, it is not difficult to understand the reasoning. If we can assume input features of Xs are numeralized, we can simply inspect each coefficient of W. Since linear regression involves a weighted sum of the input features, the magnitude of the coefficients corresponds to the relative importance of those features. For instance, in image classification, we can analyze the coefficients associated with each pixel. However, for complex tasks like image classification, we typically don't employ such simple models. Now, Deep Neural Network or DNN models such as convolutional neural networks or CNNs and transformers generally outperform linear regression models in image classification tasks. This is because DNNs can extract more intricate relationships between the vast number of pixels in an image. It does this by using many weight parameters. However, the increased complexity of DNNs makes it more challenging to comprehend their behavior in a way that allows for straightforward interpretation. Although BNN is a very popular and common approach, their widespread adoption is hindered by their black box nature, making them less interpretable than simpler models. This inherent tradeoff between complexity and interpretability poses a challenge in understanding the reasoning behind DNNs. It should be noted that complexity doesnt always lead to better performance. You should always consider using simple models when the performance gain of using a more complex model is small. To help understand very deep and complex models, a lot of modern interpretability techniques exist. There are many ways to categorize different types of interpretability techniques, so we classified them into several subcategories. Every interpretability technique first falls into one of two intrinsic or post hoc. Intrinsic is the ability of a model to be directly interpreted by examining its structure or learning process. This means that the models internals are inherently transparent and can be understood without the need for additional tools. This approach is only possible for simple methods such as linear regression models, where you merely need to look at the trained weights for each feature, decision forests, bayesian networks, and so on. Note that some models still require sophisticated visualization to become interpretable. Next, post-hoc. Post-hoc refers to methods that are applied after a model is trained. These methods aim to provide insights into the model's behavior and explain its predictions. This approach is necessary for non intrinsic models, which are often more complex, and it provides a consistent way to assess interpretability across different model types. Post-hoc methods can be applied to intrinsically interpretable models too. So intrinsically interpretable models remain among the most popular interpretability methods because they are fairly easy to understand. At the same time, care should be given when describing these models to stakeholders. Why? First, if your model isn't complex enough to accurately describe the underlying system and fit the data well, explanations aren't going to be useful because the models are not useful. Second, ML models learn patterns and correlations, not causal structures. Third, model internals and interpretable ML methods tell you about your model, not necessarily your data or the underlying data generation system. And lastly, heap explanations to claims about the model only. Be careful making inferences on underlying populations of interest. Intrinsic explanations vary in their characteristics, particularly in terms of linearity, monotonicity, and the ability to consider feature interactions. For example, intrinsic explanation of linear regression is linear and monotone, but it doesn't consider interactions because we simply look at coefficients of each feature. So now let's focus more on post-hoc techniques, which are more flexible in terms of model selections. Post hoc interpretation techniques can provide either local or global interpretability. Local techniques provide explanations for each data point. We could ask the model, why do you classify this image as a husky dog? Using a local technique, we can then delve into the reasoning behind a specific prediction for a particular data point. Global techniques provide more aggregated explanations, referring to the entire model's prediction space. Using a global technique, we can ask the model, which feature do you prioritize most in general? Global methods provide a more complete map of the prediction space, but can fail to accurately capture the nuances of individual predictions. It can also fail to effectively explain the model's behavior in specific regions of the prediction space. Now let's talk about the terms model agnostic and model specific. Both local and global interpretation methods fall into one of these two barriers. At a high level, model specific methods use the internal details of the model, whereas model agnostic methods examine the model's behavior by manipulating the data fed into the model. Model agnostic interpretability methods don't rely on model internals. Instead, they analyze how changes in input features affect the model's output predictions. This approach allows these techniques to be applied to a wide range of machine learning models. Model specific interpretability method is restricted to usage on specific types of machine learning models. For example, some techniques rely on the gradients of a neural network and can only be applied to differentiable models like deep neural networks. Interpretability techniques can be categorized based on the type of output they produce. Here's a breakdown of the major categories based on output types. Although this is not an exhaustive list, it covers major categories. Most of the major interpretability methods fall into the category called feature-based explanations. These provide attribution scores, masks, or some visualizations that highlight the importance of each input feature for a given task. For example, feature-based explanations might indicate which features are most relevant in classifying an image as a husky dog. Concept-based explanations are similar to the feature-based explanation, but it outputs relative important scores to broader different concepts instead of each feature. These concepts, such as long nose-ness or white fur-ness, are designed to be more intuitive and interpretable for humans. Example-based explanations provide very different types of output. Instead of returning the importance of feature or concept explanations, it provides data points that our target model thinks is similar to a given query data point. This kind of output is also useful to understand how models interpret data and extract meaningful embeddings from the input.

### Video - [Feature based explanations: Model agnostic](https://www.cloudskillsboost.google/course_templates/989/video/515716)

* [YouTube: Feature based explanations: Model agnostic](https://www.youtube.com/watch?v=YVIrVKI70E8)

>> Lets look more in depth into each model. Agnostic approach. First, feature based explanations. Again, feature-based explanations provide attribution scores, masks, or some visualizations that highlight the importance of each input feature for a given task. What this looks like depends on the type of data youre using. For each data modality of images, tabular data, and text, feature attributions look different for images. Feature attributions will show you which pixels or regions within an image most contributed to the model's classification. For example, for this image of a husky, it looks like the model is focusing mostly on the Husky's nose and facial features, which is likely what we would hope for with a model seeking to classify dog breeds for tabular data. Feature attributions tell you how much each feature column contributed to either a specific prediction or the model overall. Some methods just return the magnitude of impotence as positive values, while others can give you directions of feature contribution to outputs like positive or negative contribution. For text, feature attributions tell you how much each word or token in a sentence or utterance contributing to the sentiment score. For example, in this sentence, the cake tastes delicious. Feature attributions could show you that the high sentiment score of 0.9 is largely attributable to the word delicious, which had a strongly positive influence on the text classification, whereas the other words in the phrase had significantly less influence. Let's look at some popular feature-based explanation techniques. There are global techniques like permutation feature importance and partial dependence plots. And local methods such as LIME, Shapley values, integrated gradients, and XRAI. First, permutation feature importance this is a pulse hoc global model agnostic method that measures the impact of each feature on a model's performance. It works by randomly shuffling the values of a single feature and observing the resulting change in the model's error rate. The higher the increase in error, the more important the feature is considered to be. Here is an overview of the process. For each feature, randomly shuffle its values while keeping all other features unchanged. Use the shuffle feature values to generate predictions. Calculate the error rate of the model with the shuffle feature. Compare the new error to the original error rate to determine how much the model's performance was affected by shuffling the feature. Based on this, then, a feature is considered important if randomly shuffling and increases the model error. This indicates that the model relied heavily on that feature for making accurate predictions. A feature is considered unimportant if randomly shuffling its values results in little to no error differences. This indicates the model wasn't heavily influenced by the feature in the first place. So, in summary, the importance of a feature is determined by the magnitude of the error increase caused by shuffling its values. Permutation feature technique can be intuitive and is easy to implement, but sometimes it can be misleading. Randomly shuffling a feature can create artificial patterns in the data and potentially cause misleading results. Additionally, the random nature of this method can cause the importance scores to vary significantly across different trials. Partial dependence plots, or PDPs, are a post-hoc, global, and model agnostic technique used to visualize the relationship between a model's predictions and the values of specific input features. They show how the models predictions change as we vary the values of one input feature while holding all other features constant. More specifically, the partial dependence function at a particular feature value represents the average prediction if we force all data points to assume that feature value. PDPs often visualize feature distributions in plots, which can show areas in feature space with limited data. While it is possible to visualize PDPs from multiple feature interactions, humans can have difficulty interpreting high dimensional visualizations. Therefore, PDPs are typically limited to two interacting features, one on each axis. To do this, choose the input feature you want to analyze systematically. Change the values of the selected feature while keeping all other features constant. For each feature value, compute the average predicted outcome across all data points. This represents the partial dependence of the prediction on that feature value. Plot the feature values on the x axis and the corresponding average predictions on the y axis. This visualizes the partial dependence relationship. PDPs provide a valuable tool for understanding the relationship between input features and model predictions. They can help identify important features, detect nonlinear relationships, and uncover potential biases in the model. Now, let's take a look at local methods. First, is LIME which stands for local interpretable model agnostic explanations. This has been popular for a number of years. LIME is post-hoc, local and model agnostic. LIME creates an explanation by approximating the underlying model locally with an interpretable one. We usually use a linear model or a decision tree. The process for the LIME method looks like this. Select your instance of interest. For example, an image of a tree frog in image case. Split the image into superpixels which are a subset of segments. Generate perturbations of the input image by hiding some features or image superpixels. For each perturbation, use a target model such as a deep neural network model we want to interpret in order to predict the output. Construct a new dataset where each input represents the presence or absence of a feature and the corresponding output is the predicted probability of the perturbation being a frog. Calculate weights for each perturbation based on a distance metric. This typically measures the difference between the perturbed image and the original image, such as the cosine distance. On the dataset constructed in the step above, train an intrinsically interpretable model, such as a decision tree or linear regression. Using the newly constructed dataset of perturbations, interpret the trained model to identify which areas of the image have a stronger association with the prediction of a tree frog. This can be achieved by examining the model's coefficients or decision rules to understand how it weighs different features in the perturbed images. Interpreting a simpler surrogate model instead of a complex one is an intriguing approach. However, the perturbation procedure varies depending on the data type. For images, replace superpixels with meaningless values such as gray values. For text, substitute words with a special token like an unknown word token to eliminate meaning. For tabular data, sample data from a feature distribution and calculate weights based on similarity instead of hiding information. Next, let's talk about Shapley values. Shapley values are a concept from cooperative game theory. It has been adapted to explain the contribution of individual features to a machine learning model's predictions. In game theory, Shapley values are used to distribute the payoff of a cooperative game among the players based on the magnitude of the contribution to the game. In machine learning, the players are the features and the payoff is the attribution score for the model's prediction. Shapley values are a post-hoc technique. For an understanding of this, let's look at an example. Imagine Sangita, Tomio, and Devon form a team for an e-sports competition, collectively winning $1,000. Recognizing their varying skill levels, they decided to split the prize based on their individual contributions to the game. Instead of an equal division, Sangita, a mathematician, proposes using Shapley values to quantify each players contribution. They create a gaming simulator and repeatedly play the same game with different team combinations, resulting in varying prize amounts. The Shapley value method for this scenario involves changing the combination of featured teams, marginalizing the impact of other features, and calculating the average prediction. The result tablet is rearranged as a path to add players one by one in different orders. In this case, there are six possible paths based on the previous steps. Result table the result difference is calculated for each cell along each path. In this case, all differences are positive, indicating that adding a player increases the overall price. However, in machine learning predictions, differences can be negative, reflecting the influence of features that push predictions towards smaller values. By averaging the result gaps when a player joins a team, we can compute the Shapley value, direction, and magnitude of each player's contribution. Sangita's Shapley value is 400 higher than Tomio and Devon's. A key characteristic of Shapley values is their additivity. Thanks to this property, Sangita, Tomio, and Devon's Shapley value sum up to $1,000, the amount they earn together. This allows them to easily distribute the prize based on their respective Shapley values. Although Shapley values offer a mathematically sound approach to explaining feature importance, their computational complexity poses a practical challenge. Calculating Shapley values for a large number of features becomes computationally challenging as the number of features increases exponentially. For example, computing Shapley values for 1000 features requires running a simulation or making predictions equal to two to the power of 1000 times, which is simply not feasible. So should we avoid using this method if we have many features? Fortunately, many efficient approximation algorithms of Shapley values are available, such as Sampled Shapley, Kernel SHAP, and Tree SHAP. Each has slightly different approximation strategies and applications, but are all widely used in many machine learning interpretation use cases.

### Video - [Feature based explanations: Model specific](https://www.cloudskillsboost.google/course_templates/989/video/515717)

* [YouTube: Feature based explanations: Model specific](https://www.youtube.com/watch?v=UiwZudpId0k)

Let's look at model specific approaches, particularly, methods applicable to deep neural network models. The term model specific might sound inflexible, but in fact, can provide finer explanations for very complex models. The first method is integrated gradient or IG. IG is a feature-based post-hoc explanation method that aims to explain the relationship between the model's predictions in terms of its features. It is designed for differentiable models such as neural networks. IG primarily focuses on explaining individual predictions. Differentiability is crucial for gradient-based explanations like IG, as it allows us to compute gradients of the model's output with respect to its input features. Neural networks. A common type of differentiable model employed back propagation for training. Back propagation is an algorithm that efficiently propagates the error from the output layer back to the input layer, allowing the network to adjust its parameters in a way that minimizes the error. While back propagation focuses on gradients with respect to model parameters, interpretability techniques like IG require gradients with respect to input features. These gradients represent the sensitivity of the model's output to changes in a particular input feature. A larger gradient indicates a stronger influence of the corresponding feature on the model's output. In essence, the differentiability of the target model enables gradient-based explanations like IG to quantify the impact of individual input features on model predictions, providing valuable insights into the model's decision making process. Then why not visualize the gradients over the image? The image in the middle is the gradient mask of a DNN-based image classification model for a camera image. As you can see, the image appears too blurred to distinguish the classified camera object. Here lies the issue. As your model learns the relationship between the range of an individual pixel and the correct class, the gradient becomes increasingly small. It might even go to zero. This is called saturation. In comparison, on the right, notice how integrated gradients are much better at identifying the edges of the camera object. In particular, highlighting the pixels around the lens as being important. It captures a better representation of the camera that is more human interpretable. How do integrated gradients overcome gradient saturation? Let's walk through an example. Let's say, we had a model that correctly predicted this image as a fireboat. In IG, we first set a baseline image, usually, a completely black image and we generate a linear interpolation between the baseline and the original image. Interpolated images of small steps denoted as Alpha in the feature space. Rather than directly calculating gradients for the original image, we calculate the gradients for images along a path from the original image to a baseline image. Integrating these gradients yields the integrated gradients. But why do we need to integrate the gradients? To answer this, let's visualize the gradients from the previous step to connect theory to practice. The left plot shows how your model's confidence in the fireboat class varies across Alphas. Notice how the gradients or slope of the line largely flattens or saturates between 0.3 and 1.0 before settling at the final fireboat predicted probability of about 85%. The right plot shows the average gradient magnitudes over Alpha. Note how the values sharply approach and even briefly dip below zero after 0.2. In fact, your model learns the most even at lower values of Alpha. Intuitively, you can think of this as your model has learned that the pixels of water cannons to make the correct prediction is at 0.2 sending these pixel gradients to zero after that. However, the model is still uncertain and is focused on bridge or water jet pixels as the Alpha values approach the original input image. Here are the actual gradient maps on each Alpha interpolation, starting with a baseline of an all black image. We can see the important information like that of the water cannon is actually captured around Alpha 0.2. At Alpha 1, large gradients are only looking at the background. Now, we can integrate all the gradients. Practically, we just compute the Reimann sum approximation by using each Alpha interpolation because we don't know this function itself. Here is the result of integrated gradient showing the water cannon pixels that captured the fireboat class. However, note that the model is not looking at the boat itself. It's worth checking whether this model works well, even when a fireboat is not using a water cannon. Integrated gradient also has its drawbacks. One of the biggest difficulties is related to the baseline image. The implication of a black baseline is if black pixels are important to the prediction, they receive no attribution as a result, the choice of the baseline plays a central role in interpreting and visualizing IG's pixel feature importances. On the left, you can see how IG attributions completely miss the solid black beetle in the top image. On the right, you can see how changing the baseline to a white image corrects the interpretation of the IG attributions. How can we improve upon IG and its baseline selection problem? Through XRAI, an explainable AI method from Google Research. XRAI solve for the baseline selection problem. XRAI also added a novel region based attribution method for clear mass generation. Look at the image on the right. Instead of identifying individually important pixels, XRAI creates and highlights a region of the original image as important. XRAI overcomes the baseline selection problem by using a black and a white baseline image together to compute IG attributions. XRAI also improves upon integrated gradients IG by over-submitting the image into smaller regions and ranking them based on their IG positive attributions. This approach provides more localized and intuitive explanations for natural images. Although IG remains valuable for domains requiring fine-grained feature explainability, such as the medical image domain. XRAI excels in generating intuitive explanations for natural images.

### Video - [Concept-based and example-based explanations](https://www.cloudskillsboost.google/course_templates/989/video/515718)

* [YouTube: Concept-based and example-based explanations](https://www.youtube.com/watch?v=P7o3AFFsjEg)

Let's move on to the concept-based explanation. Concept-based explanations are a type of interpretability technique used to understand how machine learning models make predictions through higher-level concepts that are more meaningful to humans instead of individual input features. Let's say we have an image classification model that classifies the zebra class. If we can get an attribution score for user-friendly concepts, like stripedness or horse shapeness, that would be intuitive and informative. TCAV, or Testing with Concept Activation Vectors, stands as a pioneering approach in the field of concept-based explanations. It aims to provide explanations for arbitrary concepts, enabling a deeper understanding of how machine learning models utilize these concepts in their decision-making process. The TCAV methodology involves the following steps. Gather a user-defined set of examples that exemplify the concept of interest, such as strike patterns, along with a set of random examples. Collect, label training data instances representing the target class. For example, zebras, employed a trained target neural network to process the prepared concept data. In a feature space of an intermediate layer of the model, train a linear classifier to distinguish between the representations produced by the concepts examples and those from random examples. The concept activation vector is the vector perpendicular to the classification boundary and represented by the red arrow. For the class of interest, zebras, use the directional derivative to quantify the model sensitivity to the concept. By following these steps, TCAV enables the computation of concept attribution for a wide range of concepts, providing valuable insights into how machine learning models rely on these concepts for making predictions. TCAV is also useful for fairness testing purposes, such as ensuring that a class of CEO doesn't have a stronger concept attribution for male concepts over female concepts. TCAV provides flexibility regarding the choice of concepts. But in some situations, you might want to automate the concept selection process since creating a dataset for a concept can be complex. ACE, or Automatic Concept-based Explanation, offers this automation capability. In this approach, concept activation vectors are automatically created through the following steps. Split the target image into smaller crop segments, then rescale back to the original image resolution to ensure compatibility with the target neural network. Apply K-means clustering inside the feature space of an intermediate layer of the target neural network. Each cluster is regarded as an automatically created concept. Calculate concept activation vectors for each cluster to determine the attribution of each concept to the prediction. The calculation of the concept activation vector is the same as TCAV. Here are some output examples of the ACE method. Although we can't provide hand-picked concepts like stripedness or horse shapeness, it still gives us interpretable results. For example, it seems like the uniform logo and basketball surface are important concepts for the basketball class. Let's talk about the example-based explanation. The focus here is on explaining the model's results by looking at the training data. Example-based explanations is another interpretability technique that allows users to understand model behavior and how predictions are made. It relies on providing approximate nearest neighbor-based explanations. We can generate example-based explanations for multiple types of data such as image, tabular, and text. For images, example-based explanations will show you which other images in the training data appeared most similar to the new image that you want to classify. For example, for this image of a dog, the model is classifying it as a husky, and all similar examples in the training data were also labeled as huskies. This works very similar for tabular and text data. The focus is on explaining model results by showing the most similar examples from the training data. How can we compute the nearest neighbors? First of all, all neural network models create some embedding as intermediate representations of input regardless of the type, like image, text, or tabular. So to compute the nearest neighbors, we can investigate representation of a layer, usually a layer close to the output layer, but before the final output layer. First, compute embeddings of the training dataset. Then we can query the nearest neighbors by using a target image embedding. We can use this explanation in many ways. Let's say you have a classification model that misclassified this bird image as a plane. We used example-based explanations to retrieve other images in the training data which appeared most similar to this misclassified bird image for the model. Examining those, we identified that both the misclassified bird image and the similar images were dark silhouettes. This, in turn, signals a potential lack of images of birds with dark silhouettes in the training data, and an immediate action to gather more data with images of silhouetted birds in order to improve the model.

### Video - [Tools for interpretability](https://www.cloudskillsboost.google/course_templates/989/video/515719)

* [YouTube: Tools for interpretability](https://www.youtube.com/watch?v=HlNJHKgiNL4)

We discussed many interpretability techniques. Now let's look into useful tools to apply these techniques to your machine learning models. We are going to look at three tools; open source libraries, SHAP and saliency, learning interpretability tool, an open source platform for visualization and understanding of models and vertex explainable AI, a Google Cloud solution for post-hoc explanations. First, SHAP Python Library provides popular implementations of approximate Shapley values including sampled Shapley, Kernel SHAP, Tree SHAP and so on. It's a post hoc technique that generates explanations for individual predictions that can also be aggregated for global model feature importances, onctangular and text data. SHAP's biggest drawback is its computational cost on large feature sets that have limited its application to domains such as images. Next is saliency. Google's people and AI research team or pair team, offers a Saliency Python library that mainly covers gradient-based explanation techniques including integrated gradients, XRAI, and a lot of their variants. The learning interpretability tool or LIT is for researchers and practitioners who want to understand model behavior through a visual, interactive, and extensible tool. It mainly supports natural language processing or NLP with some preliminary support for tabular and image data. You can use LIT to ask and answer questions such as, what kind of examples does my model perform poorly on? Why did my model make this prediction? Does the model properly focus on important features instead of obviously unimportant features like image background? Does my model behave consistently if I change things like textual style, verb tense, or pronoun gender? Does this method relate to counterfactual analysis in AI fairness and bias? LIT has lots of functionalities and it even provides you with the ability to add custom techniques, metrics, visualizations, and more. You can also customize the layer itself to select your modules and groups of interest. Let's get a general overview of the UI to get a feel of what capabilities it offers. LIT is divided into two workspaces. A main workspace in the upper half of the interface and a group-based workspace in the lower half. Each workspace is a logical combination of groups each containing different modules. This allows you to view different visualizations and methods side by side. LIT supports many models and features. Models like classification, regression, sequence to sequence, etc and features like an embedding projector, TCAV, counterfactual analysis, etc. Two of the most important features to mention are token-based input salience methods and pixel-based salience methods. Token-based input salience methods include gradient-based methods and black box techniques like line. Pixel-based salience methods are used for models that take images as inputs. The output for both types of salience methods is rendered in the Salience maps module in the LIT UI which allows for comparison of multiple methods at once. Vertex Explainable AI is Google Cloud managed service for interpretability. It offers feature-based and example-based explanations to provide better understanding of model decision making even for complex models. With example-based explanations, vertex AI uses nearest neighbor search to return a list of examples, typically from the training set that are most similar to the input. It currently supports only TensorFlow models that can provide an embedding or latent representation for inputs. With feature-based explanations, vertex AI uses sampled sharply, integrated gradients or XRAI. It works on tabular, image, video, and text data. It currently supports all types of models such as AutoML, BQML models, and custom trained models on vertex AI and frameworks like TensorFlow, scikit-learn, and XG Boost for BigQuery ML and AutoML. Google Cloud provides simple ways to look at predictions within the product. BigQuery ML provides five different AI explanation functions that you can use in SQL. For AutoML, you can simply access explanations from the test and use tab, select and configure the prediction type you want, and get explanations. In the screenshot here you are seeing an example of online prediction for text data with samples sharply for local feature importance. Global importance is also supported. On vertex I it is also easy to set up explanations. This is an example to configure it from the console. But remember that you can also set them up for gcloud CLI, REST and Python. All you need to do is import the model and model registry and configure your desired explanations in the Explanability tab.

### Video - [Data and Model Transparency](https://www.cloudskillsboost.google/course_templates/989/video/515720)

* [YouTube: Data and Model Transparency](https://www.youtube.com/watch?v=qIhJL1oGEoU)

Let's explore transparency in detail, specifically data and model transparency. Transparency is a clear, easily understandable, and plain language explanation of what something is, what it does, and why it does that. Machine learning transparency involves sharing information about system behaviors and organizational processes. This might include documenting and sharing how models and datasets were created, trained, and evaluated. Transparency artifacts are a form of structured information reporting that focuses on transparency during product creation and performance to encourage responsible AI adoption and application. Think of this as being similar to nutritional labels for food products. Data cards and model cards are two types of transparency artifacts. Let's first talk about data cards. Data cards are structured summaries of essential facts about various aspects of ML datasets that are needed by stakeholders across a project's life cycle for responsible AI development. You can take advantage of these templates to simply and reliably document your datasets. Data cards can include any of the following that are appropriate for your use case. One, upstream sources. Two, data collection and annotation methods. Three, training and evaluation methods. Four, intended use and five, decisions affecting model performance. When creating documentation for the data that you use, be sure to involve the whole team. This will ensure you ask all the questions that you need to understand about your data and that you include clear and actionable answers to these questions in the data card. You should also involve the following roles when interacting with the data card. Producers, the individuals or teams who will be creating the data card. Consumers, the individuals or teams who will be using the data card and end users, the individuals who might be taking actions based on the system that is or will be built on the dataset. At Google, we provide a data card template that captures 15 themes besides a summary. These are the themes we frequently look for when making decisions, many of which are not traditionally captured in technical dataset documentation. Each of the themes is a section on the data card template like shown in the screenshot. You can find an example of a full data card template in the resources. The data card playbook is a toolkit for transparency in AI dataset documentation. Google provides a data card playbook to help AI ML practitioners achieve and maintain proactive data transparency. The playbook contains four modules designed with participatory activities that define long term transparency for datasets and in context. The transparency patterns capture practical ways to create data cards that are people centric, purposeful and actionable. Now that we've learned about the data card, let's look at another tool kit that provides transparency. The model card, model cards explain what the model is supposed to be used for, how its performance was tested and other important details. Model cards offer benchmark assessments for a range of conditions including diverse, cultural, demographic or phenotypic groups, as well as intersectional groups pertinent to the models intended application domains. They also reveal the context for which the model is designed, elaborate on the performance evaluation procedures and provide additional relevant information. Model developers are primarily responsible for creating a model card. However, anyone involved in idea generation, development, or testing should also be involved in the process, since the requisite knowledge is often distributed across the team. The model card tool kit or MCT library streamlines and automates generation of model cards. Jinja templates form the underlying structure of a model card document. The model card toolkit provides a few ready made templates, but you have the freedom to modify these templates or even create your own for users of tensor flow extended or T effects that MCT can automatically fill these fields using ML metadata or MOND. Additionally, you can manually populate model card fields through a Python API. Here's an example. To get started, import the model card toolkin module in Python. Then initialize the model card tool kit with a path for storing generated assets. Next, initialize the Model Card Toolkit Model card with a path for storing generated assets. Then, initialize the Model Card Toolkit model card, which can be flexibly customized. Finally, we write the model card data to a JSON file and generate the model card document as an XML page. If you're using Tensor Flow Extended or TFX, you can integrate model card generation into your TFX pipeline by using the model card generator component. However, as the model card generator component is being migrated to the TFX avons library, you'll need to install the TFX add ons package beforehand. The model card for the census income classifier serves as a prime example. In the model detail section, you'll find an overall description of the model including its purpose, the data said it was trained on and other relevant information. Additionally, you can access information about the models, version owners and references. The consideration section delves into the models, use cases, limitations and ethical considerations. The lower portion of the model card presents graphs for the train set and eval set, which provide insights into the main training data elements and performance evaluation metrics. Transparency in AI is a fundamental principle that should be accessible to all. This is why model cards are not intended to be a proprietary Google product, but a shared evolving framework that draws from diverse perspectives and contributions.

### Video - [Lab: Vertex Explainable AI](https://www.cloudskillsboost.google/course_templates/989/video/515721)

* [YouTube: Lab: Vertex Explainable AI](https://www.youtube.com/watch?v=jdO7EY4PDDo)

Let's perform an exercise in a hands-on lab. This lab showed you how to train a classification model on image data and deploy it to vertex AI to serve predictions with explanations or feature attributions. In this lab, you will explore the dataset, build and train a custom image classification model with vertex AI, deploy the model to an endpoint, serve predictions with explanations, and visualize feature attributions from integrated gradients.

### Lab - [Explaining an Image Classification Model with Vertex Explainable AI](https://www.cloudskillsboost.google/course_templates/989/labs/515722)

In this lab, you learn how to deploy an explainable image model with Vertex AI.

* [ ] [Explaining an Image Classification Model with Vertex Explainable AI](../labs/Explaining-an-Image-Classification-Model-with-Vertex-Explainable-AI.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/989/quizzes/515723)

#### Quiz 1.

> [!important]
> **Your manager is a strong advocate for responsible AI development. During a project review, they emphasize the importance of transparency and documentation around the dataset you're using and the resulting AI model. They want to know what steps you're taking to ensure this. Which of the following tools or techniques are you using to provide a clear understanding of your dataset and model?**
>
> * [ ] Data card, Model card.
> * [ ] SHAP library.
> * [ ] Vertex Explainable AI
> * [ ] Learning Interpretability Tool (LIT).

#### Quiz 2.

> [!important]
> **You're working on an image classification model for identifying different types of clouds. During testing, you notice some strange results. The model seems to be focusing on irrelevant areas of the images (like background objects) instead of the actual cloud features. To understand why your model is behaving this way, which of the following tools or techniques would be the most helpful?**
>
> * [ ] XRAI (eXplainable Region-based Artificial Intelligence)
> * [ ] Permutation Feature Importance
> * [ ] ACE (Automatic Concept-based Explanation)
> * [ ] TCAV (Testing with Concept Activation Vectors)

#### Quiz 3.

> [!important]
> **As an AI engineering team manager, you're giving a presentation to explain why interpretability and transparency in AI development is important for engineers. Which of the following statements would be best to include in your slides?**
>
> * [ ] Compliance.
> * [ ] Reduce project cost.
> * [ ] Understand model behaviors
> * [ ] Shorten project lead time.

## Course Summary

This module provides a summary of the entire course by covering the most important concepts, tools, and technologies.

### Video - [Course Summary](https://www.cloudskillsboost.google/course_templates/989/video/515724)

* [YouTube: Course Summary](https://www.youtube.com/watch?v=c2P8JMY_BMA)

You've completed the course Responsible AI for Developers, Interpretability and Transparency. Let's recap what you have learned in this course, we introduced interpretability and transparency in AI. Interpretability and transparency are key to mitigating unfair biases in AI and are related to the fourth of Google's AI principles. Be accountable to people. You also learned about interpretability techniques and how they are categorized into feature based, concept based, and example based methods. You learned about feature based explanations where there are global techniques such as permutation, feature importance and partial dependence plots. You learned about local methods such as LIME, Shapley values, Integrated gradients, and XR AI. You also gained knowledge around concept based explanations such as TCAV, which aims to provide explanations for arbitrary concepts and you learned about example base explanations which provide approximate nearest neighbor base explanations. Lastly, you explored a few interpretability tools such as Open Source Library SHAP, Learning Interpretability Tool, and Vertex Explainable AI, as well as a few transparency tools such as data cards for data transparency and model cards for model transparency. As artificial intelligence continues its rapid ascent, the conversation around responsible AI becomes ever more vital. New technological developments constantly present fresh challenges and opportunities in this domain. It's even more important now to ensure that when you develop your AI, you are equipped with the latest insights and best practices for responsible AI implementation.

### Document - [Reading](https://www.cloudskillsboost.google/course_templates/989/documents/515725)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
