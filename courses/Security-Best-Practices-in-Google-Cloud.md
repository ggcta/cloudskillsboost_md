---
id: 87
name: 'Security Best Practices in Google Cloud'
type: Course
url: https://www.cloudskillsboost.google/course_templates/87
date_published: 2024-11-13
topics:
  - Google Cloud Services
  - Cloud Key Management Service
  - Access Control
---

# [Security Best Practices in Google Cloud](https://www.cloudskillsboost.google/course_templates/87)

**Description:**

This self-paced training course gives participants broad study of security controls and techniques on Google Cloud. Through recorded lectures, demonstrations, and hands-on labs, participants explore and deploy the components of a secure Google Cloud solution, including Cloud Storage access control technologies, Security Keys, Customer-Supplied Encryption Keys, API access controls, scoping, shielded VMs, encryption, and signed URLs. It also covers securing Kubernetes environments.

**Objectives:**

* Apply techniques and best practices to secure Compute Engine
* Apply techniques and best practices to secure cloud data
* Apply techniques and best practices to secure applications
* Apply techniques and best practices for securing Google Kubernetes Engine (GKE) resources.

## Welcome to Security Best Practices in Google Cloud

Welcome to Security Best Practices in Google Cloud! In this course we will build upon the foundations laid during the earlier course in this series, Managing Security in Google Cloud. In this section, expect to learn more about how to implement security "best practices" to lower the risk of malicious attacks against your systems, software and data.

### Document - [Welcome and Getting Started Guide!](https://www.cloudskillsboost.google/course_templates/87/documents/514079)

## Securing Compute Engine: Techniques and Best Practices

In this module we will start with a discussion of service accounts, IAM roles and API scopes as they apply to compute engine. We will also discuss managing VM logins, and how to use organization policies to set constraints that apply to all resources in your organization's hierarchy. Next, we will review compute engine best practices to give you some tips for securing compute engine.Lastly, we will cover encrypting persistent disks with Customer-Supplied Encryption keys.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/87/video/514080)

* [YouTube: Module overview](https://www.youtube.com/watch?v=9VauKCHKVGk)

Welcome to the first module of Security Best Practices in Google Cloud, Securing Compute Engine: Techniques and Best Practices. Compute Engine security encompasses many different topics. In this module we will start with a discussion of service accounts, IAM roles and API scopes as they apply to Compute Engine. We will also discuss managing VM logins, organization policy controls, as well as shielded VMs and confidential VMs. We will wrap up by discussing Certificate Authority Service and best practices for Securing Compute Engine.

### Video - [Service accounts, IAM roles, and API scopes](https://www.cloudskillsboost.google/course_templates/87/video/514081)

* [YouTube: Service accounts, IAM roles, and API scopes](https://www.youtube.com/watch?v=0Go5hjSTnFs)

OK, let’s get started with Service accounts, IAM roles, and API scopes. Service accounts are an identity that a resource such as a VM instance can use to run API requests on your behalf. When launching a virtual machine in Compute Engine, a service account can be associated directly to that VM. When a service account is specified, the VM authenticates using the identity of that service account when making calls to the Google APIs. It’s also possible to specify no service account association. In that case, API requests running on the VM will not assume the service account identity by default and would therefore need to be manually configured. Since service accounts control how resources are managed and used, it’s very important that you provide them roles and permissions using the principle of least privilege. Every project has a default service account that is automatically created when Compute Engine is first enabled for the project. In this instance the service account is assigned the role of Project Editor and is used by default when launching VMs. You can also create and manage your own service accounts using Identity and Access Management. These user-managed service accounts are granted “necessary” permissions just like any member in IAM - by assigning roles. This gives you full control over exactly which permissions the service account will have. If you do not grant any roles, the service account will not have any access to services. When you create a new service account, it can be assigned to instances in exactly the same way as the default service account. The only difference is user-managed service accounts do not use the “access scope” concept, which we’ll cover in the next slide. Instead, permissions are controlled through the IAM roles assigned to the account. Applications running on instances associated with the service account can make authenticated requests to other Google APIs using the service account identity. The IAM Project Editor role contains permissions to create and delete resources for most Google Cloud services and can be dangerous to use as-is. Access scopes provide the ability to limit what permissions are allowed when using the default service account containing this role. Before the existence of IAM roles, access scopes were the only mechanism for granting permissions to service accounts. Although they are not the primary way of granting permissions now, you must still configure access scopes when initiating an instance to run under the default service account. It is important to remember that access scopes only apply on a per-instance basis. You set access scopes when creating an instance and the access scopes persists only for the life of the instance. There are several options when setting access scopes. The first is called “Allow default access”. The default access scope is actually very narrow and allows read-only access to Cloud Storage, as well as access to Cloud Logging and Cloud Monitoring. Other API access using the default service account will obviously be restricted. Consider the situation where your VMs need access to other APIs, such as BigQuery, Datastore, Cloud SQL, Pub/Sub, or Cloud Bigtable. The default access scope does not include these APIs and would cause a security error when accessing these, or other APIs not included in scope. The next access scope option is to “allow full access”. This grants full access to all Cloud APIs. Choosing this option would violate the principle of least privilege, and therefore is definitely NOT a best practice! There is another option, and that is to set each API access required individually. This will allow you to grant access to only the APIs required by the programs running on the VM. You can choose only the scopes required by your application. If you are using the default service account, then this is a much better practice than granting full API access. This diagram illustrates an example of where a service account might fit in an enterprise’s architecture running on Google Cloud.

### Video - [Lab Intro: Configuring, Using, and Auditing VM Service Accounts and Scopes](https://www.cloudskillsboost.google/course_templates/87/video/514082)

* [YouTube: Lab Intro: Configuring, Using, and Auditing VM Service Accounts and Scopes](https://www.youtube.com/watch?v=nrbrii6kQe8)

OK, now you will get a chance to configure and use service accounts and scopes. In this lab, you will learn how to: Create and manage service accounts. Create a virtual machine and associate it with a service account. Use client libraries to access BigQuery from a service account. And run a query on a BigQuery public dataset from a Compute Engine instance.

### Lab - [Configuring, Using, and Auditing VM Service Accounts and Scopes](https://www.cloudskillsboost.google/course_templates/87/labs/514083)

Configuring, Using, and Auditing VM Service Accounts and Scopes

* [ ] [Configuring, Using, and Auditing VM Service Accounts and Scopes](../labs/Configuring-Using-and-Auditing-VM-Service-Accounts-and-Scopes.md)

### Video - [Connecting to virtual machines](https://www.cloudskillsboost.google/course_templates/87/video/514084)

* [YouTube: Connecting to virtual machines](https://www.youtube.com/watch?v=aw16IYxwNOA)

In this section, we will discuss various options for logging into and securing VMs. Connecting to virtual machines in the cloud is generally very easy. By default, Linux instances on Google Cloud are accessed with Secure Shell (i.e SSH) and require a username and an SSH key for authentication. Password authentication is disabled by default. Windows instances are accessed with Remote Desktop Protocol (i.e., RDP) and require a username and password to authenticate. When connecting to Linux instances, the Cloud console provides a built-in SSH access mechanism. To connect to an instance, simply click the SSH button in the console and a SSH terminal session will open in a new browser window. As part of the connection process, the browser window performs an HTTPS connection to a Google web server, which in turn creates an SSH connection to the instance. SSH keys are automatically generated and propagated to the instance during this process. For this to work, the VM must have a public IP address and a firewall rule to allow TCP port 22 traffic from Google’s servers. It is also possible to connect to a Linux instance via SSH using gcloud SDK. Once you have the Cloud SDK installed and configured, simply connect with the command gcloud compute ssh, passing in the instance-name and the zone-name. In this command, the instance-name is the name given to the instance when it was launched. Notice you do not need to connect to, or even know, the VMs IP address. If your VM instance has no external IP address, you need a firewall rule in place that allows IAP TCP forwarding traffic. The zone-name is the name of the zone where the instance is running. Running this command will automatically generate SSH keys and place a copy of them in your local home/. ssh folder. But what if you just want to SSH into the instance and do not have access to either the console or gcloud credentials? Don’t worry, there are further options available. It is possible to connect from alternative SSH clients such as PuTTY on Windows or similar applications on Linux or Mac operating systems. To connect, remember you just need to SSH to the public IP address of the instance and provide a valid username and SSH private key to authenticate. The firewall rule for the instance still requires TCP port 22 to be allowed for the IP address range of your SSH client. Note the SSH keys to use in this case are managed outside of Google Cloud. You can manually create your own SSH key pairs, using tools like PuTTYgen or ssh-keygen. The public key must be provided to the instance that you wish to authenticate against, but your private key never leaves your infrastructure. The public key is provided to the instance using the project metadata. The project metadata can be accessed in the Cloud console from the Compute Engine dashboard. Simply select the “add item” option and upload your public key. Note that, by default, all keys added to the project metadata are available to ALL VMs in the project. However, if you do not want keys to be available to all VMs in the project, you can configure individual VMs to not use project-wide keys. When launching a VM, the “Block project-wide SSH keys” option can be selected to enable this restriction. SSH keys can also be added to instance-specific metadata and will only be available to that instance.

### Video - [Connecting to VMs without external IPs](https://www.cloudskillsboost.google/course_templates/87/video/514085)

* [YouTube: Connecting to VMs without external IPs](https://www.youtube.com/watch?v=z57fUmrf6iI)

Let’s review a couple of different ways to connect to your instance if it does not have a public IP address. One solution to this is to use a bastion host. To implement this solution, create a second VM with a Public IP address in the same network as the instance you want to connect to. Then, connect to the bastion host and from there SSH to the private VM. Be sure to harden the bastion host and ensure the firewall rules limit the source IPs able to connect to the Bastion, and then only allow SSH traffic to private instances from the bastion. An even better practice would be to use a VPN or some other more secure form of connection, such as Cloud Interconnect, for ordinary activities. Only use SSH with the bastion host as the maintenance avenue of last resort. Another solution to this type of situation is to use IAP TCP forwarding. IAP TCP forwarding allows you to establish an encrypted tunnel over which you can forward SSH—as well as RDP and other traffic—to VM instances. For general TCP traffic, IAP creates a listening port on the local host that forwards all traffic to a specified instance. IAP then wraps all traffic from the client in HTTPS. Users gain access to the interface and port if they pass the authentication and authorization check of the target resource's Identity and Access Management (IAM) policy. IAP TCP forwarding allows you to establish an encrypted tunnel over which you can forward SSH—as well as RDP and other traffic—to VM instances. For general TCP traffic, IAP creates a listening port on the local host that forwards all traffic to a specified instance. IAP then wraps all traffic from the client in HTTPS. Users gain access to the interface and port if they pass the authentication and authorization check of the target resource's Identity and Access Management (IAM) policy. For Windows VMs, connect using RDP and login in with a username and password. The username and password can be set from the Admin Console or using gcloud. From the console, click the down arrow next to the RDP button and select Set WIndows password. From gcloud, the command is: gcloud compute reset-windows-password instance-name and then specify the username (i.e. --user=) whose password will be reset. To connect, simply use an RDP client and connect to the external address of the instance. You can optionally download an RDP file if you wish. Compute Engine will automatically generate a random password for your Windows instance. Once you connect, you should change this to a custom password.

### Video - [OS Login](https://www.cloudskillsboost.google/course_templates/87/video/514086)

* [YouTube: OS Login](https://www.youtube.com/watch?v=e4MDSKa-YAo)

Now that we’ve covered the different ways on how we can connect to our VMs, let’s talk about a method you can use to manage access to instances—OS Login. OS Login is used to manage SSH access to your instances using IAM without having to create and manage individual SSH keys. OS Login maintains a consistent Linux user identity across VM instances and is the recommended way to manage many users across multiple instances or projects. OS Login simplifies SSH access management by linking your Linux user account to your Google identity. Administrators can easily manage access to instances at either an instance or project level by setting IAM permissions. OS Login provides the following benefits: Automatic Linux account lifecycle management: You can directly tie a Linux user account to a user's Google identity so that the same Linux account information is used across all instances in the same project or organization. Fine grained authorization using IAM: Project and instance-level administrators can use IAM to grant SSH access to a user's Google identity without granting a broader set of privileges. For example, you can grant a user permissions to log into the system, but not the ability to run commands such as sudo. Google checks these permissions to determine whether a user can log into a VM instance. Automatic permission updates: With OS Login, permissions are updated automatically when an administrator changes IAM permissions. For example, if you remove IAM permissions from a Google identity, then access to VM instances is revoked. Google checks permissions for every login attempt to prevent unwanted access. Ability to import existing Linux accounts: Administrators can choose to optionally synchronize Linux account information from Active Directory (AD) and Lightweight Directory Access Protocol (LDAP) that are set up on-premises. For example, you can ensure that users have the same user ID (UID) in both your Cloud and on-premises environments. Supports 2-factor authentication: If you use OS Login to manage access to your virtual machine (VM) instances, you can add an extra layer of security by using 2-step verification. For more information on OS Login, refer to the documentation link in this module’s course resources.

### Video - [Organization policy controls](https://www.cloudskillsboost.google/course_templates/87/video/514087)

* [YouTube: Organization policy controls](https://www.youtube.com/watch?v=ZgrM2jeOs7Y)

We are now going to talk about Organization policy controls and how they apply to securing your Compute Engine resources and workloads. The Organization Policy Service gives you both centralized and programmatic control over your organization's cloud resources. To define an organization policy, you choose a constraint, which is a particular type of restriction against either a Google Cloud service or a group of Google Cloud services. The compute.trustedImageProjects constraint has an interesting use case. By default, project users can create persistent disks or copy images using either public images and any other images that project members can access through IAM roles. However, you may want to restrict your projects to only use images that contain approved software to create boot disks that meets your policy or security requirements. The Trusted Images Policy can be used to enforce which images can be used in your organization. This allows you to host organization-approved, hardened images in your Google Cloud environment. Let’s see how you would apply the compute.trustedImageProjects constraint to a project. First, get the existing policy settings for your project by using the resource-manager org-policies describe command. Second, open the policy.yaml file in a text editor and modify the compute.trustedImageProjects constraint. Add the restrictions that you need and remove the restrictions that you no longer require. When you have finished editing the file, save your changes. Third, apply the policy.yaml file to your project. If your organization or folder has existing constraints, those constraints might conflict with project-level constraints that you set. To apply the constraint, use the resource-manager org-policies set-policy command.

### Video - [Shielded VMs](https://www.cloudskillsboost.google/course_templates/87/video/514088)

* [YouTube: Shielded VMs](https://www.youtube.com/watch?v=sh5W0rHax2M)

Now let’s discuss Shielded VMs and Confidential VMs. Protecting your hardware and firmware and host and guest operating systems is an important part of securing your workloads and data from malicious use and attacks. Unfortunately, some types of malware attacks can remain undetected on your virtual machines for long periods of time. Shielded VMs offer verifiable integrity of your Compute Engine VM instances, so you can be confident that your instances haven't been compromised by boot-level or kernel-level malware or rootkits, or that your secrets are exposed and used by others. When creating a Shielded VM, there are a wide range of image options. This slide shows a list of currently available Google-curated images. You can find even more shielded VM images on the Google Cloud Marketplace. In addition, if your organization relies on custom images, you can now transform an existing VM into a Shielded VM that runs on Google Cloud. Each time your VM starts up, secure boot makes certain that the software it is loading is authentic and unmodified by verifying that the firmware has been digitally signed with Google’s Certificate Authority Service (CAS). Shielded VM instances use Unified Extensible Firmware Interface (UEFI) firmware, which securely manages the certificates that contain the keys used by the software manufacturers to sign the system firmware, the system boot loader, and any binaries loaded. UEFI firmware verifies the digital signature of each boot component in turn against its secure store of approved keys, and if that component isn't properly signed (or isn't signed at all), it isn't allowed to run. This verification ensures that the instance's firmware is unmodified and establishes the “root of trust” for Secure Boot. Measured boot creates a hash of each component as it loads, concatenates that hash with other components that have already been loaded, and then rehashes it. This allows measured boot to record the number of components loaded on boot-up and their sequence. The first time your Shielded VM is booted, this initial hash is securely stored and used as the baseline for verification of that VM during subsequent boots. This is called “integrity monitoring,” and it helps ensure that your VM’s boot components and boot sequence have not been altered. Shielded VMs use a virtual Trusted Platform Module, which is the “virtualized” version of a specialized computer chip you can use to protect objects, like keys and certificates, that are used to provide authenticated access to your system. This vTPM allows Measured Boot to perform the measurements needed to create a known good boot baseline, called the integrity policy baseline, upon the first bootup of your Shielded VM.

### Video - [Confidential VMs](https://www.cloudskillsboost.google/course_templates/87/video/514089)

* [YouTube: Confidential VMs](https://www.youtube.com/watch?v=36AIyvBrK-I)

A Confidential VM is a type of Compute Engine VM that ensures that your data and applications stay private and encrypted even while in use. You can use a Confidential VM as part of your security strategy so you do not expose sensitive data or workloads during processing. Confidential VM runs on hosts with AMD EPYC processors which feature AMD Secure Encrypted Virtualization (SEV). Incorporating SEV into Confidential VM provides the following benefits and features. You can enable Confidential Computing whenever you create a new VM. Creating a Confidential VM only requires an extra checkbox or 1-2 more lines of code than creating a standard VM. You can continue using the other tools and workflows you're already familiar with. Adding Confidential Computing requires no changes to your existing applications. Confidential VMs provide end-to-end encryption. End-to-end encryption is comprised of three states. Encryption-at-rest protects your data while it is being stored. Encryption-in-transit protects your data while it is moving between two points. Encryption-in-use protects your data while it is being processed. Confidential Computing VMs give you the last piece of end-to-end encryption: encryption-in-use. Confidential Computing VMs provide: Isolation. Encryption keys are generated by the AMD Secure Processor (SP) during VM creation and reside solely within the AMD System-On-Chip (SOC). These keys are not even accessible by Google, offering improved isolation. Attestation. Confidential VM uses Virtual Trusted Platform Module (vTPM) attestation. Every time an AMD SEV-based Confidential VM boots, a launch attestation report event is generated. And high performance. AMD SEV offers high performance for demanding computational tasks. Enabling Confidential VM has little or no impact on most workloads, with only a 0-6% degradation in performance.

### Video - [Certificate Authority Service](https://www.cloudskillsboost.google/course_templates/87/video/514090)

* [YouTube: Certificate Authority Service](https://www.youtube.com/watch?v=ZfItKebER6w)

Now let’s discuss Certificate Authority Service. Enterprise organizations are frequently surprised at the extent of extra work that they must do having bought CA technology to bridge the gap between technology and their organizational and business goals. A globally scalable CA is often hard to deploy and manage because it requires deep expertise that many organization do not have. Traditional CAs are also not well suited for new cases, such as microservices and DevOps because they are often inflexible, don’t scale well, and are not integrated into the deployment and access control infrastructures commonly used within the Cloud Services. Lastly, traditional CAs are expensive with high Capex and infrastructure, hardware and software licensing, in addition to expensive ongoing costs. With Google Cloud’s Certificate Authority Service, you can solve these challenges. Certificate Authority Service is a highly-available, scalable Google Cloud service that enables IT and security teams to simplify and automate the deployment, management, and security of private certificate authorities (CA) while staying in control of their private keys. Certificate Authority Service provides you with Simpler deployment and management. Simplify the deployment, management, and security of your enterprise infrastructure with a cloud service that automates time-consuming, risky, and error-prone infrastructure tasks, giving you more time to focus on higher-value projects. Tailored for you. Customize Certificate Authority Service to your needs by configuring custom CAs and certificates, enforcing granular access controls, automating common tasks with robust APIs, and integrating with your existing systems. Enterprise-ready. Have peace of mind knowing that the service is highly available, scalable, backed by an SLA, auditable, and ready to help you achieve compliance with advanced hardware and software security controls.

### Video - [What Certificate Authority Service provides](https://www.cloudskillsboost.google/course_templates/87/video/514091)

* [YouTube: What Certificate Authority Service provides](https://www.youtube.com/watch?v=8r-zLRicboQ)

Certificate Authority Service allows you to: Deploy in minutes. Create a private CA in minutes versus the days and weeks that it takes to deploy a traditional CA. Leverage descriptive RESTful APIs to acquire and manage certificates without being a PKI expert. Focus on higher-value tasks. Offload time-consuming tasks like hardware provisioning, infrastructure security, software deployment, high-availability configuration, disaster recovery, backups, and more to the cloud. Pay-as-you-go (at GA). Lower your total cost of ownership and simplify licensing with pay-as-you-go pricing and zero capital expenditures. Pay only for what you use when the service becomes generally available. Certificate Authority Service allows you to: Customize to your needs. Scale from simple use cases to advanced by configuring the root CA (e.g. existing on-premises or cloud), custom key sizes and algorithms, region of the CA independent of the root CA, and more. Manage it your way. Manage, automate, and integrate private CAs and certificates in a way that is most convenient for you: via APIs, gcloud command line, or the Google Cloud console. And enforce granular access. Define granular and context-aware access controls and virtual security perimeters with IAM and VPC Service Controls (at GA). Certificate Authority Service gives you the ability to: Protect the keys with a HSM. Store the CA keys in Cloud HSM, which is FIPS 140-2 Level 3 validated and available in several regions across the Americas, Europe, and Asia Pacific. Audit user activity. Obtain tamper-proof logs and gain visibility into who did what, when, and where with Cloud Audit Logs. And scale with confidence. Scale with confidence knowing that the service supports 25 queries per second (QPS) per instance, can issue millions of certificates, and comes with an enterprise-grade SLA (at GA).

### Video - [Compute Engine best practices](https://www.cloudskillsboost.google/course_templates/87/video/514092)

* [YouTube: Compute Engine best practices](https://www.youtube.com/watch?v=hQRdBbmVQv4)

Now let’s discuss Compute Engine best practices. First of all, always ensure the proper permissions are given to control access to resources. Projects form the basis for creating, enabling, and using all Google Cloud services, including managing resource permissions. Build for success, and utilize projects and IAM roles to control access. Host Compute Engine resources on the same VPC network where they require network based communication. If the resources aren’t related and don’t require network communication among themselves, consider hosting them on different VPC networks. Secure connections to public cloud providers are a concern for all organizations. You can securely extend your data center network into projects with Cloud Interconnect or Cloud VPN. Use Cloud Audit Logging to generate logs for API operations performed in Google Compute Engine. Audit logs help you determine “who did what”, “where”, and “when”. Specifically, audit logs track how Compute Engine resources are modified and accessed within projects for auditing purposes. By default, users in a project can create persistent disks or copy images using any of the public images and any images that your project members can access through IAM roles. You may want to restrict your project members so that they can create boot disks only from images that contain approved software that meets your policy or security requirements. You can define an organization policy that only allows Compute Engine VMs to be created from approved images. This can be done by using the trusted Images Policy to enforce which images can be used in your organization. This allows you to host organization-approved, hardened images in your Google Cloud environment. Hardening a custom OS image will help reduce the attack surface for the instance. Making hardened images available in your organization can help reduce your organization’s overall risk profile. However, if you create a custom image, formulate a plan for how to maintain the image with security patches and other updates. Compute Engine doesn't automatically update the OS or the software on your deployed instances. You will need to patch or update your deployed Compute Engine instances when necessary. However, it is not recommended that you patch or update individual running instances. This could end up being a lot of work and risks a chance that something could be missed in the process. Instead, it is best to patch the image that was used to launch the instance and then replace each affected instance with a new copy. In general, Google recommends that each instance that needs to call a Google API should run as a service account with the minimum permissions necessary for that instance to do its job. In practice, this means you should configure service accounts for your instances like this: Create a new service account rather than using the Compute Engine default service account. Grant IAM roles to that service account for only the resources that it needs. And configure the instance to run as that service account. Subscribe to “gce-image-notifications” announcements to receive release notes and other updates regarding public Compute Engine images. A link is provided in this module’s course resources. This will be of interest to anyone looking to keep up with the latest information about Compute Engine Images, feel free to subscribe.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/87/video/514093)

* [YouTube: Module review](https://www.youtube.com/watch?v=eBOTq522ItA)

Before we move to the next module, let’s review some key concepts from this one. Default service accounts are how projects communicate within Google Cloud - but they need to be properly configured. Every Google Cloud project has a default service account that is automatically created when compute engine is first enabled for the project. This default service account is assigned the “project editor role” and is used by default when launching VMs. You can also create and manage your own service accounts using Google Identity and Access Management. Access scopes provide the ability to limit what permissions are allowed when using the default service account with IAM Project Editor role permissions. You set access scopes when creating an instance and the access scopes persists only for the life of that instance. Another option is to set the access for each API individually, which allows you to grant access to only to the APIs required by the programs running on the VM. There are several options for accessing machines remotely on Google Cloud. By default, Linux instances on Google Cloud are accessed with SSH and require a username and an SSH key for authentication. Password authentication is disabled by default. It is also possible to connect to a Linux instance via SSH using the gcloud SDK. Window instances are accessed using RDP and require a username and password to authenticate. The username and password can be set from the Cloud Console or using gcloud. It is also possible to SSH from any SSH client such as PuTTY on Windows or ssh applications on Linux or Mac computer using the public IP address of the instance, a valid username and SSH private key. You can manually create your own SSH key pairs, using tools like PuTTYgen or ssh-keygen. You can also use OS Login to manage SSH access to your instances using IAM without having to create and manage individual SSH keys. The Organization Policy Service gives you centralized and programmatic control over your organization's cloud resources. An organization policy administrator can configure restrictions across your entire resource hierarchy. A constraint is a particular type of restriction (List or Boolean) against a Google Cloud service or a list of Google Cloud services. Compute Engine best practices can help you create more secure instances as well as keep them secure. Compute Engine best practices include: control access, isolate machines, connect securely and regularly monitor and audit logs. In addition, be sure to keep instances updated, and avoid using the default service account, especially if it is unmodified. Ok, you are now ready to move on to the next module!

### Quiz - [Quiz: Securing Compute Engine](https://www.cloudskillsboost.google/course_templates/87/quizzes/514094)

#### Quiz 1.

> [!important]
> **Which TWO recommendations below ARE considered to be Compute Engine "best practices?"**
>
> * [ ] Utilize projects and IAM roles to control access to your VMs.
> * [ ] Cloud Interconnect or Cloud VPN can be used to securely extend your data center network into Google Cloud projects.
> * [ ] Hardened custom images, once added to your Organization's resources, are then maintained by Google with automatic security patches and other updates.
> * [ ] Always run critical VMs with default, scope-based service accounts.

#### Quiz 2.

> [!important]
> **Which of the following TWO statements about Google Cloud service accounts are TRUE?**
>
> * [ ] VMs without service accounts cannot run APIs.
> * [ ] Virtual Machine (VM) instances use service accounts to run API requests on your behalf.
> * [ ] Custom service accounts use "scopes" to control API access.
> * [ ] Service accounts are a type of identity.

#### Quiz 3.

> [!important]
> **Which TWO of the following statements is TRUE when discussing the Organization Policy Service?**
>
> * [ ] Organization Policy Services allow centralized control for how your organization's resources can be used.
> * [ ] Descendants of a targeted resource do not inherit the parent's Organization Policy.
> * [ ] To define an Organization Policy, you will choose and then define a constraint against either a Google Cloud service or a group of Google Cloud services.

## Securing Cloud Data: Techniques and Best Practices

In this module we discuss controlling IAM permissions and access control lists on Cloud Storage buckets, auditing cloud data, including finding and remediating data that has been set to publicly accessible, how to use signed Cloud Storage URLs and signed policy documents, and encrypting data at rest. In addition, BigQuery IAM roles and authorized views will be covered to demonstrate managing access to datasets and tables. The module will conclude with an overview of storage best practices

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/87/video/514095)

* [YouTube: Module overview](https://www.youtube.com/watch?v=wJFrl66cSdA)

Welcome to the module Securing Cloud Data: this is the Techniques and Best Practices module, part of the Security in Google Cloud course. Securing your cloud data is obviously extremely important. In this module, we are going to cover many topics related to securing Google Cloud storage, and there will be several labs so you can get some hands-on experience with the topics we discuss. We will start with controlling IAM permissions and access control lists on Cloud Storage buckets. We will also discuss auditing cloud data, including finding and remediating data that has been set to publicly accessible. Then we will cover how to use signed Cloud Storage URLs and signed policy documents to provide detailed control on what users without Google Cloud accounts can download or upload to a bucket. Next, we will discuss encrypting data at rest with customer managed encryption keys and customer supplied encryption keys and give you a chance to see it in action with a few hands on labs. After that, we will talk about adding extra security to how data is encrypted and decrypted using Cloud HSM - or, the Cloud Hardware Security Module. In addition, BigQuery IAM roles and authorized views to demonstrate managing access to datasets and tables. You will also get a chance to try this out in a lab. This module will conclude with a few considerations on storage best practices.

### Video - [Cloud Storage IAM permissions and ACLs](https://www.cloudskillsboost.google/course_templates/87/video/514096)

* [YouTube: Cloud Storage IAM permissions and ACLs](https://www.youtube.com/watch?v=5FAlx79TSGk)

OK, let’s get started with Cloud Storage IAM permissions and ACLs. You can control who has access to your Cloud Storage buckets and objects as well as what level of access they have. Members can be granted access to Cloud Storage at the organization, folder, project, or bucket levels. Any allocated permissions flow down from higher levels. Consider using the principle of least privilege. You can also define deny rules that prevent certain principals from using certain permissions, regardless of the roles they're granted. A link to more information on deny rules is provided in this module’s course resources. A few of the pre-defined storage roles are shown here. The Storage Object Admin role, as its name implies, provides full control of Cloud Storage objects. The Storage Object Creator role, provides the ability to get and list projects as well as create Cloud Storage objects. The Storage Object Viewer role provides the ability to get and list projects as well as get and list Cloud Storage objects. The orgpolicy.policy.get permission allows principals to know the organization policy constraints that a project is subject to. This permission is currently only effective if the role is granted at the project level or above. Resource Manager is the name of the Google Cloud API that you use to manage your organization's Google Cloud resources. For more information about the resourcemanager.projects. * permissions, refer to the Access control for projects with IAM documentation. IAM permissions get you broad control over your projects and buckets, but not fine-grained control over individual objects. You can use Access Control Lists (or ACLs) to define who has access to individual buckets and objects, as well as what level of access they have. You apply ACLs to individual buckets and objects. IAM and ACLs can work in tandem to grant access to your buckets and objects. A user only needs a permission from either IAM or an ACL to access a bucket or object. In most cases, you should use IAM permissions instead of ACLs. Use ACLs only when you need fine-grained control over individual buckets or objects. In Cloud Storage it is possible to make entire buckets or individual objects public. An entire bucket can be made public by granting the Storage Object Viewer role to the allUsers group on the bucket. To make individual objects in a bucket public, grant the Reader access to allUsers on the individual objects. Be very careful on which objects or buckets you make public - use this option with extreme caution. Making buckets or objects public should only be done for publicly accessible web content.

### Video - [Auditing cloud data](https://www.cloudskillsboost.google/course_templates/87/video/514097)

* [YouTube: Auditing cloud data](https://www.youtube.com/watch?v=8Hj1GdLayG4)

Another important security concern is how to audit cloud data and record project related activity. In Google Cloud, Cloud Storage bucket administrative activity is logged automatically - there is nothing to enable. Admin Activity includes operations that modify the configuration or metadata of a bucket, or object. Note that Data Access logs pertaining to Cloud Storage operations are not recorded by default and must be configured. Data Access includes operations that modify objects or read a project, bucket, or object. All logs can be viewed in Cloud Logging. The data access logs can be turned on at the bucket level. To do so, first create a new bucket to hold the log files. You must allow write access to that bucket for cloud-storage-analytics@google.com. To be safe, we also set the ACL of the logging bucket to project-private. Then, enable logging for the original bucket, specifying that the bucket holds logs with the -b option. One of the most effective ways to analyze log files is to leverage BigQuery. To do so, you must first create a BigQuery dataset and this will be used to hold the log data with the bq mk  command. From this point, the log files can then be loaded into a BigQuery table with the bq load command. The example shown here is loading some log files into two different BigQuery tables, one called “usage” and the other “storage”. Once the logs are exported to BigQuery, they can then be analyzed using the power that BigQuery offers.

### Video - [Signed URLs and policy documents](https://www.cloudskillsboost.google/course_templates/87/video/514098)

* [YouTube: Signed URLs and policy documents](https://www.youtube.com/watch?v=19T8idfEkdc)

In this section, we will cover how to use signed Cloud Storage URLs and signed policy documents which provide detailed control over users; such as those without Google Cloud accounts, or visitors, enabling them to download or upload to a bucket. In some scenarios, you might not want to require your users to have a Google account in order to access Cloud Storage. Signed URLs provide a great way to give time-limited read or write access to anyone in possession of the URL, regardless of whether they have a Google account. Signed URLs can be created manually using the gsutil tool or from a program. Google App Engine applications can use the App Engine Identity service. When creating a signed URL with gsutil, it is a 3 step process: First, you need to create a service account with the desired rights and then use this to generate the signed URL for storage object Next, create a key for the service account and store it in a file such as key.json Lastly, use the gsutil signurl command to specify the service account key and object that we want to allow access to. Note the -d parameter specifies how long (i.e. the duration) the URL will be available for. Once requested, the gsutil command will return the signed URL. An example of a signed URL is shown on this slide. Signed Policy Documents specify what can be uploaded to a bucket with a form POST. Policy documents allow greater control over size, content type, and other upload characteristics, in comparison to signed URLs, and can also be used by website owners to allow visitors to upload files to Cloud Storage. A policy document is constructed in JavaScript Object Notation (JSON). In this slide you can see an example of a typical policy document that defines the following conditions: The form expires on August 15, 2023 at 11:11:11 UTC. The file name can start with any valid character. To specify this, you can use the “starts-with condition modifier” with a value of an empty string An ACL of “bucket-owner-read” is applied to the file. If the upload is successful, the user is redirected to http://www.example.com/success.html. The form also allows only jpeg images to be uploaded. And the form restricts uploading files larger than 1 megabyte. Using a policy document for an HTML form post requires the following: Create a policy document for your form, ensuring it is UTF-8 encoded. Encode the policy document as a Base64 representation. Sign your policy document using RSA with SHA-256 encoding using the secret key provided to you in the Google Cloud console. This step will create a message digest. Encode the message digest as a Base64 representation. And add the policy document information to the HTML form. Here is an example HTML form using the policy document shown earlier. Notice there are hidden form fields for each value and most match the values from the policy document. The GoogleAccessId is used to specify the service account required for access The value of the policy field must be the base64 encoded policy document The value of the signature field must be the base64 signature of the policy document.

### Video - [Encrypting with CMEK and CSEK](https://www.cloudskillsboost.google/course_templates/87/video/514099)

* [YouTube: Encrypting with CMEK and CSEK](https://www.youtube.com/watch?v=mzKCBB5lf9k)

Let’s now discuss data encryption. Google Cloud encrypts all customer data stored at rest, without any action required from you, the customer. A common cryptographic library is used to implement encryption consistently across almost all Google Cloud products. This includes data stored in Cloud Storage, Compute Engine persistent disks, Cloud SQL databases - virtually everything! Even disk snapshots and custom Compute Engine virtual machine images are encrypted. Let’s have a closer look at how Google Cloud encrypts data at rest. All data stored in Google Cloud is encrypted with a unique data encryption key or DEK. More specifically, data is then broken into sub file chunks for storage; each chunk can be up to several gigabytes in size. Each chunk of data is then encrypted at the storage level with a unique key. Note that two chunks will not have the same encryption key, even if they are part of the same Cloud Storage object, owned by the same customer, or stored on the same machine. The encrypted data chunks are then distributed across Google’s storage infrastructure. This partition of data, each using a different key, means the "blast radius" of a potential data encryption key compromise is limited to only that data chunk. The data encryption keys are encrypted with (or “wrapped” by) key encryption keys, or KEKs. The wrapped data encryption keys are then stored with the data. The key encryption keys are exclusively stored and used inside Keystore, a repository built specifically for storing keys. Keystore was formerly known as Google's key management service. It is different from Cloud KMS, which manages the encryption keys for Google Cloud customers and helps customers to create their own tenant keys. Keystore-held keys are also backed up for disaster recovery purposes, and are indefinitely recoverable. Decrypting data requires the unwrapped data encryption key (DEK) for that data chunk. When a Google Cloud service accesses an encrypted chunk of data, here’s what happens: For each chunk, the storage system pulls the wrapped DEK stored with that chunk, and calls Keystore to retrieve the unwrapped data encryption key for that data chunk. Keystore then passes the unwrapped DEK back to the storage system, which is then able to decrypt the data chunk. Let's take a moment to briefly review Google Cloud Encryption. Google Cloud encrypts all customer data stored at rest, without any action required from you, the customer. A common cryptographic library, Keyczar, is used to implement encryption consistently across almost all Google Cloud products. This includes data stored in Cloud Storage, Compute Engine persistent disks, Cloud SQL databases, virtually everything! Even disk snapshots and custom Compute Engine virtual machine images are encrypted. By default, KEKs are stored and used inside Keystore and are fully managed by Google. There is nothing for you, the customer, to enable or configure. This entire process is enabled by default and is fully managed by Google - including the key encryption keys. There is absolutely nothing to enable or configure. Keystore was built solely for the purpose of managing KEKs. By design, KEKs used by storage systems aren't exportable from Keystore; all encryption and decryption with these keys must be done within Keystore. This helps to prevent leaks and misuse, and it enables Keystore to create an audit trail when keys are used. Keystore can automatically rotate KEKs at regular time intervals, using Google's common cryptographic library to generate new keys. Though we often refer to just a single key, we really mean that data is protected using a key set: one key is active for encryption, and a set of historical keys is active for decryption. You can control the generation of the keys, the rotation periods, and when to expire keys. Customer-managed keys are still stored in Cloud KMS, but you control the keys’ lifecycle. Cloud KMS uses an object hierarchy: a key belongs to a key ring, and a key ring resides in a particular location. A keyring is a grouping of keys attached to a project and created for organizational purposes. When creating keys, you must first create a key ring and specify its location, which can be regional, multi-regional, or global. A key can then be created and added to the key ring. Cloud KMS supports both symmetric and asymmetric key types. The keys rotation period can also be defined to meet your requirements. Using customer-managed keys is as simple as choosing the key when creating VMs, disks, images, or storage buckets. You also need to grant permissions to the service account to be able to use your key. The general process is the same as when using the default Google-managed keys. Another available option for encryption is customer-supplied keys. Remember, Google will not store these keys. You are responsible for ALL key management and rotation. Be sure to not lose these keys. If you lose them, there will be no way to decrypt your data. When using customer-supplied encryption, since Google never stores the keys, you must provide the appropriate key whenever accessing the storage resource. For example, if a persistent disk is created with a customer-supplied encryption key, that key must be specified each time the disk is attached to an instance. Customer-supplied encryption keys (CSEK) are a feature in Cloud Storage and Compute Engine. CSEKs allow you to use their own encryption keys to encrypt data at rest in Google Cloud. When you supply your own encryption keys, Google uses your key to protect the Google-generated keys that are used to encrypt and decrypt your data. To end this section on encryption, let's talk about Cloud External Key Manager, or Cloud EKM. With Cloud EKM, you can use keys that you manage within a supported external key management partner to protect data within Google Cloud. You can protect data at rest in supported CMEK integration services, or by calling the Cloud Key Management Service API directly. Cloud EKM provides several benefits: The first is key provenance. You control the location and distribution of your externally managed keys. Externally managed keys are never cached or stored within Google Cloud. Instead, Cloud EKM communicates directly with the external key management partner for each request. The second is access control. You manage access to your externally managed keys. Before you can use an externally managed key in Google Cloud, you must grant the Google Cloud project access to use the key. You can revoke this access at any time. And finally, you have centralized key management. You can manage your keys and access policies from a single user interface, whether the data they protect resides in the cloud or on your premises. In all cases, the key resides on the external system, and is never sent to Google. You can communicate with your external key manager via the internet or via a Virtual Private Cloud (VPC). Cloud EKM works like this: First, you create or use an existing key in a supported external key management partner system. This key has a unique URI or key path. Next, you grant your Google Cloud project access to use the key, in the external key management partner system. In your Google Cloud project, you then create a Cloud EKM key using the URI or key path for the externally managed key. Within Google Cloud, the key appears alongside your other Cloud KMS and Cloud HSM keys, with protection level EXTERNAL or EXTERNAL_VPC. The Cloud EKM key and the external key management partner key work together to protect your data. The external key is never exposed to Google. This diagram shows how Cloud KMS fits into the key management model. While this diagram uses Compute Engine and BigQuery as two examples, you can also see the full list of services that support Cloud EKM keys by referring to the documentation link in this module’s course resources.

### Video - [Lab Intro: Using Customer-Supplied Encryption Keys with Cloud Storage](https://www.cloudskillsboost.google/course_templates/87/video/514100)

* [YouTube: Lab Intro: Using Customer-Supplied Encryption Keys with Cloud Storage](https://www.youtube.com/watch?v=TA8vSoduOMQ)

Ok, It’s time for a lab to see Customer-Supplied Encryption Keys in action. In this lab, you learn how to perform the following tasks: Firstly you create an encryption key and wrap it within the Google Compute Engine RSA public key certificate. You will encrypt a new persistent disk with your own key, as well as attach the disk to a Compute Engine instance. Finally you will create a snapshot from an encrypted disk

### Lab - [Using Customer-Supplied Encryption Keys with Cloud Storage](https://www.cloudskillsboost.google/course_templates/87/labs/514101)

Using Customer-Supplied Encryption Keys with Cloud Storage

* [ ] [Using Customer-Supplied Encryption Keys with Cloud Storage](../labs/Using-Customer-Supplied-Encryption-Keys-with-Cloud-Storage.md)

### Video - [Lab Intro: Using Customer-Managed Encryption Keys with Cloud Storage and Cloud KMS](https://www.cloudskillsboost.google/course_templates/87/video/514102)

* [YouTube: Lab Intro: Using Customer-Managed Encryption Keys with Cloud Storage and Cloud KMS](https://www.youtube.com/watch?v=zZV1UFXkAqk)

Great, now let’s see how to use Customer-managed Encryption Keys. In this lab, you learn how to perform the following tasks: Firstly you will manage keys and encrypted data using Cloud Key Management Service (Cloud KMS). You will Create KeyRings and CryptoKeys, as well as set a default encryption key for a storage bucket. You will then encrypt an object with a Cloud KMS key. Next, you learn how to rotate encryption keys. Finally you will manually perform server-side encryption with Cloud KMS keys

### Lab - [Using Customer-Managed Encryption Keys with Cloud Storage and Cloud KMS](https://www.cloudskillsboost.google/course_templates/87/labs/514103)

Using Customer-Managed Encryption Keys with Cloud Storage and Cloud KMS

* [ ] [Using Customer-Managed Encryption Keys with Cloud Storage and Cloud KMS](../labs/Using-Customer-Managed-Encryption-Keys-with-Cloud-Storage-and-Cloud-KMS.md)

### Video - [Cloud HSM](https://www.cloudskillsboost.google/course_templates/87/video/514104)

* [YouTube: Cloud HSM](https://www.youtube.com/watch?v=OiTcJNAowQk)

Some industries need increased security regarding data encryption. In these situations, encryption and decryption that uses software only is not sufficient; this must be done in a secure environment, using a hardware module designed explicitly for this purpose. While Google Cloud encrypts all customer data-at-rest, some customers, especially those who are sensitive to compliance regulations, like those in finance, must maintain control of the keys used to encrypt their data. In this section, you will learn about Cloud HSM—or Cloud Hardware Security Module—and its features. An HSM, or hardware security module, is a physical device that manages digital keys. These keys are used to encrypt content in an extremely secure manner. It also uses those keys to encrypt and decrypt data with secure microprocessing chips. These chips were designed and certified to perform encryption and decryption in the hardware, with more security than can be offered by a software-only encryption and decryption solution. Doing encryption and decryption inside a separate, physical hardware module makes the keys and the data harder to hack, which adds an extra layer of security. The level of security provided by the HSM is beyond what is possible in a software-only based solution. Cloud HSM provides an HSM hardware cluster for you. Google manages and maintains the cluster for you. All you have to do is use it with the Google Cloud Console or with APIs. Cloud HSM is available in all US regions. It is also available in multiple regions worldwide. Consult the Google documentation for the closest location to your site. A link is provided in this module’s course resources. Cloud HSM supports FIPS 140-2 level 3. FIPS 140-2 is a US government computer security standard for cryptographic modules. Level 3 includes tamper-evident physical security mechanisms and has a high probability of detecting and responding to attempts at physical access, use, or modification of the cryptographic module. The physical security mechanisms may include the use of strong enclosures and tamper detection and response circuitry. Cloud HSM supports FIPS 140-2 level 3. FIPS 140-2 is a US government computer security standard for cryptographic modules. Level 3 includes tamper-evident physical security mechanisms and has a high probability of detecting and responding to attempts at physical access, use, or modification of the cryptographic module. The physical security mechanisms may include the use of strong enclosures and tamper detection and response circuitry. Cloud HSM supports Cavium version 1 and version 2 attestation formats. An attestation statement shows that the key is HSM-protected. It is a token that is cryptographically signed directly by the physical hardware and can be verified by the user. Cloud HSM is fully integrated with Cloud KMS. Everything you learned earlier about creating key rings and keys applies here as well. When creating a key, you can specify that you would like to use Cloud HSM by setting the Protection level to HSM. When HSM is selected, Google uses a physical HSM device within its environment to create and encrypt the key. The key can be generated and managed by Google or by the customer, as you learned earlier. For keys created in Cloud HSM, an attestation statement can be generated. Google provides scripts that can verify the authenticity of the attestation and parse the attestation contents. Information about scripts and how to use them can be found in the Cloud HSM documentation. A link is provided in this module’s course resources. The attestation statement provides evidence that the key is HSM protected. It contains a token that is cryptographically signed directly by the physical hardware. This can be verified by the user; for example, by using a program or script to parse the contents. Google provides scripts that can verify the authenticity of the attestation signature and parse and verify the attestation contents, such as the version ID and the metadata. Information about the scripts and how to use them is in the Cloud HSM documentation.

### Video - [Demo: Using and Verifying Keys in Cloud HSM](https://www.cloudskillsboost.google/course_templates/87/video/514105)

* [YouTube: Demo: Using and Verifying Keys in Cloud HSM](https://www.youtube.com/watch?v=k27-MemLgxo)

>> In this demo, you will learn how to generate and verify an encryption key in HSM. Il-Sung: My name is Il-Sung and I'm a product manager at Google Cloud. Today, we'll be walking through one of the top questions we get from customers, how do I generate and verify an encryption key in an HSM on Google Cloud Platform? So the first thing that we'll do is we'll go and create a key in Cloud HSM, and I want to demonstrate to you how easy this truly is. So once you create your keyring and select a region, you go to create key up there, and then you'll be faced with a UI that tells you what to do. Now, here's a key part, the only thing you have to do is change the protection level from software to HSM and that's it. And from that point forward, the key will be in HSM and the system will take care of the creation for you. So let's just create a key, let's call it demo key two, cause Eric already created demo key one. And you get different options around what kind of key, let's do a asymmetric signing, and then you also have different types of algorithms that you can choose. And this case, we'll do something, we'll use elliptic curve P256, and then we're going to generate it. We can also import if we like and that's it. And so once I hit create, the key's been created. So now that we've created our key, I'm going to show you how you can download the attestation. And the attestation is essentially a signed statement that's verified by both Google and the HSM vendor that indicates that the key was created inside the HSM and that it has certain properties. So in order to do that, all you have to do is go and click on the hamburger button and click, get attestation, and then hit download. Okay. Now once that key is downloaded, we can actually go and run some scripts that will actually do the parsing for you, or you can do it yourself, but we provide scripts that you can get through our documentation that make it a lot easier. To do this, I'm going to go and activate Cloud shell, and once Cloud shell starts, now that I've downloaded the file, let's upload the file to G-cloud so that we can actually do some cool things with it. The first thing that we want to do is that we want to validate the centure of the file to make sure that it's valid. And so in this case, I'm going to validate using the Cavium certificate chain to indicate that Cavium is signed and tested to the contents of the file itself. And so, as you can see, the centure is verified, which means that it's a valid file and everything in there should be taken as something that Cavium has vouched for. Now, I'm going to actually parse the contents of the file, and again, there are scripts available online that you can actually download to do this. And so now the part that falls parse. The next thing I'm going to do is I'm going to check the key ID that's indicated in the attestation file to make sure it matches the key ID that we expect. So to do that, I'm just going to set up a local variable to represent the key, then I'm going to use a simple command to compare the contents of the hash of the key ID that's in that attestation file with what we expect. And as you can see from the output, the two hashes match, so we know that the key that we think we're talking about is actually the key that's indicated in the attestation file itself. Now that we sure we're talking about the right key, let's actually use this to actually check for certain attributes of the key to validate and make ourselves feel good about the fact that the key has certain properties. First of all, let's check to make sure that the key is not exportable. In order to do this, you need to understand a little bit about PKCS11, but not too much. So there's a key property that's indicate by the hex 0162, which indicates that the key is exportable. And so as you can see here, there's two outputs, one says yes, and one says no, and that's to indicate the fact that there's one public key and one private key. The public key is obviously exportable and the private key is not. The next thing you can do is that you can check to make sure that the key that you created is the same kind of key that the HSM believes it created. In order to do so, you will look up property hex 0100, and again, this is something that's listed in the PKCS 11 standard, and it comes back with the value of hex 03. Now, if you look that up, you would know that this is a elliptic curve key, so it is the type of key that we specified when we generate the key in the first place. Next, we're going to look for the property that tells us whether the key was generated locally. In other words, whether it was imported or not. And so if you graph hex 0163, you'll see that it says zero one, which means that both the keys were generated locally within the HSMs and they were not imported. There are other things that you can do with attestations. So in the attestation, there's also shell 5256 hash of the public key itself, so if you were to download the public key from the UI or from the command line, you should be able to actually take a shot 2V26 hash of it, compare against the one that's in the attestation file and match them. If they match, then you now have strong cryptographic evidence that you're talking about the same key that's in the attestation file.

### Video - [BigQuery IAM roles and authorized views](https://www.cloudskillsboost.google/course_templates/87/video/514106)

* [YouTube: BigQuery IAM roles and authorized views](https://www.youtube.com/watch?v=zm7blFNCK3s)

This section covers BigQuery IAM roles and authorized views to demonstrate managing access to datasets and tables. You will also get a chance to try this out in a lab. BigQuery uses Identity and Access Management (IAM) to manage access to resources and permissions that are granted at the dataset level. BigQuery lets you assign roles individually to certain types of resources within datasets, like tables and views, without providing complete access to the dataset's resources. Table-level permissions determine the users, groups, and service accounts that can access a table or view. It also supports access control at column-level security through policy tags and at row-level security through row-level access policies. A few of the BigQuery IAM roles are shown here. Notice how the role names are mapped to job functions, such as BigQuery admin or BigQuery Data Viewer. When using BigQuery, you can assign IAM roles to individual users or groups. It is always better to assign roles to groups. There is much less operational overhead managing groups than managing each user individually. The roles you assign provides the required access permission to the datasets: for example; Viewer, Editor, or Owner. By default the user who created the dataset is the owner, but additional members can be given permissions including the owner role. What if I want admins or super users to see all the data in a table, and others to only see a subset of the data? To do this, use authorized views. Views provide row or column level permissions to datasets. To create authorized views, create a second dataset with different permissions from the first. Add a view to the second dataset that selects the subset of data you want to expose from the first dataset. In the first dataset, you must give the view access to the underlying data.

### Video - [Lab Intro: Creating a BigQuery Authorized View](https://www.cloudskillsboost.google/course_templates/87/video/514107)

* [YouTube: Lab Intro: Creating a BigQuery Authorized View](https://www.youtube.com/watch?v=GfF4CRMAZUs)

In this lab, you learn how to create a BigQuery authorized view. First, you will set permissions on BigQuery Datasets. You will then use authorized views to provide audiences read-only access to subsets of tables. Finally you will use the CURRENT_USER() function to limit access to specific rows within a table/view.

### Lab - [Creating a BigQuery Authorized View](https://www.cloudskillsboost.google/course_templates/87/labs/514108)

Creating a BigQuery authorized view

* [ ] [Creating a BigQuery Authorized View](../labs/Creating-a-BigQuery-Authorized-View.md)

### Video - [Storage best practices](https://www.cloudskillsboost.google/course_templates/87/video/514109)

* [YouTube: Storage best practices](https://www.youtube.com/watch?v=1ihHWqUJ138)

And now we conclude with a few best practices for Cloud Storage and BigQuery Storage. The first recommendation is, don't use user IDs, email addresses, project names, project numbers, or any personally identifiable information in bucket names because anyone can probe for the existence of a bucket. Remember, Cloud Storage bucket names must be unique across the entire Cloud Storage namespace, so come up with a naming convention. If you need a lot of buckets, use GUIDs or an equivalent for bucket names, put retry logic in your code to handle name collisions, and keep a list to cross-reference your buckets. Another option is to use buckets based on domain-name and then manage the bucket object names as sub-domains. Similarly, do not use any personally identifiable information as object names. Remember object names appear in URLs. Before adding objects to a bucket, first check that the default object ACLs meet your requirements. This could save you a lot of time updating ACLs for individual objects. If you need to make content available securely to users who don't have Google accounts, use signed URLs. Sometimes it may be desirable to make public buckets readable - for example: hosting publicly available web content. However, there should never be a need to make public buckets writable. Doing so would be extremely dangerous and careless. You can assign lifecycle management rules to a bucket. When an object meets the criteria of one of the rules, Cloud Storage automatically performs a specific action on the object. For example: downgrade the storage class of objects older than 365 days to Coldline Storage, or Delete objects older than 3 years. Leverage this capability to remove sensitive data once it is no longer needed. This can help reduce the scope of your security management. To get the most secure usage from BigQuery for your data storage and your applications, be sure to leverage the different BigQuery IAM roles to ensure users are provided with ONLY the permissions that align with their job functions. At a minimum, be sure to separate who is allowed to create and manage datasets from those who can query the datasets and process the data. Always ensure you are providing the principle of least privilege when it comes to providing access to sensitive data. BigQuery Authorized View can limit users to see only a subset of the data. You can control storage costs and optimize storage usage by setting the default table expiration for newly created tables in a dataset. If you set the property when the dataset is created, any table created in the dataset is deleted after the expiration period. If you set the property after the dataset is created, only new tables are deleted after the expiration period.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/87/video/514110)

* [YouTube: Module review](https://www.youtube.com/watch?v=rkZj3Ans14M)

In this module, we discussed how to secure your data on Google Cloud. Here are some of the key points to take away from this section... You can control who has access to your Cloud Storage buckets and objects as well as what level of access they have. Firstly, members can be granted access to Cloud Storage at the organization, folder, project, or bucket levels. There are many pre-defined roles that are related to Cloud Storage: Storage Object Admin role, Storage Object Creator role, and Storage Object Viewer role. Access Control Lists (or ACLs) is a mechanism you can use to define who has access to individual buckets and objects, as well as what level of access they have. In Cloud Storage, administrative operations that modify the configuration or metadata of a bucket, or object, is logged automatically. Data Access operations that modify objects or read a project, bucket, or object, is not recorded by default and must be configured. Also, all logs can be viewed in Cloud Logging and analyzed in BigQuery. Signed URLs and Signed Policy Documents give limited time permissions. Signed URLs provide a way to give time-limited read or write access to anyone in possession of the URL, even if they don’t have a Google account. Signed Policy Documents specify what can be uploaded to a bucket with a form POST. BigQuery uses Identity and Access Management (IAM) to manage access to resources. Permissions that are granted at the dataset level and tables, rows and columns are child resources of datasets. BigQuery IAM roles include Admin, Data Owner, Data Editor, Data Viewer, Job Editor, and User. Do not use any personally identifiable information as bucket or object names. Remember object names and bucket names will appear in URLs! If you need to make content available securely to users who don't have Google accounts, use signed URLs. Finally, assign lifecycle management rules to a bucket to automatically downgrade or delete data.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/87/quizzes/514111)

#### Quiz 1.

> [!important]
> **Which TWO of the following statements are TRUE when discussing Cloud Storage and IAM permissions?**
>
> * [ ] Using deny rules prevent certain principals from using certain permissions, regardless of the roles they're granted.
> * [ ] A user needs permission from both IAM or an ACL to access a bucket or object.
> * [ ] Using IAM permissions alone gives you control over your projects, buckets, and individual objects.
> * [ ] Access can be granted to Cloud Storage at the organization, folder, project, or bucket levels.

#### Quiz 2.

> [!important]
> **Which TWO of the following statements is TRUE with regards to security in BigQuery and its datasets?**
>
> * [ ] A BigQuery Authorized View allows administrators to restrict users to viewing only subsets of a dataset.
> * [ ] It is always better to assign BigQuery roles to individuals as this will help to lower operational overhead.
> * [ ] Using IAM, you can grant users granular permissions to BigQuery tables, rows and columns.
> * [ ] BigQuery has its own list of assignable IAM roles.

#### Quiz 3.

> [!important]
> **Which TWO of the following statements are TRUE when discussing storage and BigQuery best practices?**
>
> * [ ] Do not use any personally identifiable information as object names.
> * [ ] One option to serve content securely to outside users is to use signed URLs.
> * [ ] In most cases, you should use Access Control Lists (ACLs) instead of IAM permissions.
> * [ ] BigQuery data can be adequately secured using the default basic roles available in Google Cloud.

## Application Security: Techniques and Best Practices

In this module we will discuss application security techniques and best practices. We will see how Web Security Scanner can be used to identify vulnerabilities in your applications, and dive into the subject of Identity and Oauth phishing. Lastly, you will learn how Identity-Aware Proxy, or IAP, can be used to control access to your cloud applications.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/87/video/514112)

* [YouTube: Module overview](https://www.youtube.com/watch?v=jT_4Tu3Kyek)

Welcome to module 3 of Security Best Practices in Google Cloud—Application Security: Techniques and Best Practices. In this module, we will start with a discussion of a few common types of application security vulnerabilities. And together we will see how Google Cloud’s Web Security Scanner can be used to identify vulnerabilities in your applications. You will get to perform a lab that uses Web Security Scanner to detect a vulnerability in an App Engine application. Then will will discuss the threat of Identity and Oauth phishing. After that, you will see how the Identity-Aware Proxy or IAP can be used to control access to your cloud applications. You will also get to secure Compute Engine applications with IAP in a lab. Finally, we will discuss general application security techniques and best practices.

### Video - [Types of application security vulnerabilities](https://www.cloudskillsboost.google/course_templates/87/video/514113)

* [YouTube: Types of application security vulnerabilities](https://www.youtube.com/watch?v=dNuQ3ApW-Tc)

Let’s begin with some types of application security vulnerabilities. Developers are frequently given a requirements document that defines the desired features and functional requirements for the application. The code is written and tested many times against these features … but, the security of the application is often undefined and remains untested. Deadline pressures also promote the release of vulnerable code. Attackers also know this and will attack applications more often than any other target - more often than networks, more often than infrastructure. Often times security is not part of the design process in application development—in many cases it is an afterthought. This means instead of being proactive about security, many times application security becomes a reactive process. To help you better secure your applications, let’s discuss a few common application vulnerabilities. Please note that this is not meant to be a comprehensive list of all vulnerabilities - that would be impossible to create. That is because application vulnerabilities are quite variable, and are highly dependent upon how an application is constructed, and what resources they use. Therefore, in this module, we will mainly be discussing some of the more commonly found vulnerabilities. Some of the most common application vulnerabilities are categorized as “Injection flaws.” Injection flaws occur when some form of malicious content can be “injected” into an application from an attacker and the application will then accept and interpret that content. Injection flaws come in many flavors, including SQL injection, LDAP injection, and HTML injection. Another common vulnerability is cross-site scripting, or XSS as it is also known. Cross-site scripting is actually a form of injection where the attacker is able to inject javascript into an application, and the code injection originates from a different site. Cross-site scripting allows attackers to execute scripts in the victim's browser which can then hijack user sessions, deface web sites, or redirect the user to other malicious sites. Authentication, access control, and session management are “application logic” functions which are often implemented insecurely. When insufficient control over authentication and access is exercised, this vulnerability allows attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users' identities. Applications in general, including many web applications, also do not properly protect sensitive user data. Attackers may steal or modify weakly-protected data to facilitate credit card fraud, identity theft, or other data and identity crimes. Sensitive data risks being compromised any time it is transferred without extra protection. Secure data transfer requires encryption at rest or in transit, and special precautions when exchanged with the browser. Security misconfiguration is commonly a result of insecure default configurations, incomplete or ad hoc configurations, open cloud storage, misconfigured HTTP headers, and verbose error messages containing sensitive information. Not only must all operating systems, frameworks, libraries, and applications be securely configured, but they must also be patched/upgraded in a timely fashion in order to keep them secure. Applications using components with known vulnerabilities may undermine application defenses and enable various attacks and impacts. Components, such as libraries, frameworks, and other software modules generally run with the same privileges as the main application itself. As a result, when a vulnerable component is exploited, an attack may facilitate serious data loss or even a server takeover.

### Video - [Web Security Scanner](https://www.cloudskillsboost.google/course_templates/87/video/514114)

* [YouTube: Web Security Scanner](https://www.youtube.com/watch?v=ivN8q4iQ5S0)

Next, let’s look at how Web Security Scanner can be used to identify vulnerabilities in your applications. Afterwards, you will have hands-on practice with Web Security Scanner to detect a vulnerability in an App Engine application. Web Security Scanner is a web security scanner which probes for common vulnerabilities in App Engine, Google Kubernetes Engine, and Compute Engine applications. It can automatically scan and detect four common vulnerabilities, including cross-site-scripting, Flash injection, mixed content (HTTP in HTTPS), and outdated/insecure libraries. You can easily set up, run, schedule, and manage your security scans using the Scanner and this is free for Google Cloud users. Web Security Scanner supports categories in the OWASP Top Ten, a document that ranks and provides remediation guidance for the top 10 most critical web application security risks, as determined by the Open Web Application Security Project (OWASP). A link to the OWASP Top Ten document is contained in this module’s course resources. After you set up a scan, Web Security Scanner automatically “crawls” through your application, following all links within the scope of your starting URLs. Certain links can also be excluded if needed. Once the scanner has been launched. It will attempt to exercise or activate as many user inputs, controls, and event handlers as possible. If you need to test areas of an application that are only accessible after authentication, the scanner can be provided with application credentials that will allow it to authenticate to the system. Once authenticated, the scanner can then scan resources which are only available to authenticated users. Additionally certain properties can be configured in the scanner, including the user agent type, and a maximum request rate (queries per second) to be used when performing a scan. The scanner’s application logic has been optimized to help avoid false positives. You can find the vulnerabilities reported in a few different places: Firstly, in the Web Security Scanner "Results” tab. Next up, in Security Command Center, and finally in Cloud Logging. Scans can run on a schedule, or can be manually initiated at any time. This allows the scanner to both perform scans on demand or have them scheduled to run in the future. Scheduled scans can be configured to run regularly on an automated scheduled, for example, once per day, or once per month. The duration of the scan can depend on the size and complexity of your application. Large, complex applications can take hours to be completely scanned, and as a result, such scans may need to be run less frequently. When using Web Security Scanner, there are a few things to consider. When performing scans, the scanner generates a real measurable load against your application as it populates fields, pushes buttons, clicks links, and so on. The scanner can, and almost certainly will, change state data in your application, so it should always be used with caution. For example, when scanning a blog application that allows public comments, Web Security Scanner may post test strings as comments on all your blog articles. Tactics to avoid unwanted impact may include: Running scans in a test environment to ensure your production systems are not affected. Ensure that your test environment is as close as similar as possible to your production environment so you don’t receive any false positives or negatives. Using test accounts when providing the scanner with authentication information. Many applications present a special workflow during a user's first-time login, such as accepting terms, creating a profile, and so on. If your application has a different flow for first-time users, keep this in mind when choosing a user account for your scan. Because of the different workflow, a new user account completing the first time flow can yield different scan results than an established user account. It's best to scan with an account that is in the normal user state, after the first-time flow is complete. Blocking specific UI elements that you do not want to be activated by applying the CSS class “inq-no-click”. Event handlers attached to this element are not activated during crawling and testing, regardless of whether they are inline JavaScript, or attached using addEventListener, or attached by setting the appropriate event handler property. Blocking specific URLs in the scanner by specifying URL patterns that will not be crawled or tested. And, finally, by making a backup of all data before scanning so the data can be restored to its original state prior to a scan if needed. Depending upon the type and size of your application, there may be other steps you can take to minimize disruption and impact on your services when scanning.

### Video - [Lab Intro: Identify Application Vulnerabilities with Security Command Center](https://www.cloudskillsboost.google/course_templates/87/video/514115)

* [YouTube: Lab Intro: Identify Application Vulnerabilities with Security Command Center](https://www.youtube.com/watch?v=d2TpEa9Pn6A)

In this lab you’ll use Web Security Scanner, a built-in service of Security Command Center, to detect a vulnerability in a Python Flask application. The vulnerability will be fixed and then Web Security Scanner will be used to verify it has been corrected.

### Lab - [Identify Application Vulnerabilities with Security Command Center](https://www.cloudskillsboost.google/course_templates/87/labs/514116)

In this lab, you will use Web Security Scanner—one of Security Command Center's built-in services—to scan a Python Flask application for vulnerabilities.

* [ ] [Identify Application Vulnerabilities with Security Command Center](../labs/Identify-Application-Vulnerabilities-with-Security-Command-Center.md)

### Video - [Threat: Identity and Oauth phishing](https://www.cloudskillsboost.google/course_templates/87/video/514117)

* [YouTube: Threat: Identity and Oauth phishing](https://www.youtube.com/watch?v=MXlIoyEnl0M)

Next, we will consider identity and Oauth phishing threats. Phishing attacks, an attempt to obtain sensitive information from a user or site, pose a constant and significant threat to business. The information targeted can be anything of potential value to the hacker, especially usernames, passwords, and credit card numbers. Sometimes information by itself does not appear to be sensitive, like your age or email address. But when combined with other information collected, these seemingly innocuous identity fragments could help an attacker impersonate you. Identity - especially online - is a complex set of fragments that can be gathered and combined to compromise a system or a user. The key to reducing risk of impersonation is to hide or secure as many of the pieces of the “identity puzzle” as possible. One way a phisher may attempt to gain identity information is by creating web sites that look almost exactly like trusted web sites, and then creating a scenario where a user is strongly enticed to click into that site and give up personal information. For example, a hacker may create a duplicate Facebook website on their own servers, designed to lure in the unwary, and then send an email or popup notice to a visitor that convinces them that they must log into “Facebook” for some reason. However, once the user has typed their email address and password into the site, that information is then given to the hacker, and they will have been phished. The hacker could then login to their real Facebook account and take all the info available there. Depending on how you have your Facebook account set up, which games you may have played and which services you may have used, they may be able to gain enough additional information to compromise your identity on other sites. It can be a long process to steal enough identity fragments to compromise someone’s identity, but it can be worth it to the hacker. OAuth phishing is a type of credential phishing that takes advantage of the Open Authentication (OAuth) standard to gain access to user accounts without needing to know the user’s password. OAuth phishing exploits the trust relationship users have with well-known online service providers, as well as the trust relationship those providers have with their own third-party applications. It is accomplished by tricking users into granting persistent access to their accounts. OAuth phishing attacks do not use the typical approach used in email phishing (spoofed URL link, sign-in request, or attached file). That makes this type of phishing more difficult for a less experienced user to detect and may produce a higher rate of success for the hacker. In some cases, the deception is very well designed and may even affect more experienced and competent users.

### Video - [Identity-Aware Proxy (IAP)](https://www.cloudskillsboost.google/course_templates/87/video/514118)

* [YouTube: Identity-Aware Proxy (IAP)](https://www.youtube.com/watch?v=K7RSnjiRXW4)

You will now see how the Identity-Aware Proxy, or IAP, can be used to control access to your cloud applications. You will also get to secure Compute Engine applications with IAP in a lab. In 2011, Google came up with a new approach for access management - the BeyondCorp enterprise security model. It started as an internal Google initiative to enable every employee to work from untrusted networks without the use of a traditional VPN. BeyondCorp shifts access controls from the network perimeter to individual users and devices, thereby allowing employees to work more securely from any location. This transforms work into a truly edgeless world. As end users work outside of the office more often and from many different types of devices, enterprises have common security models they are looking to extend to all users, devices, and applications. Here are some common use cases: Ensure employees are prevented from copying and pasting sensitive data into email or saving data into personal storage such as Google Drive. Only allow enterprise-managed devices to access certain key systems. Provide DLP protections for corporate data. Gate access based on a user's location. Protect applications in hybrid deployments that use a mix of Google Cloud, other cloud services platforms, or on-premises resources. This diagram demonstrates the baseline solution components for BeyondCorp Enterprise. As you can see, each solution component can be fulfilled by a specific Google Cloud service. There are three key enforcement points that we will discuss. The first enforcement point is covered at length in the Cloud Identity lesson of the Securing access to Google Cloud module in the Managing Security in Google Cloud course. This is used to extend context-aware for Google Workspace tools, enforce access controls for Gmail, Docs, Sheets, and 3rd party solutions via SAML. We also talked about VPC Service Controls, which have a number of important security features that allow you to mitigate data exfiltration risks, configure private communication between cloud resources and hybrid VPC networks, and enforce context-aware access controls. For example, allow access based on user location or source IP address. You can also manage these virtual security perimeters centrally, so it’s easy to add or take away projects and services from a service access zone that you set up. IAP is an enterprise security model that enables every employee to work from untrusted networks without the use of a VPN. Identity-Aware Proxy (IAP) provides a central authentication and authorization layer for your applications over HTTPS. IAP replaces end-user VPN tunnels or the need to apply an authentication and authorization layer in front of a web application hosted on Google Cloud. At Google, we use IAP to control access to internal applications without the need to use end-user VPNs. A Google employee can simply access needed applications from anywhere. Whenever you attempt to log into an application, your request is forwarded to IAP, which requires the user to log in. Once logged in, the proxy will determine if the user is allowed to access that application. If authorization is obtained for the user, they are then forwarded to the requested application page. IAP lets you manage access to App Engine instances, Compute Engine instances, and Google Kubernetes Engine clusters with a central authentication and authorization layer for your applications over HTTPS. IAP provides context-aware access for SSH and RDP, which allows you to control access to VMs based on a user’s identity and context (e.g. device security status, location, etc). IAP also supports web-apps that even run outside Google Cloud. IAP provides a much simpler administration process and with reduced operational overhead than more traditional VPN solutions. There is no VPN to implement or VPN clients to install and maintain. It also makes the end user experience more streamlined, as the user no longer must launch the VPN client and sign into the VPN.

### Video - [Lab Intro: Securing Compute Engine Applications with BeyondCorp Enterprise](https://www.cloudskillsboost.google/course_templates/87/video/514119)

* [YouTube: Lab Intro: Securing Compute Engine Applications with BeyondCorp Enterprise](https://www.youtube.com/watch?v=D3VnmTLYSpk)

In this lab, you learn how to perform the following tasks: Configure OAuth Consent. Set up OAuth access credentials. Set up IAP access for the deployed application. And use IAP to restrict access to the application.

### Lab - [Securing Compute Engine Applications with BeyondCorp Enterprise](https://www.cloudskillsboost.google/course_templates/87/labs/514120)

Securing Compute Engine Applications and Resources using BeyondCorp Enterprise

* [ ] [Securing Compute Engine Applications with BeyondCorp Enterprise](../labs/Securing-Compute-Engine-Applications-with-BeyondCorp-Enterprise.md)

### Video - [Secret Manager](https://www.cloudskillsboost.google/course_templates/87/video/514121)

* [YouTube: Secret Manager](https://www.youtube.com/watch?v=oGzCk41YDhs)

In this lesson, we will discuss Secret Manager and how it is used to store and access sensitive data. Many applications require credentials to authenticate; for example, API keys, passwords, or certificates. For applications to start or open automatically, this information must be readily available and stored somewhere in your environment. Storing this information in a flat text file makes the information easy to access, but it requires file protection. You do not want everyone to see this information. Of course, one way to protect files is to use the file operating system to apply permissions to restrict access as needed. This can lead to “secret sprawl” - with secrets scattered across an organization’s cloud and on-premises infrastructure. Secret Manager provides a secure, convenient way to store sensitive information. You can store, manage, and access secrets as binary blobs or text strings. Only users with the appropriate permissions can view the contents of the secret. Secret Manager reduces secret sprawl within a Google Cloud deployment. When secrets are secured in a single place, secret sprawl is eliminated. You can use IAM to determine who has access to the secrets, and also the kind of access they have. Global names and replication. The secret name is global but, optionally, its data can be stored regionally. You will learn more about this later in the module. Secrets can be versioned. Each version can have different secret data to protect. There is no limit on the number of versions that can be stored. Principles of least privilege: Secrets are created at the project level. Only project owners have permissions to create and access secrets within their project. Other roles must explicitly be granted permissions through IAM. Audit logging: With Cloud Audit Logging enabled, every interaction with Secret Manager generates an audit entry. You can ingest these logs into anomaly detection systems to spot abnormal access patterns and alert on possible security breaches. Strong encryption: Secret Manager manages server-side encryption keys on your behalf using the same hardened key management systems used for Google’s own encrypted data, including strict key access controls and auditing. Secret Manager encrypts user data at rest using AES-256. There is no setup or configuration required, no need to modify the way you access the service, and no visible performance impact. Your secret data is automatically and transparently decrypted when accessed by an authorized user. The Secret Manager API always communicates over a secure HTTP(s) connection. By default, only project owners can create and access secrets within their project. Use IAM to grant roles and permissions at the level of the Google Cloud organization, folder, project, or secret. On the next slide, we will look at roles that control access to secrets. The Secret Manager Admin role can view, edit, and access a secret. Effectively, this role provides full access to administer Secret Manager resources. The Secret Manager Secret Assessor role can only access secret data. This is useful for service accounts or for situations when all that is needed is the ability to access the sensitive data held inside the secret. The Secret Manager Viewer role can view the secret’s metadata and its versions but cannot edit or access secret data. As always, apply permissions at the lowest level of the resource hierarchy to obtain the result you desire. For example, if you want a user to have access to five secrets within the project, grant that user permission only to those five secrets. For information about specific permissions within each role, refer to the Secret Manager documentation or use the Google Cloud Console. Secrets can be versioned. There is no limit to the number of versions that can be created for a given secret. Each version is automatically assigned an ordinal version ID: 1, 2, 3, 4, etc. You can refer to individual versions of a secret to view the secret value or the secret’s metadata. In addition to the version ID, the most recent version is automatically assigned a version label called latest. You cannot modify a version, but you can disable it or delete it. Instead of modifying a new version, add a version with the correct data. There is no limit on the number of versions that can be kept. Individual versions can be selected and then disabled or destroyed. This prevents users from accessing data for a secret that is incorrect. It is possible to disable all the versions of a secret or to delete the entire secret. To rotate a secret, add a new version to it. All versions are numbered, starting from 1. Users can refer to the latest version using the label “latest. Any version can be accessed, as long as it is enabled. To prevent a version from being used, disable that version. Secret Manager supports rotation schedules on secrets. Secret Manager sends messages to Pub/Sub topics configured on the secret based on the provided rotation frequency and rotation time. Periodically rotating secrets helps to: Limit the duration a leaked secret remains valid and exposes a vulnerability. As well as continuously exercise rotation flow to ensure process reliability. For more information, see the Create rotation schedules in Secret Manager documentation. Admin Activity Access audit logs cannot be disabled. These logs do not count toward your log ingestion quota. All activity that creates or changes secret data is logged here. Setting or getting IAM policy information about secrets is logged. Only the version access is not logged here. You will learn more about that on the next slide. Note that Secret Manager doesn't write System Event audit logs. Data Access audit logs are disabled by default. Data Access logs do count toward your log ingestion quota, and, therefore, there is a cost involved. When enabled, Data Access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data. Data Access audit logs do not record the data-access operations on resources that are publicly shared. In other words, nothing is recorded for anything that is available to All Users or All Authenticated Users or that can be accessed without logging into Google Cloud.

### Video - [Lab Intro: Configuring and Using Credentials with Secret Manager](https://www.cloudskillsboost.google/course_templates/87/video/514122)

* [YouTube: Lab Intro: Configuring and Using Credentials with Secret Manager](https://www.youtube.com/watch?v=ukh2Bo7nHuU)

In this lab, you will learn how to add application credentials to Secret Manager and then access those credentials using the Secret Manager API.

### Lab - [Configuring and Using Credentials with Secret Manager](https://www.cloudskillsboost.google/course_templates/87/labs/514123)

In this lab exercise, you will use Secret Manager from Google Cloud Console and also the CLI.

* [ ] [Configuring and Using Credentials with Secret Manager](../labs/Configuring-and-Using-Credentials-with-Secret-Manager.md)

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/87/video/514124)

* [YouTube: Module review](https://www.youtube.com/watch?v=KZN4kRkef7g)

In this module, we discussed application security, and how to mitigate common application vulnerabilities. Let’s do a brief review. Application security is often ill-defined and untested in software development projects. Hackers know this, so they target applications more often than they target infrastructure or networks! A complete list is impossible to create, but there are known common application vulnerabilities that can be mitigated. Common application vulnerabilities include injection vulnerabilities, where malicious code is “injected” into the system and the data underlying it. These generally take the form of SQL injection, LDAP injection, and HTML injection. Another common vulnerability is cross-site scripting, or XSS, which is actually a form of injection where the attacker is able to inject javascript from a different site into an application, allowing attackers to execute malicious scripts in the victim's browser. Web Security Scanner is a web security scanner which probes for common vulnerabilities in Google App Engine and compute engine applications. It can automatically scan and detect common vulnerabilities, including cross-site-scripting, Flash injection, mixed content (HTTP in HTTPS), and outdated/insecure libraries. Web Security Scanner can be run manually, or on a schedule. Web Security Scanner can impact the performance of your applications while it is scanning, but there are steps you can take to minimize this impact. Web Security Scanner can impact the performance of your applications while it is scanning, but there are steps you can take to minimize this impact. Another common threat to application security is identity and OAuth “phishing.” Phishing attacks are an attempt to obtain sensitive information from a user or site, especially usernames, passwords, and credit card numbers. OAuth phishing is a type of credential phishing that takes advantage of the Open Authentication (OAuth) standard to gain access to user accounts without needing to know the user’s password by exploiting the trust relationship users have with well-known online service providers and third-party applications. IAP replaces end-user VPN tunnels or the need to apply an authentication and authorization layer in front of a web application hosted on Google Cloud. Your request is forwarded to IAP, which requires the user to log in, and the proxy will determine if the user is allowed to access that application. IAP works with Compute Engine instances and Google Kubernetes Engine clusters with a central authentication and authorization layer over HTTPS. Secret Manager makes sensitive data needed by your applications secure and easy to access. It also simplifies the management of sensitive data by putting it in a single place, controlled by IAM. I hope this module has given you some helpful information and tools you can use to secure your applications.

### Quiz - [Quiz: Application Security](https://www.cloudskillsboost.google/course_templates/87/quizzes/514125)

#### Quiz 1.

> [!important]
> **Which TWO of the following vulnerabilities are scanned for when you use Web Security Scanner?**
>
> * [ ] Outdated or insecure libraries.
> * [ ] Mixed content.
> * [ ] Personalized data in object names.
> * [ ] User data in images.
> * [ ] Insecure logins.

#### Quiz 2.

> [!important]
> **Which TWO of the following statements about Application Security are TRUE?**
>
> * [ ] "Injection Flaws" are the least frequently found application security issue.
> * [ ] Applications in general, including many web applications, do not properly protect sensitive user data.
> * [ ] Developers are commonly given a requirements document that clearly defines security requirements for the application.
> * [ ] Applications are the most common target of cyberattack.

#### Quiz 3.

> [!important]
> **Which TWO of the following statements are TRUE when discussing the threat of OAuth and Identity Phishing?**
>
> * [ ] Look-alike phishing sites are generally pretty easy to spot.
> * [ ] Being "hacked" on a social site can lead to being "hacked" on more critical websites, depending on your social site's account settings.
> * [ ] Credit card data is the only information that is useful to cyber hackers.
> * [ ] Even small, unimportant pieces of personal data need to be secured from phishing attacks.

## Securing Google Kubernetes Engine: Techniques and Best Practices

Protecting workloads in Google Kubernetes Engine involves many layers of the stack, including the contents of your container image, the container runtime, the cluster network, and access to the cluster API server. In this module, you will learn how to securely set up your Authentication and Authorization, how to harden your clusters, secure your workloads, and monitor everything to make sure it stays in good health.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/87/video/514126)

* [YouTube: Module overview](https://www.youtube.com/watch?v=5ZlN97wWw6w)

Welcome to the Securing Google Kubernetes Engine Techniques and Best Practices module. Protecting workloads in Google Kubernetes Engine involves many layers of the stack, including the contents of your container image, the container runtime, the cluster network, and access to the cluster API server. In this module, we will begin with an overview of Kubernetes - in particular, of Google Kubernetes Engine, or GKE. Next, we will discuss how authentication and authorization work in Google Kubernetes Engine. Then, we will talk about hardening your clusters, securing your workloads, and how to use logging and monitoring to make sure everything remains in good health.

### Video - [Introduction to Kubernetes/GKE](https://www.cloudskillsboost.google/course_templates/87/video/514127)

* [YouTube: Introduction to Kubernetes/GKE](https://www.youtube.com/watch?v=lVnN1CUX5N0)

In the next few slides, you will be given a brief overview of Kubernetes and its main architectural components. These slides serve only to provide a context for how to set up and manage security for Kubernetes. Kubernetes is a virtualized environment in which you run, manage, and scale applications. Each application runs in a container. Google Kubernetes Engine (GKE) refers to the Kubernetes offering on Google Cloud. Kubernetes is also available from other vendors. Each of your applications runs as one or more containers. A container is a lightweight, isolated user space for running application code. Containers are lightweight because they do not contain a full copy of the operating system. Instead, they contain a very scaled-down operating system with just the bare minimum needed to run a container. Containers can be packed tightly onto the underlying system, which is very efficient. Starting and stopping a container means starting and stopping its scaled down operating system processes, not booting an entire virtual machine and initializing an operating system. Similar to a virtual machine, a container has its own filesystem, CPU, memory, process space, and more. Because they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions. Containers execute within a pod. The most common use case is for a pod to execute a single container. However, it is possible for a pod to run multiple containers. This is useful when containers are tightly coupled or share resources. A pod makes its environment available to the containers running within it. For example, containers use the pod’s network namespace, including its IP address and ports. Containers within a pod can communicate with each other using localhost (127.0.0.1). A pod can define storage volumes, which its containers can use. Workload is a general term that refers to applications, microservices, daemons, or jobs: the sort of things that are implemented by pods. Nodes are the worker machines on which the pods exist. In GKE, each node is a separate, virtual machine. Each node (in other words, each virtual machine) has its own instance of the operating system. Nodes provide services to the pods, such as hardware and the network infrastructure. The pods share these resources and then share them among the containers. Clusters are a set of one or more nodes. The control plane, which is the primary node, controls the other nodes in the cluster. The cluster is visible in the Google Cloud console. GKE manages the control plane internally and hides it from view; in the Cloud Console, you will not see a “control plane” within the cluster. Kubernetes scales nodes in the cluster up or down as needed based on demand and the Kubernetes configuration. For more information, enroll in the Architecting with Google Kubernetes Engine course, or refer to the Kubernetes documentation link in this module’s course resources. Kubernetes secrets contain sensitive data, such as passwords, OAuth tokens, and SSH keys. Secrets can be encrypted. By default, GKE encrypts customer content stored at rest, including secrets. GKE handles and manages this default encryption for you without any additional action on your part. Pods use secrets to gain the access they need within your environment to accomplish tasks. GKE secrets are not the same as secrets managed within Secret Manager. You will learn more about this later. GKE secrets are similar to Google Cloud secrets but they are not the same: they are separate entities. In GKE, a secret is created using the Kubernetes API and is available only within GKE. It is not accessible within other areas of Google Cloud. By default, secrets created within Google Cloud are not available within Kubernetes. Later, you will learn that you can use Workload Identity to configure a Kubernetes service account to act as a Google service account. For more information about GKE secrets, refer to the documentation link in the course resources.

### Video - [Authentication and authorization](https://www.cloudskillsboost.google/course_templates/87/video/514128)

* [YouTube: Authentication and authorization](https://www.youtube.com/watch?v=xfmKU4nsW3w)

Protecting workloads in Google Kubernetes Engine involves many layers of the stack, including the contents of your container image, the container runtime, the cluster network, and access to the cluster API server. User accounts are accounts that are known to Kubernetes, but are not managed by Kubernetes; for example, you cannot create or delete them using kubectl. Kubernetes calls these types of accounts “normal users” and leaves their administration to an outside, independent service, like Google Cloud. Google Kubernetes User (“normal users”) accounts also come in two types: Google account, and Google Cloud service account. Once created, both types of accounts need to be authorized to create, read, update, or delete Kubernetes resources. Service accounts are accounts that are created and managed by Kubernetes, but can only be used by Kubernetes-created entities, such as pods. These accounts are managed through the Kubernetes API, are bound to specific namespaces, and can be created automatically by the API server or manually via API calls. If a request is received that is not tied to either a known user or a known service account, it is treated as an anonymous request. Despite having similar names, Kubernetes service accounts and Google service accounts are different types of accounts. Kubernetes service accounts are part of the cluster in which they are defined and… are typically used only within that cluster. In contrast, Google Cloud service accounts are always a part of a Google Cloud project. Google service accounts can easily be granted permissions by IAM both within clusters and Google Cloud projects, as well as to any other Google Cloud resource. This means that Google Cloud service accounts are more powerful than Kubernetes service accounts. Therefore, in order to follow the security principle of least privilege, you should consider using Google Cloud service accounts only when their extra capabilities are needed, and use IAM to manage their permissions. Role-Based Access Control allows you to create detailed policies that define which operations and resources you allow users and service accounts to access. Role Based Access Control controls access to Google Accounts, Google Cloud service accounts, and Kubernetes service accounts.

### Video - [Hardening your clusters](https://www.cloudskillsboost.google/course_templates/87/video/514129)

* [YouTube: Hardening your clusters](https://www.youtube.com/watch?v=GvFxwGzE3Rk)

With the speed of development in Kubernetes, new features (including new security features) are released quite often. This module will guide you through recommended current guidelines for hardening your GKE clusters. First, keeping your version of Kubernetes up to date with recommended patches and updates is one of the most critical, yet the easiest, way to increase the security of your clusters. With Google Kubernetes Engine, the control planes are patched and upgraded for you automatically, and Node auto-upgrade also automatically upgrades nodes in your cluster. There are three types of automatic upgrades. From most frequent to least frequent updates, they are Rapid, Regular, and Stable. Rapid provides the quickest access to new features, as soon as they are released. Regular provides updates on a more predictable cadence; it is the default update channel and is recommended for most users. Stable provides the least frequent updates, allowing more time for feature validation. Rapid, regular, and stable updates are described in the Google Kubernetes documentation, on the release channels page. You can choose to disable Node auto-upgrade; however, if you do this, we recommend that you still upgrade monthly or on another set schedule of your choosing. Refer to this module’s course resources for a link to the release channels. It is a good security practice to regularly audit your cluster configurations for deviations from your defined settings. Many of the recommendations covered in this section, as well as common misconfigurations, can be automatically monitored using the new Security Health Analytics. When Security Health Analytics has been enabled, it will run scans on your selected resources automatically, twice a day (12 hours apart) and alert you if it finds any anomalies. Another best practice for hardening your clusters is to limit the exposure of your cluster control plane and nodes to the internet. By default, the GKE cluster control plane and nodes have internet-routable addresses that can be accessed from any IP address, but it is possible to change this if you do so at the time of cluster creation. It is possible to create a “private cluster” that provides network-level protection to your GKE cluster control plane. There are three options for this: Public endpoint access disabled prevents all internet access to both control planes and nodes and works with networks using Cloud Interconnect and Cloud VPN. Public endpoint access enabled, control plane authorized networks enabled gives the control plane a public IP address, but installs a customer configurable firewall in front that allows public internet connections without the use of VPN. And Public endpoint access enabled, control plane authorized networks disabled, which is the default setting that allows any public internet user to make connections to the control plane. To disable direct internet access to nodes, specify the gcloud tool option --enable-private-nodes at cluster creation. Using groups to control identities and access is a security best practice both in the Cloud and on-premises. Now, you can use Group Authentication with your GKE clusters, which removes the need to update your Role Based Access Control configuration whenever anyone is added or removed from the group. Keep in mind that Google Groups for GKE must be enabled while you are creating your clusters. Each node in GKE is given the Compute Engine default service account, which gives it broad access to resources, but more permissions than are likely to be required to run your GKE cluster. You should create and use a minimally privileged service account to run your GKE cluster instead of using this Compute Engine default service account. When creating this minimally privileged service account, note that GKE requires, at a minimum, the service account to have the monitoring.viewer, monitoring.metricWriter, and logging.logWriter roles. A foundational security concept is to give teams the least amount of privileges required to do their job, and you can do that in Kubernetes by creating separate namespaces or clusters for each team and environment. It is also a good plan to assign cost centers and appropriate labels to each namespace for accountability and chargeback. Just as Shielded VMs provide verifiably secure Compute Engine instances, Shielded GKE nodes provide verifiable node identity and integrity. Upon cluster creation or update, specify the gcloud option --enable-shielded-nodes. Shielded GKE nodes should be enabled with secure boot; however, secure boot should not be used if you will need to use third-party unsigned kernel modules. Pods in the same cluster, by default, are able to communicate with each other without restrictions. Allowing this level of communication is often unnecessary, and may even be inadvisable. Restricting network access to services makes it much more difficult for attackers to move laterally within your cluster, and also offers services some protection against accidental or deliberate denial of service. The two recommended ways to restrict traffic between pods are to use Istio (which also offers load balancing, service authorization, throttling, quota, and metrics) or to use Kubernetes Network Policies to provide basic access control functionality. The Container-Optimized OS with containerd (cos_containerd) image is a variant of the Container-Optimized OS image with containerd as the main container runtime directly integrated with Kubernetes. containerd is the core runtime component of Docker and has been designed to deliver core container functionality for the Kubernetes Container Runtime Interface (CRI). It is significantly less complex than the full Docker daemon, and therefore has a smaller attack surface. Secret management provides another level of security for authentication “secrets,” which are generally stored in the /etcd directory. To do this you will need to configure a third-party secrets manager, such as HashiCorp Vault, which can be integrated with GKE clusters and will need to be set up before creating your clusters. Another option is to use Kubernetes secrets natively in GKE, making sure to encrypt these at the application layer with a key that you manage. Some solutions will work both in GKE and in Anthos GKE deployed on-premises, and so may be more desirable if you are running workloads within a hybrid cloud environment. By default, clusters come with a permissive set of discovery ClusterRoleBindings which can give broad access to information about a cluster's APIs. You should be aware that the system:authenticated Group can include any authenticated user (including any user with a Google account), and therefore does not represent a meaningful level of security for clusters on GKE. To harden your clusters against discovery exploits, you can: Configure authorized networks to restrict access to only a set of IP ranges. Set up a private cluster to restrict access to only certain VPCs. And finally, curate the subjects of the default system:discovery and system:basic-user ClusterRoleBindings to allow access by only certain known Users and Groups. For more tips on how to harden your cluster’s security, refer to the documentation link in this module’s course resources. GKE creates ingress firewall rules automatically when creating GKE clusters, GKE Services, and GKE Ingresses. The priority for all automatically created firewall rules is 1000, which is the default value for firewall rules. If you would like more control over firewall behavior, you can create firewall rules with a higher priority. Firewall rules with a higher priority are applied before automatically created firewall rules. To avoid unexpected behavior in your clusters, do not modify or delete firewall rules created by GKE. For more information on the firewall rules automatically created by GKE, refer to the documentation link in the course resources.

### Video - [Securing your workloads](https://www.cloudskillsboost.google/course_templates/87/video/514130)

* [YouTube: Securing your workloads](https://www.youtube.com/watch?v=pwF9lzcgdPA)

Kubernetes allows users to quickly provision, scale, and update container-based workloads. This lesson describes tactics that administrators and users can employ to limit the effect a running container can have on other Google Cloud resources. Limiting pod container permissions is both critical to securing your workloads and an important part of securing your clusters. Google Kubernetes Engine allows you to set security-related options via the Security Context on both pods and containers. These settings allow you to change the security settings of your workload processes, for example, changing the “run as” user and group. To change settings at the cluster level, you will need to implement PodSecurityPolicy or PodSecurity admission controllers based on the Kubernetes version, to ensure that all Pods in a cluster adhere to a minimum baseline policy that you have defined. The Kubernetes project deprecated PodSecurityPolicy and removed the feature entirely in Kubernetes v1.25. An application has needs. Maybe it needs to connect to a data warehouse, or connect to a machine learning training set. No matter what your application needs to do, there’s a good chance it needs to connect to other services to get it done. More specifically, applications running on GKE must authenticate to use Google Cloud APIs such as the Compute APIs, Storage and Database APIs, or Machine Learning APIs. If that app runs on Kubernetes, this kind of authentication has traditionally been a challenge, requiring workarounds and suboptimal solutions. The solution to this problem? Workload Identity. With Workload Identity, you can configure a Kubernetes service account to act as a Google service account. Workload Identity works by creating a relationship between Kubernetes service accounts and IAM service accounts, so you can use Kubernetes-native concepts to define which workloads run as which identities, and permit your workloads to automatically access other Google Cloud services - all without having to manage Kubernetes secrets or IAM service account keys! Essentially, Workload Identity enables you to assign fine-grained identity and authorization for applications in your cluster. Workload Identity is the recommended way to access Google Cloud services from applications running within GKE due to its improved security properties and manageability. Workload Identity also: Allows you to easily assign identity and prove it to external identity solutions, Provides strong security guarantees, Enforces principle of least privilege, And preserves the Kubernetes abstraction layer. Binary authorization allows you to enforce deploying only trusted containers into GKE. It provides software supply-chain security for applications that run in the Cloud. Binary Authorization works with images that you deploy to GKE from Container Registry or another container image registry. This allows you to ensure that the internal processes that safeguard the quality and integrity of your software have successfully completed before an application is deployed to your production environment. So, how does Binary Authorization work? That comes in 4 easy steps. First, you enable binary authorization on your GKE cluster. Next, you add a policy that requires signed images. Then, when an image is built by Cloud Build an “attestor” verifies that it was from a trusted repository (for example, Source Repositories, which are fully featured, private Git repositories hosted on Google Cloud). Finally, Container Registry includes a vulnerability scanner that scans containers.

### Video - [Monitoring and logging](https://www.cloudskillsboost.google/course_templates/87/video/514131)

* [YouTube: Monitoring and logging](https://www.youtube.com/watch?v=KTaMBliY2cI)

GKE includes native integration with Cloud Monitoring and Cloud Logging. When you create a GKE cluster, Kubernetes Engine Monitoring is enabled by default and provides a monitoring dashboard specifically tailored for Kubernetes. You can control whether Kubernetes Engine Monitoring collects application logs, and you can even disable Cloud Monitoring and Cloud Logging completely. In the Kubernetes Engine Monitoring dashboard summary pane, you can view a cluster's key performance metrics, such as CPU utilization, memory utilization, and the number of open incidents. There are controls associated with this view that allow you to filter and summarize these metrics. The dashboard toolbar controls the time window for observations and provides dashboard settings and filters. The timeline event selector lets you select a specific time and display summaries of alerts. The details section lets you choose how your cluster information is presented to you. The Kubernetes Engine Monitoring dashboard viewing tabs let you organize your cluster information using different hierarchies. Infrastructure aggregates resources by Cluster, then Node, then Pod, and then by Container. Workloads aggregates resources by Cluster, then Namespace, then Workload, then Pod, and lastly by Container. And services aggregates resources by Cluster, then Namespace, then Service, then Pod, and lastly by Container. You can use the “resource type” filter to narrow the information reported in the dashboard to a specific kind of resource, which allows you to more closely inspect namespaces, nodes, workloads, services, pods, and containers. For pods and containers, you can also view metrics as a function of time and view log entries using the Logs Explorer. You can use the default Kubernetes Engine Monitoring or opt in to use Google Cloud’s operations suite, formerly known as Stackdriver. Both options are generally available as of GKE version 1.22 and later. When deciding which monitoring and logging service to use, take the following into account: Kubernetes Engine Monitoring is the default option, starting with GKE version 1.15. Only Google Cloud’s operations suite lets you disable Cloud Logging while still using Cloud Monitoring. GKE also supports Access Transparency Logging, which records the actions taken by Google personnel in your Google Cloud resources (if ever taken).

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/87/video/514132)

* [YouTube: Module review](https://www.youtube.com/watch?v=K-zXraJBJS8)

In this module, we learned that Kubernetes is a virtualized environment in which you run, manage, and scale applications. Each application runs in a container. While Kubernetes is also available from other vendors, Google Kubernetes Engine, or GKE, refers to the Kubernetes offering on Google Cloud. Google Kubernetes supports two types of authentication. User accounts are accounts that are known to Kubernetes, but are not managed by Kubernetes. Service accounts are accounts that are created and managed by Kubernetes, but can only be used by Kubernetes-created entities. To ensure good cluster security, it is crucial that the current range of GKE “hardening” guidelines are followed. Securing workloads by limiting pod container permissions, using Workload Identity, and enabling Binary Authorization limits the effect containers have on other resources. GKE includes native integration with Cloud Monitoring and Cloud Logging. When you create a GKE cluster, Kubernetes Engine Monitoring is enabled by default and provides a monitoring dashboard specifically tailored for Kubernetes. You can control whether Kubernetes Engine Monitoring collects application logs, and you can even disable Cloud Monitoring and Cloud Logging completely.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/87/quizzes/514133)

#### Quiz 1.

> [!important]
> **"Kubernetes service account" and "Google service account" are different names for the same type of service account.**
>
> * [ ] False
> * [ ] True

#### Quiz 2.

> [!important]
> **Which ONE of the following is NOT a security best practice on Kubernetes.**
>
> * [ ] Restrict access between pods.
> * [ ] Use shielded GKE nodes.
> * [ ] Upgrade your GKE infrastructure.
> * [ ] Disable Workload Identity.

#### Quiz 3.

> [!important]
> **GKE has logging and monitoring functions built in.**
>
> * [ ] True
> * [ ] False

## Course Resources

PDF links to all modules

### Document - [Security Best Practices in Google Cloud - Course Resources](https://www.cloudskillsboost.google/course_templates/87/documents/514134)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
