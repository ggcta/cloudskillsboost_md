---
id: 36
name: 'Networking in Google Cloud: Routing and Addressing'
datePublished: 2025-02-13
topics:
- Cloud Networking
- Private Connectivity
- Networking
type: Course
url: https://www.cloudskillsboost.google/course_templates/36
---

# [Networking in Google Cloud: Routing and Addressing](https://www.cloudskillsboost.google/course_templates/36)

**Description:**

Welcome to the second course in the networking and Google Cloud series routing and addressing.
In this course, we'll cover the central routing and addressing concepts that are relevant to Google Cloud's networking capabilities.
Module one will lay the foundation by exploring network routing and addressing in Google Cloud, covering key building blocks such as routing IPv4, bringing your own IP addresses and setting up cloud DNS. In Module two will shift our focus to private connection options, exploring use cases and methods for accessing Google and other services privately using internal IP addresses.

By the end of this course, you'll have a solid grasp of how to effectively route and address your network traffic within Google Cloud.

**Objectives:**

- Define key routing and addressing concepts relevant to Google Cloud, including IP addresses, subnets, route tables, firewalls, BYOIP, and NATs.
- Describe the configuration and management options for Google Cloud DNS, including private and managed zones.
- Configure and manage route tables to control traffic flow, resolve domain names effectively, and utilize NAT rules for secure access.
- Differentiate various Private Connection options and its use cases.

## Welcome to Networking in Google Cloud

Welcome to the Networking in Google Cloud: Routing and Addressing course.

### Video - [Networking in Google Cloud Introduction](https://www.cloudskillsboost.google/course_templates/36/video/525298)

- [YouTube: Networking in Google Cloud Introduction](https://www.youtube.com/watch?v=s1xnhBfanQA)

Welcome to the second course in the Networking in Google Cloud series, Routing and Addressing. In this course, we'll cover the essential routing and addressing concepts that are relevant to Google Cloud's networking capabilities. Module 1 will lay the foundation by exploring network routing and addressing in Google Cloud, covering key building blocks such as routing IPv4, bringing your own IP addresses, and setting up Cloud DNS. In module 2, we'll shift our focus to private connection options, exploring use cases, and methods for accessing Google and other services privately using internal IP addresses. By the end of this course, you'll have a solid grasp of how to effectively route and address your network traffic within Google Cloud.

## Network Routing and Addressing in Google Cloud

In this module, we will explore foundational elements that guide traffic through the Google Cloud network. We'll start with the building blocks: routing, IPv4, and BYOIP. We'll also explore Cloud DNS.

### Video - [Routes and route preferences](https://www.cloudskillsboost.google/course_templates/36/video/525299)

- [YouTube: Routes and route preferences](https://www.youtube.com/watch?v=MC2ikxhthFs)

Welcome to the Network Routing and Addressing module. In this module, we will explore foundational elements that guide traffic through the Google Cloud network. We'll start with the building blocks, routing, IPv4, and BYOIP. We'll also explore Cloud DNS. Before we cover routes, let's cover IP addresses. Speaking of IP addresses of a subnet, Google Cloud VPCs let you increase the IP address space of any subnets without any workload shutdown or downtime. This diagram illustrates a network with subnets that have different subnet masks, allowing for more instances in some subnets than others. This gives you flexibility and growth options to meet your needs. But there are some things to remember. The new subnet must not overlap with other subnets in the same VPC network in any region. Each IP range for all subnets in a VPC network must be a unique valid CIDR block. Also, the new subnet IP address ranges are regional internal IP addresses and have to fall within valid IP ranges. Subnet ranges cannot match, be narrower, or be broader than a restricted range. Subnet ranges cannot span a valid RFC range and a privately used public IP address range. Subnet ranges cannot span multiple RFC ranges. The new network range must be larger than the original, which means the prefix length value must be a smaller number. In other words, you cannot undo an expansion. Now, auto mode subnets start with a /20 IP range. They can be expanded to a /16 IP range, but no larger. Alternatively, you can convert the auto mode subnetwork to a custom mode subnetwork to increase the IP range further. A route is created when a network or subnet is created, enabling traffic delivery from anywhere. Routes define the paths that network traffic takes from a virtual machine, or VM, instance to other destinations. These destinations can be inside your Google Cloud Virtual Private Cloud (VPC) network for example, in another VM, or outside of it. Routes match packets by destination IP address. Forward traffic to highest priority or specific route. However, no traffic will flow without also matching a firewall rule. A route is created when a network is created, which enables traffic delivery from anywhere. Also, a route is created when a subnet is created. This is what allows VMs on the same network to communicate. Network tags fine-tune which route is picked. If a route has a network tag, it can be applied only to instances that have the same network tag. Routes without network tags can apply to all instances in the network. This slide shows a simplified routing table. A route can be of many types. There are system-generated, custom peering, NCC, and policy based routes. System-generated routes are simple and can be used by default. When they do not provide the desired granularity, create custom routes. For example, custom routes can be used to route traffic between subnets through a network virtual appliance. Peering routes are used for network peering. VPC Network Peering routes in a different VPC network connected using peering. NCC routes that represent a subnet IP range in a VPC spoke. Policy based routes apply to packets based on source IP, destination IP, protocol, or a combination thereof. Next, you'll learn more about system generated and custom route types. Peering Network Connectivity Center routes and policy-based routes are covered in another module. When you create a VPC network, it includes a system-generated IPv4 default route, 0.0.0.0/0. When you create a dual-stack subnet with an external IPv6 address range in a VPC network, a system generated IPv6 default route (::/0) is added. If the default route doesn't exist, it isn't added. The IPv4 and IPv6 default routes that serve these purposes define a path out of the VPC network to external IP addresses on the Internet. If you access Google APIs and services without using a private service connect endpoint, the default system-generated route can serve as the path to Google APIs and services. Private Service Connect enables you to publish and consume services by using the internal IP addresses that you define. You'll learn more about Private Service Connect later in this course. For more information, in the Google Cloud documentation, refer to configuring Private Google Access and accessing APIs from VMs with external IP addresses. Google Cloud only uses a default route if a route with a more specific destination does not apply to a packet. For information about how destination specificity and route priority influence route selection, see routing order in the Google Cloud documentation. To completely isolate your network from the Internet, or to replace the default route with the custom route, you can delete the default route. For IPv4 only, to route Internet traffic to a different next hop, you can replace the default route with a custom static or dynamic route. For example, you could replace it with a custom static route whose next hop is a proxy VM. If you delete the default route and you do not replace it, packets to IP ranges not covered by other routes are dropped. If you don't have custom static routes that meet the routing requirements for Private Google Access, deleting the default route might disable Private Google Access. Some organizations do not want a default route pointing to the Internet. Instead, they want the default route to point to an on-premises network. To do that, you can create a custom route. You will learn about custom routes later in this module. When you create a subnet, system-generated subnet routes are automatically created. Subnet routes always have the most specific destination and cannot be overridden by higher priority routes. Recall that lower priority number indicates higher priority, so 1 would have a higher priority than 10. Each subnet has at least one subnet route whose destination matches the primary IP range of the subnet. If the subnet has secondary IP ranges, each secondary IP address range has a corresponding subnet route. Custom static routes forward packets to a static route next hop and are useful for all topologies. Dynamic routing generally provides quicker routing performance. Unlike dynamic routing, no processing power is devoted to maintaining and modifying the routes, hence the quicker performance. Custom static routing is more secure than dynamic routing because there's no route advertisement. Note these custom static routing limitations. A custom static route cannot point to a VLAN attachment. It also requires more maintenance because routes are not dynamically updated. For example, a topology change on either network requires you to update static routes. Also, if a link fails, static routes can't reroute traffic automatically. Manually configured routes, which are called custom learned routes, can be used to overcome this limitation. For small, stable topologies, this is not always a significant concern. Here we have two VMs, Pekoe and Oolong, set up in two different VPC, VPC1 and VPC2. A VPN gateway is set up between these two VPCs and two IPSec tunnels have been created. A static route has been created for Pekoe to be able to ping Oolong and enable traffic to flow through the tunnel. A static tunnel forwards packets to a static route next hop and supports various destinations. The supportive static route next hop are instances, internal passthrough Network Load Balancers, and Classic VPN tunnel next hops. Each VM instance has a controller that is kept informed of all routes from the network's routing table. Route changes are propagated to the VM controllers. When you add or delete a route, a set of changes is propagated to the VM controllers. In this example, if you change any of the routes to the Oolong VM, Pekoe can still route packets to Oolong. You can create custom static routes either manually or automatically. To create custom static routes manually, use the Google Cloud app, the Google Cloud CLI compute routes create command, or the routes.insert API. When creating a Classic VPN tunnel without dynamic routing in the Google Cloud console, Cloud VPN may automatically generate static routes. To create the routes, you can also use the Google Cloud app to create a Classic VPN tunnel with policy-based routing, or as a route-based VPN. For more information, see Cloud VPN networks and tunnel routing. Another code-based approach would be to use an IaC system such as Terraform. For large organizations that turn up several networks and test them, static routes can be painful. In the above topology, VMs in the Google VPC route traffic to the VPN gateway through static routes. The VPN gateway encrypts traffic to and from the on-premises network. The on-premises environment has a firewall and a router that knows how to route traffic to the Cloud VPN gateway. As the on-premises network expands through the server network, routes have to be manually configured to route traffic to the resource in the server network. A topology change on either network requires you to manually update static routes. Also, static routes cannot automatically reroute traffic when there is a link failure. A solution is for a network to automatically and rapidly discover topology changes and then route traffic accordingly to minimize disruption. This is exactly the function of Cloud Router. Whenever a link fails, Cloud Router will automatically reroute traffic if another path is possible. Cloud Router peers with an on-premises VPN gateway or router. The router exchanges topology information through a border gateway protocol, or BGP. Cloud Router advertises subnets from its VPC network to the on-premises gateway via BGP. Then, topology changes automatically between your VPC and on-premises network. Dynamic routes are managed by Cloud Routers in the VPC network. Their destinations always represent IP address ranges outside of your VPC network, which are advertised from a BGP peer router. BGP peer routers are typically outside the Google network, like on-premises or on another cloud provider. Dynamic routes are used by: Dedicated Interconnect, Partner Interconnect, Cross-Cloud Interconnect, HA VPN tunnels, Classic VPN tunnels that use dynamic routing, and NCC Router appliances. Routes are added and removed automatically by Cloud Routers in your VPC network. The routes apply to VMs according to the VPC network's dynamic routing mode. This example shows a VPC network connected to an on-premises network that uses Dedicated Interconnect. Cloud Router handles the BGP advertisements and adds them as custom routes. Cloud Router creates a BGP session for the VLAN attachment and its corresponding on-premises peer router. The Cloud Router receives the routes that your on-premises router advertises. These routes are added as custom dynamic routes in your VPC network. The Cloud Router also advertises routes for Google Cloud resources to the on-premises peer router.

### Video - [IPv6](https://www.cloudskillsboost.google/course_templates/36/video/525300)

- [YouTube: IPv6](https://www.youtube.com/watch?v=3DsVZ1SO4xk)

In this module, we will discuss IPv6. VPC networks now support IPv6 addresses. Support for IPv6 addresses can vary per subnet. To support IPv6, Google Cloud has introduced the concept of a subnet stack. The subnet stack defines the type of address that can be assigned to objects in the subnet. Single and dual-stack subnets support IPv4 and IPv6. There's no subnet that only supports IPv6. IPv6 addresses can be assigned to objects in a subnet that supports IPv6. In other words, you can only assign IPv6 addresses to objects in a dual-stack subnet. You can configure the IPv6 access type to be internal or external. Internal IPv6 addresses are used for VM to VM communication within VPC networks. These use unique local addresses, ULAs, which can only be routed within VPC networks and cannot be routed to the Internet. External IPv6 addresses can be used for communication between VMs within VPC networks. These use global unicast addresses or GUAs, and are also routable on the Internet. Connected VMs inherit the IPv6 access type from the subnet. When configuring your VPC networks and subnets to use a IPv6 address, consider these caveats. Dual-stack subnets are not supported on auto mode VPC networks or legacy networks. If you have an auto mode VPC network that you want to add dual-stack subnets to, you can convert the auto mode VPC network to custom mode. If you're converting a legacy custom network, create new dual-stack subnets or convert existing subnets to dual-stack. Single stack IPv6 subnets are not supported. If IPv6 is required, IPv4 must also be configured on a subnet.

### Video - [BYOIP (bring your own IP)](https://www.cloudskillsboost.google/course_templates/36/video/525301)

- [YouTube: BYOIP (bring your own IP)](https://www.youtube.com/watch?v=1dpHaiPaoZg)

The next topic we'll be discussing is BYOIP. Enterprises want to move their applications to the Cloud, but worry about having to swap their IP addresses for one's from their Cloud provider. We hear from our customers that managing the migration of IP addresses can be one of the most challenging aspects of a Cloud migration for network administrators. Here at Google Cloud, we now allow you to bring your own IP, or BYOIP addresses to Google's network infrastructure across all our 24 regions. By bringing over your own IP addresses, you can accelerate your migration while minimizing downtime, as well as significantly reducing your network infrastructure costs. With Google Cloud, your BYOIP prefixes can be broken into blocks as small as 16 addresses, or S/28. It can be distributed to any region, and can also be used for global load balancers, creating more flexibility with the resources you already have. You can also advertise the IP addresses you bring to Google Cloud globally to all peers. BYOIP enables customers to assign IP addresses from a public IP range that they own, to Google Cloud resources. With BYOIP, customers can route traffic directly from the Internet to their VMs without having to go through their own physical networks. After the IP addresses are imported, Google Cloud manages them in the same way as Google-provided IP addresses with these exceptions. The IP addresses are available only to the customer who brought them, and idle or in-use IP addresses incur no charges. The object that the IP address is assigned to can have a regional scope, like a VM or the forwarding rule of a network load balancer. It can also have a global scope, like the forwarding rule of a global external application load balancer. It must support an external address type, because BYOIP ranges will be advertised by Google to the public Internet. BYOIP, bring your own IP addresses, are a way to incorporate your existing static external IP addresses into Cloud environments. They are compatible with most resources that support static external IP addresses. You can leverage BYOIP addresses for Classic VPN gateway tunnels as PRIP addresses, external Google Kubernetes Engine forwarding rules, and configuring static IP addresses for VMs within state-full managed instance groups. However, there are certain limitations. BYOIP addresses cannot be used as PRIP addresses for HA VPN gateway tunnels, or as external IP addresses for VPN gateway tunnels in general. Additionally, shared VPC service projects, Google Kubernetes Engine nodes and pods, and managed instance groups with automatic IP allocation, do not support the use of BYOIP addresses. BYOIP prefixes cannot overlap with subnet or alias ranges in the VPC used by the customer. For BYOIP, the IP address must be IPv4. Importing IPv6 addresses is not supported. Overlapping BGP route announcements can be problematic. BGP is a routing protocol that picks the most efficient route to send a packet. If Google and another network advertise the same route with matching or mismatched prefix lengths, BGP can not work properly, you might experience unexpected routing and packet loss. For example, suppose you're advertising a 203.0.112.0/20 address block, and you're using BGP to route packets, you could bring a 203.0.112.0/23 address block that you own to Google, using BYOIP, and set it up to route externally. Because the /23 block is contained within the /20 block, BGP route announcements may overlap. If you're maintaining the routing registry correctly, BGP routing practices cause the more specific route to take precedence. Thus, the /23 block will take precedence over the /20 block. However, if the 23 route ever stopped being advertised, the /20 block could be used.

### Video - [Cloud DNS](https://www.cloudskillsboost.google/course_templates/36/video/525302)

- [YouTube: Cloud DNS](https://www.youtube.com/watch?v=WvXhdSxRNE0)

Next, let's talk about Cloud DNS. Before we talk about Cloud DNS, let's quickly review how DNS or domain name system works. DNS provides a lookup for sites on the Internet. You can think of it as a phonebook, but instead of using the name of an organization to look up its phone number, you use the name of an organization defined in IP address. A DNS service is provided by your ISP or Internet service provider. For example, suppose a request comes from a client computer to access cymbal.com. To direct the client computer to the cymbal.com site, the Internet service provider needs the IP address of cymbal.com. The ISP connects to get this information from its DNS service. The DNS service recursive resolver issues a request to look up the IP address of cymbal.com from one of its name servers. The name server responds with the ISP, and the recursive resolver sends the IP to the client. When the name server cannot satisfy the lookup, the DNS service might contact another DNS service for this information. Some organizations don't rely on their ISP to provide DNS service, so they create and maintain their own DNS servers. Organizations sometimes do this to limit or customize the information that is returned or because they can achieve better performance if they use their own DNS servers. Alternatively, they can purchase DNS services from another organization. Obviously, there's a lot more that can be said about DNS and its components, but that's not covered in this course. Various companies provide DNS services. Google Cloud is one of them. Cloud DNS lets you create and update millions of DNS records without the burden of managing your own DNS servers and software. Instead, you use a simple user interface, command line interface, or API. On Linux by default, the VM's metadata server 169.254.169.254 resolves internal DNS names. Now, Windows by default, subnet default gateway resolves internal DNS names. Google provides a monthly uptime percentage of serving DNS queries from at least one of the Google managed authoritative name servers to customers of 100% SLO. Important notes, exclusions. It's crucial to understand there are certain situations such as maintenance, force majeure events or actions on your part that void this SLA. Intermittent issues. The downtime definition specifically focuses on at least 60 consecutive seconds of unavailability. Intermittent issues, less than a minute might not count towards the SLA. Private zones are used to provide a namespace that is visible only inside the VPC or hybrid network environment. For example, an organization would use a private zone for a domain dev.gcp.example.com, which is reachable only from within the company Internet. Public zones are used to provide authoritative DNS resolution to clients on the public Internet. For example, a business would use a public zone for its external website cymbal.com, which is accessible directly from the Internet. Don't confuse the concept of a public zone with Google public DNS 8.8.8.8. Google public DNS is just a public recursive resolver. For more information, refer to the Cloud DNS documentation. Cloud DNS policies provide a flexible way to define how your organization uses DNS. After you create the DNS zones and artifacts needed for lookups, create Cloud DNS policies. Cloud DNS supports different types of policies. Server policies apply private DNS configuration to a VPC network. Response policies enable you to modify the behavior of the DNS resolver by using rules that you define. Routing policies steer traffic based on geolocation or round robin. Next, let's look at each of these types of policies. Use server policies to set up hybrid deployment for DNS resolution. You can configure DNS server policy for each virtual private cloud or VPC network. You can set up an inbound server policy depending on the direction of the DNS resolutions. If your workloads plan to use an on-premises DNS resolver, you can set up DNS forwarding zones by using an outbound server policy. If you want your on-premises workloads to resolve Cloud DNS private zones, you can set up an inbound server policy. The policy can specify inbound DNS forwarding, outbound DNS forwarding or both. In this section, inbound server policy refers to a policy that permits inbound DNS forwarding. Outbound server policy refers to one possible method for implementing outbound DNS forwarding. If a policy implements the features of both, it can be an inbound server policy and an outbound server policy. DNS server policies are not available for legacy networks. DNS server policies require VPC networks. For detailed information about server policies, see server policies overview in the Google Cloud documentation. To configure and apply DNS server policies, see apply Cloud DNS server policies in the Google Cloud documentation. A response policy is a Cloud DNS private zone concept that contains rules instead of records. These rules can be used to achieve effects similar to the DNS response policy zone, RPZ draft concept. In other words, you can use response policies to create a DNS firewall by returning modified DNS results to clients. For example, you can use response policies to block access to specified HTTP servers. The response policy feature lets you introduce customized rules in DNS servers within your network that the DNS resolver consults during lookups. If a rule in the response policy affects the incoming query, it's processed. Otherwise, the lookup proceeds normally. For more information, see manage response policies and rules in the Google Cloud documentation. A response policy is different from an RPZ or response policy zone. An RPZ is an otherwise normal DNS zone with specifically formatted data that causes compatible resolvers to do special things. Response policies are not DNS zones and are managed separately in the API. To create and modify response policies in Cloud DNS, use the response policies API. Response policies are separate from managed zones and cannot be managed by using either the managed zones API or the RR set API. DNS routing policies let you steer your traffic based on specific criteria. Google Cloud supports three types of DNS routing policies: weighted round robin, geolocation, geofencing and failover. A weighted round robin routing policy lets you specify different weights per DNS target, and Cloud DNS ensures that your traffic is distributed according to the weights. You can use this policy to support manual, active active, or active passive configurations. You can also split traffic between production and experimental versions of software. A geolocation routing policy let's you map traffic originating from source geographies, Google Cloud regions to specific DNS targets. Use this policy to distribute incoming requests to different service instances based on the traffic's origin. You can use this feature with the Internet, with external traffic, or with traffic originating within Google Cloud and bound for internal load balancers. Google Cloud uses the region where the queries enter Google Cloud as the source geography. A failover routing policy, lets you set up active backup configurations. This option is only available for private zones. Next, you will implement a geolocation routing policy as part of a lab exercise. An example is shown on the screen. Routing policies use geolocation to route requests to the closest load balancer. In a lab exercise, you will configure a routing policy that uses geolocation. To create, edit, or delete DNS routing policies, see manage DNS routing policies in the Google Cloud documentation. Let us look at a scenario before we move on to the lab exercise. Carl is a network engineer at Cymbal Corporation. Cymbal has a hybrid environment with an on-premises data center in Europe and a Google Cloud environment with VPC subnets in Asia and the US. Carl does not want to juggle managing multiple DNS providers for their global web presence. Fragmented setup leads to inconsistencies, manual configuration overhead, and potential for human error. Carl is looking for a solution that can provide a single endpoint for a hybrid application so that external clients in multiple regions resolve www.cymbal.com to the nearest region. Carl can leverage Cloud DNS for its global anycast network for low latency DNS resolution and high availability. The intuitive interface simplifies record management, offering features like health checks and traffic routing. Cloud DNS provides global traffic management for www.cymbal.com, routing users to the closest available server. Asia and US-based visitors are directed to network load balancers within their respective regions, which connect them to GKE instances. European users, a load balancer in European data center handles traffic, forwarding to GKE on VMware. This setup demonstrates flexibility in application hosting. Carl can create this configuration by using the following steps. Carl creates the network load balancers, and the on-premises load balancer in each region. Carl sets up a public Cloud DNS zone for his domain. Cloud DNS automatically assigns anycast name servers to handle domain record storage. These name servers are strategically located in a fully managed shared environment. Then they create a DNS routing policy. In the policy, you set the type to GEO, and you set the routing policy data value to a list of target regions that are mapped to the corresponding network load balancers. When configuring routing policies, consider these caveats. Only one type of routing policy can be applied to a resource record set at a time, and nesting or otherwise combining routing policies is not supported.

### Video - [Lab intro: Traffic Steering Using Geolocation](https://www.cloudskillsboost.google/course_templates/36/video/525303)

- [YouTube: Lab intro: Traffic Steering Using Geolocation](https://www.youtube.com/watch?v=yqEvz0VMQco)

Next, we will test our skills in a lab. In this lab, you will configure and test the geolocation routing policy. The geolocation routing policy applies to the nearest match for the source location when the traffic source location doesn't match any policy items exactly. The lab tasks are to: Launch client VMs, one in each region. Launch server VMs, one in each region except asia-south1. Create a private zone, like example.com. Create a geolocation routing policy using gcloud commands. And test the configuration.

### Lab - [Cloud DNS - Traffic Steering using Geolocation Policy](https://www.cloudskillsboost.google/course_templates/36/labs/525304)

 In this lab you will configure and test the Geolocation routing policy.

- [ ] [Cloud DNS - Traffic Steering using Geolocation Policy](../labs/Cloud-DNS-Traffic-Steering-using-Geolocation-Policy.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/36/quizzes/525305)

#### Quiz 1.

> [!important]
> **You must create a VM that has an IPv6 address. How do you do it?**
>
> - [ ] Create a dual-stack subnet, and create the VM with an IPv6 address.
> - [ ] Create a single-stack network, and create the VM with an IPv6 address.
> - [ ] Create an IPv6-only subnet, and create the VM with an IPv6 address.
> - [ ] Create a dual-stack network, and create the VM with an IPv6 address.

#### Quiz 2.

> [!important]
> **To set up hybrid deployments for DNS resolution, which type of DNS policy should you use?**
>
> - [ ] Traffic policy
> - [ ] Routing policy
> - [ ] Response policy
> - [ ] Server policy

### Video - [Debrief](https://www.cloudskillsboost.google/course_templates/36/video/525306)

- [YouTube: Debrief](https://www.youtube.com/watch?v=g7xHlJNKIlc)

In this module, you learned about some fundamental Google Cloud VPC networking concepts. We began with an overview of Google Cloud VPC networks. We then discussed how to use IPv6 addressing and the configuration that must be done at the subnet level. After that, we discussed routes and route preferences, including system-generated routes, custom routes, and dynamic routes. We continued with information about bringing existing external IP addresses into Google Cloud, also known as BYOIP. Then we discussed using multiple network interfaces on compute engine VMs, as well as some important caveats. After that, we use Cloud DNS policies to refine how an organization uses Cloud DNS. We concluded the module with a lab exercise and a brief quiz to test your knowledge of what you've learned.

## Private Connection Options

In this module, we'll discuss some general methods of accessing Google and other services privately by using internal IP addresses. Then, we will cover each of the methods: Private Google Access, Private Service Connect, private services access and Cloud NAT. After that, you will try what you learned in a Cloud NAT lab exercise.

### Video - [Private access overview](https://www.cloudskillsboost.google/course_templates/36/video/525307)

- [YouTube: Private access overview](https://www.youtube.com/watch?v=fw3Ij6hec2k)

Private connection options. In this module, we'll discuss some general methods of accessing Google and other services privately by using internal IP addresses then we will cover each of the methods, private Google access, private service connect, private services access, and Cloud NAT or network address translation. After that, you will try what you learned in a Cloud NAT lab exercise. Private access uses internal IP addresses therefore, consumers connect to supported APIs and services with an internal connection. Unless a consumer connects to Google Cloud by using an external connection, private access communication does not go through the public Internet. Access is quicker and more secure. Choose a private access option based on your needs. All Google Cloud APIs and services support private access. You can also set up private access to APIs and services that you publish. You can access these API and services from Google Cloud, other Public Clouds, or on-premises. Google APIs and services have public URLs and are accessible on the public Internet. Google provides several private access options. Each option allows VM instances with internal IP addresses to reach certain APIs and services. You could configure one or all of these options because they operate independently of each other. Private Google Access for on-premises hosts let's your on-premises hosts connect Google APIs and services through the default Internet gateway of the VPC network. Your on-premises hosts don't need external IP addresses. Instead, they use internal IP addresses. Private service connect lets you connect to a Google or third-party managed BPC network through a service attachment. As with private Google access, the connection is internal. Serverless VPC access connects serverless products to your VPC network to access Google, third party, or your own services with internal IP addresses. For example, Cloud Run, App Engine standard, and Cloud functions environments send packets to the internal IPV for address of the resource. Serverless VPC access is not covered in this module. For more information, refer to connect from serverless Google services to VPC networks in the Google Cloud documentation. Private Services access is a private connection between your VPC network and a service producer VPC network. This connection is implemented as a VPC network peering connection. The service producer network is created exclusively for you, and it's not shared with other customers. For more information, see private access options for services in the Google Cloud documentation.

### Video - [Private Google Access](https://www.cloudskillsboost.google/course_templates/36/video/525308)

- [YouTube: Private Google Access](https://www.youtube.com/watch?v=zPgfzjym_YA)

Next, let's discuss how to use private Google access to connect to Google APIs and services over an internal connection. Joe, Cymbol Corporation's network engineer faces a challenge. Cymbol runs a recommendation engine on Google Cloud VMs that analyzes customer data to provide tailored product suggestions. This data is highly sensitive, and they need to protect it from potential Internet based attacks and comply with strict data privacy regulations. They also run a large fleet of internal systems, for example, inventory management and point of sale systems. These need regular updates, but don't require direct Internet access for their core functionality. Challenge. The challenge is providing the recommendation engine access to Google Cloud APIs, such as BigQuery for data analysis and Cloud storage for model storage would traditionally require public IPs or a NAT setup. Using public IPs means exposing the VMs to the Internet, increasing the attack surface. Using complex NAT setups involves adding management overhead and potential bottlenecks. Joe wonders if there's a way for Google Cloud VMs and fleet of VMs to securely connect to Google Cloud API without exposing IPs or creating complex NAT. Private Google access enables securely connecting VMs without external IP addresses to essential Google APIs and services. You enable the private Google access feature on a subnet by subnet basis by editing the subnet in Google Cloud console or the Google Cloud CLI. If you disable private Google access for a subnet, VMs with internal IP addresses can only send traffic within the VPC network. Later in this module, you will learn about Cloud NAT, which can allow these BMs to send traffic outside of the VPC network. Private Google access has no effect on instances that have external IP addresses. For a list of services that are supported by private Google access, see private access options for services in the Google Cloud documentation. In the sample topology, the VPC network has two subnets, subnet A and subnet B. The network has been configured to meet the domain name system or DNS routing and firewall network requirements for Google Cloud APIs and services. Private Google access has been enabled on subnet A, but not on subnet B. VM A1 can access Google APIs and services, including Cloud storage, because its network interface is located in subnet A, which has private Google access enabled. Private Google access applies to the instance because it only has an internal IP address. VM B1 can't access Google APIs and services because it only has an internal IP address and private Google access is disabled for subnet B. VM A2 and VM B2 can both access Google APIs and services, including Cloud storage, because each of them has its own IP address. Private Google access has no effect on whether these instances can access Google APIs and services because both have external IP addresses. Cymbol Bank is expanding and wants to use internal IP addresses to access Cloud SQL and Cloud TPU from their Tokyo on premises network. This example shows how this access can be achieved. In the example, the on premises network is connected to a VPC network through a Cloud VPN tunnel. Traffic from on premises host to Google API is traveled through the tunnel to the VPC network. After traffic reaches the VPC network, it's sent through a route that uses the default Internet gateway as its next hub. This next hub allows traffic to leave the VPC network and be delivered to restricted.googleapis.com. 199.36.153.4/30. The on-premises DNS configuration maps [inaudible]. googleapis.com to restricted.googleapis.com, which resolves to the 199.36.153.4/30 address range. In this example, Cloud router uses a custom route advertisement for this IP address range. This route sends traffic through the Cloud VPN tunnel. The traffic that goes to Google APIs is routed through the tunnel to the VPC network. A custom static route was added to the VPC network. This route directs traffic with the destination 199.36.153.4/30 to the default Internet gateway as the next hub. Google then directs traffic to the appropriate API or service. In this example, network administrators at Cymbol Bank created a Cloud DNS managed private zone for.googleapis.com that maps to the 199.36. 153.4/30 address range. The network administrators authorize the BPC network to use that zone. Requests to the googleapis.com domain are sent to the IP addresses that are used by restricted.googleapis.com. Only the supported APIs are accessible to this configuration, which might cause other services to be unreachable. Cloud DNS doesn't support partial overrides. If you require partial overrides, use BIND a software that interacts with DNS. Private Google access has a few caveats. Because private Google access is enabled on a per subnet basis, you must use a VPC network. Legacy networks are not supported because they don't support subnets. Enable the Google Cloud APIs that you want to use. You enable these desired APIs on the APIs and services page in the Google Cloud console. Your VPC network must have appropriate routes and egress firewalls defined. This network must also have appropriate routes for the destination IP ranges that are used by Google APIs and services. If you use the private.googleapis.com or the restricted.googleapis.com domain names, you must create DNS records to direct traffic to the IP addresses that are associated with those domains. For more information, see network configuration on the configure private Google access page of the Google Cloud documentation. These domain names only offer IPv4 connectivity. If you want to use IPv6 to connect to Google APIs and services, your VM must be configured with the /96 IPv6 address range. The software on the VM must send packets whose sources match one of those IPv6 addresses from that range. You must send the packets to the IPv6 addresses or the default domains.

### Video - [Private Service Connect](https://www.cloudskillsboost.google/course_templates/36/video/525309)

- [YouTube: Private Service Connect](https://www.youtube.com/watch?v=AoLP6VanvJs)

Next, let's discuss Private Service Connect, which lets you use internal IP addresses to consume, produce, and make services available. Neuer is a network engineer at Cymbal. A large financial institution offers a real-time transaction processing API for its partners. This API handles sensitive financial data and needs to be highly secure with restricted access, easily accessible to authorized partners over the Internet, and scalable to handle fluctuating traffic loads. The challenges. Security. Exposing the API directly to the public Internet poses security risks. Control. Traditional methods, IP whitelisting or VPNs, can be cumbersome for managing access and don't always scale well. Flexibility. The API should be consumable by partners who might be in different Cloud environments or have their own on-premises infrastructure. Private Service Connect provides a secure, scalable, and flexible way to expose services to specific partners or networks. It's ideal for scenarios involving sensitive APIs or the need for custom IP addressing. Private endpoint. The financial institution creates a Private Service Connect endpoint attached to their API within their VPC. This endpoint gets a private IP address within their network. Service publishing. The service transaction processing API is published, making it discoverable by authorized consumers. Controlled access. Partners create PSC consumer endpoints in their respective VPCs. These endpoints are assigned private IP addresses from a range the financial institution specifies, facilitating fine grain control. Secure, scalable consumption. Authorized partners can now consume the API using the private IP address of the published service. Traffic flows through Google's network. Load balancing is handled on the service producer's side. As with Private Google Access, you can use Private Service Connect to access Google APIs and services with a global internal IP address. Private Service Connect also lets you access third-party services and services provided within the organization with an internal IP address. To access resources with Private Service Connect, use a Private Service Connect endpoint or a back-end. Organizations can choose the internal IP address to associate with each endpoint. Private Service Connect has lined rate performance and scales to enterprise size networks. In other words, Private Service Connect is fast and grows with your organization. Look at the example shown on the right. VM 1 in the consumer network uses a Private Service Connect endpoint to connect to services that run in the producer network on VM 2. The Private Service Connect endpoint has an internal IP address, 10.0.20.10. VM 1 uses the internal address to access services on VM 2. Next, let's talk about other things you can do with Private Service Connect. The consumer contacts a producer service or VM by using the Private Service Connect endpoint in their VPC network. This endpoint has an internal IP address and maps to the service attachment in the producer VPC network. A service attachment refers to services from a producer. In this example, the service attachment receives requests redirected from the Private Service Connect endpoint and sends it to a forwarding rule. The forwarding rule sends the request to the appropriate VM or service. You can see this flow in the example with the red-dotted line. Refer to the documentation for more details. Using a load balancer provides some additional features. Private Service Connect back-ends let Google Cloud load balancers send traffic through Private Service Connect to reach published services or Google APIs. You can assign DNS names to these internal IP addresses, or even Google APIs and services with meaningful names for your organization. For example, if you have a service with the name spanner.example.com, you can map spanner.cymbal.com or some other name that makes sense for your organization. These names and IP addresses are internal to your VPC network. On-premises networks use Cloud VPN tunnels or VLAN attachments to connect to it. Also, you can control which traffic goes to which endpoint and demonstrate that the traffic stays within Google Cloud. You can configure the load balancer to log all requests to Cloud logging. With Private Service Connect and an application load balancer, you can use a URL map to choose which services are available to consumers. You could also use it to evaluate the request and route it to the correct VM or service. For added security between clients and the load balancer, you can use customer-managed transport layer security, or TLS, certificates. You can also enable data residency in transit by connecting to regional endpoints for Google APIs from workloads in that same region. In other words, you can be certain that data at rest is stored in the region you configure. This example shows an on-premises network that is connected to a subnet of a Google Cloud VPC network in the us-central-1 region. The requests that a VM or service in an on-premises network make to Cloud logging, Pub/Sub, or Cloud KMS are not routed on the public Internet. They remain within the Google Cloud backbone network. Requests to other Google Cloud services are routed over the public Internet. The internal application load balancer in the consumer network and the desired Google services are both located in the us-central-1 zone. You can see that after the request from the on-premises network is sent through the Cloud Interconnect connection to the load balancer, it remains in the us-central-1 zone. If desired, access through the load balancer can be sent to Cloud logging. With Private Service Connect and consumer HTTPS service controls that use a global external application load balancer, consumers connect to an external IP address. Private Service Connect uses a network endpoint group to route the request to the service producer. Let's look at a more detailed example of Private Service Connect that uses a global external application load balancer. This topology shows a Private Service Connect endpoint based on a global external application load balancer. This topology lets service consumers with Internet access connect to the load balancer. The load balancer then directs the request to the appropriate network endpoint group and a consumer network. Each of these endpoints is associated with a service attachment. A forwarding rule then routes the request to the appropriate VM instance or service. Private Service Connect lets you configure access to specific Google and third-party services using internal IP addresses. Except for the global external application load balancer use case, connections use internal IP addresses. Traffic stays on the Google backbone network. Connections are thus more secure and much faster than over the public Internet. Services that use Private Service Connect interact like services on a private network. Configuration is simple. Private Service Connect works with the internal IP address range that you provide and sets up the routing tables. Private Service Connect is highly scalable and supports thousands of consumers. Consumers use consumer VPC networks to access VMs and services from producer VPC networks. Consumers can control the internal IP address that is used to connect to a managed service. They don't need to reserve internal IP address ranges for back-end services that are consumed in their VPC network. Instead, consumers choose an IP address from their own subnet to connect to the producer services. For security purposes, all communications between the consumer VPC network and service producer VPC network must be initiated by the consumer. Service producers can't initiate this communication. This unidirectional connectivity drastically simplifies firewall configuration, but also reduces risk from rogue traffic originating from the service producer. Service producers make VMs and services available to consumers. Producers can choose to deploy a multi-tenant model, where your VPC network contains services that are used by multiple consumer VPCs. The consumer networks can have overlapping subnet ranges. Service producers can scale services to as many VM instances as required without asking consumers for more IP addresses. Service producers don't need to change firewall rules based on the subnet ranges in the consumer VPC networks. You can simply create firewall rules for the network address translation, or NAT, IP address range configured for your service. A Private Service Connect interface is a special type of network interface that refers to a network attachment. A Private Service Connect interface enables services in a producer VPC network to securely reach resources and destinations within a consumer VPC network. Producer and consumer networks can be in different projects and organizations. If the service consumer accepts the connection, Google Cloud allocates the interface and IP address from a subnet in the consumer VPC network that's specified by the network attachment. The VM of the Private Service Connect interface has a second standard network interface that connects to the producer's VPC network. A connection between a Private Service Connect interface and a network attachment is similar to the connection between a Private Service Connect endpoint and a service attachment, but it has two key differences. A Private Service Connect interface lets a producer network initiate connections to a consumer network, managed service egress, while an endpoint lets a consumer network initiate connections to a producer network, managed service ingress. A Private Service Connect interface connection is transitive. This means that a producer network can communicate with other networks that are connected to the consumer network. A common use case is when a managed service needs to securely access data within a customer's VPC network. Private Service Connect interface lets the service securely access the data, whether the data resides in the Cloud, on-premises, via VPN or Cloud Interconnect, or with a third-party service. This maintains privacy and isolation for sensitive information. There are many different types of Private Service Connect interface, so producers will provide documentation on how to use it with their services. For example, some producers will use an API, where others might develop a user interface. A consumer network administrator and a consumer service administrator are working together to get an easier way to configure Private Service Connect or the producer network to be able to initiate a connection to the consumer network. Satisfy both of these needs using service connection policies. A service connection policy is a regional Google Cloud resource. It lets a network administrator specify which producer services can be deployed and connected through service connectivity automation. If a service connection policy exists for a managed service, a consumer service administrator can deploy that service. Service connection policies have the following fields. Service class. This specifies the type of managed service that the policy is for. Each producer that supports service connection policies has its own globally unique service class. VPC network, specifies the VPC network that the policy is scoped for. Subnets, specifies the subnets that IP addresses for Private Service Connect endpoints are allocated from. Connection limit, specifies the maximum number of Private Service Connect connections that a producer can create in the policy's VPC network and region. Deploying an instance of a managed service by using service connection policies involves the following steps. A consumer network administrator creates a service connection policy for their VPC network. This policy lets Google automatically deploy Private Service Connect endpoints on behalf of a consumer service administrator. Consumer network administrators have more control over who can create and use Private Service Connect endpoints. The service connection policy references a service class, a globally unique resource that identifies a specific producer service. A single-service connection policy is scoped to a single-service class and a single-consumer VPC network, which delegates the ability to configure connectivity within that scope. A consumer service administrator deploys a managed service using the service administrative API or UI. Google producer of service attachments can be found using the UI or a described command. Self-hosted and third-party service attachment URLs may be shared programmatically or through email depending on the implementation. The producer receives the consumer's connectivity configuration and passes this information to a service connection. Private Service Connect service connectivity automation creates an endpoint in the consumer VPC network. This endpoint connects to a service attachment in the producer VPC network. Private Service Connect has a few caveats. You can't create a Private Service Connect endpoint in the same VPC network as the published service that you are accessing. The endpoint can only be used to access a published service in another VPC network. The address counts toward the project quota for global internal IP addresses. Private Service Connect endpoints are not accessible from peered VPC networks. Instead, create a Private Service Connect endpoint in the peered VPC network. You can then configure workloads to refer to that endpoint. Connections from on-premises environments to non-Google services must use Cloud VPN tunnels. These on-premises environments must be in the same region as the Private Service Connect endpoint. For information about accessing Private Service Connect endpoints from on-premises environments that are connected using Cloud VPN, see access the endpoint from on-premises hosts in the Google Cloud documentation.

### Video - [Private services access](https://www.cloudskillsboost.google/course_templates/36/video/525310)

- [YouTube: Private services access](https://www.youtube.com/watch?v=W91ggX1ozaU)

Now let's discuss how to use private services access to provide access to producer services. Consider a scenario, Mark, a Cymbal corporation's network architect. Within Cymbal, a data science team has developed a proprietary machine learning API deployed with a private VPC subnet on Google Cloud. These systems need to access essential Google Cloud services like Cloud SQL or Vertex AI. The challenge, ensuring connectivity to essential Google services while keeping the API inaccessible from the public Internet. PSA is the solution. Let us see how. Like with private service connect, private services access lets consumers use internal IPv4 addresses to consume producer services. You will locate an internal IP range within your own VPC for PSA. The connection between consumer and producer uses VPC network peering. Private services access automates much of the VPC network peering configuration. With private service connect, you had to import and export routes between the consumer and producer VPC networks. Because the connection between the consumer and the producer is made using VPC network peering, you don't need to import and export routes. Subnet routes that don't use privately used public IP addresses are always exchanged between peered VPC networks. Private services access is available only for supported producer services, like Apigee, Cloud SQL, and Cloud TPU. For a complete list of supported producer services, see private services access supported services in the Google Cloud documentation. To offer private connectivity, the service producer must complete a one-time onboarding process. To complete the onboarding process, contact your Google representative. For more information, see onboarding process on the enabling private services Access page of the Google Cloud documentation. After the onboarding process is complete, you can configure private services access. To use private services access, both service consumers and producers must activate the service networking API in their projects. The consumer and producer VPC networks require some configuration as well. Service producers must allocate an IPv4 address range in the VPC network that contains the service. This address range is used for each connection from a service consumer. Service consumers must also allocate an IPv4 address range in their VPC network for each service producer. For example, to use services from three different producer VPC networks, the consumer must allocate three IPv4 address ranges, one for each producer VPC network. After the service producer has completed the initial configuration, consumers can create a private services access connection to the producer VPC network. You can use the Google Cloud Console or the Google Cloud CLI to create this connection. Consumers and producers can use the Google Cloud Console to edit their VPC network to configure private services access. If a service producer offers multiple services, you only need one private connection. For example, if a consumer uses Cloud SQL and Cloud TPU, only one private connection is created. Google Cloud uses VPC network peering to implement the connection between the consumer and producer VPC networks. Consumers can disable the private services access connection between their VPC network and the producer VPC network. Consumers can also edit their VPC network settings to disable access. Disabling the private services access connection does not delete the VPC network peering to the producer VPC network. You can delete the VPC network peering connection by editing the VPC network. Likewise, disabling the private services access connection does not release the IPv4 address range. Consumers must edit the VPC network to release the IPv4 address range. This example shows a sample private services access topology. The customer of VPC network allocated the 10.240.0.0/16 address range for Google services and established a private connection that uses the allocated range. Google then creates a project for the customer. With that project, each Google service creates a subnet from the allocated block to provision new resources in a given region. In the example, you can see a Cloud SQL instance. Cloud SQL instance is assigned an IP of 10.240.0.2, which is within the 10.240.0.0/16 range. In the customer VPC network, requests with the destination of 10.240.0.2 are routed over the private connection to the producer VPC network. The request is then sent to the correct resource in the producer VPC network. If the service supports cross-region communication, VM instances in the customer network can access service resources in any region. Some services might not support cross-region communication. For more information, see the documentation of the relevant service. Private services access has a few caveats. For private services access to an on-premises network to work, you must export custom routes from the on-premises network to the producer VPC network. Not all Google services are supported. For a complete list of supported Google services, see private services access supported services in the Google Cloud documentation. The same quota and limits that apply to VPC network peering also apply to private services access. Private services access uses VPC network peering to implement connections and thus has the same restrictions as VPC network peering. This table outlines three Google Cloud networking features. Private Google Access, PGA, simplifies how VMs without public IPs connect to essential Google services. Private service connect, PSC, allows you to expose your own services or Google produced or third party services securely to external consumers, even outside of Google Cloud. Private services access, or PSA, enables private access to services provided by Google or third parties within your VPC.

### Video - [Cloud NAT](https://www.cloudskillsboost.google/course_templates/36/video/525311)

- [YouTube: Cloud NAT](https://www.youtube.com/watch?v=AI1HXzrC19Y)

Next, you will learn how to use Cloud NAT to provide access to the public Internet for resources without an external IP address. Let's start with a simple use case. Janet is a network engineer at Cymbal Corporation. Cymbal has several non-production environments, development, testing, and staging, on Google Cloud. These environments host various VMs that need occasional outbound Internet access for tasks such as downloading software updates and dependencies, and accessing external testing tools or resources. Cymbal needs a streamlined, cost-effective way to handle outbound traffic without burning through their limited pool of public IPs. The solution, Janet decides to use Cloud NAT. This managed service provides a pool of highly available IP addresses that Janet can dynamically assign to their servers on demand. Cloud NAT translates the private IP addresses of Cymbal servers to a public IP address for outbound communication, seamlessly masking the internal network from the outside world. Cloud NAT is the Google managed network address translation service. It lets you provision your application instances without public IP addresses, and it also lets them access the Internet in a controlled and efficient manner. With Cloud NAT, your private instances can access the Internet for updates, patching, configuration management and more. Cloud NAT, as shown on the right, offers several advantages when compared to other NAT offerings, as shown on the left. As a fully managed software-defined service, Cloud NAT differs from traditional NAT proxy solutions. There are no NAT middle proxies in the path from the instance to the destination. Instead, each instance is a located NAT IP address, along with a slice of the associated port range. This allocated IP address and port range are used by the instance to perform NAT. This design is free of checkpoints and is highly reliable, performant, and scalable. Cloud NAT lets you configure multiple NAT IP addresses per NAT gateway. You can scale based on the size of your network without having to add or manage another NAT gateway. NAT IP allocation has two modes: manual and auto. The manual mode provides full control when specifying IP addresses. If you want to allow NAT addresses on the receiving side, use the manual mode. The auto mode enables the NAT IP addresses to be allocated and scaled automatically based on the number of instances. For a full overview of Cloud NAT features, see Cloud NAT overview in the Google Cloud documentation. In this diagram, Cloud NAT enables two private instances to access and update server on the Internet, which is referred to as outbound NAT. However, Cloud NAT does not implement inbound NAT. In other words, hosts outside your VPC network can't directly access any of the private instances behind the Cloud NAT gateway. Your VPC networks remain isolated and secure. With Cloud NAT, VMs without external IP addresses can access destinations on the Internet. For example, you might have VMs that only need Internet access to download updates or complete provisioning. Cloud NAT allows you to configure these VMs with an internal IP address. Thus, your organization needs fewer external IP addresses. Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses. Cloud NAT supports VMs that belong to managed instance groups, including those with auto scaling enabled. Cloud NAT is not dependent on a single physical gateway device. Cloud NAT is a distributed, software-defined, managed service. You configure a NAT gateway on a Cloud Router, which provides the control plane for NAT. Cloud Router contains the NAT configuration parameters. Google Cloud runs and maintains processes on the physical machines that run your Google Cloud VMs. With Private Google Access, Cloud NAT never performs NAT for traffic sent to the selected external IP addresses of Google APIs and services. Google Cloud routes this traffic internally. When you configure a Cloud NAT gateway to apply to a subnet range, Google Cloud automatically enables Private Google Access for that range. Thus, any VMs in that subnet range use Private Google Access to connect to Google APIs and services. If the gateway provides NAT for a subnet range, Private Google Access is in effect for that range and can't be disabled manually. There are two types of Cloud NAT available in Google Cloud. First up, we have public NAT. Think of this as your gateway to the Internet. It enables the resources inside your virtual private cloud, those without public IP addresses, to securely access the online world. Here's how it works. When a resource needs to connect to the Internet, public NAT dynamically assigns it a public IP address from a shared pool. This way, your resource can browse, download updates, and do all the things it needs to online, while its internal IP address remains safely tucked away. Now, let's explore private NAT. This type of NAT you'll use for secure communication between different networks. One important subtype of private NAT is inter-VPC NAT. This comes in handy when you have multiple virtual private clouds that need to talk to each other. It acts like a translator, making sure communication flows smoothly between these different networks, all through a central hub like the Network Connectivity Center. With both public and private NAT options at your disposal, you have the flexibility to securely connect your resources in Google Cloud, whether they need to access the Internet or communicate with other private networks. Before we wrap up this module, let us take a look at a sample Gemini prompt related to private service access. Here's an example of an operational prompt. How can I use private service access to connect my VMs to third-party services? You'll be provided with a well-structured response that outlines the prerequisites, steps, and detailed commands that can help you configure private service access. The output structure makes it easy to follow and minimizes the risk of errors during implementation.

### Video - [Lab Intro: Implement Private Google Access and Cloud NAT](https://www.cloudskillsboost.google/course_templates/36/video/525312)

- [YouTube: Lab Intro: Implement Private Google Access and Cloud NAT](https://www.youtube.com/watch?v=ppopjyRtPC0)

Next, you will apply what you learned by completing a lab exercise. In this lab, you will complete the following tasks. Configure a VM instance that doesn't have an external IP address, connect to a VM instance using an identity-aware proxy tunnel, enable private Google access on a subnet, configure a Cloud NAT gateway, and verify access to public IP addresses of Google APIs and services and other connections to the Internet.

### Lab - [Implement Private Google Access and Cloud NAT](https://www.cloudskillsboost.google/course_templates/36/labs/525313)

In this lab, you configure Private Google Access and Cloud NAT for a VM instance that doesn't have an external IP address. Then, you verify access to public IP addresses of Google APIs and services and other connections to the internet.

- [ ] [Implement Private Google Access and Cloud NAT](../labs/Implement-Private-Google-Access-and-Cloud-NAT.md)

### Video - [Debrief](https://www.cloudskillsboost.google/course_templates/36/video/525314)

- [YouTube: Debrief](https://www.youtube.com/watch?v=g6WwjJsJdMA)

In this module, you learned about several ways to connect privately from internal IP addresses to Google Cloud APIs, Google Services, and other resources. You can use Private Google Access to connect to Google APIs and Services. In addition to Google APIs and services, Private Service Connect lets you connect to other configured resources. Private Services Access simplifies creating a VPC network period connection between consumer and producer VPC networks. However, Private Services Access only works for some Google products and services. We then discussed using Cloud NAT to provide public Internet access for resources without public IP addresses. We finished the module with a lab exercise to implement Private Google Access with Cloud NAT, followed by a short quiz.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/36/quizzes/525315)

#### Quiz 1.

> [!important]
> **You want to provide access to services that you created in a VPC network. The services should be available to other specified VPC networks through endpoints that have internal IP addresses. Some of these VPC networks have subnets with overlapping internal IP addresses. Which product can you use?**
>
> - [ ] Private services access
> - [ ] Private Service Connect
> - [ ] Private Google Access
> - [ ] Cloud NAT

#### Quiz 2.

> [!important]
> **To enable Private Google Access for a VPC network:**
>
> - [ ] Enable it on the VPC network.
> - [ ] Enable it on the VPC network, on the desired subnets, and on Cloud Router.
> - [ ] Enable it on all desired subnets in the VPC network.
> - [ ] Enable it on all desired subnets and on Cloud Router.

#### Quiz 3.

> [!important]
> **Private services access automatically configures which Google Cloud product to implement communication between the producer and consumer VPC networks?**
>
> - [ ] Cloud NAT
> - [ ] Private Google Access
> - [ ] Shared VPC
> - [ ] VPC Network Peering

## Course Resources

Student PDF links to all modules

### Document - [Networking in Google Cloud: Routing and Addressing Course Resources](https://www.cloudskillsboost.google/course_templates/36/documents/525316)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
