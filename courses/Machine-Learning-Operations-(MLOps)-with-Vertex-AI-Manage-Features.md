---
id: 584
name: 'Machine Learning Operations (MLOps) with Vertex AI: Manage Features'
datePublished: 2024-02-23
topics:
- Machine Learning Operations
- MLOps
- Machine Learning Pipeline
type: Course
url: https://www.cloudskillsboost.google/course_templates/584
---

# [Machine Learning Operations (MLOps) with Vertex AI: Manage Features](https://www.cloudskillsboost.google/course_templates/584)

**Description:**

This course introduces participants to MLOps tools and best practices for deploying, evaluating, monitoring and operating production ML systems on Google Cloud. MLOps is a discipline focused on the deployment, testing, monitoring, and automation of ML systems in production.

Learners will get hands-on practice using Vertex AI Feature Store's streaming ingestion at the SDK layer.

**Objectives:**

- Containerize ML workflows for reproducibility, reuse, and scalable training and inference on Google Cloud.
- Efficiently share, discover, and re-use ML features at scale while conducting reproducible ML experiments with Vertex AI Feature Store.

## Welcome to the Machine Learning Operations (MLOps) with Vertex AI: Manage Features

Introduction to the course.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/584/video/457047)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=bXbwub5gMzU)

Welcome to Machine Learning Operations, MLOps, with Vertex AI, Manage Features, the second course of the new series of the Machine Learning Operations topic. Before we begin, let's quickly recap the content from our ML AI curriculum. In the first series of courses, called Machine Learning on Google Cloud, you'll learn about how machine learning on Google Cloud can make tasks better, faster and easier. The second series of courses, Advanced Machine Learning on Google Cloud, focuses on more wide ranging machine learning applications, including computer vision, natural language processing, and recommendation systems. This series of courses is all about Machine Learning Operations and focuses on machine learning models from an operational perspective. This particular course focuses on addressing data-related challenges in MLOps, and how to mitigate them. This content is designed for dedicated or aspiring machine learning data scientists, engineers, and analysts who are interested in learning about machine learning in the cloud and using ML models and Vertex AI. To get the most out of this specialization, it's recommended you have proficiency with Python on topics covered in the Crash Course on Python offered by Google. And prior experience with the foundational machine learning concepts and building machine learning solutions on Google Cloud as covered in the Machine Learning on Google Cloud courses. In the first part of the course, introduction to Vertex AI Feature Store, you will refresh your memory on how Vertex AI can help with MLOps processes, especially on data management and governance. Then you will explore the main challenges related to data and potential solutions to mitigate them. In the second part, Vertex AI Feature Store, an in-depth look, you will discover the key capabilities of Vertex AI Feature Store, which will let you build and deploy machine learning models. Then, you'll learn about the data model and terminology associated with Vertex AI Feature Store resources and components. And finally, you will explore the various storage methods available in Vertex AI Feature Store and learn how to effectively create and manage feature stores on the Vertex AI platform. Through a hands-on lab at the end, you will work on Vertex AI Feature Store streaming ingestion at the SDK layer. More specifically, you will perform the following tasks. Download and prepare data for BigQuery, create a new feature store, create a new entity type, create and write features to the feature store, read features back from the feature store. You also test your knowledge throughout the course with graded assessments. By the end of this course, you'll be able to understand the challenges of managing data in MLOps. Use Vertex AI Feature Store to manage and govern data, automate and streamline the MLOps process from a data perspective. Enroll today to learn about machine learning operations.

## Introduction to Vertex AI Feature Store

Vertex AI and its MLOps capabilities. Main challenges related to data and potential solutions to mitigate them.

### Video - [Recap: How does Vertex AI help with the MLOps workflow?](https://www.cloudskillsboost.google/course_templates/584/video/457048)

- [YouTube: Recap: How does Vertex AI help with the MLOps workflow?](https://www.youtube.com/watch?v=GkpCvL_HJ6o)

Welcome to the first section of the machine-learning operations or MLOps Fundamentals course. Before we explore the Vertex AI feature store, Let's recap how Vertex AI can help with MLOps processes. As you learned in the previous course, Vertex AI is a managed machine-learning platform that helps ML practitioners build, deploy, and scale ML models more efficiently and reliably. One of the primary focuses of Vertex AI is it enable ML practitioners in achieving end-to-end MLOps to efficiently and responsibly manage and govern AI. This includes managing data, features, models, and experiments. Therefore, end-to-end MLOps ensures efficient and responsible management, monitoring, governance, and explainability of ML projects throughout the entire development life cycle. Including experimentation, training, model deployment, continuous monitoring, model management, and governance. Then ML practitioners can streamline their workflows by reaching maturity Level 2 in the ML process. Which involves automating and integrating, training, validation, and deployment phases. When examining and managing the governing capabilities of Vertex AI, you'll find that the data and features play a crucial role in achieving mature MLOps processes. Vertex AI lets you create and manage features which enables effective data management. Moreover, it builds upon the challenges you explored in the previous course where ML practitioners face various obstacles when operationalizing and deploying models in production. These challenges encompass aspects like data, model architectures, hyper-parameters in experiments. In this course, the primary focus is on addressing these data-related challenges in exploring potential solutions to mitigate them. By understanding the nuances and complexities associated with data in machine learning projects, you can enhance your ability to effectively utilize and manipulate data throughout the entire development life cycle.

### Video - [Introduction to Vertex AI Feature Store](https://www.cloudskillsboost.google/course_templates/584/video/457049)

- [YouTube: Introduction to Vertex AI Feature Store](https://www.youtube.com/watch?v=-Xecf48U9kU)

Now it's time for a Vertex AI feature store demonstration. Let's start with identifying the main challenges with data and CO Vertex AI helps solve them.

### Video - [Introduction to Vertex AI Feature Store - Demo](https://www.cloudskillsboost.google/course_templates/584/video/457050)

- [YouTube: Introduction to Vertex AI Feature Store - Demo](https://www.youtube.com/watch?v=k2VmIEYuQAw)

Did you know that most of the time spent by data scientists goes into Wrangling data? More specifically in feature engineering, which is transforming raw data into high quality input signals for ML models. But this process is often inefficient and brittle. Well, I'm Priyanka and in this video we will identify the key challenges with feature engineering, how Vertex Feature Store helps solve them, and see a quick demo. Now, what are the key challenges with ML features? The first is that they are hard to share and reuse across your different steps of the ML workflow and across projects, which results in duplicate efforts. Second is that it is hard to serve in production reliably with lower latency. And the third is that there is an inadvertent skew in feature values between training and serving usually which causes your model quality to degrade over time. That is exactly where Vertex Feature Store comes in. It's a fully managed and unified solution to share, discover and serve machine learning features at scale across different teams within your organization. And it also helps reduce the time to build and deploy your AI ML applications. By making it easy to manage and organize your ML features in one place. It makes the features reusable, easy to serve and avoids skew. Now, let's see how to set it up. In the console, in Vertex AI we see the feature, tab to get started, let's click on this documentation and explore using Feature Store section. Now, the first thing you need is a Feature Store. At the time of this recording, Feature Store is in preview. So just know that depending on when you're watching this, there might be more options and updates that you would see. You cannot create a feature store in the console, so let's use this sample notebook to learn how to create it using the SDK. This sample uses a Movie Recommendations data set and the task is to train a model to predict if a user is going to watch a movie and serve this model online. We will learn to import our features into Feature Store, serve online prediction requests using the Imported features, and then access imported features in offline jobs such as training jobs. To set up, we install some additional packages, set up our project and authenticate our Google Cloud account. Step 1 is to create data set for output. We are creating BigQuery data set to host the output data. Input the name of the data set and the table we want to store the output data. Then we are defining constants and feature store related inputs. Here's how the Vertex Feature Store actually works. It organizes the data with the three hierarchical concepts. Feature Store, which is the place to store your features, entity type under Feature Store, describes an object to be modeled, real or virtual. And feature itself, under entity type describes an attribute of that entity type. Now, in our Movies prediction example, we will create a Feature Store called Movie Prediction. This store has two entity types, users and movies. The user's entity type has age, gender, and like genre features. The movie to entity type has the genre and average rating features. The first thing we do is to create the feature store. The method to create a Feature Store returns a long running operation that starts an asynchronous job. This may take about three minutes or so. And once the Feature Store is created, we can see it in the console. And we can create our entities in this store. I'm creating two entities here, user and Movies. We can also create features within these entities. Here I've created age, gender and like genres under user and title genres and average rating under our movies. If we want, we can search through and filter on these features. Now we need to import feature values before we can use them for online offline serving. Let's head back into our notebook to see how to import features in bulk using the Python SDK. We define the data source, the BigQuery table or cloud storage bucket, and the destination Feature Store, entity and the features to be imported. You do this for both users and the movies entity. Now, for a latency sensitive service such as Online Model Prediction, we would need to serve our feature values online. For example, for a movie service, we might want to quickly show movies that the current user would most likely watch by using online predictions. You can read one entity per request or even read multiple entities per request. Now, if you need feature values for high throughput, typically for training a model or batch prediction, then serving feature values in batch is a better idea than serving online. Consider this example. If the task is to prepare a training data set to train a model which predicts if a given user will watch a given movie then to achieve this we need two sets of inputs. Features that we have already imported and labels, which is the ground truth data recorded that user X has watched movie Y. It also includes the timestamp which indicates when the ground truth was actually observed. As labels and feature values are collected over time, those feature values change. The Feature Store can perform a point in time lookup so that you can fetch the feature values at a particular time. It's literally the data version of going back to a previous version of your source code in GitHub. Imagine freezing the state of the feature values at two different timestamps. And that was a quick summary of Vertex Feature Store. Just know that at the time of this recording, the feature store is in preview, so chances are there are more options and features available when you're watching this video. To explore more up to date information, check out the documentation that I've linked below. Any questions? Let me know in the comments or on Twitter at Pvergadia.

## Machine Learning Operations (MLOps) with Vertex AI: Manage Features An In-depth Look

Key capabilities of Vertex AI Feature Store

### Video - [Main capabilities of Vertex AI Feature Store](https://www.cloudskillsboost.google/course_templates/584/video/457051)

- [YouTube: Main capabilities of Vertex AI Feature Store](https://www.youtube.com/watch?v=xDY_vBwHESw)

Welcome to the second section of the machine learning operations fundamentals course. You are now familiar with the main challenges with feature engineering. Difficulty in sharing and reusing features across different ML workflows and projects, serving features reliably in production with low latency. And dealing with feature value skew between training and serving are indeed main pain points for ML practitioners. These challenges can be addressed by a fully managed and unified solution such as Vertex AI Feature Store. Vertex AI Feature Store streamlines the feature engineering process and enhances collaboration among ML practitioners. In this chapter, we explore how to use Vertex AI Feature Store to build and deploy a machine learning model. Let's start with the main capabilities that Vertex AI Feature Store provides. The first capability is sharing features across your organization. Vertex AI Feature Store enables efficient sharing of features among teams, therefore, you can quickly share them with others for training or serving tasks. Next is reducing duplicate efforts. Vertex AI Feature Store eliminates the need for feature reengineering across different projects. By managing and serving features from a central repository, organizations can ensure consistency and minimize redundant work, particularly for valuable features. Vertex AI Feature Store provides a centralized repository for storing and serving features. This can help to improve data quality and consistency, and it can also make it easier to track the usage of features. The next capability of Vertex AI Feature Store is search and filter capabilities. This means that others can easily discover and reuse existing features. This can be done by searching by feature name, entity type, or other criteria. For each feature, you can view relevant metadata to determine the quality and usage patterns of the feature. Vertex AI Feature Store is a managed solution for online feature serving. This capability provides low latency serving and eliminates the need for manual setup and management of low latency data serving infrastructure. This integrated management approach significantly reduces the challenges associated with feature engineering. And empowers ML practitioners to focus on their tasks instead of complex deployment processes. Next is mitigating training serving skill. Training serving skill refers to the mismatch between the feature data distributions used for training a machine learning model and serving it in production, leading to performance disparities. Vertex AI Feature Store contributes to reducing training serving skill by addressing two important aspects. Vertex AI Feature Store ensures that a feature value is ingested once into a feature store, and that same value is reused for both training and serving. Vertex AI Feature Store also provides point in time lookups to fetch historical data for training, which can help mitigate data leakage. Another capability is detecting drift. Significant changes in the featured data distribution can also affect the ML model performance. Detecting drift in the data distribution indicates a change in the underlying data, which can render the previous training data ineffective or inaccurate for your ML model. Vertex AI Feature Store empowers you to detect drift in feature data and enables proactive measures to address it. By continuously tracking the distribution of feature values ingested into the feature store, Vertex AI Feature Store can identify significant changes or anomalies in the feature data. And therefore, ensures that your models remain updated in evolving data environments. Overall, Vertex AI Feature Store is a powerful tool that can help organizations improve their machine learning workflows. If you are looking for a way to improve the efficiency, scalability, and reliability of your machine learning workflows, then Vertex AI Feature Store is a great option to consider.

### Video - [Data Model in Vertex AI Feature Store](https://www.cloudskillsboost.google/course_templates/584/video/457052)

- [YouTube: Data Model in Vertex AI Feature Store](https://www.youtube.com/watch?v=z1rEfRi6o08)

Now that you're familiar with the main capabilities of Vertex AI feature store, let's explore the data model and terminology that is used in Vertex AI feature store to describe its resources and components. Vertex AI feature store uses a time series data model to store a series of values for features. This model enables Vertex AI feature store to maintain feature values as they change over over time. Vertex AI feature store organizes resources hierarchically in the following order feature store, entity Type, feature. You must create these resources before you can ingest data into Vertex AI feature store. Let's look at each of these terms in detail. Feature Store, a feature store is a container for storing and managing features. Features are measurable attributes of entities such as customers, products, or orders. Organizations typically create one shared feature store for feature ingestion, serving and sharing across all teams. However, sometimes organizations might choose to create multiple feature stores within the same project to isolate environments. For example, organizations might have separate feature stores for experimentation, testing, and production. Feature stores can be a valuable asset for organizations that are looking to improve their machine learning capabilities. By centralizing the storage and management of features, feature stores can help to improve the efficiency and accuracy of machine learning models. Entity Type, an entity type is a group of features that are related to each other in some way. For example, a movie service might have entity types for movies and users. Each entity type would contain features that are relevant to that type of entity. For example, the movie entity type might contain features such as the movie's title, release date, and genre. The user entity type might contain features such as the user's name, email address, and age. Entity types are used to organize features in a feature store. This can help improve the efficiency of feature management and clarify the relationships between features. When you create an entity type, you specify the following information the name of the entity type, the description of the entity type, the features that belong to the entity type. After you create an entity type, you can use it to store and manage features. For example, you can create a feature called movie_rating and associate it with the entity type movie. This will allow you to store the rating of each movie in the feature store. Entity, an entity is a specific instance of an entity type. For example, movie_one and movie_two are entities of the entity type movie. In a feature store Entity IDs are used to identify entities in a feature store. This can be helpful for tracking entities and for querying entities. Entity IDs must be unique within a feature store. This means that no two entities can have the same ID. Entity IDs must be of the type string. This means that they must be a string of characters. Entity IDs can be used to query entities in a feature store. For example, you can query all entities that have a specific ID. Entity IDs can also be used to track entities. For example, you can track the changes that have been made to an entity over time. Feature, a feature is a measurable property or attribute of an entity type. For example, the movie entity type has features such as Average, Rating, and Title. Features are associated with entity types, which means that they can only be used to describe entities of that type. For example, the average_rating feature can only be used to describe movies. Features must be unique within a given entity type, but they do not need to be globally unique. This means that no two features in the same entity type can have the same name, but two different entity types can have features with the same name. For example, the movie entity type and the user entity type can both have a feature named Title. When reading feature values, you provide the feature and its entity type as part of the request. For example, to read the average_rating feature for the Movie entity type, you would specify the feature name as average_rating and the entity type name as Movie. When creating a feature, you specify its value type. The value type determines what types of values you can store for the feature. For example, the value type for the average_rating feature is double. This means that you can only store floating point numbers for the average_rating feature. For more information about the supported value types, see the value type in the API reference. Feature value in Vertex AI feature Store, feature values are captured at specific time points, which allows a single entity to have multiple values for a given feature. For instance, the movie_one entity can have different values for the average_rating feature, such as 4.4 at one time, and 4.8 at a later time. Each feature value in Vertex AI feature store is associated with a tuple identifier comprising the Entity ID, Feature ID and Timestamp. This identifier is used to retrieve feature values during serving time. Although time is continuous, Vertex AI feature stores discrete values. When you request a feature value at a particular time, T, Vertex AI feature store will return the last stored value at or above time T. Vertex AI feature store will return the latest stored value at or before time, T. For example, if the location information of a car is stored in Vertex AI feature store at times 100 and 110, the location at time 100 will be used for requests made between 100 inclusive and 110 exclusive. If you need a higher resolution, you can interpolate the location between values or increase the sampling rate of your data. Here is an example of how the Vertex AI feature store data model can be used to store and manage feature values. Let's say we have an entity type called Product. There will be multiple entities belonging to that entity type. In the Vertex AI feature store, the ID of each entity is always represented as a string. The product entity type can have many features associated with it. For example, a product could have features such as price, quantity, and color. Each feature has a value which is the value of the feature for a specific entity. For example, the value of the price feature for the product with ID 12345 might be $10. Feature values in most cases will not be static. For example, the number of purchases in the past week for a product might change over time. Hence, in the Vertex AI feature store, each feature value has a corresponding timestamp associated with it. The timestamp indicates when the feature value was recorded. This information can be used to track changes in feature values over time. Feature Ingestion, Feature ingestion is the process of importing feature values computed by your feature engineering jobs into a feature store. Before you can ingest data, the corresponding entity type and features must be defined in the feature store. Vertex AI feature store offers batch and streaming ingestion, letting you add feature values in bulk or in real time. For example, you might have computed source data that lives in locations such as Big Query or Cloud Storage. You can batch ingest data from those sources into a central feature store, so that those feature values can be served in a uniform format. As your source data changes, you can use streaming ingestion to quickly get those changes into your feature store. That way, you have the latest data available for online serving scenarios. Feature Serving, feature serving is the process of exporting stored feature values for training or inference. Vertex AI feature store offers two methods for serving features batch and online. Batch serving is for high throughput and serving large volumes of data for offline processing, like for model training or batch predictions. Online serving is for low latency data retrieval of small batches of data for real time processing, like for online predictions. Entity View, when you retrieve values from a feature store, the service returns an entity view that contains the feature values that you requested. You can think of an entity view as a projection of the features and values that Vertex AI feature store returns from an online or batch serving request. For online serving requests, you can get all or a subset of features for a particular entity type. For batch serving requests, you can get all or a subset of features for one or more entity types. For example, if features are distributed across multiple entity types, you can retrieve them together in a single request which joins those features together. You can then use the results to feed to a machine learning or batch prediction request. Export Data, vertex AI feature store lets you export data from your feature stores so that you can backup and archive feature values. You can choose to export the latest feature values, snapshot or a range of values, full export. For more information, see the Export feature Values Documentation. Before we move into the next section, let's look at requirements for the source data that can be ingested into Vertex AI feature store. Note that these requirements might change over time. For the latest data requirements, check the documentation. For batch ingestion, you can use tables in Big Query or files in Cloud Storage. For streaming ingestion, you can provide the feature values to ingest as part of the API request. Each item or row must have a column for Entity IDs, and the values must be of type string. Your source data value types must match the value types of the destination feature in the feature store. All columns must have a header that are of type string. If you provide a column for feature generation timestamps, use one of the following timestamp formats. For Big Query tables and Big Query views, timestamps must be in the Timestamp column. For Avro, timestamps must be of the type Long and Logical type timestamp micros. For CSV files, timestamps must be in the RFC 339 format. CS V files cannot include array data types. Use Avro or Big Query instead. For array types, you cannot include a null value in the array. However, you can include an empty array.

### Video - [Introduction to Vertex AI Feature Store](https://www.cloudskillsboost.google/course_templates/584/video/457053)

- [YouTube: Introduction to Vertex AI Feature Store](https://www.youtube.com/watch?v=4oagNPpDe9k)

In this section, we will discuss the different storage methods available in Vertex AI Feature Store, and learn how to create, list, describe, update, and delete feature stores. Let's start with storage methods. Vertex AI Feature Store uses two storage methods, online storage and offline storage. All feature stores have offline storage and optionally online storage. Online storage retains the latest timestamp values of your features to efficiently handle online serving requests. Offline storage stores data until the data reaches the retention limit or until you delete the data. You can control offline storage costs by managing how much data you keep. You can also override the default online store data retention limit for your feature store and the offline data retention limit for an entity type. You can always view the amount of online and offline storage you are currently using through the Google Cloud Console. Another important concept here is online serving nodes, which are a type of virtual machine used to serve feature values to online requests. Online serving notes are always running and can handle a large number of requests per second. Online serving nodes are an important part of Vertex AI Feature Store because they offer many benefits, including low latency, scalability, and reliability. The number of online serving nodes that you require is directly proportional to the following two factors. The number of online serving requests, queries per second that the feature store receives, the number of ingestion jobs that write to online storage. You can switch between the following options to configure your number of online serving nodes. Auto-scaling. The feature store automatically changes the number of nodes based on CPU utilization. Allocating a fixed node count. Vertex AI Feature Store maintains a consistent number of nodes regardless of the traffic patterns. If you choose auto-scaling, you must consider four additional points. After adding online serving notes, the online store needs time to re-balance the data. If you've submit online serving requests to the feature store without online serving nodes, the operation returns an error. If you don't require online serving and want to prevent incurring charges for online serving nodes, set the number of online serving nodes to zero. If you set the number of online serving nodes to zero, the entire online store, including its data, is deleted. Now, let's look at how to create, list, describe, update, and delete feature stores. Note that the instructions provided here are just a general overview and it is recommended to refer to the official documentation for more detailed guidance on each operation. Create a feature store. To store entity types and features, you can create a feature store using the Google Cloud Console if a feature store isn't already created in the Google Cloud project for the selected region. If a feature store already exists for the project and region, you can use Terraform or send a post request by using the feature store create method, or client libraries such as Python, Java, or Node.js. Here are a couple of important notes about creating a feature store. The feature store location must match the location of your source data. You can ingest data from Cloud Storage buckets in the same location as the feature store, or in the US multi-region location. For big query, you can ingest data from tables in the feature stores location or in the US multi-region location. Ensure that your source data meets the requirements specified in the documentation. Create a feature store with customer managed encryption key, CMEK. You can also create a feature store with customer managed encryption key or CMEK, if you want to have full control over the encryption of your data. With CMEK, you can create and manage your own encryption keys which are used to encrypt data at rest and in transit. This gives you the peace of mind, knowing that your data is protected, even if Google systems are compromised. If you don't have an existing CMEK, set up a customer managed encryption key by using Cloud Key Management Service, KMS, and configure your appropriate permissions. Here are some specific reasons why you should use CMEK for your feature store. Compliance requirements. If you're required to encrypt their data in order to comply with regulations such as HIPAA or GDPR, CMEK can help you meet these requirements. Data sovereignty. If you want to keep their data within their own control or within the control of a specific region, CMEK might help you achieve this goal. Security. CMEK gives you the ability to manage your own encryption keys, which can provide an additional layer of security for your data. View feature store details. You can get details about a feature stores such as its name, an online serving configuration through the Vertex AI section of the Google Cloud Console or REST API. If you use the Google Cloud Console, you can also view Cloud monitoring metrics for feature stores. Delete a feature store. To delete a store that contains existing entity types and features, use the force query parameter. However, be mindful when using this parameter because it will irreversibly delete the feature store and its data. Before we move on to the next section, where we will review the main concepts you learned throughout this course, let's practice what we learned.

### Video - [Lab Intro: Feature Store: Streaming Ingestion SDK](https://www.cloudskillsboost.google/course_templates/584/video/457054)

- [YouTube: Lab Intro: Feature Store: Streaming Ingestion SDK](https://www.youtube.com/watch?v=MDWMDfuK334)

Now it's time to practice with vertex AI feature store. In this lab, you get hands-on practice using vertex AI feature stores streaming ingestion at the SDK layer

### Lab - [Feature Store: Streaming Ingestion SDK](https://www.cloudskillsboost.google/course_templates/584/labs/457055)

In this lab, you learn how to ingest data from BigQuery into Feature Store using the Streaming Ingestion SDK.

- [ ] [Feature Store: Streaming Ingestion SDK](../labs/Feature-Store-Streaming-Ingestion-SDK.md)

## Summary

Summary of the course

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/584/video/457056)

- [YouTube: Summary](https://www.youtube.com/watch?v=l-Orn4UrlLk)

Congratulations, you've made it to the end of the MLOps with Vertex AI manage features course. Throughout the course, you were introduced to the feature store concept and the main capabilities of Vertex AI feature store. In the first part of this course, Introduction to Vertex AI feature store, you explored data related challenges. You reviewed how Vertex AI can help with ML 's processes, especially on data management governance. Then you were introduced to the main challenges related to data and potential solutions to mitigate them. In the second part of the course, Vertex AI feature store an in-depth look, you explored the key capabilities of Vertex AI feature store, which let you build and deploy machine learning models. Then you learn about the data model and terminology associated with Vertex AI feature store resources and components. And finally, you discovered the various storage methods available in Vertex AI feature store and learned how to effectively create and manage feature stores on the Vertex AI platform. In the hands-on lab at the end, you worked on Vertex AI feature store streaming ingestion at the SDK layer, you started with preparing data from BigQuery and creating a new feature store. Then you created a new entity type and create and wrote features to the feature store, and finally, you read features back from the feature store. This course is the second part of your machine learning operations journey, stay tuned for future machine learning operation courses with Google Cloud. For more training and hands-on practice with ML and AI, explore the options available at cloud.google.com/training/machine learning-ai. And if you're interested in validating your expertise and showcasing your ability to transform businesses with Google Cloud technology, you might consider working towards a Google Cloud certification. You can learn more about Google Cloud certification offerings at cloud.google.com/certifications. Thanks for completing this course, we'll see you next time.

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
