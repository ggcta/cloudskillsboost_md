---
id: 864
name: 'Observability in Google Cloud'
type: Course
url: https://www.cloudskillsboost.google/course_templates/864
date_published: 2024-12-02
topics:
  - Observability
  - Cost Management
---

# [Observability in Google Cloud](https://www.cloudskillsboost.google/course_templates/864)

**Description:**

Welcome to the second part of the two part course, Observability in Google Cloud.

This course is all about application performance management tools, including Error Reporting, Cloud Trace, and Cloud Profiler.

**Objectives:**

* Install and manage Ops Agent to collect logs for Compute Engine.
* Explain Cloud Operations for GKE.
* Analyze VPC Flow Logs and firewall rules logs.
* Analyze resource utilization cost for monitoring related components within Google Cloud.

## Introduction

Welcome to Observability in Google Cloud! We will cover the pre-requisites, audience and the course objectives.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/864/video/515449)

* [YouTube: Course Introduction](https://www.youtube.com/watch?v=kZ_V-_LYAao)

Welcome to the second part of the two part course, Observability in Google Cloud. The first course covered the operations-focused components including Logging, Monitoring, and Service Monitoring. This course is all about application performance management tools, including Error Reporting, Cloud Trace, and Cloud Profiler which tend to be more for developers who are trying to perfect or troubleshoot applications that are running in one of the Google Cloud compute products. In this course, we will shift our focus to Application Performance Management (APM) and delve into monitoring various compute options available within Google Cloud and also cover profiling, analyzing, and optimizing application code and infrastructure to identify performance bottlenecks. In this course you will learn to: Install and manage Ops Agent to collect logs for Compute Engine Use Google Cloud Managed Service for Prometheus Analyze VPC Flow logs and Firewall Rules logs Analyze resource utilization cost for monitoring related components within Google Cloud This course is designed to equip Cloud Architects, Administrators, SysOps personnel, Cloud Developers, and DevOps personnel with the essential skills and knowledge needed to excel in logging and monitoring your applications and workloads on Google Cloud. The prerequisites for this course are: Basic scripting or coding ability Google Cloud Fundamentals: Core Infrastructure or equivalent experience Proficiency with command-line tools and Linux operating system environments Without further adieu, let us get started!

## Configuring Google Cloud Services for Observability

In this module, we will take some time to examine the art of configuring Google Cloud services for observability. 

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/864/video/515450)

* [YouTube: Module Overview](https://www.youtube.com/watch?v=EhlnTUEy7SE)

In the next part of our Google Cloud Observability discussion, let’s take some time to examine the art of configuring Google Cloud services for observability. In this module, we're going to spend a little time learning how to use Ops Agent with Compute Engine. We will also explain the benefits of using Google Cloud Managed Service for Prometheus and usage of Prometheus Query Language (or PromQL) to query Cloud Monitoring metrics. We will then finally cover custom metrics.

### Video - [Introduction to Ops Agent](https://www.cloudskillsboost.google/course_templates/864/video/515451)

* [YouTube: Introduction to Ops Agent](https://www.youtube.com/watch?v=F3EcdgXXdu0)

The first section of this module focuses on how to collect metrics from applications deployed on a Google Cloud Compute Engine. Monitoring data can originate from a number of different sources. With Google Compute Engine instances, because the VMs are running on Google hardware, Cloud Monitoring can access some instance metrics without the agent, including CPU utilization, some disk traffic metrics, network traffic, and uptime information, but that information can be augmented by installing agents into the VM operating system. Many mission critical services use compute infrastructure directly and run on Google Compute Engine instances. How can we improve observability for those workloads? The Ops Agent is the primary agent for collecting telemetry data from your Compute Engine instances. Combining logging and metrics into a single agent, the Ops Agent uses Fluent Bit for logs, which supports high-throughput logging, and the Open Telemetry Collector for metrics. These agents are required for security reasons, the hypervisor cannot access some of the internal metrics inside a VM, for example, memory usage. You can configure the Ops agent to monitor many third-party applications such as Apache, mySQL, Oracle database, SAP HANA, and NGINX. The Ops Agent collects metrics inside the VM, not at the hypervisor level. For a detailed list, refer to the documentation. The Ops Agent supports most major operating systems such as CentOS, Ubuntu and Windows. We learned what Ops Agent is, let us next understand why we need Ops Agent. Here is an example of an infrastructure summary dashboard for a project with a few Compute Engine VMs in it. Notice that we're not getting any data about how our instances are using memory. That's because the VM hypervisor only knows how much memory is allocated to each VM, not how much of the allocated memory the VM is actually using. This is just one example of data that can only be gathered by a process running in the guest VM, such as an agent. There are other benefits of running the Ops Agent inside the VM: It monitors your VM instances without the need for any additional configuration after the installation. It helps monitor 3rd party applications and also supports both Windows and Linux guest OS. It exposes many additional process metrics beyond memory, and gives you better visibility to CPU, disk, and network performance. It exposes metrics beyond the 80+ metrics that Cloud Monitoring already supports for Compute Engine. The Ops Agent unifies gathering of metrics and logs into a single agent. And also ingests any user defined (Custom) metrics in Prometheus format. You can install Ops Agent by using three different methods: Use the Google Cloud CLI or the Google Cloud console to install the agent on individual VMs. Use an Agent Policy that installs and manages agents on your fleet of VMs. Use automation tools, like Ansible, Chef, Puppet, and Terraform, to install and manage agents on your fleet of VMs. We will cover the first two methods. For the automation process, refer to the documentation. Installing the Ops Agent is well documented on the Google site. In this slide we will cover the process using the agent policy. The first step is to install the beta component. Then enable the APIs and finally create a policy. Here, you see an example to create a policy named ops-agents-test-policy. The policy targets a single CentOS 7 VM instance named test-instance. It also installs both Logging and Monitoring agents on that VM instance. On this slide, we will cover the process for installing the Ops Agent on individual VMs. Go to the VM instances page in the Google Cloud console. Click the name of the VM that you want to install the agent on. The Details page opens. Click the Observability tab. The Observability page opens. Click Install. A new window open. Click Run in Cloud Shell. Cloud Shell opens and pastes the installation command. Press Enter on your keyboard to run the command. Click Authorize to allow Cloud Shell to install the agent. You can install a specific version of the agent and the steps also vary based on the operating system. For details refer to the documentation. Normally, you won't have to perform this step, but if the agent is not sending logs to Cloud Logging: First, check the metrics module in syslog If there are no logs, then agent service is not running. If you see a 403 permission errors when writing to the Monitoring API, enable the Logging API and Logs Writer role Make sure to check the Google documentation if you have questions. We learned how to install the Ops agent. Let us next look at how the data collected can be previewed as Dashboards. You can preview the dashboards and charts for telemetry data collected from the third-party applications such as Apache Web Server, MySQL, and Redis for deployments that run on Compute Engine and Google Kubernetes Engine. To view the logs, metrics and dashboards for data collected through Ops agent, navigate to Monitoring, and select the Integrations page. Integrations brings together metrics, logs, dashboards, and alerts to give you quick access to rich data for your application stack. Built on an open-source foundation, you can customize the data to fit your needs.

### Video - [Non-VM resources](https://www.cloudskillsboost.google/course_templates/864/video/515452)

* [YouTube: Non-VM resources](https://www.youtube.com/watch?v=wlyQBR2v_1U)

In addition to Google Cloud virtual machines, there are a lot of Google Cloud resources that support some type of monitoring. Let's look at a few of these. When monitoring any of the following non-virtual machine systems in Google Cloud, the Ops Agent is not required, and should not be installed: App Engine standard environment has monitoring built-in. App Engine flexible environment is built on top of GKE and has the Monitoring agent pre-installed and configured. With Standard Google Kubernetes Engine nodes (VMs), Cloud Monitoring and Cloud Logging is an option which is enabled by default. Cloud Run and Cloud Run functions provide integrated monitoring support. Google's App Engine standard and flexible environments both support monitoring. Make sure to check Google's documentation for the metric details. They also both support logging by writing to stdout or stderr. For refined logging capabilities, review the language-specific logging APIs, such as Winston for Node.js. Also, the logs are viewable under the GAE Application resources. Cloud Run functions offers lightweight, purpose-built functions, typically invoked in response to an event. For example, you might upload a PDF file to a Cloud Storage bucket, the new file triggers an event that invokes a Cloud Run functions instance, which translates the PDF from English to Spanish. Cloud Run functions monitoring is automatic and can provide you with access to invocations, execution times, memory usage, and active instances in the Google Cloud console. These metrics are also available in Cloud Monitoring, where you can set up custom alerting and dashboards for these metrics. Cloud Run functions also support simple logging by default. Logs written to stdout or stderr will appear automatically in the Google Cloud console. The logging API can also be used by extending log support. Cloud Run is Google’s managed container service. It can run in a fully managed version, in which it acts as a sort of App Engine for containers, and it can also run on GKE, in which case it’s a managed version of the open-source KNative. Cloud Run is automatically integrated with Cloud Monitoring with no setup or configuration required. This means that metrics of your Cloud Run services are captured automatically when they are running. You can view metrics either in Cloud Monitoring or on the Cloud Run page in the console. Cloud Monitoring provides more charting and filtering options. The resource type differs for fully managed Cloud Run and Cloud Run for Anthos: For fully managed Cloud Run, the monitoring resource name is "Cloud Run Revision" (cloud_run_revision). For Cloud Run for Anthos, the monitoring resource name is "Cloud Run on GKE Revision" (knative_revision). Cloud Run has two types of logs which are automatically sent to Cloud Logging, request logs and container logs. Request logs are logs of requests sent to Cloud Run services. And container logs are logs emitted from the container instances from your own code, written to stdout or stderr streams, or using the logging API.

### Video - [Cloud Operations for GKE](https://www.cloudskillsboost.google/course_templates/864/video/515453)

* [YouTube: Cloud Operations for GKE](https://www.youtube.com/watch?v=PvRM1maZcF0)

We looked at monitoring for Compute Engine and other compute options. Now lets look at monitoring options explicitly available for GKE. Google Kubernetes Engine (GKE) includes integration with Cloud Logging and Cloud Monitoring and Google Cloud Managed Service for Prometheus. When you create a GKE cluster that runs on Google Cloud, Cloud Logging and Cloud Monitoring are enabled by default and provide observability specifically tailored for Kubernetes. You also enable Google Cloud Managed service for Prometheus to collect Prometheus metrics and monitor workloads running on GKE and non-GKE compute workloads. We will explore this in detail in the next section. You can configure Cloud Logging and Cloud Monitoring for GKE clusters either during creation or after creation. During cluster creation, navigate to Standard mode under Operations. For new cluster, Cloud Logging and Cloud Monitoring are enabled by default. You can change the components for which the metrics and logs are collected under the Components section. Enable managed collection by selecting Managed Service for Prometheus.

### Video - [Google Cloud Managed Service for Prometheus](https://www.cloudskillsboost.google/course_templates/864/video/515454)

* [YouTube: Google Cloud Managed Service for Prometheus](https://www.youtube.com/watch?v=Tr1pdwnfxl4)

Next, let’s look at what Google Cloud Managed Service for Prometheus is all about and how it helps collect metrics. Google Cloud Managed Service for Prometheus is a fully managed service that makes it easy to collect, store, and analyze Prometheus metrics. Managed Service for Prometheus lets users collect metrics from both Kubernetes and VM environments at incredible scale without operational overhead by leveraging Monarch, Google's own globally available and scalable time series database. For those getting started with Prometheus for the first time, Managed Service for Prometheus helps make Google Kubernetes Engine even easier to use with maintained metric collectors. It's built on top of Monarch, the same globally-scalable data store as Cloud Monitoring. Monarch is an end-to-end monitoring system with high-level data modeling, data collection, querying, alerting and data management features. You can replace your existing Prometheus deployment to collect cluster and workload metrics and then query the data across multiple clusters by using PromQL. Managed Service for Prometheus splits responsibilities for data collection, query evaluation, rule and alert evaluation, and data storage into multiple components. Monarch handles the query evaluation and data storage. Monarch can execute queries and union results across all Google Cloud regions. It also supports two years of metric retention by default at no additional cost. When it comes to data collection, there are multiple choices available for data collection, which includes self-deployed, Ops Agent, OpenTelemetry, and managed collection. These collectors are responsible for scraping local exporters and forwarding that data to Monarch. Rule evaluation on the other hand is handled by locally run and configured rule evaluators. Refer to the documentation for latest information on the storage granularity and retention timeline. Another important Query Validator activity of Prometheus monitoring is making queries. Any UI that can call the Prometheus query API is also supported in the managed service for Prometheus. That includes Grafana and Cloud Monitoring. Your existing dashboards in Grafana continue to work just as before and you can keep using any PromQL found in popular open source repositories and forums. PromQL can be used to query: 1,500 free metrics in Cloud Monitoring Free Kubernetes metrics, custom metrics, and log-based metrics For collecting the data Prometheus will monitor, you can use the service in one of four ways: Managed data collection Self-deployed data collection Using Ops Agent OpenTelemetry collection Managed Service for Prometheus Managed Service for Prometheus offers an operator for managed data collection in Kubernetes environments. We recommend that you use managed collection; because it eliminates the complexity of deploying, scaling, sharding, configuring, and maintaining Prometheus servers. Managed collection is supported for both GKE and non-GKE Kubernetes environments. With self-deployed data collection, you manage your Prometheus installation as you always have. The only difference from upstream Prometheus is that you run the Managed Service for Prometheus drop-in replacement binary instead of the upstream Prometheus binary. You can configure the Ops Agent on any Compute Engine instance to scrape and send Prometheus metrics to the global data store. Using an agent simplifies VM discovery and eliminates the need to install, deploy, or configure Prometheus in VM environments. OpenTelemetry Collector uses a single collector to collect metrics from any environment and then sends them to any compatible backend. It is deployed either manually or by using Terraform in any compute or Kubernetes environment. When you choose between collection options, consider the following aspects: Managed collection is a recommended approach for all Kubernetes environments and is especially suitable for more hands-off fully managed experience. Self-deployed collection is suitable for quick integration into more complex environment. Using the Ops Agent is the easiest way and is recommended to collect and send Prometheus metric data originating from Compute Engine environments, including both Linux and Windows distros. The OpenTelemetry Collector is best to support cross-singal workflows such as exemplars. Managed Service for Prometheus provides a standalone rule evaluator for evaluating, recording, and alerting rules against all Monarch data accessible in a metrics scope. No need to co-locate the data in a single Prometheus server or on a single Google Cloud project. The rule evaluator uses the standard Prometheus rule files format, which makes migration to Managed Service for Prometheus easier. You can enable a managed collection for your resource by selecting the GKE cluster you need and clicking ENABLE SELECTED. After enabling managed collection, the in-cluster components are running, but no metrics are generated yet. You must deploy a PodMonitoring resource that scrapes a metrics endpoint to see any data in the Query UI. The manifest shown on slide defines a PodMonitoring resource, prom-example, in the namespace. The resource uses a Kubernetes label selector to find all the pods in the namespace that have the label app with the value prom-example. The matching pods are scraped on a port named metrics, every 30 seconds, on the /metrics HTTP path. To apply this resource, run the command on screen. To configure a horizontal collection that applies to a range of pods across all namespaces, use the ClusterPodMonitoring resource. The ClusterPodMonitoring resource provides the same interface as the PodMonitoring resource but does not limit discovered pods to a given namespace. When working with metric data, including data from Managed Service for Prometheus, in Cloud Monitoring, you can use the following query tools provided by Cloud Monitoring: PromQL Monitoring Query Language (MQL) Monitoring filters The simplest way to verify that your Prometheus data is being exported is to use the Cloud Monitoring Metrics Explorer page in the Google Cloud console: In the Google Cloud console, Go to Monitoring. In the Monitoring navigation pane, click Metrics Explorer. Select the PromQL tab. Enter the query. Here we are running a simple up query. And simply click Run Query. For MQL and filter options, refer to the documentation. Here is another example of using PromQL. This PromQL query show the average CPU utilization of the compute instances in your Google Cloud environment. The chart shows the visual representation of the utilization within a span of 1 hour.

### Video - [Exposing user-defined metrics](https://www.cloudskillsboost.google/course_templates/864/video/515455)

* [YouTube: Exposing user-defined metrics](https://www.youtube.com/watch?v=S_-ArsQu39A)

In addition to the more than 1,000 metrics that Google automatically collects, you can use code to create your own. Application-specific metrics, also known as user or custom metrics, are metrics that you define and collect to capture information that the built-in Cloud Monitoring metrics cannot. You capture such metrics by using an API provided by a library to instrument your code, and then you send the metrics to Cloud Monitoring. Custom metrics can be used in the same way as built-in metrics. That is, you can create charts and alerts for your custom metric data. . There are two fundamental approaches to creating custom metrics for Cloud Monitoring: You can use the classic Cloud Monitoring API. Or you can use the OpenTelemetry protocol and Ops Agent. The OpenTelemetry Protocol (OTLP) receiver is a plugin installed on the Ops Agent that helps collect the user-defined metrics from the application and send those metrics to Cloud Monitoring for analysis and visualization. These metrics can then be used to create dashboards, uptime checks, and altering policies. The ingestion and authorization is not required as it is handled at the agent level. To configure OTLP, you must install an Ops Agent and modify the user configuration file to include the OTLP file. By default, the receiver uses the Prometheus API; the default value for the metrics_mode option is googlemanagedprometheus. To receive the custom metrics from the OTLP receiver, set the OTLP receiver metrics_mode to googlecloudmonitoring. The steps used to create a custom metric using the API are well documented. To begin, the data you collect for a custom metric must be associated with a descriptor for a custom metric type. In this example, we create a gauge double metric named my_metric. It's a gauge metric of type double, with the description "Custom metric example." Once you collect the information you need for creating your custom metric type, call the create method, passing into a MetricDescriptor object. You write data points by passing a list of TimeSeries objects to create_time_series. You write data points by passing a list of TimeSeries objects to the function create_time_series. Each time series is identified by the metric and resource fields of the TimeSeries object. These fields represent the metric type and the monitored resource from which the data was collected. In this example, we use the my_metric described on the last slide to link our metric to the specified Compute Engine instance. Next, we create the point by adding it to the series and adding the details. Each TimeSeries object must contain a single Point object. Finally, we report our metric. After you configure the metrics_mode to Prometheus API or the Cloud Monitoring API in the OTLP receiver, you can then query the metrics by using Metrics Explorer, dashboards or even alternate interface. Here we see an example from the Metrics explorer, where you query the user defined metrics ingested by the monitoring API.

### Lab - [Monitoring a Compute Engine by using Ops Agent](https://www.cloudskillsboost.google/course_templates/864/labs/515456)

In this lab you will learn how to monitor an Apache Web Server installed on a Compute Engine virtual machine (VM) instance using Ops Agent.

* [ ] [Monitoring a Compute Engine by using Ops Agent](../labs/Monitoring-a-Compute-Engine-by-using-Ops-Agent.md)

### Video - [Module Summary](https://www.cloudskillsboost.google/course_templates/864/video/515457)

* [YouTube: Module Summary](https://www.youtube.com/watch?v=bdlayEQoEio)

In this module, you learned how to: Use Ops Agent with Compute Engine. Explain the benefits of using Google Cloud Managed Service for Prometheus. Explain the usage of PromQL to query Cloud Monitoring metrics. Explain custom metrics.

### Quiz - [Configuring Google Cloud Services for Observability](https://www.cloudskillsboost.google/course_templates/864/quizzes/515458)

#### Quiz 1.

> [!important]
> **Management wants to see an analysis of resources divided by development team, department, cost center, and application status. What could you do to make this easier?**
>
> * [ ] Add appropriate tags to your Google Cloud resources.
> * [ ] Add appropriate labels to your Google Cloud resources.
> * [ ] Used standardized prefixes on the names of all resources.
> * [ ] Use customized logging messages that include appropriate resource metadata.

#### Quiz 2.

> [!important]
> **What is used to collect metrics inside the VM instead of at the hypervisor level?**
>
> * [ ] Monarch
> * [ ] Graphana
> * [ ] Cloud Monitoring
> * [ ] Ops Agent

#### Quiz 3.

> [!important]
> **What are the three ways to install the Ops Agent?**
>
> * [ ] On a fleet of VMs
> * [ ] On a Single VM
> * [ ] Using Terraform
> * [ ] On a storage bucket
> * [ ] Using Cloud Run functions to trigger install
> * [ ] On a container

## Monitoring Google Cloud Network

Monitoring is all about keeping track of exactly what's happening with the resources we've spun up inside of Google's Cloud. In this module, we'll take a look at options and best practices as they relate to monitoring project architectures. We'll differentiate the core IAM roles needed to decide who can do what as it relates to monitoring. Just like architecture, this is another crucial early step. We will examine some of the Google created default dashboards, and see how to use them appropriately. We will create charts and use them to build custom dashboards to show resource consumption and application load. And, finally, we will define uptime checks to track liveliness and latency.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/864/video/515459)

* [YouTube: Module Overview](https://www.youtube.com/watch?v=XZ5Zryd7xoA)

In this module, let’s spend some time analyzing Google’s Virtual Private Cloud. Specifically, you learn to: Collect and analyze VPC Flow Logs, Firewall Rules Logging, load balancer logs, and Cloud NAT logs so you can see what's happening to the traffic across your network. Enable Packet Mirroring so you can replicate packets at the virtual machine network interface, and forward it for further analysis. We will also cover the capabilities of the Network Intelligence Center.

### Video - [VPC Flow Logs](https://www.cloudskillsboost.google/course_templates/864/video/515460)

* [YouTube: VPC Flow Logs](https://www.youtube.com/watch?v=EqpMIPywh3w)

Let’s start with monitoring the network. VPC Flow Logs records a sample (about one out of ten packets) of network flows sent from and received by VM instances, including Google Kubernetes Engine nodes. These logs can be used for network monitoring, traffic analysis, forensics, real-time security analysis, and expense optimization. VPC Flow Logs is part of Andromeda, the software that powers VPC networks. VPC Flow Logs introduces no delay or performance penalty when enabled. This slide shows an example of a VM to external traffic flow pattern. VPC Flow Logs provides visibility into traffic, which helps monitor the flow between zones and IP addresses. In this example, the traffic flows between a VM and an external network connected either through a Cloud VPN or Cloud Interconnect. Traffic flows are reported from the VM only and include: Igness traffic: The logs reported with VM as its destination. In this example, the ingress traffic is reported from the source VM 10.30.0.2 to 10.10.0.2 Egress traffic: The logs reported with VM as its source. In this example, the egress traffic is reported from the source VM 10.10.0.2 to 10.30.0.2 These are some of the main properties you must remember when working with VPC Flow Logs: VPC Flow Log samples are from a VM’s perspective. For this reason, if an egress firewall is denied, those packets are sampled by VPC Flow Logs. Similarly, the ingress blocked packets are not logged because they are sampled after the ingress firewall rules. VPC Flow Logs samples TCP, UDP, ICMP, ESP and GRE flows from each VM. It records inbound and outbound flows for each VM, thus capturing traffic between VM’s, VM to on-premises, VM to another host on the internet. VMs support multiple network interface and can be enabled at the subnet level. You can activate or deactivate VPC Flow Logs per VPC subnet. When enabled for a subnet, VPC Flow Logs collects data from all VM instances in that subnet. To enable VPC Flow Logs, during subnet creation, select On next to Flow Logs. You can optionally adjust log sampling and aggregation to adjust the metadata and sample rate that is written to logs. Each log entry contains a record of different fields. For example, this table illustrates the IP connection information that is recorded. Information consists of the source IP address and port, the destination IP address and port, and the protocol number. This set is commonly referred to as 5-tuple. Other fields include the start and end time of the first and last observed packet, the bytes and packets sent, instance details including network tags, VPC details, and geographic details. For more information on all data recorded by VPC Flow Logs, see the documentation. Logs Explorer can be used to access the VPC Flow Logs. The entries will be vpc_flows below the Compute Engine section. Searching the log names for vpc_flows works well. Log Analytics powered by BigQuery provides new capabilities to analyze flow log data and generate useful insights. With Log Analytics: You can analyze ad-hoc query-time without complex pre-processing as before. You can use BigQuery to query data and upgrade buckets to use Log Analytics and then create a linked dataset. Refer to the documentation for curated sample queries to get started with Flow Log Analysis.

### Video - [Firewall Rules Logging](https://www.cloudskillsboost.google/course_templates/864/video/515461)

* [YouTube: Firewall Rules Logging](https://www.youtube.com/watch?v=EkFPUy78djw)

Another essential part of knowing what's happening at the VPC network level is knowing what the firewall rules are doing. VPC firewall rules let you allow or deny connections to or from your virtual machine (VM) instances based on a configuration that you specify. Enabled VPC firewall rules are always enforced, and protect your instances regardless of their configuration and operating system, even if they didn’t start. Firewall Rules Logging lets you audit, verify, and analyze the effects of your firewall rules. It can help answer questions like: Did my firewall rules cause that application outage? How many connections match the rule I just created? Are my firewall rules stopping (or allowing) the correct traffic? See the Firewall Rule Logging documentation for details. By default, Firewall Rules Logging is disabled. You can enable it on a per-rule basis. In the slide screenshot, you’re editing the firewall rule named enable-rdp. Selecting the radio button will enable firewall rules. Note: Firewall Rules Logging can only record TCP and UDP connections. For other protocols, use Packet Mirroring. Caution: Firewall Rules Logging can generate a lot of data, which might have a cost implication. Firewall Rules Logging can also be activated on existing firewall rules by using the CLI. See these two examples on this slide. In both, the [NAME] tag will be the name of your firewall rule. Like all Google Cloud logs, use Logs Explorer to view logs in real time or to configure exports. To filter for firewall logs and network policy firewall logs, below the Compute Engine resource, select firewall. Many are familiar with classic segmentation or gateway-centric firewalls. In this example, you can see a private network, possibly at your office or home. At the network boundary, where the private network meets the outside internet, sits a firewall. A segmentation firewall is designed to segment and secure a protected network from an outside insecure network. Google Cloud VPC firewalls are micro-segmentation firewalls. These firewalls function more like a bunch of micro-firewalls, each operating over the Network Interface Controller (NIC) of every VM connected to the VPC. The micro-firewalls can then grant or deny any configured incoming or outgoing traffic. Now, imagine we have an issue. We have two different web servers. After some configuration changes by a particular DevOps team, the web servers can no longer access the application server they both share. How can we tell if the issue is firewall-related? Let's see. If the connectivity issue is related to a firewall, then there are two major possibilities: A firewall rule is actively blocking the incoming connections from the web servers. Or Network traffic is blocked by default in most networks. A firewall rule might not be allowing the traffic from the web servers as it should. Logging all denied connections could generate significant data that would take time and effort to monitor. So, instead of starting with option one, start with option two. Create a temporary high-priority rule designed to allow the web server traffic through to the app server. Enable Cloud logging on it so you can examine the entries. Suddenly the traffic is getting through, so you know it's firewall related. Now examine the log entries. Also, find the existing rule supposed to be allowing the traffic and see what you can find. Hey, look at that! The rule that's supposed to allow the traffic is based on a network tag named webserver. The web server machines are actually using the network tag web-server. There it is, that's your problem.

### Video - [Load balancer logs](https://www.cloudskillsboost.google/course_templates/864/video/515462)

* [YouTube: Load balancer logs](https://www.youtube.com/watch?v=R6D5ZHkmjDM)

Several of Google Cloud load balancers support monitoring or logging. While all the Google Cloud load balancers support Cloud Logging and Cloud Monitoring, the log type and log fields supported vary based on the type of the load balancers. These include: Internal and external Application Load Balancers, Internal and external Network Load Balancers, and internal and external Proxy Load Balancers. Cloud Logging for load balancing logs all the load balancing requests sent to your load balancer. These logs can be used for debugging and analyzing your user traffic. You can view request logs and export them to Cloud Storage, BigQuery, or Pub/Sub for analysis. For example, in network load balancer, per-connection logging gives you insight into how each connection is routed to serving backends. For external Application Load Balancers with backend buckets, logging is automatically enabled and cannot be disabled. You can activate logging on a per backend service basis. A single internal Application Load Balancer URL map can reference more than one backend service. You might need to enable logging for more than one backend service, depending on your configuration. It will be enabled by default for all new load balancers backends. But backends created before the Globally Available (GA) release of load balancer logging might require manual configuration. Application Load Balancing log entries contain information useful for monitoring and debugging your HTTP(S) traffic. Make sure to check the documentation for further details. Log entries contain the following types of information: LogEntry format includes general information shown in most logs, such as severity, project ID, project number, timestamp, and so on. However, HttpRequest.protocol is not populated for Application Load Balancing logs. This can include a method, a URL, remote IP address, a protocol, a latency string, or a user agent. resource contains the monitored resource type associated with the log entry. jsonPayload contains the statusDetails field. This field holds a string that explains why the load balancer returned the HTTP status that it did. Redirects (such as HTTP response status code 302 Found) issued from the load balancer are not logged. Redirects issued from the backend instances are logged. Let’s take an example of how to use log record information to troubleshoot a load balancing issue. Consider a scenario where the load balancer generates an HTTP error resource code 5XX and sends the same error code to the client. Refer to the load balancer logs to determine the source of an error: Within the statusDetails field: the response_sent_by_backend indicates it is a backend issue. Whereas, failed_to_pick_backend indicates that the load balancer failed to pick a healthy backend to handle a request.

### Video - [Cloud NAT Logs](https://www.cloudskillsboost.google/course_templates/864/video/515463)

* [YouTube: Cloud NAT Logs](https://www.youtube.com/watch?v=CX4ubkdNgnA)

Another piece of the network telemetry features in Google Cloud is Cloud NAT logs. Cloud NAT is the Google-managed Network Address Translation service. It lets you provision your application instances without public IP addresses, and it also lets them access the internet in a controlled and efficient manner. With Cloud NAT, your private instances can access the internet for updates, patching, configuration management, and more. There are many Cloud NAT benefits. VMs without external IP addresses can access destinations on the internet. For example, you might have VMs that only need internet access to download updates or complete provisioning. Cloud NAT lets you configure these VMs with an internal IP address. Thus, your organization needs fewer external IP addresses. Cloud NAT can be configured to automatically scale the number of NAT IP addresses that it uses. Cloud NAT supports VMs that belong to managed instance groups, including those with autoscaling enabled. Cloud NAT is not dependent on a single, physical gateway device. Cloud NAT is a distributed, software-defined managed service. You configure a NAT gateway on a Cloud Router, which provides the control plane for Cloud NAT. Cloud Router contains the NAT configuration parameters. Cloud NAT logging lets you log NAT TCP and UDP connections and errors. When Cloud NAT logging is enabled, a log entry can be generated when a network connection that uses Cloud NAT is created, and/or when an egress packet is dropped because no port was available for Cloud NAT. You can opt to log both kinds of events, or just one or the other. Logs contain TCP and UDP traffic only, and the log rate threshold will reach a maximum of 50-100 log events per vCPU before log filtering. Cloud NAT logging might be enabled when a new Cloud NAT gateway is first created, or by editing the settings of an existing gateway. To view the collected logs in Logs Explorer, filter to the Cloud NAT Gateway resource and optionally, restrict to a particular region or Gateway.

### Video - [Packet Mirroring](https://www.cloudskillsboost.google/course_templates/864/video/515464)

* [YouTube: Packet Mirroring](https://www.youtube.com/watch?v=ebtw40TUYJE)

Another way to monitor the network traffic flowing in and out of your Compute Engine virtual machines is to use Packet Mirroring. Packet Mirroring clones the traffic of specific instances in your Virtual Private Cloud (VPC) network and forwards it for examination. Packet Mirroring captures all ingress and egress traffic and packet data, such as payloads and headers. The mirroring happens on the virtual machine (VM) instances, not on the network. Therefore, Packet Mirroring consumes additional bandwidth on the hosts. Packet Mirroring is useful when you need to monitor and analyze your security status. It exports all traffic, not only the traffic between sampling periods. For example, you can use security software that analyzes mirrored traffic to detect all threats or anomalies. Also, you can inspect the full traffic flow to detect application performance issues and to provide network forensics for Payment Card Industry Data Security Standards (PCI DSS) compliance and other regulatory use cases. We will elaborate on this further in the next few slides. Obviously, Packet Mirroring can generate significant data, so collector destination is generally an instance group behind a TCP/UDP load balancer or equivalent technology. One of the major limitations of Packet Mirroring is bandwidth consumption. Packet Mirroring consumes the egress bandwidth of the mirrored instances. However, there is a work around. Use filters to reduce the traffic collected for mirrored instances. This filter can be used for IP address ranges, protocols, traffic directions and lot more. The current maximum number of filters that can be used for Packet mirroring is 30. For more information, refer to the documentation. Two main use cases where Packet Mirroring is useful in security and monitoring. Let’s explore each of these use cases in detail. Network and application monitoring: Network engineers can use the data from Packet Mirroring to: Maintain integrity of deployment. Troubleshoot packet loss issues by analyzing protocols. Troubleshoot reconnection and latency issues by analyzing real time traffic patterns. Security and compliance: Implement zero-trust by monitoring network traffic across and within the trust boundaries without any network re-architecture. Packet Mirroring helps capture multiple packets for a single flow. This information can be quite useful for the implementation and usage of the following security tools: Intrusion detection systems match signatures with multiple packets of a single flow. Deep Packet Inspection engines inspect payloads for anomalies. Network forensics for PCI compliance: Packet mirroring help capture, process and preserve forensic of different attack vectors. Packet Mirroring exports monitoring data about mirrored traffic to Cloud Monitoring. You can use monitoring metrics to check whether traffic from a VM instance is being mirrored as intended. For example, you can view the mirrored packet or byte count for a specific instance. You can also view the monitoring metrics of mirrored VM instances or instances that are part of the collector destination (internal load balancer). For mirrored VM instances, Packet Mirroring provides metrics specific to mirrored packets such as mirrored packets count, mirrored bytes count and dropped packets count. Monitoring can also spot where packet mirroring is being used unnecessarily or unexpectedly. Remember that, as noted earlier, mirroring generates significant data that requires storage and processing. Also, note that it slows the network throughput of the virtual machines being monitored and might accidentally expose sensitive data.

### Video - [Network Intelligence Center](https://www.cloudskillsboost.google/course_templates/864/video/515465)

* [YouTube: Network Intelligence Center](https://www.youtube.com/watch?v=PwV23yKmcRo)

This section is a bit of a detour, but let’s at least mention the Network Intelligence Center and how it helps with network analysis. Network Intelligence Center gives you centralized monitoring and visibility into your network. It reduces troubleshooting time and effort and increases network security, all while improving the overall user experience. Currently, it offers five modules: Network Topology, Connectivity Tests, Performance Dashboard, Firewall Insights, and Network Analyzer. Network Topology visualizes your Google Cloud network as a graph. You can use the graph to explore your existing configurations and quickly troubleshoot networking issues. You can select network entities, filter, see lines of communication with bandwidth information, expand and collapse hierarchies, and select time boundaries. The Connectivity Tests tool in Network Intelligence Center helps you to quickly diagnose connectivity issues and prevent outages. These tests let you self-diagnose connectivity issues within Google Cloud or from Google Cloud to an external IP address (the connectivity issue could be on-premises or in another cloud). The results help to isolate whether the issue is in Google Cloud. Run tests to help verify the effect of configuration changes and ensure that network intent captured by these tests is not violated, proactively preventing network outages. These tests also help assure network security and compliance. Performance Dashboard gives you visibility into the performance of your VPC. The Packet Loss tab shows the results of active probing between your VMs in a given VPC. To get this data, it runs workers on the physical hosts that house your VMs. These workers insert and receive probe packets that run on the same network as your traffic, revealing issues on that network. Workers run on the physical host and not on your VM. Therefore, these workers do not consume VM resources and the traffic is not visible on your VMs. Packet loss is aggregated for all zone pairs. The Latency tab aggregates latency information based on a sample of your actual Transmission Control Protocol (TCP) VM traffic. The method used is similar to the one used for VPC Flow Logs. The latency is calculated as the time that elapses between sending a TCP sequence number (SEQ) and receiving a corresponding Acknowledgement (ACK) that contains the network Round Trip Time (RTT) and TCP stack related delay. The latency metric is only available if TCP traffic is around 1,000 packets per minute or higher. Firewall Insights, a component product of Network Intelligence Center, produces metrics and insights that let you make better decisions about your firewall rules. It provides data about how your firewall rules are being used, exposes misconfigurations, and identifies rules that could be made more strict. Firewall Insights uses Cloud Monitoring metrics and Recommender insights. Cloud Monitoring collects measurements to help you understand how your applications and system services are performing. A collection of these measurements is generically called a metric. The applications and system services being monitored are called monitored resources. Measurements might include the latency of requests to a service, the amount of disk space available on a machine, the number of tables in your SQL database, the number of widgets sold, and so forth. Resources might include virtual machines, database instances, disks, and so forth. Recommender is a service that provides recommendations and insights for using resources on Google Cloud. These recommendations and insights are per-product or per-service, and are generated based on heuristic methods, machine learning, and current resource usage. You can use insights independently from recommendations. Each insight has a specific insight type. Insight types are specific to a single Google Cloud product and resource type. A single product can have multiple insight types, where each provides a different type of insight for a different resource. Firewall Insights metrics let you analyze the way that your firewall rules are being used. Firewall Insights metrics are available through Cloud Monitoring and the Google Cloud console. Metrics are derived through Firewall Rules Logging. With Firewall Insights metrics, you can perform the following tasks: Verify that firewall rules are being used in the intended way. Over specified time periods, verify that firewall rules allow or block their intended connections. Perform live debugging of connections that are inadvertently dropped because of firewall rules. Discover malicious attempts to access your network, in part by getting alerts about significant changes in the hit counts of firewall rules. Network Analyzer automatically monitors your VPC network configurations and detects misconfigurations and suboptimal configurations. It provides insights on Network Topology, firewall rules, routes, configuration dependencies, and connectivity to services and applications. It identifies network failures, provides root cause information, and suggests possible resolutions. Network Analyzer runs continuously and triggers relevant analyses based on near real-time configuration updates in your network. If a network failure is detected, it tries to correlate the failure with recent configuration changes to identify root causes. Wherever possible, it provides recommendations to suggest details on how to fix the issues. Network Analyzer provides insights that help identify common issues such as connectivity blockage, load balancing errors, external IP address that are not used but allocated, invalid next hop, GKE network misconfiguration and lot more. It also identifies the root cause of the insights and also provides recommended fixes. In the example above, an insight of the type Error, a GKE node to control plane connectivity is generated. The insight page also describes the following: The root cause: an ingress firewall rule is blocking the connection between the node and the plane. This indicated that the default drywall rules were modified, removed, or shadowed by another firewall rule. A solution: if the root of the problem is a deleted firewall, create a new firewall rule. If it's a shadowed firewall rule, then increase the priority.

### Lab - [Analyzing Network Traffic with VPC Flow Logs](https://www.cloudskillsboost.google/course_templates/864/labs/515466)

In this lab, you configure a network to record traffic to and from an apache web server using VPC Flow Logs. You will then export the logs to BigQuery to analyze them.

* [ ] [Analyzing Network Traffic with VPC Flow Logs](../labs/Analyzing-Network-Traffic-with-VPC-Flow-Logs.md)

### Video - [Module summary](https://www.cloudskillsboost.google/course_templates/864/video/515467)

* [YouTube: Module summary](https://www.youtube.com/watch?v=V4LwEoe2k0U)

After completing this module, you know how to: Collect and analyze VPC Flow Logs, Firewall Rules Logging, load balancer logs, and Cloud NAT logs so you can see what's happening to the traffic across your network. Enable Packet Mirroring so you can replicate packets at the virtual machine network interface and forward it for further analysis. And explain the capabilities of the Network Intelligence Center.

### Quiz - [Monitoring Google Cloud Network](https://www.cloudskillsboost.google/course_templates/864/quizzes/515468)

#### Quiz 1.

> [!important]
> **What is one of the disadvantages of using packet mirroring?**
>
> * [ ] Bandwidth consumption
> * [ ] Expensive
> * [ ] Increased Latency
> * [ ] No Disaster Recovery

#### Quiz 2.

> [!important]
> **What lets you perform a live debugging of connections that are inadvertently dropped?**
>
> * [ ] Firewall Rules Logging
> * [ ] Cloud NAT logs
> * [ ] Load Balancer Logs
> * [ ] Firewall Insights

#### Quiz 3.

> [!important]
> **What logs help you monitor a network by recording a portion of network flows sent and received by VM instances (including GKE nodes)?**
>
> * [ ] Cloud NAT logs
> * [ ] Firewall logs
> * [ ] Load balancer logs
> * [ ] VPC Flow logs

## Investigating Application Performance Issues

When deploying applications to Google Cloud, the Application Performance Management products provide a suite of tools to give insight into how your code and services are functioning, and to help troubleshoot where needed.

### Video - [Module Introduction](https://www.cloudskillsboost.google/course_templates/864/video/515469)

* [YouTube: Module Introduction](https://www.youtube.com/watch?v=R50GaO9G7OI)

When deploying applications to Google Cloud, the Application Performance Management products (Cloud Trace and Cloud Profiler) provide insight into how your code and services are functioning. The tools also help troubleshoot when needed. In this module, you will learn to: Explain the features and benefits of Error Reporting, Cloud Trace, and Cloud Profiler. List and explain the functionalities of the Error Reporting, Cloud Trace, and Cloud Profiler components.

### Video - [Error Reporting](https://www.cloudskillsboost.google/course_templates/864/video/515470)

* [YouTube: Error Reporting](https://www.youtube.com/watch?v=IjQonGMLrmE)

Application Performance Management (APM) combines the monitoring and troubleshooting capabilities of Cloud Logging and Cloud Monitoring with Error Reporting, Cloud Trace, and Cloud Profiler. APM helps you reduce latency and cost so that you can run more efficient applications. Let's start with Error Reporting. Error Reporting looks through all the logs that your application and infrastructure has reported. It then counts, analyzes, and aggregates the exceptions to report them on your preferred notification channel such as email, mobile app, slack, or through web hooks. Error Reporting can only analyze log entries that are stored in Cloud Logging buckets that are in the global region. The source and destination Google Cloud projects must be the same, and customer-managed encryption keys (CMEK) must be disabled. If you route logs to a different Cloud project, regionalized buckets, or enable CMEK, then Error Reporting doesn't capture and analyze those logs. Error Reporting has several features. It helps understand errors. See at a glance the top or new errors for your application in a clear dashboard. Looking at a log stream to find important errors can slow you down when you’re troubleshooting. Error Reporting brings you the processed data directly to help you understand and fix the root causes faster. Real production problems can be hidden in mountains of data. Error Reporting helps you see the problems through the noise by constantly analyzing your exceptions. Problems are intelligently aggregated into meaningful groups tailored to your programming language and framework. It provides instant error notification. You do not wait for your users to report problems. Error Reporting is always watching your service and instantly alerts you when a new application error cannot be grouped with existing ones. Directly navigate from a notification to the details of the new error. Error Reporting is available on desktop and in the Google Cloud app for iOS and Android. It also provides popular language and product support. Support is available for many popular languages, including Go, Java, Node.js, PHP, Python, Ruby, or . NET. Use our client libraries, REST API, or send errors with Cloud Logging. Error Reporting can aggregate and display errors for: App Engine standard environment and flexible environment, Cloud Run functions, Apps Script, Cloud Run, Compute Engine, Amazon EC2, and Google Kubernetes Engine (or GKE). Setting up Error Reporting is simple and dependent on the language and compute environment. Here’s an error-catching example written in Node.js and run on Cloud Run. To report errors, the code needs the Error Reporting Writer Identity and Access Management role. Enable the Error Reporting API, and install the client library by using npm. The easiest way to manually log errors to Error Reporting in Node.js is to import the Error Reporting library. You then instantiate a client to start reporting errors to Error Reporting. Optionally, you can also customize the behavior of the Error Reporting library for Node.js. These can be configured by passing objects to options. Use error message builder to customize all fields. Call the report method to manually report an error. You can also integrate the Error Reporting library for Node.js to web frameworks like Express.js. Refer to the documentation for more details. Error Reporting Library for Node.js can be configured on many Google Cloud environments. Let’s explore the process for all. For App Engine flexible environment and standard environment, Cloud Run, Cloud Run functions, and Apps Script, Error Reporting is automatically enabled. For Google Kubernetes Engine, add cloud-platform access scope during cluster creation. For Compute Engine, ensure the service account used has the Error Reporting Writer role. Outside Google Cloud, provide the Google Cloud project ID and service account credentials to the Error Reporting library for Node.js. To see your errors, open the Error Reporting page in the Google Cloud console. By default, Error Reporting shows you a list of recently occurring open and acknowledged errors in order of frequency. Errors are grouped and de-duplicated by analyzing their stack traces. When Auto reload is turned on, Error Reporting automatically reloads the error list every 10 seconds. Error Reporting recognizes the common frameworks used for your language and groups errors accordingly. You can sort errors based on occurrences (first and last seen). You can also link an issue tracker link to an error group by clicking the + icon below Insert link. Selecting an error entry will let you expand into the Error Details page. On this page, you can examine information about the error group, including the history of a specific error, specific error instances, and diagnostic information. To view the log entry associated with a sample error, click View logs from any entry in the Recent samples panel. You’re taken to Logs Explorer in the Cloud Logging console.

### Video - [Trace](https://www.cloudskillsboost.google/course_templates/864/video/515471)

* [YouTube: Trace](https://www.youtube.com/watch?v=wD_SF30vC9Y)

Now, let’s have a look at Cloud Trace. Cloud Trace is a distributed tracing system that collects latency data from your applications and displays it in the Google Cloud console. You can track how requests propagate through your application and receive detailed near-real time performance insights. Trace automatically analyzes all of your application traces to generate in-depth latency reports to surface performance degradations. It helps with Issue detection. Trace continuously gathers and analyzes trace data from your project to automatically identify recent changes to the performance of your application. These latency distributions, available through the Analysis Reports feature, can be compared over time or versions. If Trace detects a significant shift in the latency profile of your app, you’re automatically alerted. It also helps with identification of performance bottlenecks. Cloud Trace helps inspect detailed latency information for a single request or view aggregate latency for your entire application. Using the various tools and filters provided, you can quickly find where bottlenecks are occurring and more quickly identify their root cause. Trace is based off the tools used at Google to keep our services running at extreme scale. Trace offers broad platform support. The language-specific SDKs of Trace can analyze projects that run on VMs (even VMs not managed by Google Cloud). The Trace SDK is available for Java, Node.js, Ruby, and Go. The Trace API can be used to submit and retrieve trace data from any source. A Trace consists of a tracing client, which collects spans and sends them to Cloud Trace. You can then use the Google Cloud console to view and analyze the data collected by the agent. A trace describes the time that it takes an application to complete a single operation. A trace is a collection of spans. A span describes how long it takes to perform a complete suboperation. For example, a trace might describe how long it takes to process an incoming request from a user and return a response. A span might describe how long a particular RPC call requires. If an OpenCensus library is available for your programming language, you can simplify the process of creating and sending trace data by using OpenCensus. In addition to being simpler to use, OpenCensus implements batching that might improve performance. If an OpenCensus library doesn't exist, instrument your code by importing the Trace SDK library and using the Cloud Trace API. The Cloud Trace API collects trace data and sends it to your Google Cloud project. There are two ways to send trace data to Cloud Trace: The first one is automatic tracing. Some configurations support automatic tracing. These include: App Engine standard environment with Java 8, Python 2, and PHP 5 applications. HTTP requests and latency data from Cloud Run functions and Cloud Run. And the second one is by instrumenting the application. You can do this by using Google client libraries or OpenTelemetry, which is the recommended option. Trace will need to offload tracing metrics to Google Cloud. As far as the required IAM permissions are concerned, for external systems, or Compute Engine and GKE environments that don't run under the default service account, ensure that they run under a service account with at least the Cloud Trace Agent role. App Engine, Cloud Run, Cloud Run functions, Google Kubernetes Engine, and Compute Engine have default access. Compute Engine and GKE get that access through the default Compute Engine service account. Analysis reports in Trace show you an overall view of the latency for all requests, or a subset of requests, to your application. Analysis reports will be similar to the daily report viewed on the Trace Overview main page. Custom reports can be created for particular request URIs and other search qualifiers. The report can show results as both a density distribution, as in the screenshot, or as a cumulative distribution.

### Lab - [View application latency with Cloud Trace](https://www.cloudskillsboost.google/course_templates/864/labs/515472)

In this lab, you learn how to use Cloud Trace by creating a trace by sending an HTTP request to the sample application and use the Cloud Trace interface to view the latency information of the trace.

* [ ] [View application latency with Cloud Trace](../labs/View-application-latency-with-Cloud-Trace.md)

### Video - [Profiler](https://www.cloudskillsboost.google/course_templates/864/video/515473)

* [YouTube: Profiler](https://www.youtube.com/watch?v=gfxBdeOO_bg)

Finally, let's look at Cloud Profiler. Understanding the performance of production systems is notoriously difficult. Attempting to measure performance in test environments usually fails to replicate the pressures on a production system. Continuous profiling of production systems is an effective way to discover where resources like CPU cycles and memory are consumed when the service operates in its working environment. Cloud Profiler is a statistical, low-overhead profiler that continuously gathers CPU usage and memory-allocation information from your production applications. It attributes that information to the source code that generated it, which helps you identify the parts of your application that are consuming the most resources. The insights provided illuminate the performance of your application characteristics. Cloud Profiler has many features. It provides low-impact production profiling Although it's possible to measure code performance in development environments, the results don‚Äôt often show what happens in production. Many production profiling techniques slow down code execution or can only inspect a small subset of a codebase. A comprehensive application performance view is provided without slowing it down. It offers practical application profile creation Poorly performing code increases latency and costs for web applications and services every day, without anyone knowing or doing anything about it. Cloud Profiler changes this situation by continually analyzing the performance of CPU or memory-intensive functions that run in an application. Cloud Profiler presents the call hierarchy and resource consumption of the corresponding function in an interactive flame graph. This graph helps developers figure out which paths are consuming the most resources and what are the different ways your code is called. It also offers broad language and product support Cloud Profiler enables developers to analyze applications that run anywhere, including Google Cloud and other on-premises or cloud platforms that support Java, Go, Node.js, and Python. You can find a full explanation of language compatibility in the documentation. The profiling types available vary by language. Check the Cloud Profiler documentation for the most recent options. For the CPU metrics, you will find the following: CPU time is the time that the CPU spends executing a block of code. The time it was waiting or processing instructions for something else is not included. Wall time is the time that it takes to run a block of code, including all wait time, including that for locks and thread synchronization. The wall time for a block of code can never be less than the CPU time. For heap, you have the following: Heap is the amount of memory allocated in the heap of the program when the profile is collected. Allocated heap is the total amount of memory that was allocated in the heap of the program. Allocated heap includes memory that has been freed and is no longer in use. And for threads you have: Contention, which provides information about threads stuck waiting for other threads. Threads, which contain thread counts. Profiler instruments applications that run in most Google and non-Google compute technologies. Note that Windows guest OS is not supported. Like with other Google application performance management products, the exact setup steps vary by language, so check the documentation to find more information. Here, we are sticking with our Python application, which will run on App Engine. Before you start, ensure that the Profiler API is enabled in your project. Start by importing the googlecloudprofiler package. Install the C/C++ compiler and development tools, pip, and Profiler package. Then, early as possible in your code, start the profiler. In this example, we are setting the logging level (verbose) to 3, or debug level. That setting will log all messages. The default would be 0 or error only. At the top of the Profiler interface, you can select the profile that you want to analyze. You can select by options including Timespan, Service, Profile type, Zone, and Version. The Weight will limit the subsequent flame graph to particular peak consumptions. Top 5%, for example. Compare to allows the comparison of two profiles. Profiler then randomly selects a maximum of 250 profiles from this set, and uses those to construct the flame graph. As mentioned earlier, cloud Profiler displays profiling data by using Flame Graphs. Unlike trees and standard graphs, flame graphs use screen space efficiently by representing a large amount of information in a compact and readable format. Look at this example. We have a basic application with a main method, which calls foo1, which in turn calls bar. Then main calls foo2, which also calls bar. As you move through the graphic left to right, you can see how the information is collapsed. First, by removing arrows, then by creating frames, and finally by removing spaces and left-aligning. In the bottom view, you see the result as it appears in the Profiler. If you look at CPU time, then you can see that the main method takes a total of 9 seconds. Beneath the main bar, you can see how that CPU time was spent: some in main itself, but most in the calls to foo1 and foo2. And most of the foo time was spent in bar. So, if we could make bar faster, we could really save some time in main. Here we have a full example. When you hold the pointer over a frame, a tooltip opens and displays additional information, which includes: The function name The source file location And some metric consumption information If you click a frame, the graph is redrawn, which makes the call stack of the selected method more visible.

### Video - [Module summary](https://www.cloudskillsboost.google/course_templates/864/video/515474)

* [YouTube: Module summary](https://www.youtube.com/watch?v=KbDfEbiE7RY)

In this module, you learned to: Explain the features and benefits of Error Reporting, Cloud Trace, and Cloud Profiler. List and explain the functionalities of the Error Reporting, Cloud Trace, and Cloud Profiler components.

### Quiz - [Quiz - Investigating Application Performance Issues](https://www.cloudskillsboost.google/course_templates/864/quizzes/515475)

#### Quiz 1.

> [!important]
> **You deployed a new version of a service and all of a sudden significantly more instances
are being created in your Kubernetes cluster. Your service scales when average CPU
utilization is greater than 70%. What tool can help you investigate the problem?**
>
> * [ ] Error Reporting
> * [ ] Logging
> * [ ] Trace
> * [ ] Profiler

#### Quiz 2.

> [!important]
> **Which profile signifies the amount of memory allocated in the heap of the program?**
>
> * [ ] Heap
> * [ ] CPU time
> * [ ] Allocated heap
> * [ ] Wall time

#### Quiz 3.

> [!important]
> **You have an SLO that states that 90% of your http requests need to respond in less than
100 ms. You want a report that compares latency for your last two versions. What tool would you use to most easily create this report?**
>
> * [ ] Trace
> * [ ] Logging
> * [ ] Profiler
> * [ ] Error Reporting

## Optimizing the Costs for Google Cloud Observability

In our final module we discuss optimizing the costs for Google Cloud Observability. Specifically, you will learn to analyze resource utilization costs for operations related components within Google Cloud, and implement best practices for controlling the cost of operations within Google Cloud.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/864/video/515476)

* [YouTube: Module Overview](https://www.youtube.com/watch?v=uqBfoWojh9s)

In our final module, we discuss how to optimize the costs for Google Cloud Observability. Specifically, you learn to analyze resource utilization costs for operations-related components within Google Cloud and implement best practices for controlling the cost of operations within Google Cloud.

### Video - [Costs and pricing](https://www.cloudskillsboost.google/course_templates/864/video/515477)

* [YouTube: Costs and pricing](https://www.youtube.com/watch?v=udZadqR7Reo)

Let’s start with costs and pricing. Because Google Cloud Observability services are managed services, their cost is usage-based and not infrastructure-based. Although Cloud Profiler is offered at no cost, Cloud Logging, Cloud Monitoring, and Cloud Trace has associated costs. Error Reporting supports errors sent by using Cloud Logging and incurs costs associated with this service. Logging pricing is based on the volume of chargeable logs ingested. Logging incurs cost when using: Cloud Load Balancing logs, Custom logs, Error reporting costs (if your errors are ingested by Cloud Logging), The write operation in the Cloud Logging API, Logs stored beyond 30 days will incur a retention charge for non-required buckets. Refer to the documentation for updated pricing details. Cloud Monitoring prices are based on the: Volume of chargeable metrics ingested. Number of chargeable API calls. Execution of Cloud Monitoring uptime checks. Metrics ingested by using Google Cloud Managed Service for Prometheus. Example product usage that generates cost through metric volume and API calls includes using Cloud Monitoring custom metrics. AWS Metrics. The read operation in the Monitoring API, except from Google Cloud console. Trace prices are based on the number of spans ingested and eventually scanned. The free allotment was 2.5 million spans. Example product usage that generates cost through spans ingested includes adding instrumentation for your: Spans for App Engine apps outside of the default spans, Cloud Load Balancing, Custom apps. Many functions of Google Cloud Observability are free, including Using Cloud Profiler. Collecting and using the Cloud Audit Logs, Access Transparency logs, BigQuery Data Access logs and anything excluded from logs. Creating and using dashboards. Visualizing Google Cloud and Anthos metrics and log streams. App Engine standard trace spans. Uptime checks. Logs analytics when queries are running in Cloud Logging. After the free tier finishes, you will be charged for the various operation-related fees. For the latest pricing information, always check the Google Documentation. The networking logs, including VPC Flow logs, Firewall Rules Logging, and Cloud NAT, will cost you the standard log storage fees. However, if you store them in Cloud Logging, they won't cost you anything extra to generate. If you export the network telemetry logs to an external service, cost is incurred to generate logs. The cost is in addition to any destination or networking fees. Network Intelligence Center incurs costs for metrics overlaid on the network topology, Network Analyzer, and performance dashboard. Network Intelligence Center also incurs a cost for running connectivity tests and Firewall insights. Refer to the documentation for more information on pricing models for Firewall insights.

### Video - [Bill Estimation](https://www.cloudskillsboost.google/course_templates/864/video/515478)

* [YouTube: Bill Estimation](https://www.youtube.com/watch?v=RhHwGcFzW4M)

We covered some of the pure costs. Now let’s talk about bill estimation. When you try to estimate prices in Google Cloud, the page of choice should always be the Google Cloud Pricing Calculator. The Pricing Calculator is accurate, but it's only as accurate as the data that you provide it. If your operation services are already running on Google Cloud, start by pulling the prices of what you're spending on them. You can look for the requisite data in several places, let us explore these as we move forward in this section. You can also use the Cost Estimation API. It provides customer-specific estimates that include your discounts. For example, those negotiated as part of a contract and those based on committed usage. These cost estimates can help you make more informed business decisions. For more information, refer to the documentation linked in the reference material section. Start by going to your billing account Reports page. Set your date range, and then filter by Stock Keeping Unit (SKU). The SKUs you want are: Log volume Spans ingested Metric volume Monitoring API Requests Here's another view of the same page. In the chart, you can change the view to display the data. When you select Daily cumulative, the cost trend line effectively shows where you’re headed based on current spend trends. If you recently added logging, adding Daily cumulative might be a good way to estimate what your bill might do. Then, check your usages and costs. Note, if SKU usage is 0 for a metric, then it won’t appear in the list. You can do something similar to see how money was spent in the past month. In this case, the biggest item is log volume. This insight might raise the question: if money is spent on logging data, where exactly is that data coming from? To find the answer, check Metrics Explorer. Open Monitoring and then Metrics Explorer, and set the Metric to Global. Then, depending on what you want, select one of the following: Log bytes ingested provides Log bytes ingested in Cloud Logging. Monthly log bytes ingested provides a graph where each point represents the month-to-date sum of log bytes ingested in Cloud Logging. The monthly total is available on the last day of the month, when it also resets. Metric bytes ingested provides chargeable number of bytes of metric data ingested in Cloud Monitoring. Trace spans ingested: provides chargeable trace spans ingested in Cloud Trace. Monthly trace spans ingested: provides a graph where each point represents the month-to-date sum of trace spans ingested in Cloud Trace. It resets on the last day of the month; the monthly total is found on the last day of the month. In this example, the orange line represents the highest logging data. Thus, the gae_app is where all the logging data coming from Migrate for Compute Engine. You can find more information on Migrate for Compute Engine in the documentation. We learned that a metrics scope is used in Cloud Monitoring to monitor the resources you care about. The resources could be in a Google Cloud project, an AWS account, or multiple Google Cloud projects and AWS accounts. To view your Monitoring usage by metrics scope, go to Monitoring and then Settings. On the Summary tab, the Metrics Ingested table displays a summary of your metrics ingestion data by resource. This data includes the previous month total usage, the current month to-date usage, and projected usage for the current month. To get your project-level usage in detail, in the Metrics Ingested table, click View Bill. You’re taken to the Cloud Billing Reports page. After you know the projects where you’re spending on Cloud Monitoring, we want it to be easy to understand which metrics are driving these observability costs. We also want to provide insights on how to reduce spend on unused and noisy metrics. To get started, go to Monitoring > Metrics Diagnostics. This page provides many tools to understand metric ingestion and Monitoring API usage. One tool is a “Metrics” table where you can sort and filter metrics by volume or samples ingested, metric cardinality, metric name and domain, metric labels, project, error rate, and more. We recommend sorting metrics by “Metric Data Ingested” in descending order to identify exactly which metrics are primarily driving ingestion volume. Whether intended or not, our customers often find that only a few metrics or metric types drive most consumption. These are the ones that are ripe for cost reduction and optimization. To see your logs-based metrics usage, go to Logging > Logs-Based Metrics. Previous Month Usage represents the sum of bytes ingested in the logs-based metric in the previous calendar month. Usage (MTD) represents the sum of bytes ingested in the logs-based metric in the current calendar month. Clicking any of the column names lets you sort data in ascending or descending order. For example, if you want to review which metrics ingest the most data, sorting data is helpful.

### Video - [Cost Control Best Practices](https://www.cloudskillsboost.google/course_templates/864/video/515479)

* [YouTube: Cost Control Best Practices](https://www.youtube.com/watch?v=6X_XmzlGOlQ)

Finally, let’s discuss some billing best practices. We start with what we already discussed in this module, mostly in the last section. You learn to calculate costs, become aware of your operations-related spend and where it goes, and learn what the more expensive items are. If you exclude log entries, you don't pay for them. However, they will be gone once excluded, so be careful with the log entries you keep. Here are some of the common exclusions: Google has several different load balancers, and they all support various forms of monitoring and logging. Frequently, for Cloud Load Balancing that support Cloud Logging, all you need is a small percentage. Exclude 90% or more. VPC Flow Logs is another good example. Again, you can often exclude more than 95% of the entries and still get enough to monitor the VPC. Also, remember that your exclusion can filter on entry contents. Perhaps you only want to retain logs from sources with a CIDR outside your network. If you send your VPC Flow Logs to Cloud Logging, the charges for the generation of VPC Flow Logs instances are waived. Only logging charges apply. However, if you send them and then exclude your VPC Flow Logs from Cloud Logging, VPC Flow Logs charges apply. To lower the bill, they have to be deactivated completely. For more details, refer to the documentation provided at the end of this module. Another common exclusion is web applications and services that are logging HTTP 200 OK from requests. Frequently, OK messages don't provide much insight, and they can generate numerous entries. Log exports are free, but not the target resource. You can export logs yet exclude them from being ingested into Cloud Logging. You can retain the logs in Cloud Storage and BigQuery or use Pub/Sub to process the logs while excluding the logs from Cloud Logging, which can reduce costs. The fee associated with log exports are: Storage fees in Cloud Storage. Storage, streaming, and query fees in BigQuery. Pub/Sub message and networking egress fees. In fact, 2 TiBs of data access log data stored in Logging would cost about $1,000. The same 2 TiBs stored in a regional, standard class bucket would cost about $40. With archival, it would be much cheaper. You can reduce log volumes by not sending the additional logs generated by the Ops Agent to Cloud Logging. For example, you might reduce log volumes by choosing not to add the Ops Agent to VMs in your development or other nonessential environments to Cloud Logging. Your virtual machines continue to report the standard logs to Cloud Logging, but don't report logs from third-party apps nor the syslog. How you use labels on Monitoring custom metrics can affect the volume of time series that are generated. Given a custom metric with two labels (cost_center and env values), you can calculate the maximum number of time series by multiplying the cardinality of both labels. If there are 11 cost_center values and 5 env values, that means that up to 55 time series can be generated. Adding additional metric labels can add significant metric volume and, therefore, increase the cost. Where possible, limit the number of custom metric labels. Select labels thoughtfully to avoid label values with high cardinality. For example, using user_id as a label results in at least one time series for each user. If you have significant traffic, the number could be very large. Metrics sent from the Ops Agent are chargeable metrics. Although installing them is undoubtedly a best practice, there will be exceptions. Weigh the advantages and disadvantages, and remember that both the agents are customizable through configuration files. If you don't need the detailed system metrics or metrics from the third-party apps for certain VMs, reduce the volume by not sending these metrics. You can also reduce the metric volumes by reducing the number of VMs using the Ops Agent. For example, you can reduce metric volumes by choosing not to add Google Cloud projects in your development or other nonessential environments to Cloud Monitoring. Also, you can choose not to include the monitoring agent in VMs in development or other nonessential environments. Custom metrics can increase spend. Newer metrics created using OpenTelemetry support sampling to help reduce volume. When you instrument more apps to send metrics, more custom monitoring metrics are generated. If you want to reduce metric volumes, you can reduce the number of custom monitoring metrics that your apps send. Open source Prometheus documentation rarely recommends filtering metric volume, which is reasonable when costs are bounded by machine costs. But when paying a managed-service provider on a unit basis, sending unlimited data can cause high bills. To reduce the number of metrics, you can do the following: Modify your scrape configs to scrape fewer targets. Filter exported metrics when you using managed collection or self-deployed collection. Managed Service for Prometheus charges on a per-sample basis. Here are some of the ways in which you can reduce the number of samples ingested: Increasing the length of the sampling period. For example: Changing a 10-second sampling period to 30 seconds can reduce your sample volume by 66%, without much loss of information. Setting the scraping interval on a per-job or a per-target basis. For managed collection, you set the scrape interval in the PodMonitoring resource by using the interval field. If you’re configuring the service by using self-deployed collection, for example with kube-prometheus, prometheus-operator, or by manually deploying the image, then you can reduce your samples sent to Managed Service for Prometheus by aggregating high-cardinality metrics locally. Use recording rules, flags, and environment variables to aggregate data to Monarch. Refer to the documentation for more information. Trace charges are based on the number of trace spans ingested and scanned. Use sampling to reduce the volume of traces ingested. Sampling is a crucial part of a tracing system, because it provides insight into the breakdown of latency caused by app components, such as RPC calls. Sampling is not only a best practice for using Cloud Trace: you might reduce your span volume for cost-reduction reasons too. For example, with a popular web application with 5000 queries/second, you might gain enough insight from sampling 5% of your app traffic instead of 20%. A smaller sample reduces the number of spans ingested into Trace to one-fourth. The OpenTelemetry lets you specify a sampling rate. You can enforce span quotas with the API-specific quota page in the Google Cloud console. Setting a quota that is lower than the default product quota means that you guarantee that your project won't go over the specific quota limit. Quotas are a way to ensure that your costs are expected. Your app might be called by another app. If your app reports spans, the number of spans reported by your app might depend on the incoming traffic that you receive from the third-party app. For example, if you have a frontend microservice that calls a checkout microservice, and both are instrumented with OpenTelemetry, the sampling rate for the traffic is at least as high as the frontend sampling rate. Understanding how instrumented apps interact lets you assess the effect of the number of spans ingested.

### Video - [Module summary](https://www.cloudskillsboost.google/course_templates/864/video/515480)

* [YouTube: Module summary](https://www.youtube.com/watch?v=dg9gKlBvP2Y)

In this module, you learned how to: Analyze resource utilization costs for operations-related components within Google Cloud. And implement best practices for controlling the cost of operations within Google Cloud.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/864/quizzes/515481)

#### Quiz 1.

> [!important]
> **What is one of the best practices to reduce monitoring costs?**
>
> * [ ] Increasing custom metric usage.
> * [ ] Reduce the number of time series
> * [ ] Reduce Ops Agents usage
> * [ ] Increase label usage

#### Quiz 2.

> [!important]
> **What is Cloud Logging pricing based on?**
>
> * [ ] Read operation in the Cloud Logging API
> * [ ] Volume of chargeable logs
> * [ ] Profiler Usage
> * [ ] Number of uptime checks

## Course Summary

We will summarize the topics covered in this couse. 

### Video - [Course Summary](https://www.cloudskillsboost.google/course_templates/864/video/515482)

* [YouTube: Course Summary](https://www.youtube.com/watch?v=AvKv-eW0rC0)

This brings us to the end of the “Observability in Google Cloud” course! Let’s do a quick recap of what was covered. In this course we learned how to: Install and manage Ops Agent to collect logs for Compute Engine Use Google Cloud Managed Service for Prometheus Analyze VPC Flow logs and Firewall Rules logs Analyze resource utilization cost for monitoring related components within Google Cloud. Thank you for taking this course. I hope you feel more comfortable with using the different operations suite products that we covered. Now it’s your turn. Go ahead and apply what we have learned by monitoring your applications in Google Cloud. See you next time!

## Course Resources

Student PDF links to all modules

### Document - [Course Resources](https://www.cloudskillsboost.google/course_templates/864/documents/515483)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
