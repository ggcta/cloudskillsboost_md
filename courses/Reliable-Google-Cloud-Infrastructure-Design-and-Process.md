---
id: 41
name: 'Reliable Google Cloud Infrastructure: Design and Process'
type: Course
url: https://www.cloudskillsboost.google/course_templates/41
date: 2025-04-03
datePublished: 2025-01-10
topics:
- Cloud Networking
- Monitoring
- Observability
---

# [Reliable Google Cloud Infrastructure: Design and Process](https://www.cloudskillsboost.google/course_templates/41)

**Description:**

This course equips students to build highly reliable and efficient solutions on Google Cloud using proven design patterns. It is a continuation of the Architecting with Google Compute Engine or Architecting with Google Kubernetes Engine courses and assumes hands-on experience with the technologies covered in either of those courses. Through a combination of presentations, design activities, and hands-on labs, participants learn to define and balance business and technical requirements to design Google Cloud deployments that are highly reliable, highly available, secure, and cost-effective.

**Objectives:**

- Apply a tool set of questions, techniques and design considerations
- Define application requirements and express them objectively as KPIs, SLO's and SLI's
- Decompose application requirements to find the right microservice boundaries
- Leverage Google Cloud developer tools to set up modern, automated deployment pipelines
- Choose the appropriate Google Cloud Storage services based on application requirements
- Discuss Google Cloud network architectures, including hybrid architectures.
- Implement reliable, scalable, resilient applications balancing key performance metrics with cost
- Choose the right Google Cloud deployment services for your applications
- Secure cloud applications, data and infrastructure
- Monitor service level objectives and costs using Google Cloud Observability.

## Introduction

Welcome to the Reliable Google Cloud Infrastructure: Design and Process course. Learn about the course structure and it's content.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/41/video/520352)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=TNBb-E8Rz5w)

Philipp: Hello, I am Philipp Maier, a Course Developer at Google. Stephanie: And I'm Stephanie Wong, a Developer Advocate at Google. We want to welcome you to the Reliable Cloud Infrastructure: Design and Process course. Philipp: This course is about architecting, design, and process. A Cloud Architect's job is to determine which Cloud services to use in order to most effectively implement the applications and services they are building. Stephanie: This is not an easy job. Philipp: That's right. Many services seem interchangeable. In many cases, multiple different services would work for the same use case. Stephanie: The intent of this course is to simulate the process you can use to design a system that will run on Google Cloud. This course is not about implementing specific Cloud features. It's about architecture, design, and process. Philipp: We like to joke that the job of an architect is to draw rectangles and point arrows at them. Stephanie: Which, to a certain extent, is true. That is an important step in designing complex systems. In this course, you will focus on that design and planning. Specifically, you will work on architecting and designing a case study in this course. The starting point for any software development is to figure out what the software is supposed to do, who your users are, and why this is important. You will begin with this requirements gathering phase. Philipp: Once you understand your software's requirements and your users, you can start laying out the overall design. In software, this is a process of decomposition, breaking the big thing, your program, into smaller, manageable units that you can start programming. In a modern Cloud based system, it is considered a best practice to break your application into microservices. Stephanie: Microservices refers to an architectural style for developing applications. Microservices allow a large application to be decomposed into independent constituent parts with each part having its own area of responsibility. To serve a single user or API requests, a microservice's base application can call many internal microservices to compose its response. Philipp: The architecture in the course will be microservice based. This has a significant effect on the agility of the application and aspects such as developing speed, deployment, and monitoring. We will consider the advantages and disadvantages of this architectural style. We'll also help you choose the best storage and deployment services using objective criteria. Stephanie: Choosing the right ones can be complicated. Do you want a relational database, a NoSQL database, or a data warehouse? You also need to consider your compute platform. Do you want to deploy your apps to virtual machines, a Kubernetes cluster, or an automated platform, like App Engine? Philipp: You will learn what the factors and how to choose the right services for your various microservices. Stephanie: Google provides many services that you can use to make your applications reliable. Availability, durability, cost, and disaster recovery are all important considerations when designing systems. If you understand your requirements, you can choose the right Google Cloud services to meet your application's goals for reliability, while optimizing costs. Philipp: Now, there's a saying, "Security is not icing on the cake. It is baked into the cake." Before implementing a system on Google Cloud, you should carefully consider its security requirements and use the appropriate security services. Security in computer systems is implemented in layers. Google Cloud handles some things for you. Stephanie: For example, Google secures the physical hardware that Google Cloud is running on. Google also provides many controls to help secure your applications and data. Philipp: Security is a shared responsibility, though. The way you configure your networks, storage services, and machines will determine how your application is secured. When you design your case study, you will consider its security requirements and bake them into the design. Stephanie: At the end, you will monitor your app to see whether you're meeting your service objectives. In Google Cloud, there are many services for monitoring your applications. Philipp: These include dashboards, logs, error reporting, and tracing. You start by defining your application requirements. As you develop your solution, you can use the monitoring tools to determine how successful you are at meeting your application goals. Stephanie: The Reliable Cloud Infrastructure: Design and Process course is a part of the Cloud Infrastructure learning path. This path is designed for IT professionals who are responsible for implementing, deploying, migrating, and maintaining applications in the Cloud. Philipp: The prerequisite for this course is either the Architecting with Google Compute Engine or the Architecting with Google Kubernetes Engine course. Stephanie: In other words, this course is not intended to be your first exposure to Google Cloud. Philipp: Now, the course consists of lecture, design activities, and hands-on labs. Stephanie: You should spend a significant amount of time on the design and architecture assignments we will give you. As with many situations, there will be no one right answer and typically, different people come up with different solutions. Architecting systems is a matter of weighing the pros and cons of various solutions and trying to find the best solution given your requirements and constraints. Philipp: Now, the more effort you put into these design activities, the more you will learn from this course. Stephanie: Besides this introduction module, there are nine modules in this course. Philipp: First, you will analyze and design a case study application using a microservice architecture. Stephanie: Then, we will cover Google Cloud tools for DevOps and automation, and you will choose the appropriate storage services for your case study. Philipp: After that, you will learn about network design for Cloud and Hybrid applications, and learn how to choose the appropriate deployment service. Stephanie: We will finish by designing for reliability and security and by monitoring your applications.

### Document - [Workbook](https://www.cloudskillsboost.google/course_templates/41/documents/520353)

### Video - [Activity Intro: Defining your case study](https://www.cloudskillsboost.google/course_templates/41/video/520354)

- [YouTube: Activity Intro: Defining your case study](https://www.youtube.com/watch?v=HA414QD3qFw)

Person: Slides are great for explaining concepts, but let's start working on the design activity workbook of this course. You can find the full workbook in the resources section of this course. In the first activity, you need to come up with a case study idea. Whatever your idea is, you don't want it to be too trivial. It should be a complex enough idea that designing the solution will be challenging. For example, you might want to design an online banking portal, a ride sharing application, or an online shopping site. These are all complex applications with many interesting design possibilities. Now, all of these examples have been designed before. Feel free to use your imagination to come up with something that hasn't been done before. You can use something that you might want to develop as part of your work. It's up to you. As this course is recorded, we won't be able to provide you with specific feedback on your design. Instead, we will provide you with a sample solution for an online travel portal application for each design activity. Now, part A of this activity is to come up with an interesting case study. For part B, write a short description and list the main features and roles of some typical users of your application.

### Video - [Activity Review: Defining your case study](https://www.cloudskillsboost.google/course_templates/41/video/520355)

- [YouTube: Activity Review: Defining your case study](https://www.youtube.com/watch?v=rNu1gUDnkuY)

person: In this first activity, you were asked to come up with a case study, write a short description, list some of its main features, and list some of the roles users would play while using the application. To give you an idea of what to come up with, we'll go over our sample solution for an online travel portal application. Let's call it ClickTravel. ClickTravel is a global travel agency that wants to build a scalable e-commerce platform to serve a global customer base. Let's go over the main features. Travelers can search and book travel like hotels, flights, trains and cars. Pricing will be individualized based on the customer's preferences and demand. There should be a strong social media integration with reviews, posts, and analytics. Suppliers like airlines and hotels can upload their inventory. As for roles of typical users, we envision a customer, a traveler, an inventory supplier, and a manager. I'm sure we could come up with more features and roles, but this gives us a good feel for the application we would like to build. Now we can get started working on more detailed requirements and a design.

## Defining Services

In this module, you will learn to describe users of a system in terms of the roles and personas they take. You will learn how to measure success using Key performance indicators (KPIs) and you will examine service level objectives (SLOs), service level indicators (SLIs), and service level agreements (SLAs).

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/41/video/520356)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=pgG6kq1-6ww)

Philipp: In this module, we focus on defining services. A new development begins with the planning and design phases. These require information gathering, starting with business requirements. Once the requirements are defined, it is important to measure that they're providing business value. In this module, we will look at gathering requirements and then techniques for measuring the impact of these solutions. Let's take a closer look at what we will cover. In this module, you will learn to describe users of a system in terms of the roles and personas they take. These users will then help define and refine the qualitative requirements, which will be captured in the form of user stories. These provide a context for the architectural design and subsequent technical decisions you will make as a Cloud architect. Example of business requirements include accelerating the pace of software development, reducing capital expenditure, and reducing time to recover incidents. The technical requirements of a systems are both the functional and non-functional features required. To help identify the most important requirements and measure impact, you will learn how to measure success using key performance indicators, or KPIs. We will also discuss the importance of using SMART criteria when defining KPIs. We'll finish by considering the most suitable service level objectives, or SLOs, and service level indicators, or SLIs, and from these, service level agreements, or SLAs.

### Video - [Requirements, Analysis, and Design](https://www.cloudskillsboost.google/course_templates/41/video/520357)

- [YouTube: Requirements, Analysis, and Design](https://www.youtube.com/watch?v=BFPG2d-TNC0)

Person: Let's start talking about requirements, analysis and design. Useful questions to ask a cloud architect to help build the requirements are: Who, what, why, when and and how? The who is about determining not only the user of the system, but also the developers and stakeholders. The aim is to build a full picture of who the system will affect both directly and indirectly. The what is both simple and difficult. We need to establish the main areas of functionality required, but in a clear, unambiguous manner. Why the system is needed is a really important question. What is the problem the proposed system aims to address or solve? Without a clear understanding of the need it is likely that extra requirements will be added. The why will also potentially help in defining KPI's and and SLO's and SLA's. When helps determine a realistic timeline and can help contain the scope. How helps to determine a lot of the non functional requirements. These could be for instance, how many users the system needs to support concurrently? What is the average payload size of service requests? Are there latency requirements, et cetera. They could be that the users will be located across the world, or in a particular region only. All of these requirements are vital to capture, because they impact the potential solution you, as the cloud architect, will provide. In the previous design activity you defined user roles for your application. Roles represent the goal of a user at some point, and they enable the analysis of a requirement in a particular context. It is important to note that a role is not necessary a person. It is an actor on the system and could be another system such as a microservice client that is accessing another microservice. The role should describe the user's objective when using the system. For example the role of a shopper on an e-commerce application clearly defines what the user wants to do. There are many ways to determine the roles for the requirement you're working on. One process that works particularly well is, first, brainstorm an initial set of roles. Write as many roles as you can think of with each role being a single user. Now organize this initial set. Here you can identify overlapping roles and related roles and group these together. with the set of roles now grouped, consolidate the roles. The aim here is to consolidate and condense the roles to remove duplication. Finally refine the roles, including internal and external roles, and the different users patterns. Here extra information can be provided, such as the user's level of expertise in the domain, or the frequency of use of the proposed software. Following a simple process like this provides structure and brings focus to the task. Identifying user roles is a useful technique as part of the requirements gathering process. An additional technique, in particular for more important roles, can be to create a persona for the role. A persona is an imaginary representation of a user role. The aim of the persona is to help the architect and developers think about the characteristics of users by personalizing them. Often a role has multiple personas. Consider the example of requirements for a banking application. We can think in terms of users of the system, and many requirements can be gathered this way. Using personas can provide further insights. For example Jocelyn is a person who is a busy working mom. Jocelyn wants to save time and money, as well as perform the standard banking operations online and receive benefits such as cash back. Using a persona helps build a fuller picture of the requirements. For instance Jocelyn's wanting to save time indicates that the task to be performed should possibly be automated, which affects latency and service design. In this example when a question arises from the architect, or maybe a developer, they can often better answer that question by thinking, what would Jocelyn want here? Now user stories describes one thing a user wants the system to do. They are written in a structured way, typically using the form, "As a type of user I want to do something so that I can get some benefit." Another commonly used form is, "Given some context when I do something then this should happen." So when writing stories give each story a title that describes it's purpose as a starting point. Follow this with a concise one sentence description of the story that follows one of the forms just described. This form describes the user role, what they want to do and why they want to do it. As an example consider a banking system and a story to determine the available balance of a bank account. The title of the story could be "Balance Inquiry." Then, following the template we describe the story, "As an account holder, I want to check my available balance at any time of day so I am sure not to overdraw my account." This explains the role, what they want to do and why they want to do it. User stories provide a clear and simple way of agreeing to requirements with a customer/end user. The INVEST criteria can be used to evaluate user stories. Let me go through each letter of these criteria. Independent, a story should be independent to prevent problems with prioritization and planning. Negotiable, they are not written contracts but are used to stimulate discussion between customer and developers until there is a clear agreement, they add collaboration. Valuable, stories should provide value to users. Think about outcomes and impacts, not outputs and deliverables. Estimatable, the story must be estimatable. If it is not, it often indicates missing details or the story is too large. Small, good stories should be small. This helps keeps scope small, and therefore less ambiguous, and supports fast feedback from users. Testable, stories must be testable so that developers can verify that the story has been implemented correctly and validate when the requirement has been met/is done.

### Video - [Activity Intro: Analyzing your case study](https://www.cloudskillsboost.google/course_templates/41/video/520358)

- [YouTube: Activity Intro: Analyzing your case study](https://www.youtube.com/watch?v=XL0h2xKyFFM)

Person: In this design activity, you're going to work on Activities 2a and 2b off the Design Workbook. In the first activity, you defined roles. For an online store, examples of roles might be account holder, shopper, customer, administrator and seller. Roles are played by people, and different people playing the same role might be significantly different. In Activity 2a, you will write some user personas. Personas are stories that describe typical people playing some role while using your system. It's important to understand your users when designing a system, so it's important to write some personas. Now, in a real project, you should find some users and interview them. For this course, feel free to use your imagination. In Activity 2b, you will write some user stories. User stories are short, one-sentence descriptions of your applications' features. In the first activity, you listed some features. Now, turn those features into user stories. Write your user stories in the form, "As a," and then you fill in the role, "I want to," and then tell me your goal, "so that," and then you say why this is important to you. For example, an online store might have a feature to search your products. The user story might be, "As a shopper, I want to be able to quickly search for products by name, keyword or category, so I can quickly find information about products I want to purchase." Here's an example persona. Jocelyn is a busy working mom who wants to access MegaCorp bank to check her account balances and to make sure that there are enough funds to pay for her kids' music and sports lessons. She also uses the website to automate payments with bills and see her credit account balances. Jocelyn wants to save time and money. She wants a credit card that gives her cash back. Here's an example, user story, for a feature: balance inquiry. As a checking-account holder, I want to check my available balance at any time of day so that I'm sure not to overdraw my account.

### Video - [Activity Review: Analyzing your case study](https://www.cloudskillsboost.google/course_templates/41/video/520359)

- [YouTube: Activity Review: Analyzing your case study](https://www.youtube.com/watch?v=xoKBNMQSE8A)

In this second activity, you are asked to write personas and user stories for your case study. Here are a couple of examples of personas for our online travel portal. Karen is a busy businesswoman who likes to take luxury weekend breaks, often booked at the last minute. A typical booking comprises of a hotel and flight. Recommendations play a major role in the choice Karen makes, as does customer feedback. Karen likes to perform all operations from her phone. Andrew is a student who likes to travel home to visit parents and also takes vacations twice yearly. His primary concern is cost, and he will always book the lowest price travel regardless of convenience. Andrew has no loyalty, and will use whichever retailer can provide the best deal. Here are a couple of examples of user stories for our online travel portal. For the Search for Flight and Hotel feature, I could write, as a traveler, I want to search for a flight-hotel combination to a destination on dates of my choice so that I can find the best price. For the Supply Hotel Inventory feature, I could write, as a hotel operator, I want to bulk supply hotel inventory so that ClickTravel can sell it on my behalf. For the Analyze Sales Performance feature, I could write, as a ClickTravel manager, I want to analyze the sales performance data of all of our suppliers so that I can identify poor performers and help them improve.

### Video - [KPIs and SLIs](https://www.cloudskillsboost.google/course_templates/41/video/520360)

- [YouTube: KPIs and SLIs](https://www.youtube.com/watch?v=dSvyAekVUdE)

PERSON: With a set of requirements in place, we will now move on to consider how to measure whether the technical and business requirements have been met. To manage a service well, it is important to understand which behaviors matter, and how to measure and evaluate these behaviors. These must always be considered in the context of the constraints, which are usually time, funding and people. Then we consider what can be achieved. The type of system being evaluated determines the data that can be measured. For example, for user-facing systems, was a request responded to, which refers to availability, how long did it take to respond, which refers to latency, how many requests can be handled, which refers to throughput? For data storage systems, how long does it take to read and write data? That's latency. Is the data there when we need it? That's availability. If there is a failure, do we lose any data? That's durability. The key to all of these items is that the questions can be answered with data gathered from the services. Business decision makers want to measure the value of projects. This enables them to better support the most valuable projects and not waste resources on those that are not beneficial. A common way to measure success is to use KPIs. KPIs can be categorized as business KPIs and technical KPIs. Business KPIs are a formal way of measuring what the business values, such as ROI, in relation to a project or service. Others include earnings before interest and taxes, or impact on users, such as customer churn, or maybe employee turnover. Technical or software KPIs can consider aspects such as how effective the software is through page views, user registration and number of checkouts. These KPIs should also be closely aligned with business objectives. As an architect, it is important that you understand how the business measures success of the systems that you design. Now, a KPI is not the same thing as a goal or objective. The goal is outcome or result you want to achieve. The KPI is a metric that indicates whether you are on track to achieve the goal. To be the most effective, KPIs need an accompanying goal. This should be the starting point in defining KPIs. Then for each goal, define the KPIs that will allow you to monitor and measure progress. For each KPI, define targets for what success looks like. Monitoring KPIs against goals is important to achieving success and allows readjustment based on feedback. As an example, a goal may be to increase turnover for an online store, and an associated KPI may be the percentage of conversions on the website. For KPIs to be effective, they must be specific rather than general. For example, user friendly is not specific. It's very subjective. Section 508 accessible is much more specific. Measurable is vital because monitoring the KPIs indicates whether you're moving toward or away from your goal. Being achievable is also important. For example, expecting 100 percent conversions on a website is not achievable. Relevant is absolutely vital. Without a relevant KPI, the goal probably will not be met. In our example of increasing turnover, if we're improving the conversion rate, a subsequent increase in turnover should be achievable assuming a similar number of users. Time-bound helps with measuring the KPI. Some KPIs are more sensitive to time. For example, is availability per day, per month or per year? So to summarize, KPIs are used to measure success or progress toward a goal. Let's introduce service level terminology. To provide a given level of service to customers, it is important to define service level indicators, or SLIs, objectives, or SLOs, and agreements, or SLAs. These are measurements that describe basic properties of the metrics to measure, the values those metrics should read and how to react if the metrics cannot be met. Service level indicator is a quantitative measure of some aspect of the level of service being provided. Examples include throughput, latency and error rate. Service level objective is an agreed-upon target or range of values for a service level that is measured by an SLI. It is normally stated in the form of SLI is smaller than equal to target or lower bound smaller and equal to SLI, smaller or equal to upper bound. An example of an SLO is that at average latency of HTTP requests for our service should be less than 100 milliseconds. Service level agreement is an agreement between a service provider and a consumer. They define the responsibilities for delivering a service, and consequences when these responsibilities are not met. The SLA is a more restrictive version of the SLO. We want to architect a solution and maintain an agreed SLO so that we provide ourselves spare capacity against the SLA. Understanding what users want from a service will help inform the selection of indicators. The indicators must be measurable. For example, fast response time is not measurable, whereas HTTP GET requests that respond within 400 milliseconds aggregated per minute is clearly measurable. Similarly, highly available is not measurable, but percentage of successful requests over all requests aggregated per minute is measurable. Not only must indicators be measurable, but the way they are aggregated needs careful consideration. For example, consider requests per second to a service. How is the value calculated? By measurements obtained once per second, or by averaging requests over a minute? The once per second measurement may hide high request rates that occur in bursts of a few seconds. For example, consider a service that receives 1,000 requests per second on even-numbered seconds and zero requests on odd-numbered seconds. The average request per second could be reported over a minute as 500. However, the reality is that the load at times is twice as large as the average. Similar averages can mask user experience when used for metrics like latency. It can mask the requests that take a lot longer to respond than the average. It is better to use percentiles for such metrics where a high order percentile such as 99 percent shows worst case values while the 50th percentile will indicate a typical case.

### Video - [SLOs and SLAs](https://www.cloudskillsboost.google/course_templates/41/video/520361)

- [YouTube: SLOs and SLAs](https://www.youtube.com/watch?v=-xQLmWtWybA)

The relevancy of SLOs is vital. You want objectives that help or improve the user experience. It is easy to define a SLOs based around what is easy to measure rather than what is useful. For clarity, SLOs should specify how they are measured and the conditions when they are valid. Consider availability as measured with an uptime check over ten seconds aggregated per minute. It is unrealistic as well as undesirable to have SLOs with a 100% target. Such a target results in expensive, overly conservative solutions, that are still unlikely to reach the SLO. It is better to track the rate at which SLOs are missed and work to improve this. In many cases 99% may be good enough availability and be far easier to achieve as well as engineer. It is also highly likely to be much more cost-effective to run. The use case needs to be considered also. For example, if a HTTP service for photo uploads requires 99% of uploads to be complete within 100 milliseconds aggregated per minute, this may be unrealistic or overkill if the majority of users are using mobile phones. In such a case, an SLO of 80% is much more achievable and good enough. It is often okay to specify multiple SLOs. Consider the following, 99% of HTTP get calls will complete in less than 100 milliseconds. This is a valid SLO, but it may be the case that the shape of the performance curve is important. In this case, the SLO could be written as follows. 90% of HTTP get calls will complete in less than 50 milliseconds, 99% of HTTP get calls will complete in less than 100 milliseconds. And 99.9% of HTTP get calls will complete and less than 500 milliseconds. Selecting SLOs has both product and business implications. Often trade-offs need to be made based on constraints such as staff, time to market and funding. As the slide states, the aim is to keep users happy, not to have an SLO that requires heroic efforts to maintain. Let me give you some tips on selecting SLOs. Do not make them too high. It is better to have lower SLOs to begin with and tighten them over time as you learn about the system, instead of defining those that are unattainable and require a significant effort and cost try and achieve. Keep them simple. More complex SLIs can obscure important changes in performance. Avoid absolute values. To have a SLO that states 100% availability is unrealistic. Such an SLO increases the time to build, complexity, and cost to operate. And in most cases is highly unlikely to be required. Minimize SLOs. A common mistake is to have too many SLOs. The recommendation is to have just enough SLOs to give coverage of the key system attributes. In summary, good SLOs should reflect what the users care about. They work as a forcing function for development teams. A poor SLO will result in a significant amount of wasted work if it is too ambitious or a poor product if it is to relaxed. An SLA is a business contract between the service provider and the customer. A penalty will apply if the service provider does not maintain the levels agreed on. Not every service has an SLA, but all services should have a SLOs. As with SLO, it is better to be conservative with SLAs because it is too difficult to change or remove SLAs that offer little value or cause a large amount of work. In addition, because they can have a financial implication through compensation to the customer, setting them too high can result in unnecessary compensation being paid. To provide protection and some level of safety, an SLA should have a threshold that is lower than the SLO. This should always be the case. Let's consider an example of a service, An SLI, SLO and SLAs for the service. The service is an HTTP endpoint accessed using HTTP get. The SLI is the end-to-end latency of successful HTTP responses. That is HTTP-200. These are averaged over one minute. The SLO has been agreed that the latency of 99% of the responses must be less than or equal to 200 milliseconds. The SLA is set that the user is compensated if the 99th percentile latency exceeds 300 milliseconds. The SLA has clearly built a buffer over the SLO, which means that even if the SLO is exceeded there is some capacity before the SLA is broken. This is the wanted position in the relationship between SLO and SLA.

### Video - [Activity Intro: Defining SLIs and SLOs](https://www.cloudskillsboost.google/course_templates/41/video/520362)

- [YouTube: Activity Intro: Defining SLIs and SLOs](https://www.youtube.com/watch?v=iss_qwAYNG8)

Philipp: In this design activity, you will define SLIs and SLOs for your case study. Let's say you wanted to write an SLI and SLO for an online shopping application related to availability. The SLI is what you want to measure. For example, you might want to measure the fraction of successful versus unsuccessful HTTP responses from an API endpoint aggregated per day. The SLO is the target you're trying to achieve. For your online store's search for product service that might be 99.95%. Here are a couple of more examples for an imaginary online bank. The SLI defines what you want to measure, like the fraction of 200 versus 500 HTTP responses from an API endpoint or the time to last byte GET requests measured every ten seconds aggregated per minute. The SLO defines the target you want to achieve, like 99.95% availability or that 95% of requests will complete in under 300 milliseconds.

### Video - [Activity Review: Defining SLIs and SLOs](https://www.cloudskillsboost.google/course_templates/41/video/520363)

- [YouTube: Activity Review: Defining SLIs and SLOs](https://www.youtube.com/watch?v=1HVTY1D4IWE)

Philipp: In this third activity, you were asked to write SLIs and SLOs for your case study. Here are some example SLOs and SLIs for our travel portal application. Notice that the SLI describes what we're going to measure and how. For example, fraction of 200 versus 500 HTTP responses from API endpoint measured per month. This example is a way of measuring availability. The SLO represents the goal we're trying to achieve for a given SLI. For example, available 99.95% of the time. Feel free to pause the video to read through the other SLOs and SLIs for each user story.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/41/quizzes/520364)

#### Quiz 1.

> [!important]
> **Which most accurately describes a user story?**
>
> - [ ] It is a short description of a typical person using the system.
> - [ ] It is a narrative that describes the sequence of steps a typical user would perform to accomplish some task or goal when using the system.
> - [ ] It is a short description of a feature written from the user's point of view.
> - [ ] It is a requirement of the system you are developing.

#### Quiz 2.

> [!important]
> **Using SMART criteria, which below would be the least effective KPI?**
>
> - [ ] User sign ups per month
> - [ ] User experience design
> - [ ] Clicks per session
> - [ ] Page views per hour

#### Quiz 3.

> [!important]
> **Which best describes an SLO?**
>
> - [ ] It is a contract with end users that guarantees service quality
> - [ ] It is a measurable, time bound key performance indicator for your application.
> - [ ] It is a target measure you want your service to achieve.
> - [ ] It is a short, measurable description of an application feature.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/41/video/520365)

- [YouTube: Module Review](https://www.youtube.com/watch?v=8UxEkwvtxFc)

In this module, we learned about qualitative and quantitative requirements. Qualitative requirements are things that the user cares about, like features. We can express qualitative requirements in the form of user stories. In order to understand our users better, we should write personas. Quantitative requirements are things we can measure. We can express them as key performance indicators, or KPIs. KPIs and software are things like user sign-ups, clicks per session, completed purchases, or customer retention. We can also express quantitative requirements as SLOs and SLIs. These are lower level metrics, things like latency, availability, or response time.

## Microservice Design and Architecture

In this module, we introduce application architecture and microservice design.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/41/video/520366)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=YOVQO-5dP3Y)

Priyanka: Hi, I'm Priyanka Vergadia, a developer advocate for Google Cloud. In this module, we introduce Application Architecture, and Microservice Design. Specifically, you will learn about microservice architectures and how to decompose monolithic applications into microservices. The benefits of a microservice architecture for Cloud-native applications is discussed and contrasted with a monolith. The challenges of decomposing applications into microservices with clear boundaries to support independently deployable units are investigated. You will learn how to architect for some of the major technical challenges of microservices architecture, such as state management, reliability, and scalability. Once a Cloud-native microservice architecture has been chosen, the best practices for development and deployment are introduced based around the widely recognized 12-factor best practices. At the end of the module, we will go over a core component of the microservice architecture, which is the design of consistent, loosely-coupled service interfaces.

### Video - [Microservices](https://www.cloudskillsboost.google/course_templates/41/video/520367)

- [YouTube: Microservices](https://www.youtube.com/watch?v=PO1RhuwxavM)

Let's begin by looking at microservices in more detail. Microservices divide a large program into a number of smaller, independent services as shown on the right, unlike a monolithic application which implements all its features in a single code base with a database for all data as shown on the left. Microservices are the current industry trend, however, it's important to ensure that there is a good reason to select this architecture. The primary reason is to enable teams to work independently and deliver through to production at their own cadence. This supports scaling the organization: adding more teams increases speed. There is also the additional benefit of being able to scale the microservices independently based on their requirements. Architecturally, an application designed as a monolith or around microservices should be composed of modular components with clearly-defined boundaries. With a monolith, all the components are packaged at deployment time and deployed together. With microservices, the individual components are deployable. Google Cloud provides several compute services that facilitate deploying microservices. These include App Engine, Cloud Run, GKE, and Cloud Run functions. Each offers different levels of granularity and control, and will be discussed later in this course. To achieve independence on services, each service should have its own datastore. This lets the best datastore solution for that service be selected and also keeps the services independent. We do not want to introduce coupling between services through a datastore. A properly designed microservice architecture can help us achieve the following goals. Define strong contracts between the various microservices. Allow for independent deployment cycles, including rollback. Facilitate concurrent A/B release testing on subsystems. Minimize test automation and quality assurance overhead. Improve clarity of logging and monitoring. Provide fine-grained cost accounting. And increase overall application scalability and reliability through scaling smaller units. However, the advantages must be balanced with the challenges this architectural style introduces. Some of these challenges include: It can be difficult to define clear boundaries between services to support independent development and deployment. Increased complexity of infrastructure, with distributed services having more points of failure. The increased latency introduced by network services and the need to build in resilience to handle possible failures and delays. Due to the networking involved, there is a need to provide security for service-to-service communication, which increases complexity of infrastructure. Strong requirement to manage and version service interfaces. With independent deployable services, the need to maintain backward compatibility increases. Now, decomposing applications into microservices is one of the biggest technical challenges of application design. Here, techniques like domain-driven design are extremely useful in identifying logical functional groupings. The first step is to decompose the application by feature or by function groupings to minimize dependencies. Consider for example an online retail application. Logical functional groupings could be product management, reviews, accounts, and orders. These groupings then form many applications which expose an API. Each of these many applications will be implemented by potentially multiple microservices internally. Internally, these microservices are then organized by architectural layer, and each should be independently deployable and scalable. Any analysis will also identify shared services, such as authentication, which are then isolated and deployed separately from the many applications. When you are designing microservices, services that do not maintain state but obtain their state from the environment or stateless services are easier to manage. That is, they are easy to scale, to administer, and to migrate to new versions because of their lack of state. However, it is generally not possible to avoid using stateful services at some point in a microservice-based application. It is therefore important to understand the implications of having stateful services on the architecture of the system. These can include introducing significant challenges in the ability to scale and upgrade the services. Being aware of how state will be managed is important in the very early stages of microservice application design. Let me introduce some suggestions and best practices on how this can be achieved. In memory, shared state has implications that impact and negate many of the benefits of a microservice architecture. The auto-scaling potential of individual microservices is hindered because subsequent client requests have to be sent to the same server that the initial request was made to. In addition, this requires configuration of the load balancers to use sticky sessions, which in Google Cloud is referred to as session affinity. A recognized best practice for designing stateful services is to use backend storage services that are shared by frontend, stateless services. For example, for persistent state, the Google Cloud-managed data services such as Firestore or Cloud SQL may be suitable. Then to improve the speed of data access, the data can be cached. Memorystore for Redis, which is a highly available Redis-based service, is ideal for this. This diagram displays a general solution that shows the separation of the frontend and backend processing stages. A load balancer distributes the load between the backend and frontend services. This allows the backend to scale if it needs to, to keep up with the demand from the frontend. In addition, the stateful servers and services are also isolated. The stateful services can make use of persistent storage services and caching as previously discussed. This layout allows a large part of the application to make use of the scalability and fault tolerance of Google Cloud Services as stateless services. By isolation of the stateful servers and services, the challenges of scaling and upgrading are limited to a subset of the overall set of services.

### Video - [Microservices Best Practices](https://www.cloudskillsboost.google/course_templates/41/video/520368)

- [YouTube: Microservices Best Practices](https://www.youtube.com/watch?v=hA4lZZZYnSo)

Let's discuss microservice best practices. The twelve-factor app is a set of best practices for building web or software-as-a-service applications. 12-factor design helps you decouple components of the application, so that each component can be deployed to the cloud, using continuous deployment and scale up or down seamlessly. The design principles also help maximize portability to different environments. Because the factors are independent of any programming language or software stack, 12-factor design can be applied to a wide variety of applications. Let's take a look at these best practices. The first factor is codebase. The codebase should be tracked in a version control, such as Git. Cloud Source Repositories provide fully featured, private repositories. The second factor is dependencies. There are two main considerations when it comes to dependencies for twelve-factor apps: dependency declaration and dependency isolation. Dependencies should be declared explicitly and stored in version control. Dependency tracking is performed by language-specific tools such as Maven for Java and Pip for Python. An app and it's dependencies can be isolated by packing them into a container. Artifact Registry can be used to store the images and provide fine-grained access control. The third factor is configuration. Every application has a configuration for different environments, like test, production, and development. This configuration should be kept external to the code and is usually kept in environment variables for deployment flexibility. The fourth factor is backing services. Every backing service, such as database, cache, or message service, should be accessed via URLs and set by configuration. The backing services act as abstractions for the underlying resource. The aim is to be able to swap one backing service for a different implementation, easily. The fifth factor is build, release, run. The software deployment process should be broken into three distinct stages: build, release, and run. Each stage should result in an artifact that is uniquely identifiable. Build will create a deployment package from the source code. Every deployment package should be linked to a specific release that's the result of combining a runtime environment configuration with a build. This allows for easy rollbacks and a visible audit trail of the history of every production deployment. The run stage then simply executes the application. The sixth factor is processes. Applications run as one or more stateless processes. If state is required, the technique discussed earlier in this module for state management should be used. For instance, each service should have its own datastore and caches using, for example, Memorystore, to cache and share common data between services used. The seventh factor is port binding. Services should be exposed using a port number. The applications bundle the web server as part of the application and do not require a separate server like Apache. In Google Cloud, such apps can be deployed on platform services such as Compute Engine, GKE, App Engine, or Cloud Run. The eighth factor is concurrency. The application should be able to scale out by starting new processes and scale back in as needed to meet demand and load. The ninth factor is disposability. Applications should be written to be more reliable than the underlying infrastructure they run on. This means they should be able to handle temporary failures in the underlying infrastructure and gracefully shut down and restart quickly. Applications should also be able to scale up and down quickly, acquiring and releasing resources as needed. The tenth factor is dev/production parity. The aim should be to have the same environments used in development and test/staging as are used in production. Infrastructure as code and Docker containers make this easier. Environments can be rapidly and consistently provisioned and configured via environment variables. Google Cloud provides several tools that can be used to build workflows and keep the environments consistent. These tools include Cloud Source Repositories, Cloud Storage, Artifact Registry, and Terraform. Terraform uses the underlying APIs of each Google Cloud service to deploy your resources. The eleventh factor is logs. Logs provide an awareness of the health of your apps. It's important to decouple the collection, processing, and analysis of logs from the core logic of your apps. Logging should be to the standard output and aggregating into a single source. This is particularly useful when your apps require dynamic scaling and are running on public clouds, because it eliminates the overhead of managing the storage location of the logs and the aggregation from distributed (and often ephemeral) VMs or containers. Google Cloud offers a suite of tools that help with the collection, processing, and structured analysis of logs. The twelfth factor is admin processes. These are usually one-off processes that should be decoupled from the application. This should be automated and repeatable, not manual, processes. Depending on your deployment on Google Cloud, there are many options for this, including cron jobs in GKE, cloud tasks on App Engine, and Cloud Scheduler.

### Video - [Activity Intro: Designing microservices for your application](https://www.cloudskillsboost.google/course_templates/41/video/520369)

- [YouTube: Activity Intro: Designing microservices for your application](https://www.youtube.com/watch?v=u4g9ZRdfjt8)

person: In this design activity, you're going to work on activity four of the design workbook. You will design microservices for your application. The primary aim is to diagram the microservices required by your case study application. Some of the things to consider are the microservice boundaries and state management as well as common services. Use the principle we've discussed in this module so far. Here's an example diagram for microservices for the website and the mobile phone application of an online banking service. Draw a diagram similar to the one shown here for your case study.

### Video - [Activity Review: Designing microservices for your application](https://www.cloudskillsboost.google/course_templates/41/video/520370)

- [YouTube: Activity Review: Designing microservices for your application](https://www.youtube.com/watch?v=LsTC67YFr7g)

Person: In this activity, you were asked to diagram your case study application using microservice-style architecture. The number of microservices appropriate for an application and recognizing the microservice boundaries is not obvious. Two programs might look similar, but their architecture might be considerably different based on the number of users, size of data, security, and many other factors. Fewer services might make deployment and communication between services easier, but also make development and adding new features harder. Having more, smaller services makes each individual service easier to understand and implement, but may make the overall architecture of your program more complicated. Like many things in life, you are looking for the right balance-- trading off one type of complexity for a different type, hoping to make the system overall as simple and as maintainable as possible. Here is a sample diagram depicting the microservices of our online travel portal. I suppose we could lay this out in many different ways. There really isn't one and only right way to design an application. Notice: we have separate services for our web and mobile UIs. There's a shared authentication service and we have microservices for search, orders, inventory, analytics, and reporting. Remember, each of these services will be deployed as a separate application. Where possible, we want to seed less services, but the orders and inventory services will need databases, and the analytics service will provide a data warehouse. This might make a good starting point, and we could adjust as needed when we start implementing the application.

### Video - [REST](https://www.cloudskillsboost.google/course_templates/41/video/520371)

- [YouTube: REST](https://www.youtube.com/watch?v=V5bW-KStoFY)

person: Let's talk about the design of microservices based on REST and HTTP to achieve loosely coupled independent services. One of the most important aspects of microservices-based applications is the ability to deploy microservices completely independent of one another. To achieve this independence, each microservice must provide a versioned, well-defined contract to its clients, which are other microservices or applications. Each service must not break these versioned contracts until it's known that no other microservice relies on a particular versioned contract. Remember that other's microservices may need to roll back to a previous code version that requires a previous contract, so it's important to account for this fact in your deprecation and turndown policies. A culture around strong, versioned contracts is probably the most challenging organizational aspect of a stable microservices-based application. At the lower level of detail, services communicate using HTTPS with text-based payloads, for example JSON or XML, and use the HTTP words, such as "GET" and "POST," to provide meaning for the actions requested. Clients should just need to know the minimal details to use the service: the URI, the request, and the response message formats. REST architecture supports loose coupling. REST stands for Representational State Transfer, and is protocol independent. The HTTP is the most common protocol, but gRPC is also widely used. REST supports loose coupling, but still requires strong engineering practices to maintain that loose coupling. A starting point is to have a strong contract. HTTP-based implementations can use a standard, like open API, and gRPC provides protocol buffers. To help maintain loose coupling, it is vital to maintain backward compatibility of the contract and to design an API around a domain, and not particular use cases or clients. If the latter is the case, each new use case or application will require another special purpose REST API, regardless of the protocol. While request response processing is the typical use case, streaming may also be required and can influence the choice of protocol. gRPC supports streaming, for example. Resources are identified by URIs, or endpoints, and responses to requests return an immutable representation of the resource information. REST applications should provide consistent, uniform interfaces and can link to additional resources. Hypermedia as the engine of application state is a component of REST that allows the client to require little prior knowledge of a service, because links to additional resources are provided as part of the responses. It is important that API design is part of the development process. Ideally, a set of API design rules is in place that helps the REST APIs provide a uniform interface. For example, each service reports error consistently, the structure of the URIs is consistent, and the use of paging is consistent. Also consider caching for performance and resource optimization for immutable resources. In REST, a client and server exchange representations of a resource. A resource is an abstract notion of information. The representation of a resource is a copy of the resource information. For example, a resource could represent a dog. The representation of a resource is the actual data for a particular dog, for example, Noir, who is a Schnoodle, or Bree, who is a mutt, two different representations of a resource. The URI provides access to a resource. Making a request for that resource returns a representation of that resource, usually in JSON format. The resource requested can be single items or a collection of items. For performance reasons, returning collection of items instead of individual items can be beneficial. These types of operations are often referred to as batch APIs. Representation of a resource between client and services are usually achieved using text-based standard formats. JSON is the norm for text-based formats, although XML can be used as well. For public-facing or external-facing APIs, JSON is the standard. For internal services, gRPC may be used, in particular, if performance is key.

### Video - [HTTP](https://www.cloudskillsboost.google/course_templates/41/video/520372)

- [YouTube: HTTP](https://www.youtube.com/watch?v=yMO2ESRc0rk)

Person: ... client accessing HTTP service forms and HTTP requests. HTTP requests are built in three parts, the request line, header variables and request body. The request line has the HTTP verb Get, Post, Put, et cetera, the requested URI and the protocol version. The header variables contain key value pairs. Some of these are standard, such as user region which helps the receiver identify the requesting software agent. Metadata about the message format or preferred message formats is also included here for HTTPS-based rest services. You can add custom headers here. The request body contains data to be sent to the server and is only relevant for HTTP commands that send data, such as Post and Put. Here we see two examples of HTTP client text-based messages. The first example shows an HTTP Get request to the URL using HTTP version 1.1. There is one request header variable named Host with the value pets.drehnstrom.com. The second example shows an HTTP Post request to the URL /add using HTTP version 1.1. There are three request header variables, Host, content type set to JSON, content length set to 35 bytes. There is the request body which has the JSON document, name, Noir, breed, schnoodle. This is the representation of the pet being added. As part of a request, the HTTP verb tells the server the action to be performed on a resource. HTTP as a protocol provides nine verbs, but usually only the four listed here are used in Rest. Get is used to retrieve resources. Post is used to request the creation of a new resource. The service then creates the resource and usually returns the unique ID generated for the new resource to the client. Put is used to create a new resource or make a change to an existing resource. Put requests should be idempotent which means that no matter how many times the request is made by the client to a service, the effects on the resource are always exactly the same. Finally, a Delete request is used to remove a resource. HTTP services return responses in a standard format defined by HTTP. These HTTP responses are built in three parts, the response line, header variables and response body. The response line has the HTTP version and a response code. The response code are broken on boundaries around the 100s. The 200s range means okay. For example, 200 is okay, 201 means a resource has been created. The 400 range means the client request is in error. For example, 403 means forbidden due to requester not having permission, 404 means requested resource not found. The 500 range means the server encountered an error and cannot process the request. For example, 500 is internal server error, 503 is not available, usually because the server is overloaded. The response header is a set of key value pairs, such as content type which indicates to the receiver the type of the content the response body contains. The response body has the resource representation requested in the format specified in the content type header and can be JSON, XML, HTML, et cetera. The guidelines listed here focus on achieving consistency on the API. Singular nouns should be used for individual resources and plural nouns for collections or sets. For an example, consider the following URI /pet. Then a Get request for URI /pet/1 should fetch a pet with ID 1, whereas Get /pets should fetch all the pets. Do not use URIs such as Get /getpets. The URI should refer to the resource not the action on the resource. That is the role of the verb. Remember that URIs are case insensitive and that they include version information. It is a good practice to diagram services. This diagram shows that there is a service that provides access to the resource known as pets. The representation of the resource is the pet. When a request is made for the resource via the service, one or more representations of a pet are returned.

### Video - [APIs](https://www.cloudskillsboost.google/course_templates/41/video/520373)

- [YouTube: APIs](https://www.youtube.com/watch?v=UZDoXo6AK3I)

Let's move on to API design. It is important to design consistent APIs for services. Google provides an API design guide with recommendations on items such as names, error handling documentation, versioning, and compatibility. This guide, and the API stylebook, are linked in the slides. For examples of best practices, it is useful to examine the Google Cloud APIs. Each Google Cloud service exposes a REST API. Functions are defined in the form of: service.collection.verb. The service represents the service endpoint; For example, for the Compute Engine API, the service endpoint is https://compute.googleapis.com. Collections include instances, instanceGroups and instanceTemplates. The verbs then include LIST, GET, and INSERT. To see all of your Compute Engine instances, make a GET request to the link shown on the slide. Parameters are passed either to the URL or on the request body in a JSON format. OpenAPI is an industry standard for exposing APIs to clients. Version 2.0 of the specification was known as Swagger. Swagger is now a set of open source tools built around Open API that, with associated tooling, supports designing, building, consuming, and documenting APIs. OpenAPI supports an API-first approach. Designing the API through OpenAPI can provide a single source of truth, from which source code for client libraries and server stubs can be generated automatically, as well as API user documentation. OpenAPI is supported by Cloud Endpoints and Apigee. The example documentation shows a sample of an OpenAPI specification of a petstore service. The URI is petstore.swagger.io/v1. Note the version in the URI here. The example then shows an endpoint, /pets, which is accessed by using the HTTP verb GET and will provide a list of all pets. Developed at Google, gRPC is a binary protocol that is extremely useful for internal microservice communication. It provides support for many programming languages, has strong support for loose coupling via contracts defined using protocol buffers, and is high performing because it's a binary protocol. It is based on HTTP/2 and supports both client and server streaming. The protocol is supported by many Google Cloud services, such as the Application Load Balancer, and Cloud Endpoints for microservices, as well as on GKE by using an envoy proxy. Google Cloud provides three tools for managing APIs: Cloud Endpoints, Apigee, and API Gateway. Cloud Endpoints is an API management gateway which helps you develop, deploy, and manage APIs on any Google Cloud backend. It runs on Google Cloud and leverages a lot of Google's underlying infrastructure. Apigee is an API management platform built for enterprises, with deployment options for on-cloud, on-premises, or hybrid. The feature set includes an API gateway, customizable portal for onboarding partners and developers, monetization, and deep analytics around APIs. You can use Apigee for any HTTP or HTTPS backends, no matter where they are running - on-premises or any public cloud. API Gateway enables you to provide secure access to your backend services through a well-defined REST API that is consistent across all of your services, regardless of the service implementation. The three solutions provide tools for services such as user authentication, monitoring, and securing, also for OpenAPI and gRPC.

### Video - [Activity Intro: Designing REST APIs](https://www.cloudskillsboost.google/course_templates/41/video/520374)

- [YouTube: Activity Intro: Designing REST APIs](https://www.youtube.com/watch?v=j2J2i4x44po)

Person: You will now design the APIs for the microservices identified for your application. The aim of this activity is to gain experience designing the APIs and considering aspects such as the API URL structure, the message request response formats, and versioning. Let's say we were defining an API for an online store. The API might look similar to what's shown here. Whether a service provides a user interface or some backend functionality, it requires a programmatic interface that is callable via HTTPS. The only difference between a website and a web service is the format of the data that is returned. For a UI service, we might return HTML. But a backend service might return JSON or XML. Use the principles we have discussed in this module so far and refer to activity 5 in the design workbook.

### Video - [Activity Review: Designing REST APIs](https://www.cloudskillsboost.google/course_templates/41/video/520375)

- [YouTube: Activity Review: Designing REST APIs](https://www.youtube.com/watch?v=RoQUNL53HMw)

person: In this activity, you were asked to design a RESTful API for the microservices used in your case study. Here's an example for an online travel portal. Obviously, our API would be larger than this, but in a way, the APIs are all more of the same. Each service manages and makes available some collection of data. For any collection of data, there are a handful of typical operations we do with that data. This is similar to Google Cloud APIs. For example, in Google Cloud, we have a service called Compute Engine, which is used to create and manage virtual machines, networks, and the like. The Compute Engine API has collections, like instances, instance groups, networks, subnetworks, and many more. For each collection, various methods are used to manage the data. For example, here are the methods for adding, managing, and deleting firewalls. When you design your APIs, you should strive to be as consistent as possible. This will make development easier and will also make it easier for clients to learn to use your APIs.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/41/quizzes/520376)

#### Quiz 1.

> [!important]
> **You're building a RESTful microservice. Which would be a valid data format for returning data to the client?**
>
> - [ ] JSON
> - [ ] XML
> - [ ] All options are correct.
> - [ ] HTML

#### Quiz 2.

> [!important]
> **You've re-architected a monolithic web application so state is not stored in memory on the web servers, but in a database instead. This has caused slow performance when retrieving user sessions though. What might be the best way to fix this?**
>
> - [ ] Make sure all web servers are in the same zone as the database.
> - [ ] Increase the number of CPUs in the database server.
> - [ ] Move session state back onto the web servers and use sticky sessions in the load balancer.
> - [ ] Use a caching service like Memorystore for Redis.

#### Quiz 3.

> [!important]
> **Which below would violate 12-factor app best practices?**
>
> - [ ] Keep development, testing, and production as similar as possible.
> - [ ] Explicitly declare and isolate dependencies.
> - [ ] Store configuration information in your source repository for easy versioning.
> - [ ] Treat logs as event streams and aggregate logs into a single source.

#### Quiz 4.

> [!important]
> **You're writing a service, and you need to handle a client sending you invalid data in the request. What should you return from the service?**
>
> - [ ] A 200 error code
> - [ ] A 400 error code
> - [ ] A 500 error code
> - [ ] An XML exception

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/41/video/520377)

- [YouTube: Module Review](https://www.youtube.com/watch?v=Co2dOrEWEak)

person: In this module, we focused on Microservice Design and Architecture. We started out defining what a Microservice Architecture is and the advantages and disadvantages of using Microservices. We also enumerated some Microservice best practices. Then we covered how to design service APIs and implement a REST style architecture.

## DevOps Automation

This module introduces DevOps automation, a key factor in achieving consistency, reliability, and speed of deployment.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/41/video/520378)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=N0RxISNfqIY)

This module introduces DevOps automation, a key factor in achieving consistency, reliability, and speed of deployment. Specifically, we will talk about services that support continuous integration and continuous delivery practices, part of DevOp's way of working With DevOps and microservices, automated pipelines for integrating, delivering, and potentially deploying code are required. These pipelines ideally run on on-demand provisioned resources. This module introduces the Google Cloud tools for delivering code and creating automated delivery pipelines that are provisioned on-demand. We will talk about using Cloud Source Repositories for source and version control, Cloud Build, including build triggers for automating builds, and managing containers with Artifact Registry. We will finish by reviewing Infrastructure as Code tools, like Terraform.

### Video - [Continuous Integration Pipelines](https://www.cloudskillsboost.google/course_templates/41/video/520379)

- [YouTube: Continuous Integration Pipelines](https://www.youtube.com/watch?v=g35mTX59dsY)

Let's begin by talking about continuous integration pipelines. Continuous integration pipelines automate building applications. This graphic shows a very simplistic view of a pipeline, which would be customized to meet your requirements. The process starts by checking code into a repository where all the unit tests are run. On successful passing of these tests, a deployment package is built as a Docker image. This image is then saved in an artifact registry from where it can be deployed. Each microservice should have its own repository. Typical extra steps include linting of code/quality analysis by tools such as SonarQube, integration tests, generating test reports, and image scanning. Google Cloud provides the components required to build a continuous integration pipeline. Let's go through each of those. The Cloud Source Repositories service provides private Git repositories hosted on Google Cloud. These repositories let you develop and deploy an app or service in a space that provides collaboration and version control for your code. Cloud Source Repositories are integrated with Google Cloud, so it provides a seamless developer experience. Cloud Build executes your builds on Google Cloud infrastructure. It can import source code from Cloud Storage, Cloud Source Repositories, GitHub, or Bitbucket, execute a build to your specifications, and produce artifacts such as Docker containers or Java archives. Cloud Build executes your build as a series of build steps, where each build step is run in a Docker container. A build step can do anything that can be done from a container, irrespective of the environment. There are standard steps, or you can define your own steps. A Cloud Build trigger automatically starts a build whenever you make any changes to your source code. You can configure the trigger to build your code on any changes to the source repository or only changes that match certain criteria. Artifact Registry is a single place for your team to manage Docker images or deployment packages, perform vulnerability analysis, and decide who can access what with fine-grained access control. Let's go through each of these services in more detail. Cloud Source Repositories provide managed Git repositories. You can use IAM to add team members to your project and grant them permissions to create, view, and update repositories. Repositories can be configured to publish messages to a specific Pub/Sub topic. Messages can be published when a user creates or deletes a repository or pushes a commit. Some other features of Cloud Source Repositories include the ability to use audit logging to provide insights into what actions were performed where and when, and direct deployment to App Engine. It's also possible to connect an existing GitHub or Bitbucket repository to Cloud Source Repositories. Connected repositories are synchronized with Cloud Source Repositories automatically. Developers gain complete control over defining workflows for building, testing, and deploying across multiple environments including VMs, serverless, and Kubernetes. There is no more need to provision or maintain build environments: all is handled by Cloud Build. You write a build config to provide instructions to Cloud Build on what tasks to perform. These are defined as a series of steps. Each step is executed by a cloud builder. Cloud builders are containers with common languages and tools installed in them. Builders can be configured to fetch dependencies, run unit tests, static analyses and integration tests, and create artifacts with build tools such as Docker, Gradle, Maven, Bazel, and Gulp. Cloud Build executes the build steps you define. Executing build steps is similar to executing commands in a script. You can either use the build steps provided by Cloud Build and the Cloud Build community or write your own custom build steps. A GitHub link to a list of available cloud builders can be found in the course resources for this module. Build triggers watch a repository and build a container whenever code is pushed. Google's build triggers support Maven, custom builds, and Docker. A Cloud Build trigger automatically starts a build whenever a change is made to source code. It can be set to start a build on commits to a particular branch or on commits that contain a particular tag. You can specify a regular expression to match the branch or tag value. A GitHub link to the syntax for the regular expression is available in this module's course resources. The build configuration can be specified either in a Dockerfile or a Cloud Build file. The configuration required is shown on this slide. First, a source is selected. This can be Cloud Source Repositories, Github, or Bitbucket. In the next stage, a source repository is selected, followed by the trigger settings. The trigger settings include information like the branch or tag to use for the trigger, and build configuration, for example the Dockerfile or Cloud Build file. Artifact Registry is a universal packet manager for build artifacts and dependencies. Artifact Registry can store Docker and OCI container images in a Docker registry. Artifact Registry integrates with Google Cloud CI/CD services or your existing CI/CD tools. You can store artifacts from Cloud Build and deploy artifacts to Google Cloud runtimes, including Google Kubernetes Engine, Cloud Run, Compute Engine, and App Engine flexible environment. Identity and Access Management provides consistent credentials and access control. Artifact Registry implements a Docker protocol so that you can push and pull images directly with Docker clients, including the Docker command-line tool. Google Cloud services that typically integrate with Artifact Registry, such as Cloud Build and Google Kubernetes Engine, are configured by default with permissions to access repositories in the same project and do not require a separate client. Artifact Analysis is a family of services that provide software composition analysis, metadata storage and retrieval. Its detection points are built into a number of Google Cloud products such as Artifact Registry and Google Kubernetes Engine for easy and quick enablement. The service works with both Google Cloud's first-party products and also allows you to store information from third-party sources. The scanning services leverage a common vulnerability store for matching files against known vulnerabilities. Now, binary authorization allows you to enforce the deployment of only trusted containers into GKE. Binary authorization is a Google Cloud service and is based on the Kritis specification. For this to work, you must enable binary authorization on your GKE cluster where your deployment will be made. A policy is required to sign the images. When an image is built by Cloud Build, an attestor verifies that it was from a trusted repository; for example, Source Repositories. Artifact Registry includes a vulnerability scanner that scans containers. A typical workflow is shown in the diagram. Checking of code triggers a Cloud Build. As part of the build, Artifact Registry will perform a vulnerability scan when a new image is uploaded. The scanner publishes messages to Pub/Sub. The Kritis signer listens to Pub/Sub notifications from an artifact registry vulnerability scanner and makes an attestation if the image scanning passed the vulnerability scan. Google Cloud binary authorization service then enforces the policy requiring attestations by the Kritis signer before a container image can be deployed.

### Video - [Infrastructure as Code](https://www.cloudskillsboost.google/course_templates/41/video/520380)

- [YouTube: Infrastructure as Code](https://www.youtube.com/watch?v=PY1zVLjaFiQ)

Let's now move on to consider infrastructure as code. Moving to the cloud requires a mindset change. The on-demand, pay-per-use model of cloud computing is a different model to traditional on-premises infrastructure provisioning. A typical on-premises model would be to buy machines and keep these running continuously. The compute infrastructure is typically built from fewer, larger machines. From an accounting view, the machines are capital expenditure that depreciates over time. When using the cloud, resources are rented instead of purchased, and as a result we want to turn the machines off as soon as they are not required to save on costs. The approach is to typically have lots of smaller machines—scale out instead of scale up—and to expect and engineer for failure. From an accounting view, the machines are a monthly operating expense. In other words, in the cloud, all infrastructure needs to be disposable. The key to this is infrastructure as code (IaC), which allows for the provisioning, configuration, and deployment activities to be automated. Having the process automated minimizes risks, eliminates manual mistakes, and supports repeatable deployments and scale and speed. Deploying one or one hundred machines is the same effort. The automation can be achieved using scripts or declarative tools such as Terraform which we will discuss later. It is really important that no time is spent trying to fix broken machines or installing patches or upgrades. These will lead to problems recreating the environments at a later date. If a machine requires maintenance, remove it and create a new one instead. Costs can be reduced by provisioning ephemeral environments, such as test environments that replicate the production environment. Terraform is one of the tools used for Infrastructure as Code or IaC. Before we dive into understanding Terraform, let's look at what infrastructure as code does. In essence, infrastructure as code allows for the quick provisioning and removing of infrastructures. The on-demand provisioning of a deployment is extremely powerful. This can be integrated into a continuous integration pipeline that smoothes the path to continuous deployment. Automated infrastructure provisioning means that the infrastructure can be provisioned on demand, and the deployment complexity is managed in code. This provides the flexibility to change infrastructure as requirements change. And all the changes are in one place. Infrastructure for environments such as development and test can now easily replicate production and can be deleted immediately when not in use. All because of infrastructure as code. Several tools can be used for IaC. Google Cloud supports Terraform, where deployments are described in a file known as a configuration. This details all the resources that should be provisioned. Configurations can be modularized using templates, which allows the abstraction of resources into reusable components across deployments. In addition to Terraform, Google Cloud also provides support for other IaC tools, including: Chef, Puppet, Ansible, and Packer. In this course however, we will focus on Terraform. Terraform lets you provision Google Cloud resources—such as virtual machines, containers, storage, and networking—with declarative configuration files. You just specify all the resources needed for your application in a declarative format and deploy your configuration. HashiCorp Configuration Language (HCL) allows for concise descriptions of resources using blocks, arguments, and expressions. This deployment can be repeated over and over with consistent results, and you can delete an entire deployment with one command or click. The benefit of a declarative approach is that it allows you to specify what the configuration should be and let the system figure out the steps to take. Instead of deploying each resource separately, you specify the set of resources that compose the application or service, which allows you to focus on the application. Unlike Cloud Shell, Terraform will deploy resources in parallel. Terraform uses the underlying APIs of each Google Cloud service to deploy your resources. This enables you to deploy almost everything we have seen so far, from instances, instance templates, and groups, to VPC networks, firewall rules, VPN tunnels, Cloud Routers, and load balancers. For a full list of supported resource types, see the documentation at Using Terraform with Google Cloud. The Terraform language is the user interface to declare resources. Resources are infrastructure objects such as Compute Engine virtual machines, storage buckets, containers, or networks. A Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories. The syntax of the Terraform language includes: Blocks that represent objects and can have zero or more labels. A block has a body that enables you to declare arguments and nested blocks. Arguments are used to assign a value to a name. An expression represents a value that can be assigned to an identifier. Terraform can be used on multiple public and private clouds. Terraform is already installed in Cloud Shell. The example Terraform configuration file shown starts with a provider block that indicates that Google Cloud is the provider. The region for the deployment is specified inside the provider block. The resource block specifies a Google Cloud Compute Engine instance, or virtual machine. The details of the instance to be created are specified inside the resource block. The output block specifies an output variable for the Terraform module. In this case, a value will be assigned to the output variable "instance_ip."

### Video - [Lab Intro: Building a DevOps Pipeline](https://www.cloudskillsboost.google/course_templates/41/video/520381)

- [YouTube: Lab Intro: Building a DevOps Pipeline](https://www.youtube.com/watch?v=btfEgTl4Ndc)

In this first lab, you will build a DevOps pipeline using Cloud Source Repositories, Cloud Build, and Artifact Registry. Specifically, you will create a Git repository. You will then write a simple Python application and add it to your repository. After that you will test your web application in Cloud Shell and then define a Docker build. Once you define the build, you will use Cloud Build to create a Docker image and then store the image in Artifact Registry. Then, you will see how to automate builds using triggers. Once you have a trigger, you will test it by making a change to your program and pushing that change to your Git repo.

### Lab - [Building a DevOps Pipeline](https://www.cloudskillsboost.google/course_templates/41/labs/520382)

In this lab, you will build a continuous integration pipeline using Cloud Source Repositories, Cloud Build, Build Triggers and Container Registry.

- [ ] [Building a DevOps Pipeline](../labs/Building-a-DevOps-Pipeline.md)

### Video - [Lab Review: Building a DevOps Pipeline](https://www.cloudskillsboost.google/course_templates/41/video/520383)

- [YouTube: Lab Review: Building a DevOps Pipeline](https://www.youtube.com/watch?v=tRMgUaGHp_s)

In this lab, you learned how to use Google Cloud tools to create a simple and automated continuous integration pipeline. You used Cloud Source repositories to create a Git repository, and then used Cloud Build and triggers to automate the creation of your Docker images when code was checked into that repo. When Cloud Build created your Docker images, it stored them in Artifact Registry. You saw how to access those images and test them in a Compute Engine VM. You can stay for the lab walk through, but remember, Google Cloud's user interface can change. So your environment might look slightly different. So here I am in the GCP console. The first thing I'm going to do is I'm going to go create a new repository. For that, in the navigation menu, I'm going to scroll down to Source Repositories. That opens in a new tab. In here, I'm going to click Add repository. We're going to create a new one, click Continue, and then we're just going to give it a name, which is the one we have in the lab instructions, which is devops-repo. Now, I want to select the project that I'm working with. So for that, you want to just make sure that you're selecting the actual Qwiklabs project that is displayed in your Qwiklabs UI. So I'm going to select that, and I'm going to click Create. So this is going to create now. What I'm going to do is I'm going to return to the Cloud Console. So I'm just switching tabs here. Within here I'm going to click Activate Cloud Shell. When prompted, I'm going to click Continue. This is pretty small here. So I'm actually also going to move this to a new tab, a specific new window. I can do that right here, and then I can see a little bit more here. So within here now I'm just going to create a new directory. So let me copy the directory name from the lab instructions, and then I'm just going to navigate to that directory. So now we're going to clone the empty repository that we just created, and we should get a warning here. That is because we're actually using an empty directory. So you can see that right here, you appear to have cloned an empty directory, and that's okay. So this directory now created the folder devops-repo. So we can also navigate to that and right now, right now. That completes really your first task. Now, in the next task, we're now going to use the code editor and actually create a file in here. So up here, I'm going to click on Launch editor, and what we're going to do is we're going to go to the File menu and create a new file, but we want to make sure that we do that in the devops-repo. So let me navigate to the devops-repo here, then click File, New File, we're going to call that main.py. We can see that as now within that directory, and I'm just going to paste the Python code that we have from the lab instructions in there, and then I can save that. Now, I'm going to now create a new folder in here called templates. So in the devops-folder, I'm going to right-click and I can click on New Folder and I'm giving it a name templates. In that folder we're going to create the file layout.html, which is also from the lab instructions. So these first couple of steps are really just a lot of setting up the code here that we're going to use, and rather than just cloning this from somewhere, we're giving you all of the files and showing you what this code is so that you can understand what this code does. Again, this is just a simple Python application. So it's just really a little bit better than Hello World, but we're going to use it for the pipeline. We actually are going to use something similar throughout the other labs that we have in this course as well. Now, we also need to create an index HTML. So I'm going to in the templates folder, also add another file, call it index.html, and we're also going to copy code in there, and then we're going to save that as well. Now, we also need a requirements file. This is typical for Python applications, so it's a prerequisite, and so I'm going to, in the DevOps folder, not the templates folder now, I'm going to add a new file, and call it requirements.txt and specify Flask in there. So now we have some files, we can save them. So what we're going to nano first we need to add all the files that we just created to the local git repo. So in Cloud Shell, we're going to make sure that we navigate to this folder. So if you copy the lab instructions that just asks you to cd to that folder, and let me make this a little bit bigger here and clear this so we can see more. Navigate, we're still there, and then we're going to run the command git add --all. Now, we're going to commit those changes to the repository. Now, the lab instructions say that you want to enter an email, and then what you want to enter a name, you could use your gmail or anything else here is really up to you. So I'm going to add these first command, and what I can actually do is I can just grab the email that I have in Qwiklabs as an example. So let me get to that. So in there, for example, from my current session, I have this, so I can run that for the configuration, and then I can also go back to the lab instructions. Now, specify the username, and here I can put my name, and I could just use this part of the email for that. This is just for illustration purposes, usually you'd want to put your own email and your own name here. This is just a demo. So we've configured git, so now we can go ahead and actually commit and we give it a name. Let's just call it initial commit. So here we can see the commits, these are all the files that we just did, again the requirements is directly within a devops-repo folder as well as main.py, and then the two HTML files are within the templates. So now we can push this to the master, and we can see that this is done. Now I can go back and switch to the Source Repositories page, and I can go refresh this, and we should see the code here. So here we go. So this worked successfully and that completes task 2. So that's all great, but we want to also test this application and actually create the Docker build and all of that. So let's get on that, I am going to go back to Cloud Shell, and first we're going to install the Flask framework using pip. So I want again, just make sure I'm in the right folder by running first the cd command, and then we're going to run sudo pip3 install, and that is going to go through, and then we're going to run the program with the Python command. So we can see that this is configured to run on port 8080, and we can actually preview this, what this looks like. So up here, we have a Web preview button. So if I click on this, and then preview on port 8080, it'll open a new tab for me, and that worked. So it says Hello DevOps Fans in here. So this is working, I can stop that if I go back to Cloud Shell and just Control C out of this, and what I'm going to do now is I'm going to go into the main.py and I'm actually going to change the name. So if I go into main.py here, we see this is where that text Hello DevOp Fans was coming from. So we can change that to something else. Let's say for example, Hello Google Cloud Fans, and we're going to save that. Now we're going to take these changes we just made, and we're going to commit them. So let me just do that and give it a name, second commit, and then we'll also push to the master. Great, and now if I go back to clusters Repositories, I can go in here into main.py, and I can see that this change actually took effect. That's the end of task three. So now we're actually going to get creative here, and we're going to define a Docker build and actually work on our DevOps pipeline. The first step to that is to create a file called Dockerfile, and that file just defines how our Docker container is constructed. So let me go back to Cloud Shell, and within the DevOps repo folder, I'm going to want to create a new file. I'm just going to give it the name Dockerfile, no extension or anything, and the Dockerfile. So here at the top of the file, I'm just going to say from Python 3.7. So this is just the base image, and you can actually choose many base images. In this case, you're using one with Python already installed on it, and then we're going to enter the following. Workdir app and then copy, and these lines just copy the source code from the current folder into the app folder in the container image. Then we're also going to add commands, and this uses pip to install the requirements of the Python application into the container, and gunicorn is a Python Web server that we use to run the Web app. Then we're also going to enter the environment variables, and this just sets the port of the application running, in this case, port 80, and the last line in here runs a Web app using gunicorn Web server. So you can see that here, and then you are also given, let me just make this a little bigger so we can see this whole file. You also given what the whole file looks like again. So I could also just copy that, and just make sure one more time that I have done everything correctly, and that seems to be the case. So that's it for task four. We've defined a Docker build and now we need to actually manage the Docker images with Cloud built and Container Registry. So within Cloud Shell, first I want to make sure we're still in the right folder, and we keep doing that in the lab instructions just in case you do something else or you close Cloud Shell, and reopen it and you are not in the same folder anymore. So just want to confirm I'm in the right folder. Now we're going to create an environment variable that's specifically for the project, but that's actually already stored in devshell project ID. So let's actually verify that. If I go echo devshell_project_ID, we can see right here that I have my project ID, and it can also verify if I go back to my console, that is the current project I'm working on. So you can see that ends with 625c, and the same is true here. So now we're going to go ahead and use that environment variable to now start the build. So let's do that, and this might ask us now to enable Cloud Build, it's downloading. We're just going to wait for that. Now in Container Registry, the image name will always begin with gcr dot io slash and then followed by the project ID of the project you're working with, followed by the image, name, and the version, and then the period at the end of the command represents the path to Dockerfile. In this case, it'll be the current directory. So we're just going to wait for this to complete, and then we're going to return to the console, and look at the Container Registry. All right, it's actually done. So here you can see what I just talked about, the gcr.io. Then we have the name of the project. Here we have the project itself, and then the version, so the image name and the version. So I'm going to go back now to Container Registry. So let me go to Cloud Console, click on Container Registry and we're going to minimize that here, and here we can see that DevOps image. So now what we're going to do is we're going to go to Cloud Build and we should see this already. We're going to go to Compute Engine and create a VM that is going to directly use that container image. So let's go to the navigation menu and then we're going to go to Cloud Build. It's a long list in here, but here we go, and here we can see the history of that build that just happened. So we can see 50 seconds ago, and we could get some more information. We can see where this is stored. We can click into it and I can get all sorts of information, and if something were to go wrong, we can see that information here as well. But for the time being, let's go to Compute Engine and create a new VM, and in the instance creation page, we're going to use the container image that we just created. So I can just leave the name, it's instance one, and I'm going to scroll down, and the key here is now we're going to check this box that says deploy a container image to this VM instance, and then the image. Now, here we need to copy from Cloud Shell. So let me go back to Cloud Shell. Let me grab this whole path here, copy it once you select it, and then let me paste that in here, and obviously that's going to be different for your projects. So keep in mind, don't just copy the one I have here because you will have different project ideas. Now I also want to enable HTTP traffic. So this is just going to add a network tag, and then create a firewall rule with that tag to allow traffic there, and we're doing that because again, we enabled traffic on port 80, and then we're going to click Create. We're going to wait for this VM to start, and then we're going to make a request to the external IP address. I can close this over here. This is anytime you're using a new project, you get this Learn page, and with all of Qwiklabs, you'll always have new projects for all of your different labs. So here we have the external IP. I can click on here. Directly, and this might take a while for this to startup, so you might have to wait a minute or so. So let's actually do that and let's get back to here. So here we go. Took about a minute to two minutes just for that container to start. But here we can see now, the new text that we have in here We can see that this is running. So again, all I did is make a request to the external IP address, and this is the IP address in my case, you can just click on that directly or you could navigate to that by just entering that IP address. Great, so now what we're going to do is we want to save these changes to our Git Repository. So we're going to go back to Cloud Shell, and let me just clear this to get a bit more space. In here now we're going to make sure we're on the right folder which we already are, and then we're going to git add --all. Then we're going to first commit the changes locally with just a message that we added to Docker Support, and then we're going to push that to the master. So that worked. So now we can go back to Cloud Source Repository. So let me switch tabs to that and I'm just going to refresh this page, and there we can now also see our Dockerfile. So we can see that's in there too now. So that's all great. But now we want to actually move into the automation piece. So we're going to now automate builds with triggers. So for that we're going to go to Container Registry, and I have that in a different tab, I believe. Let me close a couple of these. Let's actually go back just to Cloud Console, Navigation menu safety, and then scroll down to Container Registry. So we have this DevOps image folder here. If I go in there, here we can see the container that we created earlier. So now we're going to go to Cloud Build. Let me go back to navigation menu. Cloud Build, and we're going to create a trigger. So let's go through the Trigger section. We're going to create a trigger, we're going to do that for our DevOps repository, we're going to give this a name. The lab instructions just has devops-trigger. Now there are a lot of defaults in here. We're going to accept those. It's just going trigger on the branch and you can say what files to include, exclude, the image name, and so on. In our case, we're just going to go ahead and create this, and then once it's created, we're going to run this trigger. So let's click Run trigger. So it says that build has started, and then we're going to go to the History link. We're going to ensure that this is actually running. So let me go back here to History, and here we can see that this has triggered. So we're going to wait for this to finish, and then we're going to click on this actual trigger here and scroll down to look at the logs, and the output of the build here is what you would have seen if you're running it on your machine. So let's just wait for this to complete and then we're going to explore that. So we can see that this is complete, I'm just going to go click on the link for that, and this is what I mentioned, if you had run this from Cloud Shell like we did earlier, this is the output that you would see. So you might see this is very familiar to what we saw earlier. So now we're going to go back to the Container Registry service and we should see the new image in there. So if I go to the navigation menu and scroll down, and go to the Container Registry, we now see the devops-image here, we have a devops-repo folder here, and here we have that new image. Here, sorry, I navigated into the wrong place. So here we see the new image, you can see that this was just created a minute ago. So we're going to go back now, we're going to make changes to our file, and we're going to try to see how these builds are done automated. Then we want to also test these build changes. So I'm going to navigate back to Cloud Shell, we're going to go to the main function, and just like we did earlier, we're going to change this title, and let's just call it what is in the lab instructions, which is Hello Build Trigger. We're going to save that, and we're going to commit that, first make sure we are in the right folder, and then we're going to push our changes to clutters repositories. So all I've done now is made the change, and they should now cause a trigger. So if we go back to the Cloud Build service now in the console, we can see here that this is automatically triggered, and that's the end of task 6. That's all great, but now we want to actually test this. So we're going to wait for this build to complete, we can then click on it to see the detail, and then under the build information, we're going to find the image link, and then we're going to create a new Virtual Machine to test this because this is a new image that we have and we're going to allow HTTP traffic on that, wait for that to start up, and then we should see a text that is different from this. It shouldn't say Hello Google Cloud Fans, it should say Hello Build Triggers. So here we can see that the build is completed and I can now click on it, and I'm now looking for the image so I can go through the Build Log or it can also go to the Build Artifacts, and here I see the image. So let me just make some more space here and let's copy that, and then we're going to go to the navigation menu and we're going to head over to Compute Engine and create a new instance on this. So I'll go to Compute Engine, click on Create Instance. I'm going to leave the name and region zone as default. Now importantly, need to click Deploy a Container. I'm going to paste in the image, allow HTTP traffic, and then we're going to go create that. We're going to wait for this VM to startup, and once it's started, we're going to click in the external IP address, and just like before, that won't be automatically ready because we have to wait for the container to start. So let's just do that. So here we can see that this is now finished, the container is running, and it now it says, Hello Build Trigger. That's the end of the lab.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/41/quizzes/520384)

#### Quiz 1.

> [!important]
> **Which Google Cloud tools can be used to build a continuous integration pipeline?**
>
> - [ ] Artifact Registry
> - [ ] Cloud Build
> - [ ] All of these
> - [ ] Cloud Source Repositories

#### Quiz 2.

> [!important]
> **What Google Cloud feature would be easiest to use to automate a build in response to code being checked into your source code repository?**
>
> - [ ] Build triggers
> - [ ] Cloud Scheduler
> - [ ] App Engine
> - [ ] Cloud Run functions

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/41/video/520385)

- [YouTube: Module Review](https://www.youtube.com/watch?v=o4i-gruFXko)

In this module you learned about services that you can use to help automate the deployment of your cloud resources and services. You used Cloud Source Repositories, Cloud Build, triggers, and Artifact Registry to create continuous integration pipelines. A CI pipeline automates the creation of deployment packages like Docker images in response to changes in your source code. You also saw how to automate the creation of infrastructure using the infrastructure as code tool, Terraform.

## Choosing Storage Solutions

In this module, we discuss Google Cloud storage and data solutions and how to select the most suitable one to meet your business and technical requirements.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/41/video/520386)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=kDRcszj9C_M)

Priyanka: In this module, we discuss Google Cloud Storage and data solutions and how to select the most suitable one to meet your business and technical requirements. Google Cloud provides a rich set of different storage options that cater to different types of data, sizes of data, life cycle and also data access patterns. We will discuss storing binary data with Cloud Storage, relational data with Cloud SQL or Spanner and NoSQL or unstructured data using Firestore and Bigtable. In addition, we will consider caching for fast data access using Memorystore and finally aggregating data for queries and reports using BigQuery as a data warehouse.

### Video - [Key Storage Characteristics](https://www.cloudskillsboost.google/course_templates/41/video/520387)

- [YouTube: Key Storage Characteristics](https://www.youtube.com/watch?v=beMBLzlBAkI)

Let's get started by considering key storage characteristics. Google Cloud has a wide range of managed storage and database options in its portfolio. Knowing the characteristics of each and being able to select a suitable solution is vital as an architect during the design process. From a high level, the services range from relational, NoSQL object storage, data warehouse, to in-memory. These services are fully managed, scalable, and backed by industry leading-SLAs. Making a decision on which storage solution is right for your requirement is a balance of a number of characteristics, including: type of data, scale, durability, availability, and location requirements. We'll discuss ways in which you can make the best decision based on your requirements in this module. Different data storage services have different availability SLAs. For a service, the availability SLA is often dependent on the configuration of the service. For example, for Cloud Storage as the slide shows, the availability varies depending on whether multi-regional, regional, or coldline buckets are created. The same can be seen for Spanner and Firestore, with multi-regional offering higher availability than single region configurations. This is where requirements are extremely important as they will help inform the storage choice. The availability SLAs are typically defined per month. Monthly Uptime Percentage means the total number of minutes in a month, minus the number of minutes of downtime suffered from all downtime periods in a month, divided by the total number of minutes in a month. For up-to-date SLA numbers, refer to the documentation. Durability of data represents the odds of losing the data. Depending on the storage solution, the durability is a shared responsibility. Google Cloud's responsibility is to ensure that data is durable in the event of a hardware failure. Your responsibility is performing backups of your data. For example, Cloud Storage provides you with 11 9's durability, and versioning is a feature. However, it's your responsibility to determine when to use versioning. It is recommended that you turn versioning on and have older versions archived as part of an object lifetime management policy. For other storage services to achieve durability, it usually means taking backups of data. For disks, this means snapshots, so snapshot jobs should be scheduled. For Cloud SQL, you can create a backup at any time (on-demand). This could be useful if you are about to perform a risky operation on your database, or if you need a backup and you do not want to wait for the backup window. Google Cloud also provides automated backups, point in time recovery, and optionally a failover server. You can create on-demand backups for any instance, whether the instance has automatic backups enabled or not. To improve durability, SQL database backups should also be run. Spanner and Firestore provide automatic replication, and you should run export jobs with the data being exported to Cloud Storage. The amount of data, and the number of reads and writes, are important to know when selecting a data storage service. Some services scale horizontally by adding nodes; for example, Bigtable and Spanner, which is in contrast to Cloud SQL and Memorystore, which scale machines vertically. Other services scale automatically with no limits; for example, Cloud Storage, BigQuery, and Firestore. Strong consistency is another important characteristic to consider when designing data solutions. A strongly consistent database will update all copies of data within a transaction and ensure that everybody gets the latest copy of committed data on reads. Google Cloud services providing strong consistency include Cloud Storage, Cloud SQL, Spanner, and Firestore. If an instance does not use replication, Bigtable provides strong consistency, because all reads and writes are sent to the same cluster. Eventually consistent databases typically have multiple copies of the same data for performance and scalability. They support handling large volumes of writes. They operate by updating one copy of the data synchronously and all copies asynchronously, which means that not all readers are guaranteed to read the same values at a given point in time. The data will eventually become consistent, but not immediately. Bigtable and Memorystore are examples of Google Cloud data services that have eventual consistency. When designing a data storage solution, calculating the total cost per GB is important to help determine the financial implications of a choice. Bigtable and Spanner are designed for massive data sets and are not as cost-efficient for small datasets. Firestore is less expensive per GB stored, but the cost for reads and writes must be considered. Cloud Storage is not as expensive but is only suitable for certain data types. BigQuery storage is relatively cheap but does not provide fast access to records, and a cost is incurred for each query. So as you see, the choice of the right storage solution is not simple. It has to be based on type of data, size of data, and read/write patterns.

### Video - [Activity Intro: Defining storage characteristics](https://www.cloudskillsboost.google/course_templates/41/video/520388)

- [YouTube: Activity Intro: Defining storage characteristics](https://www.youtube.com/watch?v=0xKM0VfEl7k)

person: You will now define storage characteristics for each of your case-study services. In a microservice architecture, there is not one big database to store everything for the entire application. Each service should maintain its own data. Storage characteristics include whether the data is structured or unstructured and relational or NoSQL. Ask yourself if you require strong consistency or if eventual consistency is good enough. Also, think about how much data you have and if you need read/write or read only. Here's an example table for an account service that specifies all the business and technical requirements that I just mentioned. Refer to activity six in your design workbook to fill out a similar table for your service.

### Video - [Activity Review: Defining storage characteristics](https://www.cloudskillsboost.google/course_templates/41/video/520389)

- [YouTube: Activity Review: Defining storage characteristics](https://www.youtube.com/watch?v=ckFCft2cpCg)

person: In this activity you are asked to define the storage characteristics for the case study you are designing. These characteristics will help you choose Google Cloud storage services most appropriate for your application. Here's an example for our online travel portal Click Travel. We focused on the inventory, inventory uploads, ordering, and analytic services. As you can see, each of these services has different requirements that might result in choosing different Google Cloud services.

### Video - [Choosing Google Cloud Storage and Data Solutions](https://www.cloudskillsboost.google/course_templates/41/video/520390)

- [YouTube: Choosing Google Cloud Storage and Data Solutions](https://www.youtube.com/watch?v=DqScRRYMLr4)

Now that you have documented the data characteristics of your services, let's talk about how to select Google Cloud storage and data solutions. The Google Cloud storage and database portfolio covers relational, file, NoSQL, object, block, data warehouse, and in-memory stores, as shown in this table. Let's discuss each service from left to right. Relational. Cloud SQL is a fixed schema database with a storage limit of 64 terabytes. It is offered using MySQL, PostgreSQL, and SQL Server. These services are good for web applications such as CMS or eCommerce. Spanner is also a relational and fixed schema but scales infinitely and can be regional or multi-regional. Example use cases include scalable relational databases greater than 30 GB with high availability and also global accessibility, like supply-chain management and manufacturing. AlloyDB is a fixed schema, fully managed PostgreSQL-compatible database. It provides enterprise-grade performance and availability while maintaining 100% compatibility with open-source PostgreSQL. File. Filestore is a schemaless, high-performance, fully managed file store service for applications that require a filesystem interface and a shared file system for data. Filestore gives users a simple, native experience for standing up managed Network Attached Storage with their Compute Engine and Google Kubernetes Engine instances. Google Cloud's NoSQL datastores are schemaless. Firestore is a completely managed document datastore with a maximum document size of 1 MB. It is useful for hierarchical data; for example, a game state or user profiles. Bigtable is also a NoSQL datastore that scales infinitely. It is good for heavy read and write events and use cases including financial services, Internet of Things, and digital advert streams. Object. For object storage, Google Cloud offers Cloud Storage. Cloud Storage is schemaless and completely managed with infinite scale. It stores binary object data and so it's good for storing images, media serving, and backups. Block. Persistent Disk volumes are schemaless, durable network storage devices that your virtual machine instances can access like physical disks in a desktop or a server. The data on each Persistent Disk volume is distributed across several physical disks. Warehouse. Data warehousing is provided by BigQuery. The storage uses a fixed schema and supports completely managed SQL analysis of data stored. It is excellent for performing analytics and business intelligence dashboards. In-memory. For in-memory storage, Memorystore provides a schemaless-managed Redis database. It is excellent for caching for web and mobile apps and for providing fast access to state in microservice architectures. Let's summarize the services in this module with this decision chart. First, ask yourself: Is your data structured, and will it need to be accessed using its structured data format? If the answer is no, then ask yourself, do you need a shared file system? If you do, then choose Filestore. If you don't, then choose Cloud Storage. If your data is structured and needs to be accessed in this way, than ask yourself, does your workload focus on analytics? If it does, you will want to choose Bigtable or BigQuery, depending on your latency and update needs. BigQuery is recommended as a data warehouse, is the default storage for tabular data, and optimized for large-scale, ad-hoc SQL-based analysis and reporting. While BigQuery data manipulation language enables you to update, insert, and delete data from your BigQuery tables, because it has a built-in cache BigQuery works really well in cases where the data does not change often. Bigtable is a NoSQL wide-column database. It's optimized for low latency, large numbers of reads and writes, and maintaining performance at scale. In addition to analytics, Bigtable is also suited as a 'fast lookup' non-relational database for datasets too large to store in memory, with use cases in areas such as IoT, AdTech and FinTec. If your workload doesn't involve analytics, check whether your data is relational. If it's not relational, do you need application caching? If caching is a requirement, choose Memorystore, an in-memory database. Otherwise choose Firestore, a document database. If your data is relational and you need Hybrid transaction/analytical processing, choose AlloyDB. If you don't need HTAP and don't need global scalability, choose Cloud SQL. If you don't need HTAP and need global scalability, choose Spanner. Depending on your application, you might use one or several of these services to get the job done. For more information on how to choose between these different services, please refer to the links on storage options and databases in the Course Resources. You might also want to consider how to transfer data in Google Cloud. A number of factors must be considered, including cost, time, offline versus online transfer options, and security. While transfer into Cloud Storage is free, there will be costs with the storage of the data and maybe even appliance costs if a transfer appliance is used or egress costs if transferring from another cloud provider. If you have huge datasets, the time required to transfer across a network may be unrealistic. Even if it is realistic, the effects on your organization's infrastructure may be damaging while the transfer is taking place. This table shows the challenge of moving large datasets. The color coded cells highlight unrealistic timelines that require alternative solutions. Let's go over online and offline data transfer options. For smaller or scheduled data uploads, use Cloud Storage Transfer Service, which enables you to: Move or back up data to a Cloud Storage bucket from another cloud storage provider such as Amazon S3, from your on-premises storage, or from any HTTP/HTTPS location. Move data from one Cloud Storage bucket to another, so that it is available to different groups of users or applications. Periodically move data as part of the data processing pipeline or analytical workflow. Storage Transfer Service provides options that can make data transfers and synchronization easier. For example, you can: Schedule one-time transfer operations or recurring transfer operations. Delete existing objects in the destination bucket if they don't have a corresponding object in the source. Delete data source objects after transferring them. Schedule periodic synchronization from a data source to a data sink with advanced filters based on file creation dates, file-name filters, and times of day you prefer to import data. Use the Storage Transfer Service for on-premises data for large-scale uploads from your data center. The Storage Transfer Service for on-premises data allows large-scale online data transfers from on-premises storage to Cloud Storage. With this service, data validation, encryption, error retries, and fault tolerance are built in. On-premises software is installed on your servers—the agent comes as a Docker container and a connection to Google Cloud is set up. Directories to be transferred to Cloud Storage are selected in the Cloud Console. Once the data transfer begins, the service will parallelize the transfer across many agents supporting scale up to billions of files and 100s of TBs. Via the Cloud Console, a user can view detailed transfer logs and also the creation, management, and monitoring of transfer jobs. To use the Storage Transfer Service for on-premises, a Posix-compliant source is required and a network connection of at least 300Mbps. Also, a Docker-supported Linux server that can access the data to be transferred is required with ports 80 and 443 open for outbound connections. The use case is for on-premises transfer of data whose size is > 1TB. For large amounts of on-premises data that would take to long to upload, use Transfer Appliance. Transfer Appliance is a secure, rackable, high-capacity storage server that you set up in your data center. You fill it with data and ship it to an ingest location, where the data is uploaded to Google. The data is secure, you control the encryption key, and Google erases the appliance after the transfer is complete. The process for using a transfer appliance is that you request an appliance, and it is shipped in a tamper-evident case. Data is transferred to the appliance, and the appliance is shipped back to Google, data is loaded to Cloud Storage, and you are notified that it is available. Google uses tamper-evident seals on shipping cases to and from the data ingest site. Data is encrypted to AES256 standard at the moment of capture. Once the transfer is complete, the appliance is erased per NIST-800-88 standards. You decrypt the data when you want to use it. There is also a transfer service for BigQuery. The BigQuery Data Transfer Service automates data movement from SaaS applications to BigQuery on a scheduled, managed basis. Data Transfer Service initially supports Google application sources like Google Ads, Campaign Manager, Google Ad Manager, and YouTube. There are also data connectors that allow easy data transfer from Teradata, Amazon Redshift, and Amazon S3 to BigQuery. The screenshots on the slide show a source type is selected for a transfer, a schedule is configured, and a data destination is selected. For the transfer, the data formats are also configured.

### Video - [Activity Intro: Choosing Google Cloud storage and data services](https://www.cloudskillsboost.google/course_templates/41/video/520391)

- [YouTube: Activity Intro: Choosing Google Cloud storage and data services](https://www.youtube.com/watch?v=Qf-9jOfJmxk)

person: You will now select storage products for each of your case-study services. Use these storage characteristics and your understanding of Google Cloud's various services that we covered in this module to help you choose which storage services would be most appropriate for each of your microservices. Here's an example table for an account service that should leverage Cloud SQL because we require a relational database that's strongly consistent with gigabytes of data and read-write capabilities. Refer to Activity 7 in your design workbook to fill out a similar table for your applications and services.

### Video - [Activity Review: Choosing Google Cloud storage and data services](https://www.cloudskillsboost.google/course_templates/41/video/520392)

- [YouTube: Activity Review: Choosing Google Cloud storage and data services](https://www.youtube.com/watch?v=uds-koN4J8s)

person: In this activity, you were asked to select the appropriate Google Cloud storage services for your case study. Here's an example for our online travel portal ClickTravel. For the inventory service, we will use Cloud Storage for the raw inventory uploads. Suppliers will upload the inventory as JSON data stored in txt files. That inventory will then be imported into a Firestore database. The order service will store its data in a relational database running in Cloud SQL. The analytic service will aggregate data from various sources into a data warehouse for which we'll use BigQuery.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/41/quizzes/520393)

#### Quiz 1.

> [!important]
> **You are a global financial services company with users all over the world. You need a database service that can provide low latency worldwide with strong consistency. Which service might you choose?**
>
> - [ ] BigQuery
> - [ ] Firestore
> - [ ] Cloud SQL
> - [ ] Spanner

#### Quiz 2.

> [!important]
> **Currently, you are using Firestore to store information about products, reviews, and user sessions. You'd like to speed up data access in a simple, cost-effective way. What would you recommend?**
>
> - [ ] Cache the data using Memorystore.
> - [ ] Move the data to Spanner.
> - [ ] Move the data to Bigtable.
> - [ ] Move the data to BigQuery.

#### Quiz 3.

> [!important]
> **You need to store user preferences, product information, and reviews for a website you are building. There won't be a huge amount of data. What would be a simple, cost-effective, managed solution?**
>
> - [ ] Firestore
> - [ ] BigQuery.
> - [ ] Cloud SQL
> - [ ] Spanner

#### Quiz 4.

> [!important]
> **You want to analyze sales trends. To help achieve this, you want to combine data from your on-premises Oracle database with Google Analytics data and your web server logs. Where might you store the data so it is both easy to query and cost-effective?**
>
> - [ ] Spanner
> - [ ] Cloud SQL
> - [ ] BigQuery
> - [ ] Firestore

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/41/video/520394)

- [YouTube: Module Review](https://www.youtube.com/watch?v=im_qiecjFJI)

In this module, we covered the various storage services available in Google Cloud. These include Cloud Storage for binary data, Cloud SQL and Spanner for relational databases, Bigtable and Firestore for NoSQL databases, and BigQuery for data warehouses. We also talked about different storage characteristics and how they can be used to help you choose where to store your data.

## Google Cloud and Hybrid Network Architecture

In this module, we discuss Google Cloud network architectures, including hybrid architectures.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/41/video/520395)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=90ihRKfBybg)

In this module, we discuss Google Cloud network architectures including hybrid architectures. We will start by talking about how to design VPC networks to optimize for cost, security, and performance. Then, we'll cover the configuration of global and regional load balancers to provide access to services. As part of the load balancer configuration, you can enable Cloud CDN to provide a lower latency and decrease network egress, which ultimately decreases your networking costs. We will also introduce the Network Intelligence Center to evaluate your networks architecture, and go over the network connection options including peering, VPN and Cloud Interconnect.

### Video - [Designing Google Cloud Networks](https://www.cloudskillsboost.google/course_templates/41/video/520396)

- [YouTube: Designing Google Cloud Networks](https://www.youtube.com/watch?v=PTy9TN6-77s)

Let's get started by designing Google Cloud networks and load balancers. Google runs a worldwide network that connects regions all over the world. You can use this high-bandwidth infrastructure to design your cloud networks to meet your requirements such as location, number of users, scalability, fault tolerance, and latency. Let's take a closer look at Google Cloud's network. This map represents Google Cloud's reach. On a high level, Google Cloud consists of regions, which are the icons in blue; points of presence, or PoPs, which are the dots in grey; a global private network, which is represented by the blue lines; and services. A region is a specific geographical location where you can run your resources. This map shows several regions that are currently operating, as well as future regions and their zones. The PoPs are where Google's network is connected to the rest of the internet. Google Cloud can bring its traffic closer to its peers because it operates an extensive global network of interconnection points. This reduces costs and provides users with a better experience. The network connects regions and PoPs and is composed of a global network of fiber optic cables with several submarine cable investments. In Google Cloud, VPC networks are global, and you can either create auto mode networks that have one subnet per region or create your own custom mode network where you get to specify which region to create a subnet in. Resources across regions can communicate using their internal IP addresses without any added interconnect. For example, the diagram on the rights shows two subnets in different regions with a server on each subnet. They can communicate with each other using their internal IP addresses because they are connected to the same VPC network. Selecting which regions to create subnets in depends on your requirements. For example, if you are a global company, you will most likely create subnetworks in regions across the world. If users are within a particular region, it may be suitable to select just one subnet in a region closest to these users and maybe a backup region close by. Also, you can have multiple networks per project. These networks are just a collection of regional subnetworks or subnets. To create custom subnets you specify the region and the internal IP address range, as illustrated in the screenshots on the right. The IP ranges of these subnets don't need to be derived from a single CIDR block, but they cannot overlap with other subnets of the same VPC network. This applies to primary and secondary ranges. Secondary ranges allow you to define alias IP addresses. Also, you can expand the primary IP address space of any subnets without any workload shutdown or downtime. Once you defined your subnets, machines in the same VPC network can communicate with each other through their internal IP address regardless of the subnet they are connected to. Now, a single VM can have multiple network interfaces connecting to different VPC networks. This graphic illustrates an example of a Compute Engine instance connected to four different networks covering production, test, infra, and an outbound network. A VM must have at least one network interface but can have up to 8, depending on the instance type and the number of vCPUs. A general rule is that with more vCPUs, more network interfaces are possible. All of the network interfaces must be created when the instance is created, and each interface must be attached to a different network. Shared VPC allows an organization to connect resources from multiple projects of a single organization to a common VPC network. This allows the resources to communicate with each other securely and efficiently using internal IPs from that network. This graphic shows a scenario where a shared VPC is used by three other projects, namely service projects A, B, and C. Each of these projects has a VM instance that is attached to the Shared VPC. Shared VPC is a centralized approach to multi-project networking, because security and network policy occurs in a single designated VPC network. This allows for network administrator rights to be removed from developers so they can focus on what they do best. Meanwhile, organization network administrators maintain control of resources such as subnets, firewall rules, and routes while delegating the control of creating resources such as instances to service project administrators or developers.

### Video - [Designing Google Cloud Load Balancers](https://www.cloudskillsboost.google/course_templates/41/video/520397)

- [YouTube: Designing Google Cloud Load Balancers](https://www.youtube.com/watch?v=Rv2_crvSbGw)

There are two primary types of load balancers offered by Google Cloud, each designed for specific use cases. Application Load Balancers operate at the application layer (Layer 7) of the OSI model. They are ideal for applications that require load balancing based on HTTP or HTTPS headers, cookies, or URL paths. Application Load Balancers provide features like SSL TLS termination, session affinity, and content-based routing. Network Load Balancers operate at the network layer (Layer 4) of the OSI model. They are suitable for load balancing based on IP addresses and ports. Network Load Balancers are often used for TCP and UDP traffic, as well as for scenarios where low latency and high throughput are critical. They support features like TCP UDP load balancing and health checks. For more information on Load Balancers, please refer to the Cloud Load Balancing Overview documentation. With public IPs, traffic will be traversing the internet, so it is a best practice to use SSL for both Application Load Balancers and proxy Network Load Balancers. When configuring an Application Load Balancer, HTTP is the default protocol in the Google Cloud console for the frontend configuration, and HTTPS requires explicit selection together with the certificate. For a proxy Network Load Balancer, SSL is the default in the frontend configuration. Now, if you're using Application Load Balancing, you should leverage Cloud CDN to achieve lower latency and decrease egress costs. You can enable Cloud CDN by simply checking a box when configuring a global external Application Load Balancer. Cloud CDN caches content across the world using Google Cloud's edge-caching locations. This means that the content is cached closest to the users making the requests. The data that is cached can be from a variety of sources, including Compute Engine instances, GKE pods, or Cloud Storage buckets. To determine which Cloud Load Balancing product to use, you must first determine what traffic type your load balancers must handle. As a general rule, you'd choose an Application Load Balancer when you need a flexible feature set for your applications with HTTP(S) traffic. You'd choose a proxy Network Load Balancer to implement TLS offload, TCP proxy, or support for external load balancing to backends in multiple regions. You'd choose a passthrough Network Load Balancer to preserve client source IP addresses, avoid the overhead of proxies, and to support additional protocols like UDP, ESP, and ICMP. You can further narrow down your choices depending on your application's requirements: whether your application is external (internet-facing) or internal and whether you need backends deployed globally or regionally. If you prefer a table over a flow chart, we recommend this summary table. The load-balancing scheme is an attribute on the forwarding rule and the backend service of a load balancer and indicates whether the load balancer can be used for internal or external traffic. The term *_MANAGED in the load-balancing scheme indicates that the load balancer is implemented as a managed service either on Google Front Ends (GFEs) or on the open source Envoy proxy. In a load-balancing scheme that is *_MANAGED, requests are routed to either the GFE or to the Envoy proxy. For more information on Network Service Tiers, refer to the documentation. As part of our discussion in designing VPC networks, I also want to mention the Network Intelligence Center. The Network Intelligence Center is a Google Cloud service that can be used to visualize your VPC's network topology and test network connectivity. The left-hand graphic shows a simple network topology visualization with external clients in three different regions and traffic routed through an external load balancer to resources in us-central1. This facility is extremely valuable for confirming the network topology when configuring a network or performing diagnostics. The right-hand graphic shows the configuration of a connectivity test between a source and destination along with a protocol and port. The following tests can be performed: Between the source and destination endpoints in your (VPC) network, From your VPC network to and from the internet, From your VPC network to and from your on-premises network.

### Video - [Activity Intro: Defining network characteristics](https://www.cloudskillsboost.google/course_templates/41/video/520398)

- [YouTube: Activity Intro: Defining network characteristics](https://www.youtube.com/watch?v=Wedv9D52scQ)

Philipp: In this design activity, you specify the network characteristics for your case study and select the type of load balancer required for each service. In the first part of this activity, describe the network characteristics of each of your services by filling out this table. The example shown here is for the account service. Because this is a backend service, it will only be accessed internally using TCP, and we don't plan to deploy this service in multiple regions. Then based on the network characteristics for each of your services, select the right load balancer using this table. Based on the parameters from the last slide, we'll use the regional TCP load balancer. Refer to activities 8A and B in your design workbook to fill out similar tables for your services. And feel free to explore Cloud CDN to decrease latency and network egress costs.

### Video - [Activity Review: Defining network characteristics](https://www.cloudskillsboost.google/course_templates/41/video/520399)

- [YouTube: Activity Review: Defining network characteristics](https://www.youtube.com/watch?v=Uqp3ExwYW2c)

Philipp: In this activity, you were asked to specify the network characteristics of each of your services and choose the appropriate load balancers for each one. Here's a completed example for our online travel portal, Click Travel. The inventory and order service are internal and regional-using TCP. The other services need to be facing the Internet using HTTP. We decided to deploy these to multiple regions for lower latency, higher performance and high availability to our users who are in multiple countries around the world. Based on those network characteristics, we chose the global HTTP load balancer for our public-facing services and the internal TCP load balancer for our internal-facing services.

### Video - [Connecting Networks](https://www.cloudskillsboost.google/course_templates/41/video/520400)

- [YouTube: Connecting Networks](https://www.youtube.com/watch?v=H4a4vtP4Fjw)

Let's focus our attention on Google Cloud's network connectivity products, which are peering, Cloud VPN, and Cloud Interconnect. If you're trying to connect two VPC networks, you might want to consider VPC peering. VPC peering allows private RFC 1918 connectivity across two VPC networks regardless of whether they belong to the same project or the same organization. Now remember, each VPC network will have firewall rules that define traffic that is allowed or denied between the networks. This diagram shows a VPC peering connection between two networks belonging in different projects and different organizations. You might notice that the subnet ranges do not overlap. This is a requirement for a connection to be established. Speaking of the connection, network administrators for each VPC network must configure a VPC peering request for a connection to be established. Cloud VPN securely connects your on-premises network to your Google Cloud VPC network through an IPsec VPN tunnel. Traffic traveling between the two networks is encrypted by one VPN gateway and then decrypted by another VPN gateway. This protects your data as it travels over the public Internet. That's why Cloud VPN is useful for low volume data connections. As a managed service, Cloud VPN provides an SLA of 99.9 percent monthly uptime for the classic VPN Configuration and 99.99 percent monthly uptime for the high availability VPN configuration. The classic VPN gateways have a single interface and a single external IP address. Whereas the high availability VPN gateways have two interfaces with two external IP addresses, one for each gateway. The choice of VPN gateways comes down to your SLA requirements and routing options. Cloud VPN support, site-to-site VPN, static routes and dynamic routes using Cloud Router, and IKEv1 and IKEv2 ciphers. However, static routes are only supported by classic VPN. Also, Cloud VPN doesn't support use cases where clients computers need to dial in to a VPN using Client VPN software. For more information on the SLA and these features refer to the documentation. Let's walk through an example of Cloud VPN. This diagram shows a classic VPN connection between your VPC and on-premises network. Your VPC network has subnets in us-east1 and us-west, with the Google Cloud resources in each of those regions. These resources are able to communicate using their internal IP addresses because routing within a network is automatically configured, assuming that firewall rules allow the communication. Now, in order to connect your on-premises network and it's resources, you need to configure your Cloud VPN gateway, your on-premises VPN gateway, and two VPN tunnels. The Cloud VPN gateway is a regional resource that uses a regional external IP address. Your on-premises VPN gateway can be a physical device in your data center, or a physical or software based VPN offering in another Cloud providers network. This VPN gateway also has an external IP address. A VPN tunnel then connects your VPN gateways and serves as a virtual medium to which encrypted traffic is passed. In order to create a connection between two VPN gateways, you must establish two VPN tunnels. Each tunnel defines a connection from the perspective of it's gateway and traffic can only pass when a pair of tunnels is established. Now, one thing to remember when using Cloud VPN is the maximum transmission unit or MTU for your on-premises VPN gateway cannot be greater than 1,460 bytes. This is because of the encryption and the capsulation of packets. For more information on this MTU consideration, refer to the documentation. In addition to classic VPN, Google Cloud also offers a second type of Cloud VPN gateway, HA VPN. HA VPN is a high availability Cloud VPN solution that lets your security connect to your on-premises network, to your Virtual Private Cloud through IPSec VPN connection in a single region. HA VPN provides an SLA of 99.99 percent service availability. To guarantee a 99.99 percent availability SLA for HA VPN connections, you must properly configure two or four tunnels from your HA VPN gateway to your peer VPN gateway, or to another HA VPN gateway. When you create a HA VPN gateway, Google Cloud automatically chooses two external IP addresses. One for each of it's fixed number of two interfaces. Each IP address is automatically chosen from a unique address pool to support high availability. Each of the HA VPN gateway interfaces support multiple tunnels. You can also create multiple HA VPN gateways. When you delete the HA VPN gateway, Google Cloud releases the IP addresses for reuse. You can configure HA VPN gateway with only one active interface and one external IP address. However, this configuration does not provide a 99.99 percent service availability SLA. VPN tunnels connected to HA VPN gateways must use dynamic routing, BGP. Depending on the way that you configure root priorities for HA VPN tunnels, you can create an active active, or active passive routing configuration. HA VPN supports site-to-site VPN in one of the following recommended topologies or configuration scenarios. A HA VPN gateway to peer VPN devices, a HA VPN gateway to an Amazon Web Services virtual private gateway, or two HA VPN gateways connected to each other. Let's explore these configurations in a bit more detail. There are three typical peer gateway configurations for HA VPN. A HA VPN gateway to two separate peer VPN devices, each with its own IP address, a HA VPN gateway to one peer VPN device that uses two separate IP addresses, and a HA VPN gateway to one peer VPN device that uses one IP address. Let's walk through an example. In this topology, one HA VPN gateway connects to two peer devices. Each peer device has one interface and one external IP address. The HA VPN gateway uses two tunnels, one tunnel to each peer device. If your peer side gateway is hardware-based, having a second peer side gateway provides redundancy and fail-over on that side of the connection. A second physical gateway lets you take one of the gateways offline for software upgrades or for other scheduled maintenance. It also protects you if there is a failure in one of your devices. In Google Cloud, the redundancy type for this configuration takes the value two IPs redundancy. The example shown here provides 99.99 percent availability. When configuring a HA VPN external VPN gateway to Amazon Web Services, you can use either a transit gateway or a virtual private gateway. Only the transit gateway supports equal cost multipath or ECMP route. When enabled, ECMP equally distributes traffic across active tunnels. Let's walk through that example. In this topology, there are three major gateway components to set up for this configuration. A HA VPN gateway in Google Cloud with two interfaces, two AWS virtual private gateways, which connect to your HA VPN gateway, and an external VPN gateway resource in Google Cloud that represents your AWS virtual private gateway. This resource provides information to Google Cloud about your AWS gateway. The supported AWS configuration uses a total of four tunnels, two tunnels from one AWS virtual private gateway to one interface of the HA VPN gateway, and two tunnels from the other AWS virtual private gateway to the other interface of the HA VPN gateway. You can connect two Google Cloud VPC networks together using a HA VPN gateway in each network. The configuration shown provides 99.99 percent availability. From the perspective of each HA VPN gateway, you create two tunnels. You connect interface zero on one HA VPN gateway to interface zero on the other HA VPN gateway, and interface one on one HA VPN gateway to interface one on the other HA VPN gateway. For more information on HA VPN, refer to the documentation. For information on moving to HA VPN, also refer to the documentation. I mentioned earlier that Cloud VPN supports both static and dynamic routes. In order to use dynamic routes, you need to configure Cloud Routers. A Cloud Router can manage routes for a Cloud VPN tunnel using Border Gateway Protocol or BGP. This routing method allows for routes to be updated and exchanged without changing the tunnel configuration. This allows for new subnets like staging in the VPC network and rack 30 in the peer network to be seamlessly advertised between networks. If you need a dedicated high-speed connection between networks, consider using Cloud Interconnect. Cloud Interconnect has two options for extending on-premises networks. Dedicated Interconnect and Partner Interconnect. Dedicated Interconnect provides a direct connection to a co-location facility. The co-location facility must support either 10 gigabits per second or 100 gigabits per seconds circuits. A dedicated connection can bundle up to eight 10 gigabits per second connection or two 100 gigabits per second connection for a maximum of 200 gigabits per second. Partner Interconnect provides a connection through a service provider. This can be useful for lower bandwidth requirements, starting from 50 megabits per second. In both cases, Cloud Interconnect allows access to VPC resources using an internal IP address space. You can even configure Private Google Access for on-premises hosts to allow them to access Google services using private IP addresses. In order to use Dedicated Interconnect, you need to provision a cross connect between the Google network and your own router in a common co-location facility, as shown on this diagram. To exchange routes between the networks, you configure a BGP session over the interconnect between the Cloud Router and your on-premises router. This will allow for user traffic from the on-premises network to reach Google Cloud resources on the VPC network and vice-versa. Partner Interconnect provides connectivity between your on-premises network and your VPC network through a supported service provider. This is useful if your data center isn't a physical location that cannot reach a Dedicated Interconnect co-location facility or if your data needs don't warrant a Dedicated Interconnect.

### Video - [Activity Intro: Diagramming your network](https://www.cloudskillsboost.google/course_templates/41/video/520401)

- [YouTube: Activity Intro: Diagramming your network](https://www.youtube.com/watch?v=9YcREj3Iksc)

In this design activity, you draw a diagram that depicts the network requirements of your case study. Let me show you a simple example. This network diagram shows where the network boundaries are and how traffic is served from our users through a load balancer to our backend. We could also include the use of Cloud CDN, Cloud VPN, or any Cloud Interconnect services that are relevant to our network design. Refer to Activity 9 in your workbook to create a similar network diagram for your services.

### Video - [Activity Review: Diagramming your network](https://www.cloudskillsboost.google/course_templates/41/video/520402)

- [YouTube: Activity Review: Diagramming your network](https://www.youtube.com/watch?v=vH3twrn0K3M)

person: In this activity, you were asked to create a diagram that depicts the network requirements of your application. Here's an example for our online travel portal, ClickTravel. User traffic from mobile and web will first be authenticated using a third party service. Then, a global HTTP load balancer directs traffic to our public facing search and web UI services. From there, regional TCP load balancers direct traffic to the internal inventory and order services. The analytic service could leverage Bitquery as the data warehouse with an on-prem reporting service that accesses the analytic service over a VPN. This might be good enough to start, and we could refine this once we start implementing it.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/41/quizzes/520403)

#### Quiz 1.

> [!important]
> **You are deploying a large-scale web application with users all over the world and a lot of static content. Which load balancer configuration would likely be the most suitable?**
>
> - [ ] Network Load Balancer with SSL configured.
> - [ ] Application Load Balancer with SSL configured.
> - [ ] Network Load Balancer with SSL configured and the CDN enabled.
> - [ ] Application Load Balancer with SSL configured and the CDN enabled.

#### Quiz 2.

> [!important]
> **You are a large bank deploying an online banking service to Google Cloud. The service needs high volume access to mainframe data on-premises. Which connectivity option would likely be most suitable?**
>
> - [ ] HTTPS
> - [ ] Cloud Interconnect
> - [ ] Peering
> - [ ] VPN

#### Quiz 3.

> [!important]
> **You want a secure, private connection between your network and a Google Cloud network. There is not a lot of volume, but the connection needs to be extremely reliable. Which configuration below would you choose?**
>
> - [ ] VPN with high availability and Cloud Router.
> - [ ] Cloud Interconnect
> - [ ] VPC peering
> - [ ] VPN

#### Quiz 4.

> [!important]
> **You have a contract with a service provider to manage your Google VPC networks. You want to connect a network they own to your VPC. Both networks are in Google Cloud. Which Connection option should you choose?
**
>
> - [ ] Cloud Interconnect
> - [ ] VPC peering
> - [ ] VPN
> - [ ] VPN with high availability and Cloud Router.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/41/video/520404)

- [YouTube: Module Review](https://www.youtube.com/watch?v=RkEIE1GWrqY)

Philipp: In this module, you learned about Google Cloud networking and how to design networks that meet your application's security, performance, reliability, and scalability requirements. We also covered the different options to connect networks using peering, VPN, and Cloud Interconnect.

## Deploying Applications to Google Cloud

In this module, we discuss the different options of deploying applications to Google Cloud. Google Cloud offers many possible deployment platforms, and the choice is not always immediately obvious.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/41/video/520405)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=r4bI4m8-jPE)

In this module, we discuss the different options of deploying applications to Google Cloud. Google Cloud offers many possible deployment platforms, and the choice is not always immediately obvious. Let me give you a high level overview of how you could decide on the most suitable platform for your application. First, ask yourself whether you have specific machine and OS requirements. If you do, then Compute Engine is the platform of choice. If you have no specific machine or operating system requirements, then the next question to ask is whether you are using containers. If you are, then you should consider Google Kubernetes Engine or Cloud Run, depending on whether you want to configure your own Kubernetes cluster. If you are not using containers, then you want to consider Cloud Run functions if your service is event-driven, and App Engine if it's not. We'll talk through each of these services in this module, and you will get to explore them in a lab. Let's get started.

### Video - [Google Cloud Infrastructure as a Service](https://www.cloudskillsboost.google/course_templates/41/video/520406)

- [YouTube: Google Cloud Infrastructure as a Service](https://www.youtube.com/watch?v=0VO7ZTXdZ7Y)

person: Let's begin by talking about Compute Engine, which is Google Cloud's infrastructure as a service or IaaS offering. Compute Engine is a great solution when you need complete control over your operating systems, or if you have an application that is not containerized-- an application built on a microservice architecture or an application that is a database. Instance groups and autoscaling, as shown on this slide, allow you to meet variations in demand on your application. Let's take a closer look at instance groups. Managed instance groups create VMs based on instance templates. Instance templates are just a resource used to define VMs and manage instance groups. The templates define the boot disk image or container image to be used, the machine type, labels, and other instance properties Iike a startup script to install software from a Git repository. The virtual machines in a managed instance group are created by an instance group manager. Using a managed instance group offers many advantages, such as auto healing to recreate instances that don't respond and creating instances in multiple zones for high availability. I recommend using one or more instance groups as the backend for load balancers. If you need instance groups in multiple regions, use a global load balancer and if you have static content, simply enable cloud CDN, as shown on the right.

### Video - [Google Cloud Deployment Platforms](https://www.cloudskillsboost.google/course_templates/41/video/520407)

- [YouTube: Google Cloud Deployment Platforms](https://www.youtube.com/watch?v=UQlUjAZfh8s)

Let's go through the other deployment platforms: GKE, Cloud Run, App Engine, and Cloud Run functions. Google Kubernetes Engine, or GKE, provides a managed environment for deploying, managing, and scaling containerized applications using Google's infrastructure. The GKE environment consists of multiple Compute Engine virtual machines grouped together to form a cluster. GKE clusters are powered by the Kubernetes open source cluster management system. Kubernetes provides the mechanisms with which to interact with the cluster. Kubernetes commands and resources are used to deploy and manage applications, perform administration tasks and set policies, and monitor the health of deployed workloads. This diagram on the right shows the layout of a Kubernetes cluster. A cluster consists of at least one cluster control plane and multiple worker machines that are called nodes. These control plane and node machines run the Kubernetes cluster orchestration system. Pods are the smallest, most basic deployable objects in Kubernetes. A pod represents a single instance of a running process in a cluster. Pods contain one or more containers, such as Docker containers, that run the services being deployed. You can optimize resource use by deploying multiple services in the same cluster. GKE has the Autopilot and Standard modes of operation, which offer different levels of flexibility, responsibility, and control. Google recommends the fully-managed Autopilot mode, in which Google manages your nodes for you and provides a workload-focused, cost-optimized, production-ready experience. For more information, see Choose a GKE mode of operation You can deploy a container image stored in Artifact Registry to Cloud Run. You can also deploy directly from source to Cloud Run, which includes automatically creating a container image for your built source and storing the image in Artifact Registry. When deploying from source code, Cloud Run can automatically, containerize local source code, push the container image to an Artifact Registry repository, deploy the container image to Cloud Run from the repository, and push and pull images using the repository cloud-run-source-deploy, in the region that you specify at deploy time. If the repository does not exist, Cloud Run creates it for you if your account has the required permissions. You can also deploy a container image by tag or digest that is stored in Artifact Registry. Deploying to a service for the first time creates its first revision. Note that revisions are immutable. If you deploy from a container image tag, it will be resolved to a digest and the revision will always serve this particular digest. You can deploy a container using Google Cloud console or the gcloud command line. App Engine is a fully managed, serverless application platform supporting the building and deploying of applications. Applications can be scaled seamlessly from zero upward without having to worry about managing the underlying infrastructure. App Engine was designed for microservices. For configuration, each Google Cloud project can contain one App Engine application, and an application has one or more services. Each service can have one or more versions, and each version has one or more instances. App Engine supports traffic splitting so it makes switching between versions and strategies such as canary testing or A/B testing simple. The diagram on the right shows the high-level organization of a Google Cloud project with two services, and each service has two versions. These services are independently deployable and versioned. Let me show you a typical App Engine microservice architecture. This could be an example of a retailer that sells online. Here App Engine serves as the frontend for both web and mobile clients. The backend of this application is a variety of Google Cloud storage solutions with static content such as images stored in Cloud Storage, Cloud SQL used for structured relational data such as customer data and sales data, and Firestore used for NoSQL storage such as product data. Firestore has the benefit of being able to synchronize with client applications. Memcache is used to reduce the load on the datastores by caching queries, and Cloud Tasks are used to perform work asynchronously outside a user request (or service-service request). There's also a batch application that generates data reports for management. Cloud Run functions are a great way to deploy loosely coupled, event-driven microservices. They've been designed for processing events that occur in Google Cloud. The functions can be triggered by changes in a Google Cloud Storage bucket, a Pub/Sub message, or an HTTP request. The platform is completely managed, scalable, and inexpensive. You do not pay if there are no requests, and processing is paid for by execution time in 100ms increments. This graphic illustrates an image translation service implemented with Cloud Run functions. When an image is uploaded to a Cloud Storage bucket, it triggers an OCR Cloud Run function that identifies text in the image using Google Cloud's Vision API. Once the text has been identified, this service then publishes a message to a Pub/Sub topic for translation, which triggers another Cloud Run function that will translate the identified text in the image using the Cloud Translation API. After that, the translator Cloud Run function will publish a message to a file write topic in Pub/Sub, which triggers a Cloud Run function that will write the translated image to a file. This sequence illustrates a typical use case of Cloud Run functions for event-based processing.

### Video - [Lab Intro: Deploying Apps to Google Cloud](https://www.cloudskillsboost.google/course_templates/41/video/520408)

- [YouTube: Lab Intro: Deploying Apps to Google Cloud](https://www.youtube.com/watch?v=67YVGKECmDM)

In the first lab of this course, you used Cloud Build to create Docker images and store those images in Container Registry. If you follow the 12-Factor best practices when writing your applications, you should be able to create applications that are portable across different cloud providers and also portable between different deployment services provided by the cloud. That's why, in this lab, you deploy code to App Engine, Google Kubernetes Engine, and Cloud Run. Let me quickly go over why we chose those services for this lab. Deploying your Dockerized application to a virtual machine using Compute Engine in the first lab was easy, but that might not be the most effective option. A more automated way might be using App Engine, which will be the first compute platform of this lab. App Engine is great for those who just want to focus on their code and not worry at all about the underlying infrastructure, like networks, load balancers, and autoscalers, which are completely managed by App Engine. Now, sometimes developers want more freedom to customize their environments. Google Kubernetes Engine, or GKE, provides a balance where you have a lot of customization over your environment, similar to Compute Engine. However, GKE also helps you optimize your spend by allowing you to deploy multiple services into the same cluster of virtual machines. This provides an excellent balance between flexibility, portability, and cost optimization. Kubernetes can get pretty complicated though. That's where Cloud Run comes in. Cloud Run allows you to deploy your own stateless Dockerized services onto Kubernetes clusters that are managed by Google. Google Cloud takes care of the hard parts of managing the cluster and configuring the load balancers, autoscalers, and health checkers, so that you just focus on the code. Deploying a single application on all of these platforms might help you choose the right platform for your own services.

### Lab - [Deploying Apps to Google Cloud](https://www.cloudskillsboost.google/course_templates/41/labs/520409)

In this lab, you will deploy applications to the Google Cloud services App Engine, Kubernetes Engine, and Cloud Run.

- [ ] [Deploying Apps to Google Cloud](../labs/Deploying-Apps-to-Google-Cloud.md)

### Video - [Lab Review: Deploying Apps to Google Cloud](https://www.cloudskillsboost.google/course_templates/41/video/520410)

- [YouTube: Lab Review: Deploying Apps to Google Cloud](https://www.youtube.com/watch?v=Wl0bazNx-zo)

In this lab, you saw how to deploy an application to App Engine, GKE, and Cloud Run. Hopefully, this lab gives you a better feel for how each of these compute platforms work, and will help you choose the right platform for your own services. I recommend that you follow the 12-Factor best practices that we covered earlier in this course, automate as much as possible by using continuous integration and infrastructure as code tools, and containerize your services using Docker. If you do that, your apps will be very portable, and deployment will become easier and more flexible no matter which compute platform you use, you can stay for a lab walkthrough. But remember that Google Cloud's user interface can change, so your environment might look slightly different. So here, I'm in the Cloud Console, and the first thing I'm going to do is activate Cloud Shell. And with any project, you'll have to say continue. And I like opening this in a new window. So we're going to do is we're going to create a directory and navigate to it. And then we're going to clone a GitHub repository that we have. Let me make a directory, we're going to navigate to that directory. Then we're going to clone our GitHub repository. We use this for a lot of our other courses. It's a public GitHub repository. And the benefit of that is it actually allows you to contribute. So if you think there could be a great modification that we can make either because something's broken, or we could enhance something, you can actually make a pull request. And we'll review that. Now, we're cloning that. And once that is done, we're going to navigate to the folder that it has created. And we're just going to test the program that is in here, the Python Flask app. So let me navigate to that. And then we're going to launch this. Here it is running, it says it's running on port 8080, and Cloud Shell gives you a web preview button. So we can actually go in and see what that looks like. And it's currently just saying, "Hello GCP" So somewhere there's a file in here that defines that. Great, so we can now close that. So that's really the first step. We just wanted to see what this application is. And now we're going to move on and deploy this to App Engine, Kubernetes Engine, and Cloud Run, and look at some of the differences and similarities between those. Now, first we're going to do App Engine. For that, I'm going to open the editor. And we're going to create a file called app.yaml. And in that file, we need to define the runtime. App Engine actually runs with multiple languages, you can use Python, Java, JavaScript, and Go. So I'm going to define that by going into here: courses, design and process. It's the same directory that we are currently in down here. And here, by the way, if I go to main.py, we can see this is where it said "Hello GCP" So within this folder, I'm going to create a new file and call it app.yaml. And in there, I'm going to define the runtime. Now, there are other settings that we could specify here, but in this case, the language runtime is the required one. So let's go to File > Save, we're going to make this a little bit bigger so we can see a bit more here. And now we're going to run the gcloud-app-create command. And for that, we need to specify the region where this app is created. So we're going to use the instructions to specify this is in us-central. And it is telling us it currently hasn't picked up the project. This is actually a pretty good error that you might get. So let's actually work through this. So it currently says that the project property is set to an empty string, and that's invalid. Great. So let's go to gcloud config set project, it's telling us to run that command. Let me run that. And I'm going to go grab my project ID by going to the Cloud Console. And I actually have it here. Or I could go in here and grab it as well. Or you could even grab it from your quick labs UI. So let me go ahead and set that. I just updated that. Now let me just try to run that command again. And now we can see that it's creating an App Engine application for that project in the region that we specified. Now, once this is complete, we're going to deploy the app, we're just initially going to call it version one. So let's do that. And this is going to take a couple minutes. And once this is complete, we're going to go into App Engine and actually test this program. All right. So we can see that the command is complete. So now I'm going to go to Cloud Console by switching tabs, and we're going to navigate to App Engine. So we go to the navigation menu, App Engine. And I'm going to create some more space here, hide some of these things. So here we see our dashboard. We currently have one version. It's serving all of our traffic. And if I click on this link, which by the way, by default, this URL is always going to be in the format of HTTPS, the project ID appspot.com. So let me click on that. And we should see "Hello GCP" So we now have this running on App Engine. Great. So let's actually make a change to the program and see how easy App Engine makes managing versions. So I'm going to go back to Cloud Shell. And I mentioned earlier that this main.py file here says Hello GCP, and that's what we're seeing. So let's change this to Hello App Engine. Okay. So we're going to make the change and save it. What we need to do now is we need to redeploy this. But rather than overriding our existing version, let's actually create a new version. So I'm going to run the command gcloud app deploy version-two. And I'm going to use the no-promote parameter to tell App Engine to continue serving the requests to the old version, version one. And this is great, because it allows us to test a new version before putting it into production. So let's wait for this to complete. Okay, so we can see that this has been completed. And if I go back to my application by just changing tabs to it, and click the Refresh button, you see it still says Hello GCP. And that's because version one is the one that's currently serving all the traffic. So let's explore that more. I'm going to go back to App Engine, and specifically the dashboard. And on the left here, I can click on versions. And if I click refresh, we can see that we currently have two versions. But the original one, version one is still serving all of our traffic. So what we can do now is we could test this. And once it's ready, we can migrate our traffic. So let me actually test this. I can click on version two, and I can see that, yeah, this says Hello App Engine. And obviously, this is a very simple application, you would want to do some more testing here. But once I'm comfortable with this, I can go back to versions. And I can go to split the traffic. Traffic allocated to two. And there are some different options here, you can split by IP address, Cookie, or random. And I'm going to click Save, because I'm going to say this is going to receive 100% of the traffic. Now I could, if I add a version, I could actually split and split by in this case, I'm really just saying, Hey, all traffic should go to version two. So this is going to take a minute or two now to complete. And once this is done, it actually says saved successfully. So let's go back, we can see that it is saying that all the traffic is there. And now if I go back to the dashboard and use the link, we can see that it now says Hello App Engine. Okay. That's it for deploying this in App Engine. So now what we're going to do is we're going to go ahead and deploy this to Kubernetes Engine. And Kubernetes Engine allows you to create a cluster of machines and deploy any number of applications to it. So before we get into that, I'm going to close some of these tabs just to make some space here. And we're going to go now in the navigation menu to Kubernetes Engine, so navigation menu, it's under Compute, click on Kubernetes Engine. And within here, we're going to reate a cluster. There's a lot of different settings in here, this course is not really an introduction course on Kubernetes. There's this new experience here that you can now use, we're just going to accept the defaults. So we see just it has a name, it has a region, you can specify the version of the master and nodes and so on. But I'm just going to keep all the defaults for now. And just click Create. Because really, what we're doing is we're comparing how to deploy an application to App Engine, Kubernetes Engine, and Cloud Run. So once this cluster is up and running, we're going to connect to it, which is really going to give us some commands for Cloud Shell. So let's wait for this cluster to be ready. And while you wait, feel free to go to the navigation menu and head over to Compute Engine. And if you refresh this here- we're not quite here yet- you'll see the different nodes that are going to be created because they're actually Compute Engine instances. So if I go maybe back, refresh, we could sit in here for a while. And then we'll see that we actually get some Compute Engine instances. Let's wait for that to complete and come back. So here we can see that the Kubernetes cluster is now completed, it's running. And if I head back to Compute Engine, as I showed earlier, here you can see the actual nodes. So Kubernetes Engine is using Compute Engine virtual machines as the nodes, but they're being managed for you. And if you remember back in the previous lab, you actually launch the container on a Compute Engine VM instance. Here we are now using Kubernetes Engine to have all of this managed for us. So we want to connect to this. So if we go to Kubernetes Engine, we can see there's a Connect button here. And here's the command line way of connecting, it's very specifically for the cluster that we have the zone it's in and the project. I could run this in Cloud Shell with this button, or I can just copy this, and go to my existing Cloud Shell session over here, and paste the command in there. So we're going to run that. And now we can see this configured. And we're going to now run the command, kubectl get nodes. And this is just going to show us the node. So here we can see, again, these nodes. And these are the same that I can see in Compute Engine as well. So now we're going to want to make some changes to this main.py file. I'm already in the code editor. And rather than this saying Hello App Engine and Kubernetes, I would like it to say Hello Kubernetes Engine. So let's type that in there and let's save that. Now, we haven't deployed anything to the cluster yet. So we're going to have to do that now. Specifically, we're going to create a configuration file in yaml format. So within the deploying-apps-to-gcp folder, I'm going to right click and say New File, give it the name kubernetes-config.yaml. And we already have a configuration for you. Again, this course is not about how to really work with Kubernetes, we're really just comparing, but if we scroll through here, we can see some of the configurations in here, you can see information on the front end, we see information about the container and the image. We actually don't have an image yet, so we're going to have to put that there. We can see there's going to be a LoadBalancer on port 80. So it's it's just an HTTP application. All right. So this, again, first section of this yaml file is really about the configuration, we're deploying three instances. So three replicas of the Python web app. We have these image attributes again. And if you want more information, the lab instructions actually have some links on deployment and creating the external load balancer. So we see here this load balancer. In our case, we want to now really just create, and we're going to build a Docker image. So for that, let's first make sure we're in the right folder. We can copy that in there, which we still are. And then we're going to run the command gcloud builds summit. And that's going to take a project ID, which is again an environment variable. And we're now going to create an image and we're going to call it version 0.2. So we're going to let this build now. And at the end, we should see the image with our project ID, the name and the version. And we're then going to copy that and we're going to paste it into our kubernetes-config.yaml file. Alright, so the image is complete, and it's right here. So we can now take that and paste it into our configuration file. So there we go. And we can save that. And now we're going to have to apply this configuration. So the command for that is in the lab instructions, kubectl apply, and we specify the yaml file. Great. So now we can look at the pods, there should be three replicas. And here we can see that the container is being created. It's only been eight seconds, so we can wait for that. You want to make sure that you wait for all of these to be ready. And you can just keep querying to check on the status of these. So here, you can see I queried just 20 seconds later and now all of these are running. Now, the configuration file also mentions a load balancer. So if we go down here, we can see a LoadBalancer. So let's actually get to the services. And here we have an external IP address. So let's go grab that external IP address, and navigate to it and see if we can already see this application. So I'm going to just open a new tab. Navigate to that by typing the external IP address. And there we can see it says Hello Kubernetes Engine. And that really completes task three. So we've seen App Engine, we've seen Kubernetes Engine, there's one more to go. Let's move on to Cloud Run. Now, Cloud Run simplifies and automates deployments to Kubernetes. So when you use Cloud Run, you don't need a configuration file, you simply choose a cluster for your application. And with Cloud Run, you can use a cluster managed by Google or you can use your own Kubernetes cluster. So we want to do the same thing we did before, we want to change this main.py file and then we're going to have to, in this case, need to create a new Docker image. So let's go back to Cloud Shell. Again, I don't need a configuration file, I needed that both for App Engine and for Kubernetes Engine. So I'm just going to main.py, and call this Hello Cloud Run, and Save. And now to use Cloud Run, we need to build a Docker image. So in Cloud Shell, we're going to enter some commands to use Cloud Build to create the image and store it in our Container Registry. So let me just clear this here. And let me make sure I navigate to the right directory, and then create a new image here. And once that is complete, we're going to go to Cloud Run and create a service from here. So we can see the build completed. And here we have the image. So now let's navigate to Cloud Run. So I'm going to switch to the Cloud Console, go to the navigation menu, and under Compute, select Cloud Run. Now, in case Cloud Run was enabled, which is the case here, we can see Start Using Cloud Run, this is going to enable the API. You can see it's enabling the Cloud Run API, I can close this panel here. And we're going to now create a service. It's telling us the API has been enabled. So let's do that one more time. And we're going to now select the container image. So under cloud-run-image, we can see this image that we just created just now. Let's go, click Continue. And then there are some other things we can set here where this is running and so on. And let's just expand all these options. And the most important one that we're going to select is the 'Allow unauntheticated invocations', because we're creating a public API website. We can scroll all the way down, we can leave all these other defaults and click Create. Now, it really shouldn't take too long for this service to deploy. And we should see a green check once this is ready. And then we can click on the URL that will automatically be generated and test it, and hopefully it will say Hello Cloud Run. Okay, this took maybe one to two minutes, which was not as long as the Kubernetes cluster, but it still takes some time. You can see the green checkmark here. We see the URL. So if I click on that, you can see Hello Cloud Run. And that's the end of the lab.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/41/quizzes/520411)

#### Quiz 1.

> [!important]
> **You have containerized multiple applications using Docker and have deployed them using Compute Engine VMs. You want to save on costs and simplify container management. What might you do?**
>
> - [ ] Rewrite the applications to run in Cloud Run functions.
> - [ ] Write Terraform scripts for all deployment.
> - [ ] Migrate the containers to GKE.
> - [ ] Rewrite the applications to run in App Engine standard environment.

#### Quiz 2.

> [!important]
> **You need to deploy an existing application that was written in .NET version 4. The application requires Windows servers, and you don't want to change it. Which should you use?**
>
> - [ ] App Engine
> - [ ] GKE
> - [ ] Compute Engine
> - [ ] Cloud Run functions

#### Quiz 3.

> [!important]
> **You've been asked to write a program that uses Vision API to check for inappropriate content in photos that are uploaded to a Cloud Storage bucket. Any photos that are inappropriate should be deleted. What might be the simplest, cheapest way to deploy in this program?**
>
> - [ ] Cloud Run functions
> - [ ] GKE
> - [ ] Compute Engine
> - [ ] App Engine

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/41/video/520412)

- [YouTube: Module Review](https://www.youtube.com/watch?v=IAN70eQ8CCA)

Stephanie: In this module, we covered the various deployment services provided by Google. These include Compute Engine if you need complete control over your deployment environment, Google Kubernetes Engine if you want the flexibility, portability, and automation that is provided by Kubernetes, and App Engine and Cloud Run if you want a completely managed platform as a service. Again, each of these choices has advantages and disadvantages. Make sure you understand each one, so that you can make an informed decision when deploying your services.

## Designing Reliable Systems

In this module, we talk about how to design reliable systems.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/41/video/520413)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=1Zl0bj_Lseg)

Stephanie: In this module, we talk about how to design reliable systems. Specifically, we'll go over how to design services to meet requirements for availability, durability, and scalability. We will also discuss how to implement fault-tolerant systems by avoiding single points of failure, correlated failures, and cascading failures. Overload can also be a challenge in distributed systems, and we will see how to avoid overload failures by using design patterns such as the circuit breaker and truncated exponential backoff. How to design resilient data storage with lazy deletion is introduced. When things go wrong, we want our system to stay in control, so we discuss design decisions for different operational states, including normal, degraded, and failure scenarios. Finally, we cover how to analyze disaster scenarios and plan, implement, and test for disaster recovery.

### Video - [Key Performance Metrics](https://www.cloudskillsboost.google/course_templates/41/video/520414)

- [YouTube: Key Performance Metrics](https://www.youtube.com/watch?v=qcfvsijw_AY)

person: Let's start by considering the key performance metrics for reliable systems. When designing for reliability, consider availability, durability, and scalability as the key performance metrics. Let me explain each of these. Availability is the percent of time a system is running and able to process requests. To achieve high availability, monitoring is vital. Health checks can detect when an application reports that it is okay. More detailed monitoring of services using white box metrics to count traffic successes and failures will help predict problems. Building in fault tolerance by, for example, removing single points of failure, is also vital for improving availability. Backup systems also play a key role in improving availability. Durability is the chance of losing data because hardware or system failure. Ensuring that data is preserved and available is a mixture of replication and backup. Data can be replicated in multiple zones. Regular restores from backup should be performed to confirm that the process works as expected. Scalability is the ability of a system to continue to work as user load and data grow. Monitoring and autoscaling should be used to respond to variations in load. The metrics for scaling can be the standard metrics, like CPU or memory, or you can create custom metrics, like number of players on a game server.

### Video - [Designing for Reliability](https://www.cloudskillsboost.google/course_templates/41/video/520415)

- [YouTube: Designing for Reliability](https://www.youtube.com/watch?v=2DkXJoaMZC0)

Person: Now that we've covered the key performance metrics, let's design for reliability. Avoid single points of failure by replicating data and creating multiple virtual machine instances. It is important to define your unit of deployment and understand its capabilities. To avoid single points of failure, you should deploy two extra instances or N plus two to handle both failure and upgrades. These deployments should ideally be in different zones to mitigate for zonal failures. Let me explain the upgrade consideration. Consider three BMs that are load balanced to achieve N plus two. If one is being upgraded and another fails, 50 percent of the available capacity of the compute is removed, which potentially doubles the load on the remaining instance and increases the chance of that failing. This is where capacity planning and knowing the capability of your deployment unit is important. Also, for ease of scaling, it is a good practice to make the deployment units interchangeable, stateless clones. It is also important to be aware of correlated failures. These occur when related items fail at the same time. At the simplest level, if a single machine fails, all requests served by that machine fail. At a hardware level, if a top-of-rack switch fails, the complete rack fails. At the cloud level, if a zone or region is lost, all the resources are unavailable. Servers running the same software suffer from the same issue. If there's a fault in the software, the service may fail at a similar time. Correlated failures can also apply to configuration data. If a global configuration system fails and multiple systems depend on it, they potentially fail too. When we have a group of related items that could fail together, we refer to it as a failure or fault domain. Several techniques can be used to avoid correlated failures. It is useful to be aware of failure domains, then servers can be decoupled using microservices distributed among multiple failure domains. To achieve this, you can divide business logic into services based on failure domains and deploy to multiple zones and/or regions. At a finer level of granularity, it is good to split responsibilities into components and spread these over multiple processes. This way a failure in one component will not affect other components. If all responsibilities are in one component, a failure in one responsibility has a high likelihood of causing all responsibilities to fail. When you design microservices, your design should result in loosely coupled, independent but collaborating services. A failure in one service should not cause a failure in another service. It mail cause a collaborating service to have reduced capacity or not be able to fully process its workflows, but the collaborating service remains in control and does not fail. Cascading failures occur when one system fails, causing others to be overloaded and subsequently fail. For example, a message queue could be overloaded because a backend fails and it can not process messages placed on the queue. The graphic on the left shows a cloud load balancer distributing load across two backend servers. Each server can handle a maximum of 1,000 queries per second. The load balancer is currently sending 600 queries per second to each instance. If server B now fails, all 1,200 queries per second have to be sent to just server A, as shown on the right. This is much higher than the specified maximum and could lead to a cascading failure. So, how do we avoid cascading failures? Cascading failures can be handled with support from the deployment platform. For example, you can use health checks in Compute Engine or readiness and liveliness probes in GKE to enable the detection and repair of unhealthy instances. You want to ensure that new instances start fast and ideally do not rely on other backends or systems to start up before they are ready. The graphic on this slide illustrates a deployment with four servers behind a load balancer. Based on the current traffic, a server failure can be absorbed by the remaining three servers as shown on the right-hand side. If the system uses Compute Engine with instance groups and auto-healing, the failed server would be replaced with a new instance. As I just mentioned, it's important for that new server to start up quickly to restore full capacity as quickly as possible. Also, this setup only works for stateless services. You also want to plan against query of death, where a request made to a service causes a failure in the service. This is referred to as a query of death because the error manifests itself as overconsumption of resources, but in reality is due to an error in business logic itself. This can be difficult to diagnosis and requires good monitoring, observability and logging to determine the root cause of the problem. When the requests are made latency, resource, utilization and error rates should be monitored to help identify the problem. You should also plan against positive feedback cycle overload failure where a problem is caused by trying to prevent problems. This happens when you try to make the system more reliable by adding retries in the event of a failure. Instead of fixing the failure, this creates the potential for overload. You may be actually adding more load to an already overloaded system. The solution is intelligent retries that make use of feedback from the service that is failing. Let me discuss two strategies to address this. If a service fails, it is okay to try again, however, this must be done in a controlled manner. One way to use the exponential backoff pattern. This performs a retry, but not immediately. You should wait between retry attempts, waiting a little longer each time a request fails, therefore giving the failing service time to recover. The number of tries should be limited to a maximum and the length of time before giving up should also be limited. As an example, consider a failed request to a service. Using exponential backoff, we may wait 1 second plus a random number of milliseconds and try again. If the request fails again, we wait 2 seconds plus a number of milliseconds and try again. Fail again, then wait 4 seconds plus a random number of milliseconds before retrying and continue until a maximum limit is reached. The circuit breaker pattern can also protect the service from too many retries. The pattern implements a solution for when a service is in a degraded state of operation. It is important because if a service is down or overload and all its clients are retrying, the extra requests actually make matters worse. The circuit breaker design pattern protects the service behind a proxy that monitors the service health. If the service is not deemed healthy by the circuit breaker, it will not forward requests to the service. When the service becomes operational again the circuit breaker will begin feeding requests to it again in a controlled manner. If you are using GKE, the Istio service mesh automatically implements circuit breakers. Lazy deletion is a method that builds in the ability to reliably recover data when a user deletes the data by mistake. With lazy deletion, a deletion pipeline similar to that shown in this graphic is initiated and the deletion progresses in phase. The first stage is that the user deletes the data but it can be restored within a pre-defined time period. In this example, it's 30 days. This protects against mistakes by the user. When the pre-defined period is over, the data is no longer visible to the user but moves to the soft deletion phase. Here the data can be restored by user support or administrators. This deletion protects against any mistakes in the application. After the soft deletion period of 15, 30, 45 or even 60 days, the data is deleted and no longer available. The only way to restore the data is by whatever backups or archives were made of the data.

### Video - [Activity Intro: Designing Reliable, Scalable Applications](https://www.cloudskillsboost.google/course_templates/41/video/520416)

- [YouTube: Activity Intro: Designing Reliable, Scalable Applications](https://www.youtube.com/watch?v=bC9TQ-eco-Q)

In this design activity, you draw a diagram that depicts how you can deploy your application for high availability, scalability, and durability. Let me show you an example of what to draw. This diagram is for an online bank application with customers in the United States. I want the web UI to be highly available, so here I depict it as being deployed behind a global external Application Load Balancer across multiple regions and multiple zones within each region. I chose us-central-1 as the main region, because it's somewhere in the middle of the U.S. I also have a backup region in us-east-1, which is on the east coast of the U.S. I deploy the accounts and product services as backends to just the us-central-1 region, but I'm using multiple zones, us-central1-a and us-central1-b, for high availability. I even have a failover Cloud SQL database. The Firestore database for the products service is multi-regional, so I don't need to worry about a failover. In case of a disaster, I'll keep backups in a multi-regional Cloud Storage bucket. That way if there is a regional outage, I can restore in another region. Refer to Activity 10 in your workbook to create a similar diagram for your services.

### Video - [Activity Review: Designing Reliable, Scalable Applications](https://www.cloudskillsboost.google/course_templates/41/video/520417)

- [YouTube: Activity Review: Designing Reliable, Scalable Applications](https://www.youtube.com/watch?v=HEY3ACNByxA)

In this activity, you are asked to draw a diagram that depicts how your application can be deployed for high availability, scalability, and durability. For our online travel application, ClickTravel, I'm assuming that this is an American company, but that I have a large group of customers in Europe. I want the UI to be highly available, so I've placed it into us-central1 and europe-west2 behind a global external Application Load Balancer. This load balancer will send user requests to the region closest to the user unless that region cannot handle the traffic. I could also deploy the backends globally, but I'm trying to optimize cost. I could start by just deploying those in us-central1. This will create latency for our European users, but I can always revisit this later and have a similar backend in europe-west2. To ensure high availability, I've decided to deploy the Orders and Inventories services to multiple zones. Because the Analytics service is not customer-facing, I can save some money by deploying it to a single zone. I again have a failover Cloud SQL database and the Firestore database and BigQuery data warehouse are multi-regional, so I don't need to worry about a failover for those. In case of a disaster, I'll keep backups in a multi-regional Cloud Storage bucket. That way, if there's a regional outage, I can restore it in another region.

### Video - [Disaster Planning](https://www.cloudskillsboost.google/course_templates/41/video/520418)

- [YouTube: Disaster Planning](https://www.youtube.com/watch?v=ns-Qw3FwiZ0)

Person: Now that we've designed for reliability, let's explore disaster-planning. High availability can be achieved by deploying to multiple zones in a region. When using Compute Engine for higher availability, you can use a regional instance group which provides built-in functionality to keep instances running. Use auto healing with an application health check and load balancing to distribute load. For data, the storage solution selected will affect what is needed to achieve high availability. For Cloud SQL, the database can be configured for high availability which provides data redundancy and a standby instance of the database server in another zone. This diagram shows a high availability configuration with a regional managed instance group for a web application that's behind a load balancer. The master Cloud SQL instance is in us-central 1-a, with a replica instance in us-central 1-f. Some data services, such as Firestore or Spanner, provide high availability by default. In the previous example, the regional managed instance group distributes VMs across zones. You can choose between single zones and multiple zones or regional configurations when creating your instance group, as you can see in this screenshot. Google Kubernetes Engine clusters can also be deployed to either a single or multiple zones, as shown in this screenshot. A cluster consists of a master controller and collections of node pools. Regional clusters increase the availability of both a clusters master and its nodes by replicating them across multiple zones of a region. If you are using instance groups for your service, you should create a health check to enable auto healing. The health check is a test endpoint in your service. It should indicate that your service is available and ready to accept requests and not just that the server is running. A challenge with creating a good health check endpoint is that if you use other back-end services, you need to check that they are available to provide positive confirmation that your service is ready to run. If the services it is dependent on are not available, it should not be available. If a health check fails the instance group, it will remove the failing instance and create a new one. Health checks can also be used by the load balancers to determine which instances to send requests to. Let's go over how to achieve high availability for Google Cloud's data storage and database services. For Google Cloud Storage, you can achieve high availability with multi-region storage buckets if the latency impact is negligible. As this table illustrates, the multi-region availability benefit is a factor of two, as the unavailability decreases from 0.1 percent to 0.05 percent. If you are using Cloud SQL and need high availability, you can create a failover replica. This graphic shows the configuration where a master is configured in one zone and a replica is created in another zone but in the same region. If the master isn't available, the failover will automatically be switched to take over the master. Remember that you are paying for the extra instance with this design. Firestore and Spanner both offer single and multi-region deployments. A multi-region location is a general geographical area, such as the United States. Data in a multi-region location is replicated in multiple region. Within a region, data is replicated across zones. Multi-region locations can withstand the loss of entire regions and maintain availability without losing data. The multi-region configurations for both Firestore and Spanner offer five 9s of availability which is less than 6 minutes of downtime per year. Now, I already mentioned that deploying for high availability increases costs because extra resources are used. It is important that you consider the costs of your architectural decisions as part of your design process. Don't just estimate the cost of the resources used, but also consider the cost of your service being down. This table shown is a really effective way of assessing the risk versus cost, by considering the different deployment options and balancing them against the cost of being down. Now let me introduce some disaster recovery strategies. A simple disaster recovery strategy may be to have a cold standby. You should create snapshots of persistent disks, machine images and data backups and store them in a multi-region storage. This diagram shows a simple system using this strategy. Snapshots are taken that could be used to recreate the system. If the main region fails, you can spin up service in the backup region using the snapshot images and persistent disks. You will have to route requests to the new region, and it's vital to document and test this recovery procedure regularly. Another disaster recovery strategy is to have a hot standby, where instance groups exist in multiple regions, and traffic is forwarded with a global load balancer. This diagram shows such a configuration. I already mentioned this, but you can also implement this for data storage services like multi-regional Cloud Storage buckets and database services like Spanner and Firestore. Now, any disaster recovery plan should consider its aims in terms of two metrics, the recovery point objective and the recovery time objective. The recovery point objective is the amount of data that would be acceptable to lose, and the recovery time objective is how long it can take to be back up and running. You should brainstorm scenarios that might cause data loss or service failures and build a table similar to the one shown here. This can be helpful to provide structure on the different scenarios and to prioritize them accordingly. You will create a table like this in the upcoming design activity, along with the recovery plan. You should create a plan for how to recover based on the disaster scenarios that you define. For each scenario, devise a strategy based on the risk and recovery point and time objectives. This isn't something that you want to simply document and leave. You should communicate the process for recovering from failures to all parties. The procedure should be tested and validated regularly, at least once per year, and ideally, recovery becomes a part of daily operations which helps streamline the process. This table illustrates the backup strategy for different resources, along with the location of the backups and the recovery procedure. This simplified view illustrates the type of information that you should capture. Before we get into our next design activity, I just want to emphasize how important it is to prepare a team for disaster by using drills. Have you decided what you think can go wrong with your system? Think about the plans for addressing each scenario and document these plans. Then practice these plans periodically in either a test or a production environment. At each stage, assess the risks carefully and balance the costs of availability against the cost of unavailability. The cost of unavailability will help you evaluate the risk of not knowing the system's weaknesses.

### Video - [Activity Intro: Disaster planning](https://www.cloudskillsboost.google/course_templates/41/video/520419)

- [YouTube: Activity Intro: Disaster planning](https://www.youtube.com/watch?v=ahFKYf1aNVM)

Philipp: In this design activity, you brainstorm disaster scenarios for your case study and formulate a disaster recovery plan. Here's an example of such a brainstorming activity. The purpose of this activity is to think of disaster scenarios and assess how severely they would impact your services. The recovery point objective represents the maximum amount of data you would be willing to lose. Obviously, this would be different for your orders database, where you wouldn't want to lose any data, and your products rating database, where you could tolerate some loss. The recovery time objective represents how long it would take to recover from a disaster. In this example, we are estimating that we could recover our product rating service database from a backup in an hour and our orders service within two minutes. Now, like all things in life, we should prioritize. There's never time to get everything done, so work on the highest priority items first. Once you've identified these scenarios, come up with plans for recovery. These plans will be different based on your recovery point and recovery time objectives, as well as the priorities. In this example, performing daily backups of our ratings database is good enough to meet our objectives. For the orders database, that won't be adequate, so we'll implement a failover replica in another zone. Refer to activities 11A, B, and C in your design workbook to document similar disaster scenarios for your case study and formulate a disaster recovery plan for each.

### Video - [Activity Review: Disaster planning](https://www.cloudskillsboost.google/course_templates/41/video/520420)

- [YouTube: Activity Review: Disaster planning](https://www.youtube.com/watch?v=JrASbBKqhH4)

Philipp: In this activity, you were asked to come up with disaster recovery scenarios and plans for the case study you've been working on. Here's an example of some disaster scenarios for our online travel portal, ClickTravel. Each of our services use different database services and has different objectives and priorities. All of that affects how we design our disaster recovery plans. Our analytics service in BigQuery had the lowest priority, therefore, we should be able to reimport data to rebuild analytics tables if a user deletes them. Our orders service can't tolerate any data loss and has to be up and running almost immediately. For this we need a failover replica in addition to binary logging and automated backups. Our inventory service uses Firestore, and for that we can implement daily automated backups to a multi-regional Cloud Storage bucket. Cloud Functions and Cloud Schedule can help with this recovery procedure.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/41/quizzes/520421)

#### Quiz 1.

> [!important]
> **You need a relational database for a system that requires extremely high availability (99.999%). The system must run uninterrupted even in the event of a regional outage. Which database would you choose?**
>
> - [ ] Cloud SQL
> - [ ] BigQuery
> - [ ] Spanner
> - [ ] Firestore

#### Quiz 2.

> [!important]
> **You're creating a service and you want to protect it from being overloaded by too many client retries in the event of a partial outage. Which design pattern would you implement?**
>
> - [ ] Truncated exponential backoff
> - [ ] Circuit breaker
> - [ ] Lazy caching
> - [ ] Overload feedback repudiation

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/41/video/520422)

- [YouTube: Module Review](https://www.youtube.com/watch?v=MsiX4cbrBWc)

In this module, we covered how to deploy our applications for high availability, durability, and scalability. For high availability, we can deploy our resources across multiple zones and regions. For durability, we can keep multiple copies of data and perform regular backups. And for scalability, we can deploy multiple instances of our services and set up autoscalers. We also introduced disaster recovery planning, which is defined by coming up with the scenarios that would cause you to lose data and having a plan in place for recovery if a disaster happens.

## Security

In this module, we cover security. Google has been operating securely in the cloud for 20 years. There is a strong belief that security empowers innovation. The approach of the cloud architect should be that security should be put first; everything else will follow from this.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/41/video/520423)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=GzDzM9hl4PA)

In this module, we cover security. Google has been operating securely in the cloud for 20 years. There's a strong belief that security empowers innovation. The approach of the cloud architect should be that security should be put first. Everything else will follow from this. In this module, you will learn how to design secure systems using industry best practices. For example, when designing a secure system, you will learn how to apply the principle of least privilege and the practice of separation of concerns. Performing regular audits is also a key part of running a secure system. Leveraging Google's Security Command Center can help to identify vulnerabilities as part of your process. Also when securing systems, governance is a major consideration. You will learn how organization policies and folders can simplify governance. Preventing access to unwanted visitors is always a challenge. Authentication and authorization is one approach and can be implemented in many different ways. You will learn how best to use IAM roles, Identity-Aware proxy and Identity Platform. You'll also learn to manage the access and authorization of resources by machines and processes using service accounts. At a lower level of access control, you will learn how to secure network access using private IP's, firewalls, and Google Cloud private access. Last but not least, you'll learn how to mitigate DDoS attacks by leveraging Cloud DNS and Google Cloud Armor. As you see there is a lot to security. Let's get started.

### Video - [Security Concepts](https://www.cloudskillsboost.google/course_templates/41/video/520424)

- [YouTube: Security Concepts](https://www.youtube.com/watch?v=VXPZ0nlHwO0)

Let's begin by talking about some security concepts and introducing some of the best practices for security design. When you move an application to Google Cloud, Google handles many of the lower layers of the overall security stack. Because of its scale, Google can deliver a higher level of security at these layers than most of its customers could afford to do on their own. This does not mean that Google is responsible for all the security aspects. Google Cloud security is a shared responsibility between you and Google. So it is important that there is a clear separation of duties, and there is no ambiguity between what is provided by the platform and what you are responsible for. For this, there needs to be transparency. There are certain actions you as a client are responsible for ,and some that Google is responsible for. Google Cloud provides the controls and features required to leverage the platform together with the tools to monitor your services. Google implements security in layers. At the base is custom built hardware and servers that are loaded using a verified boot loading system. All the way through the stack, security is at the forefront. When you take your part in security, for example, establishing firewall rules or configuring IAM, as long as they are configured correctly, you have a safe environment. There are tools Google Cloud provides that can be used for monitoring and auditing your networks, which we will discuss shortly, or you can also install your own tools. Let's talk about some best practices when implementing security. The principle of least privilege is the practice of granting a user only the minimal set of permissions required to perform a duty. This should apply to machine instances and processes, as well as users. Google Cloud provides cloud IAM to help apply this principle. You can use it to identify users with their login or identify machines using service accounts. Roles should be assigned to users and service accounts to restrict what they can do, always following the principle of least privilege. Separation of duties is another best practice and it has two primary objectives. One, prevention of conflict of interest and two, the detection of control failures. For example, security breaches and information theft. From a practical perspective, this means that no one person can change or delete data without being detected. No one person can steal sensitive data and no single person is in charge of designing, implementing, and reporting on sensitive systems. For example, a developer who writes the code should not be responsible for deploying that code, and anybody that has the permission to deploy should not be able to change the code. One approach to achieve this separation of duties in Google Cloud is to use multiple projects to separate duties. Different people can be given suitable rights to different projects, with these permissions following the principle of separation of duties. Folders are especially useful for organizing multiple projects. It is also vital to audit Google Cloud logs to discover attacks and potential security breaches. All Google Cloud services write to audit logs, so there is a rich source of information available. These logs include admin, data access, VPC flow, firewall, and system logs. So an in-depth view of activity is provided for audit. Now, moving to the cloud often requires maintaining compliance with regulatory requirements, or guidelines. Google Cloud meets many third party and government compliance standards worldwide. While Google cloud has been certified as secure, for example to ISO/IEC 27001, HIPAA and SOC 1, that does not mean your application running on Google Cloud is certified. Your concern should always be on what you build. Google Cloud also offers the Security Command Center, which provides access to organizational and project security configuration. As as you can see in this screenshot, the Security Command Center provides a dashboard that reports security health analysis, threat detections, anomaly detection, and a summary report. Once a threat is detected, a set of actionable recommendations is provided.

### Video - [Securing People](https://www.cloudskillsboost.google/course_templates/41/video/520425)

- [YouTube: Securing People](https://www.youtube.com/watch?v=-5jBmUDkrYo)

Let's move on to talk about securing people. When granting people access to your projects, you should add them as members and assign them one or more roles. Roles are simply a list of permissions. To see what permissions are granted to roles, use the Cloud Console as shown on the right. Here, you can see the role BigQuery user and the associated 15 permissions the role has assigned to it. You can assign these predefined roles to its members or customize your own roles. Now, any member added to your project will be identified by their login. For simplifying management of members and their permissions, I recommend that you create groups. That way, you just need to add members to a group and new members automatically acquire the permissions of the group. The same applies for removing members from a group, which also removes the permissions of that group. I also recommend using organizational policies and folders to simplify securing your environments and managing your resources. Organizational policies apply to all resources underneath an organization. Cloud IAM policies are also inherited top to bottom as shown on the right. Folders inherit policies of the organization, projects inherit policies of the folders, and so on. I already mentioned that roles should be granted to groups, not individuals, because it simplifies management. Make sure to define groups carefully and make them more granular than job roles. It's better to use multiple groups for better control. When it comes to roles, it's better to use predefined roles over custom roles. Google has defined the roles for a reason, and it should be an exceptional case that a role does not fit your use case. When granting roles, remember the principle of least privilege. Always grant the smallest scope required. Owner and editor roles should be limited. These are not or should not be required by the majority of users. I also recommend leveraging Cloud Identity-Aware Proxy or Cloud IAP. Cloud IAP provides managed access to applications running in App Engine standard environment, App Engine flexible environment, Compute Engine, and GKE. It allows employees to securely access web-based applications deployed on Google Cloud without requiring a VPN. Administrators control who has access and users are required to log on to gain access to the applications. The screenshots on the right show Cloud IAP being enabled on an App Engine application and the dialogue for adding new members or permissions. Google Cloud also offers Identity Platform as a Customer Identity and Access Management, CIAM platform for adding Identity and Access Management to applications. In other words, Identity Platform provides sign up and sign-in for end user applications. Now, you need to select a service provider to use Identity Platform. A broad range of protocol support is available, including SAML, OpenID, e-mail and password, phone, social, and Apple. This graphic shows a part of the configuration with a list of potential providers.

### Video - [Securing Machine Access](https://www.cloudskillsboost.google/course_templates/41/video/520426)

- [YouTube: Securing Machine Access](https://www.youtube.com/watch?v=eZLSqNIaH9c)

We just talked about assigning roles to members. We focused on users and Google Groups, but there's another kind of member that helps secure machine access. A service account is a special kind of account used by an application, a virtual machine instance, or a GKE node pool. Applications or services use service accounts to make authorized API calls. The service account is the identity of the service and defines permissions which control the resources the service can access. A service account is both an identity and a resource. A service account is used as an identity for your application or service to authenticate, for example, a Compute Engine VM running as a service account. To give the VM access to the necessary resources, you need to grant the relevant Cloud IAM roles to the service account. At the same time, you need to control who can create VMs with the service account so random VMs cannot assume the identity. Here, the service account is the resource to be permissioned. You assign the ServiceAccountUser role to the users you trust to use the service account. Each service account is associated with public/private RSA key-pairs that are used to authenticate to Google. These keys can be Google-managed or user-managed. Google-managed keys, both the public and private keys, are stored by Google, and they are rotated regularly. The maximum usage period is two weeks. For user-managed keys, the developer owns both public and private keys. They can be used from outside Google Cloud. User-managed keys can be managed by the Cloud IAM API, gcloud command-line tool, or the service accounts page in the Cloud Console. It is possible to create up to 10 key-pairs per service account to support key rotation. User-managed keys are extremely powerful credentials, and they can represent a security risk if they are not managed correctly. You can limit their use by applying the constraints/iam.disableServiceAccountKeyCreation Organization Policy Constraint to projects, folders, or even your entire organization. After applying the constraint, you can enable user-managed keys in well-controlled locations. To minimize the potential risk caused by unmanaged keys, consider using Cloud Key Management Service (Cloud KMS) to help securely manage your keys. The slide shows the generation of a key using the Cloud Console. The private key can be seen in the screenshot. It's your responsibility for storing the private key securely. For developers to gain controlled access to resources without acquiring access to the Cloud Console, it is possible to configure the gcloud command-line utility to use service account credentials to make requests. The command on this slide, gcloud auth activate-service-account, serves the same purpose as gcloud auth login, but uses the service account instead of user credentials. The key file contains the private key in JSON format, which I just discussed.

### Video - [Network Security](https://www.cloudskillsboost.google/course_templates/41/video/520427)

- [YouTube: Network Security](https://www.youtube.com/watch?v=qQ4CbEbTTE0)

In our previous module, we talked about networks, but didn't get into a lot of network security concepts. Let's do that now. First, I recommend removing external IPs to prevent access to machines from outside their network whenever possible. Several options are available for securely communicating with VMs that do not have public IP addresses. These services do not have a public IP address because they are deployed to be consumed by other instances in the project, or maybe through dedicated interconnect options. However, for those instances that do not have an external IP address, it can be a requirement to gain external access, for instance, for updates or patches to be applied. The options for accessing the VMs include a bastion host for external access to private machines, Identity-Aware Proxy to enable SSH access or Cloud NAT to provide egress to the Internet for internal machines. The diagram on the right shows an external client accessing Compute Engine resources via a bastion host. The host is behind a firewall where access can be filtered. Whichever method you choose, all internet traffic should terminate at a load balancer, third-party firewall or API Gateway or through Cloud IAP. That way, internal services cannot be launched and get public IP addresses. Now, VM instances that only have internal IP addresses can use private Google access to access Google services that have external IP addresses. The diagram on the right shows a Compute Engine instance accessing a Cloud storage bucket using its internal IP address. Private Google access must be enabled when creating the subnet. You can achieve this either with the G Cloud command shown here or through the Cloud Console. Regardless of whether you're VM instances have public IP addresses, you should always configure a firewall rules to control access. By default, ingress on all ports is denied and all egress is allowed. It's your responsibility to define separate rules to allow or deny access to specific instances for specific IP ranges, protocols, and ports. This graphic shows some scenarios where firewall rules can be configured. Egress from Compute Engine to external servers is the first scenario. For ingress, firewall rules should be configured if direct access to an instance is being provided or if via a load balancer. The right-hand graphic shows the scenario of VM instance to instance communication. Firewall rules should be considered here to control access also. Remember, you're still responsible for application level security. If you need to manage APIs, you can use Cloud Endpoints. Endpoints is an API management gateway that helps you develop, deploy, and manage APIs on any Google Cloud backend. It provides functionality to protect and monitor your public APIs, control who has access, using, for example, Auth0 zero and validate every call with a JSON Web Token signed with the service account private key. Cloud Endpoints also integrates with Identity Platform for authentication. All Google Cloud Service Endpoints use HTTPS. I recommend that you use TLS for your service endpoints and it is your responsibility to configure your service endpoints for TLS. When configuring load balancers, only ever create secure front ends. This dialog shows the configuration of a front end and the protocol selected is HTTPS, with the certificate also being selected. Google provides Infrastructure DDoS support through global load balancers at level 3 and level 4 traffic. If you have enabled CDN, this will also protect backend resources because a DDOS results in a cache hit instead of hitting your resources as shown on the right. We already mentioned Google Cloud armor in the networking module. For additional features over the built-in DDoS protection, you can use Google Cloud armor to create network security policies. For example, you can create allow lists that allow known/required addresses through and deny lists to block known attackers. This dialog shows a typical security policy configuration where you begin by selecting it as an allow list or a deny list with allow or deny for the rule. If it's a deny, the appropriate action in this example should be a 403 error. In addition to layer 3 and layer 4 security, Google Cloud armor supports layer 7 application rules. For example, predefined rules are provided for cross-site scripting, XSS, and SQL injection attacks. Google Cloud armor provides a rules language for filtering request traffic. As an example, consider the first expression on this slide. In IP range origin.ip 9.9.9.0/24. In this case, the expression returns true if the origin IP and our request is within the 9.9.9.0/24 range. The second line, request.headers['cookie'] . contains('80=BLAH'), returns true if the cookie 80 with value "blah" exists in the request header and the third line is true if the origin region code is AU. The expressions can be combined logically with logical AND, and OR. The expressions are all assigned to an allow or deny rule that is then applied to incoming traffic.

### Video - [Encryption](https://www.cloudskillsboost.google/course_templates/41/video/520428)

- [YouTube: Encryption](https://www.youtube.com/watch?v=KQB-67rvLYU)

Last but certainly not least, let's go over encryption. Google Cloud encrypts customer data stored at rest by default with no additional action required from users. A data encryption key or DEK using AES 256 symmetric key is used and the key itself is encrypted by Google using a key encryption key, KEK. This is so that the DEK can be stored local to the encrypted data for fast decryption with no visible performance impact to the user. To protect the KEKs they are stored in Cloud KMS. The keys are rotated periodically and automatically for added security. This diagram shows a simple App Engine application that uses Cloud Storage. The data is encrypted using AES 256 using a DEK and decrypted transparently to the application where the data is read. Now, for compliance reasons you may need to manage your own encryption keys rather than the automatically generated keys as just discussed. In this scenario you can use Cloud Key Management Service or Cloud KMS to generate what are known as customer managed encryption keys, CMEK. These keys are stored in Cloud KMS for direct use by cloud services. You can manually create the key using a dialogue similar to the one shown here and specify the rotation frequency which defaults to 90 days. The keys you create can then be used when creating storage resources such as disks or buckets. When you're required to generate your own encryption key or manage it on premises, Google Cloud supports customer supplied encryption keys, CSEK. Those keys are kept on premises and not in Google Cloud. The keys are provided as part of API service calls and Google only keeps the key in memory and uses it to decrypt a single payload or block of returned data. Currently, customer supplied encryption keys can be used with Cloud Storage and Compute Engine. You should also consider the Data Loss Prevention API to protect sensitive data by finding it and redacting it. Cloud DLP provides fast scalable classification and redaction for sensitive data elements like credit card numbers, names, social security numbers, US and selected international identifier numbers, phone numbers, and Google Cloud credentials. Cloud DLP classifies this data using more than 90 predefined detectors to identify patterns, formats, and checksums, and even understands contextual clues. Some of these are shown on the right. You can optionally redact data as well, using techniques like masking, secure hashing, tokenization, bucketing, and format preserving encryption.

### Video - [Activity Intro: Modeling Secure Google Cloud Services](https://www.cloudskillsboost.google/course_templates/41/video/520429)

- [YouTube: Activity Intro: Modeling Secure Google Cloud Services](https://www.youtube.com/watch?v=yWDHFLvYKk8)

In this design activity, you draw a diagram that depicts your case study's security requirements. Let me show you an example of what to draw. This diagram illustrates a custom VPC network with two subnets in the US. Maybe us-central1 is our primary region and us-east1 is our backup region. The firewall rules allow HTTPS ingress from the internal and SSH from known sources. Otherwise, all other incoming traffic is disabled by the implied deny all ingress firewall rule that every VPC network has. Because we're allowing HTTPS from anywhere, it's useful to configure Google Cloud Armor on a global external Application Load Balancer to block any denied IP addresses at the edge of Google Cloud's network. This is a simple design, but a great starting point because it allows us to grow our backends without changing our security design. Refer to Activity 12 in your workbook to create a similar diagram for your case study.

### Video - [Activity Review: Modeling Secure Google Cloud Services](https://www.cloudskillsboost.google/course_templates/41/video/520430)

- [YouTube: Activity Review: Modeling Secure Google Cloud Services](https://www.youtube.com/watch?v=GT37ftmQP2M)

In this activity, you were asked to draw a diagram depicting the security requirements for your case study. Here's the diagram that I drew for our online travel portal, ClickTravel. This is a similar design to what I showed you earlier. First, I configured Google Cloud Armor on a global external Application Load Balancer to block any denied IP addresses. My custom VPC network has subnets in us-central1 for my American customers, and a backup subnet in us-east1, and a subnet in europe-west2 for my European customers. My firewall rules only allow SSH from known sources, and although I allow HTTPS from anywhere, I can always deny IP addresses with Google Cloud Armor at the edge of Google Cloud's network. I also configured Cloud VPN tunnels to securely communicate with my on-premises network for my Reporting service. Now, while my load balancer needs a public IP address, I can secure my backend services by creating them without external IP addresses. In order for those instances to communicate with the Google Cloud database services, I enable Private Google Access. This enables the Inventory, Orders, and Analytic services' traffic to remain private, while reducing my networking costs.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/41/quizzes/520431)

#### Quiz 1.

> [!important]
> **You don't want programmers to have access to production resources. What's the easiest way to do this in Google Cloud?**
>
> - [ ] Use different service accounts for production and development resources with your project.
> - [ ] Set up private access and Identity-Aware Proxy.
> - [ ] Create a firewall rule that blocks developer access to production servers and databases.
> - [ ] Create development and production projects, and don't give developers access to production.

#### Quiz 2.

> [!important]
> **What Google Cloud service can you use to enforce the principle of least privilege when using Google Cloud?**
>
> - [ ] IAM members and roles
> - [ ] Firewall rules
> - [ ] SSL certificates
> - [ ] Encryption keys

#### Quiz 3.

> [!important]
> **Which Google Cloud features could help reduce the risk of DDoS attacks?**
>
> - [ ] Global external Application Load Balancer
> - [ ] Cloud CDN
> - [ ] All of these
> - [ ] Google Cloud Armor

#### Quiz 4.

> [!important]
> **What do you have to do to enable encryption when using Cloud Storage?**
>
> - [ ] Nothing as encryption is enabled by default.
> - [ ] Simply enable encryption when configuring a bucket.
> - [ ] Enable encryption and upload a key.
> - [ ] Create an encryption key using Cloud Key Management Service, and select it when creating a Cloud Storage bucket.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/41/video/520432)

- [YouTube: Module Review](https://www.youtube.com/watch?v=TGRMG_Ucs04)

In this module, we covered how to secure our Google Cloud resources. This includes securing both the network and our stored data. We also covered how to secure people using IAM, Cloud Identity, and Identity Aware Proxy, and how we can secure our applications and machines using service accounts. Remember, security should be put first. Everything else will follow from this.

## Maintenance and Monitoring

In this final module of this course, we cover application maintenance and monitoring.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/41/video/520433)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=Yy8q-bdomvo)

In this final module of this course, we cover application maintenance and monitoring. Maintenance is primarily concerned with how updates are made to running applications, the different strategies available, and how different deployment platforms support them. For monitoring, I discuss this vital area for cloud-native applications from two perspectives. First, I will talk about the cost perspective to make sure that resources are being best provisioned against demand. After all, why should you pay for resources that you don't need? Second, I will discuss how to implement monitoring and observability to determine and alert on the health of services and applications using Cloud Monitoring and Dashboards. This will also allow us to define Uptime Checks and use Cloud Monitoring Alerts to identify service outages. Let's get started.

### Video - [Managing Versions](https://www.cloudskillsboost.google/course_templates/41/video/520434)

- [YouTube: Managing Versions](https://www.youtube.com/watch?v=FfTBbZ6ce2w)

Person: Let's begin by taking a look at version management. The key benefit of a microservice architecture is the ability to independently deploy microservices. This means that the service API has to be protected. Versioning is required, and when new versions are deployed care must be taken to ensure backward compatibility with a previous version. Some simple design rules can help, such as indicating the version in the URI and making sure you change the version when you make a backward and compatible change. Deploying new versions of software always carries risk. We want to make sure we test new versions effectively before going live, and when ready to deploy a new version, we do so with zero downtime. Let me discuss some strategies that can help achieve these objectives. Rolling updates allow you to deploy new versions with no downtime. The typical configuration is to have multiple instances of a service behind a load balancer. A rolling update will then update one instance at a time. This strategy works fine if the API is not changed, or is backward compatible or if it is okay to have two versions of the same service running during the update. If you are using instance groups, rolling updates are a built-in feature. You just define the rolling update strategy when you perform the update. For Kubernetes, rolling updates are there by default. We just need to specify the replacement Docker image. Finally, for App Engine, rolling updates are completely automated. Use a blue-green deployment when you don't want multiple versions of a service to run simultaneously. Blue-green deployments use twofold deployment environments. The blue deployment is running the current deployed production software while the green-deployment environment is available for deploying updated versions of the software. When you want to test a new software version, you deploy it to the green environment. Once testing is complete, the workload is shifted from the current, which would be the blue in this case, to the new, the green environment. This strategy mitigates the risk of a bad deployment by allowing they switchback to the previous deployment if something goes wrong. For Compute Engine, you can use DNS to migrate requests, while in Kubernetes you can configure your service to route to new pods using labels, which is just a simple configuration change. App Engine allows you to split traffic, which you explored in the previous lab of this course. Now, you can use Canary releases prior to rolling update to reduce risk. With a Canary release, you make a new deployment with the current deployment still running. Then you send a small percentage of traffic to the new deployment and monitor it. Once you have confidence in your new deployment, you can route more traffic to the new deployment until 100 percent is routed this way. In Compute Engine, you can create a new instance group and add it to the load balancer as an additional back end. In Kubernetes, you can create a new pod with the same labels as the existing pods. The service will automatically divert a portion of the request to the new pod. In App Engine, you can again use the traffic splitting feature to drive a portion of traffic to the new version.

### Video - [Cost Planning](https://www.cloudskillsboost.google/course_templates/41/video/520435)

- [YouTube: Cost Planning](https://www.youtube.com/watch?v=cqQxDUAU1eU)

Cost planning is an important phase in your design that starts with capacity planning. I recommend that you treat capacity planning not as a one off task, but as a continuous, iterative cycle, as illustrated on this slide. Start with a forecast that estimates the capacity needed. Monitor and review this forecast. Then allocate by determining the resources required to meet the forecasted capacity. This allows you to estimate costs and balance them against risks and rewards. Once the design and cost is approved, deploy your design and monitor it to see how accurate your forecasts were. This feeds into the next forecast as the process repeats. A good starting point for anybody working on cost optimization is to become familiar with the VM instance pricing. It is often beneficial to start with a couple of small machines that can scale out through auto scaling as demand grows. To optimize the cost of your virtual machines, consider using committed use discounts, as these can be significant. Also, if your applications are fault tolerant and can withstand possible instance preemptions, then Spot instances can reduce your Compute Engine costs by up to 91%. Compute Engine provides sizing recommendations for your VM instances, as shown on the right. This is a really useful feature that can help you select the right size of VM for your workloads and optimize costs. For more details on Spot VMs see the Google Cloud documentation. A common mistake is to over-allocate disk space. This is not cost-efficient, but selecting a disk is not just about size. It is important to determine the performance characteristics your applications display: the I/O patterns, do you have large reads, small writes, vice versa, mainly only read data? This type of information will help you select the correct type of disk. As the table shows, SSD persistent disks are significantly more expensive than standard persistent disks. Understanding your I/O patterns can help provide significant savings. To optimize network costs, it is best practice to keep machines as close as possible to the data they need to access. This graphic shows the different types of egress: within the same zone, between zones in the same region, intercontinental egress, and internet egress. It is important to be aware of the egress charges. These are not all straightforward. Egress in the same zone is free. Egress to a different Google Cloud service within the same region using an external IP address or an internal IP address is free, except for some services such as Memorystore for Redis. Egress between zones in the same region is charged and all internet egress is charged. One way to optimize your network costs is to keep your machines close to your data. Another way to optimize cost is to leverage GKE usage metering, which can prevent over-provisioning your Kubernetes clusters. With GKE usage metering, an agent collects consumption metrics in addition to resource requests by polling PodMetrics objects from the metrics server. The resource request records and resource consumption records are exported to two separate tables in a BigQuery dataset that you specify. Comparing requested with consumed resources makes it easy to spot waste and take corrective measures. This graphic shows a typical configuration where BigQuery is used for request-based metrics collected from the usage metering agent and, together with data obtained from billing export, it is analyzed in a Looker Studio dashboard. Earlier in this course, we talked about all of the different storage services. It's important to compare costs of the different options as well as their characteristics. In other words, your storage and database service choice can make a significant difference on your bill. Your architectural design can also help optimize your costs. For example, if you use Cloud CDN for static content or Memorystore as a cache, you can save instead of allocating more resources. Similarly, instead of using a datastore between two applications, consider message/queuing with Pub/Sub to decouple communication between services and reduce storage needs. The pricing calculator should be your go-to resource for estimating costs. Your estimates should be based on your forecasting and capacity planning. The tool is great for comparing costs between compute and storage services, and you will use it in the upcoming design activity. To monitor the cost of your existing service, leverage the Cloud Billing Reports page as shown here. This report shows the changes in cost compared to the previous month, and you can use the filters to search for particular projects, products, and regions, as shown on the right. The sizing recommendations for your Compute Engine instances will also be in this report. For advanced cost analysis I recommend exporting your billing data to BigQuery, as shown in this screenshot. You can then analyze the billing data to identify large expenses and optimize your Google Cloud spend. For example, let's assume you label VM instances that are spread across different regions. Maybe these instances are sending most of their traffic to a different continent, which could incur higher costs. In that case, you might consider relocating some of those instances or using a caching service like Cloud CDN to cache content closer to your users, which reduces your networking spend. You can even visualize spend over time with Looker Studio, which turns your data into informative dashboards and reports that are easy to read, easy to share, and fully customizable. The service data is displayed in a daily and monthly view, providing at-a-glance summaries that can also be drilled down to provide greater insights. To help with project planning and controlling costs, you can set a budget. Setting a budget lets you track how your spend is growing towards that amount. Set a budget name and specify which project this budget applies to. Set the budget at a specific amount or match it to a previous month's spend. Set the budget alerts. These alerts send emails to Billing Admins after spend exceeds a percentage of the budget or a specified amount. In our case, it would send an email when spending reaches 50%, 90%, and 100% of the budget amount. You can even choose to send an alert when the spend is forecasted to exceed the percent of the budget amount by the end of the budget period. In addition to receiving an email, you can use Pub/Sub notifications to programmatically receive spend updates about this budget. You could even create a Cloud Run function that listens to a Pub/Sub topic to automate cost management.

### Video - [Monitoring Dashboards](https://www.cloudskillsboost.google/course_templates/41/video/520436)

- [YouTube: Monitoring Dashboards](https://www.youtube.com/watch?v=AP0MDMbpIHk)

Let's get into monitoring and visualizing information with dashboards. Google Cloud unifies the tools you need to monitor your service SLOs and SLAs in real time. These tools include Monitoring, Logging, Trace, Error Reporting, and Profiler. All of these enable you to gain the insights you need to achieve your SLOs and to determine the root cause in those rare cases that you do not achieve your SLOs. Dashboards are one way for you to view and analyze metric data that is important to you. This includes your SLIs to ensure that you are meeting your SLAs. The Monitoring page of the Cloud Console automatically provides pre-defined dashboards for the resources and services that you use. It is important that you monitor these things you pay for to determine the trends, bottlenecks, and potential cost savings. Here's an example of some charts in a Monitoring dashboard. On the left, you can see the CPU usage for different Compute Engine instances. And on the right is the ingress traffic for those instances. Charts like these provide valuable insights into usage patterns. To help you get started, Cloud Monitoring creates default dashboards for your project resources as shown in this screenshot. You can also create custom dashboards which you can explore in the upcoming lab. Now, it's a good idea to monitor latency because it can quickly highlight when problems are about to occur. As shown on the slide, you can easily create uptime checks to monitor the availability and latency of your services. So far, there is a 100% uptime with no outages. Latency is actually one of the four golden signals called out in Google's Site Reliability Engineering, or SRE book. SRE is a discipline that applies aspects of software engineering to operations whose goals are to create ultra-scalable and highly reliable software systems. This discipline has enabled Google to build, deploy, monitor, and maintain some of the largest software systems in the world. I've linked the SRE book in the slides of this module. Your SLO will be more strict than your SLA, so it is important to be alerted when you are not meeting an SLO because it's an early warning that the SLA is under threat. Here's an example of what creating an alerting policy looks like. On the left, you can see an HTTP check condition on the summer01 instance. This will send an email that is customized with the content of the documentation section on the right.

### Video - [Activity Intro: Cost estimating and planning](https://www.cloudskillsboost.google/course_templates/41/video/520437)

- [YouTube: Activity Intro: Cost estimating and planning](https://www.youtube.com/watch?v=MyX2fiG1l54)

person: In this design activity, use Google Cloud's pricing calculator to create an initial estimate for deploying your case study application. The pricing calculator gives you a form for each service, which you fill out to estimate the cost of using that service. For example, in this screenshot, I calculated the cost of one custom SQL instance with four cores, 16 gigabytes of RAM, and 500 gigabytes of SSD storage. This could represent the orders database of my online travel application. Some of these estimates aren't easy to generate, because you might not know how much data your storage and database services need and how much compute your deployment platforms require. However, it can be more challenging to estimate things like network egress or the number of reads and writes. Start with a rough estimate and refine it as your capacity plans improve. Refer to activity 13 in your workbook for similar cost estimates for your case study.

### Video - [Activity Review: Cost estimating and planning](https://www.cloudskillsboost.google/course_templates/41/video/520438)

- [YouTube: Activity Review: Cost estimating and planning](https://www.youtube.com/watch?v=anycyQqJoCM)

person: In this activity, you were asked to use the Google Cloud pricing calculator to estimate the cost of your case study application. Here's a rough estimate for the database applications of my online travel portal, ClickTravel. I adjusted my Orders database to include a failover replica for high availability and came up with some high-level estimates for my other services. My Inventory services uses Cloud Storage to store JSON data stored in text files. Because this is my most expensive service, I might want to reconsider the storage class or configure Object Lifecycle Management. Again, this is just an example, and your cost would depend on your case study.

### Video - [Lab Intro: Monitoring Applications in Google Cloud](https://www.cloudskillsboost.google/course_templates/41/video/520439)

- [YouTube: Lab Intro: Monitoring Applications in Google Cloud](https://www.youtube.com/watch?v=35mYPEZF5o4)

- We started this course with a discussion on defining SLOs and SLIs for your services. This helps with the detail design and architecture and helps developers know when they're done implementing a service. However, the SLIs and SLOs aren't very useful if you don't monitor your applications to see whether you are meeting them. That's where the monitoring tools come in. In this lab you will see how to use some of these tools. Specifically, you will examine logs, view profile information, explore tracing, monitor your resources using dashboards, and create uptime checks and alerts.

### Lab - [Monitoring Applications in Google Cloud](https://www.cloudskillsboost.google/course_templates/41/labs/520440)

In this lab, you will deploy an application to GCP and then use the tools provided by Google Cloud to monitor it. You will use Cloud Logging, Trace, Profiler, and dashboards and create uptime checks and alerting policies.

- [ ] [Monitoring Applications in Google Cloud](../labs/Monitoring-Applications-in-Google-Cloud.md)

### Video - [Lab Review: Monitoring Applications in Google Cloud](https://www.cloudskillsboost.google/course_templates/41/video/520441)

- [YouTube: Lab Review: Monitoring Applications in Google Cloud](https://www.youtube.com/watch?v=_E4J5SrF2SM)

In this lab, you saw how to monitor your applications using built-in Google Cloud tools. First, you deployed an application to App Engine and examined Cloud Logs. Then you viewed profile information and explored Cloud Trace. Last, but not least, you monitored your applications with dashboards and created uptime checks and alerts. You can stay for our lab walk-through, but remember, Google Cloud's user interface can change, so your environment might look slightly different. So the first thing we're going to do is Activate Cloud Shell and then we're going to use that to download a sample app from GitHub. So I'm just going to click Continue, because this is a new project, and I like to open this in a new window just so I have a little bit more space. That's just telling us that the Cloud SDK is pre-installed, that's great, and I'm going to wait for the machine to be provisioned. I'm going to create a directory, navigate to that directory, and then clone a simple Python Flask application from GitHub. So let me just make that directory, going to navigate to that directory, and then I'm going to git clone from the same repository that we saw in the previous lab. So this has now been completed, so let me change directories to that, and then I'm also going to launch the code editor, and in here, I am also going to navigate through courses, design-process, to this application. And here we can see the main file. So what I'm going to do now is I'm going to make some modifications to the main file because I want to use the Google Cloud Profiler. The first thing I'm going to do is import the Profiler. I'm going to specify that on line 2, import profiler, and this allows the Profiler to monitor the resources the application uses. Now I'm also going to, after the main function, add a code snippet to start the Profiler. So let me go down here, and maybe add this on line 13, and then they're going to try to add that. And now I'm going to confirm that the code in here matches what is shown in the lab instructions, and that looks good to me. So this code simply turns the Profiler on and once the Profiler is on, it starts reporting application metrics to Google Cloud. Now, we also have to add the Profiler to, the Profiler library, I should say, to our requirements.txt. So let me go do that, I'm going to open the requirements.txt, and on line 2, I'm just going to specify that we're going to use the Google Cloud Profiler. And so I've added it everywhere. Now, the last thing I need to do is enable it for the project. We can do that directly in Cloud Shell. So I'm just going to clear this to give myself more space and I'm going to run the command in the lab instructions to enable the Cloud Profiler from the Google APIs. So let me run that, and once that has been completed, we're going to test the program by first installing the requirements and then starting the program. And there we can see the Google Profiler in here, that's successfully built, and let's test that. Now, within Cloud Shell, I have the web preview option to preview this in port 8080. And if I do that, we see that the program is currently working and it's just displaying, Hello GCP. So that's it for Task 1. Now, in Task 2, we're going to take this application and deploy it to an App Engine application. So let me go back, we are going to stop this. And I'm now going to create a new file, the app.yaml, which we did similarly in the previous lab, so I'm just going to, right click here on this folder, New File, app.yaml. And the minimum that we need to specify in here is the runtime. So I'm going to specify Python, then Save those changes, and now we're going to start off by specifying the region where we want this app to be created. So I'm going to use the gcloud app create region us-central1, and that's now creating that for our project in that region. And once that's up and running, we're going to deploy our app. Okay, so now I can deploy it and we're going to wait for that to complete. So the app has been deployed, so let's go to the Cloud Console to view it. So I'm just going to navigate to the Cloud Console, can make this a bit smaller here. And in the navigation menu, I'm going to go to App Engine. Collapse these two so I have a bit more space, and we currently only have one version, and here's our application, and if I click on that, we should see the same page we saw earlier, Hello GCP. And I can refresh this a couple of times now to start generating some traffic and we're going to do a little bit more of that in a second for the Profiler. But first, let's go to Task 3 and examine the Cloud Logs. So for that, I'm going back to App Engine, and I'm going to click on Versions. Here is my version that's serving all of the traffic. And if I go to the right under Diagnose > Tools, I'm going to leverage Logs. And we'll see here some of the requests I've been making, and we will also see here that the Stackdriver Profiler [now Cloud Profiler] agent has started and has created a profile, so we can see that that has been successful. So now we can move on to Task 4 and view the Profiler information. So I can go to the navigation menu, scroll down to Operations, and here is the Profiler. So this gray bar here at the top represents the total amount of CPU time used by the program. And the bars below that represent the amount of CPU time used by the program's function relative to the total. At this point there's no traffic, really, so the chart is not very interesting. So what we're going to do now is we're going to throw some load at the application. We're going to do that by creating a virtual machine using Compute Engine that is in a different region than our App Engine app, and then we will use Bench to create some traffic on here. So let me go to the navigation menu, go to Compute Engine, And I'm going to click Create. And in here now, we're going to just choose a different region. So instead of us-central1, we're going to place this in, let's say, europe-west1, and then I'm just going to click Create. And once this virtual machine is up and running, we're going to SSH to this instance. We're going to run sudo apt update and then also install apache2 utils. So we go to SSH and resize this window a little bit. And then we're going to go ahead and run that and then use Apache Bench to generate the traffic. So first, I'm going to update. Then install Apache Bench. And when I run the Apache Bench command now, I'm going to run it 1,000 times, 10 requests at a time, but I need the site, the HTTPS address, for my App Engine application. This is always in the form of, by default, of project ID.appspot.com, so I could just use that, or I am going to go back and just grab it in my browser, I still have it in there. And it's important that you have the slash at the end. So I'm going to run that, and we can run that a couple times to just generate some traffic. It might take a while for the Profiler to show something very interesting. But if you generate enough traffic, and again, you might have to try just a couple times for the information to start showing up, you will definitely see some more interesting graphs, that again, where you have these bars. And each bar represents a function and the width of the bars represent how much CPU time each function consumed, so I'm just going to keep generating some more traffic. I'll look at the Profiler and then move on to Task 5. All right, so I've generated some more traffic. So I think it's time to go back to the Cloud Console, and from the navigation menu, I'm going to go back to Profiler, and indeed, this looks a lot more interesting. So again, each of these bars is a function and the width of the bar represents the amount of CPU time that the program has consumed for each of those functions. So that finishes Task 4, so it's time to move on to Task 5, where we're going to explore Cloud Trace. So every request to your application is added to a Trace list. So let's go to the navigation menu and as well under Operations go to Trace. So this is an overview screen and it shows recent requests and allows you to create reports to analyze traffic. But because our program is new and really only has one page it's not very interesting, but in a real lab, there will be lots of useful information in here. So I'm going to first click on the Trace list and this is going to show a history of the requests and their latency. So here are all of the different requests. I've made just over the last couple of minutes. I can see all the latency. Some of them definitely took a little bit longer, and I can also see all of them here and their latency. So what we could do now, is I could go back to my SSH window of my virtual machine. I could just go ahead and generate more and more traffic and I'm going to do that. I'm going to come back and look at this Trace list again. So I've run the Apache bench command a couple more times. And what I did is I've actually changed it and you could do that to to just play a little bit with the values of N and C. So I'm requesting it 10,000 times and a hundred times a time. So now let me go in here and reload my Trace list and now you can see that I have a lot more requests and because I'm requesting so many, doing so many requests at the same time, some of them certainly have a higher latency here. So again, you could keep playing with this. Keep in mind, the lab is only open for so long. But this is just to give you an idea of Trace and how to use the Trace lists. I'm now going to move on to Task 6, where we're going to monitor resources using dashboards. So in the Cloud Console, I'm going to navigate to Monitoring which again is under the Operations section. And what this is gong to do is it's going to first set up a Workspace for us. So let's wait for this to complete. So the Workspace is now established, so we get this Welcome page here and we can now use the navigation bar over here on the left. So first I'm going to head to Dashboards and here we can see Dashboards for different resources. It's currently not showing GC instances. We might have to refresh and wait for a while it to come back. For now let's just go to App Engine and here we see our project and our application. And I can click on that to just see a dashboard for our application. Now it's just just showing me some responses on HTTP. I can also go to SYSTEM and DATASTORE and look at a couple different options or I could actually go ahead and create my own custom dashboard. So if I go back to Dashboards, I could also click CREATE DASHBOARD, give it a name, My Dashboard. And since it's currently not showing GC instances, let's just create something ourself. So Dashboard is just sort of a canvas and you just add Charts to it. So I'm going to look for GCE VM instance. There we go, and I'm going to select CPU utilization. No, I only have one instance, so that is the one shown here. And here I get a little bit idea of what kind of load the Apache Bench command has put in my instance that I have. So obviously, when I run the command there's high utilization than when I don't, you can see that here. And I could put other things in here like Filters or Group this or I could just save it and have this chart. So here we can now see my CPU utilization, and I could create other charts in here that might be interesting. Now, that's it for Task 6, so let's move to Task 7 where we're going to create uptime checks and alerts. So here in the left hand side I'm going to click on Uptime checks and CREATE UPTIME CHECK. Now we're just going to give it a name, so let's make this the Uptime Check for our App Engine application. The check type is going to be HTTPS. We're just going to use a resource by URL and now I need to put in the Hostname and then . appspot.com. So I'm just going to grab that again, I'm navigating back to the browser where I have this app open already and put that in here. I already set this to HTTPS. And this is the path, so I can remove that up here and then I can just say, sure, check every minute. And I could now test this to see this working, response okay. Great, and then I can Save that. Now it's saying, hey, great, we've created this uptime check, but do you also want to create an alert policy? This is actually pretty useful because if for whatever reason you have some downtime in your application and you're not currently looking at this uptime check, you won't be notified unless you create a policy. So let's create an alert policy. So it's already looking at that and making sure that's uptime. So that's great. So this is the uptime check sort of metric itself, the condition, so I can just give that a name. Say My Uptime Alert. Or actually the, sorry, the lab instructions are saying, Uptime Check Alerts. So let's do that because a lot of times the scores that we're giving the labs require that you follow the instructions. So that's good, great, so this is the metric. And then I'm going to, this is the, sorry, this is the condition. I can also put that same name in here and then I could have several conditions. So if anything else is going on and I could say, hey, it's either an or or an and condition with these triggers. And then really importantly, optional, but important you should add a notification channel. So if I click on that in here you have different types. So you could use, for example, Email and you could send yourself an email. I don't really recommend putting your own email in here. You could and test it, but the App Store not going to go down until this project is deleted and then you might get tons of emails that you may not like. So as an example, I can just put in the email from this qwiklabs project and add that. And then also importantly is to actually put in some documentation. So whoever gets this email, what should they be doing when they receive this alert? I'd be very specific, I'd have the actual disaster plan in here, right? If this application goes down, do XYZ, notify these people, do that. So that's really what you want to put into the documentation. So I'm just going to go ahead and save that because that's required for the scoring that we have in here. And actually what you can do is you could now go ahead and disable the application and just see that the uptime check will then fail. So let's actually look at the uptime check. Currently, if I go to the uptime check, we see these different continents from where we're checking. This uptime check, it's working from everywhere. I can click on the name to actually get more information. I see so far it's been 100% uptime. I see the latency and I can drill down into that by looking at the latency from different regions. So here we can see this is actually being tested from different regions, specifically in these different continents. We have different regions from where we're checking this and here we can see our overall configuration one more time. We also see that we have an Alert Policy and we could create some other Alert Policies. So let's go ahead and disable the application. So I'm going to go into the navigation menu and go back to App Engine. All right, so here I am on App Engine, since I'm going to scroll down and go to Settings. And I'm going to click Disable application. Now for safety reasons, it's telling me that I need to type in the name. So let's type in the App ID, I can actually copy that and click Disable. So this is now getting disabled and now I can go back to the Dashboard. And if I try that URL, we see we get a 404. Great, that's what we're trying to replicate. So let me go to the navigation menu and let's go back to Monitoring and head to our Uptime check. And we're going to wait until this check is run again, which keep in mind, is being run every minute and see that this should then be failing. Okay, so here we can see it's already failing, only took a couple seconds. In Europe it's failing, here one out of three tests have failed, and if I just navigate to Alerting and back to Uptime checks, I can see, what? All of North America, all of South America, Asia Pacific, probably needs to run one more time. If I go in here now, we see the Percent Uptime is already down over the last hour. My Uptime Latency Zone and here apac-singapore's the last region and if we refresh there we go, all of these have failed. So if I go to Alerting, I see that I have my Alert policy. They should be firing any moment now to send me an actual alert depending on the the conditions that are defined in this alert, which is just at what point does it actually send that alert? And if you've put in your own email address, you will actually be getting that alert. So we could refresh this a couple times and wait for that and once you actually see that the incidence fired or you get the alert, you do want to go back here and edit the notification channel of this alert. And remove your email address so that you don't accidentally get any spam about this failing in the future because it's going to keep failing so you will keep getting alerts. So you can do that in here by just editing this. And clicking the trash icon next to the email and then clicking Save. And then you could also go to the Uptime check and delete the Uptime check itself, as well, just for safety so that, again, you're not being notified about this. And that's the end of the lab.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/41/quizzes/520442)

#### Quiz 1.

> [!important]
> **You've made a minor fix to one of your services. You want to deploy the new version with no downtime. Which would you choose?**
>
> - [ ] Canary deployment
> - [ ] Blue/green deployment
> - [ ] A/B test
> - [ ] Rolling update

#### Quiz 2.

> [!important]
> **You made a minor update to a service and would like to test it in production by sending a small portion of requests to the new version. Which would you choose?**
>
> - [ ] Blue/green deployment
> - [ ] Rolling update
> - [ ] A/B testing
> - [ ] Canary deployment

#### Quiz 3.

> [!important]
> **Your service has an availability SLO of 99%. What could you use to monitor whether you are meeting it?**
>
> - [ ] Health check
> - [ ] Liveness probe
> - [ ] Uptime check
> - [ ] Readiness probe

#### Quiz 4.

> [!important]
> **You're deploying test environments using Compute Engine VMs. Some downtime is acceptable, and it is very important to deploy them as inexpensively as possible. What single thing below could save you the most money?**
>
> - [ ] Preemptible machines
> - [ ] Sustained use discount
> - [ ] Committed use discount
> - [ ] Sole tenant nodes

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/41/video/520443)

- [YouTube: Module Review](https://www.youtube.com/watch?v=J90gkb267Nk)

Philipp: In this module, you learned about managing new versions of your microservices using rolling updates, canary deployments, and blue-green deployments. It's important when deploying microservices that you deploy new versions with no downtime, but also that the new versions don't break the clients that use your services. You also learned about cost planning and optimization, and you estimated the cost of running your case study application. You finished the module by learning how to leverage the monitoring tools provided by Google Cloud. These tools can be invaluable for managing your services and monitoring your SLIs and SLOs.

### Video - [Course Review](https://www.cloudskillsboost.google/course_templates/41/video/520444)

- [YouTube: Course Review](https://www.youtube.com/watch?v=Hc6bDz0K88U)

Philipp: Thank you for taking the Reliable Cloud Infrastructure Design and Process Course. We hope you have a better understanding of how to design applications and services that make the best use of the platform services provided by Google Cloud. Person: We also hope that the design activities and labs made you feel more comfortable with design and process in Google Cloud. Philipp: Now it's your turn. Go ahead and apply what you have learned by designing your own applications, deployments and monitoring. Person: See you next time.

### Document - [What’s Next? Get Certified](https://www.cloudskillsboost.google/course_templates/41/documents/520445)

### Document - [Workbook Example Solution](https://www.cloudskillsboost.google/course_templates/41/documents/520446)

## Course Resources

PDF links to all modules

### Document - [Course Resources](https://www.cloudskillsboost.google/course_templates/41/documents/520447)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
