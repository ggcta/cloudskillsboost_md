---
id: 1036
name: 'Responsible AI for Developers: Privacy & Safety'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1036
date_published: 2024-11-04
topics:

---

# [Responsible AI for Developers: Privacy & Safety](https://www.cloudskillsboost.google/course_templates/1036)

**Description:**

This course introduces important topics of AI privacy and safety. It explores practical methods and tools to implement AI privacy and safety recommended practices through the use of Google Cloud products and open-source tools.

**Objectives:**

* Define what AI privacy and AI safety is.
* Describe methods used to address AI privacy in both data and models.
* List key considerations for AI safety implementation.
* Describe techniques used when implementing AI safety.

## Course Introduction

This module introduces the course structure and objectives.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/1036/video/513287)

* [YouTube: Course Introduction](https://www.youtube.com/watch?v=EfAf0xYVo_s)

Welcome to the course. Responsible AI for Developers Privacy and safety. This course introduces important topics of AI privacy and safety. It explores practical methods and tools to implement AI privacy and safety best practices through the use of Google Cloud products and open source tools. In this course, you will learn to define what is AI privacy, describe the methods used to address AI privacy in both data and models, Define what is AI safety, List the key considerations for AI safety implementation and describe techniques to use when implementing AI safety. Let's begin the course.

## AI Privacy

This module focuses on AI Privacy. It provides various techniques and tools to help achieve privacy in data and machine learning training.

### Video - [Overview of AI Privacy](https://www.cloudskillsboost.google/course_templates/1036/video/513288)

* [YouTube: Overview of AI Privacy](https://www.youtube.com/watch?v=Jhrv1aqzBFc)

Welcome to AI Privacy. This module consists of eight lessons. Today, you will learn to define privacy in AI, discover some best practices on privacy, describe the types of security behind privacy, explore techniques and tools for data and model security for privacy and explain how to apply security best practices for generative AI models in Google Cloud. Let's start with an overview of AI privacy. Privacy relates to Google's fifth AI principle, incorporate privacy design principles. Although it's worth noting that the principles often intersect. Why do you need to take privacy into consideration? There are several reasons. This includes not only respecting the legal and regulatory requirements, but also considering social norms and typical individual expectations. A sensitive attribute is a human attribute that may be given special consideration for legal, ethical, social, or personal reasons. This includes, but is not limited to, personal identifiable information such as full names, date of birth, address and phone number, social data such as ethnicity, religion, sexual orientation, and political affiliation, financial data such as credit card numbers, income, and tax records, health and medical data such as diagnosis, prescriptions, and genetic data, geolocation data such as tracking data from mobile devices, biometric data such as facial recognition, voice, and fingerprints, user authentication data such as usernames, passwords, security questions and answers. Legal data, including intellectual property and trade secrets. There may be enormous benefits to building a model that operates on sensitive data such as an AI system trained on electronic health records to predict an individual's risk of chronic diseases, then deployed to make personalized health recommendations. However, it is always essential to consider the potential privacy implications in using sensitive data. How can you ensure you are building a secure machine learning based system? There are several aspects to consider. Security in training data. Since machine learning is always trained on a dataset, the protection of sensitive and confidential data used for AI systems is critical. Security in training process. You can address security by adopting recommended practices for secure machine learning training like federated learning and differentially private stochastic gradient descent or DP-SGD. Implementing privacy preserving techniques across hardware, software, communication channels, and infrastructure is crucial for comprehensive AI system security. We'll look at data security in detail first, then discuss the other two aspects.

### Video - [Privacy in Training Data: De-identification techniques](https://www.cloudskillsboost.google/course_templates/1036/video/513289)

* [YouTube: Privacy in Training Data: De-identification techniques](https://www.youtube.com/watch?v=u7q_rqepj88)

Lets examine privacy in training data in our context of privacy in ML, data security is the protection of sensitive data used in AI systems. Keep in mind that you want to minimize the use of sensitive data as much as possible, preferably at the data collection phase, such as when collecting aggregated data rather than, for example, during data processing or collecting individual interactions. We recommend two different, not exclusive, approaches to de identify your data through de identification techniques such as redaction, replacement, masking, tokenization, bucketing, and shifting. Randomize your data through methods such as data perturbation and differential privacy, De-identification approaches can be evaluated by two factors, if an approach is reversible, can you re-identify the data? Referential integrity is the relationship between records maintained after de-identification, whether these factors are a positive aspect or a potential risk depends on the specific use case, and data privacy requirements. For example, reversibility is great for audibility, and recovery, but it can easily lead to data leakage, otherwise known as reidentification, even though it is less likely. The same risks exist with referential integrity, but referential integrity has the added benefits of improving ML performance, and consistency when guaranteed. First de-identification technique redaction is the process of deleting all or parts of a sensitive value, this approach is not reversible and it does not maintain referential integrity. We will follow this example for all the de-identification techniques, let's say you have a transactional table with ID, date, time, amount in dollars, email and product. If you want to redact the column email, you can simply delete it from the view, redaction removes data, which can lead to a loss of information that could be valuable for machine learning models. This can affect the accuracy, generalizability, and overall effectiveness of the model, for example, if we redact locations from a dataset, this action might hinder the model's ability to learn geospatial patterns. The second de-identification technique is replacement, this means replacing a sensitive value with a surrogate value that you specify, it can be only one value or a value randomly selected from a list of values. The approach is irreversible and it does not maintain referential integrity, note that replacement can have the same negative effect as redaction does on the model's ability to learn due to data loss. For example, let's say you want to apply for a replacement on the email column, then you can replace all emails with its info type, such as email address. In this example, its worth mentioning that it has the same effect as redaction, thus, we need to be aware of the same limitations here about data loss, masking is the third deed identification technique. Masking replaces some or all characters of a sensitive value with a surrogate, this surrogate value can be selected randomly from a list if desired, but the approach is not reversible, and it does not maintain referential integrity. For example, lets say you want to apply masking on the email column to keep the email domain but not keep the exact handle, in this case, you can replace all values in the email column with hashes. While replacement, and masking may sound similar, masking covers a portion or all of the existing data depending on the parameters you set, whereas replacement removes the data completely, and replaces it with a value. Tokenization is the last de-identification technique we will explore for categoricals, tokenization replaces a sensitive value with randomly generated tokens, each unique to the value it de-identifies. This approach is reversible and also maintains referential integrity where ML models can still leverage information from the tokenized features. However, the data can be vulnerable to malicious attacks that attempt to reverse the data to the original information, for example, if you apply tokenization on the email column, then you get new random tokens which uniquely identify that value. With tokenization, ML models can still leverage the information of tokenized features, but be careful as it can be vulnerable to malicious attacks that reverse it to the original information. The next de-identification approach, bucketing is for numerical values, Bucketing generalizes a sensitive value by replacing it with a range of values. This approach is not reversible and it does not maintain referential integrity, for example, lets say you want to apply bucketing to the amount in dollars column and you want to bucket intends. Then the procedure would replace each entry with its correct bucket, for example 56 is replaced with 50 to 60, depending on how broadly the buckets are defined, the deed identified data may be less useful for machine learning purposes or may affect the model performance. Be careful about the size and granularity of brackets so that you can find a good balance between the model performance and privacy lastly, shifting, which is a technique for date-time values, this method shifts a sensitive date and time value by a random amount of time. This transformation is reversible and it keeps referential integrity because sequence and duration are kept for example, if you apply shifting to the date time column, then the procedure could randomly select one day as shift, resulting in one day added to each entry. Similar to tokenization, shifting can also be vulnerable to malicious attacks if the random shifting factor is identified. Now, what are the risks of re-identification? None of these approaches guarantee 100% that data cannot be re-identified even the non-reversible approaches instead, what these approaches do is strive towards minimizing the risk. Re-identification risk analysis can help us identify the risk of re-identification after performing a de-identification technique. Identify the most effective de identification strategy before de identification there are two classic techniques that support this risk analysis. K-anonymity a dataset is k anonymous. If every combination of values for sensitive features in the data set appears for at least k different records, this tells us something important. Features can be correlated, and even if you de-identify a sensitive value, it can still be re-identified given its relationship to other features. L diversity l diversity is an extension of k-anonymity that ensures that each anonymized group contains at least l distinctive values for each sensitive attribute. L diversity is a measurement of the diversity of sensitive data, you can apply de-identification as well as data perturbation techniques to achieve k-anonymity at a specific k and or l diversity at a specific l. The combination of k-anonymity and l diversity provides a more comprehensive understanding of privacy, especially in scenarios where datasets contain mixed data types.

### Video - [Privacy in Training Data: Randomization techniques](https://www.cloudskillsboost.google/course_templates/1036/video/513290)

* [YouTube: Privacy in Training Data: Randomization techniques](https://www.youtube.com/watch?v=2I07r046N0U)

Let's move to randomization techniques for privacy training in data. Randomization techniques aim to preserve data privacy by adding noise or perturbation to the data. When preserving data privacy, you can apply techniques like data perturbation or differential privacy. Data perturbation is a technique you can implement easily. Whereas differential privacy provides a more rigorous privacy guarantee. Let's explore each of these techniques by looking at each method. Data perturbation introduces some random noise or makes small modifications to obflicate as sensitive value. The perturbed data still reflects the general trends and patterns present in the original data set while preventing the direct identification of individuals. Some examples of data perturbation techniques include random noise addition, adding random values from a small range to each data point in the dataset. This works for numerical values. Random swap, exchanging the values of different data points randomly. It works for both numerical and categorical values. Random rounding, rounding data points to random precision levels. This works for numerical values. Random category mapping, randomly mapping categorical values to different categories within the same variable. The level of perturbation must strike a balance between privacy protection and data utility, ensuring that the data remains informative and valuable for legitimate analytical purposes. Differential privacy ensures that the inclusion or exclusion of any individual's data does not significantly affect the output or result of the analysis. This is particularly useful when you want to share data or allow inferences about groups of people while preventing someone from learning information about an individual. It is referred to as a rigorous approach since it provides strong provable privacy guarantees. Let's take a look at an example of applying differential privacy. Imagine you are attending a conference and the moderator shares some insights about the audience. The moderator shows the country of origin of the participants in the room for day 1. The data is aggregated, so it does not reveal information about any of the individuals in the room. The data reveals the top country of origin is Italy, whereas Germany has the least number of participants, and France and Spain participant numbers are in the middle. On day 2, the moderator shows the stats for the day. It looks very similar to the chart from day 1. However, before the day 2 session started, you met someone who attended day two, but not day 1. Knowing this information, you compare the data from both days together. Are you able to determine the country of origin for the new attendee based on the datasets? Yes, the New attendee is from Germany. This is an example of how given two aggregated datasets. The moderator has not protected the privacy of particular participant. To address the data privacy leak issue, the moderator needs a new approach. Let's try to randomly change the numbers a little to improve data privacy. Now, look at how the day one stats look compared to the real numbers. They're slightly different. It is less precise, but you still get the same insights about the statistical trends. The same approach is applied to the data from day 2. Now, if you compare the stats for day 1 and day 2, you can't tell which country the single new attendee is coming from, and it's not worth trying to figure out since you know the numbers were changed a little. This is the principle behind differential privacy. Differential privacy introduces the right amount of noise to the data. We're able to provide useful statistically significant insights while ensuring that no one can tell if a particular individual's information was used in the computation. The best thing about this principle is that it is mathematically proven. Differential privacy relies on this equation. It essentially says that something is differentially private. If the output doesn't change much when a single person is added to the database. Going back to the previous example, a differentially private version of those charts doesn't allow us to tell much about the participants when a new one is added. Differential privacy is characterized by two key parameters. The privacy parameter Epsilon is a non negative value that quantifies the privacy level provided by the differential privacy mechanism. A smaller Epsilon value indicates stronger privacy protection, but it comes with the risk of too much noise being added to the query results, potentially reducing data utility. Sensitivity measures how much the output of a query can change, specifically the maximum absolute difference when a single individual's data is added or removed from the dataset. Sensitivity is crucial for determining the amount of noise that needs to be added to ensure privacy. Techniques in de identification and randomization are used to protect sensitive data. These techniques will help improve privacy in your training data for AI models.

### Video - [Privacy in Machine Learning Training: DP-SGD](https://www.cloudskillsboost.google/course_templates/1036/video/513291)

* [YouTube: Privacy in Machine Learning Training: DP-SGD](https://www.youtube.com/watch?v=L8NxHH2_5TY)

Lets talk about privacy in machine learning training. After building machine learning models, it is crucial to carefully consider security measures before exposing them to users, as malicious individuals may attempt various attacks to extract sensitive data or exploit model vulnerabilities. There are many types of attacks and many measures we can take to safeguard against them. While there are many methods, two popular ones for secure models are, differentially private stochastic gradient descent or DP-SGD, and federated learning. DP-SGD is a secure optimization strategy that incorporates differential privacy into stochastic gradient descent or SGD, to train models in a more secure way. As mentioned earlier, differential privacy can be used for many use cases where we aggregate information in a secure way. Machine learning optimization strategies like SGD, which update model parameters by computing gradients of them with respect to a group of data samples, can be viewed as aggregation algorithms. This means differential privacy can be integrated into optimization algorithms like SGD to protect sensitive information during the training process. Although there are multiple implementations, lets take a look at the simplest DP-SGD algorithm here. DP-SGD extends the normal SGD algorithm which computes gradients of each parameter and updates them using the gradients. DP-SGD adds an extra step to introduce differential privacy via gradient clipping and noise edition. The amount of noise can be controlled by the sampling distribution. Larger noise is more secure but can worsen the model performance. With this additional step, we can apply optimization and training in a more secure way. There are many improvements and additional considerations for differential privacy on machine learning training, but you can easily adopt them by using existing implementations. These include TensorFlow Privacy and Google's Differential Privacy Library. TensorFlow Privacy is a library integrated well with the TensorFlow ecosystem. You can easily apply differentially private optimizers including DP-SGD to TensorFlow models using this library. Google's Differential Privacy Library is a more generic library for differential privacy as it is used not only for machine learning purposes. It includes an end-to-end differential privacy framework built on top of Apache Beam, as well as lower level libraries in C++, Go, and Java to implement differentially private aggregations.

### Video - [Privacy in Machine Learning Training: Federated Learning](https://www.cloudskillsboost.google/course_templates/1036/video/513292)

* [YouTube: Privacy in Machine Learning Training: Federated Learning](https://www.youtube.com/watch?v=hhig8z3IfHg)

Now, let's move on to the next methodology, Federated Learning. Federated learning is a useful method for training large-scale machine learning models using edge devices such as smartphones, while preserving data privacy by avoiding the need to share raw data. This technique is adopted in many Google services and applications, including Gboard. Gboard has several machine learning models to enhance users' typing experiences on smartphones based on users' inputs. Since every user has different usages and needs, fine-tuning and personalizing the models for each user is critical to support unique needs, while it is also important to sync updated models among all users to improve the product itself. However, the typing input can contain sensitive information, including PII, which users don't want to send out to a central server for machine learning training. In this use case, federated learning is a critical method for ensuring both privacy and personalization. Google has been implementing many systems that use this technology. Let's take a look at how it works. Imagine you built a mobile application which includes a machine-learning model. You trained the first model with a standard data set and distributed the app to users' devices. Similar to how Gboard personalization works, federated learning can update machine learning models on user devices like Smartphones using their own data. After these local updates, the approved model parameters or gradients are shared with the central server so that the changes are reflected in the central model. This way, the central model is enhanced with updated models using fresh data that benefits all users without compromising their privacy. Lots of update information is gathered from the numerous user devices without collecting any raw data from those devices. Practically, you should carefully design the user sampling method to avoid unintended results like data skew. Then aggregate the gradients to update the central model. By following these federated learning steps, you can update models using actual user's data while keeping them on each device. This can be repeated to update the model. Although the concept of federated learning may seem straightforward, it's implementation and configuration is tricky, since it requires multiple systems working together. TensorFlow Federated, or TFF, offers a useful toolbox for this purpose. With TFF, you can simulate the federated learning environment quickly and apply complex novel algorithms. You can even use TFF for non-machine learning training purposes like computing the mean of all user behavior. This is called federated analytics. Federated learning technology sounds cool, but it still has some issues we need to tackle. For example, membership inference attack. This is a privacy-focused attack against machine learning models. In this attack, an adversary with access to the gradients or central model provider tries to determine whether a specific data record or data point was used to train the target model based on inferences from gradients. Sensitive property breach. This is a similar attack to membership inference attack, but involves the exposure of sensitive characteristics or patterns that the model has learned. This exposure comes from reconstructing sensitive features from gradients. Model poisoning. This occurs when a user who participates in the federated learning process wants to manipulate the model behavior and intentionally creates poisoned data points on its device in order to degrade the performance of the model, cause it to make incorrect predictions or behave in a way that benefits the attacker. Since the provider doesn't know the data itself, even if they apply anomaly detections on shared gradients, it is difficult to tell if a set of gradients came from a user with malicious intent or from a user with unique behavior. Although there is not a perfect solution that can solve all problems at once, the secure aggregation method can help you avoid membership inference attacks and sensitive property breaches. Secure aggregation is a cryptographic method used to aggregate multiple data in a secure way by encrypting data before it is sent to the server. Let's say we compute a sum of three data points from Emma, Takumi, and Nick. In secure aggregation, instead of simply adding the data up on the central server, the participants modify the values a little by adding a value shared with other participants. The pair two values add up to zero. In this case, Emma and Nick share a green value pair, Emma and Takumi share purple, and Takumi and Nick share orange. The server receives these perturbed values and aggregates them. The values don't affect the result of aggregation since the pairs add up to zero. At the same time, it is much harder for the server to reconstruct the original data because of the modifications. By adding secure aggregation, we can apply federated learning in a more secure way. You can also use differential privacy in federated learning as well. Clients can add noise to their gradients or the server can take care of it. By applying differential privacy in federated learning, you mitigate the risk of gradients revealing sensitive information of a specific user. In addition, you can make a more robust update strategy to avoid data poisoning. However, you should note that there is no perfect solution, and each technique has trade-offs between multiple factors such as privacy, performance, robustness, and fairness. If you apply large noise using differential privacy in order to provide stronger privacy, it may affect performance. Although differential privacy can make the process robust, it could mean the system deprioritized data from users that were unique or in the minority compared to the rest of the data. This could potentially lead to a bias issue. In summary, to ensure stronger privacy in machine learning models, it is important to understand the requirements of a system, then re-evaluate the system using multiple proper metrics and criteria.

### Video - [System Security on Google Cloud](https://www.cloudskillsboost.google/course_templates/1036/video/513293)

* [YouTube: System Security on Google Cloud](https://www.youtube.com/watch?v=5n4aydIGThM)

So far, we have looked at privacy and data and model training. Let's take some time now to look at system security on Google Cloud. In addition to thinking about data and model training privacy considerations, it is important to follow the standard system security best practices, including access control and monitoring. Sensitive data protection, incorporate a system that automatically detects and prevents online threats so that you can be confident that private information is safe. Encryption, keep data private and secure while in transit and at rest. Access control, follow the least privileged access configurations. You can ensure that people and non-people are granted minimum access necessary to private information. Monitoring, implement point in time incident analysis and proactive security alerts to help protect private information by reacting promptly to unexpected behavior. Google Cloud provides a very comprehensive security offering. Sensitive data protection is offered through the Cloud data loss prevention, or DLP API, encryption through Cloud Key Management Service or KMS, access control through identity and access management, or IAM, and monitoring via Cloud monitoring. Sensitive data protection through the Cloud Data Loss Prevention Service helps you discover, govern, protect, and report on sensitive data across your ecosystem. The service provides data identification techniques such as masking and tokenization with support for structured and unstructured data. Ability to measure re-identification risk in structured data. Over 150 built-in information type or info type detectors and the ability to define your own for automatically scanning for sensitive information. Built-in support for Cloud storage, BigQuery, BigLake, and Cloud SQL to allow support for additional sources and workloads. Lots of additional customizations such as adjust detection thresholds, create detection rules, and so on. For encryption, Google provides default encryption at rest, which is fundamental for the privacy of your AI datasets. Data in transit is also encrypted by default using TLS. Google Cloud encrypts your data on the server side before it is written to disc at no additional charge. This data can be previously encrypted if desired. Google Cloud's Key Management Service is a Cloud hosted service that lets you manage symmetric asymmetric cryptographic keys for your Cloud services. You can use Cloud KMS to extend your control over encryption keys. This allows you to create import and manage cryptographic keys and perform cryptographic operations in a single centralized Cloud service. For access control, Google uses IAM services, which give you full control and visibility to manage Google Cloud resources centrally. IAM provides all the functionalities you'd expect for access management within a single access interface such as fine-grained roles, built-in audit trail and workforce identity federation to use external providers as well. To secure your email applications, set IAM permissions on the data models and serving endpoints, following the principle of lease privilege. Cloud monitoring collects metrics, events, and metadata from Google Cloud, hosted uptime probes, and application instrumentation. What's of interest from a security perspective is that every request is logged. This traceability gives you the ability to be notified for anomalies based on custom rules and policies so that you can investigate any incident.

### Video - [System Security on Gen AI](https://www.cloudskillsboost.google/course_templates/1036/video/513294)

* [YouTube: System Security on Gen AI](https://www.youtube.com/watch?v=Jhj4RB1-AUo)

Let's look at system security and how it relates to generative AI. Generative AI models are trained on vast sources of unstructured data that pull from a range of sources. It is likely that they could contain some sensitive data, and it is possible that they could reflect some of these private details in the output. In fact, the larger the language model, the more easily it memorizes training data. It is a known fact that through a simple query, it is not impossible to extract specific pieces of training data that the model has memorized. This makes generative AI models particularly sensitive to training data extraction attacks. In a training data extraction attack, the attacker iteratively inputs a prompt or a series of prompts crafted to intentionally extract individual model training examples. This type of attack has the greatest potential for harm would applied to a model that is available to the public, but the dataset used in the training is not publicly available. By design, language models make it very easy to generate a large amount of output data. By seating the model with random short phrases, the model can generate millions of continuations, for example, probable phrases that complete a sentence. Most of the time, these continuations are benign strings of sensible text. Sometimes they can leak sensitive information. Let's look at one example taken from one of the first papers to analyze this adversarial attack. A user enters a prompt East Stroudsburg Stroudsburg into the GPT-2 language model. The model then auto completes a long block of text that contains the full name, phone number, email address, and physical address of a particular person whose information happened to be included in GPT-2 training data. This results in the model allowing the user to obtain sensitive information. At Google, data governance is a key aspect of privacy. As part of Google Cloud's AI/ML privacy commitment, our models do not contain customer data in order to avoid situations such as this. Google's commitment for privacy extends to Google Cloud generative AI products and solutions. Google Cloud guarantees the following. The foundation model development is privacy compliant. By default, Google Cloud does not use customer data to train its foundation models as part of Google Cloud's AI/ML privacy commitment. When a request is submitted with a prompt to our foundation model, customer data is encrypted in transit and input to the foundation model to generate a response. Google processes customer data to provide the service requested only. The adapter weights are specific to the customer and only available to the customer who tuned those weights. During inference, the foundation model receives the adapter weights, runs through the request, and returns the results without modifying the foundation model or storing the request. Input data is customer data and is stored securely at every step along the way, encrypted at rest and in transit. Tuned weights are also stored securely, and customers will have sole access to use any tuned models. The customer is able to control the encryption of stored adapters by using customer managed encryption keys or CMEK and can delete adapter weights at any time. That is how Google Cloud helps ensure security best practices are applied to privacy when training machine learning models.

### Video - [Lab: Differential Privacy in Machine Learning with TensorFlow Privacy](https://www.cloudskillsboost.google/course_templates/1036/video/513295)

* [YouTube: Lab: Differential Privacy in Machine Learning with TensorFlow Privacy](https://www.youtube.com/watch?v=iJLI24StZvo)

Now let's perform an exercise in a hands on lab. This lab helps you learn how to use differential privacy in machine learning, using TensorFlow Privacy. In this lab, you will wrap existing optimizers into their differentially private counterparts using TensorFlow Privacy. Practice checking hyperparameters introduced by differentially private machine learning. Measure the privacy guarantee provided using analysis tools included in TensorFlow Privacy.

### Lab - [Differential Privacy in Machine Learning with TensorFlow Privacy](https://www.cloudskillsboost.google/course_templates/1036/labs/513296)

In this lab, you learn how to use differential privacy in machine learning using TensorFlow Privacy.

* [ ] [Differential Privacy in Machine Learning with TensorFlow Privacy](../labs/Differential-Privacy-in-Machine-Learning-with-TensorFlow-Privacy.md)

### Quiz - [Module 1: Quiz](https://www.cloudskillsboost.google/course_templates/1036/quizzes/513297)

#### Quiz 1.

> [!important]
> **A customer is working with a large dataset of healthcare records to develop a diagnostic AI model. They want to ensure privacy of sensitive data, by restricting the contribution of specific data on the model during training. Which privacy-focused technique would be best for the customer?**
>
> * [ ] TFF (TensorFlow Federated)
> * [ ] Data perturbation
> * [ ] DP-SGD (Differentially Private - Stochastic Gradient Descent)
> * [ ] Federated Learning

#### Quiz 2.

> [!important]
> **In machine learning, privacy measures often introduce trade-offs with other important factors. Which of the following does NOT represent a typical trade-off?**
>
> * [ ] Privacy and Model Performance
> * [ ] Privacy and Training Efficiency
> * [ ] Privacy and Fairness
> * [ ] Privacy and Transparency

#### Quiz 3.

> [!important]
> **You've applied various de-identification techniques to a customer dataset containing sensitive information before using it to train an AI model. Which of the following concepts should you use to evaluate the suitability of your de-identification techniques?**
>
> * [ ] Complexity and time consumption.
> * [ ] Reversibility and referential integrity.
> * [ ] Scalability and efficiency.
> * [ ] Productivity and efficiency.

## AI Safety

This module focuses on AI Safety. It provides various techniques and tools to help achieve safety in AI.

### Video - [Overview of AI Safety](https://www.cloudskillsboost.google/course_templates/1036/video/513298)

* [YouTube: Overview of AI Safety](https://www.youtube.com/watch?v=vpCFcCJv1XI)

Welcome to AI safety. This module consists of seven lessons. Today, you will learn to define safety for AI. Discover key considerations for safety. Explore techniques for AI safety, and describe which Google Cloud products can help with AI safety. Let's start with the overview of what AI safety means. The concept of safety directly relates to the third of Google's AI principles. Be built and test it for safety. Safety is also strongly related to other Google AI principles, especially. Avoid creating or reinforcing unfair bias. Since the concept of safety often comes from the definition of fairness and bias and proper safety implementation makes systems more fair. There are strong crossovers between fairness and safety. Be accountable to people as the accountability of the AI system helps promote safe use of AI and be made available for uses that accord with these principles. This is especially important when you make your AI system available to a wide group of users. As AI capabilities grow rapidly, keep in mind that AI systems are not necessarily used in a way you imagined. AI safety involves many perspectives and considerations, and it has many difficulties such as unknown action space. When machine learning is applied to problems that are difficult for humans to solve, it becomes challenging to predict all scenarios ahead of time, and especially so in the era of generative AI. Performance and safety tradeoffs. It is difficult to build systems that provide both the necessary proactive restrictions for safety, as well as the flexibility needed to generate creative solutions or adapt to unusual inputs. Attackers adapt fast to new technology. As AI technology continues to evolve, attackers will evolve too and will surely find new means to attack, resulting in new solutions that will need to be developed in tandem. AI safety is especially difficult when it comes to generative AI with classic discriminative AI models, such as those trained for classification and regression tasks. The output space is fairly well known. Classification models can only predict the classes they were trained on, so we can simply exclude classes we deem to be harmful. On the other hand, regression models can only predict a number and we know what that number is supposed to represent. That means when we build the models, we have a good idea of what the model can at least predict. This doesn't mean that the models are entirely safe and can't cause harm, but it does make it easier to find problems. However, with generative AI, we're creating a system that allows users to explore the breadth and depth of their creativity. This means that the models are trained on large amounts of data, and they are designed to learn to generate outputs that are very different from the data they were trained on. The creativity of the user combined with the emergent creativity of the model means that it is difficult to know the breadth and depth of a model's responses in advance. To approach safety, there are roughly two different approaches which are complimentary to each other. First, is a technical approach that seeks technical solutions through changes to the model or the system we build or through engineering practices. On the other hand, a non-technical or institutional approach is also very important as it seeks formal and informal institutional solutions to AI safety at the lab. Usually, this involves industry-wide, national and international scale discussions and commitments. This approach is also known as a topic of AI governance. And while this is a technical course for developers focusing on the technical aspects of AI safety, note that non-technical aspects often promote technical approaches, and they are complimentary to each other. How can we approach AI safety from a technical perspective, especially in generative AI? There are multiple measures we can take. This diagram shows an overview of the generative AI system. The system receives prompt inputs, passes it to the AI model, and returns outputs. Between the prompt input and the final output, you can see input safeguards and output safeguards. These safeguards take care of input-output filtering based on defined safety criteria. By using safeguards properly, we can harness the system input-output to avoid unintended harmful outcomes. In addition to safeguards. You can also try to intervene in the model training and tuning process in order to embed safety concepts into the model and align its behavior toward safety. It is also critical to understand how to evaluate AI systems throughout the entire process. Evaluation involves exhaustive testing to find an unintended behavior of the system. This is called adversarial testing. We'll look at each item as we progress through this course.

### Video - [Safety Evaluation](https://www.cloudskillsboost.google/course_templates/1036/video/513299)

* [YouTube: Safety Evaluation](https://www.youtube.com/watch?v=G9CFaxOXYZ0)

Let's explore safety evaluation. Without understanding the safety criteria, it is not possible to design and implement a safer AI system. Numerous key considerations should be taken into account. Let's check the major failure modes and considerations we need to understand. First of all, you need to understand your product and users to define the failure mode for your product. Different products should have different definitions and goals related to some sensitive characteristics. However, there are some common failure modes you should prohibit for safety purposes. Generative AI systems should be designed to avoid providing child sexual abuse material or CSAM. Content on or from generative AI products must not include child sexual abuse material, nor content that appears to abuse, sexualize, endanger or otherwise exploit children. Personally identifiable information or PII and sensitive personally identifiable information or SPII of specific individuals. Generative AI should not reveal an individual's personal information and sensitive demographic information of specific individuals. Hate speech. Generative AI should not generate hate speech. Defined as content that promotes violence, incites hatred, promotes discrimination, or disparages towards any groups. There are many other common failure modes we usually need to avoid. However, the actual nuance and the threshold depends on actual products. To control the balance, it is also very important to design a system that we can flexibly control and adjust based on the needs and contexts of users. Because it is important to ensure that your generative AI model is safe, with respect to the failure modes, this is where adversarial testing comes in. Adversarial testing is a method for systematically evaluating an ML model with the intent of learning how it behaves when provided with malicious or inadvertently harmful input. Adversarial testing can help teams improve models and products by exposing current failures to guide mitigation pathways such as fine tuning, model safeguards or filters. Moreover, it can help inform product launch decisions by measuring risks that may be unmitigated, such as the likelihood that the model will output policy violating content. But what is considered malicious input and inadvertently harmful input? An input is considered malicious when the input is clearly designed to produce an unsafe or harmful output. For example, an input that asks the text generation model to generate a hateful speech about a particular minority group would be considered malicious input. This is also known as explicitly adversarial queries. These types of queries may contain policy violating language or express policy violating points of view in an attempt to trick the model into saying something unsafe, harmful or offensive. An input is considered inadvertently harmful when the input itself may be innocuous but elicits a harmful output. For example, an input that asks the text iteration model to describe a person of a particular ethnicity and the model provides a racist output. This is also known as implicitly adversarial queries. They can contain culturally sensitive or contentious topics and might include information on demographics, health, finance, or religion. When malicious or harmful outputs occur, it is important to study the output to learn more about how the model performs when being exposed to such inputs. This is accomplished through what is called an adversarial testing workflow. There are four steps in the workflow. Find or create a test dataset, run model inference using the test dataset, annotate the model output, and analyze and report results. To conduct effective adversarial testing, find or create a test dataset. You should not use standard evaluation data sets, as these are typically designed to accurately reflect the distribution of data that the model will encounter in the product. For adversarial tests, the key to creating or finding a test dataset is to design a test that pushes the model's limits. To accomplish this, you should focus on out of distribution examples and edge cases that could lead to outputs which violate safety policies. The dataset should comprehensively cover safety dimensions, real world use cases, and be both lexically and semantically diverse. Lexical diversity focuses on the variety of unique words used within a piece of text or speech. Semantic diversity focuses on a range of meanings and ideas expressed within a text or speech. Next, run a model inference using the test dataset. The goal here is to generate model outputs based on the test dataset. In certain cases, generating multiple model outputs per adversarial query can help. Third, annotate the model's outputs to identify any policy violations. There are two methods for doing this , automatically and manually. Automatic annotations use machine learning and other AI techniques to automatically label or tag data that aligns with specific policy categories and failure modes. Manual nations involve human raters, either internal or external, who label and tag the data. This is accomplished using their own specific instructions and platforms designed for this task. When should you use automatic nations versus manual? A good example is the notation of data that contains hate speech. Sometimes, automatic notation accuracy may be low for signals that try to detect constructs that are not strictly defined, as in the case of hate speech. In this situation, it is critical to use human raters to check and correct classifier generated labels for which scores are uncertain. The final step is to summarize test results in a report. You can pick any type of chart of your choice to effectively communicate results in a report to your stakeholders and decision makers. These results can guide model improvements and inform model safeguards such as the creation or improvement of filters.

### Video - [Harms Prevention](https://www.cloudskillsboost.google/course_templates/1036/video/513300)

* [YouTube: Harms Prevention](https://www.youtube.com/watch?v=OcrK0RpsIXA)

Let's learn some methods to address harm prevention. Harm prevention means to avoid showing harmful content, which may still be generated despite best responsible generation efforts. Input and output safeguards are essential aspects of AI safety that play crucial roles in preventing harm. They act as protective measures to guard what goes into an AI system, as an input, and what is produced by the AI system, as an output to ensure that AI behavior aligns with safety standards and ethical principles. Machine learning-based classification systems that classify whether an input text is safe or not, it's called a safety classifier, while it is technically possible to build an original safety classifier by yourself. Practically, it is a challenging task since a safety classifier needs to be trained carefully with specially consolidated datasets and with special care and attention towards fairness. Fortunately, there are a few safety classifiers available in the AI industry to help people prevent outputting harm from a model. In 2017, Google's Jigsaw team released prospective API, one of the first safety classifiers in the world. Prospective API is designed to help flag harmful speech that pushes people out of online spaces, pushes the towards violence, or even worse. By the time 2021 came around, prospective API reached around 500 million requests daily due to high usage in many online platforms. Currently, prospective API is widely used for input safeguards to guard input for the AI and for output safeguards to monitor output. While prospective API was one of the first safety classifiers, safety classifiers now exist from other organizations, including moderation API from OpenAI and Llama guard from Meta. May people think of block lists when talking about input safeguards. In very simple situations, block lists can be used to identify and block unsafe prompts. However, this method is brittle and often not sustainable over time. A better long-term solution is using classifiers to tag each prompt with potential harms or adversarial signals. Then you can apply different strategies on how to handle the request based on the type of harm detected. Strategies included to block, rewrite or let the request pass through. If the input is overtly adversarial or abusive in nature, the system can block it from being sent to the model, and instead, output a pre-scripted response. For example, if you ask a chat bot how to rob a bank, the classifiers will recognize this input as a harmful prompt and suggest the system to block it. You will get a reply. I cannot help you with that from the chat bot. Along with why it is a wrong idea and a national support helpline to reach out to, if you believe you are in a difficult situation. If the input is likely to elicit an unsafe response, then prompt engineering, control tokens, or style transfers can be used to attempt to steer the model towards safer generation. Depending on how much the AI model has already been safety tuned or custom tuned for the application use. You can pass through the input with toxic data and trust the model to safely handle the generated output. A well tuned model can produce a more capable response which can be more insightful than the prefix errors or messages. It's worth emphasizing that understanding what kind of toxic data your safety tuned AI model can take is very important in this approach. Even if the input safeguards are in place and the model has been safety tuned, the model may still output unacceptable content. This is why output safeguards are equally important. With output safeguards, you can use classifiers to detect harmful output and perform one of the following actions on the output. Provide an error message, provide a pre-scripted output, reiterate another response with ranking. If the classifier determines that the output is harmful without a doubt, you can simply configure the system to produce an error message in order to avoid delivering responses that are unsafe, bias, or offensive. Using a chat bot as an example, if your Corti contains such content, it will output an error message or refusal to respond. We can also configure the system to output a pre-scripted or semi-scripted output. Sometimes a semi-scripted output conveys more information than a simple error message. Semi-scripted outputs are generated by the AI model to provide a certain level of information on why it is unsafe. For example, instead of delivering an error message, the AI model can output, I cannot help you with that because the information about the weapon can be used to harm people and it violates our policy. Since most generative AI models are designed to be able to create multiple outputs for the same prompt, sometimes it is helpful to use this capability to handle risky outputs. When an unsafe output is detected, we generate multiple additional answers using generative AI and rank them based on the safety score. The model then replies with the output that has the highest safety score. It's worth emphasizing that the final output needs to meet all safety criteria. Meanwhile, there are also some safety fairness trade offs to be aware of. Classifiers are trained on large datasets to learn what is harmful and what is not. An example of this is using large amounts of comments on Wikipedia discussion pages to train a classifier while having a group of workers to review each one for attributes like harassment or personal attack. However, human judgment of toxicity is inherently biased and often doesn't represent the full spectrum of harassment. Even with a large and robust dataset, the model may not always learn the right patterns from it. One concern is around protecting underrepresented groups. Many LLM developers are using classifiers with low toxicity thresholds to remove hate speech, meaning they are flighting more things as toxic when the model is uncertain, which increases the risk for false positives. This may end up eliminating the LLM's ability to say things about underrepresented communities in ways that reinforce negative effects on the historically disadvantaged groups. The performance of classifiers around non-English prompts is important. The models are also more likely to interpret innocent non-English phrases as hate speech and harassment. When hate speech is written in veiled ways, such as with slag or in non-English languages, classifiers can be easily fooled. Machine learning models will always make some mistakes, so it's essential to build in mechanisms for humans to catch and correct accordingly. Having a human involved in a process instead of full automation, also known as human in the loop is always important and highly recommended despite the fact that you can employ classifiers to automate input and output bar railing. Integrating human oversight into the validation and review of AI outputs is important for critical or high risk applications. This allows humans to intervene if necessary and provide feedback to the AI system.

### Video - [Model Traing for Safety: Instruction Fine-tuning](https://www.cloudskillsboost.google/course_templates/1036/video/513301)

* [YouTube: Model Traing for Safety: Instruction Fine-tuning](https://www.youtube.com/watch?v=aIRQ66a0xbY)

Let's take some time to explore model training for safety and discuss what you can do to tune the model itself. Although harm prevention is important to mitigate unintentional behavior and related risks, it provides evasive methods that can reduce AI's helpfulness and capabilities. How do we train an AI model that more closely aligns with our safety values while at the same time, ensuring it still retains AI helpfulness and capabilities. The first simple idea is to filter the training data for safety purposes. By filtering toxic data out from training datasets, you can naturally reduce the possibility that the generative AI model generates harmful output. You can achieve this by using safety classifiers to filter through the data. However, while this approach works for reducing toxic outputs to some extent, it also causes another issue. By limiting the access to toxic sentences, the model may result in poor performance, especially for tasks related to underrepresented groups. This is partially because the automated filtering methods cause false positives, especially for marginalized groups. This results in a reduction in the ability of the model to generate text about these groups, even in a positive way. In this case, there is a trade-off between safety and fairness. Instead of removing toxic data, can we actually teach the concept of safety via a fine-tuning process? While it is still an emerging research topic, there are several known methods, including instruction tuning and reinforcement learning from human feedback, or RLHF. Instruction tuning is a generic fine-tuning method that uses a collection of datasets described via instructions. Incorporating safety-related datasets and instructions allows you to embed the safety concept into the model. Another approach is to use RLHF, this technique optimizes language models directly from human preference feedback. By providing safety-related feedback, you were able to help make the model more aligned to human values. Let's look a bit deeper into the instruction-tuning. Large language models are trained with massive pre-training datasets and pre-training tasks. These tasks are mainly designed to acquire general capabilities, handle language. Instruction-tuning usually follows the pre-training phase and aims to teach more task-related capabilities using a collection of specific test datasets, such as translation, reasoning, question answering, and so on. Each task has different inputs and outputs. For example, in the translation task, the target model gets instruction inputs as a prompt, such as translate this sentence to Spanish. The new office building was built in less than three months. Then it is supposed to provide an output accordingly. The model is trained based on the gap between the responses and the prescripted target outputs. You can include safety-related tests, such as toxic language detection in this instruction-tuning phase. During the tuning process, the model is asked to detect whether a given text has toxic meaning by replying toxic or non-toxic. This approach helps detoxify models significantly from testing. As shown in this graph, from a paper, you can see instruction-tuned PaLM models labeled as Flan-PaLM have a lower probability to generate toxic sentences even based on a prompt that elicits toxic continuation or response than the base PaLM model.

### Video - [Model Traing for Safety: RLHF](https://www.cloudskillsboost.google/course_templates/1036/video/513302)

* [YouTube: Model Traing for Safety: RLHF](https://www.youtube.com/watch?v=gW6JP2f9DIg)

RLHF also tries to embed the concept of safety by optimizing the model using human feedback. RLHF works in the following way. It starts from training a reward model that returns preference scores on a given prompt. To train a reward model, The first step is to generate sets or pairs of responses using an LLM, which can be the target model itself, or another model. Then the responses are passed to a human moderator who ranks them or simply judges which response is preferable. The reward model is then trained using this human preference data set for the next step. For safety purposes, you would usually use a set of input prompts that are picked or designed to elicit harmful responses and the human moderator would evaluate not only the helpfulness of the responses, but also their harmlessness. In the next step, the target model, you want to tune receives prompts and then generates responses accordingly. The trained reward model evaluates the responses and returns the preference score, which is supposed to reflect the human preference it was trained on. Here, the target model is iteratively trained via a reinforcement learning strategy to internalize the human preference through the reward model. By using special prompts for safety and carefully building a human preference dataset for safety with human moderators, you can then embed the safety concept into the model. RLHF is a widely used technique for generated models. But at the same time, you may need to start thinking about how to scale this manual supervision process as AI models acquire general capabilities. Since human supervision can't be perfect and as AI grows, at some point it may start leveraging human incapability to hide undesired outcomes in supervision. Researchers are trying to find a way to scale the supervision approach by using AI in the process. One of these attempts is constitutional AI released by Anthropic. Constitutional AI uses AI evaluation in two types of tuning, supervise and reinforcement learning based tuning. Supervised learning incorporates the process of self critique and revises responses by AI itself. The revised responses are used later as supervised learning targets to fine tune the model. In the reinforcement learning process, AI itself works as the moderator. AI is used to evaluate and create the preference dataset that trains the reward model. Hence, this process is called reinforcement learning from AI feedback or RLAIF. In the entire tuning of constitutional AI, the only human oversight provided is through a list of rules or principles as a form of prompt for self critique and supervised learning and for self evaluation in reinforcement learning. The alignment of AI for safety is still a hot topic and as AI evolves, there will continue to be many interesting ongoing discussions on this.

### Video - [Safety in Google Cloud GenAI](https://www.cloudskillsboost.google/course_templates/1036/video/513303)

* [YouTube: Safety in Google Cloud GenAI](https://www.youtube.com/watch?v=SApdKkUj1Ok)

Now, let's take a look at what Google Cloud tools we can use to ensure AI safety. Google Cloud helps create safer AI models in many ways from data collection, preprocessing, scalable training, prediction environments, and so on. In this lesson, we will focus on direct harm prevention tools that you can use in generative AI applications. Natural language API is one such tool that has the capability to do input-output moderation and pre-trained models that have built-in safeguards capabilities like Gemini. The natural language API provides a powerful set of tools to understand and process text using machine learning, including text classification, sentiment analysis, entity extraction, and so on. In addition to these tools, it also has the text safety moderation capability, which we can use for input and output safeguards. Natural language API has text moderation capabilities that analyze a document against a list of safety attributes, which include harmful categories and topics that may be considered sensitive. Each safety attribute has an associated confidence score between 0.0 and 1.0 to reflect the likelihood of the input or response belonging to a given category. You can define the confidence threshold that is right for your business for each category. To moderate content from a document, make a post request to the document's moderate text, rest method, and provide the appropriate request body. As shown in the examples, we use the sentence shut up as the text to moderate. You can either provide the text as a string or provide the cloud storage bucket file path where the file is stored. Let's say that the text body contains the words shut up. What you would see in the API tool is a response body that contains data with a list of classification categories and their associated confidence score. In this case, the API tool has a high confidence score of 0.8 that the text shut up has a high likelihood of being toxic. You can use a variety of programming languages to call the Google Cloud Natural Language APIs documents moderate text in point, like Python, Java, and Go. Using curl commands is just one of them. Because it is easy to use, we can then easily incorporate safe guardrailing capabilities in any generative AI systems by employing natural language APIs. Some foundational models, including Gemini, have in-built safe guardrailing capability. Gemini is a family of powerful multimodal large language models, LLMs, developed by Google DeepMind. It is also available via Google Cloud. Gemini models can accept text, and image, and prompts, depending on what model variation you choose and output text responses. The Gemini API has adjustable safety settings to help you tailor the safety settings to your business requirements and use case. During the prototyping stage, you can adjust safety settings on four dimensions to quickly assess if your application requires more or less restrictive configuration. With Gemini safeguarding on Google Cloud, safety settings block content with medium or high probability of being unsafe across four categories: harassment, hate speech, sexually explicit, and dangerous. There are four thresholds: Block, always show content regardless of the probability of unsafe content; Block only high, block when there is high probability of unsafe content; Block medium and above, block when there is medium or high probability of unsafe content; And block low and above, block when there is low, medium, or high probability of unsafe content. The safety categories and threshold settings should always be established in pairs based on what you determine is appropriate for your use case. In Gemini, safety ratings include the category and the probability of the harm classification. Gemini outputs a probability of the block confidence levels ranging from negligible, low, medium, and high. So if the probability returns back as high, this means the content has a high probability of being unsafe to that category. For example, while prototyping a new first-person shooter game, a game developer might deem it acceptable to allow more content that's rated as dangerous due to the nature of the game. While Gemini API safety filters are adjustable, core harms such as content and dangers child safety are built-in protections and cannot be adjusted. In Gemini, we can set the category and threshold in pairs to what we need for our use case. For example, you could enter the harm category and harm block threshold into the safety settings parameters. Let's look at a scenario. Let's say we wanted to set the category of hate speech to a threshold of block low and above. This setting is sent to the API along with a prompt you want to provide. In our example, you can see that the output is blocked due to safety reasons. You can also see the safety feedback response for each category and learn which category it was blocked from. With Gemini by default, safety settings block content, including prompts, with medium or higher probability of being unsafe across any dimension or category. This baseline safety is designed to work for most use cases, so you should only adjust your safety settings if it's consistently required for your application. In our example, we cannot set the threshold for the harassment category. Therefore, the block medium and above threshold was applied. Thus, the content was blocked due to the harassment category having a medium probability. It also returns blocked true, along with the category and probability. Meanwhile, the blocked content is not returned. In this feedback, the content has a medium probability in the harassment category, negligible probability in the hate speech category, negligible in the sexually explicit category, and negligible in the dangerous category. The Gemini API gives you access to the latest generative models from Google. Once you're familiar with the general features available to you through the API, you can use your language of choice to develop for your use case.

### Video - [Lab: Safeguarding with Vertex AI Gemini API](https://www.cloudskillsboost.google/course_templates/1036/video/513304)

* [YouTube: Lab: Safeguarding with Vertex AI Gemini API](https://www.youtube.com/watch?v=p71KNl_lMw4)

Now, let's exercise in a hands-on lab. This lab helps you learn how to inspect the safety ratings returned from the Vertex AI Gemini API and how to set a safety threshold to filter responses. In this lab, you will learn how to use the Vertex AI gemini API, inspect the safety ratings of responses, define a threshold for filtering safety ratings.

### Lab - [Safeguarding with Vertex AI Gemini API](https://www.cloudskillsboost.google/course_templates/1036/labs/513305)

In this lab, you learn how to inspect the safety ratings returned from the Vertex AI Gemini API and how to set a safety threshold to filter responses.

* [ ] [Safeguarding with Vertex AI Gemini API](../labs/Safeguarding-with-Vertex-AI-Gemini-API.md)

### Quiz - [Module 2: Quiz](https://www.cloudskillsboost.google/course_templates/1036/quizzes/513306)

#### Quiz 1.

> [!important]
> **Your manager asked you to lead a project to research the key considerations in AI safety and develop a failure mode catalog to assist evaluate safety in Generative AI products. Which of the following should not be included as a failure mode?**
>
> * [ ] Citation
> * [ ] Discrimination.
> * [ ] Hate speech
> * [ ] Violence.

#### Quiz 2.

> [!important]
> **You want to embed the concept of safety into a pre-trained LLM by fine-tuning it. You have a dataset of prompts and pairs of possible answers, along with labels created by human safety evaluators indicating their preference for one answer over the other. Which technique is the most suitable for fine-tuning the LLM in this scenario?**
>
> * [ ] Transfer Learning
> * [ ] Zero-shot Learning
> * [ ] Instruction Tuning
> * [ ] Reinforcement Learning from Human Feedback (RLHF)

#### Quiz 3.

> [!important]
> **You're pitching an AI-powered customer support solution to a potential client. Their company handles highly sensitive user information, and they have reservations about potential risks associated with AI systems. To address their concerns and win their trust, which key message about AI safety would be most effective to emphasize?**
>
> * [ ] "AI safety mechanisms make our systems easier to understand, allowing you greater control and oversight."
> * [ ] "Adherence to AI safety principles helps us build reliable systems that minimize unexpected errors, safeguarding your customer data."
> * [ ] "AI safety prioritizes fairness and avoids biased outcomes, protecting the interests of your diverse customer base."
> * [ ] "AI safety practices increase development speed, enabling us to deliver your project faster and at a lower cost."

## Course Summary

This module provides a summary of the entire course by covering the most important concepts, tools, and technologies.

### Video - [Course Summary](https://www.cloudskillsboost.google/course_templates/1036/video/513307)

* [YouTube: Course Summary](https://www.youtube.com/watch?v=QyCqtLbZN6k)

You've completed this course, responsible AI for developers, privacy, and safety. Let's recap what you have learned. In this course, we introduce privacy in machine learning and some best practices on privacy. Privacy relates to the fifth of Google's AI principles, incorporate privacy design principles. We described tube approaches to help developers achieve privacy and training data, de-identify your data, and randomize your data. To de-identify your data, you can apply redaction, replacement, masking, tokenization, bucketing, or shifting. To randomize your data, you can apply data perturbation or differential privacy. It is recommended to apply the most suitable technique based on your business requirements for data privacy. We introduced two approaches to help developers achieve privacy in machine learning model training, Differentially Private Stochastic Gradient Descent, also known as DP-SGD, and federated learning. You also learned about system security best practices in Google Cloud, focus on sensitive data protection with the Cloud Data Loss Prevention DLP API, encryption with Key Management Service, KMS, access control with Identity and Access Management, IAM, and monitoring. We also introduced AI safety. Safety directly relates to the third of Google's AI principles, be built and tested for safety. You learned about several major failure modes and adversarial testing techniques for safety evaluation. You identified what are safety classifiers and what are input, output safeguards that help you prevent harm. You learned about two approaches, instruction-tuning and reinforcement learning from human feedback to teach safety to AI models. You also learned the safety settings in Gemini and Natural Language API. As artificial intelligence continues its rapid ascent, the conversation around responsible AI becomes ever more vital. New technological developments constantly present fresh challenges and opportunities in this domain. It's even more important now to ensure that when you develop for AI, you are equipped with the latest insights and best practices for responsible AI implementation.

### Document - [Reading](https://www.cloudskillsboost.google/course_templates/1036/documents/513308)

## Course Resources

Student PDF links to all modules

### Document - [Module 0: Course Introduction](https://www.cloudskillsboost.google/course_templates/1036/documents/513309)

### Document - [Module 1: AI Privacy](https://www.cloudskillsboost.google/course_templates/1036/documents/513310)

### Document - [Module 2: AI Safety](https://www.cloudskillsboost.google/course_templates/1036/documents/513311)

### Document - [Module 3: Course Summary](https://www.cloudskillsboost.google/course_templates/1036/documents/513312)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
