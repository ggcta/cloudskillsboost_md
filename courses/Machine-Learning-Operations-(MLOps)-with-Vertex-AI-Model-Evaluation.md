---
id: 1080
name: 'Machine Learning Operations (MLOps) with Vertex AI: Model Evaluation'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1080
date_published: 2025-01-07
topics:
  - Machine Learning Operations
  - MLOps
  - Machine Learning
---

# [Machine Learning Operations (MLOps) with Vertex AI: Model Evaluation](https://www.cloudskillsboost.google/course_templates/1080)

**Description:**

This course equips machine learning practitioners with the essential tools, techniques, and best practices for evaluating both generative and predictive AI models. Model evaluation is a critical discipline for ensuring that ML systems deliver reliable, accurate, and high-performing results in production.
Participants will gain a deep understanding of various evaluation metrics, methodologies, and their appropriate application across different model types and tasks. The course will emphasize the unique challenges posed by generative AI models and provide strategies for tackling them effectively. By leveraging Google Cloud's Vertex AI platform, participants will learn how to implement robust evaluation processes for model selection, optimization, and continuous monitoring.

**Objectives:**

* Understand the nuances of model evaluation in both predictive and generative AI, recognizing its crucial role within the MLOps lifecycle.
* Identify and apply appropriate evaluation metrics for different generative AI tasks.
* Efficiently evaluate generative AI with Vertex AI's diverse evaluation services, including both computation-based and model-based methods.
* Implement best practices for LLM evaluation, to ensure robust and reliable model deployment in production environments.

## Welcome to the Machine Learning Operations (MLOps) with Vertex AI: Model Evaluation

This module covers the course objectives and  provides an overview of the course structure.

### Video - [Welcome to the course](https://www.cloudskillsboost.google/course_templates/1080/video/520167)

* [YouTube: Welcome to the course](https://www.youtube.com/watch?v=bCEWudTXzzw)



## Introduction to Model Evaluation

This module introduces model evaluation challenges and solution offerings by Vertex AI.

### Video - [Introduction to Model Evaluation](https://www.cloudskillsboost.google/course_templates/1080/video/520168)

* [YouTube: Introduction to Model Evaluation](https://www.youtube.com/watch?v=JzuGAexNFHA)

Let's begin our first section of machine learning operations with Vertex AI, exploring model evaluation. Specifically, what it is, why it matters, when it occurs, how it's done, and who cares about the results? What is model evaluation? It's a process of assessing how well a machine learning model performs. It's like a quality check to make sure the model lives up to its tasks. Is it accurate? Does it make correct predictions? Is it reliable? Does it perform consistently well, even on new data? Does it align with the business and effectively support the intended goals? Why does model evaluation matter? The first reason is performance. Through evaluation, we gain a clear understanding of a model's performance by measuring various metrics. Second is generalization. We want models to work on new data, not just the data they were trained on. This is crucial for deploying models in real world scenarios where the data may vary. Third is model selection. When there are multiple models to choose from, evaluation helps us identify the most effective one by comparing their performance, thereby, optimizing computational resources for deployment. Fourth is improvement. Performance is tracked after deployment to catch issues and show when retraining the model is necessary. Consistent evaluation is crucial for recognizing any degradation or shifts in data patterns which fosters continuous improvement and adaptation to constantly changing conditions. Last is decision making. Evaluation gives stakeholders confidence in the model by helping them make informed choices about model deployment, updates, or adjustments based on real world performance. Model evaluation is pivotal in the machine learning life cycle because it helps guarantee that models are effective, adaptable, and do what they're made to do. While model evaluation is crucial throughout the ML workflow, we'll focus on two key stages, after training and after deployment. Following the initial training of a model, evaluation metrics are examined to determine whether the model is ready for deployment. By comparing these metrics across multiple models, machine learning practitioners can select the most effective one. After a model is deployed, its performance is continuously evaluated using new data to identify any potential decline in effectiveness. If a decline is detected, the model may need to be retrained. This ongoing evaluation process is known as continuous evaluation and ensures that models remain accurate and adaptable in real world scenarios. How is model evaluation done? When it comes to actually evaluating model performance, there are two concepts, model evaluation techniques, and evaluation metrics. Before choosing the appropriate evaluation technique and metric, it's important to understand the type of model being used, for example, classification, regression, ranking, etc. Since it will ultimately inform how the performance is measured. Model evaluation techniques are the overarching processes used to assess a model's performance. These techniques typically involve dividing the dataset into separate portions for training and testing, allowing us to assess how well the model generalizes to unseen data. Common techniques include holdout validation, K-fold cross validation, and leave one out cross validation. In addition to model evaluation techniques, evaluation metrics are used to quantify a model's performance. These metrics act like a numerical scoring system, converting the model's predictions into values that tell you how well is performing a specific task. For classification tasks, examples include accuracy, precision, recall, and F1-score. For regression tasks, there are mean squared error and R-squared. For NLP tasks, there are bleu and rouge. The choices of which metrics to use depend on various factors, including the type of model, the specific problem being solved, and which aspects of performance are most important. Imagine you're baking a cake. Model evaluation techniques encompass the entire process of baking a cake. What flavors are you combining? How are you measuring and mixing the ingredients? How long do you prove and bake and at what temperature? How long do you let it cool? Similarly, model evaluation techniques involve procedures like is the data being preprocessed? How is the model being trained? How is it being tested and validated? For evaluation metrics, think about how you want to judge the quality of a cake. Are the intended flavors coming through? Does it have a pleasant texture and appearance? These metrics, just like accuracy, precision, recall, and F1-score, help determine how successful the outcomes are. Model evaluation techniques provide the structure for testing the model or baking the cake, while evaluation metrics provide the performance scores within that structure. Both are essential components to a successful model or a delicious cake, depending on which one you're doing. There are a few key factors to consider when selecting evaluation techniques and metrics. One is model type. Is it a classification, regression, ranking, or other type of model? Another is the project goal. What aspects of performance are most critical to achieving the project goal? Next is dataset size. Smaller datasets may favor simpler techniques like holdout validation, and larger datasets can handle more complex methods, like K-fold cross validation. Next is computational cost. Complex techniques can be resource intensive. Consider the trade off between computational cost and accuracy. Lastly, is bias variance trade-off. Techniques like bootstrapping can help assess model bias and variance and provide insights into potential over-fitting or under-fitting issues. Additional considerations include the cost of errors, data balance, and the use of multiple complimentary metrics. Considering the cost of errors, if an error were to occur, how critical would that be? Think about an error in a medical diagnosis, for example. Imbalanced datasets where classes are not equally represented may require alternative metrics beyond standard accuracy. Finally, are multiple complimentary metrics like precision, recall, and F1-score being employed to gain a well rounded understanding of your model's performance. This is not a one size fits all process. Thoughtful selection of techniques and metrics tailored to your specific model and project goals will lead to the most meaningful and actionable insights. Model evaluation takes a lot of consideration, but who is benefiting from all this? It's actually crucial to a wide range of people involved in the project. Data scientists and machine learning engineers rely on evaluation results to fine tune, enhance, and choose the best performing models. To business leaders, model evaluation offers concrete data on how the model impacts important business aspects, such as revenue, expenses, and customer happiness. Software developers use evaluation results to smoothly integrate models into their systems, making sure they are reliable and effective. End users care about the accuracy and trustworthiness of the models they interact with. Regulatory bodies want proof that models adhere to ethical AI standards and regulations. Researchers use the results to improve machine learning methods and develop best practices for the field. Model evaluation is a critical step in the machine learning life cycle. It ensures that models are effective, accurate, and adaptable to real world challenges.

### Video - [Model Evaluation within MLOps](https://www.cloudskillsboost.google/course_templates/1080/video/520169)

* [YouTube: Model Evaluation within MLOps](https://www.youtube.com/watch?v=PuYi1LEs8v8)

Regarding the larger picture. Machine learning operations, or MLOps, is a practice that combines the best of developer operations with machine learning. It bridges the gap between developing a machine learning model and operationalization, or in other words, integrating it into a production environment, so that it can make predictions or take actions on real world data. Machine learning operations promotes collaboration, iteration and systemization. Collaboration between data scientists, engineers, and stakeholders, iteration through continuously refining models through regular evaluation and updates, and systemization by providing a framework for managing model evaluation, ensuring long term model reliability and reproducibility. This all leads toward accurate, robust, and business driven machine learning models. An end to end platform like vertex AI simplifies this process. It unifies Google Cloud's powerful AI services, and excels at model building, deployment, and scaling, which streamlines the entire ML development lifecycle. To learn more about each of these roles, check out the machine learning operations getting started course, which is linked in the document at the end of this course. Mature MLOps governance, is achievable with vertex AI's comprehensive model evaluation capabilities. This includes robust tools for model evaluation, which are crucial for ensuring the effectiveness and reliability of your models. While models are being built, trained, and deployed, it's important that model evaluation is intentionally embedded into the MlOps lifecycle. By doing so, teams are better able to identify risks, malfunctions, biases, and other issues that can hinder model performance and user experience. You as an ML practitioner, can streamline your workflows by increasing your organization's MLOps maturity level, which involves automating and integrating training, validation, and deployment phases. Vertex AI model evaluation services, allow you to run evaluations iteratively on new datasets at scale. Utilize advanced visualization to compare and select the best model for production, and assess performance across different slices and annotations for comprehensive understanding of model capabilities. MLOps maturity levels offer a general roadmap, for developing and deploying machine learning models, and provide an overarching framework for ML processes. Vertex AI can support model evaluation and streamline workflows in various ways. For one, it provides powerful tools to assess model performance throughout the ML lifecycle. It enables automating and integrating evaluation into your training and deployment processes, which establishes a robust feedback loop, for continuous improvement crucial for achieving ML maturity. Also, with key features like scalability, insightful comparisons, and fairness checks, Vertex AI's model evaluation services empower you to deploy reliable and effective AI. Ultimately, leveraging Vertex AI for model evaluation, increases the quality of your models while accelerating your journey towards establishing a mature, production ready MLOps environment.

### Video - [Model Evaluation Challenges and Solutions offered by Vertex AI](https://www.cloudskillsboost.google/course_templates/1080/video/520170)

* [YouTube: Model Evaluation Challenges and Solutions offered by Vertex AI](https://www.youtube.com/watch?v=XXD1yuyaUt8)

Even the best models face challenges. There are some common issues in model evaluation that can impact the trustworthiness of your ML models. Getting familiar with the main challenges in model evaluation is a good start. So what are they? First are data issues. One common issue with data is overfitting, where a model performs exceptionally well on its specific training data, but struggles to generalize with new unseen data. Another challenge is data shift or concept drift, which happens when the distribution of real-world data changes over time. This can lead to a decline in model performance if it's not regularly updated. Another challenge is a lack of representative data. If the training data doesn't adequately represent the full range of scenarios the model will encounter, it may be inaccurate in real-world applications. Metric choice is another potential challenge in model evaluation. Relying on just one metric, such as accuracy, can be misleading, as it might not capture the full picture of a model's performance. A model might have high accuracy, but still be poor at classifying minority classes or making balanced predictions. It's also important to choose a metric that aligns with the specific goals of the machine learning project. A mismatch here can lead to optimizing the wrong aspects of performance. Another challenge lies in interpretability, which is understanding complex black box models. While these models, like deep neural networks, can be highly accurate, their inner workings are often difficult to decipher. This makes it harder to explain why they make specific predictions, which can be a concern in heavily regulated industries or when high-stake decisions are involved. Bias and fairness pose significant challenges in model evaluation. Inherent bias and data can lead to discriminatory outcomes. If training data has biases embedded in it, these biases can be perpetuated and amplified by the model, leading to unfair or discriminatory outcomes. Over time, a model's performance can degrade as the real-world environment changes or the data it encounters shifts. This necessitates continuous monitoring after deployment to detect any declines and intervene when necessary. Additionally, integrating the machine learning model into the larger production system can present technical, logistical, and communication challenges between different teams. While challenges exist, there are strategies to mitigate them. Comprehensive evaluation using multiple metrics such as accuracy, precision, recall, F1-score, and AUC-ROC provides a multi-dimensional view of the model's performance. Meticulous data validation and appropriate splitting techniques, like stratified sampling and cross validation, ensure that the assessment of the model's performance is reliable and reflects how well it's likely to perform in the real world. Overfitting can be addressed through various techniques. Firstly, regularization, which discourages the model from learning the noise and random fluctuations in the training data, and encourages it to focus on the underlying patterns instead. Secondly, drop out, which forces the network to learn multiple redundant representations of the data, making it more robust and less likely to overfit. Thirdly, early stopping, which is where you monitor the model's performance and stop the training process when it starts to worsen. Model monitoring tracks performance over time and detects data drift or concept drift. Bias and fairness considerations employ fairness metrics and techniques to identify and mitigate potential biases in models and data. Explainability tools use methods like LIME, SHAP, or feature importance to improve the interpretability of complex models. Although various strategies exist for effective model evaluation, implementing them all can be complex. Vertex AI offers powerful tools that map directly to these essential functionalities. Thanks to the evaluate, compare, and assess capabilities of Vertex AI model evaluation, assessment and selection of the best model for production deployment by iteratively running evaluations on new datasets can be done at scale. Also, advanced visualization is available to compare and select the optimal model for production and assess performance across different slices and annotations for a comprehensive understanding of model capabilities. To evaluate a model using Vertex AI, you'll need a trained model, which can be created through Vertex AI's AutoML functionality or custom training processes; batch prediction output, where you run a batch prediction job on your trained model to obtain prediction results; and a ground truth dataset to prepare your label data. This is the correct answer as determined by human reviewers and, typically, your test dataset used during training. Once you have these three elements, Vertex AI provides comprehensive model evaluation metrics to assess performance. The model evaluation provided by Vertex AI can fit in the typical machine learning workflow in several ways. During model selection, review model evaluation metrics after you train your model, but before you deploy, you can compare evaluation metrics across multiple models to help you decide which model to deploy. After your model is deployed, continuously evaluate your model with new incoming data. If the evaluation metrics show that your model performance is degrading, consider retraining it. This process is called, no surprise here, continuous evaluation. A typical workflow for evaluating your model in Vertex AI goes something like this. First, you train your model. You can choose to train your model by using AutoML or a custom training approach. Second, run batch predictions. Generate prediction results for your trained model by executing a batch prediction job. Third, prepare ground truth. Ensure your ground truth dataset, which is the correctly labeled data is ready for comparison. Fourth, start the evaluation. Initiate an evaluation job. This will automatically compare your model's predictions with the ground truth data. Fifth, analyze metrics. Review the evaluation metrics to understand your model's performance. Sixth, iterate and improve. Leverage insights from your metrics to refine your model. Run multiple evaluations to compare different versions and configurations to optimize performance. What are the supported data modalities and model types? Vertex AI offers robust evaluation capabilities for classification, regression, and forecasting models across various data modalities, including images, text, video, and tabular data. For more information on this, check out the Vertex AI documentation on the Google Cloud's site, which is linked in the documentation at the end of this course. Model evaluation is crucial for responsible AI. By understanding its challenges and using tools like Vertex AI, you'll build better, more reliable, and fairer models. You'll also be well-equipped to tackle the unique challenges associated with evaluating and deploying cutting edge generative AI models.

### Quiz - [MLOps: Introduction to Model Evaluation Quiz](https://www.cloudskillsboost.google/course_templates/1080/quizzes/520171)

#### Quiz 1.

> [!important]
> **You're working on a machine learning project and need to evaluate your model's performance. Which of the following scenarios would benefit from using Vertex AI's model evaluation service? Select all that apply.**
>
> * [ ] You are concerned that your model may be overfitting to your training data and want to assess its performance on unseen data.
> * [ ] You need to monitor your deployed model's performance over time and detect potential issues like concept drift.
> * [ ] You want to get detailed feedback from users about the quality and relevance of your model's predictions.
> * [ ] You have a large dataset and need to compare multiple model versions to find the best one.

#### Quiz 2.

> [!important]
> **An ML engineer is developing a customer churn prediction model. During model evaluation, they notice the model performs exceptionally well on the training data but poorly on new, unseen data. Which of the following concepts best describes this issue?**
>
> * [ ] Data shift
> * [ ] Bias-variance tradeoff
> * [ ] Overfitting
> * [ ] Generalization

#### Quiz 3.

> [!important]
> **A data science team is struggling to manage the lifecycle of their machine learning models from development to deployment. They face challenges with inconsistent model performance, difficulty tracking model versions, and a lack of collaboration between team members. Which of the following best describes how adopting an MLOps approach with Vertex AI could address these issues?**
>
> * [ ] MLOps with Vertex AI would mainly focus on model evaluation, neglecting other crucial aspects like deployment and monitoring.
> * [ ] MLOps with Vertex AI would only help with model deployment, not addressing issues like inconsistent performance or collaboration.
> * [ ] MLOps with Vertex AI would provide a structured framework for managing the entire ML lifecycle, promoting collaboration, enabling version control, and improving model performance consistency.
> * [ ] MLOps with Vertex AI would primarily focus on automating model training, but wouldn't address versioning or collaboration challenges.

#### Quiz 4.

> [!important]
> **An ML engineer is working on a large-scale project that involves training multiple machine learning models. They are evaluating a new model and want to ensure it can adapt to changes in real-world data over time. Which of the following evaluation strategies should the engineer prioritize?**
>
> * [ ] Continuous evaluation to monitor model performance on new data after deployment and retrain as needed.
> * [ ] Cross-validation to assess the model's average performance across multiple data folds.
> * [ ] Leave-one-out cross-validation to assess the model's performance by leaving out each data point and retraining.
> * [ ] Holdout validation to assess the model's performance on a single, fixed dataset.

### Document - [Reading List](https://www.cloudskillsboost.google/course_templates/1080/documents/520172)

## Model Evaluation for Generative AI

This module describes challenges of evaluating the Generative AI tasks and best practices to overcome these challenges. The module also covers the different types of model evaluation services available in Vertex AI and then  introduces Vertex AI Automatic Metrics, Automatic Side by Side  and  Safety Bias evaluation services.

### Video - [Challenges of evaluating the generative AI tasks - Introduction](https://www.cloudskillsboost.google/course_templates/1080/video/520173)

* [YouTube: Challenges of evaluating the generative AI tasks - Introduction](https://www.youtube.com/watch?v=P41wPY0_C8Q)

Now that you're aware of model evaluation's critical role in traditional machine learning, let's unlock the exciting world of generative AI, where models can create realistic images, compose new music, or generate compelling stories. With this innovative power, comes the challenge of evaluation, which is especially crucial for ensuring the reliability and quality of generative AI outputs. Though both are forms of AI, there is a distinction between non-generative and generative AI. While non-generative or predictive AI focuses on analyzing existing data to make predictions, classifications, or decisions, generative AI creates entirely new content by learning patterns from vast datasets. Therefore, the output is more open-ended and creative. While predictive and generative AI share some MLOps principles, evaluating generative AI presents novel challenges due to its creative nature. Let's narrow the focus to large language models, or LLMs, for short, which are a specific type of generative AI model that's revolutionizing the way we interact with language. While LLMs and generative AI are often used interchangeably and both refer to AI models that generate human-like text, they're not quite the same thing. Generative AI is a broader term that encompasses models capable of generating various types of content beyond text, such as images, music, or even code. LLMs are a subset of generative AI, specializing in language tasks like generating text, translating languages, and summarizing information. For our purposes in this course, we'll focus primarily on LLMs to keep our discussions simple and focused. Before evaluating LLMs in production, it's helpful to understand the fundamental components that are involved throughout the entire life cycle, which we can call the LLM block. Throughout the life cycle, evaluation plays a vital role. In generative AI, the journey begins with selecting the right pretrained model. This is a foundational decision that demands data-driven insights. At the heart of the language model ecosystems are the LLMs. These models interact with various components like data sources, prompt templates, memory, tools, agent control flows, and guard rails. Each component plays a crucial role in shaping the model's behavior and output, making the overall system more complex than traditional machine learning models. LLMs are the core reasoning engine, accessible via APIs from platforms like Google, or open source alternatives like Mistral. Data truly serves as the cornerstone of the entire process. Data sources provide contextual information through various sources like relational, graph, and vector databases, which are crucial for retrieval, augmented generation. Prompt templates consist of standardized instructions given to the model. They're shared across requests and typically versioned and managed akin to code using formats like the prompt file. Memory functions as a dynamic data source, storing and retrieving past interactions with the model for context and subsequent requests. Tools extend the model's capability by enabling interactions with external systems, such as API calls, code execution, and other integrations. Agent control flow enables the model to iteratively refine its approach to a task, making multiple attempts until predefined stopping criteria are met. Guard rails, as the name suggests, are safety mechanisms applied to the model's output before it reaches the user. Ranging from simple logic, like keyword detection, to invoking secondary models. These measures can trigger fallback to human review when necessary. These individual components present a vast and distinctive design space to explore, requiring careful configuration and consideration. It's a paradigm shift from traditional model evaluation, which primarily focuses on optimizing model parameters and hyperparameters to enhance generalization and predictive performance on unseen data, rather than orchestrating the intricate interaction of diverse components.

### Video - [The Art and Science of Evaluating Large Language Models](https://www.cloudskillsboost.google/course_templates/1080/video/520174)

* [YouTube: The Art and Science of Evaluating Large Language Models](https://www.youtube.com/watch?v=mgnS-ASWFb4)

Evaluating large language models is not like evaluating predictive models, where you establish objectives, select evaluation techniques, gather datasets, and then analyze and interpret outcomes. The scale, complexity, and diverse tasks handled by LLMs present unique evaluation challenges. These challenges include data-related issues like lack of data and data contamination, the difficulty of interpreting model decisions due to their large decision space, biases and evaluation, ensuring generalization to real-world scenarios, and security concerns like adversarial attacks. Evaluating LLMs starts with data, but unlike traditional models, finding the right data and ensuring its quality can be difficult. The initial challenge stems from a lack of data. In traditional predictive machine learning, we typically begin by assembling a substantial dataset. However, generative models can start with minimal or even no data at all. While this accelerates the starting process, lack of sufficient data can inhibit establishing a clear benchmark for what constitutes a good output. There's also data contamination. Foundation models draw upon diverse data sources, some of which may not be entirely shared by the organization that developed the LLM. Consequently, it's difficult to ensure that the training data doesn't contain instances of test data, which can undermine benchmarking procedures. Last of the data-related challenges is limited reference data. Certain evaluation methods like BLEU or ROUGE necessitate reference data for comparative analysis. However, acquiring high-quality reference data poses challenges, particularly in scenarios with multiple acceptable responses or open-ended tasks. Limited or biased reference data might fail to encompass the entirety of acceptable model outputs. Also, how do you measure dataset quality and criteria determination? In other words, what makes a good dataset for evaluating LLMs? Unlike with predictive tasks, defining a good dataset for evaluating LLMs is still an open question. Regarding the model complexity and decision-making challenge, the sheer scale and internal workings of LLMs make it difficult to decide on the best model configuration and interpret their outputs. The vast range of choices in model development from training to selection, customization, and in-context learning, presents a complex decision space. Each of these options require substantial exploration and resources. Bias in large language models is a serious concern, leading to unfair outcomes and amplifying social inequalities. To ensure fair and ethical use of LLMs, we must provide bias detection and mitigation, carefully evaluating the impact of different techniques. Benchmarking LLMs is essential for generalization and real-world applicability. It's important to remember that the real world is far messier than any standardized test. While benchmarks help us compare systems and establish performance rankings, they may not fully encapsulate the diverse challenges LLMs face in real-world deployment. Therefore, it's important to consider how well these controlled results generalize to more complex, unpredictable scenarios. Also, security is a major concern. LLMs are vulnerable to manipulation through adversarial attacks, where crafted inputs can cause them to generate wrong or harmful outputs. These attacks can involve manipulating model predictions or poisoning the training data. This vulnerability to adversarial attacks exposes a gap in current evaluation methods, highlighting the need for ongoing research and robustness assessment. LLM evaluation isn't just about technical metrics. Evaluating creative outputs inherently involves subjectivity, and navigating various evaluation methods becomes even more complex when considering this inherent subjectivity. The rapid emergence of new evaluation methods can challenge established practices. However, staying adaptable to these advancements is crucial for ensuring reliable LLM evaluation. Evaluating generative tasks is complex, and understanding evaluation results can be tricky. Unlike single-answer problems, understanding the nuances of outputs requires robust methodologies for meaningful insights. Considering this, you can understand why evaluating LLMs or, more broadly, generative AI models is so important.

### Video - [Beyond Accuracy: Mastering Evaluation Metrics for Generative AI](https://www.cloudskillsboost.google/course_templates/1080/video/520175)

* [YouTube: Beyond Accuracy: Mastering Evaluation Metrics for Generative AI](https://www.youtube.com/watch?v=8OXhclPLvV0)

Evaluating components within the LLM block starts with the selection of the right evaluation type. There's a spectrum of evaluation types available ranging from simple to more complex. The simplest form is binary evaluation, which is a yes/no or pass/fail judgment. It's straightforward and easy to implement, but lacks detailed feedback on the degree of correctness or quality. Examples include spam detection, positive or negative sentiment analysis, and appropriate or inappropriate content moderation. Next is categorical evaluation, which offers more than two options for slightly more nuanced feedback. They can avoid forced binary choices by providing categories like neutral in addition to positive and negative. But there's a challenge because of increased complexity in defining and interpreting these categories. Examples of categorical evaluation include positive, neutral, or negative sentiment analysis, product ratings of one through five stars, or toxicity classifications such as not toxic, mildly toxic, or very toxic. Third is ranking evaluation, which assesses the relative quality of multiple outputs or models, often using preference-based judgments. By considering user preferences in specific criteria, it helps identify top performing models and configurations. This approach can be resource-intensive, since it requires multiple outputs or models and may not offer absolute performance metrics. Examples are ranking different summaries of the same document based on relevance and readability and comparing the output of different dialogue systems based on user satisfaction. Fourth is numerical evaluation, which assigns a quantitative score to model outputs, this can be a single number like accuracy percentage or a set of numbers like BLEU, ROUGE, or F1 scores. More on those in a bit. While they provide objective and comparable results, they may not capture all nuances of model behavior, especially in complex tasks. Fifth is text evaluation, which uses human generated feedback in the form of comments, critiques, or ratings. These can be short summaries or detailed explanations of the output strengths and weaknesses. An example is domain experts evaluating the factual accuracy of generated summaries. These capture qualitative aspects of performance and provide rich insights that numerical metrics might miss, but can be time-consuming and subjective, making it difficult to scale or compare results. Last is multitask evaluation, which combines multiple judgment types for comprehensive evaluation, capturing both quantitative and qualitative aspects of model performance. Because of this, it provides both quantitative and qualitative insights, but it requires careful design and can be more resource-intensive than single task evaluations. An example is evaluating a language model on translation, summarization, and question answering using numerical metrics and human ratings. How do we actually apply these evaluation types to LLMs? There are some metrics used by natural language processing or NLP that can be used for LLMs. However, due to the complexity and diversity of LLM responses, there's a need for metrics that can properly assess things like creativity, safety, and fairness that go beyond NLP evaluation. It might be helpful to think of these evaluation types like a teacher evaluating a student essay. Let's start with lexical similarity. Does the LLMs output use similar words to the expected answer? To get this, we use the model's output measured against one or more reference texts, which are typically created by humans. It measures the similarity between the model's output and reference text based on word overlap, sequence of words, or semantic similarity. In other words, it's just evaluating if the output is using the words it should be using, but it's not considering things like grammatical correctness or fluency. Examples are BLEU, which focuses on precision, ROUGE, which focuses on recall, and METEOR, which focuses on precision and recall. Think of it like evaluating if the student is using appropriate vocabulary words in the essay. Next is linguistic quality. This is like checking for good sentence structure and clarity because it focuses on the quality of generated texts in terms of fluency, coherence, and grammatical correctness. Examples include BLEURT, a text generation metric based on BERT, or human evaluation of fluency and coherence. Another common metric which measures the overall linguistic quality is perplexity. Perplexity quantifies how well the language model predicts the next word in a sequence. Perplexity doesn't measure human-centric qualities like fluency, coherence, relevance, or safety, which are crucial for evaluating the overall quality and usefulness of the generated text. While a lower perplexity score often correlates with better human evaluated fluency and coherence, it's important to use a combination of metrics to gain a more comprehensive understanding of the model's performance. Next is task-specific metrics. Does the LLM complete the specific tasks it's supposed to do? This is like making sure the essay addresses the assignment. Examples of measuring this are exact match for question answering, ROUGE for summarization, and BLEU for machine translation. Next is safety and fairness. Does the LLM avoid generating harmful, bias, or offensive content? This is pretty much the same for the student essay. This can be measured through human evaluation for bias and safety and specialized tools for detecting hate speech or harmful language. Next is groundedness. Does the LLM demonstrate understanding of real-world facts and concepts? Does the essay make sense? Or is the student making things up? Ways to examine this are fact-checking tools, knowledge-based integration, and human evaluation of factuality. Last is user-centric metrics. Do people find the LLM's output helpful and satisfying? Does the teacher find the essay engaging and easy to understand? Focus on the user's experience and satisfaction with the model's outputs. Some examples are user surveys, task completion metrics, and user engagement metrics. Each metric requires different evaluation methods from automated techniques to human judgment. By combining them, you gain a holistic understanding of the LLM's capabilities and limitations. It's important to note that some metrics might fit into multiple categories. For example, BLEU can be considered lexical similarity and task-specific for machine translation. Also, the specific metrics used within each category can vary depending on the task and the available resources. Diversity also plays a crucial role in LLM evaluation, ensuring models can produce a wide range of responses. Diversity metrics focus on measuring the variety and range of outputs a model can generate with the goal of avoiding repetitive or generic responses and producing creative, informative, and contextually relevant texts. One diversity metric is distinct-n, which provides a simple measure of lexical diversity by calculating the number of unique sequences of words or n-grams in the generated text. Another metric is entropy, which quantifies the unpredictability of a model's output. For example, is there a lot of randomness in the generated text or is it more repetitive and formulaic? Higher entropy generally indicates more diversity. Another is self-BLEU, which is like the BLEU metric, but it's measured against the generated text as opposed to a reference text. Lower self-BLEU suggests more diverse responses and another metric is measuring the automatic evaluation of vocabulary usage or MAUVE, which compares the distribution of words in generated text to a large collection of human written texts to assess the variety of words used. Finally, there's coverage, which measures how well the model's output includes the various words, phrases, or concepts present in a reference dataset. You can use diversity metrics in several ways, like assessing creativity, identifying repetitive patterns, comparing models, and fine tuning models. High diversity doesn't always guarantee high quality, so it's important to strike a balance between diversity and other aspects of text quality such as relevance and coherence. Supplementing automated diversity metrics with human judgment allows for a more nuanced understanding of the richness and variety of generated text.

### Video - [Best Practices for LLM Evaluation](https://www.cloudskillsboost.google/course_templates/1080/video/520176)

* [YouTube: Best Practices for LLM Evaluation](https://www.youtube.com/watch?v=T-gOrZu5644)

Now that you've explored various evaluation aspects, let's discuss some model evaluation best practices within the ML workflow. There are two main phases of model evaluation, pre-production and in-production. Pre-production evaluation includes designing prompt templates, selecting the model, and optimizing customization choices like tuning parameters, while in-production evaluation continuously monitors performance. The complexity of generative AI outputs makes evaluation a critical ongoing process. Model evaluation ensures that your generative models are tailored to the specific needs of diverse users. By measuring model performance against user-defined benchmarks and objectives, you can refine your models through techniques like prompt engineering and tuning, ensuring they align with operational needs, cultural considerations, and business objectives. These evaluations serve as a continuous feedback loop, guiding the development and improvement of your models while guaranteeing usefulness, safety, and effectiveness for your users. Let's say you're part of an AI team at a major media company, spearheading a project called Trend Identification. Your goal is to revolutionize how the company spots emerging trends in the new cycle. Currently, this process is manual, slow, and prone to errors. To address this challenge, your team is exploring the power of large language models. The vision is to build a tool that can sift through massive amounts of news, social media, and reader comments, automatically identifying patterns, and surfacing the topics that are heating up or cooling down in real time. The key is finding the right LLM for the job. Your approach involves training an LLM to pinpoint key concepts, group related articles together, and even analyze sentiment, and extract keywords. This information will be displayed on a dynamic dashboard, giving your editorial team a powerful tool to make data-driven decisions and stay ahead of the curve. Suppose that you have at least two models to evaluate more than 1,000 published articles per day and at least two metrics to consider. Given these factors, a few key challenges are likely to emerge. First, creating a scalable model evaluation framework that seamlessly handles the outputs of both models regardless of complexity or size is a major hurdle. Additionally, the evaluation framework must be able to validate results by accurately calculating metrics across a vast amount of data. Finally, to maximize efficiency and adaptability, the ideal framework should be reusable and capable of calculating at least the two required metrics with minimal effort or customization. Several key strategies can significantly enhance the effectiveness of LLM evaluation. Employ multiple evaluation metrics, avoid relying on a single metric. Instead, combine multiple metrics to comprehensively assess various aspects of LLM performance, including accuracy, fluency, coherence, relevance, and task completion. Incorporate human judgment, mitigate subjectivity by using multiple human judges and conducting inter-rater reliability checks. Consider crowd sourcing to gain diverse perspectives and increase the scale of evaluation. Leverage domain specific data. Incorporate domain-specific or industry-specific evaluation datasets to better assess the model's performance in real world scenarios. Adopt MLOps for generative AI tools. Automate LLM evaluation and streamline the process. Rather than manually assessing your LLM after each round of fine-tuning, set up an automated process to do this for you. This way, you're not just evaluating the model separately, you're building the evaluation right into the fine-tuning workflow. Therefore, every time you fine-tune your model, it will be automatically evaluated as the last step. This eliminates the need for separate manual steps to evaluate your model, making the overall development process more efficient and seamless. While a single perfect solution for LLM evaluation doesn't exist, adopting the best practices and staying informed about ongoing research allows you to continuously improve your evaluation methods and ensures their successful application in real world projects.

### Video - [Solving Evaluation Challenges](https://www.cloudskillsboost.google/course_templates/1080/video/520177)

* [YouTube: Solving Evaluation Challenges](https://www.youtube.com/watch?v=_swu3w_kgVk)

Now that you've explored the complexities of evaluating generative AI models, let's see how Vertex AI offers practical tools to implement best practices. Vertex AI provides a unified interface for AI development to kickstart projects across various tasks and modalities. For example, you can prototype, develop, and deploy both predictive and generative AI models in your application with Vertex AI. To directly address the model evaluation challenges mentioned earlier, consider leveraging the evaluation solutions designed for generative AI models within Vertex AI. Generative AI evaluation can be applied to a range of use case scenarios. When selecting pre-trained models, choose the best model for your task by comparing performance on relevant benchmarks. Experiment with configuration settings and optimize them, exploring how adjustments like temperature can refine output quality. Harness the power of prompt engineering with templates to improve user interaction and results. Finally, safeguard fine tuning by proactively addressing bias and ensuring your model avoids generating undesirable outputs. Vertex AI currently offers two evaluation methods, the traditional computation-based method that compares your new outputs to a ground truth and the model-based method, using a specifically tailored LLM as a judge to perform the tasks of evaluation. These methods are also called evaluation pipeline services, as they provide end to end solutions for evaluating generative AI models. Using Vertex AI pipelines, they orchestrate the entire evaluation process, including generating model responses, calling evaluation services, and calculating metrics. You can even customize pipelines by calling these steps individually. Due to the inherent start up latency of serverless Vertex AI pipelines. Evaluation pipeline services are most advantageous in specific scenarios. These include large scale evaluations with numerous model instances where the efficiency gains offset the initial latency. Additionally, pipeline services excel in asynchronous workflows, where immediate results are not critical, allowing evaluations to run in the background. Lastly, their seamless integration into broader MLOps workflows makes them a powerful tool for automating model evaluation and streamlining the entire model life cycle management process. In evaluating generative AI models, two primary paradigms emerge, pointwise and pairwise evaluation. Pointwise evaluation delves into the absolute performance of a single model, revealing how it behaves in real world scenarios and highlighting its inherent strengths and weaknesses. This approach is invaluable for pinpointing areas where model tuning could lead to improved performance, as well as for establishing a baseline against which future iterations can be measured. Pairwise evaluation, on the other hand, involves a direct comparison of two models. This allows for the identification of superior performance on specific tasks or datasets, aiding in crucial decision making processes such as model selection. Additionally, Pairwise evaluation can guide the choice of optimal prompts and assess the impact of tuning efforts on the baseline model. With these evaluation methods in mind, let's return to the overview of the solutions offered by Vertex AI. Computational-based metrics offer a standardized metric-driven approach to model evaluation, commonly used in academic and industry benchmarking for their speed and efficiency. However, they rely on a ground truth dataset, labeled input/output pairs used to measure consistency between LLM outputs and the gold standard. While these metrics are fast and affordable, they may not fully capture the nuances of generative tasks. Trying to cram all the good things about a summary into a formula is a challenge, and even carefully crafted datasets may not reflect every preferred summary style. Also, different metric categories provide different insights. Lexicon-based metrics measure string similarity between generated results and ground truth, examples being exact match and ROUGE. Count-based metrics such as F1 score, accuracy, and tool name match, quantify matches and mismatches with expected labels. Embedding-based metrics calculate similarities by comparing LLM generated results in ground truth within an embedding space or a numerical representation. Vertex AI simplifies the integration of these metrics into workflows, typically for pointwise evaluations of single models. But indirect comparisons between two models are also possible through analysis of their individual metric scores. Model-based evaluation, a technique pioneered by Google Research, mimics human evaluation with greater speed and efficiency. This method utilizes specialized arbiter models, carefully calibrated against human ratings to act as judges for model comparison. These arbiter models offer both numerical scores and explanations, mirroring the comprehensive assessment of human experts. Google's Auto Side by Side is a prime example of a model-based evaluation solution. It provides on demand assessment of language models, achieving results comparable to those of human raters. Key advantages include data-driven objectivity, eliminating the need for potentially biased human preference data, scalability and cost efficiency, automating the process for rapid affordable evaluations at scale, and enhanced transparency, capturing explanations and confidence scores for valuable insights into model decision making. Currently, evaluating LLMs can occur across four broad tasks consisting of summarization, question answering, tool use, and general text generation. Each task allows for evaluating LLMs to use a fixed set of metrics like quality, relevance, and helpfulness. You have the flexibility to evaluate any combination of these metrics for a given evaluation. But remember, to specify the required input parameters for each metric. To select the optimal evaluation approach for your generative AI model and Vertex AI, first, determine whether you need a pairwise comparison between two models or a pointwise assessment of a single model. Next, clarify the specific role and purpose of your model, identifying the tasks it's designed to perform. Then, pinpoint the most critical aspects of the model's responses, whether it's accuracy, creativity, safety, fluency, or other factors. If your model focuses on question answering, consider Vertex AI's specialized question answering metrics. Similarly, if safety or fluency are concerns, prioritize those specific metrics. By asking yourself what your model does and which aspects of its output are most important, you'll confidently choose the right evaluation task and metrics in Vertex AI to thoroughly assess your generative AI model's performance. Check the Google Cloud documentation page for the current evaluation metrics and how to use each one on Vertex AI. Different evaluation methods offer unique insights on model performance. Understanding how evaluation metrics are calculated and their meanings is crucial for interpreting your results effectively. In Vertex AI, the presentation of your model evaluation results will depend on whether you've opted for pointwise or pairwise evaluation. Pointwise is a numerical score, and pairwise picks the preferred of two models. Model-based evaluation in vertex AI offers more than just numerical scores. It provides explanations in string format using chain of thought reasoning to illuminate the arbiter model's decision making process, enhancing evaluation accuracy. Additionally, it provides confidence scores, numeric values between 0 and 1, reflecting the arbiters confidence in its judgment. These scores are derived using self consistency decoding, where multiple samples are taken on a single input. A higher consensus among these samples results in a higher confidence score, indicating greater certainty in the arbiter's assessment. These two features provide transparency into the autoraters decision making process, giving you a deeper understanding of the evaluation results and helping you make more informed decisions about your generative AI models.

### Video - [Streamlining Model Evaluation with Computation-based Metrics](https://www.cloudskillsboost.google/course_templates/1080/video/520178)

* [YouTube: Streamlining Model Evaluation with Computation-based Metrics](https://www.youtube.com/watch?v=DSF1AjKWpH8)

With computation-based metrics, you can quickly and effectively measure how well your models are doing by looking at different measurements that are specific to the task you're working on. This method evaluates model results based on the input prompt and output response pairs only. The methodology is in line with academic research and open benchmarks, employing several widely used metrics for various general AI tasks. Vertex AI offers computation-based metrics for both the base and tuned versions of its PaLM text-based LLM. These metrics are tailored to specific tasks with at least one metric available for each core comprehension capability. For classification tasks, for example, you'll find metrics like Micro-F1, Macro-F1, and Per class F1. For summarization, ROUGE-L is provided, which measures how well a summary captures the article's essence by identifying the longest shared word sequence. Consult the supported task section in the Vertex AI documentation to find the most suitable metrics for your specific needs. Additionally, you can run evaluations through the REST API, Vertex AI SDK, or the Google Cloud Console, with detailed instructions available in the official documentation. Using computation-based metrics involves a few simple steps. First, prepare your model evaluation dataset with prompt and ground truth pairs. Next, upload the dataset to a Google Cloud Storage bucket. Finally, submit the model evaluation job using the Vertex AI Python library. When examining the dataset, you have to provide the prompt with the instructions and the context. You'll then need to provide the ground truth, which will be used together with the generated answers to calculate metrics related to the selected task. Include at least 10 examples that closely resemble real-world application scenarios. Once uploaded to Google Cloud Storage, leverage the provided model evaluation pipeline template. Vertex AI model evaluation utilizes Vertex AI Pipelines for scalable model evaluation of GenAI models. Specify parameters like evaluation dataset location, task, output location, and the desired model. Then initiate the model evaluation pipeline job using the Vertex AI Pipeline SDK. Evaluating multiple models may seem straightforward. For example, just run evaluations for each and compare the aggregated metrics. However, these metrics can be difficult to understand. While they can indicate overall model superiority, they don't necessarily reveal which models consistently produce summaries that best align with human preferences. Determining the model that generates the most human-like summaries often requires a more nuanced approach beyond simple metric comparison. While manually examining each pair of summaries to assign a preference score is possible, it is time consuming. To efficiently evaluate LLM alignment at scale, a more automated approach is often necessary.

### Video - [Comparing performance with Model based evaluation](https://www.cloudskillsboost.google/course_templates/1080/video/520179)

* [YouTube: Comparing performance with Model based evaluation](https://www.youtube.com/watch?v=YR1eQVZXgBQ)

Automatic side-by-side or AutoSxS is an evaluation tool that facilitates on-demand A/B testing of LLMs for specific tasks. For instance, in a summarization scenario, the autorater, the judging model, assesses the original article and two generated summaries from different models, providing a win rate that highlights the superior model's performance. However, to understand why the autorater prefers one model over another, auto side-by-side provides explanations at the input level, aligning its preference with human judgment. This transparency allows users to better grasp the strengths and weaknesses of each model, ultimately clarifying the autorater's choices. At the core of auto side-by-side is the autorater, an LLM trained to evaluate the quality of responses from other models based on an initial inference prompt. This unique capability allows auto side-by-side to assess the performance of LLMs on Google Cloud. Vertex AI auto side-by-side is a versatile tool capable of evaluating any model in Vertex AI model registry that supports batch prediction, as well as pre-existing models with generated predictions. Specifically designed for assessing LLMs, auto side-by-side employs predefined evaluation criteria for both summarization and question-answering tasks. For example, the summarization criteria in auto side-by-side encompasses various aspects of model performance, including adherence to instructions and the groundedness of generated summaries. Make sure to check the latest token limit and the list of evaluation criteria for each task in Google Cloud's documentation page. Let's say you want to use auto side-by-side to evaluate an LLM like Gemini Pro on Vertex AI for summarization against another model. Assuming you already have your LLM-generated predictions, the evaluation process involves a few simple steps. First, prepare your evaluation dataset with the necessary prompts, contexts, and generated responses. Next, convert this dataset into JSON format and store it in a Cloud Storage bucket or BigQuery table. Finally, run the model evaluation pipeline job in Vertex AI. Let's explore each of these steps in more detail, starting with the evaluation dataset. Auto side-by-side uses an adaptable schema to accept a single evaluation dataset, typically in JSON format for summarization tasks. Each row represents a unique example containing identification fields, data fields for prompts and context, and pre-generated responses from different models. While the fields may not be present in this case, we have the pre-generated predictions containing the responses generated by the LLM task. Human preferences may also be included for comparison against validated data. For optimal results, include examples reflecting real-world model usage. While a single evaluation example is the minimum requirement, using 400-600 examples is recommended for optimal aggregate metrics. Since you have an understanding of how the evaluation dataset might appear and assuming you've uploaded it to a cloud bucket, we can now proceed to define the model evaluation pipeline parameters. AutoSxS released on Vertex AI facilitates running a model evaluation job through its pipeline. Here are the main supported pipeline parameters. Evaluation dataset parameter. This indicates the location of the evaluation dataset. It's either a BigQuery table or a comma-separated list of Cloud Storage paths to a jsonl dataset containing evaluation examples. ID fields. These fields distinguish unique evaluation examples. Here, we typically include ID and document. Task. This parameter specifies the type of task to evaluate, such as summarization or question-answering. In this instance, we're focusing on summarization. Additionally, AutoSxS enables configuring autorater behavior. You can specify inference instructions to guide task completion and set inference contexts to reference during task execution. Lastly, users must provide the names of columns containing predefined predictions to calculate the evaluation metric. Once these parameters are defined, you're prepared to execute the evaluation job. Google Cloud provides a pipeline template to streamline the evaluation process on Vertex AI. Initiate a pipeline job with the defined parameters and then submit a pipeline run using the Vertex AI Python SDK. Upon completion, examine the evaluation results within the Vertex AI pipeline, which include judgment tables, aggregated and align matrices, if human preferences were provided. The judgment table offers metrics for LLM performance per example, indicating the superior response with a confidence score and explanation. This analysis enables you to confidently identify the better response, backed by clear reasoning and a quantifiable confidence level. Here is an example of AutoSxS judgments output. You can see that the summary from Model B was favored over the summary of Model A due to its higher coherence and better coverage. Auto side-by-side also provides aggregated metrics, specifically win rates, derived from the judgment table. These win rates show the percentage of times the autorater favored one model response over another, providing a quick and easy way to identify the superior model for the evaluated task. Fortunately, auto side-by-side does support human preference. To include human preference in your auto side-by-side evaluation, add a column to your evaluation dataset indicating the preferred LLM-generated response. For example, human preference. Before running the pipeline, update the auto side-by-side parameters to specify this column. The remaining evaluation process, including pipeline execution using the Vertex Python SDK, stays unchanged. The result will show additional measurements that incorporate the human preference data. This inclusion of human preference data in your auto side-by-side evaluation will provide additional metrics alongside familiar ones like accuracy. You'll receive both human preference scores and autorater preference scores, indicating how closely these evaluations align. Utilizing auto side-by-side on Vertex AI for evaluating LLMs, comparing them within specific tasks, and ensuring alignment with human preference is just one of the many applications of auto side-by-side. It can also be used to assess a fine-tuned model against its foundational counterpart. Ultimately, auto side-by-side offers a flexible and powerful tool for comprehensive LLM evaluation, empowering you to make informed decisions and optimize your applications.

### Quiz - [MLOps: Model Evaluation for Generative AI Quiz](https://www.cloudskillsboost.google/course_templates/1080/quizzes/520180)

#### Quiz 1.

> [!important]
> **An ML engineer is tasked with selecting the best-performing image classification model from three candidates, all trained on the same dataset. Their primary goal is to understand how each model performs in real-world scenarios and identify areas for potential improvement. Which evaluation approach would be most effective for this initial assessment?**
>
> * [ ] Pairwise evaluation, comparing the performance of each model against the others on specific tasks.
> * [ ] Pointwise evaluation, focusing on the absolute performance of each model and identifying its strengths and weaknesses.
> * [ ] Multi-task evaluation, assessing each model's performance on a variety of image-related tasks beyond classification.
> * [ ] Binary evaluation, determining whether each model meets a pre-defined accuracy threshold.

#### Quiz 2.

> [!important]
> **When using evaluation methods like BLEU or ROUGE for LLM assessment, which of the following challenges is most likely to arise if the reference dataset is limited or biased?**
>
> * [ ] The model may require more computational resources for training to compensate for the limited reference data.
> * [ ] The evaluation may underestimate the model's true capabilities because the reference data doesn't cover the full range of acceptable responses.
> * [ ] The evaluation may overestimate the model's performance, as it's being compared against an artificially narrow set of outputs.
> * [ ] The model may become vulnerable to adversarial attacks due to the insufficient diversity of the reference data.

#### Quiz 3.

> [!important]
> **You're tasked with evaluating multiple versions of a language model for summarizing news articles. You want to know which model produces the most informative and coherent summaries. Which evaluation type would be most appropriate?**
>
> * [ ] Numerical evaluation: calculate metrics like ROUGE or BLEU to measure the similarity between generated summaries and reference summaries.
> * [ ] Multi-task evaluation: evaluate each model on additional tasks like question answering or text generation alongside summarization.
> * [ ] Binary evaluation: assign a simple pass or fail judgment to each summary based on basic criteria.
> * [ ] Ranking evaluation: have human evaluators rank the summaries from different models based on their overall quality.

#### Quiz 4.

> [!important]
> **During the evaluation of an LLM, you find that the model often produces responses that sound fluent and grammatical but are factually incorrect. Which of the following evaluation challenges does this example illustrate?**
>
> * [ ] Limited reference data
> * [ ] Lack of data
> * [ ] Model complexity and decision-making
> * [ ] Data contamination

#### Quiz 5.

> [!important]
> **You're evaluating a language model designed to generate creative stories. Which of the following evaluation approaches would be most relevant? Select all that apply.**
>
> * [ ] Assessing the diversity and originality of the generated stories.
> * [ ] Comparing the model's output to a standard grammar and syntax checker.
> * [ ] Using BLEU score to measure similarity to reference stories.
> * [ ] Calculating the perplexity of the model's output.

#### Quiz 6.

> [!important]
> **Which component of an LLM block is responsible for storing and retrieving past interactions with the model to provide context for future requests?**
>
> * [ ] Data sources
> * [ ] Guardrails
> * [ ] Prompt templates
> * [ ] Memory

#### Quiz 7.

> [!important]
> **A company is using a generative AI model to write marketing copy. Which evaluation approach would help them ensure that the generated content is both creative and relevant to their target audience?**
>
> * [ ] Combining automated metrics for diversity and relevance with human evaluation for creativity and brand alignment.
> * [ ] Using only automated metrics like BLEU and ROUGE.
> * [ ] Focusing only on grammar and syntax checks to ensure accuracy.
> * [ ] Relying solely on human evaluation for a qualitative assessment.

### Document - [Reading List](https://www.cloudskillsboost.google/course_templates/1080/documents/520181)

## Course Summary

This module provides a summary of the entire course by covering the most important concepts, tools, technologies, and products.

### Video - [Course Summary](https://www.cloudskillsboost.google/course_templates/1080/video/520182)

* [YouTube: Course Summary](https://www.youtube.com/watch?v=a44cNhXv-gs)

This brings us to the end of the Machine Learning Operations with Vertex AI model evaluation course. In addition to being introduced to the model evaluation concept, you also saw how Vertex AI integrates seamlessly within the MLOps framework, the common challenges that arise during the process. And practical solutions that effectively address these challenges and streamline the model evaluation workflow. In the second part of the course, you explored the unique challenges of evaluating generative AI models. And how vertex AI model evaluation services simplify the evaluation process. As you start or continue your work in this field, there are a few things you should keep in mind. It's a puzzle, not a test. Evaluating GenAI is about solving problems, not just getting scores. Tools aren't a one-size-fits-all. It's important to pick the right evaluation service based on your use, case and goals. MLOps is ongoing. Evaluation shouldn't be a one-time task. Make sure it's part of your continuous improvement cycle. For more training and hands-on practice with ML and AI, explore the options available at cloud.google.com/training/machinelearning- -ai. If you're interested in validating your expertise and showcasing your ability to transform businesses with Google Cloud technology, consider working towards a Google Cloud certification. You can learn more about Google Cloud certification offerings at cloud.google.com/certifications. Thanks for completing this course. We'll see you next time.

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
