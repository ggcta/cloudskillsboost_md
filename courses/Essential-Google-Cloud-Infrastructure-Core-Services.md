---
id: 49
name: 'Essential Google Cloud Infrastructure: Core Services'
datePublished: 2024-04-25
topics:
- IAM
type: Course
url: https://www.cloudskillsboost.google/course_templates/49
---

# [Essential Google Cloud Infrastructure: Core Services](https://www.cloudskillsboost.google/course_templates/49)

**Description:**

This accelerated on-demand course introduces participants to the comprehensive and flexible infrastructure and platform services provided by Google Cloud with a focus on Compute Engine. Through a combination of video lectures, demos, and hands-on labs, participants explore and deploy solution elements, including infrastructure components such as networks, systems and applications services. This course also covers deploying practical solutions including customer-supplied encryption keys, security and access management, quotas and billing, and resource monitoring.

**Objectives:**

- Administer Identity and Access Management for resources.
- Implement data storage services in Google Cloud
- Manage and examine billing of Google Cloud resources.
- Monitor resources using Google Cloud's operations suite

## Introduction

In this module, we introduce the Architecting with Google Compute Engine course series. This course series is defined for cloud solution architects, DevOps engineers, and anyone who's interested in using Google Cloud, to create new solutions or to integrate existing systems, application environments, and infrastructure with a focus on Compute Engine.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/49/video/470010)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=oB8Iro1j8nQ)

Hello. I'm Philipp Maier. I'm Mylene Biddle, we're both Course Developers, at Google Cloud and we want to welcome you to Architecting with Compute Engine, a series of three courses. Before we start using all of the different services that Google Cloud Platform, or GCP offers, let's talk about what GCP is. When you look at Google Cloud, you'll see that it's actually part of a much larger ecosystem. This ecosystem consists of open-source software, providers, partners, developers, third-party software, and other Cloud providers. Google is actually a very strong supporter of open-source software. That's right. Now, Google Cloud consists of Chrome, Google devices, Google Maps, Gmail, Google Analytics, G Suite, Google Search, and the Google Cloud Platform. GCP itself is a computing solution platform that really encompasses three core features: infrastructure, platform, and software. This map represents GCP's global infrastructure. As of this recording, GCP's well-provisioned global network connects over 60 zones to over 130 points of presence through a global network of fiber optic cables. And Google is continuously investing in this network, with new regions, points of presence, and subsea cable investments. On top of this infrastructure, GCP uses state of the art software-defined, networking and distributed systems of technologies to host and deliver your services around the world. These technologies are represented by a suite of Cloud-based products and services that is continuously expanding. Now, it's important to understand that there is usually more than one solution for a task or application in GCP. To better understand this, let's look at a solution continuum. Google Cloud Platform spans from infrastructure as a service, or IaaS, to software as a service, or SaaS. You really can build applications on GCP for the web or mobile that are global, auto-scaling, and assistive, and that provide services where the infrastructure is completely invisible to the user. It is not just that Google has opened the infrastructure that powers applications like Search, Gmail, Google Maps, and G Suite. Google has opened all of the services that make these products possible and packaged them for your use. Alternative solutions are possible. For example, you could start up your own VM in Google Compute Engine, install open-source MySQL on it and run it just like a MySQL database on your own computer in a data center. Or you could use the Cloud SQL service, which provides a MySQL instance and handles operational work like backups and security patching for you using the same services Google does to automate backups and patches. You could even move to a NoSQL database that is auto-scaling and serverless so that growth no longer requires adding server instances or possibly changing the design to handle the new capacity. This series of courses focuses on the infrastructure. An IT infrastructure is like a city infrastructure. The infrastructure is the basic underlying framework of fundamental facilities and systems, such as transport, communications, power, water, fuel, and other essential services. The people in the city are like users, and the cars and bikes, and buildings in the city are like applications. Everything that goes into creating and supporting those applications for the users is the infrastructure. The purpose of this course is to explore as efficiently and clearly as possible the infrastructure services provided by GCP. You should become familiar enough with the infrastructure services that you will know what services do and how to use them. We won't go into very deep dive case studies on specific vertical applications. But you'll know enough to put all the building blocks together to build your own solution. Now, GCP offers a range of compute services. The service that might be most familiar to newcomers is Compute Engine, which lets you run virtual machines on-demand in the Cloud. It's Google Cloud's infrastructure as a service solution. It provides maximum flexibility for people who prefer to managed server instances themselves. Google Kubernetes Engine lets you run containerized applications on a cloud environment that Google manages for you under your administrative control. Think of containerization as a way to package code that's designed to be highly portable and to use resources very efficiently. And think of Kubernetes as a way to orchestrate code in containers. App Engine is GCP's fully managed platform as a service framework. That means it's a way to run code in the cloud without having to worry about infrastructure. You just focus on your code and let Google deal with all the provisioning and resource management. You can learn a lot more about App Engine in the "Developing Applications with Google Cloud Platform" course series. Cloud Functions is a completely serverless execution environment or functions as a service. It executes your code in response to events, whether those events occur once a day or many times per second. Google scales resources as required, but you only pay for the service while your code runs. The "Developing Applications with Google Cloud" course series also discusses Cloud Functions. Cloud Run, a managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It is built on Knative, an open API and runtime environment built on Kubernetes that gives you freedom to move your workloads across different environments and platforms. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges you only for the resources you use calculated down to the nearest 100 milliseconds, so you‘ll never pay for your over-provisioned resources. In this series of courses, In this series of courses, Compute Engine will be our main focus. The Architecting with Google Compute Engine courses are part of the Cloud Infrastructure learning path. This path is designed for IT professionals who are responsible for implementing, deploying, migrating, and maintaining applications in the cloud. The prerequisite for these courses is the Google Cloud Platform Fundamentals: Core Infrastructure course, which you can find in the link section for this video. The Architecting with Google Compute Engine series consists of three courses. Essential Cloud Infrastructure: Foundation is the first course of the Architecting with Compute Engine series. In that course, we start by introducing you to GCP and how to interact with the GCP Console and Cloud Shell. Next, we'll get into virtual networks and you will create VPC networks and other networking objects. Then we'll take a deep dive into virtual machines, and you will create virtual machines using Compute Engine. Essential Cloud Infrastructure: Core Services is the second course of this series. In that course, we start by talking about Cloud IAM and you will administer Identity and Access Management for resources. Next, we'll cover the different data storage services in GCP, and you will implement some of those services. Then we'll go over resource management, where you will manage and examine billing of GCP resources. Lastly, we'll talk about resource monitoring and you will monitor GCP resources using Stackdriver services. Elastic Cloud Infrastructure: Scaling, and Automation, is the last course of the series. In that course, we start by going over the different options to interconnect networks to enable you to connect your infrastructure to GCP. Next, we'll go over GCP is load balancing and auto-scaling services. Would you will get to explore directly. Then we'll cover infrastructure automation services like Terraform so that you can automate the development of GCP infrastructure services. Lastly, we'll talk about other managed services that you might want to leverage in GCP. Now, our goal for you is to remember and understand the different GCP services and features, and also be able to apply your knowledge, analyze requirements, evaluate different options, and create your own services. That's why these courses include interactive hands-on maps through the Qwiklabs platform. Qwiklabs provisions you with a Google account and credentials, so you can access the GCP console for each lab at no cost.

### Document - [Welcome to Essential Cloud Infrastructure: Core Services](https://www.cloudskillsboost.google/course_templates/49/documents/470011)

## Identity and Access Management (IAM)

Administer Identity and Access Management for resources

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/49/video/470012)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=l0VhzWvppCw)

In this module we cover Cloud Identity and Access Management or Cloud IAM. Cloud IAM is a sophisticated system built on top of email-like address names, job type roles in granular permissions. If you're familiar with IAM from other implementations look for the differences that Google has implemented to make IAM easier to administer and more secure. I will start by introducing Cloud IAM from a high-level perspective. We will then dive into each of the components within Cloud IAM which are organizations, roles, members and service accounts. I will also introduce some best practices to help you apply these concepts in your day-to-day work. Finally, you will gain firsthand experience with Cloud IAM through a Lab. Let's get started with an overview of Cloud Identity and Access Management.

### Video - [Identity and Access Management](https://www.cloudskillsboost.google/course_templates/49/video/470013)

- [YouTube: Identity and Access Management](https://www.youtube.com/watch?v=qiePcrJrTg0)

Person: So what is Identity Access Management? It is a way of identifying who can do what on which resource. The who can be a person, group or application. The what refers to specific privileges or actions, and the resource could be any Google Cloud service. For example, I could give you the privilege or role of compute viewer. This proves you with read-only access to get and list compute engine resources without being able to read the data stored on them. Cloud IAM is composed of different objects as shown on the slide. We are going to cover each of these in this module. To get a better understanding of where these fit in, let's look at Cloud IAM policies and the Cloud IAM resource hierarchy. Google Cloud resources are organized hierarchically as shown in this tree structure. The organization node is the root node in this hierarchy. Folders are the children of the organization. Projects are the children of folders, and individual resources are the children of projects. Each resource has exactly one parent. The organization resource represents your company. Cloud IAM roles granted by this level are inherited by all resources under the organization. The folder resource could represent your department. Cloud IAM roles granted at this level are inherited by all resources that the folder contains. Projects represent a trust boundary within your company. Services within the same project have the same default level of trust.

### Video - [Organization](https://www.cloudskillsboost.google/course_templates/49/video/470014)

- [YouTube: Organization](https://www.youtube.com/watch?v=1ZHNvjAzAZU)

Let's learn more about the organization node. As I mentioned earlier, the organization resource is the root node in the GCP resource hierarchy. This node has many roles, like the organization admin. The organization admin provides a user like Bob, with access to administer all resources belonging to his organization, which is useful for auditing. There is also a project creator role, which allows a user like Alice, to create projects within her organization. I am showing the project creator role here because it can also be applied at the organization level, which would then be inherited by all the projects within the organization. The organization resource is closely associated with a G Suite or Cloud Identity Account. When a user with a G Suite or Cloud Identity Account creates a GCP project an organization resource is automatically provisioned for them. Then Google Cloud communicates its availability to the G Suite or Cloud Identity super admins. These super admin accounts, should be used very carefully because they have a lot of control over your organization and all the resources underneath it. The G Suite or Cloud Identity super administrators and the GCP organization admin are key roles during the setup process and for lifecycle control, for the organization resource. The two roles are generally assigned to different users or groups, although this depends on the organization structure and needs. In the context of GCP organization setup, the G Suite or Cloud Identity super administrator responsibilities are: assign the organization admin role to some users, be a point of contact in case of recovery issues, control the lifecycle of the G Suite or Cloud Identity account and organization resource. The responsibilities of the organization admin role are: define IAM policies, determine the structure of the resource hierarchy, delegate responsibility over critical components such as networking, billing, and resource hierarchy, through IAM roles. Following the principle of least privilege, this role does not include the permission to perform other actions, such as creating folders. To get these permissions, an organization admin must assign additional roles to their account. Let's talk more about folders, because they can be viewed as sub organizations within the organization. Folders provide an additional grouping mechanism and isolation boundary between projects. Folders can be used to model different legal entities, departments, and teams within a company. For example, a first-level of folders can be used to represent the main departments in your organization, like departments x and y. Because folders, can contain projects in other folders, each folder could then include other subfolders to represent different teams, like teams A and B. Each team folder could contain additional subfolders, to represent different applications, like products 1 and 2. Folders allow delegation of administration rights, for example, each head of a department, can be granted full ownership of all GCP resources that belong to their department. Similarly, access to resources can be limited by folder, so users in one department can only access and create GCP resources, within that folder. Let's look at some other resource manager roles, while remembering that policies are inherited from top to bottom. The organization node also has a viewer role. They grants view access to all resources within an organization. The folder node has multiple roles that mimic the organizational roles, but are applied to resources within a folder. There is an admin role that provides full control over folders. A creator role, to browse the hierarchy and create folders, and a viewer role, to view folders and projects below a resource. Similarly for projects, there is a creator role that allows a user to create new projects, making that user automatically the owner. There is also a project deleter role that grants deletion privileges for projects.

### Video - [Roles](https://www.cloudskillsboost.google/course_templates/49/video/470015)

- [YouTube: Roles](https://www.youtube.com/watch?v=UzmmDv1C9OQ)

Let's talk more about roles which define the can do what on which resource part of Cloud IAM. There are three types of roles in Cloud IAM, basic roles, predefined roles, and custom roles. Basic roles are the original roles that were available in the Cloud console, but they are broad. You apply them to a Google Crowd project, and they affect all resources in that project. In other words, IAM basic roles offer fixed, coarse-grained levels of access. The basic roles are the owner, editor and viewer roles. The owner has full administrative access. This includes the ability to add and remove members and delete projects. The editor role has modify and delete access. This allows the developer to deploy applications and modify or configure its resources. The view role has read only access. Now all of these roles are concentric. That is the owner role includes the permissions of the editor role. And the editor role includes the permissions of the viewer role. There is also a billing administrator role to manage billing and add or remove administrators without the right to change the resources in the project. Each project can have multiple owners, editors, viewers and billing administrators. GCP services, offers their own set of predefined roles, and they define where the roles can be applied. This provides members with granular access to specific GCP resources and prevents unwanted access to other resources. These roles are a collection of permissions, because to do any meaningful operations, you usually need more than one permission. For example, as shown here, a group of users is granted the instance admin role on project a. This provides the users of that group with all the Compute Engine permissions listed on the right and even more. Grouping these permissions into a role makes them easier to manage. The permissions themselves are classes and methods in the API's. For example, compute instance start can be broken down into the service, resource and verb. That mean that the permission is used to start a stopped Compute Engine instance. These permissions usually line with the actions corresponding REST API. Compute Engine has several predefined IAM roles. Let's look at three of those. The Compute Admin role provides full control of all Compute Engine resources. This includes all permissions that start with compute, which means that every action for any type of Compute Engine resource is permitted. The Network Admin role Contains permissions to create, modify and delete network resources, except for firewall rules and SSL certificates. In other words, the network admin role allows read only access to firewall rules SSL certificates, and instances to view their ephemeral IP addresses. The storage admin role contains permissions to create, modify, and delete disks, images, and snapshots. For example, if your company has someone who manages project images, and you don't want them to have the editor role in the project. Grant their account the storage admin role on that project. For the full list of predefined roles for Compute Engine, see the links section in the slides. Now, roles are meant to represent abstract functions and are customized to line with real jobs. But what if one of those roles does not have enough permissions? Or you need something even finer grained? That's what custom roles permit. A lot of companies use the least privileged model in which each person in your organization is giving the minimal amount of privilege needed to do their job. Let's say you want to define an instance operator role to allow some users to start and stop Compute Engine virtual machines, but not reconfigure them. Custom roles allow you to do that.

### Video - [Demo: Custom roles](https://www.cloudskillsboost.google/course_templates/49/video/470016)

- [YouTube: Demo: Custom roles](https://www.youtube.com/watch?v=TuB5UotB3RY)

Let me show you how to create a custom role in GCP. My goal is to create an instance operator role that allows some users to start and stop Compute Engine virtual machines but not reconfigure them. So here I am in the GCP console and I'm going to click the "Navigation" menu to go to "I am an Admin" and specifically actually want to go to "Roles". Here I can see all the different roles that are available. Now, I could select one of these roles and create a role from that selection and then either remove or assign more permissions. You can see over here the permissions that are assigned to a role, or I can just create a role from scratch. So let me go to that. I'm going to click "Create role" and I'm going to give it a name. I'm going to call this the instance operator. There's also an ID to that and that must be unique and cannot be changed. There is a launch stage selection, Alpha, Beta, general availability, and disabled. This is essentially just a launch stage. So you want to make sure that you start small, tested Alpha, and then roll it out at some point so that other users know that they can leverage that availability. So what I'm going to do now is click "Add permissions" because currently there are no assigned permissions given that I started from scratch. So let's go in here. Now we have over 2,000 different permissions. So we obviously want to filter for that just a little bit and specifically I'm interested in the permissions for compute instances. So let me type compute.instances. and hit "Enter". Now I'm down to 44. So I want to select a couple different ones from here. I'm interested in "Get", I want to be able to get the different instances. I want to be able to list all of the instances as well as reset them and resume. Resume is if an instance was suspended which is equivalent to if it's sleep or in standby mode. I also want to start and stop, and suspend. So I can go click "Add" now and I can see the permissions that I just assigned. So I can get, list, reset, resume, start, stop, and suspend. From here, I can now click "Create". It's created that and I can click on it here. I can review that. I have an ID and I have a launch stage, and these are my permissions. That's how easy it is to create a custom role in GCP. Alternatively, I could have started with the instance admin role as a base and remove the permissions that I don't want the role to have. Now remember, that custom roles are not maintained by Google. That means that when new permissions, features or services are added to GCP, your custom roles will not be updated automatically.

### Video - [Members](https://www.cloudskillsboost.google/course_templates/49/video/470017)

- [YouTube: Members](https://www.youtube.com/watch?v=pAO_VuWtte4)

Let’s talk more about members, which define the “who” part of “who can do what on which resource.” There are five different types of members: Google Accounts, Service Accounts, Google Groups, Google Workspace domains, and Cloud Identity domains. A Google account represents a developer, an administrator, or any other person who interacts with Google Cloud. Any email address that is associated with a Google account can be an identity, including gmail.com or other domains. New users can sign up for a Google account by going to the Google account signup page, without receiving mail through Gmail. A service account is an account that belongs to your application instead of to an individual end user. When you run code that is hosted on Google Cloud, you specify the account that the code should run as. You can create as many service accounts as needed to represent the different logical components of your application. A Google group is a named collection of Google accounts and service accounts. Every group has a unique email address that is associated with the group. Google groups are a convenient way to apply an access policy to a collection of users. You can grant and change access controls for a whole group at once instead of granting or changing access controls one-at-a-time for individual users or service accounts. A Workspace domain represents a virtual group of all the Google accounts that have been created in an organization's Workspace account. Workspace domains represent your organization's internet domain name, such as example.com, and when you add a user to your Workspace domain, a new Google account is created for the user inside this virtual group, such as username@example.com. Google Cloud customers who are not Workspace customers can get these same capabilities through Cloud Identity. Cloud Identity lets you manage users and groups using the Google Admin Console, but you do not pay for or receive Workspace’s collaboration products such as Gmail, Docs, Drive, and Calendar. Now it’s important to note that you cannot use IAM to create or manage your users or groups. Instead, you can use Cloud Identity or Workspace to create and manage users. A policy consists of a list of bindings. A binding binds a list of members to a role, where the members can be user accounts, Google groups, Google domains, and service accounts. A role is a named list of permissions defined by IAM. Let’s revisit the IAM resource hierarchy. A policy is a collection of access statements attached to a resource. Each policy contains a set of roles and role members, with resources inheriting policies from their parent. Think of it like this: resource policies are a union of parent and resource, where a less restrictive parent policy will always override a more restrictive resource policy. The IAM policy hierarchy always follows the same path as the Google Cloud resource hierarchy, which means that if you change the resource hierarchy, the policy hierarchy also changes. For example, moving a project into a different organization will update the project's IAM policy to inherit from the new organization's IAM policy. Also, child policies cannot restrict access granted at the parent level. For example, if we grant you the Editor role for Department X, and we grant you the Viewer role at the bookshelf project level, you still have the Editor role for that project. Therefore, it is a best practice is to follow the principle of least privilege. The principle applies to identities, roles, and resources. Always select the smallest scope that’s necessary for the task in order to reduce your exposure to risk. You can also use a recommender for role recommendations to identify and remove excess permissions from your principals, improving your resources’ security configurations. Each role recommendation suggests that you remove or replace a role that gives your principals excess permissions. At scale, these recommendations help you enforce the principle of least privilege by ensuring that principals have only the permissions that they actually need. Recommender identifies excess permissions using policy insights. Policy insights are ML-based findings about permission usage in your project, folder, or organization. You can grant access to Google Cloud resources by using allow policies, also known as IAM policies, which are attached to resources. The allow policy controls access to the resource itself and any descendants of that resource that inherit the allow policy. An allow policy associates, or binds, one or more principals (also known as a member or identity) with a single IAM role and any context-specific conditions that change how and when the role is granted. In the example on this slide, Jie (jie@example.com) is granted the Organization Admin predefined role (roles/resourcemanager.organizationAdmin) in the first role binding. This role contains permissions for organizations, folders, and limited projects operations. In the second role binding, both Jie and Raha (raha@example.com) are granted the ability to create projects via the Project Creator role (roles/resourcemanager.projectCreator). Together, these role bindings grant fine-grained access to both Jie and Raha, and Jie is granted more access than Raha. IAM deny policies let you set guardrails on access to Google Cloud resources. With deny policies, you can define deny rules that prevent certain principals from using certain permissions, regardless of the roles they're granted. Deny policies are made up of deny rules. Each deny rule specifies a set of principals that are denied permissions, and the permissions that the principals are denied, or unable to use. Optionally, you can define the condition that must be true for the permission to be denied. When a principal is denied a permission, they can't do anything that requires that permission, regardless of the IAM roles they've been granted. This is because IAM always checks relevant deny policies before checking relevant allow policies. IAM Conditions allow you to define and enforce conditional, attribute-based access control for Google Cloud resources. With IAM Conditions, you can choose to grant resource access to identities (members) only if configured conditions are met. For example, this could be done to configure temporary access for users in the event of a production issue or to limit access to resources only for employees making requests from your corporate office. Conditions are specified in the role bindings of a resource's IAM policy. When a condition exists, the access request is only granted if the condition expression evaluates to true. Each condition expression is defined as a set of logic statements allowing you to specify one or more attributes to check. An organization policy is a configuration of restrictions, defined by configuring a constraint with the desired restrictions for that organization. An organization policy can be applied to the organization node, and all of its folders or projects within that node. Descendants of the targeted resource hierarchy inherit the organization policy that has been applied to their parents. Exceptions to these policies can be made, but only by a user who has the organization policy admin role. What if you already have a different corporate directory? How can you get your users and groups into Google Cloud? Using Google Cloud Directory Sync, your administrators can log in and manage Google Cloud resources using the same usernames and passwords they already use. This tool synchronizes users and groups from your existing Active Directory or LDAP system with the users and groups in your Cloud Identity domain. The synchronization is one-way only; which means that no information in your Active Directory or LDAP map is modified. Google Cloud Directory Sync is designed to run scheduled synchronizations without supervision, after its synchronization rules are set up. Google Cloud also provides single sign-on authentication. If you have your identity system, you can continue using your own system and processes with SSO configured. When user authentication is required, Google will redirect to your system. If the user is authenticated in your system, access to Google Cloud is given; otherwise, the user is prompted to sign in. This allows you to also revoke access to Google Cloud. If your existing authentication system supports SAML2, SSO configuration is as simple as 3 links and a certificate, as shown on this slide. Otherwise, you can use a third-party solution, like ADFS, Ping, or Okta. Also, if you want to use a Google account but are not interested in receiving mail through Gmail, you can still create an account without Gmail.

### Video - [Service Accounts](https://www.cloudskillsboost.google/course_templates/49/video/470018)

- [YouTube: Service Accounts](https://www.youtube.com/watch?v=OkJHtHj_SE0)

As mentioned earlier, another type of member is a service account. A service account is an account that belongs to your application instead of to an individual end user. This provides an identity for carrying out service-to-service interactions in a project without supplying user credentials. For example, if you write an application that interacts with Google Cloud Storage, it must first authenticate to either the Google Cloud Storage XML API or JSON API. You can enable service accounts and grant read-write access to the account on the instance where you plan to run your application. Then, program the application to obtain credentials from the service account. Your application authenticates seamlessly to the API without embedding any secret keys or credentials in your instance, image, or application code. Service accounts are identified by an email address, like the example shown here. There are three types of service accounts: user-created or custom, built-in, and Google APIs service accounts. By default, all projects come with the built-in Compute Engine default service account. Apart from the default service account, all projects come with a Google Cloud APIs service account, identifiable by the email: project-number@cloudservices.gserviceaccount.com. This is a service account designed specifically to run internal Google processes on your behalf, and it is automatically granted the Editor role on the project. Alternatively, you can also start an instance with a custom service account. Custom service accounts provide more flexibility than the default service account, but they require more management from you. You can create as many custom service accounts as you need, assign any arbitrary access scopes or IAM roles to them, and assign the service accounts to any virtual machine instance. Let’s talk more about the default Compute Engine service account. As I mentioned, this account is automatically created per project. This account is identifiable by the email project-number-compute@developer.gserviceaccount.com, and it is automatically granted the Editor role on the project. When you start a new instance using gcloud, the default service account is enabled on that instance. You can override this behavior by specifying another service account or by disabling service accounts for the instance. Now, authorization is the process of determining what permissions an authenticated identity has on a set of specified resources. Scopes are used to determine whether an authenticated identity is authorized. In the example shown here, Applications A and B contain Authenticated Identities (or service accounts). Let’s assume that both applications want to use a Cloud Storage bucket. They each request access from the Google Authorization server, and in return they receive an access token. Application A receives an access token with read-only scope, so it can only read from the Cloud Storage bucket. Application B, in contrast, receives an access token with read-write scope, so it can read and modify data in the Cloud Storage bucket. Scopes can be customized when you create an instance using the default service account, as shown in this screenshot. These scopes can be changed after an instance is created by stopping it. Access scopes are actually the legacy method of specifying permissions for your VM. Before the existence of IAM roles, access scopes were the only mechanism for granting permissions to service accounts. For user-created service accounts, use IAM roles instead to specify permissions. Now, roles for service accounts can also be assigned to groups or users. Let’s look at the example shown on this slide. First, you create a service account that has the InstanceAdmin role, which has permissions to create, modify, and delete virtual machine instances and disks. Then you treat this service account as the resource, and decide who can use it by providing users or a group with the Service Account User role. This allows those users to act as that service account to create, modify, and delete virtual machine instances and disks. Users who are Service Account Users for a service account can access all the resources that the service account has access to. Therefore, be cautious when granting the Service Account User role to a user or group. Here is another example. The VMs running component_1 are granted Editor access to project_b using Service Account 1. VMs running component_2 are granted objectViewer access to bucket_1 using an isolated Service Account 2. This way you can scope permissions for VMs without re-creating VMs. Essentially, IAM lets you slice a project into different microservices, each with access to different resources, by creating service accounts to represent each one. You assign the service accounts to the VMs when they are created, and you don’t have to ensure that credentials are being managed correctly because Google Cloud manages security for you. Now, you might ask, how are service accounts authenticated? There are two types of service account keys. By default, when using service accounts within Google Cloud (for example, from Compute Engine or App Engine) Google automatically manages the keys for service accounts. However, if you want to be able to use service accounts outside of Google Cloud, or want a different rotation period, it is possible to also manually create and manage your own service account keys. All service accounts have Google-managed key-pairs. With Google-managed service account keys, Google stores both the public and private portion of the key, and rotates them regularly. Each public key can be used for signing for a maximum of two weeks. Your private key is always held securely in escrow and is never directly accessible. You may optionally create one or more user-managed key pairs (also known as "external" keys) that can be used from outside of Google Cloud. Google only stores the public portion of a user-managed key. The User is responsible for security of the private key and performing other management operations such as key rotation, whether manually or programmatically. Users can create up to 10 service account keys per service account to facilitate key rotation. User-managed keys can be managed by using the IAM API, the gcloud command-line tool, or the Service Accounts page in the Google Cloud console. Google does not save your user-managed private keys, so if you lose them, Google cannot help you recover them. You are responsible for keeping these keys safe and also responsible for performing key rotation. User-managed keys should be used as a last resort. Consider the other alternatives, such as short-lived service account credentials (tokens), or service account impersonation. The gcloud command line shown on this slide is a fast and easy way to list all of the keys associated with a particular service account.

### Video - [Organization Restrictions](https://www.cloudskillsboost.google/course_templates/49/video/470019)

- [YouTube: Organization Restrictions](https://www.youtube.com/watch?v=dKH-11hy_eI)

Let’s talk now about Organization Restrictions. The Organization Restrictions feature lets you prevent data exfiltration through phishing or insider attacks. For managed devices in an organization, the Organization Restrictions feature restricts access only to resources in authorized Google Cloud organizations. There is a need in organizations to restrict access of their employees only to resources in authorized Google Cloud organizations. Google Cloud administrators who administer Google Cloud, and egress proxy administrators, who configure the egress proxy, engage together to set up organization restrictions. The managed device is governed by the organizational policies of a company. Employees of an organization use a managed device to access the organization resources. An egress proxy administrator configures the proxy to add organization restrictions headers to any requests originating from a managed device. This proxy configuration prevents users from accessing any Google Cloud resources in non-authorized Google Cloud organization. The Organization Restrictions feature in Google Cloud inspects all requests for organization restrictions header, and allows or denies the requests based on the organization being accessed. Organization Restrictions can be used to restrict access to employees in your organization so that employees can access resources only in your Google Cloud organization and not other organizations. They can also be used to allow your employees to read from Cloud Storage resources but restrict employee access only to resources in your Google Cloud organization. Or, allow your employees to access a vendor Google Cloud organization in addition to your Google Cloud organization.

### Video - [IAM best practices](https://www.cloudskillsboost.google/course_templates/49/video/470020)

- [YouTube: IAM best practices](https://www.youtube.com/watch?v=gpMMYJ4KgLI)

Let's talk about some Cloud IAM best practices to help you apply the concepts you just learned in your day-to-day work. First, leverage and understand the resource hierarchy. Specifically, use projects to group resources that share the same trust boundary. Check the policy granted on each resource and make sure you recognize the inheritance. Because of inheritance, use the principle of least privilege when granting roles. Finally, audit policies using Cloud audit logs and audit memberships of groups using policies. Next, I recommend granting roles to groups instead of individuals. This allows you to update group membership instead of changing a Cloud IAM policy. If you do this, make sure to audit membership of groups used in policies and control the ownership of the Google group used in Cloud IAM policies. You can also use multiple groups to get better control. In the example on this slide, there is a network admin group. Some of those members also need a read write role to a Cloud Storage bucket, but others need the read only role. Adding and removing individuals from all three groups controls their total access. Therefore, groups are not only associated with job roles but can exist for the purpose of role assignment. Here are some best practices for using service accounts. As mentioned before, be very careful when granting the service accounts user role because it provides access to all the resources of the service account has access to. Also when you create a service account give it a display name that clearly identifies its purpose, ideally using an established naming convention. As for keys, establish key rotation policies and methods and audit keys with the serviceAccount.keys.list method. Finally, I recommend using Cloud Identity Aware Proxy or Cloud IAP. Cloud IAP lets you establish a central authorization layer for applications accessed by HTTPS. So you can use an application level access control model instead of relying on network level firewalls. Applications and resources protected by Cloud IAP can only be accessed through the proxy by users and groups with the correct Cloud IAM role. When you grant a user access to an application or resource by Cloud IAP, they're subject to the fine-grained access controls implemented by the product in use without requiring a VPN. Cloud IAP performs authentication and authorization checks when a user tries to access a Cloud IAP secure resource as shown on the right.

### Video - [Lab Intro: Exploring IAM](https://www.cloudskillsboost.google/course_templates/49/video/470021)

- [YouTube: Lab Intro: Exploring IAM](https://www.youtube.com/watch?v=3_UTSXFX-CM)

It's time to apply what you learned. In this lab, you'll grant and revoke roles to change access. Specifically, you will use Cloud IAM to implement access control, restrict access to specific features and resources, and use the service account user role. Now, anytime you make changes to IAM roles, the GCP Console refreshes faster than the actual system. Therefore, you should expect some short delays when making changes to a member's role.

### Lab - [Exploring IAM](https://www.cloudskillsboost.google/course_templates/49/labs/470022)

In this lab you will rehearse several activities associated with Identity and Access Management.

- [ ] [Exploring IAM](../labs/Exploring-IAM.md)

### Video - [Lab Review: Exploring IAM](https://www.cloudskillsboost.google/course_templates/49/video/470023)

- [YouTube: Lab Review: Exploring IAM](https://www.youtube.com/watch?v=DjXuPF_dkS4)

In this lab, you granted and revoked Cloud IAM roles, first to a user username 2, and then to a service account user. Having access to both users allow you to see the results of the changes you made. You can stay for a lab walk-through, but remember that GCP's user interface can change. So your environment might look slightly different. Welcome to the walk-through of the Cloud IAM lab. In this lab, we have set up two users for you, and at this point I have logged into the console as username 1. So Qwiklabs will provide you with two usernames to log into and we'll do some operations with both, but right now I have already logged in as username 1. So the first instruction tells you to log into the console in another tab as username 2. So console, I'm going to grab that username, certainly going here, I'm going to say add account, username 2, and luckily we've been given the same password for both usernames and login here, and with Qwiklabs you have new accounts. So it's always going to ask you for all of this new user acceptance, and I'll accept this terms, I'm good to go. So task two is to explore the IAM console. So I'm going to go to the username 1 tab. I'm going to go to IAM, and I'm going to click on there. If I hit Add, I can look around at the different roles I can provide. Let me go ahead and click Cancel, feel free to explore as much as you want, you can see here there are roles based on different products and services that we have. I'll hit Cancel. Let me go to username 2, and I'm going to do the same thing. Let me go to IAM. So I'm going to browse this list now and I am going to look for the names associated with username 1, which in my case ends in 82462, I see it here, and here is username 2. You can see there are different roles associated with each one of them. Username 1 has App Engine admin, BigQuery admin, editor, owner, and viewer. Whereas, username 2, which is the one that I'm logged in in this tab only has Viewer Access. So now I'm going to move on to task three. So I'm going to go back to username 1, there's going to go to be a lot of switching back and forth in this lab. So make sure you keep track of which tab is username 1 and which is username 2. I am going to go to Google Cloud Storage here, and I'm going to create a bucket in here. So buckets need to be globally unique. So I am going to use my Cloud Project ID because it is pretty unique, and I'm going to click Create and keep all the other defaults, and make sure to note the name of your bucket because we'll use it as your bucket name across the lab. So here I'm going to go to upload files, and let me find just any sample file here just a screenshot, and I've uploaded it there. Once it's uploaded, I'm going to rename it here, and I'm going to call it sample.txt. The reason I'm doing this is because it's going to be much easier to run any of the commands I'm going to do with sample.txt as a name as opposed to that long name I already had. So at this point in the lab, you can hit the Check my progress button inside the lab and it'll give you a green check and five points. If you've correctly created a bucket and uploaded a sample file. So now I'm going to switch to username 2 and I'm going to go to storage browser. I'm going to verify that username 2 has view access to that bucket, and here it is. Because it's inherited that, I can view the sample file. Task four is I'm going to remove project viewer role for username 2. So in order to do that I have to go back to username 1. I'm going to go back to IAM, and then I'm going to find username 2 which is this one right here, 73. I'm going to hit Edit, and then I am going to hit the garbage can so that I can remove it. I hit Save, and then I at this point I can also check my progress and I'll get five points and a green check mark. I should have 10 points out of 20 in the lab. If I have properly done that. Throughout these labs if you ever get to a point where you realize that you didn't get the points necessary, it's probably because you missed a step or two, granted sometimes the lab is actually broken because they're based on technology that changes a lot. But if a lab isn't broken, chances are you just missed a step. So I usually recommend go back three steps. Check your work, make sure you did everything. Usually, that's what happened. So now we're going to verify that the username 2 has lost access. So I'm going to go back to the username 2 bucket tab, and then I'm going to click Home, and then I'm going to go back to storage to verify. I could've just refresh the screen as well, Refresh. List of buckets could not be loaded. So as you can see I do not have access anymore. So now the next task, task five, is to add Storage Access. So I'm going to copy the value of username 2 from the Qwiklabs lab name, from their connection details on the left of your lab instructions. So I'll copy that, I'm going to go back to username 1 tab, and I'm already in I am. So I'm going to hit Add, and then for new members I'm going to paste the value here. That is it, and I am going to select Storage, scroll down. Luckily it's alphabetical, and I am giving it Storage Object Viewer. Then I'm going to hit Save. This is another checkpoint in the lab where you can go back and hit Check my progress, and you should get another five points that you have checked, that you have actually provided the right permissions. Now we had one in the modules that sometimes the permissions upload faster than will be displayed in the GCP Console. Sometimes you just have to be patient. Maybe click Check my progress, wait a couple seconds if you didn't get it, and then you'll get the five points and the green check. So the next piece of task 5 is to verify that username 2 now has storage access. So if I go back here, and I'm going to start Cloud Shell. Because username 2 doesn't have project viewer roles, so it won't be able to see anything in the console, but we can see things in Cloud Storage. So we're going to use Cloud Shell for that. So let's make sure I know my bucket name. I copied it earlier, but I have definitely forgotten it by now. So let me go back here to Storage easily, and I can easily copy paste the bucket name here. Copy that, and back in Cloud Shell, I am going to do a gsutil ls to list for that bucket, gs://my bucket name, and username 2 should be able to see that there's sample.txt in the bucket and there it is. So now I can close username 2 tab because the rest of the lab is done in the username 1 console. So task 6 is to set up the service account user. So in IAM, I'm going to go to Service accounts. I'm going to create a service account, and the service account name is going to be read-bucket-objects, and I'm going to hit Create. It's going to ask me which role to provide, and I am going to be giving it Storage, Storage Object Viewer. Hit Continue, and then I'm going to hit Done. So now we've created our service account, so we're going to go back to the main IAM page, and we are going to select the service account we just created, and we're going to hit Add. In order to add members, normally you could perform this activity for a specified user group or a domain. But for training purposes and for this video, we're just going to grant the service account user role to everyone at a company called autostrat.com, which is a fake company used for demonstrating and training. So the new member is going to be autostrat.com, and I am going to give it Service Accounts, Service Account User, and then I'm going to hit Save. So now I'm going to go back to IAM, and I am going to add, and I am going to provide compute engine access. So the new member is autostrat.com. Make sure you're typing it correctly. I am giving it Compute Engine, and Compute Instance Admin V1 and save. So essentially, that step is a rehearsal of activity that you would probably perform for a specific user. It gives the user limited abilities with a VM Instance. It would be able to connect via SSH to a VM and perform possibly some administration tasks. So now, I am going to create a VM with the service account I created. Create, I am going to use the same name provided in a lab, demoIAM. I'm using us-central1. The zone is us-central1-c, and the machine type is an F1-micro. It is just for demonstration purposes. So let's not waste resources, and the service account is the read bucket objects account, and I'm going to hit Create. So this is another checkpoint in the lab, and you should be able to hit Check my progress, and verify that you have gotten the last five points in the lab. Again, this is another one that might take a couple seconds to propagate. So just give it a second, and make sure that you get the green check in the final five points. Task 7, you explore the service account user role, and now you've already completed all of the tasks in the labs. So this is just for learning purposes. So I'm going to go in here, and I'm going to SSH into this account into the VM that I just created. Then I am going to run a gcloud compute instances list. I am expecting to see an error because I do not have the correct permissions to list those for my project. Just wait for that to show up, and there you can see error. Some request did not succeed because I don't have permission to do that. So now I'm going to try to copy the file from the bucket that I created earlier. So my bucket name is the Project ID which I've forgotten already. Here gets/sample.txt, and you can see it successfully copied it. Now I'm going to copy it into another file, and then I'm going to try to upload into my bucket. Bucket name is here, and you'll see I can download, but I cannot add. In review in this lab, you granted and revoked Cloud IAM roles, first for user, and then to service account user. I hope you enjoyed the walkthrough. Thank you.

### Quiz - [Quiz: Identity and Access Management](https://www.cloudskillsboost.google/course_templates/49/quizzes/470024)

#### Quiz 1.

> [!important]
> **What abstraction is primarily used to administer user access in IAM ?**
>
> - [ ] Credentials, an abstraction of an authorization token.
> - [ ] Privileges, an abstraction of access rights.
> - [ ] Roles, an abstraction of job roles.
> - [ ] Leases, an abstraction of periodic entitlements.

#### Quiz 2.

> [!important]
> **Which of the following is not a type of IAM member?**
>
> - [ ] Google Group
> - [ ] Google Account
> - [ ] Google Workspace domain
> - [ ] Cloud Identity domain
> - [ ] Service Account
> - [ ] Organization Account

#### Quiz 3.

> [!important]
> **Which of the following is not a type of IAM role?**
>
> - [ ] Advanced
> - [ ] Basic
> - [ ] Custom
> - [ ] Predefined

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/49/video/470025)

- [YouTube: Module Review](https://www.youtube.com/watch?v=sHYMhs3jSXU)

Person: In this module, we covered Identity and Access Management along with its components and best practices. IAM builds on top of other Google Cloud and entity services. The creation and administration of corporate identities occurs through the workspace admin or cloud identity interface and is commonly handled by a person separate from the Google Cloud administrator. Google Groups are a great way for these two business functions to collaborate. You establish the roles and assign them through the group, and then the workspace admin administers membership in the group. Finally, remember that service accounts are very flexible, and that they can enable you to build an infrastructure-based level of control in your application.

## Storage and Database Services

Implement data storage services in Google Cloud

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/49/video/470026)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=xcgB1-T7Z-Q)

In this module, we cover storage and database services in Google Cloud. Every application needs to store data, whether it's business data, media to be streamed, or sensor data from devices. From an application-centered perspective, the technology stores and retrieves the data. Whether it's a database or an object store is less important than whether that service supports the application’s requirements for efficiently storing and retrieving the data, given its characteristics. Google offers several data storage services to choose from. In this module, we will cover Cloud Storage, Filestore, Cloud SQL, Cloud Spanner, AlloyDB, Firestore, Cloud Bigtable, and Memorystore. Let me start by giving you a high-level overview of these different services. This table shows the storage and database services and highlights the storage service type, what each service is good for, and intended use. BigQuery is also listed on the right. I’m mentioning this service because it sits on the edge between data storage and data processing. You can store data in BigQuery, but the intended use for BigQuery is big data analysis and interactive querying. For this reason, BigQuery is covered later in the course. If tables aren’t your preference, here’s a decision tree to help you identify the solution that best fits your application. Let’s walk through this together. First, ask yourself: Is your data structured? If its not, then ask yourself if you need a shared file system. If you do, then choose Filestore. If you don't, then choose Cloud Storage. If your data is structured, does your workload focus on analytics? If it does, you will want to choose Bigtable or BigQuery, depending on your latency and update needs. BigQuery is recommended as a data warehouse, is the default storage for tabular data, and is optimized for large-scale, ad-hoc SQL-based analysis and reporting. While BigQuery data manipulation language (DML) enables you to update, insert, and delete data from your BigQuery tables, because it has a built-in cache BigQuery works really well in cases where the data does not often change. Bigtable is a NoSQL wide-column database. It's optimized for low latency, large numbers of reads and writes, and maintaining performance at scale. In addition to analytics, Bigtable is also suited as a ‘fast lookup’ non-relational database for datasets too large to store in memory, with use cases in areas such as IoT, AdTech and FinTec. If your workload doesn’t involve analytics, check whether your data is relational. If it’s not relational, do you need application caching? If caching is a requirement, choose Memorystore, an in-memory database. Otherwise choose Firestore, a document database. If your data is relational and you need Hybrid transaction/analytical processing, also known as HTAP, choose AlloyDB. If you don’t need HTAP and don’t need global scalability, choose Cloud SQL. If you don’t need HTAP and need global scalability, choose Spanner. Depending on your application, you might use one or several of these services to get the job done. For more information on how to choose between these different services, please refer to the links provided in the course resources for this module. Before we dive into each of the data storage services, let’s define the scope of this module. The purpose of this module is to explain which services are available and when to consider using them from an infrastructure perspective. I want you to be able to set up and connect to a service without detailed knowledge of how to use a database system. If you want a deeper dive into the design, organizations, structures, schemas and details on how data can be optimized, served and stored properly within those different services, I recommend Google Cloud’s Data Engineering courses. Let’s look at the agenda. This module covers all of the services we’ve mentioned so far. To become more comfortable with these services, you will apply them in two labs. We’ll also provide a quick overview of Memorystore, which is Google Cloud’s fully managed Redis service. Let’s get started by diving into Cloud Storage and Filestore!

### Video - [Cloud Storage](https://www.cloudskillsboost.google/course_templates/49/video/470027)

- [YouTube: Cloud Storage](https://www.youtube.com/watch?v=6_XvGqSqOOI)

Cloud Storage is Google Cloud’s object storage service, and it allows world-wide storage and retrieval of any amount of data at any time. You can use Cloud Storage for a range of scenarios including serving website content, storing data for archival and disaster recovery, or distributing large data objects to users via direct download. Cloud Storage has a couple of key features: It’s scalable to exabytes of data; The time to first byte is in milliseconds; It has very high availability across all storage classes; And It has a single API across those storage classes. Some like to think of Cloud Storage as files in a file system but it’s not really a file system. Instead, Cloud Storage is a collection of buckets that you place objects into. You can create directories, so to speak, but really a directory is just another object that points to different objects in the bucket. You’re not going to easily be able to index all of these files like you would in a file system. You just have a specific URL to access objects. Cloud Storage has four storage classes: Standard, Nearline, Coldline and Archive, and each of those storage classes provide 3 location types: There’s a multi-region is a large geographic area, such as the United States, that contains two or more geographic places. Dual-region is a specific pair of regions, such as Finland and the Netherlands. A region is a specific geographic place, such as London. Objects stored in a multi-region or dual-region are geo-redundant. Now, let’s go over each of the storage classes: Standard Storage is best for data that is frequently accessed (think of "hot" data) and/or stored for only brief periods of time. This is the most expensive storage class but it has no minimum storage duration and no retrieval cost. When used in a region, Standard Storage is appropriate for storing data in the same location as Google Kubernetes Engine clusters or Compute Engine instances that use the data. Co-locating your resources maximizes the performance for data-intensive computations and can reduce network charges. When used in a dual-region, you still get optimized performance when accessing Google Cloud products that are located in one of the associated regions, but you also get improved availability that comes from storing data in geographically separate locations. When used in multi-region, Standard Storage is appropriate for storing data that is accessed around the world, such as serving website content, streaming videos, executing interactive workloads, or serving data supporting mobile and gaming applications. Nearline Storage is a low-cost, highly durable storage service for storing infrequently accessed data like data backup, long-tail multimedia content, and data archiving. Nearline Storage is a better choice than Standard Storage in scenarios where slightly lower availability, a 30-day minimum storage duration, and costs for data access are acceptable trade-offs for lowered at-rest storage costs. Coldline Storage is a very-low-cost, highly durable storage service for storing infrequently accessed data. Coldline Storage is a better choice than Standard Storage or Nearline Storage in scenarios where slightly lower availability, a 90-day minimum storage duration, and higher costs for data access are acceptable trade-offs for lowered at-rest storage costs. Archive Storage is the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery. Unlike the so-to-speak "coldest" storage services offered by other Cloud providers, your data is available within milliseconds, not hours or days. Archive Storage also has higher costs for data access and operations, as well as a 365-day minimum storage duration. Archive Storage is the best choice for data that you plan to access less than once a year. Let’s focus on durability and availability. All of these storage classes have 11 nines of durability, but what does that mean? Does that mean you have access to your files at all times? No, what that means is you won't lose data. You may not be able to access the data, which is like going to your bank and saying well my money is in there, it's 11 nines durable. But when the bank is closed we don't have access to it, which is the availability that differs between storage classes and the location type. Cloud Storage is broken down into a couple of different items here. First of all, there are buckets which are required to have a globally unique name and cannot be nested. The data that you put into those buckets are objects that inherit the storage class of the bucket and those objects could be text files, doc files, video files, etc. There is no minimum size to those objects and you can scale this as much as you want as long as your quota allows it. To access the data, you can use the gcloud storage command, or either the JSON or XML APIs. When you upload an object to a bucket, the object is assigned the bucket's storage class, unless you specify a storage class for the object. You can change the default storage class of a bucket but you can't change the location type from regional to multi-region/dual-region or vice versa. You can also change the storage class of an object that already exists in your bucket without moving the object to a different bucket or changing the URL to the object. Setting a per-object storage class is useful, for example, if you have objects in your bucket that you want to keep, but that you don't expect to access frequently. In this case, you can minimize costs by changing the storage class of those specific objects to Nearline, Coldline or Archive Storage. In order to help manage the classes of objects in your bucket, Cloud Storage offers Object Lifecycle Management. More on that later. Let’s look at access control for your objects and buckets that are part of a project. We can use IAM for the project to control which individual user or service account can see the bucket, list the objects in the bucket, view the names of the objects in the bucket, or create new buckets. For most purposes, IAM is sufficient, and roles are inherited from project to bucket to object. Access control lists or ACLs offer finer control. For even more detailed control, signed URLs provide a cryptographic key that gives time-limited access to a bucket or object. Finally, a signed policy document further refines the control by determining what kind of file can be uploaded by someone with a signed URL. Let’s take a closer look at ACLs and signed URLs. An ACL is a mechanism you use to define who has access to your buckets and objects, as well as what the level of access is they have. The maximum number of ACL entries you can create for a bucket or object is 100. Each ACL consists of one or more entries, and these entries consist of two pieces of information: A scope, which defines who can perform the specified actions (for example, a specific user or group of users). And a permission, which defines what actions can be performed (for example, read or write). The allUsers identifier listed on this slide represents anyone who is on the internet, with or without a Google account. The allAuthenticatedUsers identifier, in contrast, represents anyone who is authenticated with a Google account. For more information on ACLs, refer to the links of this video. For some applications, it is easier and more efficient to grant limited-time access tokens that can be used by any user, instead of using account-based authentication for controlling resource access. (For example, when you don’t want to require users to have a Google account). Signed URLs allow you to do this for Cloud Storage. You create a URL that grants read or write access to a specific Cloud Storage resource and specifies when the access expires. That URL is signed using a private key associated with a service account. When the request is received, Cloud Storage can verify that the access-granting URL was issued on behalf of a trusted security principal, in this case the service account, and delegates its trust of that account to the holder of the URL. After you give out the signed URL, it is out of your control. So you want the signed URL to expire after some reasonable amount of time.

### Video - [Cloud Storage Features](https://www.cloudskillsboost.google/course_templates/49/video/470028)

- [YouTube: Cloud Storage Features](https://www.youtube.com/watch?v=UnICmttQThw)

There are also several features that come with Cloud Storage. We will cover these at a high-level for now because we will soon dive deeper into some of them. Earlier in the course series, we already talked a little about Customer-supplied encryption keys when attaching persistent disks to virtual machines. This allows you to supply your own encryption keys instead of the Google-managed keys, which is also available for Cloud Storage. Cloud Storage also provides Object Lifecycle Management which lets you automatically delete or archive objects. Another feature is object versioning which allows you to maintain multiple versions of objects in your bucket. You are charged for the versions as if they were multiple files, which is something to keep in mind. Cloud Storage also offers directory synchronization so that you can sync a VM directory with a bucket. Object change notifications can be configured for Cloud Storage using Pub/Sub. When enabled, Autoclass manages all aspects of storage classes for a bucket. We will discuss this later. In Cloud Storage, objects are immutable, which means that an uploaded object cannot change throughout its storage lifetime. To support the retrieval of objects that are deleted or overwritten, Cloud Storage offers the Object Versioning feature. Object Versioning can be enabled for a bucket. Once enabled, Cloud Storage creates an archived version of an object each time the live version of the object is overwritten or deleted. The archived version retains the name of the object but is uniquely identified by a generation number as illustrated on this slide by g1. When Object Versioning is enabled, you can list archived versions of an object, restore the live version of an object to an older state, or permanently delete an archived version, as needed. You can turn versioning on or off for a bucket at any time. Turning versioning off leaves existing object versions in place and causes the bucket to stop accumulating new archived object versions. Google recommends that you use Soft Delete instead of Object Versioning to protect against permanent data loss from accidental or malicious deletions. A link to the Object Versioning documentation can be found in the Course Resources for this module. Soft Delete provides default bucket-level protection for your data from accidental or malicious deletion by preserving all recently deleted objects for a specified period of time. The objects stored in Cloud Storage buckets are immutable. If you overwrite or change the data of an object, Cloud Storage deletes its earlier version and replaces it with a new one. Soft Delete retains all these deleted objects, whether from a delete command or because of an overwrite, essentially capturing all changes made to bucket data for the configured retention duration. When you create a Cloud Storage bucket, the Soft Delete feature is enabled by default with a retention duration of seven days. During the retention duration, you can restore deleted objects, but after the duration ends, Cloud Storage permanently deletes the objects. By updating the bucket's configuration, you can increase the retention duration to 90 days or disable it by setting the retention duration to 0. A link to the Soft Delete documentation can be found in the Course Resources for this module. To support common use cases like setting a Time to Live for objects, archiving older versions of objects, or "downgrading" storage classes of objects to help manage costs, Cloud Storage offers Object Lifecycle Management. You can assign a lifecycle management configuration to a bucket. The configuration is a set of rules that apply to all the objects in the bucket. So when an object meets the criteria of one of the rules, Cloud Storage automatically performs a specified action on the object. Here are some example use cases: First, downgrade the storage class of objects older than a year to Coldline Storage. Second, delete objects created before a specific date. For example, January 1, 2017. Or third, keep only the 3 most recent versions of each object in a bucket with versioning enabled. Object inspection occurs in asynchronous batches, so rules may not be applied immediately. Also, updates to your lifecycle configuration may take up to 24 hours to go into effect. This means that when you change your lifecycle configuration, Object Lifecycle Management may still perform actions based on the old configuration for up to 24 hours. So keep that in mind. A link to the Object Lifecycle Management documentation can be found in the Course Resources for this module. The Object Retention Lock feature lets you set retention configuration on objects within Cloud Storage buckets that have enabled the feature. A retention configuration governs how long the object must be retained and has the option to permanently prevent the retention time from being reduced or removed. This helps you meet data retention regulatory and compliance requirements, such as those associated with FINRA, SEC, and CFTC. This also helps provide Google Cloud immutable storage solutions with leading enterprise backup software vendor partners. A link to the Object Retention Lock documentation can be found in the Course Resources for this module. The Cloud Console allows you to upload individual files to your bucket. But what if you have to upload terabytes or even petabytes of data? There are three services that address this: Transfer Appliance, Storage Transfer Service, and Offline Media Import. Transfer Appliance is a hardware appliance you can use to securely migrate large volumes of data (from hundreds of terabytes up to 1 petabyte) to Google Cloud without disrupting business operations. The images on this slide are transfer appliances. The Storage Transfer Service enables high-performance imports of online data. That data source can be another Cloud Storage bucket, an Amazon S3 bucket, or an HTTP/HTTPS location. Finally, Offline Media Import is a third party service where physical media (such as storage arrays, hard disk drives, tapes, and USB flash drives) is sent to a provider who uploads the data. For more information on these three services, refer to the Course Resources. When you upload an object to Cloud Storage and you receive a success response, the object is immediately available for download and metadata operations from any location where Google offers service. This is true whether you create a new object or overwrite an existing object. Because uploads are strongly consistent, you will never receive a 404 Not Found response or stale data for a read-after-write or read-after-metadata-update operation. Strong global consistency also extends to deletion operations on objects. If a deletion request succeeds, an immediate attempt to download the object or its metadata will result in a 404 Not Found status code. You get the 404 error because the object no longer exists after the delete operation succeeds. Bucket listing is strongly consistent. For example, if you create a bucket, then immediately perform a list buckets operation, the new bucket appears in the returned list of buckets. Finally, object listing is also strongly consistent. For example, if you upload an object to a bucket and then immediately perform a list objects operation, the new object appears in the returned list of objects.

### Video - [Choosing a storage class](https://www.cloudskillsboost.google/course_templates/49/video/470029)

- [YouTube: Choosing a storage class](https://www.youtube.com/watch?v=i3zCA5g_ybw)

Let’s explore the decision tree to help you find the appropriate storage class in Cloud Storage. If you will read your data less than once a year, you should consider using Archive storage. If you will read your data less than once per 90 days, you should consider using Coldline storage. If you will read your data less than once per 30 days, you should consider using Nearline storage. And if you will be doing reads and writes more often than that, you should consider using Standard storage. You also want to take into account the location type: Use a region to help optimize latency and network bandwidth for data consumers, such as analytics pipelines, that are grouped in the same region. Use a dual-region when you want similar performance advantages as regions, but also want the higher availability that comes with being geo-redundant. Use a multi-region when you want to serve content to data consumers that are outside of the Google network and distributed across large geographic areas, or when you want the higher data availability that comes with being geo-redundant. If your data has a variety of access frequencies, or the access patterns for your data are unknown or unpredictable, you should consider Autoclass. The Autoclass feature automatically transitions objects in your bucket to appropriate storage classes based on the access pattern of each object. Even if a different storage class is specified in the request, all objects added to the bucket begin in Standard storage. The feature moves data that is not accessed to colder storage classes to reduce storage cost. Data that is accessed is also moved to Standard storage to optimize future accesses. When object data is read, the object transitions to Standard storage if it's not already stored in Standard storage. Autoclass simplifies and automates cost saving for your Cloud Storage data. When enabled on a bucket, there are no early deletion charges, no retrieval charges, and no charges for storage class transitions. For more information, view the storage classes documentation. So far we have only considered unstructured data. Before we look at unstructured data, let's explore a high-performance, fully managed file storage offering; Filestore.

### Video - [Filestore](https://www.cloudskillsboost.google/course_templates/49/video/470030)

- [YouTube: Filestore](https://www.youtube.com/watch?v=v63FT-Jl3GI)

Filestore is a managed file storage service for applications that require a file system interface and a shared file system for data. Filestore gives users a simple native experience for standing up managed network attached storage with either Compute Engine or Google Kubernetes Engine instances. The ability to fine tune Filestore's performance and capacity independently, leads to predictably fast performance for your file-based workloads. Filestore offers native compatibility with existing enterprise applications and supports any NFSV3 compatible clients. Applications gain the benefit of features such as scale-out performance, hundreds of terabytes of capacity, and file locking without the need to install or maintain any specialized plug-ins or client-side software. Filestore has many use cases. Using Filestore, you can expedite migration of enterprise applications. Many on-premises applications require a file system interface to data. As these applications continue to migrate to the Cloud, Filestore can support a broad range of enterprise applications that need a shared file system. For media rendering, you can easily meant filestore file shares on Compute Engine instances, enabling visual effects artists to collaborate on the same file share. As rendering workflows typically run across fleets of Compute Machines, all of which meant a shared file system. Filestore and Compute Engine can scale to meet your jobs rendering needs. Electronic Design Automation, or EDA, is all about data management. It requires the ability to batch workloads across thousands of cores and has a large memory needs. Filestore offers the necessary capacity and scale to meet the needs of manufacturing customers doing intensive EDA, and also make sure that files are universally accessible. Data analytics workloads include Compute complex financial models or analysis of environmental data. These workloads are latency sensitive. Filestore offers low latency for file operations, and as capacity or performance needs change, you can easily grow or shrink your instances as needed. As a persistent and shareable storage layer, Filestore enables immediate access to data for high-performance, smart analytics without the need to lose valuable time on loading an offloading data to clients drives. Genome sequencing requires an incredible amount of raw data in the order of billions of data points per person. This type of analysis requires speed, scalability, and security. Filestore meets the needs of companies and research institutions performing scientific research while also offering predictable prices for the performance. Web developers and large hosting providers also rely on Filestore to manage and serve web content, including needs such as WordPress hosting.

### Video - [Lab Intro: Cloud Storage](https://www.cloudskillsboost.google/course_templates/49/video/470031)

- [YouTube: Lab Intro: Cloud Storage](https://www.youtube.com/watch?v=iaPyYBDZVAs)

Let's take some of the cloud storage concepts that we just discussed and apply them in a lab. In this lab, you'll create buckets and perform many of the advanced options available in cloud storage. You'll set access control list to limit who can have access to your data and what they're allowed to do with it. You'll use the ability to supply and manage your own encryption keys for additional security. You'll enable object versioning to track changes in the data and you'll configure lifecycle management, so that objects are automatically archived or deleted after a specified period. Finally, you'll use the directory synchronization feature that I mentioned and share your buckets across projects using Cloud IAM.

### Lab - [Cloud Storage](https://www.cloudskillsboost.google/course_templates/49/labs/470032)

In this lab you create and use Cloud Storage buckets and exercise many of the advanced features including restricting access with Access Lists, implementing version control, loading and managing your own encryption keys, directory synch and more.

- [ ] [Cloud Storage](../labs/Cloud-Storage.md)

### Video - [Lab Review: Cloud Storage](https://www.cloudskillsboost.google/course_templates/49/video/470033)

- [YouTube: Lab Review: Cloud Storage](https://www.youtube.com/watch?v=IyLUYkB4stI)

In this lab, you learn to create and work with Buckets and Objects and apply the following Cloud Storage features, Customer Supplied Encryption Keys, Access Control Lists, Life-cycle Management, Object Versioning, Directory Synchronization, and Cross-Project Resource Sharing using IAM. Now that you're familiar with many of the advanced features of Cloud storage, you might consider using them in a variety of applications that you might not have previously considered. A common, quick, and easy way to start using GCP is to use Cloud storage as a backup service. You can stay for a lab walk-through, but remember that GCP's user interface can change. So your environment might look slightly different. Welcome to the walk-through of the Cloud Storage Lab. At this point, I've already started the lab in Qwiklabs and I am logged into the GCP console using the username and password that was provided by Qwiklabs for me to log in to the GCP console. So the first task is preparation. I'm going to create a bucket in here. When I go to create a bucket, it specifically tells me that I should be using a globally unique ID. So I'm going to use my project ID, which is pretty unique. I'm going to call it myproj- and then my project ID, and it's telling us multi-regional. So storage class is multi-regional, and then it's telling me access control is set object-level and bucket-level permissions, and I'm going to hit create. So at this point, you can now go back to the lab page, and you can hit check my progress, and you should get a check mark in five points that you created the Cloud Storage Bucket. Next step is downloading a file. So I'm going to start Cloud Shell so I can do the curl command, and the first thing I'm going to do is I'm going to set an environment variable to the bucket name of the bucket I just created, just for ease of copy paste of commands. Export bucket name one equals and the bucket name. If I want to verify that that worked, I'm going to do an echo dollar sign, and the variable name to make sure that it got set correctly, and there it is. So now I'm going to download a file, which is just a publicly available Hadoop documentation, HTML file, and if I do an ls, I can see there's my setup.html and I am now going to copy it a couple times to make a setup two and a setup three. If I do an ls, I should see three files. There they are. So the second task is ACLs. We're going to copy this file into the bucket and then configure the access control list for it. So the first one is gsutil command, where I am copying setup.html into my bucket. Once it's copied, I then want to get the default access list that has been assigned to setup.html, which is based on the bucket because that's how we set it. Then right here, I piped it into acl.txt, and now I'm going to cut that, and we can see all of the permissions that had been assigned. So now I want to set the permissions to private. So I'm going to set it to private, and then in order to see it, I'm going to pipe it into acltwo.txt, and then cut that file, and you can see it's now set to private. Update the access list to make the file publicly readable by running the following command, and then I'm going to pipe it into aclthree, so that I can verify what that looks like. You can see it is readable by all users. This is another check point in a lab where you can hit check my progress and in this case is checking if you properly made that file publicly readable. So now I'm going to verify in my bucket using the console that my file is there and that is publicly viewable, and you can tell that based on this little icon and the public link that says that it's accessible to the public. So now in Cloud Shell, I'm going to remove the setup.html in my local Cloud Shell Instance. There it is. Let me remove it from the search here. If I do an ls, I'll see setup two and setup three, but not setup. You can see it got deleted. Let's say I accidentally deleted that from my Cloud Shell Instance, but now I want the copy that was in the bucket back on my local Cloud Shell. So I could just copy from the bucket to my local Cloud Shell, and if I do an ls again, I'll see all three setup files. There they are. The third task is to generate a customer supplied encryption key. To create the key, I'm going to run this command, and that's going to give me some output, and then I can copy this. But first, I'm going to see if I have a boto file. I'm going to do ls-al, and I do not see a boto file. So what I'm going to do is I'm going to run gsutilconfig-n, and then I'm going to do ls-al, and I should now see a boto file. There it is. So I'm going to do a nano.boto, and then I'm going to find the encryption key field, which I'm going to exit back out because I didn't not copy the key that I created, which I need. That is right here. Let me copy that, and let me go back to nano, and let me find the line with encryption underscore key. Could need to expand this because it's very hard to see. See decryption key here is encryption key. I'm going to uncomment this, and then I'm going to paste in my key here. I'm going to press control l, write that file, and then control x to exit nano. So now that I've set that up, I am going to upload the remaining setup two and setup three into the bucket. There's one, and there's the other. Now back in the console, let's go down, I'm going to refresh the bucket. I can see both of these files, and it shows that they are encrypted by a customer supply key. So this is another opportunity to check my progress and make sure I got the points for doing that step. Now what I'm going to do is I am deleting my local files by running remove setup star. So it's going to delete setup, setup two, and setup three. Now I am going to copy the files down from the bucket again, and if I want to cut the encrypted files to see whether I need them back, you can see there they are, and I successfully was able to bring them back even though they're encrypted. So now I'm going to move the current customer supplied encryption key to the decrypt key. So let's go to nano.boto. I'm going to find the comment out the line that I added earlier. I should've noted the line number, so that I wouldn't have to find it again. Decrypt keys in the GSUtil section. Let's see. I think I'm close. I'm looking for that line. So I'm going to comment out encryption key line and uncomment decryption key one right there. Then, I'm going to copy this into decryption key one, and then we save x. So a best practices you would actually delete the old customer key from the encryption line. But in this case, we just copy pasted it. So it's not a big deal. So I'm going generate a new key and then I'm going back to boto files. So I am going to add a new encryption key line, make sure that I copied the new key I made, and then do the same thing again. Sparsed it so I am adding a new encryption key equals, and I'll paste in the new key. Then control O to save, control X to exit. Now, I'm going to rewrite the key for file 1, and comment out the old decrypt key. Again, to bottom. Then I am going to comment out the decryption key 1. Now, while the instructions have you using nano, you definitely could use the Cloud Shell editor as well. That might be a little more pleasant than using this tool, but I'll leave it to you. You would just access that by hitting this little pencil here. It's fine. Decryption key 1 real quick. So we're commenting that out. Then, we're going to save it, and click. Now, we're going to download setup 2, and download setup 3. What happened, no decryption key matches because we commented it out, which makes sense. So the last task in this lab is, we are going to run the following command to view the current life-cycle policy. So we're going to do this. It says it has no life-cycle configuration. So I'm going to create a JSON lifecycle policy file. I'm going to paste the following rule in here. So it's saying if it's over 31 days I'm going to delete it. Writing exit. Then, to set the policy I'm going to run the command provided in the box, and to verify that the policy worked, I'm going to press that. This is another opportunity for you to check your progress and get more points in the lab. This point you should have about 20 out of 35 points. The task 6 is enabling versioning and you can do that by using the following command. Says it suspended, which means it's not enabled. So if we want to enable versioning, we're going to run this command. Then if we were to run the Get-Command again, we would not see that it was suspended we would say that it was enabled. There it is. So check your progress again you'll get more points. The next step, we're going to create several versions of the sample file in the bucket. So I'm going do an ls here. Going to open the setup HTML file. Delete any five lines to change the size. So I'm going to comment out this link and then I'm going to delete all of these links. Probably a faster way to do this thing just holding down delete. That's what I'm doing here. I'm going to delete it all the way to the banner. So I have now effectively changed the size of the file. So I'm going to control O, enter, control X. I'm going to copy the file to the bucket. I'm going to go back to setup.html, delete another five lines. Let's delete some more links. I'm just going to delete up to here. I'm going to save it. Then I'm going to copy it again. So if I wanted to list all versions of the file, which each subsequent one I was deleting different lines and making the size smaller, I was creating a new version. You can see there are three versions: the original one, the one where I deleted the first five lines, and then the one where I deleted the next set of lines. So I am now going to store the version value in the environment variables. So I'm going to say, export version name equals, the oldest version is this one. I'm going to copy that. I'm going to set this variable here make sure it got set correctly, and it is set correctly. Now, I'm going to download the oldest version, call it recovery.text. Then I'm going to verify recovery with a couple of commands. It is saying, c ls setup.html. Looks like that piece didn't work. I think what I did was I set the version name to the wrong thing, it should have been here. So now I can do the Gsutil again, and it still didn't match. So you do this, it's because you didn't follow instructions like me. You should have copied the entire URL for that object. Usually, what happens with the lab is if you have an issue is usually not that the lab is broken. It's usually that you missed a step. So go back three steps and repeat, and that usually works, because you can see here that just worked. Ls.al Setup.html, there's the file. I want to see the recovered text. You can see that the size is different here. So task 7, we're going to synchronize a directory to a bucket and just copy these in. Then I'm going to sync the first-level directory on the VM with my bucket. I'm going to verify that versioning was enabled. How I can check in the browser, I'm going to refresh the bucket, and we go back here, first level. You can see there's a second level. We can see the same thing in the console as we do in the command line. So I can exit Cloud Shell. So now we're going to do some cross-project sharing, this is the last little piece of this lab. So I'm going to open another tab. I'm also going to go to console.cloud.google.com, and I am now signed in, I'm going to select the other project. This one I have 26. I am going to copy the project from the Qwiklabs site, I'm here at lab guide, and I'm going to select that project. This is my other project. Then I am going to now create a bucket for this project. There shouldn't be one in here because it's a new project. I'm going to also call it myproj in project ID, and Create. This will now be bucket name two. So I'm going to upload a file, any file. I've uploaded a screenshot, and this will be my file name. So I'm actually going to rename it. I can't. Now I'm going to go to IAM, service accounts, and then create a service account. I'm going to call it cross-project-storage, and click create. Then I'm going to give it the Storage Object Viewer. Click continue. I'm going to create a key. I'm going to select JSON, create, and then it's going to download that file for me. It's there. Hit close, and I can hit done. So I am now going to rename this credentials.json. Here it is. I'm going to switch back to the other project, check my progress, and I should get five more points. So now we are just five points away from finishing the lab. Now we are in project ID one, and we're going to create a VM, create. Calling it crossproject. I'm going to make it in Europe in D, and I'm making it a micro, and create. VM is ready, I'm going to SSH into it. There it is, click SSH, then move my window back here to get the bucket name of the project I created here. I'm going to verify that it worked, and then I'm going to export the file name of the file that I uploaded. Grab that, put quotes around there because that space is in it. Verify that worked, and there it is. LS what's in that bucket. Form a VM on this side, it tells me that I don't have access to do that, so now I'm going to verify that. I am going to upload here, upload file. I'm going to select the credentials.json that I downloaded. Close, then I'm going to authorize that file to verify access. I'm going to do this again, and now I can see my file in there. I can do it with the file as well. Let me try to copy these credentials so they don't have access to that project. So if I wanted to do that, I would go back to this project, and modify the role in IAM. It Should be my last step here, going back to IAM, cross-project-storage, pencil. I'm also going to give Storage Object Admin, save. Once I hit save, I can check my progress, and then you will have all of the points in the lab. The last step is optional, you're just going to return to your SSH terminal, and verify that everything is good to go but that is the entire walkthrough for this lab. I hope you enjoyed it.

### Video - [Cloud SQL](https://www.cloudskillsboost.google/course_templates/49/video/470034)

- [YouTube: Cloud SQL](https://www.youtube.com/watch?v=ehK0JgmMQ5Y)

Let’s dive into the structured or relational database services. First up is Cloud SQL. Why would you use a Google Cloud service for SQL, when you can install a SQL Server application image on a VM using Compute Engine? The question really is, should you build your own database solution or use a managed service? There are benefits to using a managed service, so let’s learn about why you’d use Cloud SQL as a managed service inside of Google Cloud. Cloud SQL is a fully managed service of either MySQL, PostgreSQL, or Microsoft SQL Server databases. This means that patches and updates are automatically applied, … but you still have to administer MySQL users with the native authentication tools that come with these databases. Cloud SQL supports many clients, such as Cloud Shell, App Engine and Google Workspace scripts. It also supports other applications and tools that you might be used to like SQL Workbench, Toad and other external applications using standard MySQL drivers. Cloud SQL delivers high performance and scalability with up to 64 TB of storage capacity, 60,000 IOPS, and 624 GB of RAM per instance. You can easily scale up to 96 processor cores and scale out with read replicas. Currently, you can use Cloud SQL with either MySQL 5.6, 5.7, or 8.0, PostgreSQL 9.6, 10, 11, 12, 13, 14, or 15, or either of the Web, Express, Standard or Enterprise SQL Server 2017 or 2019 editions. Let’s focus on some other services provided by Cloud SQL: In HA configuration, within a regional instance, the configuration is made up of a primary instance and a standby instance. Through synchronous replication to each zone's persistent disk, all writes made to the primary instance are replicated to disks in both zones before a transaction is reported as committed. In the event of an instance or zone failure, the persistent disk is attached to the standby instance, and it becomes the new primary instance. Users are then rerouted to the new primary. This process is called a failover. Cloud SQL also provides automated and on-demand backups with point-in-time recovery. You can import and export databases using mysqldump, or import and export CSV files. Cloud SQL can also scale up, which does require a machine restart or scale out using read replicas. That being said, if you are concerned about horizontal scalability, you’ll want to consider Cloud Spanner which we’ll cover later in this module. Choosing a connection type to your Cloud SQL instance will affect how secure, performant, and automated it will be. If you’re connecting an application that is hosted within the same Google Cloud project as your Cloud SQL instance, and it is collocated in the same region, choosing the Private IP connection will provide you with the most performant and secure connection using private connectivity. In other words, traffic is never exposed to the public internet. Note that connecting to the Cloud SQL Private IP address from VMs in the same region is only a performance-based recommendation and not a requirement. If the application is hosted in another region or project, or if you are trying to connect to your Cloud SQL instance from outside of Google Cloud, you have 3 options. In this case, I recommend using the Cloud SQL Auth Proxy, which handles authentication, encryption, and key rotation for you. If you need manual control over the SSL connection, you can generate and periodically rotate the certificates yourself. Otherwise, you can use an unencrypted connection by authorizing a specific IP address to connect to your SQL server over its external IP address. You will explore these options in an upcoming lab. [https://cloud.google.com/sql/docs/mysql/private-ip]. To summarize, let’s explore this decision tree to help you find the right data storage service with full relational capability. Memorystore provides a fully-managed in-memory data store service for workloads requiring microsecond response times, or that have large spikes in traffic, as seen in gaming environments and real-time analytics. If you don’t need an in-memory data store, but your use case is relational data used primarily for analytics, these workloads are best supported by BigQuery. However, if your relational data workload isn’t analytics, the choice lies between Cloud Spanner and Cloud SQL. If you don’t need horizontal scaling or a globally available system, Cloud SQL is a cost-effective solution.

### Video - [Lab Intro: Cloud SQL](https://www.cloudskillsboost.google/course_templates/49/video/470035)

- [YouTube: Lab Intro: Cloud SQL](https://www.youtube.com/watch?v=lmsEIDajtBc)

Let's take some of the Cloud SQL concepts that we just discussed, and apply them in a lab. In this lab you configure a Cloud SQL Server, and learn how to connect an application to it via a proxy over an external connection. You also configure a connection over a private IP link that offers performance and security benefits. The app we chose to demonstrate in this lab is WordPress, but the information and best practices are applicable to any application that needs an SQL Server. By the end of this lab, you will have two working instances of a WordPress front end connected over two different connection types to its SQL instance back end, as shown in this diagram.

### Lab - [Implementing Cloud SQL](https://www.cloudskillsboost.google/course_templates/49/labs/470036)

In this lab you create a Cloud SQL instance and a client VM instance.  Then configure encrypted access using SSL certificates.

- [ ] [Implementing Cloud SQL](../labs/Implementing-Cloud-SQL.md)

### Video - [Lab Review: Cloud SQL](https://www.cloudskillsboost.google/course_templates/49/video/470037)

- [YouTube: Lab Review: Cloud SQL](https://www.youtube.com/watch?v=POkBsXPfWWY)

In this lab you created a Cloud SQL database and configured it to use both an external connection over a secure proxy and a private IP address, which is more secure and performant. If your application is hosted in another Region, VPC, or even project, use a proxy to secure its connection over the external connection. You can stay for a lab walkthrough, but remember that GCP's user interface can change, so your environment might look slightly different. Welcome to the lab walkthrough for implementing Cloud SQL. At this point in the lab, I have logged in with the username and password that QwikLabs has provided me. My first task is to create a Cloud SQL database. And go here to SQL, And hit Create Instance. I'm going to choose my SQL, And I'm going to call this wordpress-db. For password I'm just going to use the word password, so that I make sure I'm not forgetting it. I recommend you use something very simple. I'm going to use US Central 1. I am going to expand the configuration options, and then in Connectivity I'm going to select the private IP. Hit Enable API, and once that's been enabled, which could take a couple of seconds, I'm going to hit Allocate and Connect. And just so you know, this could take three to five minutes, so just be patient. Once that's done, it is going to make this Create button enabled. Feel free to look through some of the other things that it's calling out in the lab, like configuring the machine type and changing the storage capacity. If you add a couple of zeros, you can see the throughput increases. Set it back to 10, hit Close here. Again, this could take three to five minutes, so be patient. Once it's done, it'll say Create here. All right, so my IP has been allocated, and now I can hit Create. Here we go. And now, that took a while, but creating your Cloud SQL instance or Cloud SQL instance, might take even longer. So be patient, but while this is creating I can do other steps in the lab. The verification in step 15 is going to require that your Cloud SQL instance is running, and that there is a green check here. So you won't be able to get that step and those five points until this is done, but you can do some of the other steps while we wait. So while this is going, I'm going to open another tab so that this is still running and I can check on it. And I am now going to go to Compute Engine, which I could either go here, or this tile right here has my Compute Engine instances. As you can see, two have been created for me. wordpress-europe-proxy, which is the proxy for my Cloud SQL instance and for the private IP instance. So for this one, I'm going to click SSH. And when that is ready, to SSH into I am going to download the Cloud SQL proxy and then I'm going to make it executable. So I'm going to do a wget, Enter. It's copied it and made it executable. So in order to start the proxy, you need the connection name of the Cloud SQL instance, which requires that it's actually running. So we'll go ahead, and go back here, and see what the status is. I'm going to refresh and see if anything is there. Now it's not there, but it does let me click on it. And let's see if the connection name is there, and it is here. Instance connection name, so I'm going to copy that, and I'm going to go back to my SSH window. And I am going to create an environment variable for that connection instance. So I'm going to do an export SQL_CONNECTION =, paste that in there. And if I want to verify that that environment variable is set, I'm going to do an echo SQL, oops, spell it right. And it should output that, and there it is. So I'm not going to get too far ahead, because the rest of the steps do require that my Cloud SQL instance is running. You can see it's still creating, so it is not going to let me create a database yet. So I'm just going to leave it on this screen, so that when it is completely done it'll let me create a database, which I need for the next step. All right, at this point my instance is running. So if I go to Cloud SQL, it'll have a green checkmark next to wordpress-db. Right here, it says it's runnable. So the step I was missing is I need to create a database, and I am going to create a database called wordpress, because that's what the application expects, and I'm going to hit Create. And now I am going to return to, My SSH window, and I'm just going to make sure that it still has my environment variable. Then I am going to activate the proxy connection to my SQL database. By running this, I'll run it in the background. And then I'm going to expect that it says Ready for new connections, which it has output. I'm going to press Enter. And this is a point in the lab where you can also hit Check my Progress. And at this point you should have all ten points in the lab, but we're still going to do one more step, which is task 3. Actually, task 3 and task 4, where we're going to connect the application to the Cloud SQL instance. So I'm going to configure the wordpress application. So I'm going to copy the curl command. Go ahead and run that, and it is going to output the external IP address for my virtual machine. I'm going to copy that, and open it here, And then I'm going to hit Let's Go. I am going to leave everything with default, except I'm going to change the username to root and I'm going to put in the password that I defined, which is password. And then for database host I am going to use a localhost IP, which is 127.0.0.1, and then I'm going to hit Submit. Now, when the connection has been made, it is going to let me install WordPress, and I'm going to click Run the Installation. This could take a few moments to complete. Once this is done I should get a success window, and this can take up to three minutes depending on where you are running your lab from. So here it goes, apparently it's very fast for me. And so once the connection has been made, I am going to go here, and I am going to remove everything passed to the external IP. Delete, hit Enter. And when this loads, I should be able to see my blog. So it looks like it's still installing, so I will go ahead and be patient. Add some Information that I don't need to remember. So this is My Fake Site title. Any username, leave that password it gave me, m@b.com, and Install. And this was actually a step in the lab that I ignored, which was step 7, so don't do what I did and skip a step. So I had a successful installation. So what I'm going to do here is I'm going to remove all of the information that's after the external IP. When I hit Enter, it should take me to my blog. And Hello world! This is my blog, success. So the last task is to connect to Cloud SQL via the internal IP. So I'm going to go back here, and go to SQL and gcp. I'm going to click on wordpress-db, and then I am going to note the private IP address here. And I'm actually going to note it on a note, because pretty sure that it's going to have me copy something else. So make sure you copy it down somewhere in a clipboard, and then I'm going to go to Compute Engine. And it is going to want me to copy the external IP address for WordPress private IP. I'm going to copy that, and paste it in a new tab, press Enter. I'm going to hit Let's Go, and then the database name I'm going to leave alone, and I'm going to change this to root, and leave password because that's what I put before. Then I'm going to put the SQL private IP that I copied earlier to my Notepad, just need to find it. It's here, I'm going to copy that in here and hit Submit. And then I'm going to hit Run the Installation, and I should get Already Installed. So I created a direct connection to a private IP instead of configuring a proxy, and that connection is private. If I remove here, the same private IP should get my blog, and there it is. So in review, we created a Cloud SQL database and we configured it to use an external connection over a secure proxy as well as a private IP address. Hope you enjoyed the lab, thanks for watching.

### Video - [Cloud Spanner](https://www.cloudskillsboost.google/course_templates/49/video/470038)

- [YouTube: Cloud Spanner](https://www.youtube.com/watch?v=OvYVxi2z0OE)

If cloud SQL does not fit your requirements because you need horizontal scalability, consider using cloud spanner. Cloud spanner is a service built for the cloud specifically to combine the benefits of relational database structure with non relational horizontal scale. This service can provide petabytes of capacity and offers transactional consistency at global scale schemas, SQL and automatic synchronous replication for high availability. Use cases include financial applications and inventory applications traditionally served by relational database technology depending on whether you create a multi regional or regional instance, you'll have different monthly up time sls as shown on this slide. However, for up to date numbers, you should always refer to the documentation, which you'll find in the link section of this video. Let's compare cloud spanner with both relational and non relational databases like a relational database. Cloud spanner has schema, SQL and strong consistency. Also like a non relational database, Cloud spanner offers high availability, horizontal scalability and configurable replication, as mentioned, Cloud spanner offers the best of the relational and non relational worlds. These features allow for mission critical use cases such as building consistent systems for transactions and inventory management in the financial services and retail industries. To better understand how all of it works. Let's look at the architecture of cloud spanner. A cloud spanner instance replicates data in end cloud zones which can be within one region or across several regions. The database placement is configurable, meaning you can choose which region to put your database in. This architecture allows for high availability and global placement. The replication of data will be synchronized across zones using Google's global fiber network. Using atomic clocks ensures adamiscity whenever you are updating your data. That's as far as we're going to go with cloud spanner. Because the focus of this module is to understand the circumstances when you would use cloud spanner, let's look at a decision tree. If you have outgrown any relational database are shutting your databases for throughput, high performance, need transactional consistency, global data and strong consistency. Or just want to consolidate your database. Consider using cloud spanner. If you don't need any of these nor full relational capabilities, consider a no SQL service, such as cloud Fire store, which we will cover next. If you're now convinced that using cloud spanner as a managed service is better than using or re implementing your existing my SQL solution, see the link section for a solution on how to migrate from my SQL to cloud spanner.

### Video - [AlloyDB](https://www.cloudskillsboost.google/course_templates/49/video/470039)

- [YouTube: AlloyDB](https://www.youtube.com/watch?v=Icfw5ETwjt4)

Let’s now talk about AlloyDB. AlloyDB for PostgreSQL is a fully managed, PostgreSQL-compatible database service that's designed for demanding workloads such as hybrid transactional and analytical processing. AlloyDB pairs a Google-built database engine with a cloud-based, multi-node architecture to deliver enterprise-grade performance, reliability, and availability. AlloyDB automates administrative tasks, such as backups, replication, patching, and capacity management. AlloyDB also uses adaptive algorithms and machine learning for PostgreSQL vacuum management, storage and memory management, data tiering, and analytics acceleration. AlloyDB provides fast transactional processing, more than 4 times faster than standard PostgreSQL for transactional workloads. It's suitable for demanding enterprise workloads, including workloads that require high transaction throughput, large data sizes, or multiple read replicas. AlloyDB provides high-availability and an 99.99% uptime SLA, inclusive of maintenance. AlloyDB also provides real-time business insights and is up to 100 times faster than standard PostgreSQL for analytical queries. Built-in integration with Vertex AI, Google's artificial intelligence platform, lets you call machine learning models.

### Video - [Firestore](https://www.cloudskillsboost.google/course_templates/49/video/470040)

- [YouTube: Firestore](https://www.youtube.com/watch?v=n2xU_RfYUAc)

If you are looking for a highly scalable NoSQL database for your applications, consider using Cloud Firestore. Cloud Firestore is a fast, fully managed, serverless, cloud native, NoSQL, document database that simplifies storing, synking and querying data for your mobile web and IOT apps at global scale. Its client libraries provide live synchronization and offline support and it's security features and integrations with Firebase and GCP accelerate building truly serverless apps. Cloud Firestore also supports ACID transactions so if any of the operations in the transaction fail and cannot be retried, the whole transaction will fail. Also with automatic multi region replication and strong consistency, your data is safe and available even when disasters strike. Cloud Firestore even allows you to run sophisticated queries against your NoSQL data without any degradation in performance. This gives you more flexibility in the way you structure your data. Cloud Firestore is actually the next generation of Cloud Datastore. Cloud Firestore can operate in Datastore mode, making it backwards compatible with Cloud Datastore. By creating a Cloud Firestore database in Datastore mode, you can access Cloud Firestore's improveD storage layer while keeping Cloud Datastore system behavior. This removes the following Cloud Datastore limitations. Queries are no longer eventually consistent instead, they are all strongly consistent. Transactions are no longer limited to 25 entity groups, rights to an entity group are no longer limited to 1 per second. Cloud Firestore in native mode introduces new features such as a new, strongly consistent storage layer, a collection and document data model, real time updates, mobile and web client libraries. Cloud Firestore is backward compatible with Cloud Datastore but the new data model, real time updates in mobile and web client library features are not. To access all of the new Cloud Firestore features, you must use Cloud Firestore in native mode. A general guideline is to use Cloud Firestore in Datastore mode for new server projects and native mode for new mobile and web apps. As the next generation of Cloud Datastore, Cloud Firestore is compatible with all Cloud Datastore, APIs and client libraries. Existing Cloud Datastore users will be live upgraded to Cloud Firestore automatically at a future date. For more information, see the link section of this video, to summarize, let's explore this decision tree. To help you determine whether Cloud Firestore is the right storage service for your data. If your schema might change and you need an adaptable database, you need to scale to zero or you want low maintenance overhead scaling up to terabytes consider using Cloud Firestore. Also, if you don't require transactional consistency, you might want to consider Cloud Bigtable. Depending on the cost or size, I will cover Cloud Bigtable next.

### Video - [Cloud Bigtable](https://www.cloudskillsboost.google/course_templates/49/video/470041)

- [YouTube: Cloud Bigtable](https://www.youtube.com/watch?v=dVOwvKWn9cw)

If you don’t require transactional consistency, you might want to consider Cloud Bigtable. Cloud Bigtable is a fully managed NoSQL database with petabyte-scale and very low latency. It seamlessly scales for throughput and it learns to adjust to specific access patterns. Cloud Bigtable is actually the same database that powers many of Google’s core services, including Search, Analytics, Maps, and Gmail. Cloud Bigtable is a great choice for both operational and analytical applications, including IoT, user analytics, and financial data analysis, because it supports high read and write throughput at low latency. It’s also a great storage engine for machine learning applications. Cloud Bigtable integrates easily with popular big data tools like Hadoop, Cloud Dataflow, and Cloud Dataproc. Plus, Cloud Bigtable supports the open source industry standard HBase API, which makes it easy for your development teams to get started. Cloud Dataflow and Cloud Dataproc are covered late in the course series. For more information on the HBase API, see the links section of this video. Cloud Bigtable stores data in massively scalable tables, each of which is a sorted key/value map. The table is composed of rows, each of which typically describes a single entity, and columns, which contain individual values for each row. Each row is indexed by a single row key, and columns that are related to one another are typically grouped together into a column family. Each column is identified by a combination of the column family and a column qualifier, which is a unique name within the column family. Each row/column intersection can contain multiple cells, or versions, at different timestamps, providing a record of how the stored data has been altered over time. Cloud Bigtable tables are sparse; if a cell does not contain any data, it does not take up any space. The example shown here is for a hypothetical social network for United States presidents, where each president can follow posts from other presidents. Let me highlight some things: * The table contains one column family, the follows family. This family contains multiple column qualifiers. * Column qualifiers are used as data. This design choice takes advantage of the sparseness of Cloud Bigtable tables, and the fact that new column qualifiers can be added as your data changes. . * The username is used as the row key. Assuming usernames are evenly spread across the alphabet, data access will be reasonably uniform across the entire table. This diagram shows a simplified version of Cloud Bigtable’s overall architecture. It illustrates that processing, which is done through a front-end server pool and nodes, is handled separately from the storage. A Cloud Bigtable table is sharded into blocks of contiguous rows, called tablets, to help balance the workload of queries. Tablets are similar to HBase regions, for those of you who have used the HBase API. Tablets are stored on Colossus, which is Google's file system, in SSTable format. An SSTable provides a persistent, ordered immutable map from keys to values, where both keys and values are arbitrary byte strings. As I mentioned earlier, Cloud Bigtable learns to adjust to specific access patterns. If a certain Bigtable node is frequently accessing a certain subset of data... … Cloud Bigtable will update the indexes so that other nodes can distribute that workload evenly, as shown here. That throughput scales linearly, so for every single node that you do add, you're going to see a linear scale of throughput performance, up to hundreds of nodes. In summary, if you need to store more than 1 TB of structured data, have very high volume of writes, need read/write latency of less than 10 milliseconds along with strong consistency, or need a storage service that is compatible with the HBase API, consider using Cloud Bigtable. If you don’t need any of these and are looking for a storage service that scales down well, consider using Firestore. Speaking of scaling, the smallest Cloud Bigtable cluster you can create has three nodes and can handle 30,000 operations per second. Remember that you pay for those nodes while they are operational, whether your application is using them or not.

### Video - [Memorystore](https://www.cloudskillsboost.google/course_templates/49/video/470042)

- [YouTube: Memorystore](https://www.youtube.com/watch?v=F7rycmtpVEA)

Let's get a quick overview of Memorystore. Memorystore for Redis provides a fully managed in-memory data store service built on scalable, secure and highly available infrastructure managed by Google. Applications running on Google Cloud can achieve extreme performance by leveraging the highly scalable, available, secure Redis service without the burden of managing complex Redis deployments. This allows you to spend more time writing code so that you can focus on building great apps. Memorystore also automates complex tasks like enabling high availability, failover, patching, and monitoring. High availability instances are replicated across two zones and provide a 99.9% availability SLA. You can easily achieve the sub-millisecond latency and throughput your applications need. Start with the lowest tier and smallest size, and grow your instance effortlessly with minimal impact to application availability. Memorystore can support instances up to 300 GB and network throughput of 12 gigabits per second. Because Memorystore for Redis is fully compatible with the Redis Protocol, you can lift and shift your applications from open source Redis to Memorystore without any code changes by using the import/export feature. There's no need to learn new tools because all existing tools in client libraries just work.

### Quiz - [Quiz: Storage and Database Services](https://www.cloudskillsboost.google/course_templates/49/quizzes/470043)

#### Quiz 1.

> [!important]
> **What data storage service might you select if you just needed to migrate a standard relational database running on a single machine in a datacenter to the cloud?**
>
> - [ ] Cloud Storage
> - [ ] BigQuery
> - [ ] Persistent Disk
> - [ ] Cloud SQL

#### Quiz 2.

> [!important]
> **Which data storage service provides data warehouse services for storing data but also offers an interactive SQL interface for querying the data?**
>
> - [ ] Dataproc
> - [ ] Datalab
> - [ ] Cloud SQL
> - [ ] BigQuery

#### Quiz 3.

> [!important]
> **Which Google Cloud data storage service offers ACID transactions and can scale globally?**
>
> - [ ] Cloud Storage
> - [ ] Cloud Spanner
> - [ ] Cloud SQL
> - [ ] Cloud CDN

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/49/video/470044)

- [YouTube: Module Review](https://www.youtube.com/watch?v=6Li6CGVb33A)

In this module, we covered the different storage and database services that Google Cloud offers. Specifically, you learned about Cloud Storage, a fully managed object store; Filestore, a fully managed file storage service; Cloud SQL, a fully managed MySQL and PostgreSQL database service; Cloud Spanner, a relational database service with transactional consistency, global scale and high availability; AlloyDB, a fully managed, PostgreSQL-compatible database service; Firestore, a fully managed NoSQL document database; Cloud Bigtable, a fully managed NoSQL wide-column database; and Memorystore, a fully managed in-memory data store service for Redis. From an infrastructure perspective, the goal was to understand what services are available and how they're used in different circumstances. Defining a complete data strategy is beyond the scope of this course; however, Google offers courses on data engineering and machine learning on Google Cloud that cover data strategy.

## Resource Management

Manage and examine billing of Google Cloud resources

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/49/video/470045)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=RarWueVqv1g)

In this module we will cover resource management. Resources in GCP are billable so managing them means controlling cost. There are several methods in place for controlling access to the resources and there are quotas that limit consumption. In most cases, the default quotas can be raised on request. But having them in place provides a checkpoint or a chance to make sure that this really is a resource you intend to consume in greater quantity. In this module, we will build on what we learned in the cloud IAM module. First, I will provide an overview of the resource manager. Then we will go into quotas, labels, and names. Next we will cover billing to help you set budgets and alerts. To complete your learning experience, you will get to examine billing data with BigQuery in a lab. Let's get started with an overview of resource manager.

### Video - [Resource Manager](https://www.cloudskillsboost.google/course_templates/49/video/470046)

- [YouTube: Resource Manager](https://www.youtube.com/watch?v=l_2CTt_e9_s)

The resource manager lets you hierarchically manage resources by project, folder, and organization. This should sound familiar because we covered it in the Cloud IAM module. Let me refresh your memory: Policies contain a set of roles and members, and policies are set on resources. These resources inherit policies from their parent, as we can see on the left. Therefore, resource policies are a union of parent and resource if an IAM allow policy is associated. However, if an IAM deny policy is associated with the resource, then the policy can prevent certain principals from using certain permissions, regardless of the roles they’re granted. Although IAM policies are inherited top-to-bottom, billing is accumulated from the bottom up, as we can see on the right. Resource consumption is measured in quantities, like rate of use or time, number of items, or feature use. Because a resource belongs to only one project, a project accumulates the consumption of all its resources. Each project is associated with one billing account, which means that an organization contains all billing accounts. Let’s explore organizations, projects, and resources more. Just to reiterate, an organization node is the root node for all Google Cloud Platform resources. This diagram shows an example where we have an individual, Bob, who is in control of the organizational domain through the organization admin role. Bob has delegated privileges and access to the individual projects to Alice by making her a project creator. Because a project accumulates the consumption of all its resources, it can be used to track resources and quota usage. Specifically, projects let you enable billing, manage permissions and credentials, and enable service and APIs. To interact with Cloud Platform resources, you must provide the identifying project information for every request. A project can be identified by: The project name, which is a human-readable way to identify your projects, but it isn't used by any Google APIs. There is also the project number, which is automatically generated by the server and assigned to your project. And there is the project ID, which is a unique ID that is generated from your project name. You can find these three identifying attributes on the dashboard of your GCP Console or by querying the Resource Manager API. Finally, let’s talk about the resource hierarchy. From a physical organization standpoint, resources are categorized as global, regional, or zonal. Let’s look at some examples: Images, snapshots, and networks are global resources; External IP addresses are regional resources; and instances and disks are zonal resources. However, regardless of the type, each resource is organized into a project. This enables each project to have its own billing and reporting.

### Video - [Quotas](https://www.cloudskillsboost.google/course_templates/49/video/470047)

- [YouTube: Quotas](https://www.youtube.com/watch?v=D8A-x_Mv7f4)

Now that we know that a project accumulates the consumption of all its resources, let's talk about quotas. All resources in Google Cloud are subject to project quotas or limits. These typically fall into one of the three categories shown here. How many resources you can a project create? For example, you can only have fifteen VPC networks per project. How quickly you can make API requests in a project or rate limits. For example, by default, you can only make five administrative actions per second per project when using the Cloud Spanner API. And three - Regional quotas For example, by default, you can only have 24 CPUs per region. Given these quotas, you may be wondering, how do I spin up one of those 96 Core VMs. As your use of Google Cloud expands over time, your quotas may increase accordingly. If you expect a notable upcoming increase in usage, you can proactively request quota adjustments from the quotas page in the Cloud console. This page will also display your current quotas. If quotas can be changed, why do they exist? Project quotas prevent runaway consumption in case of error or malicious attack. For example, imagine you accidentally create 100 instead of 10 Compute Engine instances using the G Cloud command line. Quotas also prevent billing spikes or surprises. Quotas are related to billing, but we will go through how to set up budgets and alerts later, which will really help you manage billing. Finally, quotas for sizing consideration and periodic review. For example, do you really need a 96 Core instance? Or can you go with a smaller and cheaper alternative? It is also important to mention the quotas are the maximum amount of resources you can create for that resource type as long as those resources are available. Quotas do not guarantee that resources will be available at all times. For example, if a region is out of local SSDs, you cannot create local SSDs in that region, even if you still hae quota for local SSDs.

### Video - [Labels](https://www.cloudskillsboost.google/course_templates/49/video/470048)

- [YouTube: Labels](https://www.youtube.com/watch?v=MhhTxjR8MQY)

Projects and folders provide levels of segregation for resources, but what if you want more granularity? That’s where labels come in. Labels are a utility for organizing GCP resources. Labels are key-value pairs that you can attach to your resources, like VMs, disks, snapshots and images. You can create and manage labels using the GCP console, gcloud, or the Resource Manager API, and each resource can have up to 64 labels. For example, you could create a label to define the environment of your virtual machines. Then you define the label for each of your instances as either production or test. Using this label, you could search and list all your production resources for inventory purposes. Labels can also be used in scripts to help analyze costs or to run bulk operations on multiple resources. The screenshot on the right shows an example of 4 labels that are created on an instance. Let’s go over some examples of what to use labels for: I recommend adding labels based on team or cost center to distinguish instances owned by different teams. You can use this type of label for cost accounting or budgeting. For example, team:marketing and team:research. You can also use labels to distinguish components. For example, component:redis, component:frontend. Again, you can label based on environment or stage. You should also consider using labels to define an owner or a primary contact for a resource. For example, owner:gaurav, contact:opm. Or add labels to your resources to define their state. For example, state:inuse, state:readyfordeletion. Now, it’s important to not confuse labels with network tags. Labels, we just learned, are user-defined strings in key-value format that are used to organize resources, and they can propagate through billing. Network tags, on the other hand, are user-defined strings that are applied to instances only and are mainly used for networking, such as applying firewall rules and custom static routes. For more information about using labels, see the links in the Course Resources.

### Video - [Billing](https://www.cloudskillsboost.google/course_templates/49/video/470049)

- [YouTube: Billing](https://www.youtube.com/watch?v=5867HA81Mwo)

Because the consumption of all resources under a project accumulates into one billing account, let’s talk billing. To help with project planning and controlling costs, you can set a budget. Setting a budget lets you track how your spend is growing toward that amount. This screenshot shows the budget creation interface: First, you set a budget name and specify which project this budget applies to. Then, you can set the budget at a specific amount or match it to the previous month's spend. After you determine your budget amount, you can set the budget alerts. These alerts send emails to billing admins after spend exceeds a percent of the budget or a specified amount. In our case, it would send an email when spending reaches 50%, 90%, and 100% of the budget amount. You can even choose to send an alert when the spend is forecasted to exceed the percent of the budget amount by the end of the budget period. In addition to receiving an email, you can use Pub/Sub notifications to programmatically receive spend updates about this budget. You could even create a Cloud Function that listens to the Pub/Sub topic to automate cost management. Here is an example of an email notification. The email contains the project name, the percent of the budget that was exceeded, and the budget amount. Another way to help optimize your Google Cloud spend is to use labels. For example, you could label VM instances that are spread across different regions. Maybe these instances are sending most of their traffic to a different continent, which could incur higher costs. In that case, you might consider relocating some of those instances or using a caching service like Cloud CDN to cache content closer to your users, which reduces your networking spend. I recommend labeling all your resources and exporting your billing data to BigQuery to analyze your spend. BigQuery is Google’s scalable, fully managed Enterprise Data Warehouse with SQL and fast response times. Creating a query is as simple as shown in this screenshot, which you will explore in the upcoming lab. You can even visualize spend over time with Looker Studio. Looker Studio turns your data into informative dashboards and reports that are easy to read, easy to share, and fully customizable. For example, you can slice and dice your billing reports using your labels.

### Video - [Demo: Billing Administration](https://www.cloudskillsboost.google/course_templates/49/video/470050)

- [YouTube: Demo: Billing Administration](https://www.youtube.com/watch?v=xBxQF2KhLpE)

In the upcoming lab, you will examine billing data that we exported for you. Let me show you how to export billing data and demonstrate other common activities that a billing administrator performs. These actions cannot be performed in the Qwiklabs environment because of security restrictions. Therefore, I'm going to walk you through them as a demo. So here I am in the GCP console. And what I want to do is navigate to billing. So I'm going to click on the navigation menu and click on Billing. So here I'm provided with an overview. I'm actually using a trial account, as you can see, but the same concepts apply to any account. We can see the consumption for the current month. In my case, I have some promotional credits here. If I had multiple billing accounts, then you'd be able to choose from them right up here. Again, credits, billing account also has a name. If you go to Payment Overview, you'll can see the payment method that is currently selected. Now, the other really big thing we can do is here is set up budgets and alerts. So if I click on that and then click on Create Budget, I first just give it a name. So you My-Budget-Alert. I can select the specific projects that I want these alerts on. It could just be one, it could be several, click Next. Then the type so either specify an exact dollar amount or I start with my last month spend. So if last month I spent would say a certain dollar amount, it would pull that dollar amount directly in there. In our case, let's say we want to target an amount of $500. It's also including credits in this cost. You could disable that. And then we define the thresholds for the alerts. So by default, it sets up this 50, 90 at a 100%, and the dollar amounts are being pulled from the 500 that I just plugged in there earlier. You can also choose between actual and forecasted. And you can read a little bit more what this forecast it is about here. We could remove these. So let's say maybe we wanted to add an earliest threshold already at 25%, then we could do that. We could even go further and actually connect to a cloud pub/sub topic and then do all sorts of automation as we discussed in the slides. So then from here I can just click Finish. And it's then going to be sending me emails around this. I can also see sort of a menu here that shows how far I've gone in the spend. And so I can come in here at any moment and also see, am I close to that 25% mark already? Then the other thing that's pretty interesting on this page is the Transaction page. So this will show all of the different charges. Again, I have credit in here. So every charge is being offset by credit, but you can see all the different usages that I've had across Compute Engine instances, disk space that I've been using. So you could always go in here. Now more interestingly is to probably export all this information. So if I click on Billing Export, I'm presented with two options. I could export to BigQuery or export it as a file, and to enable that I just choose the one I'm interested in. Let's say BigQuery. And then go to edit settings and then I would define where this is going to go to. In this case, I would have to define a BigQuery data set, so you can navigate there and set it up and click Save. And similarly, if I wanted to export to a file, I could edit those settings as well. In this case, could be exported as a CSV or JSON file and stored in a cloud storage bucket. So I can define the name in here. So I would have to create a bucket first and then give it a prefix. And then it's going to export that to there. The other big thing, I can reviews if I click on payment method. I can review the different payment accounts. Payment profiles, payment method if it's a credit card or bank account. You can review all of that information in there. That's how easy it is to administer billing in GCP. A billing administrator can set up accounts and run reports which are ordinary tasks. But becoming familiar with the available options and seeing how these tasks are performed reduces the chances of confusion. For example, you know that reports can be generated in JSON or CSV format. Now more sophisticated processing or filtering of data occurs after the billing is exported, as you will explore in the next lab.

### Video - [Lab Intro: Examining Billing Data with BigQuery](https://www.cloudskillsboost.google/course_templates/49/video/470051)

- [YouTube: Lab Intro: Examining Billing Data with BigQuery](https://www.youtube.com/watch?v=HH0akDDsx6w)

Let's examine billing data with BigQuery. In this lab, you will sign into BigQuery and create a dataset. In this dataset, you will create a table by importing billing data that is stored in a Cloud Storage bucket. Next, you will run simple queries on the imported data, and then you will run more complex queries on a larger dataset.

### Lab - [Examining Billing data with BigQuery](https://www.cloudskillsboost.google/course_templates/49/labs/470052)

In this lab you will create datasets and tables, import data from billing reports, and conduct a variety of queries on the data using BigQuery.

- [ ] [Examining Billing data with BigQuery](../labs/Examining-Billing-data-with-BigQuery.md)

### Video - [Lab Review: Examining Billing Data with BigQuery](https://www.cloudskillsboost.google/course_templates/49/video/470053)

- [YouTube: Lab Review: Examining Billing Data with BigQuery](https://www.youtube.com/watch?v=J3t2RhBNsa4)

In this lab, you imported billing data into BigQuery that had been exported as a CSV file. You first ran a simple query on that data, next you accessed a shared dataset containing more than 22,000 records of billing information, you then ran a variety of queries on that data to explore how you can use BigQuery to gain insight into your resources billing consumption. If you use BigQuery on a regular basis, you'll start to develop your own queries for searching out where resources are being consumed in your application. You can also monitor changes in resource consumption over time. This kind of analysis is an input to capacity planning and can help you determine how to scale up your application to meet growth or scale down your application for efficiency. Welcome to the walk-through of the lab examining billing data with BigQuery. At this point in the lab, I have logged in with the username and password that Qwiklabs has provided me from the lab. So the first task is to use BigQuery to import data. So what I did is as the Billing Administrator, I exported my billing data and put it in a bucket. So I am going to go into BigQuery and I'm going to import some stuff. So BigQuery? Yes. The Cloud Council. Thank you. Make sure that you are logged in to BigQuery and have the correct Qwiklabs project ID selected at the top. So I'm going to go here and I'm going to click Create dataset and I am going to call it imported billing data. My data location is in the US and I want it to expire one day afterwards. I'm going to hit Create dataset. You can see my dataset is created and I should see it right here. There it is. So now I'm going to create a table in that dataset. Table. For the source, I'm going to use Cloud Storage, I'm going to copy and paste the bucket location from the lab and it is in CSV format. For destination, I'm using that and native table and I am going to call it sampleinfotable. Under schema, I I'm going to hit auto detect so it detects a schema and input parameters from the dataset. I'm going to open up the advance and I am going to specify that I want to skip one row because that's the headers, and then I'm going to hit Create table. So this is a point where you can check the progress in your lab. If you click check my progress, it's going to check that you have dataset at a table and that you have imported that data into that table, and you should get five points for that. So Task 2, you are going to examine the data that you've just input. So I'm going to click on my table and it's going to, by default, show the schema. I can click Details and it's going to tell me a little bit more information about number of rows. You can see it has 44 rows, it's a pretty small table. I can hit Preview and it's going to show me a couple of the first rows of the table. So it now wants me in the lab. There are some formative questions that are going to ask you just to make sure that you're understanding the learning. So I'm not going to go over that in the walk-through because that is more and to make sure that you're understanding what you're doing. So I'm going to go to Task 3 where we're composing a simple query. A couple of cool things about BigQuery, if you are in a table by default, if you click Query table, it's going to auto-populate the query editor with the project dataset table for you and then you just specify what you want to look at. So I do select star and I'm doing this. Oops. Where cost is greater than zero. So I just want to see in this table how much of it. I only want to see the rows where the cost is more than zero. You can see right here it's validating that my SQL or my SQL is right. I'm going to hit Run. Here are my query results and you can see out of a table that has 44, there are 20 rows in this table that actually have costs that are more than zero. So again, there are a couple more questions that you can answer and you can also check your progress that you've run this query. So if you run the query, then it's going to give you another five points. At this point, you're actually done with the points that are awarded in the lab, but you still have another task to go into a little more complex query. So I'm going to go ahead and copy the query from Task 4, and I'm going to erase this and paste it in. It's a valid query here. I'm going to hit Run, then I'm going to verify that the result has what my lab is telling me is, supposed to return 22,537 lines of billing data. I can see right here, that is correct. Let's say I wanted to find the latest 100 records where the charges were greater than zero. So I'm going to copy paste the query that's provided to me, and make sure it's valid. Always good to check that your SQL is valid. Hit Run, and it is going to show me the last 100 records where charges were greater than zero. Let's say I wanted to find all of the charges that were more than $3, the next query shows you that. You can feel free to click through each one of these more complex queries and feel free to try out some queries of your own. If you wanted to just peruse the data and figure out maybe the last two days of billing anything that was over $10. Any kind of question that you might need to provide data for to your senior leadership about your billing of resource usage in GCP. After all of these complex queries, in review, you imported billing data that was technically exported for you from a billing admin into BigQuery, and then you run a simple query and then you ran some more complex queries. I hope you enjoyed the lab. Thank you.

### Quiz - [Quiz: Resource Management](https://www.cloudskillsboost.google/course_templates/49/quizzes/470054)

#### Quiz 1.

> [!important]
> **How do quotas protect Google Cloud customers?**
>
> - [ ] By preventing resource use of too many different Google Cloud services.
> - [ ] By preventing resource use in too many zones in a region.
> - [ ] By preventing uncontrolled consumption of resources.
> - [ ] By preventing resource use by unknown users.

#### Quiz 2.

> [!important]
> **No resources in Google Cloud can be used without being associated with...**
>
> - [ ] A bucket.
> - [ ] A project.
> - [ ] A user.
> - [ ] A virtual machine.

#### Quiz 3.

> [!important]
> **A budget is set at $500 and an alert is set at 100%. What happens when the full amount is used?**
>
> - [ ] Everything in the associated project will be suspended because there is no more budget to spend.
> - [ ] You have a 4-hour courtesy period before Google shuts down all resources.
> - [ ] A notification email is sent to the Billing Administrator.
> - [ ] Nothing. There is no point in sending a notification when there is no budget remaining.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/49/video/470055)

- [YouTube: Module Review](https://www.youtube.com/watch?v=mtpwMtPjhLQ)

Person: In this module, we covered the Cloud Resource Manager and went into quotas, labels, and billing. Then we analyzed the billing data with BigQuery in a lab. Reporting is an important part of resource management. You can generate reports to track consumption and to establish accountability. A key principle in Google Cloud is transparency, and that means it's straightforward to access and process consumption data, as you observed in this module.

## Resource Monitoring

Monitor resources using Google Cloud's operations suite

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/49/video/470056)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=PjL7IUGve9c)

In this module, I’ll give you an overview of the resource monitoring options in Google Cloud. The features covered in this module rely on Google Cloud’s operations suite, a service that provides monitoring, logging, and diagnostics for your applications. In this module we are going to explore the Cloud Monitoring, Cloud Logging, Error Reporting, Cloud Trace, and Cloud Profiler services. Let me start by giving you a high-level overview of Google Cloud’s operations suite and its features. Let me start by giving you a high-level overview of Google Cloud’s operations suite and its features.

### Video - [Google Cloud's Operations Suite Overview](https://www.cloudskillsboost.google/course_templates/49/video/470057)

- [YouTube: Google Cloud's Operations Suite Overview](https://www.youtube.com/watch?v=bUCIk-zAsdw)

Google Cloud’s operations suite dynamically discovers cloud resources and application services based on deep integration with Google Cloud and Amazon Web Services. Because of its smart defaults, you can have core visibility into your cloud platform in minutes. This provides you with access to powerful data and analytics tools plus collaboration with many different third-party software providers. As mentioned earlier, Google Cloud’s operations suite has services for monitoring, logging, error reporting, and fault tracing. You only pay for what you use, and there are free usage allotments so that you can get started with no upfront fees or commitments. For more information about pricing, refer to the link in the Course Resources. Now, in most other environments, these services are handled by completely different packages, or by a loosely integrated collection of software. When you see these functions working together in a single, comprehensive, and integrated service, you'll realize how important that is to creating reliable, stable, and maintainable applications.

### Video - [Monitoring](https://www.cloudskillsboost.google/course_templates/49/video/470058)

- [YouTube: Monitoring](https://www.youtube.com/watch?v=4RmbSYIgZxc)

Now that you understand Google Cloud’s operations suite from a high-level perspective, let’s look at Cloud Monitoring. Monitoring is important to Google because it is at the base of site reliability engineering, a discipline that applies aspects of software engineering to operations whose goals are to create ultra-scalable and highly reliable software systems. This discipline has enabled Google to build, deploy, monitor, and maintain some of the largest software systems in the world. Cloud Monitoring dynamically configures monitoring after resources are deployed and has intelligent defaults that allow you to easily create charts for basic monitoring activities. This allows you to monitor your platform, system, and application metrics by ingesting data, such as metrics, events, and metadata. You can then generate insights from this data through dashboards, charts, and alerts. For example, you can configure and measure uptime and health checks that send alerts via email. A metrics scope is the root entity that holds monitoring and configuration information in Cloud Monitoring. Each metrics scope can have between 1 and 375 monitored projects. Now, monitoring data for all projects in that scope will be visible. A metrics scope contains the custom dashboards, alerting policies, uptime checks, notification channels, and group definitions that you use with your monitored projects. A metrics scope can access metric data from its monitored projects, but the metrics data and log entries remain in the individual projects. The first monitored Google Cloud project in a metrics scope is called the scoping project, and it must be specified when you create the metrics scope. The name of that project becomes the name of your metrics scope. To access an AWS account, you must configure a project in Google Cloud to hold the AWS Connector. Because metrics scopes can monitor all your Google Cloud projects in a single place, a metrics scope is a “single pane of glass” through which you can view resources from multiple Google Cloud projects and AWS accounts. All users of Google Cloud’s operations suite with access to that metrics scope have access to all data by default. This means that a role assigned to one person on one project applies equally to all projects monitored by that metrics scope. In order to give people different roles per-project and to control visibility to data, consider placing the monitoring of those projects in separate metrics scopes. Cloud Monitoring allows you to create custom dashboards that contain charts of the metrics that you want to monitor. For example, you can create charts that display your instances’ CPU utilization, the packets or bytes sent and received by those instances, and the packets or bytes dropped by the firewall of those instances. In other words, charts provide visibility into the utilization and network traffic of your VM instances, as shown on this slide. These charts can be customized with filters to remove noise, groups to reduce the number of time series, and aggregates to group multiple time series together. For a full list of supported metrics, please refer to the documentation. Now, although charts are extremely useful, they can only provide insight while someone is looking at them. But what if your server goes down in the middle of the night or over the weekend? Do you expect someone to always look at dashboards to determine whether your servers are available or have enough capacity or bandwidth? If not, you want to create alerting policies that notify you when specific conditions are met. For example, as shown on this slide, you can create an alerting policy when the network egress of your VM instance goes above a certain threshold for a specific timeframe. When this condition is met, you or someone else can be automatically notified through email, SMS, or other channels in order to troubleshoot this issue. You can also create an alerting policy that monitors your usage of Google Cloud’s operations suite and alerts you when you approach the threshold for billing. For more information about this, please refer to the documentation. Here is an example of what creating an alerting policy looks like. On the left, you can see an HTTP check condition on the summer01 instance. This will send an email that is customized with the content of the documentation section on the right. Let’s discuss some best practices when creating alerts: We recommend alerting on symptoms, and not necessarily causes. For example, you want to monitor failing queries of a database and then identify whether the database is down. Next, make sure that you are using multiple notification channels, like email and SMS. This helps avoid a single point of failure in your alerting strategy. We also recommend customizing your alerts to the audience’s needs by describing what actions need to be taken or what resources need to be examined. Finally, avoid noise, because this will cause alerts to be dismissed over time. Specifically, adjust monitoring alerts so that they are actionable and don’t just set up alerts on everything possible. Uptime checks can be configured to test the availability of your public services from locations around the world, as you can see on this slide. The type of uptime check can be set to HTTP, HTTPS, or TCP. The resource to be checked can be an App Engine application, a Compute Engine instance, a URL of a host, or an AWS instance or load balancer. For each uptime check, you can create an alerting policy and view the latency of each global location. Here is an example of an HTTP uptime check. The resource is checked every minute with a 10-second timeout. Uptime checks that do not get a response within this timeout period are considered failures. So far there is a 100% uptime with no outages. Monitoring data can originate at a number of different sources. With Google Compute Engine instances, because the VMs are running on Google hardware, the hypervisor cannot access some of the internal metrics inside a VM, for example, memory usage. The Ops Agent collects metrics inside the VM, not at the hypervisor level. The Ops Agent is the primary agent for collecting telemetry data from your Compute Engine instances. This diagram shows how data is collected to monitor workloads running on a Compute Engine instance. Ops agent installed on the Compute Engine collects data beyond the system metrics. The collected metric is then used by Cloud Monitoring to create Dashboards, alerts, uptime checks and notifications to drive observability for workloads running in your application. You can configure the Ops Agent to monitor many third-party applications. For a detailed list, refer to the documentation. The Ops Agent supports most major operating systems such as CentOS, Ubuntu and Windows. If the standard metrics provided by Cloud Monitoring do not fit your needs, you can create custom metrics. For example, imagine a game server that has a capacity of 50 users. What metric indicator might you use to trigger scaling events? From an infrastructure perspective, you might consider using CPU load or perhaps network traffic load as values that are somewhat correlated with the number of users. But with a Custom Metric, you could actually pass the current number of users directly from your application into Cloud Monitoring. To get started with creating custom metrics, please refer to the documentation. When you want to maintain a metric at a target value, specify a utilization target. The autoscaler creates VMs when the metric value is above the target and deletes VMs when the metric value is below the target. If the metric comes from each VM in your managed instance group, then the autoscaler takes the average metric value across all VMs in the managed instance group and compares it with the utilization target. If the metric applies to the whole managed instance group and does not come from the VMs in your managed instance group, then the autoscaler compares the metric value with the utilization target. When your metric has multiple values, apply a filter to autoscale using an individual value from the metric.

### Video - [Lab Intro: Resource Monitoring](https://www.cloudskillsboost.google/course_templates/49/video/470059)

- [YouTube: Lab Intro: Resource Monitoring](https://www.youtube.com/watch?v=MlSWXvv7mQ4)

Let's take some of the monitoring concepts that we just discussed and apply them in a lab. In this lab, you will learn how to use Stackdriver monitoring to gain insight into applications that run on GCP. Specifically, you will enable Stackdriver monitoring, add charts to dashboards, and create alerts, resource groups, and up-time checks.

### Lab - [Resource Monitoring](https://www.cloudskillsboost.google/course_templates/49/labs/470060)

In this lab you learn how to use Cloud Monitoring to gain insight into applications that run on Google Cloud Platform.

- [ ] [Resource Monitoring](../labs/Resource-Monitoring.md)

### Video - [Lab Review: Resource Monitoring](https://www.cloudskillsboost.google/course_templates/49/video/470061)

- [YouTube: Lab Review: Resource Monitoring](https://www.youtube.com/watch?v=_y4Sd8DkH9E)

In this lab you got an overview of stackdriver monitoring. You learned how to monitor your project, create alerts with multiple conditions, add charts to dashboards, create resource groups, and create uptime checks for your services. Monitoring is critical to your applications health. And Stackdriver provides a rich set of features for monitoring your infrastructure, visualizing the monitoring data and triggering alerts and events for you. You can stay for a lab walkthrough, but remember that GCP is user interface can change, so your environment might look slightly different. Welcome to the lab walkthrough for resource monitoring with stackdriver. At this point I have logged into the GCP console with the credentials that the Quick Labs lab has provided me. And in the first task I am going to verify that the proper Vms had been set up for me using deployment manager in this lab. And as you can see, there are three Vms. 3 engine X stacks right here. So now that I verified that my instances are running and were created for me, I am going to go to stackdriver monitoring which will open in a new tab. And then it is going to set up the workspace for my project. And this can take a few minutes. So just be patient or go get a cup of coffee and come back. Once your workspace has been set up for you, you are going to be redirected to the monitoring overview page. There are some questions in the labs that are going to ask you some questions and those are just making sure that your understanding and actually reading. But you don't have to fill those out in order to get the full score for the labs. So in task two, we're going to create a dashboard, so I'm going to go here. To monitoring overview when I click create dashboard. I am going to name it my dashboard instead of untitled hit enter. And then I'm going to add a chart. For the title, I am going to say this is my chart. And I am going to find GCE. VM instance. For metrics, I am going to select CPU utilization. CPU utilization and for filter. Where is filter here? I'm going to add a filter. There are various options you can filter by resource label by metadata label. I'm not going to add any filters, I want to see everything. And then we click here on view options. There's a couple of chart modes. There's color mode X Ray mode. You can preview it on the right. Stats mode and like that. So I actually like the X Ray mode, so I'm going to go ahead and click that. And then you're going to hit save to add the chart to your dashboard. There it is, looking nice. So then we also have a metrics explorer which allows you to examine resources and metrics without having to create a chart on the dashboard. So if I go to resources metrics explorer. Find resource type in metric, I can type any metric or resource name. So let's say I do CPU utilization. And as you can see I didn't have to add this, but I could still explore it. Again, you'll have another question in the lab, but that is to prompt your understanding. So now, I'm going to create an alert and add the first condition. So I'm going to go here and create a policy. And I'm going to click add condition. Here I'm going to do GCE VM instance. And for metrics, I'm going to use CPU usage. For condition, I'm going to say is above threshold. Of one minute, the threshold is 20. And then I'm going to hit save. And I'm going to add another condition. And then what said do I do it for another VM? So if I do this one. Maybe I do it for another metric going to, do you? Stu reserved course. And then above 15. I'm going to head, save. So now in policy triggers I'm going to trigger when all conditions are met. Then I'm going to configure the notifications so that I can be actually told that this has triggered. And I'm going to click here. Email, and I'm going to add some email going to do fake email. I want you guys spamming me. And then add. That has been added. And then I'm going to stick, skip the documentation step, but in reality this is a pretty important part of your notification. You want to say. What happened, why whoever it is that's getting notified is being alerted and a best practices to actually tell them how to maybe fix it. because otherwise that's not a very useful notification if they don't know how they can fix it. And then you're going to name it, and this is my. I first alerting policy. And then I'm going to hit save. This is a checkpoint in the lab where you can check your progress that you have created an alerting policy. The next task you are going to create some groups here. Create group, I'm going to give it a name. VM instances. Name, I am going to select. Contains and I'm going to type engine X. And I'm going to save group. And you can see it's showing me instances. All three of my instances because the name matches Anjanette stack. And again, you're going to have another. Question to make sure that your understanding and reading all of the extra tidbits that are available in the lab. So now, we're going to go back to the Dashboard. You' re going to go to uptime checks. Overview and then we're going to add an uptight and check. So in here we're going to add a title, so my first uptime check were using HTTP. It is to check an instance in, applies to a group and I'm going to select group I created which is VM instances and I'm going to check every one minute. I'm going to hit save. You could also hit test for it and make sure that it works. I'm going to say no thanks. I don't want to create the alert policy right now. So this is the last piece where you can hit check my progress. It's going to make sure that you created that uptime check, and if so, you're going to get the full points for the lab. So in this lab, we got to walk through monitoring your projects, creating a stackdriver workspace which gets created for you. Creating some alerts with multiple condition, adding some charts to a dashboard, and creating resource groups, and finally we created an uptime check for your services. Hope you enjoyed it.

### Video - [Logging](https://www.cloudskillsboost.google/course_templates/49/video/470062)

- [YouTube: Logging](https://www.youtube.com/watch?v=Q9hrrq3pJbE)

Monitoring is the basis of Google Cloud’s operation suite, but the service also provides logging, error reporting, and tracing. Let’s learn about logging. Cloud Logging allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud and AWS. It is a fully managed service that performs at scale and can ingest application and system log data from thousands of VMs. Logging includes storage for logs, a user interface called Logs Explorer, and an API to manage logs programmatically. The service lets you read and write log entries, search and filter your logs, and create log-based metrics. Logs are only retained for 30 days, but you can export your logs to Cloud Storage buckets, BigQuery datasets, and Pub/Sub topics. Exporting logs to Cloud Storage makes sense for storing logs for more than 30 days, but why should you export to BigQuery or Pub/Sub? Exporting logs to BigQuery allows you to analyze logs and even visualize them in Looker Studio. BigQuery runs extremely fast SQL queries on gigabytes to petabytes of data. This allows you to analyze logs, such as your network traffic, so that you can better understand traffic growth to forecast capacity, network usage to optimize network traffic expenses, or network forensics to analyze incidents. For example, in this screenshot I queried my logs to identify the top IP addresses that have exchanged traffic with my web server. Depending on where these IP addresses are, and who they belong to, I could relocate part of my infrastructure to save on networking costs or deny some of these IP addresses if I don’t want them to access my web server. If you want to visualize your logs, I recommend connecting your BigQuery tables to Looker Studio. Looker Studio transforms your raw data into the metrics and dimensions that you can use to create easy-to-understand reports and dashboards. I mentioned that you can also export logs to Pub/Sub. This enables you to stream logs to applications or endpoints.

### Video - [Error Reporting](https://www.cloudskillsboost.google/course_templates/49/video/470063)

- [YouTube: Error Reporting](https://www.youtube.com/watch?v=ctze4xzHwxc)

Let’s learn about another feature of Google Cloud’s operations suite: Error Reporting. Error Reporting counts, analyzes, and aggregates the errors in your running cloud services. A centralized error management interface displays the results with sorting and filtering capabilities, and you can even set up real-time notifications when new errors are detected. Currently, Error Reporting is generally available for App Engine on both standard and flexible environments, Apps Script, Compute Engine, Cloud Functions, Cloud Run, Google Kubernetes Engine, and Amazon EC2. In terms of programming languages, the exception stack trace parser is able to process Go, Java, . NET, Node.js, PHP, Python, and Ruby.

### Video - [Tracing](https://www.cloudskillsboost.google/course_templates/49/video/470064)

- [YouTube: Tracing](https://www.youtube.com/watch?v=7D1vAzBQmOY)

Tracing is another Cloud Operations feature integrated into Google Cloud. Cloud Trace is a distributed tracing system that collects latency data from your applications and displays it in the Google Cloud console. You can track how requests propagate through your application and receive detailed near real-time performance insights. Cloud Trace automatically analyzes all of your application's traces to generate in-depth latency reports that surface performance degradations and can capture traces from App Engine, HTTP(S) load balancers, and applications instrumented with the Cloud Trace API. Managing the amount of time it takes for your application to handle incoming requests and perform operations is an important part of managing overall application performance. Cloud Trace is actually based on the tools used at Google to keep our services running at extreme scale.

### Video - [Profiling](https://www.cloudskillsboost.google/course_templates/49/video/470065)

- [YouTube: Profiling](https://www.youtube.com/watch?v=fj_0U3OhJUk)

Finally, let’s cover the last feature of Google Cloud’s operations suite in this module, which is the profiler. Poorly performing code increases the latency and cost of applications and web services every day. Cloud Profiler continuously analyzes the performance of CPU or memory-intensive functions executed across an application. While it’s possible to measure code performance in development environments, the results generally don’t map well to what’s happening in production. Many production profiling techniques either slow down code execution or can only inspect a small subset of a codebase. Profiler uses statistical techniques and extremely low-impact instrumentation that runs across all production application instances to provide a complete picture of an application’s performance without slowing it down. Profiler allows developers to analyze applications running anywhere, including Google Cloud, other cloud platforms, or on-premises, with support for Java, Go, Node.js, and Python.

### Video - [Partner Integrations](https://www.cloudskillsboost.google/course_templates/49/video/470066)

- [YouTube: Partner Integrations](https://www.youtube.com/watch?v=xFnGo_poxnw)

Google Cloud’s operations suite also supports a rich and growing ecosystem of technology partners, as shown on this slide. This helps expand the IT ops, security, and compliance capabilities available to Google Cloud customers. The diagram shows the architecture of how BindPlane ingests logs and then how those logs are ingested into Cloud Logging plus collects metrics and then how these metrics are ingested into Cloud Monitoring. Adding Blue Medora’s software, BindPlane, helps collect the metrics and logs and push them into the open APIs that form our core observability platform. Once a log source is ingested into Cloud Logging, you can view and search through the raw log data and create metrics off of those log files just like logs collected from Google Cloud. You can use all the features of Google Cloud’s operations management suite, including viewing logs in real time in the Google Cloud console or through log-based metrics to view logs and metrics side by side and alert on logs. This diagram depicts the architecture involved in filtering and exporting log data from Cloud Logging to Splunk Enterprise. Pub/Sub is used to temporarily store logged messages as they are published from Cloud Logging, before delivering them to Splunk. Cloud Logging begins by gathering logs into a centralized location, then forwards them to Google Cloud's Pub/Sub messaging service. Pub/Sub manages the log data using a dedicated channel. A primary Dataflow pipeline extracts logs from Pub/Sub and delivers them to Splunk for analysis. To safeguard against errors, a secondary Dataflow pipeline runs in parallel, capable of resending logs if delivery fails. Finally, Splunk (deployed on-premises, in Google Cloud as SaaS, or via a hybrid approach) receives the logs and enables in-depth analysis. The workflow uses a streaming pipeline to natively push your Google Cloud data to your Splunk Cloud or Splunk Enterprise instance using the Pub/Sub to Splunk Dataflow template. Using this Dataflow template, you can export data from Pub/Sub to Splunk. So, any message that can be delivered to a Pub/Sub topic can now be forwarded to Splunk. For more information about integrations, refer to the link in the Course Resources.

### Quiz - [Quiz: Resource Monitoring](https://www.cloudskillsboost.google/course_templates/49/quizzes/470067)

#### Quiz 1.

> [!important]
> **What is the foundational process at the base of Google's Site Reliability Engineering (SRE) ?**
>
> - [ ] Testing and release procedures.
> - [ ] Monitoring.
> - [ ] Root cause analysis.
> - [ ] Capacity planning.

#### Quiz 2.

> [!important]
> **Google Cloud's operations suite integrates several technologies, including monitoring, logging, error reporting, and debugging that are commonly implemented in other environments as separate solutions using separate products. What are key benefits of integration of these services?**
>
> - [ ] Reduces overhead, reduces noise, streamlines use, and fixes problems faster
> - [ ] Better for Google Cloud only so long as you don't need to monitor other applications or clouds
> - [ ] Detailed control over the connections between the technologies
> - [ ] Ability to replace one tool with another from a different vendor

#### Quiz 3.

> [!important]
> **What is the purpose of the Cloud Trace service?**
>
> - [ ] Reporting on latency as part of managing performance.
> - [ ] Reporting on application errors.
> - [ ] Reporting on Google Cloud system errors.
> - [ ] Reporting on Google Cloud resource consumption as part of managing performance.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/49/video/470068)

- [YouTube: Module Review](https://www.youtube.com/watch?v=CRchnVTUKQk)

In this module, I gave you an overview of Google Cloud’s operations suite and its monitoring, logging, error reporting, fault tracing, and profiling features. Having all of these integrated into Google Cloud allows you to operate and maintain your applications, which is known as site reliability engineering or SRE. If you’re interested in learning more about SRE, you can explore the SRE book or some of our SRE courses.

### Video - [Course Review](https://www.cloudskillsboost.google/course_templates/49/video/470069)

- [YouTube: Course Review](https://www.youtube.com/watch?v=czS0s0ebvxw)

Thank you for taking the “Essential Cloud Infrastructure: Core Services” course. I hope you have a better understanding of how to administer IAM, choose between the different data storage services in GCP, examine billing of GCP resources and monitor those resources. Hopefully the demos and labs made you feel more comfortable using the different GCP services that we covered. Next, I recommend enrolling in the “Elastic Cloud Infrastructure: Scaling and Automation” course, of the “Architecting with Google Compute Engine” series. In that course, we start by going over the different options to interconnect networks to enable you to connect your infrastructure to GCP. Next, we’ll go over GCPs load balancing and autoscaling services, which you will get to explore directly. Then, we’ll cover infrastructure automation services like Terraform, So that you can automate the deployment of GCP infrastructure services Lastly, we'll talk about other managed services that you might want to leverage in GCP.

### Document - [Next Course: Elastic Cloud Infrastructure: Scaling and Automation](https://www.cloudskillsboost.google/course_templates/49/documents/470070)

## Course Resources

PDF links to all modules

### Document - [Course Resources](https://www.cloudskillsboost.google/course_templates/49/documents/470071)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
