---
id: 1193
name: 'AI Services and GDC Deployments and Operations'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1193
date_published: 2025-01-24
topics:
  - Machine Learning
  - Logging
  - Deployment
---

# [AI Services and GDC Deployments and Operations](https://www.cloudskillsboost.google/course_templates/1193)

**Description:**

The course explores advanced services such as machine learning, and operational topics such as application deployment, monitoring, and troubleshooting.
In addition, we'll introduce GDC software upgrades, logging, billing, and cost monitoring.


**Objectives:**

* Describe when and how to use the GDC Vertex AI pre-trained APIs.
* Explore how to manage operations in your GDC environment.
* Describe methods to manage GDC software upgrades, logging, billing, and cost monitoring.

## Course Overview

This module introduces the audience, prerequisites, and agenda for the learning pathway.

### Video - [GDC Air-Gapped Practitioner Fundamentals overview](https://www.cloudskillsboost.google/course_templates/1193/video/522151)

* [YouTube: GDC Air-Gapped Practitioner Fundamentals overview](https://www.youtube.com/watch?v=Yy98wRTKghU)

SPEAKER: Welcome to the Google Distributed Cloud, or GDC air-gapped practitioner fundamentals series of courses. Within each of the three courses that make up this series, you'll focus on a specific aspect of GDC practitioner fundamentals. The first course provides an introduction to the GDC platform, which enables you to host, control, and manage infrastructure and services directly on your premises. GDC air-gapped is one component of Google Distributed Cloud offering, which aligns to Google's digital sovereignty vision and supports public sector customers and commercial entities that have strict data residency, security, or privacy requirements. The second course examines service resources or workload components that exist in projects. You'll learn about Kubernetes in GDC, Artifact Registry, GDC object storage, Database Service, networking, and key management and security. The third course explores advanced services such as machine learning and operational topics such as application deployment, monitoring and troubleshooting. In addition, we'll introduce GDC software upgrades, logging, billing, and cost monitoring. The intended target audience of this course series consists of various cloud operations professionals and system administrators, including DevOps engineers, application developers, cloud engineers, and database engineers. Additionally, this course is designed for solution architects planning to deploy applications and create application environments targeted for the GDC platform. To benefit fully from taking this course, you should have these prerequisite skills and knowledge-- an understanding of basic networking, familiarity with Linux, familiarity with identity and access management, familiarity with virtual machines, familiarity with Kubernetes, familiarity with databases, familiarity with Cloud APIs, and experience with application development. Along with the prerequisite skills and knowledge, it may help you to be familiar with the content from some Google Cloud courses, which you can find in the Course Resources. However, it is important to note that GDC differs in several significant ways from Google Cloud. Throughout this series of courses, you'll learn about GDC in context by exploring how Cymbal Federal, a fictional organization, accomplishes its goals of deploying three applications to the air-gapped platform. This case study attempts to mirror an enterprise level scenario, and aims to help you understand how the different components of the platform fit together and what their capabilities are.

## AI services

This module provides an overview of the Vertex AI services that are available in GDC. You'll learn about the features and use cases for the three pre-trained API models: Optical Character Recognition, Speech-to-Text, and Translation. You'll also find out about using Vertex AI Workbench, which lets you set up an end-to-end notebook-based production environment. 

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1193/video/522152)

* [YouTube: Module overview](https://www.youtube.com/watch?v=042doMPtjkE)

SPEAKER: Welcome to AI Services. In this module, you'll learn about Google Distributed Cloud, GDC, air-gapped Vertex AI services and how to enable them. You'll also learn how to configure access for an application to invoke the pre-trained APIs. This module explores the different artificial intelligence, AI, services that are available to practitioners of GDC, including the pre-trained API models and Vertex AI Workbench. We'll describe some potential use cases throughout the module as we examine the capabilities of the API models. After an overview of the different AI services, you'll learn how to configure your application access to the pre-trained APIs. Then each lesson will focus on one service, optical character recognition, OCR, speech-to-text translation, and the Vertex AI Workbench.

### Video - [AI services overview](https://www.cloudskillsboost.google/course_templates/1193/video/522153)

* [YouTube: AI services overview](https://www.youtube.com/watch?v=Qd6SSegjOds)

SPEAKER: Let's start with an overview of the AI services that are available in GDC air-gapped and how to enable them. Google democratizes access to Machine Learning, ML, by providing developers with API access to pre-trained ML models. The pre-trained models simplify and accelerate development when integrating ML inference or predictions within applications. OCR extracts text from images effortlessly with advanced capabilities. The Translation API bridges language barriers by leveraging translation services that facilitate communication across diverse linguistic landscapes. And Speech-to-Text transforms spoken language into written text, enabling seamless integration of voice data into applications. In addition to the three pre-trained models for APIs, GDC offers Vertex AI Workbench for use within the data science and AI development workflow. Before we explore GDC Vertex AI services in more detail, let's explain how to enable them. By default, GDC Vertex AI is not enabled, and access is controlled by the organization IAM administrator. GDC leverages the Kubernetes Resource Model, KRM, API to enable and disable Vertex AI services. This management API, optionally used by the AI platform admin, ensures flexible control over AI resources. Only the organization IAM administrator can grant the AI platform admin role to users. This role provides permissions to enable or disable pre-trained APIs within the GDC console. Users without the correct permissions won't see the buttons to manage pre-trained APIs. It's important to note that, to use AI services, all files used for model inference or predictions must be saved locally in the GDC platform. Each pre-trained API model corresponds to a specific permission set. The ai-ocr-developer, ai-speech-developer, and ai-translation-developer roles give permissions for their related models. Platform administrator and application operator roles for Vertex AI Workbench include workbench-notebooks-admin, which gives permissions for read/write access to all notebook resources within a project namespace, and workbench-notebooks-viewer, which gives permissions for read-only access to all notebook resources within a project namespace. Developers leverage endpoints to interact programmatically with Vertex AI services. When a service is in the enabled state, its corresponding endpoint is enabled, allowing developers access for integration and interaction with Vertex AI capabilities. Before you can enable the pre-trained APIs, you must complete these steps. First, sign in to the GDC console. Second, create and grant permissions to users, allowing them to enable pre-trained APIs. Next, provision the GDC system cluster with GPUs. Then, configure the GDC ingress gateway. And, finally, set up the GDC Domain Name System, DNS. You can enable, disable, and view the endpoints of the Vertex AI services from the GDC console. Developers leverage endpoints to interact programmatically with Vertex AI services. When a service is in the enabled state, its corresponding endpoint is enabled, allowing developers seamless access for integration and interaction with Vertex AI capabilities. To view the status of a project's pre-trained APIs, sign into the GDC console. In the Navigation console, expand Vertex AI. Then, click on Pre-trained APIs. The status of the pre-trained APIs displays as either enabled or disabled. Here's the workflow for how pre-trained APIs are enabled. The platform administrator creates a project and assigns permissions, including ML API administrative access using the ai-platform-admin role. This role allows application operators, application team members, the permission to enable the pre-trained APIs. The platform administrator assigns the workbench-notebooks-admin role to the application operator. The workbench-notebooks-admin role provides permissions to create notebooks. The application operator can now create notebooks within the namespace or project. By using a service account that has access to connect the pre-trained APIs, they can create data analysis workflows within notebook cells. Developers interact with GDC Vertex AI through various interfaces, including REST API, gRPC, gRPCurl, and client libraries. These interfaces provide different methods for developers to integrate with and utilize the capabilities of GDC Vertex AI.

### Video - [Configure application access to pre-trained API](https://www.cloudskillsboost.google/course_templates/1193/video/522154)

* [YouTube: Configure application access to pre-trained API](https://www.youtube.com/watch?v=aXqo4EelRRA)

SPEAKER: Now let's discuss how to configure application access to invoke the Vertex AI pretrained API services. Here's an overview of what happens. To use Vertex AI services, you personify workloads that exist in projects with service accounts that you have granted permissions to access and invoke the APIs, which include OCR, speech-to-text, and translation. This invocation process is the same for cloud environments such as Google Cloud, because generally, the APIs provided by cloud vendors require authentication and authorization. Google provides client libraries for each API that can be used for model inference and predictions. Note that the pretrained APIs provide REST and gRPC endpoints. gRPC provides optimized payload data transmission and uses the HTTP2 protocol. Next, let's review the access configuration process in detail. The first step is for the application operator to create a service account with a service account key within the project. Next, you grant the service account the appropriate ML API roles. These could include. Vertex AI optical character recognition developer, Vertex AI speech-to-text developer, or Vertex AI translation developer. Then implement an environment variable to reference the service account authentication key. For the fourth and final step, depending on whether the application uses the client library of the API directly, you generate the authentication information for the request using either gdcloud or the Python client library. Let's explore these options. Applications designed to invoke the API using grpcurl will generate the identity token and use it as part of the invoking request as a request header. The code snippet uses the gdcloud CLI to generate the token. You then use the token to set the authorization header. Applications designed to invoke the API using the Python SDK will generate the identity token programmatically with the client library. The Python code snippet has a function called test_get_token that generates the token. The client library also constructs the API request to invoke methods of pretrained ML models.

### Video - [Optical character recognition (OCR)](https://www.cloudskillsboost.google/course_templates/1193/video/522155)

* [YouTube: Optical character recognition (OCR)](https://www.youtube.com/watch?v=y-azuirnl8Y)

SPEAKER: In this lesson, you'll learn about the OCR feature of Vertex AI, which detects typed text in a photo image or handwritten text. The OCR pre-trained API detects and transcribes text from handwriting and from PDF, TIFF, JPG, and PNG files. Data is returned in JSON format and includes page, block, paragraph, word, and break information. Responses can be stored in a database. For analysis, you must store files locally in the GDC environment. You can't use files hosted in Google Cloud Storage or files that are publicly available for text detection. The primary difference between Vision technology on GDC versus Google Cloud is that Google Cloud Vision supports image, facial, and crop hint recognition, while Vision on GDC only supports OCR. For OCR, 46 languages are supported, with 24 more in experimental mode. The text recognition feature has three categories of language support. Supported languages are prioritized with regular performance evaluation. This level includes languages such as Chinese, Danish, and French. Experimental languages are under active development. They don't have regular performance evaluations. This level includes languages such as Bosnian, Irish, and Latin. Mapped languages are supported by mapping them to another language code or a general character recognizer. For example, the language code EN-GB, or British English, is supported but not treated differently than EN, or English, for text recognition. Vertex AI tries to return the correct mapped language code in the entity locale field, but mapped languages are more likely to be misidentified than fully supported or experimentally supported languages. There are supported scripts for handwriting recognition, including Japanese, Korean, and Latin. Additional languages are experimental and under active development, such as Bengali and Cyrillic. Latin refers to all languages that use Latin-based alphabets, including English. Vertex AI on GDC supports two methods for extracting text from files and images for batch processing-- BatchAnnotateImages, which extracts text from JPEG and PNG files, and BatchAnnotateFiles, which extracts text from PDF and TIFF files. Keep in mind that only images stored locally in GDC can be processed for OCR. Asynchronous methods are not supported, and Vertex AI on GDC doesn't support any other OCR API method that is supported on Google Cloud. Now let's consider these two supported methods in more detail. Use the BatchAnnotateImages method to detect and extract text from a batch of JPEG and PNG files. Specify the type, content and, optionally, language hints in the request. Use the BatchAnnotateFiles method to detect and extract text from a batch of PDF and TIFF files. Specify the type, content, MIME_type, and, optionally, language hints and pages in the request. The OCR API detects text on a local image by sending the contents of an image file as a Base64-encoded string in the body of a request. To use the OCR client library, first install the Vertex AI client libraries. Then set the OCR endpoint to invoke within your code. This is a sample code for OCR text detection using the REST API. The image or file to be analyzed must be Base64-encoded. This is an example of OCR text detection using Python. Note that Vertex AI client libraries and Python 3.7 must be installed before using Python. A key element of the code is that authentication and authorization occurs for all GDC API requests, including the ML APIs. The client libraries assist in constructing the API request securely. The BatchAnnotateFiles and BatchAnnotateImages methods let you specify one or more language hints to aid in the detection of languages in images. If a language is not specified, Vertex AI enables automatic language detection, which usually results in the most accurate results. Language hints are not required to be set for languages based on the Latin alphabet. In rare cases, when the language of the text in the image is known, setting a hint improves the results. However, if the hint is incorrect, it might cause a significant impediment. Text detection returns an error if a specified language isn't one of the supported languages. In the example, the language hint specifies English language, EN; transform extension singleton, t; input method engine transform extension code, I0; and handwriting transform code, handwrit. This roughly says the language is English transformed from handwriting. You do not need to specify a script code, because the EN language implies Latin.

### Video - [Speech-to-Text](https://www.cloudskillsboost.google/course_templates/1193/video/522156)

* [YouTube: Speech-to-Text](https://www.youtube.com/watch?v=lG_BJkHTXeE)

SPEAKER: In this lesson, let's explore the Speech-to-Text API. Speech-to-Text facilitates seamless integration of Google's speech recognition technologies into your solutions. As ML technology, it provides advanced speech-recognition capabilities while allowing operational sovereignty. This control extends to protected speech data, ensuring compliance with data residency and other regulatory requirements. The result is a powerful and customizable solution that meets both your technological and regulatory needs. GDC Speech-to-Text has several key capabilities. It provides transcription by applying advanced deep-learning neural network algorithms from Google to automatic speech recognition. GDC Speech-to-Text deploys models that are less than 1 gigabyte in size and consumes minimal resources, and it supports 16 languages. The pre-trained APIs must be enabled before using the Speech-to-Text client library. Let's review examples of how to use the Speech-to-Text client library using cURL commands or Python. To import the Speech-to-Text client library and transcribe an audio file using Python, follow these steps. First, open a notebook as your coding environment. If you don't have an existing notebook, create a notebook. Second, install the Speech-to-Text library from a tar file and use the client library within your Python code to request a Speech-to-Text transcription. Finally, run your code to generate a Speech-to-Text transcription. Next, we'll cover a code sample to import the Speech-to-Text client library and transcribe an audio file. This example code uses Python to install the Speech-to-Text library to generate a transcription. The sample code transcribes an audio file and invokes the Speech-to-Text API using the gRPC endpoint. First, view the status of the pretrained models and identify your endpoint to use in the code. The path for the local audio file and endpoint or endpoints must be provided for the Speech-to-Text API call to transcribe the audio file. Then, replace language_code with a supported language code. The code sample sets the audible frequency rate for the audio file to 16,000 hertz. The code sample shows how to import the Speech-to-Text client library and transcribe an audio file using GRP cURL tool. There is no auto-sensing. The encoding and the sample rate must be specified each time. Replace language_code with a supported language code.

### Video - [Translation](https://www.cloudskillsboost.google/course_templates/1193/video/522157)

* [YouTube: Translation](https://www.youtube.com/watch?v=KB6xyhTnYMo)

SPEAKER: In this lesson, let's explore GDC translation. GDC translation breaks down language barriers with pre-trained models that let you synchronize translation of text and HTML. The Translation API provides standard API libraries for developers. It supports more than 100 languages, including Arabic, Farsi, French, German, Italian, Russian, and Spanish. Let's examine how you can use the Translation API to translate languages to English text. The translate text method takes input text in a supported non-English language and returns the English translated results. The input text can be HTML or plain text. In the case of HTML, only the words in between HTML tags and attributes within the tags are translated. The Translation API does not translate the tags themselves. The detect language method reads a text string and returns the determined language. The getSupportedLanguages method lists the languages that are available for translation. And createGlossary lets you create term pairs to introduce specialized vocabulary, such as industry jargon. We'll discuss glossaries in more detail next. A glossary enables you to define specific terms in your domain by creating term pairs. Each pair consists of a source language term and its corresponding target language equivalent. While using a glossary is optional, it adds significant value when dealing with domain-specific jargon or technical terms. This customization can markedly improve the quality and relevance of translated content. The glossary lets you introduce specialized vocabulary that might not be accurately translated using generic language models. You can use a glossary to specify not to translate words that refer to product names. For example, Google Home must translate to Google Home. You can also reduce ambiguity by specifying the meaning of vague words and homonyms. For example, bat can mean a piece of sports equipment or an animal. And a glossary can clarify the meaning of words adopted from a different language. For example, bouillabaisse in French translates to bouillabaisse in English, a fish stew dish. The terms in a glossary can be single tokens, words, or short phrases, usually fewer than five words. Translation ignores any matching glossary entries if the words are glossary stop words. To get started with GDC translation, enable the translation API, which makes the translation service available to all projects. Obtain a private key with the appropriate credentials and install the REST and Python client libraries to call the API. Next, let's review the translation methods to translate a language into English. This slide presents a Python code snippet using the GDC translation API. The code imports the Google Cloud translation library, initializes the translation client, translates text from French to English, and displays the translated text in English. The next code samples that we'll cover run kube control proxy to handle the authentication and access the API using cURL. Once the kube control proxy is running, you can explore the API with cURL, wget, or a browser. As we review more examples that will use cURL and grpcurl. And let's assume that the kube control proxy is configured and running. The code is an example of how to translate the Spanish text, "eso es una prueba," into, "hello, this is a test," calling the translate text method and specifying the endpoint. What if you don't want to translate a word or phrase? You can use an HTML tag to exclude parts of your text from translation. This code is an example of how to exclude hola or hello from your translation. The detect language method returns the language of a text string by sending an HTTP request. To call the detect language method, specify the endpoint. This code is an example of how to detect language using GDC Translation API written in cURL. The getSupportedLanguages method returns a list of languages supported by the Translation API. To call the getSupportedLanguages method, specify the endpoint. This code is an example of how to getSupportedLanguages using GDC Translation API written in cURL. The get method returns the latest state of a long-running operation. Use this method to pull the operation result generated by the Translation API service. To call the get method, specify your project ID and the endpoint. These are code examples for gRPC and cURL using the GDC Translation API. The list method returns a list of the operations that match a specified filter in the request. To call the list method, specify your project ID and the endpoint. These code snippets are examples for gRPC and cURL using the GDC Translation API.

### Video - [Vertex AI Workbench](https://www.cloudskillsboost.google/course_templates/1193/video/522158)

* [YouTube: Vertex AI Workbench](https://www.youtube.com/watch?v=cBYf1jc-5Dk)

SPEAKER: In this lesson, we'll discuss Vertex AI Workbench. The Vertex AI Workbench API is a programmatic interface to the Vertex AI Workbench service. Google builds the control plane APIs on top of Kubernetes using the Kubernetes resource model, KRM. The control plane performs resource management for services such as notebook creation and deletion. To set up an end-to-end notebook-based production environment, you can create JupyterLab notebook instances that come with built-in integrations for data exploration, analysis, and model development. Vertex AI Workbench includes tools for data preparation and exploration, helping you analyze and visualize data before building models. It supports ML frameworks, such as TensorFlow and PyTorch. You can build, train, and deploy models directly from the Workbench interface. Developers leverage the endpoints to programmatically integrate and interact with Vertex AI capabilities. This interaction can include making predictions, deploying and managing custom models, and managing ML resources. Remember that when a Vertex AI service is in the enabled state, its corresponding endpoint information is displayed in the console. This endpoint serves as the entry point for developers to interact with a specific service. You can use interfaces for interacting with endpoints, including REST, gRPC, and custom endpoints, and tools such as cURL and Python. However, note that the Vertex AI Workbench API doesn't support gRPC. Remember that the predefined AIM roles for Vertex AI Workbench include the Workbench Notebooks Admin and Viewer roles, and you can assign them using the console, the gdcloud CLI, or kube control. This example binds the workbench-notebooks-viewer role to a user, which grants them permission to view all notebooks in a project namespace. To begin the solution development, navigate to the Vertex AI Workbench page and click New notebook. Next, you configure settings for the Vertex AI Workbench notebook. Enter information for the instance, such as the notebook name, environment, and cluster. After reviewing the details, click Create. When the new notebook instance has been created, the Status column indicates Active. Click Open JupyterLab to connect to the notebook instance. Now your application teams can connect to the JupyterLab notebook and finish the development of the AI or ML model.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1193/video/522159)

* [YouTube: Module review](https://www.youtube.com/watch?v=glJ6oPDlzZ8)

SPEAKER: Let's review the key points from this module. In this module, we reviewed the Vertex AI offering within the GDC air-gapped platform. GDC offers three RESTful API exposed pre-trained models-- OCR, Speech-to-Text, and Translation. OCR detects and extracts text from handwriting and various file types and supports batch processing. The Speech-to-Text API supports multiple languages for audio transcription and operates with minimal resource consumption. The Translation API supports over 100 languages to English and includes a glossary feature to define domain-specific terminology. These pre-trained model APIs provide a RESTful interface and a Python client library. You follow the same process to configure workload or application access for all the pre-trained APIs. Project workloads are assigned service accounts with the required developer roles, ensuring a consistent and streamlined developer experience throughout the application development process. You also learned about the exploratory data analysis features offered by Vertex AI Workbench, which is based on Jupyter notebooks. You can build and train models using the Workbench interface. Vertex AI Workbench can be orchestrated by control plane APIs, utilizing the Kubernetes Resource Model, KRM API, fine-grained, predefined IAM roles govern access and management operations of the Vertex AI services, providing a secure and fully managed platform for your data analysis workloads.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1193/quizzes/522160)

#### Quiz 1.

> [!important]
> **Which GDC predefined IAM role can grant the AI Platform Admin role to users?**
>
> * [ ] Platform administrator
> * [ ] Organization IAM Admin
> * [ ] Workbench Notebooks Admin
> * [ ] Infrastructure Operations

#### Quiz 2.

> [!important]
> **The Cymbal Federal development team wants to process PDFs and images using the GDC OCR API as part of a new feature request. The team expects the files to be delivered as large tar files. They need to decide where to place files in order to process them. What should the team do?**
>
> * [ ] Store the files in remote network share.
> * [ ] Load and store the files locally in their GDC environment.
> * [ ] Use files from the web.
> * [ ] Host the files in Google Cloud Storage.

#### Quiz 3.

> [!important]
> **Which GDC persona enables Vertex AI services?**
>
> * [ ] Application operator
> * [ ] Infrastructure Operations
> * [ ] Vertex AI administrator
> * [ ] Platform administrator

#### Quiz 4.

> [!important]
> **Which pre-trained API is included on GDC Vertex AI?**
>
> * [ ] Document
> * [ ] Video
> * [ ] Vision
> * [ ] Translation

## Application development and deployment on Google Distributed Cloud (GDC) air-gapped

This module explores practices for deploying your applications on the GDC platform. The module begins with the GDC API interface, then focuses on developing applications on a GDC user cluster. You'll learn practices for developing, deploying, and updating applications. 

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1193/video/522161)

* [YouTube: Module overview](https://www.youtube.com/watch?v=z_EH1Oi-1V0)

SPEAKER: Welcome to Application Development and Deployment on Google Distributed Cloud, GDC, air-gapped. During this module, you will learn to describe the development of applications on GDC. You will also learn about best practices to deploy and update applications. This module includes a brief review of the GDC API interface, then focuses on developing applications on the GDC cluster. We'll discuss some best practices for developing applications and how to deploy and update applications.

### Video - [GDC application development](https://www.cloudskillsboost.google/course_templates/1193/video/522162)

* [YouTube: GDC application development](https://www.youtube.com/watch?v=VqgXkLv-EwA)

SPEAKER: Let's begin by reviewing some key points about the GDC API Interface, then discuss application development. GDC APIs are programmatic interfaces to the GDC Platform Services. There are two types of GDC APIs-- those that are Kubernetes based and those that are not. Many GDC APIs are extensions to the open-source Kubernetes API. Google builds the control-plane APIs on top of Kubernetes, using the Kubernetes Resource Model, or KRM. The API endpoint is the relevant Kubernetes server. Other non-Kubernetes-based GDC APIs, such as the Vertex pre-trained AI APIs, have their own endpoint. In addition to supporting HTTP, some of these APIs may also be accessible by gRPC, the open-source Remote Procedure Call framework. The data plane is the relative API that is used on the resource, such as Vertex AI speech-to-text. To help explain application development on GDC and some best practices, -- let's go back to the example of Cymbal Federal. Cymbal Federal wants to containerize its legacy mission logistics solution in GDC. To do this, Cymbal Federal wants to incorporate good DevOps practices to re-architect the application, which currently runs on Virtual Machines, or VMs, as a microservices-based containerized application. Once the application-development team has finished the initial code development, they will commit the code to the Git repository and build their deployment artifacts. After security checks for the new code have been accomplished, the application operators working in those projects can deploy containerized workloads to the cluster in the test environment for testing. There is no embedded application development in GDC. Google does not currently pre-package developer tools, but there are some developer-related tools that are a part of the platform, such as mesh-routing capabilities and integration of Grafana and Prometheus. Because GDC uses open APIs, GDC services are compatible with open-source services and products. Open software accelerates developer adoption by leveraging existing expertise and tools versus requiring that developers learn new, proprietary systems. You can take the code from a database you implement in GDC or an application you develop, and use that code somewhere else. However, for pure development needs, Google anticipates that most companies will have their own specific set of tooling, some even bespoke, that can be installed into GDC. In our Cymbal Federal example, the application-development team installs their own Python development tools. So let's discuss some best practices for building applications that run in GDC. Consider the reach, scalability, and high availability, and security of your applications. Your application should be responsive and accessible to users wherever they connect from. Your application should be able to handle high-traffic volumes reliably. The application architecture should leverage the capabilities of the underlying GDC platform to scale elastically in response to changes in load. Your application, just like the underlying infrastructure, should implement security best practices. Depending on the use case, you might be required to isolate your user data for security and compliance reasons. These practices can help manage your application's code and environment. Store your application's code in a code repository in a version-control system such as Git. This will enable you to track changes to your source code and set up systems for continuous integration and delivery. Do not store external dependencies, such as JAR files or external packages in your code repository. Instead, depending on your application platform, explicitly declare your dependencies with their versions and install them using a dependency manager. For example, for a Node.js application, you can declare your application dependencies in a package.json file and later install them using the NPM install command. Separate your application's configuration settings from your code. Do not store configuration settings as constants in your source code. Instead, specify configuration settings as environment variables. This enables you to easily modify settings between deployment, test, and production environments. Instead of implementing a monolithic application, consider implementing or refactoring your application as a set of microservices. Many legacy applications you may want to support in GDC or are monolithic applications. In a monolithic application, the code base becomes bloated over time, and it can be difficult to determine where code needs to be changed. Packages or components of the application can have tangled dependencies. The entire application needs to be deployed and tested, even if a change is made to a small part of the code base. This increases the effort and risk when making feature changes and bug fixes. Microservices enable you to structure your application components in relation to your business boundaries. The code base for each service is modular. It is easy to determine where code needs to be changed. Each service can be updated and deployed independently without requiring the customers to change simultaneously, and each service can be scaled independently, depending on load. Make sure to evaluate the costs and benefits of optimizing and converting a monolithic application into one that uses a microservices architecture. In today's modern distributed application architectures, clients that consume distributed services ideally handle errors gracefully. This includes both transient errors, which refers to intermittent errors, such as database-connection errors and service-availability errors. GDC APIs define a set of standard error payloads for error details, which cover the most common needs for API errors, such as quota failure and invalid parameters. Like error codes, developers should use these standard payloads whenever possible. Additional error detail types should only be introduced if they can assist application code to handle the errors. If the error information can only be handled by humans, rely on the error-message content to let developers handle the error manually rather than introducing additional error detail types. Remote operations can have unpredictable response times and can make your application seem slow. Keep the operations in the user thread at a minimum and perform backend operations asynchronously. Implement application components so that they do not store state internally or access a shared state. Accessing a shared state is a common bottleneck for scalability. Design each application component so that it focuses only on compute tasks. This approach lets you use a worker pattern to add or remove additional instances of the component for scalability. Application components should start up quickly to enable efficient scaling and shut down gracefully when they receive a termination signal. The term "Strangler" comes from strangler vines. The vines seed and start growing on the upper branches of fig trees, gradually envelop the tree, and, finally, kill the tree that acted as their host. Consider using the Strangler pattern when re-architecting and migrating large applications. In the early phases of migration, you might replace smaller components of the legacy application with newer application components or services. You can incrementally replace more features of the original application with new services. A Strangler facade can receive requests and direct them to the old application or new services. As your implementation evolves, the legacy application is strangled by the new services and no longer required. This approach minimizes risk by enabling you to learn from each service implementation without affecting business-critical operations. It's important to monitor the status of your application and services to ensure that they are always available and performing optimally. The monitoring data can be used to automatically alert operations teams as soon as a system begins to fail. Operations teams can then diagnose and address the issue promptly. Metrics are quantitative measurements from workloads deployed in your GDC projects that help you observe resources. With metrics, you can perform operations in the observability platform of GDC, such as network monitoring and server monitoring. Dashboards in the system-monitoring platform let you visualize those metrics and get insights about service performance from observability data. The monitoring target custom resource instructs the system-monitoring pipeline to scrape pods of your project. Then, the collected metrics become visible in the Grafana instance of your project, also called the system monitoring instance, to let you evaluate observability data from your applications. To configure the monitoring target custom resource, specify the selected pods of your project namespace for collecting metrics. These pods must expose an HTTP endpoint to deliver metrics in a Prometheus exposition format, for example, the open-metrics format. You can customize settings in the resource, such as the scrapping frequency, the metrics endpoint of the pods, labels, and annotations. Treat your logs as event streams. Logs constitute a continuous stream of events that keep occurring as long as the application is running. Don't manage log files in your application. Instead, write to an event stream, such as stdout, and let the underlying infrastructure collate all events for later analysis and storage. With this approach, you can set up logs-based metrics and trace requests across different services in your application. Operational logs record conditions, changes, and actions as you manage ongoing operations in GDC. These logs help test and debug applications. Finally, in addition to performing functional and performance testing, a best practice is to perform high-availability testing. Such testing will enable you to develop robust disaster-recovery plans and perform disaster-recovery practice exercises regularly. Identify failure scenarios and create disaster recovery plans that identify the people, processes, and tools for disaster recovery. For example, failure scenarios could include connectivity failure, on-premises data-center failure, deployment rollback, and data corruption caused by network or application issues. Initially, you can perform tabletop tests in which teams discuss how they would respond in failure scenarios but do not perform any real actions. Then, simulate failures in your test environment. After you understand the behavior of your application under failure scenarios, address any problems, refine your disaster recovery plan, and test the failure scenarios in your production environment.

### Video - [Deploy and update applications](https://www.cloudskillsboost.google/course_templates/1193/video/522163)

* [YouTube: Deploy and update applications](https://www.youtube.com/watch?v=uH1Y9kQyfOs)

SPEAKER: In this lesson, we'll explore the process of deploying and updating applications in GDC. Let's return to the example of Cymbal Federal. Cymbal Federal wants to containerize its legacy mission logistics solution in GDC. Now that security checks for the new code and testing have been accomplished, the new code has been approved for use in production. The application operators working in those projects want to deploy the new containerized workloads to the cluster in the production environment for use, while causing minimal disruption. To do this, they will apply a new deployment object to the cluster indicating the new image file to be deployed and repeat this process with each new iteration of the application as they adopt a continuous integration, continuous delivery approach to the updates. Continuous integration, CI, and continuous delivery, CD, are two interconnected connected practices that aim to streamline and automate the software development process, leading to faster and more reliable releases. The GDC platform allows these practices seamlessly, as it is a standards based platform. To implement CI/CD correctly, you need to construct a pipeline. A pipeline is a process that takes a version of an application's code base and performs all steps necessary to release it to production. Pipelines can be triggered manually, or they can be triggered automatically when changes are pushed to the code base. Pipelines are generally composed of four separate stages. The first stage is build. In this stage, the code base is checked out to a specific version and artifacts, for example, Docker container images, are built. After the build stage, the next stage is deploy. In this stage, the artifacts are deployed into a test environment that replicates the production environment. After the application has been deployed, it is moved to the test stage. Here, the application is tested in multiple ways to ensure application quality. The most common types of tests are unit, functional, integration, vulnerability, and performance. Finally, if the application passed all of the tests, it can move to the approved stage. In this stage, developers can manually determine whether a pipeline should proceed to the production environment. To enable CI/CD, implement a strong DevOps model with automation. Automation helps you increase release velocity and reliability. With a robust CI/CD pipeline, you can test and roll out changes incrementally instead of making big releases with multiple changes. This approach lets you lower the risk of regressions, debug issues quickly, and roll back to the last stable build, if necessary. In a continuous integration system, developers commit code into a code repository such as Git. A build system, such as Jenkins, automatically detects and commits to the repository, triggers builds, and runs unit tests. The build system produces deployment artifacts for all required runtime environments. In a continuous delivery system, a deployment system, such as Spinnaker, automatically triggers the deployment of builds to test environments. You can automatically execute integration, security, and performance tests, and then deploy successful builds to your production environment. You can set up performance metrics and alerts to monitor the application's performance in production. It's important to consider security throughout the CI/CD process. With a SecDevOps approach, you can automate security checks to confirm that the most recent and secure versions of third-party software and dependencies are used, and to scan code for security vulnerabilities. Here is an example manifest file with its target configuration. Practitioners can update the deployment using kube control. Here is a schematic of a Kubernetes rolling update. Rolling deployments are designed to reduce downtime to the cluster. A rolling deployment replaces pods running the old version of the application with the new version without downtime. So that's roll out. Next, let's discuss how to roll back updates, especially in rolling update and recreate strategies. If you want to roll back an update, such as when your deployment becomes unstable, again, you use the kube control CLI and the process is standard to Kubernetes. The kube control rollout undo command will revert the deployment to its previous revision. You can roll back to a specific version by specifying the revision number. If you are not sure of the changes, inspect the rollout history by using the kube control rollout history command. A deployment object's rollout history is preserved in the system, so you can roll back at any time. By default, the details of 10 previous replica sets are retained for rollback, and you can change this default by specifying a revision history limit under the deployment specification. To decide how to handle updates, stateful set objects use an update strategy defined in the manifest. There are two strategies, OnDelete and RollingUpdate. OnDelete does not delete and recreate pod objects when the object's configuration changes. Instead, manually delete the old pod objects to get the controller to create updated pod objects. RollingUpdate deletes and recreates pod objects when the object's configuration changes. New pod objects must be in running and ready states before their predecessors are deleted. With this strategy, changing the pod specification triggers a rollout. This is the default update strategy for stateful set objects. To update the stateful set, apply a new or updated manifest file. This is useful for making various changes to your stateful set when scaling or for specifying a new version of your application. You can view detailed information regarding the update rollout status in history of a stateful set object. You can also undo a rollout of a stateful set object. To undo a rollout, run this code.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1193/video/522164)

* [YouTube: Module review](https://www.youtube.com/watch?v=U68tq-kUPkg)

SPEAKER: Let's review the key points from this module. In this module, we explored how GDC, based on Kubernetes, is a platform in itself that can be fully orchestrated by its control plane using the KRM API. GDC's inbuilt services, such as the database service and object storage, can be programmatically managed using APIs. We discussed general best practices for developing applications by using standard tooling, such as code repositories, build systems, and deployment processes that employ continuous integration and delivery. In conclusion, the open standards employed by GDC air-gapped enable you to adopt industry best practices without being tied to specific tools. This flexibility lets practitioners continue using existing DevOps platforms, thereby protecting the investments that they have already made.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1193/quizzes/522165)

#### Quiz 1.

> [!important]
> **Cymbal Federal wants to migrate its large legacy Mission Logistics application, which runs on VMs in GDC, to run as microservices on Kubernetes. The team wants to continue to run the application while working on the migration. What approach should they use?**
>
> * [ ] Continue running the application as is on VMs, because reworking the application is too risky.
> * [ ] Continue using the old version of the application until an entirely reworked version is available.
> * [ ] Lift and shift the application as is to run in containers on Kubernetes.
> * [ ] Use the strangler pattern when rearchitecting and migrating the application functionality to containers.

#### Quiz 2.

> [!important]
> **Cymbal Federal's Mission Logistics application team wants to deploy a new version of the containerized application to the Kubernetes service. They typically have multiple instances of the application running in their deployment and they can't incur downtime. What should they do to deploy the new version?**
>
> * [ ] Deploy a new virtual machine image to update the deployment of the application.
> * [ ] Use a kubectl apply referencing the new Deployments pod specification to start a rolling update.
> * [ ] Deploy a new Kubernetes cluster to support the new version of the application.
> * [ ] During a maintenance window, stop the old application pods and start new ones.

## Google Distributed Cloud (GDC) air-gapped software deliveries and upgrades

In this module, you'll learn about the upgrade process in GDC. The module begins by explaining the processes involved with upgrading the software on the GDC platform itself and how upgrades affect end users. The module outlines the tasks of Platform Administrators and Application Operators during upgrades. 

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1193/video/522166)

* [YouTube: Module overview](https://www.youtube.com/watch?v=PDxv__Tbw3s)

SPEAKER: Welcome to Google Distributed Cloud, GDC, air-gapped software deliveries and upgrades. In this module, you'll learn about the upgrade process in GDC and its impact on users. You'll also be able to describe the role of platform administrators and application operators in the upgrade cycle. First, we'll discuss the processes involved with upgrading the software on the GDC platform itself, then the impact of the upgrade process on normal day-to-day operations. This module will outline the tasks and roles of both the platform administrator and the application operator, and their interaction with infrastructure operations during the orchestration of the upgrade. We'll use our Cymbal Federal example to help illustrate these roles and responsibilities.

### Video - [Software updates process](https://www.cloudskillsboost.google/course_templates/1193/video/522167)

* [YouTube: Software updates process](https://www.youtube.com/watch?v=85CZWG4Y_ME)

SPEAKER: Let's begin with the high-level view of package delivery and the upgrade process, then discuss the security practices and how the GDC personas are involved in upgrades. Here is an overview of the orchestrated upgrade process. First, Google packages the new release into a tarball and uploads it into a Google Cloud Storage bucket with access control. Next, the infrastructure operations team for Cymbal Federal, the operating entity that has access to this Google Cloud Storage bucket, downloads the tarball from an internet connected machine, checks for malware, and validates the hashes on the tarball to ensure that the package was not tampered with in transit. Using a secure physical device, the tarball file is transferred into the end-user domain where the deployment is hosted. The tarball file is expanded into an artifact management system backed by Harbor Container Registry. After the artifacts are in the hardware registry with the relevant metadata and context, the upgrade orchestrator, or controller, uses those documents to roll out the updates while causing minimal disruption. The platform administrator configures maintenance windows for updates to Kubernetes clusters and operating systems. Next, let's examine the software supply chain security practices that maintain the integrity of the software used within the GDC platform upgrade process. Software supply chain security, or S3C checks, are broken down into three broad categories. Source integrity checks verify the integrity and provenance or origin of the code, preventing the introduction of malicious code into the pipeline. Build integrity checks ensure that neither the build pipeline nor the package delivery process is tampered with. Runtime or dynamic checks ensure that production systems, which are the actual deployments, run what they're meant to run and that they don't have any malicious or bad code. Now, let's focus on how the GDC personas are involved in the upgrade process. When a new release is available, it must first be applied at the infrastructure operations level before the update can be applied to the organization level components. Only then will you be able to upgrade your user clusters and service instances. As you plan for upgrades, keep these points in mind. Updates occur in high-availability mode to ensure that services remain accessible and operational during the update process, minimizing downtime and disruptions. GDC will never force upgrade end-user clusters. Instead, the control over when to upgrade is left to customers. This approach lets you manage upgrades based on your specific requirements, schedules, and compatibility assessments. GDC respects workload stability. By not forcing upgrades, GDC respects the stability and continuity of the workloads running in end-user clusters. Customers can carefully plan and test upgrades to ensure that they don't impact critical business processes. Communication and planning are key. Effective communication channels and planning mechanisms become crucial. Customers need to stay informed about available updates, security patches, and enhancements to make informed decisions about when to apply upgrades. Security updates should be applied when available. While customers have control over upgrades, it's important to promptly apply critical security updates to ensure the overall security posture of the environment. GDC encourages you to test and validate upgrades. This ensures that the upgraded environment behaves as expected and doesn't introduce unforeseen issues. Let's examine the types of software upgrades, extent of changes, and frequency of releases. The versioning structure is major.minor.patch. GDCH.RC, where RC is the release candidate. An example of version number would be 2.1.0-gdch.1. Here, the major version is 2. The minor version is 1, the patch version is 0, and the release candidate is 1. This versioning format lets you clearly identify major, minor, and patch updates, and candidates within the context of the GDC platform. For the release cadence, feature or minor version releases occur three times a year. Minor upgrades enhance function or make changes from the previous revision, which is a package upgrade. For example, a minor release may fix bugs. Planned patch version releases are scheduled monthly. Patch releases address specific issues or vulnerabilities. You can configure the default patch maintenance window to start patch upgrades according to a defined schedule. Out-of-band reactive patch releases and hotfixes are on demand. On-demand patch releases are reactive patch releases and hotfixes. These types of releases address sev zero vulnerabilities or critical bug fixes that impact service-level agreements, SLAs. By default, there is one maintenance window for minor upgrades and one maintenance window for patch upgrades. Minor upgrades require at least 12 hours in a 32-day rolling window to be allocated, while the patch upgrades require at least 48 hours in a 32-day rolling window with one or more time blocks. You should allocate at least the minimum duration recommended for each of the upgrade types. These recommendations are provided to ensure sufficient time for the upgrade process, and to account for any unexpected delays or lengthy procedures. Google advises allocating additional time beyond the specified minimums to enhance the robustness of the upgrade process. Although the console shows a minimum four-hour window specification for time block duration, Google recommends that you allocate at least six hours to each time block to account for any long patch processes. In preparation for the upgrade process, you need to connect to the internet to download new GDC distributions from a Google Cloud environment. Since GDC is air-gapped, you'll need an internet connected device and the URLs are provided to access outside of your environment. All CLI commands for GDC use the gdcloud or kube control CLI and require a Linux environment. Let's examine the platform upgrade process. Because of the rolling updates and triple redundancy, there is no downtime during the upgrade. The platform administrator, in this example, Charles, specifies a maintenance window. Then, the upgrade controller will begin the upgrade process after the package is downloaded, copied to the air-gapped environment, and artifacts are extracted and pushed to the container registry. The upgrade starts with a non-leader server. If successful, it switches the leader to the upgraded server and then upgrades the remaining two servers. The HPE firmware and operating system updates and then continues to the next server. After upgrading the remaining org servers, it will proceed to upgrade the org admin clusters. The add-ons, the managed services control plane, and other components will also be upgraded. After the upgrade for the org admin servers is complete, the controller then proceeds to upgrade the system cluster control plane.

### Video - [Tenant org upgrade](https://www.cloudskillsboost.google/course_templates/1193/video/522168)

* [YouTube: Tenant org upgrade](https://www.youtube.com/watch?v=P1gWZ4mlBRk)

SPEAKER: Now, let's go through a tenant org upgrade, beginning with configuring maintenance windows. To begin configuring maintenance windows using the GDC console, the platform administrator navigates to the Maintenance tab and clicks Edit. You will need the organization upgrade admin role to edit the maintenance window and the Organization Upgrade Viewer to view scheduled upgrades. After clicking Edit from the Configure Maintenance Windows page, the Edit Maintenance Windows page opens, where you can reconfigure the patch version and minor version time windows. For patch version, the platform administrator specifies or edits the start time, length, and day of the week. For a minor version, the platform administrator specifies the start time, length, recurrence, and day. Note that the Recurrence field for a minor version indicates the first, second, third, fourth, or last instance of the specified day. When a month only has four instances of a certain day, the last and the fourth are equivalent. Click Save to apply your changes. If you're saved changes affect the recurrence, for example, if you implemented a change to the day of the week or to the month, you will receive a pop-up dialog. To confirm your changes, click Continue. To modify the patch Upgrade Maintenance Window, you might use kube control instead of the console. You also need the organization upgrade admin and Organization Upgrade Viewer pre-defined roles to use these commands. First, log into the kube control CLI and verify your access to the org admin cluster before proceeding. The three fields in the maintenance window that are used to configure the time window for the [? tenant-orgupgrade ?] are the RRule start time and end time. This example configures the maintenance windows for patch upgrade. The commands for minor upgrades are similar. From the console, you can view the updated scheduled upgrades with the skip option for each, based on the configuration changes. There is a Skip button next to each maintenance window with a pending status. Use this to skip a schedule pending upgrade.

### Video - [User clusters upgrades](https://www.cloudskillsboost.google/course_templates/1193/video/522169)

* [YouTube: User clusters upgrades](https://www.youtube.com/watch?v=KKug-4Vo9og)

SPEAKER: Next, let's discuss user clusters and automated upgrades. In this step, Kubernetes components for organization-only services automatically upgrade. You need the user cluster admin role to proceed with automated upgrades. Log into the GDC console and select Clusters in the navigation menu. The user clusters that have available upgrades are indicated by the words "Upgrade available" under the Notifications column. Next, let's examine the details of the upgrade. Clicking Upgrade Available opens the Upgrade Details page. You can view the details of the available GDC user cluster version and the corresponding available Kubernetes version. Click the Edit icon located near the GDC user cluster version name to choose the target version of the VM for the upgrade. This is located near the GDC user cluster version name. After selecting the target version for the upgrade, click Save to proceed with the upgrade. Click Upgrade to start the user cluster upgrade. And when the upgrade starts, refresh the Upgrade Details page to check the progress of the upgrade. Return to the Clusters page and verify that the user cluster status is upgrading. Once the upgrade has completed, the status for the cluster should be ready, indicating that the GDC version is the updated version.

### Video - [Manual OS upgrades](https://www.cloudskillsboost.google/course_templates/1193/video/522170)

* [YouTube: Manual OS upgrades](https://www.youtube.com/watch?v=9f15c-nr_XE)

SPEAKER: Now that you've learned about user cluster upgrades, let's discuss Manual OS upgrades. Upgrades of the VM nodes focus on the infrastructure, specifically targeting the OS to address critical security vulnerabilities for the cluster nodes that the user cluster operates. The OS upgrade operates in high-availability, HA, mode. It doesn't incur downtime because this is a non-interactive process without performance glitches. Each node processed in this upgrade incurs about 20 minutes of downtime. And there is no disruption to cluster function. The OS upgrade requires a post-upgrade check for the node to go back to the node pool. The node pools that have VM nodes to be upgraded are the user cluster worker and the user cluster control plane node pools. In order to perform the Manual OS upgrade, the org admin cluster is queried to retrieve your cluster's metadata, the name of the upgrade, and the target software version that is available. This software version is referenced within a node upgrade custom resource that will actually perform the upgrade on each node one by one as a best practice to mitigate interruptions for workloads running in the node pool. Specially, the prep work to gather the information for the node upgrade resource is authenticate to the org admin cluster. Determine the name of the software upgrade. Determine the software version of the upgrade. And confirm the node pools that will be upgraded. It is a best practice to upgrade each node one at a time. To initiate the upgrade process, create a NodeUpgrade custom resource for each node pool in the instance and its corresponding admin cluster using the NodeUpgrade custom resource template. This is sample code for creating a node upgrade custom resource template. In the first step, use the template to create the NodeUpgrade objects one at a time. In step two, wait for the previously created NodeUpgrade object to finish being upgraded before you create the next one. Step three of the upgrade is to save the NodeUpgrade custom resource in a YAML file. In step four, you create your custom resource in the corresponding admin cluster. Next, let's review the NodeUpgrade custom resource template attributes in detail. These are the relevant key value attributes for the NodeUpgrade custom resource used to configure the upgrade. The name attribute is the name that you give this NodeUpgrade process. The target release name is your workload cluster's user cluster metadata. The node type is virtual. The node pool claim reference indicates the node pool that you want to upgrade. And, finally, the version is the software target release version. Here is a completed NodeUpgrade resource manifest file. This will upgrade one node within the worker node pool only. To upgrade other nodes, you create an additional NodeUpgrade resource and apply the configuration to your workload cluster.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1193/video/522171)

* [YouTube: Module review](https://www.youtube.com/watch?v=E-94o-gltOU)

SPEAKER: Let's review the key points from this module. In this module, we presented an overview of the GDC upgrade process. Upgrades are primarily managed by the infrastructure operations team. However, there is coordination with Google and across GDC personas to ensure minimal disruption to applications and users. During the software distribution process, Google provides signed binary updates in a secure location. Infrastructure operations downloads, validates, and moves these binaries into the GDC environment for distribution and deployment. This includes updates to GDC software, the node level OS, antivirus and malware definitions, storage, and network appliances, and other binaries. In this module, we also reviewed specific GDC upgrade processes. First, Tenant Org upgrades involve updating the software and configuration specific to a tenant organizations environment within the GDC air-gapped platform. Platform administrators configure maintenance windows to govern patch and minor upgrades. Second, user cluster upgrades are automated upgrades that are designed to deliver zero downtime for user Kubernetes clusters within a tenant organization. And third, manual OS upgrades are updates to VM nodes and VMs to address critical security vulnerabilities. The GDC air-gapped upgrade process is designed to be secure and controlled, with the infrastructure operator playing a central role in managing updates. Platform administrators and application operators work together to ensure smooth upgrades that minimize downstream application and user impact.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1193/quizzes/522172)

#### Quiz 1.

> [!important]
> **Which persona is primarily responsible for configuring the maintenance windows to schedule organizational upgrades?**
>
> * [ ] Google point of contact (POC)
> * [ ] Platform administrator
> * [ ] Infrastructure Operations
> * [ ] Application operator

#### Quiz 2.

> [!important]
> **When performing software and patch upgrades for GDC, what is a common consideration in the upgrade process?**
>
> * [ ] Initiating upgrades without any prior checks.
> * [ ] Limiting the need for communication with the Google point of contact (POC).
> * [ ] Verifying the integrity of the source code and dependencies.
> * [ ] Disabling all security features during the upgrade.

#### Quiz 3.

> [!important]
> **What persona has primary responsibility for ensuring the readiness of the underlying infrastructure during a GDC distribution update?**
>
> * [ ] Application operator
> * [ ] The Google point of contact (POC)
> * [ ] Platform administrator
> * [ ] Infrastructure Operations

#### Quiz 4.

> [!important]
> **The Cymbal Federal team wants to download the GDC distribution update that includes the fix for a security breach that was discovered recently. What should happen next?**
>
> * [ ] Infrastructure Operations uses an internet-connected computer to access and download the file.
> * [ ] The automated upgrade process downloads the tarball.
> * [ ] The Google POC will download the tarball for Cymbal Federal.
> * [ ] The platform administrator can download the tarball directly into Cymbal Federal's GDC environment.

## Logging, observability, and troubleshooting

This module begins with an overview of the GDC observability platform. You'll discover GDC's integration with Grafana and discuss metrics that help you track performance. You'll also learn how to set alerts and use logging tools. Finally, you'll review some troubleshooting tips and tricks. 

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1193/video/522173)

* [YouTube: Module overview](https://www.youtube.com/watch?v=c8eS7QyvxLo)

SPEAKER: Welcome to "Logging, Observability, and Troubleshooting." In this module, you'll learn to describe monitoring tools available in Google Distributed Cloud, GDC, air-gapped, and how they are used to maintain system and application performance. You'll also learn about the steps to submit support requests. We'll start with an overview of the GDC observability platform and its integration with Grafana and metrics that help you track performance. Then we'll focus on monitoring and alerts before discussing logging tools. After some general troubleshooting tips and tricks, we'll go over the procedures that the practitioners and symbol federal would take to submit support requests, in an example, GDC air-gapped deployment.

### Video - [Observability and metrics](https://www.cloudskillsboost.google/course_templates/1193/video/522174)

* [YouTube: Observability and metrics](https://www.youtube.com/watch?v=vFJR6Wn9bwE)

SPEAKER: Let's begin by exploring the observability platform in GDC air-gapped and how metrics are collected. The observability platform receives telemetry data, such as operational logs, audit logs, and metrics from the rest of GDC, including Kubernetes objects, infrastructure, first-party apps, IAM components, and user workloads. From day zero, there are logs and metrics available, as well as out-of-the-box alerts for monitoring. You gain visibility into the performance, availability, retention, and health of the applications and infrastructure through the use of dashboards, operational logs, and the metrics, alerts, and notifications. You can use operational logs and metrics to troubleshoot issues associated with the deployment of applications in GDC. And audit logs let you track activity to meet security and compliance requirements. As part of ongoing, or day two, operations, you can define a baseline for system health and customized metrics to align with service-level objectives. For continued system support, or day 50, there are built-in observability tools available for monitoring, troubleshooting, and auditing. Observability tools on the GDC platform are based on an open-source stack. This means they can be integrated with other tools that you already use. In the stack, Prometheus collects metrics from the GDC deployment. Cortex is part of the stack that is used for storing metrics and managing system alerts. Fluentbit is used as a collector for logging information. Loki is the tool in the observability stack that provides storage and management of logging information. Grafana is an interface used to explore system logs and monitor system metrics through available dashboards and alerts. Each organization has a single instance of the following, logically partitioned by project namespace, Cortex with alert manager, a Loki instance for operational logs, and a Loki instance for audit logs. Each project within these organizations has a single instance of Grafana. Because of this, alerts are isolated between projects, just like metrics and logs. Cortex is a storage mechanism for observability data and does not have a user interface that you expose. Let's explain how each of these components fits into the overall functionality. Prometheus is the primary collection point for metrics from the infrastructure, the system, and the control plane. Optionally, it collects metrics exposed by your applications. All of these metrics are stored in Cortex and are available for visualization in dashboards within Grafana. In addition to providing storage for metrics, the embedded alerts management system in Cortex lets users set up custom metrics or log-based alerts. Alerts are customizable and can be configured for different notification channels, including email, Slack, SMS, and others. As a user interface for monitoring and troubleshooting, Grafana provides role-based access control for viewing dashboards, issuing queries to explore metrics information, and logging data. The GDC observability stack creates and uses two special projects per each organization. The Infra-obs provides infrastructure operations access to org infrastructure metrics and logs in a special Grafana instance. And the Platform-obs provides platform administrator access to org scope metrics and logs in a special Grafana instance. Observability components get deployed to either OBS-system or the project's shadow namespaces, for example, infra-obs-bs-system, platform-obs-obs-system, or alice-obs-system. To access the Grafana interface in the GDC console for the organization admin cluster, the platform administrator browses to the GDC Grafana web page link and selects it from the user interface. The application operator can view project-level metrics and project-specific logs in Grafana by browsing and logging in as the application operator. Logging in to the Grafana homepage and using the Need Help link provides additional detailed instructions about accessing the available log and metrics data. Let's explore the Grafana interface and some of the dashboard screens that a platform administrator uses to explore and monitor data. In a project context, the platform administrator can navigate to monitoring through the operations section of the global navigation in the GDC console. On this web page, you can access integrated resource metrics and external Grafana dashboards that provide critical observability data. For example, by selecting the View All in Grafana option, the platform administrator can view metrics pertaining to the Kubernetes API and others. Within the external dashboards embedded into Grafana, a platform administrator can view a variety of system health metrics, such as Kubernetes system availability, which in this example is at 99.996%. The tabs above the dashboard display let you select the type of metrics to view. You can also query these metrics. Grafana has a query interface to help build PromQL inside the UI. Use the dropdown menus to select the metrics you want to query. Below the dropdown list is the raw query PromQL that Grafana generated for the request. Every unique set of metric name and label value pairs produces a new metrics time series in the database. This lets multiple services produce the same metric. Labels create unique time series on that metric name for each service. PromQL lets you view one or all of those time series in a single query. Metrics data flows through the collection process by Prometheus to Cortex, which is then accessed with Grafana and the GDC console. GDC observability collects metrics from user workloads, system components running by default in each cluster, and platform components, such as firewalls and switches. Metrics data includes user-defined metrics and system-performance metrics. All of these are stored in the Cortex database. To send metrics to the observability platform, you expose a Prometheus compatible endpoint. Most components have Prometheus exporters, given the popularity of the Prometheus ecosystem. The targets are basically endpoint URLs, like slash metrics, where metrics data is made available through an exporter for Prometheus to scrape. Metrics and alerts data are isolated between projects and system namespaces. To access metrics data, users can query the Cortex database from the Grafana user interface or the GDC console UI. Alerts can also be pushed from the Cortex Alert Manager based on collected metrics. GDC exposes the same Cortex APIs that Grafana uses to query metrics. This lets you query for metrics and alerts through the command line. Cortex is configured to only expose the slash Cortex slash Prometheus and slash Cortex slash Alert Manager endpoints, so you cannot query all of the Cortex API. For a particular project, you can find the endpoints by browsing to the GDC project interface for Cortex. To access the endpoints, your administrator must grant you the project Cortex Prometheus Viewer and Project Cortex Alert Manager Viewer roles for the project. Open source documentation can help you determine how to use the endpoints.

### Video - [Monitoring tools and alerts](https://www.cloudskillsboost.google/course_templates/1193/video/522175)

* [YouTube: Monitoring tools and alerts](https://www.youtube.com/watch?v=Ze_gMLXrBMc)

SPEAKER: In any deployment, it is important to know what is happening in the system and whether the system is operating as it should. Monitoring tools give users information about the status of the system. Let's examine the monitoring tools that are available in GDC. This slide provides another big picture understanding of the observability components in GDC and the pipelines for each. Metrics are collected through Prometheus and stored in Cortex. Every project has one Prometheus per Kubernetes cluster. This Prometheus only scrapes metrics from within its own project. Every org Kubernetes cluster has one system Prometheus. This collects infrastructure operations level and platform administrator level metrics from services outside of projects, such as Kubernetes API, Node Exporter, and hardware metrics. Every organization has one Cortex instance for the entire organization. Root admin has one system Prometheus and one Cortex instance. Next, let's examine the monitoring components in each cluster in more detail. The diagram on this slide provides more detail about the monitoring components in each cluster. Note that for root admin, data flow is the same. The difference is that the only flow available is the system flow on the admin cluster. For system monitoring, every cluster has a single system Prometheus and a system Cortex tenant. These determine what to scrape by looking at annotations on pods in the cluster or clusters. For project monitoring, every project has a per project Prometheus and a per project Cortex tenant on every cluster. These determine what to scrape by using monitoring targets deployed in the project namespaces. Prometheus uses a pull-based model to scrape metrics from GDC endpoints. Metrics producers must expose a REST API endpoint, which can be queried over HTTP. This endpoint must provide a metrics file, which follows the open metrics format. Prometheus will periodically pull this file from each of the endpoints it targets, and then forward that data through remote write to Cortex for long-term storage. Let's consider the relationship between Prometheus and cortex. Cortex is a multi-tenant metrics aggregation server. Cortex tenants are synonymous with GDC projects. Requests to write-read data to cortex must include a tenant that uses the x-scope-orgID header. For example, infrastructure operations metrics must be sent with the header "x-scope-orgIDinfraOBS." The cortex-tenant deployment is a proxy that sits between Prometheus and Cortex. Cortex-tenant rewrites the requests from Prometheus to include the x-scope-orgID header. The header value defaults to the project the metric emitted from. The value can be overwritten by setting the _gdch_projectlabel on the metric. Currently, there are some technical limitations in the GDC environment. By default, right after bootstrapping, all of the observability components, such as Loki and Cortex, use persistent volumes to store data, which are very limited in size at 20 gigabytes. Due to these limitations, infrastructure operations must configure Cortex for bucket storage to enable alerts. If it is discovered that Loki and Cortex do not have access to StorageGrid, the platform administrator must open a support ticket to resolve the issue. This is a function that will be automated in the future, at which time StorageGrid will be operational out-of-the-box. In the GDC observability stack, the monitoring stack is responsible for metrics collection and alerting in the case of outages. But what about an outage of the monitoring stack itself? What happens if the monitoring stack has an issue? Meta monitoring keeps watch on the stack itself and provides metadata about the status of the stack components. Meta monitoring is a parallel monitoring stack that watches the primary stack. It collects metrics about the health of the primary stack and will generate an alert if any critical components go down. Synthetic monitoring is the monitoring of servers with a focus on areas, such as disk space, CPU usage, memory usage, and load averages. It provides a way of performing health checks on third-party services that do not expose metrics. The diagram on this slide gives an overview of the components and the data flow of how this type of monitoring is implemented in GDC. The synthetic monitoring stack periodically checks the public interface for an expected response and generates a metric. That metric can be used to produce alerts, if desired. No one can spend eight hours a day looking at dashboards to keep track of system functionality. So the GDC observability stack incorporates the functionality of the Cortex Alert Manager. In GDC, you can configure alerts based on metrics or logging events to send out notifications through a variety of different channels, such as email and SMS. To view configured alerts, the platform administrator navigates to the Alerting page under the Operations Navigational menu, and clicks on the Alerting menu to gain detailed information about individual configured alerts. Part of working with alerts includes managing the response to alerts as they are triggered. In addition to viewing detailed information about alerts, to reduce noise from recurring alerts, the platform administrator has the option to silence the alerts from the Overflow menu. Once the platform administrator selects Silence Alert, they configure the duration to silence the alert and enter information to justify the silencing of the alert, then click Confirm to proceed. Returning to the Alerting dashboard, the platform administrator can see that the alert has been successfully silenced. When silenced, the alert will appear under the Alerts Silenced section of the table. If the platform administrator wants to unsilence an alert, they can do so by selecting Remove Silence from the same Overflow menu on the right side of the table.

### Video - [Logging tools](https://www.cloudskillsboost.google/course_templates/1193/video/522176)

* [YouTube: Logging tools](https://www.youtube.com/watch?v=Qh8R-q8g12U)

SPEAKER: Keeping track of system events is an important part of troubleshooting and managing system security. You need to track who did what and when for the purpose of making sure the system stays secure. In this lesson, we'll go over the logging information that is available in GDC. The GDC observability suite collects operational logs and audit logs. Operational logs are Kubernetes container logs from containerized workloads that are collected automatically from the standard output, stdout, and standard error, stderr, interfaces of the containerized application. These types of logs are stored in Loki and available for visualization in Grafana. You can search for and filter available logging data using log query language or LogQL, and you can configure dashboards to display metrics data that is derived from logging information collected on the system. Logging information can also be pulled from GDC by other tools, such as SIEM systems. Audit logging is collected from GDC components and services automatically and stored in a Loki instance for long-term retention for one or more years as desired. These logs are written to a write-once read-many storage media. Audit logs can be explored and visualized through the user interface of Grafana, and can also be exported to existing tools and SIEM systems using either a push or a pull model for export. Here's how logs are collected in GDC observability. There are multiple log sources, including user workloads, operable components, and platform components. For user workloads, GDC observability provides operational logs only. For operable and platform components, GDC provides both operational and audit logs. These logs are stored in two separate Loki instances, one for audit logs and one for operational logs. You can view and query logs from the Grafana UI. Logs can also be exported to external tools, such as Splunk. To explore logging information, the platform administrator can navigate to the Logging page under Operations in the navigation window and view a collection of logging dashboards that are available in Grafana. Application Operators can also access logging information within Grafana. But what the Application Operator can view depends on their IAM permissions. Depending on permissions granted, an Application Operator can navigate to the Logging page and view a collection of logging dashboards related to their specific project. To see who did what and when, the platform administrator navigates to the Audit Logging page, which tracks and records all user and role-based activity on the GDC platform. This is an example of one of the screens for the logging interface. Notice that you can issue queries from the screen and use filtering to explore logging data. Here, the Platform Administrator can filter on labels assigned to the particular logs of interest. In this case, namespace has been selected from the dropdown menu under Label filters. You can also filter on a particular type of service that you want to access the logs from. In this particular case, API server is selected. After using the dropdown menus to filter for logs from the API server, you can view the raw query and query syntax. Grafana also displays a histogram of available logs matching the query parameters. LogQL is Grafana Loki's PromQL-inspired query language. Queries act as if they are a distributed grep to aggregate log sources. LogQL uses labels and operators for filtering. As we mentioned previously, there are currently still some technical limitations in the GDC environment that affect logging, as well as monitoring. Due to these limitations, the GDC observability stack cannot use StorageGrid out of the box. Therefore, if it is discovered that the Loki instances do not have access to StorageGrid, you must submit a support ticket. Infrastructure operations will perform steps manually to attach the instances to StorageGrid. This is a function that will be automated in the future, at which time StorageGrid will be operational out of the box. By default, after bootstrapping, the Observability platform is configured to collect out-of-the-box operational and audit logs. For operational logs, GDC observability collects all logs from GDC operable components. Out-of-the-box operational logs are based on a pod's namespaces. Most are visible to infrastructure operations. In the root admin cluster, you collect everything by default. Out-of-the-box audit logs are not namespace-based. Instead, they are service-based, such as K8's API server, Grafana, Istio, Harbor, and Native Linux logs. The visibility varies by service. The Observability platform is configured to automatically pull all audit logs from each node's file system. It also provides a Kubernetes API to collect additional operational and audit logs, such as user project logs and user workload logs. To collect these logs, a custom CRM must be deployed into the cluster. Now that we've discussed operational logs, let's focus on audit logs. Generally, audit logs tell you who did what, where, and when in your organization. These logs provide strict adherence with compliance requirements for storage, including the use of a write-once, read-many storage bucket and a retention time of, at minimum, one year for log data, during which no data can be deleted. Also, strict compliance requirements are met with respect to the collection of log data. No log data will be lost in the collection process. Logging data is generated by multiple operable components-- for example, the K8's API server, Istio service mesh, and Harbor, as previously mentioned. Because of this, log collection operations can yield a lot of data to be stored. For example, the K8's API server generates in excess of 300,000 records per minute across the organization while the system is running idle. This can easily consume about 20 to 25 gigabytes of disk space daily. Currently, GDC log collection supports both pull and push collection models. Log data could also be pushed by components from outside of the organization, such as firewalls, network switches, and HSM. For audit logs, there is also a fanout feature. In GDC, some components, like firewalls, network switches, StorageGrid, and others, generate audit logs for multiple organizations. Unfortunately, the observability stack cannot provide access for each organization. To overcome this, some external components push information to a root admin cluster externally. Then, inside the root admin cluster, there is a special configuration of Fluent Bit that processes the external logs and then splits them between organizations. To do this, each audit log must contain a special field that identifies the target organization to receive the log entry. Fluent Bit then analyzes the embedded field and distributes the log messages between organizations accordingly.

### Video - [Troubleshooting tips and tricks](https://www.cloudskillsboost.google/course_templates/1193/video/522177)

* [YouTube: Troubleshooting tips and tricks](https://www.youtube.com/watch?v=S4KwaiBStzY)

SPEAKER: In this lesson, we'll discuss some troubleshooting information and tips. You may initially need to troubleshoot authentication to gdcloud if issues arise while attempting to log into the CLI. gdcloud requires a proper certificate trust to be established on the workstation. If the proper certificates are not installed on your workstation, the gdcloud command will not connect, due to untrusted certificate use. This error indicates the SSL certificate for the console.org-1 URL was issued by an untrusted certificate authority, one that is unknown to the HTTP client within the gdcloud tooling. For a certificate authority to be trusted, its root certificate must be included in the list of trusted root certificates maintained by web browser vendors and operating system providers. A trusted certificate authority verifies the identity of a website and helps users trust that they are communicating with a legitimate website. To check the settings, gdcloud must be configured to connect to the proper organization and project. Use gdcloud init to configure and gdcloud config list to view the existing configuration. You can have multiple gdcloud configurations. This can be done by using the gdcloud CLI to create a named configuration. The list of configurations can be found using the gdcloud config configurations list command. You can set the active configuration with the command gdcloud config configurations activate your name with the default configuration named default. To verify that the organization page is reachable, browse to the organization page in a web browser on your Linux-based workstation and try authenticating with the same credentials you use for gdcloud. To ensure that you are using the correct version of gdcloud, download the current version from the web page or run the command gdcloud components update. To troubleshoot issues that might occur while you provision a new VM or a user cluster in GDC, refer to the troubleshooting section of the documentation. Troubleshooting guidance exists in GDC documentation for known issues, networking, logging and monitoring, database service issues, provisioning, and Vertex AI. There is also a listing of error codes invoking the GDC KRM APIs. This is an example of available GDC documentation used to assist in troubleshooting issues that might occur while provisioning a new VM in GDC. In this case, if you are unable to create a disk, you must run all commands against the default user cluster. This is another example of what to check if you are unable to create a disk. If a persistent volume claim, or PVC, is in a pending state, you can check for an error message, configure the driver to resolve the error, and verify with a test PVC, and delete the test PVC after verification. And, finally, this is another example of what to check if you are unable to create a disk.

### Video - [Steps to submit support requests](https://www.cloudskillsboost.google/course_templates/1193/video/522178)

* [YouTube: Steps to submit support requests](https://www.youtube.com/watch?v=6aDQSA-w0S0)

SPEAKER: Sometimes, after troubleshooting an issue, you will need additional support. In this lesson, let's explain how you submit support requests in GDC. If you require assistance for any hardware or software issues with your GDC instance, you need to submit a support request in the ServiceNow ticketing system. Navigate to the ServiceNow portal, enter your user ID, and sign in on the ServiceNow login page with the username and password associated with your identity provider. Then, on the ServiceNow Customer Service page, click Get Help. Fill out the Create New Case Template providing details about your issue.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1193/video/522179)

* [YouTube: Module review](https://www.youtube.com/watch?v=ABXUZO-QxTM)

SPEAKER: Let's review the key points from this module. In this module, we reviewed the GDC open-source observability stack, which provides workload and audit logging, monitoring, and alerting capabilities. Prometheus collects metrics and stores time-series data. Cortex is a headless, multi-tenant system for collecting, storing, and querying Prometheus metrics in real time. It also includes alert manager, which handles alerts and notification channels. FluentBit serves as an efficient data collector and forwarder for logs and metrics. Loki is a multi-tenant log aggregation system designed to collect, store, and query infrastructure and application logs efficiently. And Grafana provides an interface to explore system logs and monitor metrics through dashboards and alerts. This module also reviewed the logical logging architecture, where each organization has a single, logically separated instance of Cortex with Alertmanager. There are separate Loki instances for workload logs and audit logs, and each project has its own Grafana instance, ensuring isolation for alerts and metrics. The monitoring and logging data flow across the organization involves Prometheus collecting logs and metrics across both administrative and user workload Kubernetes clusters. This module also covered basic troubleshooting procedures for standard operations and the gdcloud command line interface. If additional support is required, the GDC platform includes a ServiceNow ticketing system. Security is a top priority within the GDC platform, with predefined and granular IAM roles governing all persona-based access to dashboards, with views scoped at both the organization and project levels. With the comprehensive observability feature set, which includes built-in monitoring, logging, and alert management, the GDC empowers practitioners to effectively operate application workloads.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1193/quizzes/522180)

#### Quiz 1.

> [!important]
> **What element in the GDC Observability stack provides a user interface to explore system logs and monitor system metrics through available dashboards and alerts?**
>
> * [ ] Cortex
> * [ ] Fluent Bit
> * [ ] Prometheus
> * [ ] Grafana

#### Quiz 2.

> [!important]
> **You are a Platform Administrator at Cymbal Federal, and you need assistance for a software issue with your GDC instance. What interface should you use to submit a support request?**
>
> * [ ] ServiceNow template
> * [ ] Prometheus agent
> * [ ] GDC API call
> * [ ] GDC console menu

## Billing and cost monitoring

In this module, you'll learn about monitoring consumption and cost in GDC. You'll find out about available billing and cost monitoring information and how to determine the cost associated with deployed resources on your projects. You'll learn about billing reports, dashboards, the cost calculator, and billing alerts. 

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1193/video/522181)

* [YouTube: Module overview](https://www.youtube.com/watch?v=U7JLrzwsJNQ)

SPEAKER: Welcome to billing and cost monitoring. In this module, you'll learn about monitoring usage and consumption in GDC air-gapped. In this lesson, we'll review the billing and cost monitoring information that is available in GDC and consider how Cymbal Federal platform administrators and application operators may use this information to determine the cost associated with deployed resources in their projects. Let's begin by taking a look at billing reports, dashboards, the cost calculator, and how to create billing alerts.

### Video - [Billing reports](https://www.cloudskillsboost.google/course_templates/1193/video/522182)

* [YouTube: Billing reports](https://www.youtube.com/watch?v=I2S93JCEf2g)

SPEAKER: In this lesson, we'll focus on billing reports. To give us some context, let's return to our example of Cymbal Federal. Remember that Cymbal Federal has adopted GDC for several of its key applications. Cymbal Federal teams are charged with monitoring and controlling costs in the GDC platform. They focus on tracking costs for application development and DevOps. The platform administrators and application operators are required to track resource utilization and associated costs for the purpose of analyzing the cost of the GDC deployment against internal-operations costs. The teams use the GDC billing and cost-tracking tools to access and analyze invoices, set up budgets and billing alerts, and perform cost projections. The applications, development, and DevOps teams are also charged with monitoring consumption for their applications during high- and low-usage periods. They use available resource-monitoring dashboards to query metrics and view resource allocation. Teams are also required to generate quarterly reports detailing resource usage and associated costs to justify deployment budgets. GDC provides a view of usage and costs for running your GDC installation using Grafana dashboards. The Billing Reports page in the GDC console lets you view costs in your organization and project. It provides links through Grafana to invoices, cost projection, services usage, and storage and compute-allocation dashboards. The GDC console defaults the Billing Reports page to the organization level. To access a dashboard, sign in to the GDC console and select Billing, then Reports in the navigation menu. On the Reports page, select the service you want to view. For example, to view the metric consumption of the managed service Vertex AI, click either GPU or vCPU under Vertex AI Workbench. It's important to be aware of your actual resource consumption compared against your cost projections and budget. GDC lets you create billing alerts to help monitor costs. You can configure billing alerts to notify you when your resource spend has reached a certain threshold. You create alert rules based on invoice metrics and your spend thresholds using the monitoring rule resource in the monitoring group for the service endpoint in the observability API Overview Article. You enable billing notifications and designate notification channels for the alerts through the console, as you would for other system alerts. To create an alert rule, create a YAML file, add in the MonitoringRule resource, and apply the contents of the YAML file to set the alert rule. Dashboards provide visualizations of organizational and project-usage data, workload data, storage allocation, backup counts, and AI metrics. Through these dashboards, you can filter the metric values by namespace, task order, or Contract Line Item Number, CLIN.

### Video - [Monitor costs and resource consumption](https://www.cloudskillsboost.google/course_templates/1193/video/522183)

* [YouTube: Monitor costs and resource consumption](https://www.youtube.com/watch?v=ikQOSb16v9o)

SPEAKER: Next, let's explore how to track costs in GDC and monitor your resource consumption. The pricing calculator estimates the cost of using GDC resources in the GDC console. The calculator is in table format with three columns. The SKU is the name of the resource represented as a stock keeping unit, SKU. Quantity is an input field column for sighting the quantity of a specific SKU. Cost is the total estimated cost and exclusive taxes that show in the calculator's footer. As you enter quantities to input fields for each line item SKU, the cost column updates with new values. However, these values only represent estimated costs based upon the information entered. Actual usage cost may vary, based upon additional factors in your deployment. Use the billing usage and projected cost dashboards to query and visualize billing metrics. The projected cost dashboard is a management tool that lets you anticipate future SKU costs for your invoices. You can filter the dashboard to view a custom projection cost of a given SKU through the task order, contract line item number, and month. To get the permissions that you need to view the projected cost dashboard, ask your organization IAM admin to grant you the project Grafana viewer role for the platform OBS project. You can track resource consumption and storage allocation using the billing usage dashboards. Storage allocation tracks how much resource usage a given service in your project has consumed. Services include resources such as backup, marketplace, Database Service, and Vertex AI. To view a certain period of time in a given dashboard, click the Time dropdown in the menu bar. The Time dropdown defaults to show the metrics of the last six hours. The Compute Usage dashboard tracks compute allocation or the resource usage on storage and compute services for a given period of time. This dashboard is available for monitoring purposes, and metrics include resources such as vCPU, GPU, and allocated memory percentage.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1193/video/522184)

* [YouTube: Module review](https://www.youtube.com/watch?v=72KWZFH6ixA)

SPEAKER: Let's review the key points from this module. In this module, we reviewed the GDC cost and billing capabilities that help you monitor compute usage and consumption. Inbuilt billing reports based on Grafana provide the ability to view invoices and cost projections. You can implement billing alerts to monitor cost and send notifications when workload costs reach a designated threshold. And you can view the usage of GDC-managed services, such as the Database Service and Vertex AI Workbench. You can track cloud spend using the pricing calculator to get granular costs per SKU and quantity. You implement least privileged access to billing dashboards with predefined AIM roles. The inbuilt billing dashboard capability reduces the management need to install, configure, and secure cost-tracking tooling, which lets you focus on managing workloads.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1193/quizzes/522185)

#### Quiz 1.

> [!important]
> **Where can you view actual consumption of resources for tracking and cost management purposes?**
>
> * [ ] Audit Logging page
> * [ ] Projected Cost dashboard
> * [ ] Billing Reports dashboard
> * [ ] Services Usage dashboard

#### Quiz 2.

> [!important]
> **What variables do you enter into the pricing calculator in the GDC console to estimate the cost of using GDC resources?**
>
> * [ ] Peak, average, and duration
> * [ ] SKU, projected cost per duration
> * [ ] Resource usage per hour and cost
> * [ ] SKU and quantity

## Review: AI services and GDC deployments and operations

Review course content


### Video - [Course 3 review](https://www.cloudskillsboost.google/course_templates/1193/video/522186)

* [YouTube: Course 3 review](https://www.youtube.com/watch?v=UOpFROePsEU)

SPEAKER: Thank you for taking the AI Services and GDC Deployments and Operations course. Hopefully, you now have a better understanding of how to deploy applications to GDC and manage your deployments. You should be familiar with the three pretrained APIs that are available with GDC Vertex AI, and how to use them in your GDC environment. The GDC API interface lets you programmatically interact with your applications in GDC. You learned about some best practices to develop, deploy, and update your applications. You should also understand the responsibilities of the platform administrator and application operator personas in the upgrade process on GDC. GDC Observability provides tools to help you monitor system and application performance and log activity. You should understand how to access and use these tools and how data is collected. Finally, you should be familiar with the available tools and dashboards to monitor usage and consumption of resources in GDC. Throughout this series of courses, you've explored the example of Cymbal Federal and the three existing applications it plans to migrate to its GDC environment. Let's take a look at how each application has leveraged the features and services of the GDC platform. The Cymbal Federal Document Archive solution uses the Kubernetes service in GDC after the application team tests and verifies that the application can be containerized. The long-term archival storage of the audio files is provided by the GDC Object Storage service. DevOps processes provide separation of duties for database management and application management. For this reason, the application and database tier will be deployed in different projects. The application and database have a different lifecycle. Transient or 1N application versions are deployed and tested against 1N databases. This slide presents Cymbal Federal's Mission Logistics solution. In replatforming the application, Cymbal Federal needs to consider the sizing of the compute resources accordingly to ensure the smooth running of the capability with the GDC platform. GDC by design offers configurable VM sizing, backup, and recovery capabilities for managing enterprise VM workloads. To maintain the separation of duties for database management and application management provided by the current DevOps processes, the application and database tier will be deployed in different projects. The initial deployment plan uses the VM service and runs its existing database engine on a VM. In the updated deployment plan, the DevOps team separates individual functions of the application to be deployed and updated as independently containerized microservices. And the solution uses the GDC Database Service. Lastly, we have Cymbal Federal's Mission CRM solution. The CRM application maintains vetted contractors providing services on behalf of the entity. The application is three tier, built on Linux VMs, Java, and Spring Boot, and has been identified and tested as a fit for containerization. Within the database, there are business-sensitive data fields that are encrypted with the customer-managed encryption key. This key is maintained in GDC Key Management service. In all three applications, Cymbal Federal has implemented a solution that leverages the features of GDC platform services. This is the last course in the GDC Practitioner Fundamentals series. Thank you again for completing this series.

### Document - [Module slides](https://www.cloudskillsboost.google/course_templates/1193/documents/522187)

### Document - [Additional resources](https://www.cloudskillsboost.google/course_templates/1193/documents/522188)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
