---
id: 53
name: 'Building Batch Data Pipelines on Google Cloud'
datePublished: 2024-09-19
topics:
- Apache Hadoop
- Data Fusion
- Cloud Composer
type: Course
url: https://www.cloudskillsboost.google/course_templates/53
---

# [Building Batch Data Pipelines on Google Cloud](https://www.cloudskillsboost.google/course_templates/53)

**Description:**

Data pipelines typically fall under one of the Extract and Load (EL), Extract, Load and Transform (ELT) or Extract, Transform and Load (ETL) paradigms. This course describes which paradigm should be used and when for batch data. Furthermore, this course covers several technologies on Google Cloud for data transformation including BigQuery, executing Spark on Dataproc, pipeline graphs in Cloud Data Fusion and serverless data processing with Dataflow. Learners get hands-on experience building data pipeline components on Google Cloud using Qwiklabs.

**Objectives:**

- Review different methods of data loading: EL, ELT and ETL and when to use what
- Run Hadoop on Dataproc, leverage Cloud Storage, and optimize Dataproc jobs
- Build your data processing pipelines using Dataflow
- Manage data pipelines with Data Fusion and Cloud Composer

## Introduction

In this module, we introduce the course and agenda

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/53/video/509028)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=_tN2rz-qGt0)

Damon: Welcome to Building Batch Data Pipelines on Google Cloud. I'm Damon, and I'm a technical curriculum creator at Google. Booting batch data pipeline on Google Cloud is the second course of the Data Engineering on Google Cloud core series. In the previous course, we talked about data lakes and data warehouses as storage options for your data. In this course, we'll concentrate on how to build batch data pipelines to get data into storage using Extract Load, Extract Transform Load, and Extract Load Transform routines. We'll cover several technologies for data transformation on Google Cloud. Including BigQuery, Executing Spark on Dataproc, serverless data processing with Dataflow, and leveraging Google Cloud in pipelines with Cloud Data Fusion and Cloud Composer.

## Introduction to Building Batch Data Pipelines

This module reviews different methods of data loading: EL, ELT and ETL and when to use what

### Video - [Module introduction](https://www.cloudskillsboost.google/course_templates/53/video/509029)

- [YouTube: Module introduction](https://www.youtube.com/watch?v=HNexh_eHTZs)

person: What are batch pipelines? These are pipelines that process a bounded amount of data and then exit. For example, you might have a batch pipeline that runs once a day. It takes all the credit, debit, and money transfer transactions over that day, balances the books, and writes out the reconciled data to the data warehouse. If you are going to write such a pipeline to balance the books, should you use EL, ELT, or ETL? EL, remember, is extract and load. ELT loads the data as is and then transforms on the fly. ETL extracts the data, transforms it, then loads it into a data warehouse. Deciding which to use depends on the kinds of transformations you need and data quality considerations. We will look at how to build EL and ELT pipelines in BigQuery, the circumstances when EL and ELT are not appropriate, and why you might want to use ETL.

### Video - [EL, ELT, ETL](https://www.cloudskillsboost.google/course_templates/53/video/509030)

- [YouTube: EL, ELT, ETL](https://www.youtube.com/watch?v=wc1ZHzQMBWQ)

person: Let's start with a quick recap of EL, ELT and ETL. EL is Extract and Load. This refers to when data can be imported as is into a system. ELT or Extract, Load and Transform, allows raw data to be loaded directly into the target and transformed whenever it is needed. For example, you might provide access to the raw data through a view that determines whether the user wants all transactions or only reconciled ones. ETL or Extract, Transform and Load, is a data integration process in which transformation takes place in an intermediate service before it is loaded into the target. For example, the data might be transformed in Dataflow before being loaded into BigQuery. When would you use EL? The bottom line is that you should use EL only if the data is already clean and correct. Perhaps you have log files in Cloud Storage. You can extract data from files on Cloud Storage and load it into BigQuery's native storage. This is a simple REST API call. You can trigger this pipeline from Cloud Composer, Cloud Functions or via a scheduled query. You might even set it to work in micro batches, not quite streaming but near real time. Whenever a new file hits Cloud Storage, the Cloud Function runs, and the function invokes a BigQuery job. The data transfer service in BigQuery will also work here. Use EL for batch loading historical data or to do scheduled loads of log files. But let me emphasize, use EL only if the data is already clean and correct. ELT starts with EL, so the loading is the same and could work the same way. File hits Cloud Storage, function invokes BigQuery load, table appended to. The big difference is what happens next. The table might be stored in a private dataset and everyone accesses the data through a view which imposes data integrity checks. Or maybe you have a job that runs a SQL query with a destination table. This way, transformed data is stored in a table that everyone accesses. When do you use ELT? One common case is when you don't know what kind of transformations are needed to make the data usable. For example, let's say someone uploads a new image. You invoke the Vision API and back comes a long JSON message about all kinds of things in the image, text in the image, whether there's a landmark, a logo, what objects. What will an analyst need in the future? You don't know, so you store the raw JSON as is. Later, if someone wants to count the number of times a specific company's logos are in this set of images, they can extract logos from the JSON and then count them. Of course, this works only if the transformation that's needed can be expressed in SQL. In the case of the Vision API, the result is JSON and BigQuery SQL has support for JSON parsing, so ELT will work in this case.

### Video - [Quality considerations](https://www.cloudskillsboost.google/course_templates/53/video/509031)

- [YouTube: Quality considerations](https://www.youtube.com/watch?v=zJu2JjfsCNE)

person: Now that we have looked at EL and ELT, let's look at some of the transformations you might want to do and how they can be done in BigQuery. To keep things precise, let's assume that our data processing needs all revolve around quality improvements. What are some of the quality related reasons why we might want to process data? The top row are characteristics of information. Information can be valid, accurate, complete, consistent and or uniform. These terms are defined in the science of logic, each is independent. For example, data can be complete without being consistent, it can be valid without being uniform. There are formal definitions for each of these terms that you can look up online. But the main practical reason for seeking them is shown in the second row, the problems they present in data analysis. It is one thing to seek each of the five badges for your data to have objectively good data quality. However, it is another thing when poor quality data interferes with data analysis and leads to incorrect business decisions. So the reason to spend time, energy, and resources detecting and resolving quality issues is that it can affect a business outcome. Thus, if data does not conform to your business rules, you have a problem of validity. For example, let's say that you sell movie tickets, and each ticket costs $10. If you have a $7 transaction, then you have a validity problem. Similarly, accuracy problems are due to data not conforming to objective truth. Completeness has to do with failing to process everything. Consistency problems are if two different operations ought to be the same but yield different results, and because you don't know what to trust, you can't derive insights from the data. Uniformity is when data values of the same column in different rows mean different things. The main causes of these problems are listed in the third row. We will explore methods of detecting each of these issues in data. Now you have found the problems, what do you do about them? ELT and BigQuery can often help fix many data quality issues. Here is an example. Imagine you plan to analyze data but there are duplicate records making it seem like one kind of event is more common, when in fact this is just a data quality issue. You cannot derive insights from the data until the duplicates are removed. So do you need a transformation step to remove the duplicates before you store the data? Maybe, but a simpler solution exists, to count unique records. You do, of course, have count distinct in BigQuery and you can use that instead. Similarly, a problem like data being out of range can be solved in BigQuery without an intermediate transformation step. Invalid data can be filtered out using a BigQuery view and everyone can access the view rather than the raw data.

### Video - [How to carry out operations in BigQuery](https://www.cloudskillsboost.google/course_templates/53/video/509032)

- [YouTube: How to carry out operations in BigQuery](https://www.youtube.com/watch?v=yf4VFnMm7KM)

In this lesson, we will look at various quality issues and talk through some BigQuery capabilities that can help you address those quality problems. We can use Views to filter out rows that have quality issues. For example, remove quantities less than zero using a WHERE clause. After you do a GROUP BY, you can discard groups whose total number of records is less than 10 using the HAVING clause. Think carefully about how you wish to treat nulls and blanks. A NULL is the absence of data. A BLANK is an empty string. Consider if you are trying to filter out both NULLS and BLANKS or only NULLs or only BLANKs. You can easily count non-null values using COUNTIF and use the IF statement to avoid using specific values in computations. For accuracy, test data against known good values. For example, if you have an order, you could compute the sub_total from the quantity_ordered and item_price and make sure the math is accurate. Similarly, you can check if a value that is being inserted belongs to a canonical list of acceptable values. You can do that with a SQL IN. For completeness, identify any missing values and either filter out, or replace them with something reasonable. If the missing value is NULL, SQL provides functions like NULLIF, COUNTIF, COALESCE, etc. to filter missing values out of calculations. You might be able to do a UNION from another source to account for missing months of data. The automatic process of detecting data drops and requesting data items to fill in the gaps is called “backfilling”. It is a feature of some data transfer services. When loading data, verify file integrity with checksum values (hash, MD5). Consistency problems are often due to duplicates. You expect that something is unique, and it isn’t, so things like totals are wrong. COUNT provides the number of rows in a table that contain a non-null value. COUNT DISTINCT provides the number of unique values. If they are different, then it means that you have duplicate values. Similarly, if you do a GROUP BY, and any group contains more than one row, then you know you have two or more occurences of that value. Another reason that you might have consistency problems is if extra characters have been added to the fields. For example, you may be getting timestamps, some of which may include a timezone. Or you have strings that are padded. Use string functions to clean such data before passing it on. What happens if you are storing some value in centimeters, and suddenly, you start getting the value in millimeters? Your data warehouse will end up with non-uniform data. You have to safeguard against this. Use SQL cast to avoid issues with data types changing within a table. Use the SQL FORMAT() function to clearly indicate units. And in general, document them very clearly. I hope that what you are coming away with is the idea that BigQuery SQL is very powerful and you can take advantage of this.

### Video - [Shortcomings](https://www.cloudskillsboost.google/course_templates/53/video/509033)

- [YouTube: Shortcomings](https://www.youtube.com/watch?v=llbg03FkyeE)

In the previous lesson, we showed you some of the ways in which you can use SQL in an E-L-T pipeline to safeguard against quality issues. The point is that you don’t always need E-T-L. E-L-T might be an option even if you need transformation. However, there are situations where E-L-T won’t be enough. In that case, E-T-L might be what you need to do. What are the kinds of situations where it is appropriate? The first example - translating Spanish to English - requires calling an external API. This cannot be done directly in SQL. It is possible to use a BigQuery remote function, invoke the Cloud Translation API, and perform content translation. But this involves programming outside of BigQuery. The second example - looking at a stream of customer actions over a time window - is rather complex. You can do it with windowed aggregations, but it is far simpler with programmatic logic. So, if the transformations cannot be expressed in SQL or are too complex to do in SQL, you might want to transform the data before loading it into BigQuery. The reference architecture for Google Cloud suggests Dataflow as an E-T-L tool. We recommend that you build E-T-L pipelines in Dataflow and land the data in BigQuery. The architecture looks like this: Extract data from Pub/Sub, Cloud Storage, Spanner, Cloud SQL, etc. Transform the data using Dataflow, And have the Dataflow pipeline write to BigQuery. When would you do this? When the raw data needs to be quality-controlled, transformed, or enriched before being loaded into BigQuery. And the transforms are difficult to do in SQL. When the data loading has to happen continuously, i.e. if the use case requires streaming. Dataflow supports streaming. We’ll look at streaming in more detail in the next course. And when you want to integrate with continuous integration / continuous delivery (CI/CD) systems and perform unit testing on all components. It’s easy to schedule the launch of a Dataflow pipeline. Dataflow is not the only option you have on Google Cloud if you want to do E-T-L. In this course, we will look at several data processing and transformation services that Google Cloud provides: Dataflow, Dataproc, and Data Fusion. Dataproc and Dataflow can be used for more complex E-T-L pipelines. Dataproc is based on Apache Hadoop and requires significant Hadoop expertise to leverage directly. Data Fusion provides a simple graphical interface to build E-T-L pipelines that can then be easily deployed at scale to Dataproc clusters. Dataflow is a fully managed, serverless data processing service based on Apache Beam that supports both batch and streaming data processing pipelines. While significant Apache Beam expertise is desirable in order to leverage the full power of Dataflow, Google also provides quick-start templates for Dataflow to allow you to rapidly deploy a number of useful data pipelines. You can use any of these three products to carry out data transformation and then store the data in a data lake or data warehouse to support advanced analytics.

### Video - [ETL to solve data quality issues](https://www.cloudskillsboost.google/course_templates/53/video/509034)

- [YouTube: ETL to solve data quality issues](https://www.youtube.com/watch?v=-6p4hTa7I1E)

So now, let's look at using ETL to solve data quality issues. Unless you have specific needs, we recommended that you use Dataflow and BigQuery. What are a few needs that cannot be met easily with Dataflow and BigQuery? Low Latency and high throughput. BigQuery queries are subject to a latency on the order of a few hundred milliseconds and you can stream on the order of a million rows per second into a BigQuery table -- this used to be 100,000 rows, but recently it got raised to 1 million per project. The typical latency number quoted for BigQuery is on the order of a second, but with BI engine it is possible to get latency on the order of 100 milliseconds -- you should always check the documentation and the solutions pages for the latest values. If your latency and throughput considerations are more stringent, then Bigtable might be a better sink for your data processing pipelines. Reusing Spark pipelines. Maybe you already have a significant investment in Hadoop and Spark. In that case, you might be a lot more productive in a familiar technology. Use Spark if that’s what you know really well. Need for visual pipeline building. Dataflow requires you to code data pipelines in Java or Python. If you want to have data analysts and non-technical users create data pipelines, use Cloud Data Fusion. They can drag-and-drop and visually build pipelines. We’ll look at all these options briefly now and in greater detail in the remainder of this course. Dataproc is a managed service for batch processing, querying, streaming, and Machine Learning. It is a service for Hadoop workloads and is quite cost effective when taking into consideration eliminating the tasks related to running Hadoop on bare metal and taking on all of the related maintenance activities. It also has a few powerful features like auto-scaling and out-of-the-box integration with Google Cloud products like BigQuery. Cloud Data Fusion is a fully-managed, cloud-native, enterprise data integration service for quickly building and managing data pipelines. You can use it to populate a data warehouse, but you can also use it for transformations and cleanup, and ensuring data consistency. Users, who can be in non-programming roles, can build visual pipelines to address business imperatives like regulatory compliance without having to wait for an IT team to write a Dataflow pipeline. Data Fusion also has a flexible API so IT staff can create scripts to automate execution. Regardless of which E-T-L is used -- Dataflow, Dataproc, Data Fusion -- there are some crucial aspects to keep in mind. First: Maintaining data lineage is important. What do we mean by Lineage? Where the data came from, what processes it has been through, and what condition it is in, are all lineage. If you have the lineage, you know for what kinds of uses the data is suited. If you find the data gives odd results, you can check the lineage to find out if there is a cause that can be corrected. Lineage also helps with trust and regulatory compliance. The other cross-cutting concern is that you need to keep metadata around. You need a way to track the lineage of data in your organization for discovery and identification of suitability for uses. On Google Cloud, Dataplex provides discoverability. But you have to do your bit by adding labels. Dataplex metadata can be viewed directly in BigQuery thereby simplifying the process of confirming data lineage. A label is a key-value pair that helps you organize your resources. In BigQuery you can attach labels to Datasets, Tables, and Views. Labels are useful for managing complex resources because you can filter them based on their labels. Labels are a first step towards a Data Catalog. Among the things that labels help with is Cloud Billing. If you attach labels to Compute Engine instances and to buckets and to Dataflow pipelines, then you have a way to get a fine-grained look at your Cloud bill because the information about labels is forwarded to the billing system, and so you can break down your billing charges by label. Data Catalog is a fully managed and highly scalable data discovery and metadata management service. It is serverless and requires no infrastructure to set up or manage. It provides access-level controls and honors source A-C-Ls for read, write, and search for the data assets; giving you enterprise-grade access control. Think of Data Catalog as a metadata-as-a-service. It provides metadata management service for cataloging data assets via custom APIs and the UI, thereby providing a unified view of data wherever it is. It supports schematized tags (e.g., E-num, Bool, DateTime) and not just simple text tags — providing organizations rich and organized business metadata. It offers unified data discovery of all data assets, spread across multiple projects and systems. It comes with a simple and easy-to-use search UI to quickly and easily find data assets; powered by the same Google search technology that supports Gmail and Drive. As a central catalog, it provides a flexible and powerful cataloging system for capturing both technical metadata (automatically) as well as business metadata (tags) in a structured format. One of the great things about the data discovery is that it integrates with the Cloud Data Loss Prevention API. You can use it to discover and classify sensitive data, providing intelligence and helping to simplify the process of governing your data. Data Catalog empowers users to annotate business metadata in a collaborative manner and provides the foundation for data governance. Specifically, Data Catalog makes all the metadata about your datasets available to search for your users, regardless of where the data are stored. Using Data Catalog, you can group datasets together with tags, flag certain columns as containing sensitive data, etc. Why is this useful? If you have many different datasets with many different tables — to which different users have different access levels — the Data Catalog provides a single unified user experience for discovering those datasets quickly. No more hunting for specific table names in the databases, which may not be accessible by all users.

### Quiz - [Introduction to Building Batch Data Pipelines](https://www.cloudskillsboost.google/course_templates/53/quizzes/509035)

#### Quiz 1.

> [!important]
> **Which of the following is the ideal use case for Extract and Load (EL)**
>
> - [ ] When the raw data needs to be quality-controlled, transformed, or enriched before being loaded into BigQuery
> - [ ] Scheduled periodic loads of log files (e.g. once a day)
> - [ ] When the data loading has to happen continuously
> - [ ] When you want to integrate with continuous integration / continuous delivery (CI/CD) systems and perform unit testing on all components.

## Executing Spark on Dataproc

This module shows how to run Hadoop on Dataproc, how to leverage Cloud Storage, and how to optimize your Dataproc jobs.

### Video - [Module introduction](https://www.cloudskillsboost.google/course_templates/53/video/509036)

- [YouTube: Module introduction](https://www.youtube.com/watch?v=NfRjbqojA-0)

- In this module, we will discuss Dataproc, Google Cloud's managed Hadoop service, and in particular, Apache Spark. In this module, we'll cover the Hadoop Ecosystem, learn about running Hadoop on Dataproc, understand the benefits of Cloud Storage instead of HDFS, learn about optimizing Dataproc, and complete a hands-on lab with Apache Spark on Dataproc.

### Video - [The Hadoop ecosystem](https://www.cloudskillsboost.google/course_templates/53/video/509037)

- [YouTube: The Hadoop ecosystem](https://www.youtube.com/watch?v=IJOAT3p4X4A)

>> Let's start by looking at the Hadoop ecosystem in a little more detail. It helps to place the services you'll be learning about in historical context. Before 2006, big bata meant big databases. Database design came from a time when storage was relatively cheap and processing was expensive, so it made sense to copy the data from its storage location to the processor to perform data processing, then the result will be copied back to storage. Around 2006, distributed processing of big data became practical with Hadoop. The idea behind Hadoop is to create a cluster of computers and leverage distributed processing. HDFS, the Hadoop Distributed File System, stored the data on the machines in the cluster and MapReduce provided distributed processing of the data. A whole ecosystem of Hadoop related software grew up around Hadoop, including Hive, Pig and Spark. Organizations use Hadoop for on-premises big data workloads. They make use of a range of applications that run on Hadoop clusters, such as Presto, but a lot of customers use Spark. Apache Hadoop is an open source software project that maintains the framework for distributed processing of large datasets across clusters of computers using simple programming models. HDFS is the main file system Hadoop uses for distributing work to nodes on the cluster. Apache Spark is an open source software project that provides a high performance analytics engine for processing batch and streaming data. Spark can be up to 100 times faster than equivalent Hadoop jobs because it leverages in memory processing. Spark also provides a few for dealing with data including resilient distributed datasets and data frames. Spark in particular is very powerful and expressive and used for a lot of workloads. A lot of the complexity and overhead of OSS Hadoop has to do with assumptions in the design that existed in the data center. Relieved of those limitations, data processing becomes a much richer solution with many more options. There are two common issues with OSS Hadoop: tuning and utilization. In many cases, using Dataproc as designed will overcome these limitations. On-premises Hadoop clusters, due to their physical nature, suffer from limitations. The lack of separation between storage and compute resources results in capacity limits and an inability to scale fast. The only way to increase capacity is to add more physical servers. There are many ways in which using Google Cloud can save you time, money and effort compared to using an on-premises Hadoop solution. In many cases, adopting a Cloud based approach can make your overall solution simpler and easier to manage. Built-in support for Hadoop. Dataproc is a managed Hadoop and Spark environment. You can use Dataproc to run most of your existing jobs with minimal alteration, so you don't need to move away from all of the Hadoop tools you already know. Managed hardware and configuration. When you run Hadoop on Google Cloud, you never need to worry about physical hardware. You specify the configuration of your cluster and Dataproc allocates resources for you, you can scale your cluster at any time. Simplified version management. Keeping open source tools up to date and working together is one of the most complex parts of managing a Hadoop cluster. When you use Dataproc, much of that work is managed for you by Dataproc versioning. Flexible job configuration. A typical on-premises Hadoop setup uses a single cluster that serves many purposes. When you move to Google Cloud, you can focus on individual tasks, creating as many clusters as you need. This removes much of the complexity of maintaining a single cluster with growing dependencies and software configuration interactions. Running MapReduce directly on top of Hadoop is very useful, but it has the complication that the Hadoop system has to be tuned for the kind of job being run to make efficient use of the underlying resources. A simple explanation of Spark is that it is able to mix different kinds of applications and to adjust how it uses the available resources. Spark uses a declarative programming model. In imperative programming, you tell the system what to do and how to do it. In declarative programming, you tell the system what you want and it figures out how to implement it. You will be learning to work with Spark in the labs in this course. There is a full SQL implementation on top of Spark. There is a common data frame model that works across Scala, Java, Python, SQL and R. And there is a distributed machine learning library called Spark ML Lib.

### Video - [Running Hadoop on Dataproc](https://www.cloudskillsboost.google/course_templates/53/video/509038)

- [YouTube: Running Hadoop on Dataproc](https://www.youtube.com/watch?v=T0j2XJgf49M)

Next, we'll discuss how and why you should consider processing your same Hadoop job code in the cloud using Dataproc on Google Cloud. Dataproc lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. When compared to traditional, on-premises products, and competing cloud services, Dataproc has unique advantages for clusters of three to hundreds of nodes. There is no need to learn new tools or APIs to use Dataproc, making it easy to move existing projects into Dataproc without redevelopment. Spark, Hadoop, Pig, and Hive are frequently updated. Here are some of the key features of Dataproc. Low cost: Dataproc is priced at 1 cent per virtual CPU per cluster per hour, on top of the other Google Cloud resources you use. In addition, Dataproc clusters can include preemptible instances that have lower compute prices. You use and pay for things only when you need them, so Dataproc charges second-by-second billing with a one-minute-minimum billing period. Super-fast: Dataproc clusters are quick to start, scale, and shutdown, with each of these operations taking 90 seconds or less, on average. Resizable clusters: Clusters can be created and scaled quickly with a variety of virtual machine types, disk sizes, number of nodes, and networking options. Open source ecosystem: You can use Spark and Hadoop tools, libraries, and documentation with Dataproc. Dataproc provides frequent updates to native versions of Spark, Hadoop, Pig, and Hive, so there is no need to learn new tools or APIs, and it is possible to move existing projects or ETL pipelines without redevelopment. Integrated: Built-in integration with Cloud Storage, BigQuery, and Bigtable ensures data will not be lost. This, together with Cloud Logging and Cloud Monitoring, provides a complete data platform and not just a Spark or Hadoop cluster. For example, you can use Dataproc to effortlessly ETL terabytes of raw log data directly into BigQuery for business reporting. Managed: Easily interact with clusters and Spark or Hadoop jobs, without the assistance of an administrator or special software, through the Cloud Console, the Cloud SDK, or the Dataproc REST API. When you're done with a cluster, simply turn it off, so money isn‚Äôt spent on an idle cluster. Versioning: Image versioning allows you to switch between different versions of Apache Spark, Apache Hadoop, and other tools. Highly available: Run clusters with multiple primary nodes and set jobs to restart on failure to ensure your clusters and jobs are highly available. Developer tools: Multiple ways to manage a cluster, including the Cloud Console, the Cloud SDK, RESTful APIs, and SSH access. Initialization actions: Run initialization actions to install or customize the settings and libraries you need when your cluster is created. And automatic or manual configuration: Dataproc automatically configures hardware and software on clusters for you while also allowing for manual control. Dataproc has two ways to customize clusters; optional components and initialization actions. Pre-configured optional components can be selected when deploying from the console or via the command line and include: Anaconda, Hive WebHCat, Jupyter Notebook, Zeppelin Notebook, Druid, Presto and Zookeeper. Initialization actions let you customize your cluster by specifying executables or scripts that Dataproc will run on all nodes in your Dataproc cluster immediately after the cluster is set up. Here's an example of how you can create a Dataproc cluster using the Cloud SDK. And we're going to specify an HBase shell script to run on the clusters initialization. There are a lot of pre-built startup scripts that you can leverage for common Hadoop cluster setup tasks, like Flink, Jupyter and more. You can check out the GitHub repo link in the Course Resources to learn more. Let's talk more about the architecture of the cluster. A Dataproc cluster can contain either preemptible secondary workers or non-preemptible secondary workers, but not both. The standard setup architecture is much like you would expect on-premise. You have a cluster of virtual machines for processing and then persistent disks for storage via HDFS. You've also got your manager node VM (or VMs) and a set of worker nodes. Worker nodes can also be part of a managed instance group, which is just another way of ensuring that VMs within that group are all of the same template. The advantage is that you can spin up more VMs than you need to automatically resize your cluster based on the demands. It also only takes a few minutes to upgrade or downgrade your cluster. Google Cloud recommends a ratio of 60/40 as the maximum between standard VMs and preemptible VMs. Generally, you shouldn't think of a Dataproc cluster as long-lived. Instead you should spin them up when you need compute processing for a job and then simply turn them down. You can also persist them indefinitely if you want to. What happens to HDFS storage on disk when you turn those clusters down? The storage would go away too, which is why it's a best practice to use storage that's off cluster by connecting to other Google Cloud products. Instead of using native HDFS on a cluster, you could simply use a cluster on Cloud Storage via the HDFS connector. It's pretty easy to adapt existing Hadoop code to use Cloud Storage instead of HDFS. Change the prefix for this storage from hdfs// to gs//. What about Hbase off-cluster? Consider writing to Bigtable instead. What about large analytical workloads? Consider writing that data into BigQuery and doing those analytical work loads there. Using Dataproc involves this sequence of events: Setup, Configuration, Optimization, Utilization, and Monitoring. Setup means creating a cluster. And you can do that through the Cloud Console, or from the command line using the gcloud command. You can also export a YAML file from an existing cluster or create a cluster from a YAML file. You can create a cluster from a Terraform configuration, or use the REST API. The cluster can be set as a single VM, which is usually to keep costs down for development and experimentation. Standard is with a single Primary Node, and High Availability has three Primary Nodes. You can choose a region and zone, or select a "global region" and allow the service to choose the zone for you. The cluster defaults to a Global endpoint, but defining a Regional endpoint may offer increased isolation and in certain cases, lower latency. The Primary Node is where the HDFS Namenode runs, as well as the YARN node and job drivers. HDFS replication defaults to 2 in Dataproc. Optional components from the Hadoop-ecosystem include: Anaconda (Python distribution and package manager), Hive Webcat, Jupyter Notebook, and Zeppelin Notebook. Cluster properties are run-time values that can be used by configuration files for more dynamic startup options. And user labels can be used to tag the cluster for your own solutions or reporting purposes. The Primary Node Worker Nodes, and preemptible Worker Nodes, if enabled, have separate VM options, such as vCPU, memory, and storage. Preemptible nodes include YARN NodeManager but they do not run HDFS. There are a minimum number of worker nodes, the default is 2. The maximum number of worker nodes is determined by a quota and the number of SSDs attached to each worker. You can also specify initialization actions, such as initialization scripts that can further customize the worker nodes. And metadata can be defined so that the VMs can share state information. Preemptible VMs can be used to lower costs. Just remember they can be pulled from service at any time and within 24 hours. So your application might need to be designed for resilience to prevent data loss. Custom machine types allow you to specify the balance of Memory and CPU to tune the VM to the load, so you are not wasting resources. A custom image can be used to pre-install software so that it takes less time for the customized node to become operational than if you installed the software at boot-time using an initialization script. You can also use a Persistent SSD boot disk for faster cluster start-up. Jobs can be submitted through the cloud console, the gcloud command, or the REST API. They can also be started by orchestration services such as Dataproc Workflow and Cloud Composer. Don't use Hadoop's direct interfaces to submit jobs because the metadata will not be available to Dataproc for job and cluster management, and for security, they are disabled by default. By default, jobs are not restartable. However, you can create restartable jobs through the command line or REST API. Restartable jobs must be designed to be idempotent and to detect successorship and restore state. Lastly, after you submit your job you'll want to monitor it, you can do so using Cloud Monitoring. Or you can also build a custom dashboard with graphs and set up monitoring of alert policies to send emails for example, where you can notify if incidents happen. Any details from HDFS, YARN, metrics about a particular job or overall metrics for the cluster like CPU utilization, disk and network usage, can all be monitored and alerted on with Cloud Monitoring.

### Video - [Cloud Storage instead of HDFS](https://www.cloudskillsboost.google/course_templates/53/video/509039)

- [YouTube: Cloud Storage instead of HDFS](https://www.youtube.com/watch?v=AHOT-yT3V8Y)

person: Let's discuss more about using Google Cloud Storage instead of the native Hadoop file system or HDFS. Network speeds were slow originally. That's why we kept data as close as possible to the processor. Now, with petabit networking, you can treat storage and compute independently and move traffic quickly over the network. Your on-premise Hadoop clusters need local storage on its disk since the same server runs computes on stores jobs. That's one of the first areas for optimization. You can run HDFS in the Cloud just by lifting and shifting your Hadoop workloads to Dataproc. This is often the first step to the Cloud and requires no code changes. It just works. But HDFS on the Cloud is a subpar solution in the long run. This is because of how HDFS works on the clusters with block size, the data locality and the replication of the data in HDFS. For block size in HDFS, you're tying the performance of input and output to the actual hardware the server is running on. Again, storage is not elastic in this scenario, you're in the cluster. If you run out of persistent disk space on your cluster, you will need to resize even if you don't need the extra compute power. For data locality, there are similar concerns about storing data on individual persistent disks. This is especially true when it comes to replication. In order for HDFS to be highly available, it replicates three copies of each block out to storage. It would be better to have a storage solution that's separately managed from the constraints of your cluster. Google's network enables new solutions for big data. The Jupyter networking fabric within a Google data center delivers over one petabit per second of bandwidth. To put that into perspective, that's about twice the amount of traffic exchanged on the entire public internet. See Cisco's annual estimate of all internet traffic. If you draw a line somewhere in a network, bisectional bandwidth is the rate of communication at which servers on one side of the line can communicate with servers on the other side. With enough bisectional bandwidth, any server can communicate with any other server at full network speeds. With petabit bisectional bandwidth, the communication is so fast that it no longer makes sense to transfer files and store them locally. Instead, it makes sense to use the data from where it is stored. Inside of a Google data center, the internal name for the massively distributed storage layer is called Colossus. Under the network inside the data center is Jupyter. Dataproc clusters get the advantage of scaling up and down VMs that they need to do the compute while passing off persistent storage needs with the ultra-fast Jupyter network to a storage products like Cloud Storage, which is controlled by Colossus behind the scenes. A historical continuum of data management is as follows. Before 2006, big data meant big databases, database design came from a time when storage was relatively cheap, and processing was expensive. Around 2006, distributed processing of big data became practical with Hadoop. Around 2010, BigQuery was released, which was the first of many big data services developed by Google. Around 2015, Google launched Dataproc, which provides a managed service for creating Hadoop and Spark clusters and managing data processing workloads. One of the biggest benefits of Hadoop in the Cloud is that separation of compute and storage. With Cloud Storage as the backend, you can treat clusters themselves as ephemeral resources, which allows you not to pay for compute capacity when you're not running any jobs. Also, Cloud Storage is its own completely scalable and durable storage service, which is connected to many other Google Cloud projects. Cloud storage could be a drop-in replacement for your HDFS backend for Hadoop, the rest of your code would just work. Also, you can use the Cloud storage connector manually on your non-Cloud Hadoop clusters if you didn't want to migrate your entire cluster to the Cloud yet. With HDFS, you must over-provision for current data and for data you might have, and you must use persistent disks throughout. With Cloud Storage however, you pay for exactly what you need when you use it. Cloud Storage is optimized for large bulk parallel operations. It has very high throughput, but it has significant latency. If you have large jobs that are running lots of tiny little blocks, you may be better off with HDFS. Additionally, you want to avoid iterating sequentially over many nested directories in a single job. Using Cloud Storage instead of HDFS provides some key benefits due to the distributed service including eliminating bottlenecks and single points of failure. However, there are some disadvantages to be aware of, including the challenges presented by renaming objects and the inability to append to objects. Cloud Storage is at its core an object store, it only simulates a file directory. So directory renames in HDFS are not the same as they are in Cloud Storage, but new objects store oriented output committers mitigate this as you see here. Disk CP is a key tool for moving data. In general, you want to use a push-based model for any data that you know you will need while pull-based may be a useful model if there is a lot of data that you might not ever need to migrate.

### Video - [Optimizing Dataproc](https://www.cloudskillsboost.google/course_templates/53/video/509040)

- [YouTube: Optimizing Dataproc](https://www.youtube.com/watch?v=-_E5rdwcSAo)

Next, let's look at optimizing Dataproc. Where is your data and where is your cluster? Knowing your data locality can have a major impact on your performance. You want to be sure that your data's region and your cluster zone are physically close in distance. When using Dataproc you can omit the zone and have the Dataproc Auto Zone feature select a zone for you in the region you choose. While this handy feature can optimize on where to put your cluster it does not know how to anticipate the location of the data you're cluster will be accessing. Make sure that the Cloud storage bucket is in the same regional location as your Dataproc region. Is your network traffic being funneled? Be sure that you do not have any network rules or roots that funnel Cloud storage traffic through a small number of VPN gateways before it reaches your cluster. There are large network pipes between Cloud Storage and Compute Engine. You don't want to throttle your bandwidth by sending traffic into a bottleneck in you're google Cloud networking configuration. How many input files and Hadoop partitions are you trying to deal with? Make sure you are not dealing with more than around 10,000 input files. If you find yourself in this situation try to combine or union the data into larger file sizes. If this file volume reduction means that now you are working with larger datasets more than approximately 50,000 Hadoop partitions you should consider adjusting the setting fs.gs.block.size to a larger value accordingly. Is the size of your persistent disk limiting your throughput? Oftentimes when getting started with Google Cloud you may have just a small table that you want to benchmark. This is generally a good approach as long as you do not choose a persistent disk that assigns to such a small quantity of data, it will most likely limit your performance. Standard persistent disk scale linearly with volume size. Did you allocate enough virtual machines to your cluster? A question that often comes up when migrating from on-premises hardware to Google Cloud is how to accurately size the number of virtual machines needed. Understanding your workloads is key to identifying a cluster size. Running prototypes and benchmarking with real data and real jobs is crucial to informing the actual VM allocation decision. Locally, the ephemeral nature of the Cloud makes it easy to write size clusters for the specific task at hand instead of trying to purchase hardware upfront, thus, you can easily resize your cluster as needed. Employing job scoped clusters is a common strategy for Dataproc clusters

### Video - [Optimizing Dataproc storage](https://www.cloudskillsboost.google/course_templates/53/video/509041)

- [YouTube: Optimizing Dataproc storage](https://www.youtube.com/watch?v=-fhY9oSllnE)

Local HDFS is a good option if: Your jobs require a lot of metadata operations—for example, you have thousands of partitions and directories, and each file size is relatively small. You modify the HDFS data frequently or you rename directories. Cloud Storage objects are immutable, so renaming a directory is an expensive operation because it consists of copying all objects to a new key and deleting them afterwards. You heavily use the append operation on HDFS files. You have workloads that involve heavy I/O. For example, you have a lot of partitioned writes, such as in this example. You have I/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation. In general, we recommend using Cloud Storage as the initial and final source of data in a big-data pipeline. For example, if a workflow contains five Spark jobs in series, the first job retrieves the initial data from Cloud Storage and then writes shuffle data and intermediate job output to HDFS. The final Spark job writes its results to Cloud Storage. Using Dataproc with Cloud Storage allows you to reduce the disk requirements and save costs by putting your data there instead of in the HDFS. When you keep your data on Cloud Storage and don't store it on the local HDFS, you can use smaller disks for your cluster. By making your cluster truly on-demand, you're also able to separate storage and compute, as noted earlier, which helps you reduce costs significantly. Even if you store all of your data in Cloud Storage, your Dataproc cluster needs HDFS for certain operations such as storing control and recovery files, or aggregating logs. It also needs non-HDFS local disk space for shuffling. You can reduce the disk size per worker if you are not heavily using the local HDFS. Here are some options to adjust the size of the local HDFS. Decrease the total size of the local HDFS by decreasing the size of primary persistent disks for the primary and workers. The primary persistent disk also contains the boot volume and system libraries, so allocate at least 100 GB. Increase the total size of the local HDFS by increasing the size of primary persistent disk for workers. Consider this option carefully— it's rare to have workloads that get better performance by using HDFS with standard persistent disks in comparison to using Cloud Storage or local HDFS with SSD. Attach up to eight SSDs to each worker and use these disks for the HDFS. This is a good option if you need to use the HDFS for I/O-intensive workloads and you need single-digit millisecond latency. Make sure that you use a machine type that has enough CPUs and memory on the worker to support these disks. And use SSD persistent disks for your primary or workers as a primary disk. You should understand the repercussions of geography and regions before you configure your data and jobs. Many Google Cloud services require you to specify regions or zones in which to allocate resources. The latency of requests can increase when the requests are made from a different region than the one where the resources are stored. Additionally, if the service's resources and your persistent data are located in different regions, some calls to Google Cloud services might copy all of the required data from one zone to another before processing. This can have a severe impact on performance. Cloud Storage is the primary way to store unstructured data in Google Cloud, but it isn't the only storage option. Some of your data might be better suited to storage in products designed explicitly for big data. You can use Bigtable to store large amounts of sparse data. Bigtable is an HBase-compliant API that offers low latency and high scalability to adapt to your jobs. For data warehousing, you can use BigQuery. Because Dataproc runs Hadoop on Google Cloud, using a persistent Dataproc cluster to replicate your on-premises setup might seem like the easiest solution. However, there are some limitations to that approach. Keeping your data in a persistent HDFS cluster using Dataproc is more expensive than storing your data in Cloud Storage, which is what we recommend. Keeping data in an HDFS cluster also limits your ability to use your data with other Google Cloud products. Augmenting or replacing some of your open-source-based tools with other related Google Cloud services can be more efficient or economical for particular use cases. Using a single, persistent Dataproc cluster for your jobs is more difficult to manage than shifting to targeted clusters that serve individual jobs or job areas. The most cost-effective and flexible way to migrate your Hadoop system to Google Cloud is to shift away from thinking in terms of large, multi-purpose, persistent clusters and instead think about small, short-lived clusters that are designed to run specific jobs. You store your data in Cloud Storage to support multiple, temporary processing clusters. This model is often called the ephemeral model, because the clusters you use for processing jobs are allocated as needed and are released as jobs finish. If you have efficient utilization, don't pay for resources that you don't use - employ scheduled deletion. A fixed amount of time after the cluster enters an idle state, you can automatically set a timer. You can give it a timestamp, and the count starts immediately once the expiration has been set. You can set a duration, the time in seconds to wait before automatically turning down the cluster. You can range from ten minutes as a minimum, to 14 days as a maximum at a granularity of one second. The biggest shift in your approach between running an on-premises Hadoop workflow and running the same workflow on Google Cloud is the shift away from monolithic, persistent clusters to specialized, ephemeral clusters. You spin up a cluster when you need to run a job and then delete it when the job completes. The resources required by your jobs are active only when they're being used, so you only pay for what you use. This approach enables you to tailor cluster configurations for individual jobs. Because you aren't maintaining and configuring a persistent cluster, you reduce the costs of resource use and cluster administration. This section describes how to move your existing Hadoop infrastructure to an ephemeral model. To get the most from Dataproc, customers need to move to an “ephemeral” model of only using clusters when they need them. This can be scary because a persistent cluster is comfortable. With Cloud Storage data persistence and fast boot of Dataproc, however, a persistent cluster is a waste of resources. If a persistent cluster is needed, make it small. Clusters can be resized anytime. Ephemeral model is the recommended route but it requires storage to be decoupled from compute. Separate job shapes and separate clusters. Decompose even further with job-scoped clusters. Isolate dev, staging, and production environments by running on separate clusters. Read from the same underlying data source on Cloud Storage. Add appropriate ACLs to service accounts to protect data. The point of ephemeral clusters is to use them only for the jobs' lifetime. When it's time to run a job, follow this process: Create a properly configured cluster. Run your job, sending output to Cloud Storage or another persistent location. Delete the cluster. Use your job output however you need to. View logs in Cloud Logging or Cloud Storage. If you can't accomplish your work without a persistent cluster, you can create one. This option may be costly and isn't recommended if there is a way to get your job done on ephemeral clusters. You can minimize the cost of a persistent cluster by: Creating the smallest cluster you can. Scoping your work on that cluster to the smallest possible number of jobs. And scaling the cluster to the minimum workable number of nodes, adding more dynamically to meet demand.

### Video - [Optimizing Dataproc templates and autoscaling](https://www.cloudskillsboost.google/course_templates/53/video/509042)

- [YouTube: Optimizing Dataproc templates and autoscaling](https://www.youtube.com/watch?v=7ufsyiZ6wgQ)

The Dataproc Workflow Template is a YAML file that is processed through a Directed Acyclic Graph or DAG. It can create a new cluster, select from an existing cluster, submit jobs, hold jobs for submission until dependencies can complete, and it can delete a cluster when the job is done. It’s available through the gcloud command and the REST API. You can view existing workflow templates and instantiated workflows through the Google Cloud console as well. The Workflow Template becomes active when it is instantiated into the DAG. The Template can be submitted multiple times with different parameter values. You can also write a template inline in a gcloud command, and you can list workflows and workflow metadata to help diagnose issues. Here's an example of a Dataproc workflow template. First, we get all the things that need to be installed in the cluster using our startup scripts and manually echoing pip install commands like the one seen here to install matplotlib. You can have multiple startup shell scripts run like you see in this example. Next, we use the gcloud command for creating a new cluster in advance of running our job. We specify cluster parameters like the template to be used in our desired architecture and what machine types and image versions we want for hardware and software. After that, we need to add a job to the newly created cluster. In this example, we have a Spark job written in Python that exists in a Cloud Storage bucket that we control. Lastly, we need to submit this template itself as a new workflow template as you see with the last command. Dataproc autoscaling provides clusters that size themselves to the needs of the enterprise. Key features include: Jobs are “fire and forget”, There’s no need to manually intervene when a cluster is over or under capacity, You can choose between standard and preemptible workers, and You can save resources such as quota and cost at any point in time Autoscaling policies provide fine-grained control. This is based on the difference between YARN pending and available memory. If more memory is needed, then you scale up. If there’s excess memory, you scale down. Obey VM limits and scale based on scale factor. Dataproc autoscaling provides flexible capacity for more efficient utilization, making scaling decisions based on Hadoop YARN Metrics. It’s designed to be used only with off-cluster persistent data, not on-cluster HDFS or HBase. It works best with a cluster that processes a lot of jobs or that processes a single large job. It doesn’t support Spark Structured Streaming, a streaming service built on top of Spark SQL. It’s also not designed to scale to zero. So it’s not the best for sparsely utilized or idle clusters. In these cases it’s equally fast to terminate a cluster that’s idle and create a new cluster when it’s needed. For that purpose you would look at Dataproc Workflows or Cloud Composer, and Cluster Scheduled Deletion. One of the things that you want to consider when working with autoscaling is setting the initial workers. The number of initial workers is set from Worker Nodes, Nodes Minimum. Setting this value ensures that the cluster comes up to basic capacity faster than if you let autoscaling handle it. Because autoscaling might require multiple autoscale periods to scale up. The primary minimum number of workers may be the same as the cluster nodes minimum. There is a maximum that caps the number of worker nodes. If there is heavy load on the cluster, autoscaling determines it is time to scale up. The scale_up.factor determines how many nodes to launch. This would commonly be one node. But if you knew that a lot of demand would occur at once, maybe you want to scale up faster. After the action, there is a cooldown period to let things settle before autoscaling evaluation occurs again. The cooldown period reduces the chances that the cluster will start and terminate nodes at the same time. In this example, the extra capacity isn't needed. And there is a graceful decommission timeout to give running jobs a chance to complete before the node goes out of service. Notice there is a scale down factor. In this case it is scaling down by one node at a time for a more leisurely reduction in capacity. After the action, there is another cooldown period. And a second scale down, resulting in a return to the minimum number of workers. A secondary min_workers and max_workers controls the scale of preemptible workers.

### Video - [Optimizing Dataproc monitoring](https://www.cloudskillsboost.google/course_templates/53/video/509043)

- [YouTube: Optimizing Dataproc monitoring](https://www.youtube.com/watch?v=otWZSXREhV4)

person: In Google Cloud, you can use Cloud Logging and Cloud Monitoring to view and customize logs, and to monitor jobs and resources. The best way to find what error caused a Spark job failure is to look at the driver output and the logs generated by the Spark executioners. Note, however, that if you submit a Spark job by connecting directly to the primary node using SSH, it's not possible to get the driver output. You can retrieve the driver program output by using the Cloud Console or by using G Cloud command. The output is also stored in the Cloud storage bucket of the Dataproc cluster. All other logs are located in different files inside the machines of the cluster. It's possible to see the logs for each container from the spark app Web UI, or from the history server after the program ends in the executer's tab. You need to browse through each Spark container to view each log. If you write logs or print to standard out or standard air in your application code, the logs are saved in the redirection of standard out or standard air. In a Dataproc cluster, Yarn is configured to collect all these logs by default, and they're available in Cloud Logging. Logging provides a consolidated and concise view of all logs so that you don't need to spend time browsing among container logs to find errors. This screen shows the login page in the Cloud Console. You can view all logs from your Dataproc cluster by selecting the cluster's name in the selector menu. Don't forget to expand the time duration in the time range selector. You can get logs from a Spark application by filtering by its ID, you can get the application ID from the driver output. To find logs faster, you can create and use your own labels for each cluster or for each Dataproc job. For example, you can create a label with the key environment or ENV as the value in the exploration and use it for your data exploration job. You can then get logs for all exploration job creations by filtering with the label environment with a value exploration in logging. Note that this filter will not return all logs for this job, only the resource creation logs. You can set the driver log level using the following G Cloud command: G Cloud, Dataproc, jobs, submit, Hadoop, with the parameter driver log levels. You set the log level for the rest of the application from the spark context, for example, Spark dot Spark context dot set log level. And for here, we'll just say the example is debug. Cloud Monitoring can monitor the cluster's CPU, disk, network usage and Yarn resources. You can create a custom dashboard to get up to date charts for these and other metrics. Dataproc runs on top of Compute Engine. If you want to visualize CPU usage, disk IO or networking metrics in a chart, you need to select a Compute Engine VM instance as the resource type, and then filter by the cluster name. This diagram shows an example of the output. To view metrics for Spark queries, jobs, stages, or tasks, connect to the spark applications Web UI.

### Video - [Lab Intro: Running Apache Spark jobs on Dataproc](https://www.cloudskillsboost.google/course_templates/53/video/509044)

- [YouTube: Lab Intro: Running Apache Spark jobs on Dataproc](https://www.youtube.com/watch?v=GozK5XOxBJ0)

person: Now it's time for our lab. In this lab, you'll be running Apache Spark jobs on Dataproc. Let's take a look at what you're going to do. First, you're going to migrate existing Spark job code to Dataproc. Then you'll be modifying your Spark jobs to use a different backend that's Cloud Storage instead of HDFS. Finally, you optimize Spark jobs to run on job specific clusters. Good luck.

### Lab - [Running Apache Spark jobs on Cloud Dataproc](https://www.cloudskillsboost.google/course_templates/53/labs/509045)

This lab focuses on running Apache Spark jobs on Cloud Dataproc.

- [ ] [Running Apache Spark jobs on Cloud Dataproc](../labs/Running-Apache-Spark-jobs-on-Cloud-Dataproc.md)

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/53/video/509046)

- [YouTube: Summary](https://www.youtube.com/watch?v=scT9YWRELko)

person: Welcome to the end of the module. Let's do a brief recap. You saw how you can run your entire Hadoop ecosystem on the Cloud with Dataproc. We covered the advantages of separating compute and storage for cost efficiency and performance by using cloud storage instead of HDFS. Lastly, we discussed how you can optimize Dataproc by resizing your cluster as your needs change and enable smart features like automatically turning down the cluster after a certain period of nonuse.

### Quiz - [Executing Spark on Dataproc](https://www.cloudskillsboost.google/course_templates/53/quizzes/509047)

#### Quiz 1.

> [!important]
> **Which of the following statements are true about Dataproc?
(Select all 2 correct answers)**
>
> - [ ] Helps you create job-specific clusters without HDFS
> - [ ] Streamlined API for Spark and Hadoop programming
> - [ ] Lets you run Spark and Hadoop clusters with minimal administration

#### Quiz 2.

> [!important]
> **Dataproc provides the ability for Spark programs to separate compute and storage by:**
>
> - [ ] Setting individual zones for compute and storage
> - [ ] Pre-copying data from Cloud Storage to persistent disk on cluster startup
> - [ ] Reading and writing data directly from/to Cloud Storage
> - [ ] Mirroring data on both Cloud Storage and HDFS

#### Quiz 3.

> [!important]
> **Match each of the terms with what they do when setting up clusters in Dataproc:
<table style='border: 1px solid black'><tr style='border: 1px solid black'><td style='border: 1px solid black'>Term</td><td style='border: 1px solid black'>Definition</td></tr><tr style='border: 1px solid black'><td style='border: 1px solid black'>__ 1. Zone</td><td style='border: 1px solid black'>A. Costs less but may not be available always</td></tr><tr style='border: 1px solid black'><td style='border: 1px solid black'>__ 2. Standard Cluster mode</td><td style='border: 1px solid black'>B. Determines the Google data center where compute nodes will be</td></tr><tr style='border: 1px solid black'><td style='border: 1px solid black'>__ 3. Preemptible</td><td style='border: 1px solid black'>C. Provides 1 primary and N workers</td></tr></table>**
>
> - [ ] B<br>C<br>A
> - [ ] A<br>B<br>C
> - [ ] C<br>A<br>B
> - [ ] C<br>B<br>A

## Serverless Data Processing with Dataflow

This module covers using Dataflow to build your data processing pipelines

### Video - [Module introduction](https://www.cloudskillsboost.google/course_templates/53/video/509048)

- [YouTube: Module introduction](https://www.youtube.com/watch?v=mkuugkL1ZZA)

Earlier in the course, you saw how to do batch data processing with Dataproc and other methods. Now it's time to introduce you to a key serverless tool that should be in your data engineering toolkit, Dataflow. This entire module will cover batch Dataflow pipelines and why Dataflow is a commonly used data pipeline tool on Google Cloud. Not to give away too much of the answer, but you can write the same code to do both batch and streaming pipelines with Dataflow. We'll cover streaming pipelines later. So the topics we will address are how to decide between Dataflow and Dataproc, why customers value Dataflow, Dataflow pipelines and Dataflow templates. Let's get started.

### Video - [Introduction to Dataflow](https://www.cloudskillsboost.google/course_templates/53/video/509049)

- [YouTube: Introduction to Dataflow](https://www.youtube.com/watch?v=qFoYF-VARQo)

person: Let's start by exploring Dataflow in more detail. The reason Dataflow is the preferred way to do data processing on Google Cloud is that Dataflow is serverless. You don't have to manage clusters at all. Unlike with Dataproc, the auto scaling in Dataflow scales step by step, it's very fine grained. Plus, as we will see in the next course, Dataflow allows you to use the same code for both batch and stream. This is becoming increasingly important. When building a new data processing pipeline, we recommend that you use Dataflow. If on the other hand, you have existing pipelines written using Hadoop technologies, it may not be worth it to rewrite everything, migrate it over to Google Cloud using Dataproc and then modernize it as necessary. As a data engineer, we recommend that you learn both Dataflow and Dataproc and make the choice based on what's best for a specific use case. If the project has existing Hadoop or Spark dependencies, use Dataproc. Please keep in mind that there are many subjective issues when making this decision, and that no simple guide will fit every use case. Sometimes the production team might be much more comfortable with a DevOps approach where they provision machines than with a serverless approach. In that case too, you might pick Dataproc. If you don't care about streaming and your primary goal is to move existing workloads, then Dataproc would be fine. Dataflow, however, is our recommended approach for building pipelines. Dataflow provides a serverless way to execute pipelines on batch and streaming data, it's scalable to process more data, Dataflow will scale out to more machines, it will do this transparently. The stream processing capability also makes it low latency, you can process the data as it comes in. This ability to process batch and stream with the same code is rather unique. For a long time, batch programming and data processing used to be two very separate and different things. Batch programming dates to the 1940s and the early days of computing where it was realized that you can think of two separate concepts, code and data. Use code to process data. Of course, both of these were on punch cards. So that's what you were processing, a box of punch cards call a batch. It was a job that started and ended when the data was fully processed. Stream processing on the other hand is more fluid. It arose in the 1970s with the idea of data processing being something that is ongoing. The idea is that data keeps coming in and you process the data, the processing itself tended to be done in micro batches. The genius of beam is that it provides abstractions that unify traditional batch programming concepts and traditional data processing concepts. Unifying programming and processing is a big innovation in data engineering. The four main concepts are P transforms, P collections, pipelines and pipeline runners. A pipeline identifies the data to be processed and the actions to be taken on the data. The data is held in a distributed data abstraction called a P collection. The P collection is immutable. Any change that happens in a pipeline ingests one P collection and creates a new one. It does not change the incoming P collection. The action or code is contained in an abstraction called a P transform. P transform handles input, transformation, an output of the data. The data in the P collection is passed along a graph from one P transform to another. Pipeline runners are analogous to container hosts such as Google Kubernetes Engine. The identical pipeline can be run on a local computer, data center VM, or on a service such as Dataflow in the Cloud. The only difference is scale and access to platform specific services. The services the runner uses to execute the code is called a backend system. Immutable data is one of the key differences between batch programming and data processing. Immutable data where each transform results in a new copy means there is no need to coordinate access control or sharing of the original ingest data. So it enables or at least simplifies distributed processing. The shape of a pipeline is not actually just a singular linear progression but rather a directed graph with brunches and aggregations. For historical reasons, we refer to it as a pipeline, but a data graph or Dataflow might be a more accurate description. A P collection represents both streaming data and batch data. There is no size limit to a P collection. Streaming data is an unbounded P collection that doesn't end. Each element inside a P collection can be individually accessed and processed. This is how distributed processing of the P collection is implemented. So you define the pipeline and the transforms on the P collection and the runner handles implementing the transformations on each element distributing the work as needed for scale and with available resources. Once an element is created in a P collection, it is immutable, so it can never be changed or deleted. Elements represent different data types. In traditional programs, a data type is stored in memory with a format that favors processing. Integers in memory are different from characters which are different from strings and compound data types. In a P collection, all data types are stored in a serialized state as byte strings. This way, there is no need to serialize data prior to network transfer and deserialize it when it is received. Instead, the data moves through the system in a serialized state and is only deserialized when necessary for the actions of a P transform.

### Video - [Why customers value Dataflow](https://www.cloudskillsboost.google/course_templates/53/video/509050)

- [YouTube: Why customers value Dataflow](https://www.youtube.com/watch?v=CZYcEYj1zwg)

person: So we've discussed what Dataflow is, but why do data engineers value Dataflow over other alternatives for data processing? To understand that, it helps to understand a bit about how Dataflow works. Dataflow provides an efficient execution mechanism for Apache Beam. The Beam pipeline specifies what has to be done. The Dataflow services chooses how to run the pipeline. The pipeline typically consists of reading data from one or more sources, applying processing to the data and writing it to one or more sinks. In order to execute the pipeline, the Dataflow service first optimizes the graph by, for example, fusing transforms together. It then breaks the jobs into units of work and schedules them to various workers. One of the great things about Dataflow is that the optimization is always ongoing. Units of work are continually rebalanced. Resources, both compute and storage, are deployed on demand and on a per job basis. Resources are torn down at the end of a job stage or on downscaling. Work scheduled on a resource is guaranteed to be processed. Work can be dynamically rebalanced across resources. This provides fault tolerance. The watermarking handles late arrivals of data and comes with restarts, monitoring and logging. No more waiting for other jobs to finish. No more preemptive scheduling. Dataflow provides a reliable, serverless, job-specific way to process your data. To summarize, the advantages of Dataflow are, first, Dataflow is fully managed and auto configured. Just deploy your pipeline. Second, Dataflow doesn't just execute the Apache Beam transforms as is. It optimizes the graph, fusing operations, as we see with C and D. Also, it doesn't wait for a previous step to finish before starting a new step. We see this with A and the Group By Key. Third, autoscaling happens step-by-step in the middle of a job. As the job needs more resources, it receives more resources. You don't have to manually scale resources to match job needs. If some machines have finished their tasks and others are still going on, the tasks queued up for the busy ones are rebalanced out to the idle machines. This way, the overall job finishes faster. Dynamic work rebalancing in mid-job removes the need to spend operational or analyst resource time hunting down hotkeys. All this happens while maintaining strong streaming semantics. Aggregations, like sums and counts, are correct even if the input source sends duplicate records. Dataflow is able to handle late arriving records. Finally, Dataflow functions as the glue that ties together many of the services on Google Cloud. Do you need to read from BigQuery and write to BigTable? Use Dataflow. Do you need to read from Pub/Sub and write to Cloud SQL? Use Dataflow.

### Video - [Building Dataflow pipelines in code](https://www.cloudskillsboost.google/course_templates/53/video/509051)

- [YouTube: Building Dataflow pipelines in code](https://www.youtube.com/watch?v=TTtl-h7y0jg)

person: Let's look in greater detail at an example Dataflow pipeline. Here's how to construct a simple pipeline where you have an input PCollection and pass it through three PTransforms and get an output PCollection. The syntax is shown in Python. You have the input, the pipe symbol, the first PTransform, the pipe symbol, the second PTransform, et cetera. The pipe operator essentially applies the transform to the input PCollection and sends out an output PCollection. The first three times, we don't give the output a name, simply pass it on the next step. The output of PTransform_3, though, we save into a PCollection variable named PCollection_out. In Java, it is the same thing, except that, instead of the pipe symbol, we use the apply method. If you want to do branching, just send the same PCollection through two different transforms. Give the output PCollection variable in each case a name. Then you can use it in the remainder of your program. Here, for example, we take the PCollection_in and pass the collection first through both PTransform_1 then through PTransform_2. The result of the first case, we store as PCollection_out_1. In the second case, we store it as PCollection_out_2. What we showed you so far is the middle part of a pipeline. You already had a PCollection, and you applied a bunch of transforms, and you end up with a PCollection, but where does the pipeline start? How do you get the first PCollection? You get it from a source. What does a pipeline do with the final PCollection? Typically, it writes out to a sink. That's what we are showing here. This is Python. We create a PCollection by taking the pipeline object P and passing it over a text file in cloud storage. That's the read from text line. Then, we apply the PTransform called FlatMap to the lines read from the text file. What FlatMap does is that it applies a function to each row of the input and concatenates all the outputs. When the function is applied to a row, it may return zero or more elements that go to the output PCollection. The function in this case is the function called count_words. It takes a line of text and returns an integer. The output PCollection then consists of a set of integers. These integers are written to a text file in cloud storage. Because the pipeline was created in a with clause and because this is not a streaming pipeline, exiting the with clause automatically stops the pipeline. Once you have written the pipeline, it's time to run it. Executing the Python program on the previous slide will run the program. By default, the program is run using the default runner, which runs on the same machine where the Python program was executed. When you create the pipeline, you can pass in a set of options. One of these options is the runner. Specify that as Dataflow to have the pipeline run on Google Cloud. This example contains hard coded variables, which in most cases is not a preferred practice for programming at scale. Of course, normally you will set up command line parameters to transparently switch between local and cloud. Simply running main runs the pipeline locally. To run on cloud, specify cloud parameters.

### Video - [Key considerations with designing pipelines](https://www.cloudskillsboost.google/course_templates/53/video/509052)

- [YouTube: Key considerations with designing pipelines](https://www.youtube.com/watch?v=803SB_xs-DU)

person: To design pipelines, you need to know how each step works on the individual data elements contained inside of a PCollection. Let's start with the input and outputs of the pipeline. First, we set up our Beam pipeline with beam. Pipeline and pass through any options. Here, we'll call the pipeline P. Now it's time to get some data as input. If we wanted to read a series of CSV files in Cloud Storage, we could use beam.io. ReadFromText and simply parse in the Cloud Storage bucket and file name. Note the use of an asterisk wild card can handle multiple files. If we wanted to read instead from a Pub/Sub topic, you would still use beam.io, but instead it's ReadStringsFromPubSub, and you'd have to parse in the topic name. What about if you wanted to read in data that's already in BigQuery? Here's how that would look. You'd prepare your SQL query and specify BigQuery as your input source and then parse in the query and source as a read function to Dataflow. These are just a few of the data sources from which Dataflow can read. But now what about writing to sinks? Take the BigQuery example but as a data sink this time. With Dataflow, you can write to a BigQuery table, as you can see here. First, you establish the reference to the BigQuery table with what BigQuery expects, your project ID, data set ID and table name. Then you use beam.io. WriteToBigQuery as a sink to your pipeline. Note that we are using the normal BigQuery options here for write_disposition. Here, we're truncating the table if it exists, meaning to drop data rows. If the table doesn't exist, we can create it if needed. Naturally, this is a batch pipeline if we're truncating the table with each load. You can also create a PCollection in memory without reading from a particular source. Why might you do this? If you have a small data set, like a lookup table or a hard coded list, you could create the PCollection yourself, as you can see here. Then we can call a pipeline step on this new PCollection just as if we sourced it from somewhere else.

### Video - [Transforming data with PTransforms](https://www.cloudskillsboost.google/course_templates/53/video/509053)

- [YouTube: Transforming data with PTransforms](https://www.youtube.com/watch?v=o8nfhWZ1fd4)

person: Now that we have looked at how to get the data in, let's look at how we transform each data element in the PCollection with PTransforms. The first step of any map produced process is the map phase, where you're doing something in parallel. In the word length example, there is one length output for each word input, so the word dog would map to three for length. In the bottom graph example, the function my_grep returns each instance of the term it's searching for in the line. There may be multiple instances of the term in a single line in a one-to-many relationship. In this case, you may want my_grep to return the next instance each time it's called, which is why the function has been implemented with a generator using yields. The yield command has the effect of preserving the seed of the function so that the next time it's called, it can continue from where it left off. FlatMap has the effect of iterating over one-to-many relationships. The map example returns a key value pair. In Python, this is simply a two tuple for each word. The FlatMap example yields the line only for lines that contain the search term. ParDo is a common intermediate step in a pipeline. You might use it to extract certain fields from a set of raw input records or convert raw input into a different format. You might also use ParDo to convert process data into an output format, like table rows for BigQuery or strings for printing. You can use ParDo to consider each element in a PCollection and either output that element to a new collection or discard it. If your input PCollection contains elements that are of a different type or format than you want, you can use ParDo to perform a conversion on each element and output the result to a new PCollection. If you have a PCollection of records with multiple fields, for example, you can use a ParDo to parse out just the fields you want to consider into a new PCollection. You can use ParDo to perform simple or complex computations on every element or certain elements of a PCollection and output the results as a new PCollection. When you apply a ParDo transform, you need to provide code in the form of a DoFn object. A DoFn is a Beam SDK class that defines a distributed processing function. Your DoFn code must be fully serializable, item potent and thread safe. In this example, we're just counting the number of words in a line and returning the length of the line. Transformations are always going to work on one element at a time here. Here we have an example from Python which can return multiple variables. In this example, we have below and above, some cutoff in our data elements and return two different types, below and above, two different variables by referencing these properties of the results.

### Video - [Lab Intro: Building a Simple Dataflow Pipeline](https://www.cloudskillsboost.google/course_templates/53/video/509054)

- [YouTube: Lab Intro: Building a Simple Dataflow Pipeline](https://www.youtube.com/watch?v=ZGpz8TlkSzs)

Person: Next, let's do a lab: A simple Dataflow pipeline to perform serverless data analysis using Python or Java. You can select which version of the lab you'd like to do.

### Lab - [A Simple Dataflow Pipeline (Python) 2.5](https://www.cloudskillsboost.google/course_templates/53/labs/509055)

In this lab, you learn how to write a simple Dataflow pipeline and run it both locally and on the cloud.

- [ ] [A Simple Dataflow Pipeline (Python) 2.5](../labs/A-Simple-Dataflow-Pipeline-(Python)-2.5.md)

### Lab - [Serverless Data Analysis with Dataflow: A Simple Dataflow Pipeline (Java)](https://www.cloudskillsboost.google/course_templates/53/labs/509056)

In this lab you will open a Dataflow project, use pipeline filtering, and execute the pipeline locally and on the cloud using Java.

- [ ] [Serverless Data Analysis with Dataflow: A Simple Dataflow Pipeline (Java)](../labs/Serverless-Data-Analysis-with-Dataflow-A-Simple-Dataflow-Pipeline-(Java).md)

### Video - [Aggregate with GroupByKey and Combine](https://www.cloudskillsboost.google/course_templates/53/video/509057)

- [YouTube: Aggregate with GroupByKey and Combine](https://www.youtube.com/watch?v=Sjc5AxUWm4A)

Now, let's look at more capabilities of the Dataflow model. What do you do after the map phase? The unnamed phase is the shuffle phase where you group together like keys. This works on a PCollection of key-value pairs or two elements tuples. Groups by common key and returns a single key-value pair where the value is actually a group of values. The idea here is that we want to find all the zip codes associated with the city. For example, New York is a city and it may have one-zero-zero-zero-one and one-zero-zero-zero-two zip codes. You could first create a key-value pair and a ParDo and then group by the key. The resulting key-value pairs are simply two tuples. We do have to be aware of data skew on we're doing this. When the same example is scaled up in the presence of skewed data, the situation becomes much worse. Let's say that you're doing your GroupByKey, but your group has 1 million items in it. One million is not too big of a deal on modern hardware, but with one billion you're forcing all of those elements to go to a single work group to be counted. This could definitely run into some issues on the network. This is the same performance concern when doing high cardinality group by queries on billions of records in BigQuery. In this example, there are a million X values and only a thousand Y values. GroupByKey will group all of the X values on one worker. The worker will take much longer to do its processing on the million values than the other worker, which only has a thousand values to process. Of course, you're paying for the worker that sits idle waiting for the other worker to complete. Dataflow is designed to avoid efficiencies by keeping the data balance. You can help by designing your application to divide work into aggregation steps and subsequent steps and to avoid grouping or to push grouping towards the end of the processing pipeline. CoGroupByKey is very similar. It groups results across several PCollections by key. The result for each key is a tuple of the values associated with that key in each input collection. Now, we can move to the reduce phase. How do we calculate totals or averages or other aggregations on our PCollections? Combined is used to combine collections of elements or values in your data. Combine has variants that work on entire PCollections and some that combine the values for each key and PCollections of key-value pairs. CombineGloballlyfn reduces a PCollection to a single value by applying the FN or the function. CombinePerKey is similar to GroupByKey, but combines the values by a combined function or a callable that takes an iterable action such as sum or max. When you apply a combine transform, you must provide the function that contains the logic for combining the elements or values. There are pre-built combined functions for common numeric combination operations such as sum, min, and max. Simple combine operations such as sums can usually be implemented as a simple function. More complex combination operations might require you to create a subclass of a combine function that has an accumulation type distinct from the input and or output site. The combining function should be commutative and associative, as the function is not necessarily invoked exactly once on all values within a given key. Because the input data including the value collection may be distributed across multiple workers, the combining function might be called multiple times to perform multiple combining on subsets of the value collection. For more complex combined functions, you can define a subclass of combine function. You should use the combine function if the action needed requires a more sophisticated accumulator, must perform additional pre or post processing, might change the output type, or takes the key into account. A general combining operation consists of four operations. When you create a subclass of combine function, you must provide four operations by overriding the corresponding methods. Create accumulator creates a new local accumulator. In the example case taking a mean average, a local accumulator tracks the running sum of values. Combine is orders of magnitude faster than GroupByKey because Dataflow knows how to parallelize a combine step. The way that GroupByKey works, Dataflow can use no more than one worker per key. In this example, GroupByKey causes all the values to be shuffled so they are all transmitted over the network. And then there is one worker for the 'x' key and one worker for the 'y' key. Combine allows Dataflow to distribute a key to multiple workers and process it in parallel. In this example, CombineByKey first aggregates values and then processes the aggregates with multiple workers. Also, only 6 aggregate values need to be passed over the network. Combine is a Java interface that tells Dataflow that the combine operation (like Count) is both commutative and associative. This allows Dataflow to shard within a key vs. having to group each key first. As a developer, you can create your own custom Combine class for any operation that has commutative and associative properties. Flatten works a lot like a SQL UNION. It's a beam transform for PCollection objects that store the same data type. Flatten merges multiple PCollection objects into a single logical PCollection. Partition is also a beam transform for PCollection objects that store the same data type. Partition splits a single PCollection into a fixed number of smaller collections. You might use partition if, for example, you wanted to calculate percentages or quartiles and the top quartile has different processing than all the others.

### Video - [Lab Intro: MapReduce in Beam](https://www.cloudskillsboost.google/course_templates/53/video/509058)

- [YouTube: Lab Intro: MapReduce in Beam](https://www.youtube.com/watch?v=2zZgnzZVutM)

In this next lab, you'll practice creating and performing Map and Reduce operations on PCollections as part of your pipeline. You can choose between Python and Java. You'll first identify the Map and Reduce operations, then execute the pipeline, and lastly, modify command-line parameters.

### Lab - [MapReduce in Beam (Python) 2.5](https://www.cloudskillsboost.google/course_templates/53/labs/509059)

In this lab, you learn how to use pipeline options and carry out Map and Reduce operations in Dataflow.



- [ ] [MapReduce in Beam (Python) 2.5](../labs/MapReduce-in-Beam-(Python)-2.5.md)

### Lab - [Serverless Data Analysis with Beam: MapReduce in Beam (Java)](https://www.cloudskillsboost.google/course_templates/53/labs/509060)

In this lab you will identify Map and Reduce operations, execute the pipeline, use command line parameters.

- [ ] [Serverless Data Analysis with Beam: MapReduce in Beam (Java)](../labs/Serverless-Data-Analysis-with-Beam-MapReduce-in-Beam-(Java).md)

### Video - [Side inputs and windows of data](https://www.cloudskillsboost.google/course_templates/53/video/509061)

- [YouTube: Side inputs and windows of data](https://www.youtube.com/watch?v=VB_NQJGjPk0)

Person: In this lesson, you'll learn about the role of side inputs and windows. In addition to the main input PCollection, you can provide additional inputs to a ParDo transform in the form of side inputs. A side input is an additional input that your do function can access each time it processes an element in the input PCollection. When you specify a side input, you create a view of some other data that can be read from within the ParDo transform's do function while processing each element. Side inputs are useful if your ParDo needs to inject additional data when processing each element in the input PCollection, but the additional data needs to be determined at runtime and not hard coded. Such values might be determined by the input data or depend on a different branch of your pipeline. Here's how side inputs work. This is an example in Python. This set of steps is actually a subgraph of our overall graph. It begins with words that run through the map function to get the length and then combine globally to compute the total lengths across the whole data set. So if we were trying to figure out if any given word is shorter or longer than the average word length, first we need to compute the average word length using these steps. But then this whole branch can be fed into this method. That's what creates the view which is static and then becomes available to all the worker nodes for later use. That is a side input you see here. Before we go to the next lab, here are a few notes about additional capabilities. Many transforms have two parts. One occurs item at a time until all items are processed, and another occurs after the last item is processed. One of the easiest analogies is the arithmetic mean. You can add up the value of each element and keep count. This is the accumulation step. After you have processed all the elements, you have a total of all the values read and a count of the number of values read. The last thing to do is divide the total by the count. This is fine so long as you know you have read the last item, but if you have an unbounded data set, there is no predetermined end, so you just keep adding and never break out of the loop and perform the division. The global window is not very useful for an unbounded PCollection, meaning streaming data. The timing associated with the elements in an unbounded PCollection is usually important to processing the data. An unbounded PCollection has no defined end or last element, so it can never perform the completion step. This is particularly important for GroupByKey and Combine, which perform the shuffle after end. The discussion about unbounded PCollections and Windows will be continued in the course on processing streaming data. The global window is a default, and here, you can see how you can set it with beam. WindowInto(window. GlobalWindows). So are streaming pipelines out of luck if they can't use the global window? No. You can use time-based windows which can be useful for processing data that comes in streaming at different times. We'll cover this in detail in the streaming course. For batch inputs, you can group by time as well. You can explicitly admit a time stamp in your pipeline instead of standard output. In this example, an offline access log is being read, and the date/time stamp is extracted and used for windowing. Here, we're using Windows to aggregate our batch data by time. Subsequent groups, aggregations and so forth are computed only within the time window. This example here uses a sliding window. As you can see, with beam. WindowInto(bean.window. SlidingWindows(60, 30)), which means capture 60 seconds worth of data but start a new window every 30 seconds. So for example, say you had all of your sales records, and you wanted to compute sales by day. You'd just extract that time stamp field that represents the time stamp. Then you would create fixed windows with a 1-day duration, and Dataflow automatically will compute the sum over each window to computer those totals. The main thing to remember here is that you can do this in batch. Discussion of streaming continues in the streaming data processing course.

### Video - [Lab Intro: Serverless Data Analysis with Dataflow: Side Inputs](https://www.cloudskillsboost.google/course_templates/53/video/509062)

- [YouTube: Lab Intro: Serverless Data Analysis with Dataflow: Side Inputs](https://www.youtube.com/watch?v=VTHXLA82Ark)

person: In this lab, you'll practice creating siphon pods to your dataflow pipeline. You once again have the choice to complete the lab using Python or Java. Specifically, you'll bring in data from BigQuery into your pipeline and then execute the job on Dataflow.

### Lab - [Serverless Data Analysis with Dataflow: Side Inputs (Python)](https://www.cloudskillsboost.google/course_templates/53/labs/509063)

In this lab you will try out a BigQuery query, explore the pipeline code, and execute the pipeline using Python.

- [ ] [Serverless Data Analysis with Dataflow: Side Inputs (Python)](../labs/Serverless-Data-Analysis-with-Dataflow-Side-Inputs-(Python).md)

### Lab - [Serverless Data Analysis with Dataflow: Side Inputs (Java)](https://www.cloudskillsboost.google/course_templates/53/labs/509064)

In this lab you will try out a BigQuery query, explore the pipeline code, and execute the pipeline using Java.

- [ ] [Serverless Data Analysis with Dataflow: Side Inputs (Java)](../labs/Serverless-Data-Analysis-with-Dataflow-Side-Inputs-(Java).md)

### Video - [Creating and re-using pipeline templates](https://www.cloudskillsboost.google/course_templates/53/video/509065)

- [YouTube: Creating and re-using pipeline templates](https://www.youtube.com/watch?v=MxFxa2MUG7A)

person: Next, we'll look at Dataflow Templates where you as a data engineer can create new templates for your team to leverage. You can also start from some of Google's pre-existing templates which we'll cover as well. Dataflow Templates allow users who don't have any coding capability to execute their Dataflow job. It enables the rapid deployment of standard types of data transformation jobs, removing the need to develop the pipeline code and removing the need to consider the management of components dependencies in the pipeline code. In the traditional workflow, the developer creates the pipeline in the development environment using the Dataflow SDK in Java or Python, and there are dependencies to the original language and SDK files. Whenever a job is submitted, it is reprocessed entirely or recompiled. There is no separation of developers from users, so the users basically have to be developers or have the same access and resources as developers. Dataflow Templates enable a new development in execution workflow. The templates help separate the development activities and the developers from the execution activities and the users. The user environment no longer has dependencies back to the development environment. The need for recompilation to run a job is limited. The new approach facilitates the scheduling of batch jobs and opens up more ways for users to submit jobs and more opportunities for automation. App developers, database administrators, analysts and data scientists can use Templates as a solution. You can also run them using the command-line tool or REST API as you see here. Simply specify the Cloud Storage location of your template that you already have. Alternatively, you can use the Google provided templates. After you create and stage your Dataflow template, execute the template with the Cloud Console, REST API, or gcloud command-line tool. You can deploy Dataflow Template jobs from many environments, including App Engine Standard Environment, Cloud Functions and other constrained environments. What if you wanted to create your own template? To create your own template, you'll add your own value providers. This is what parses the command-line or optional arguments to your template, and that is how users can specify optional arguments. Once a template file is created, you call it from an API. You might not have considered this before, but values like user options and input file that are compiled into your job, they aren't just parameters. They are compiled time parameters. To make these values available to non-developer users, they have to be converted to runtime parameters. These work through the Value Provider interface so that your users can set these values when the template is submitted. Value Provider can be used in IO, transformations and your functions. There are also static and nested versions of Value Provider for more complex cases. This is a Java example for creating your own template. Note that Value Providers are passed down throughout the whole pipeline construction phase. Sometimes we need to transform a value from what the user passes at runtime to what a source or sync expects to consume. Nested value providers meet this need. Each template has associated metadata with it upon creation. This will help your downstream users know what your template is doing and what parameters it expects. The metadata file is located in the same directory as your template and simply has the underscore metadata suffix to the name.

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/53/video/509066)

- [YouTube: Summary](https://www.youtube.com/watch?v=qXOXH7GelWU)

person: Earlier in the course, you learned how to do batch processing of your Hadoop and Spark jobs using Dataproc. This is an ideal first step through the cloud for existing jobs. Simply run them on Dataproc, and they just work. You learned that Dataflow takes a lot of the cluster resizing and other management tasks and automates them for you as a true serverless product. Use Dataflow if you're writing new pipelines or if you're ready to rewrite and migrate your Hadoop jobs to faster processing with Apache Beam on Dataflow. You then saw how to build pipelines using Apache Beam, which is open source. For the pipelines to work, we created inputs with a Beam.io syntax and walked through how you can read CSV files from Cloud Storage, streaming message queues from Pub/Sub and structured data already living in BigQuery. We then looked at some key considerations when designing your pipeline. Recall that you should consider using combine when you can instead of GroupByKey, especially if your data is heavily skewed. This will prevent a single worker from being a bottleneck if you have a high cardinality data set. To do the actual transformations, you practiced writing PTransforms in your labs. Remember that the P in PTransforms and PCollections means parallel. Recall that the PCollection itself is immutable. Data is never processed in place. A new PCollection is always created, and the individual elements of a PCollection are massively distributed over many workers to perform the parallel transform. This is a whole map part of map reduce. For the reduce part of map reduce, we looked at aggregation functions like GroupByKey and combine. Keep in mind you can have multiple parallel parts of your pipeline combine into a single PTransform like in aggregation. The pipeline does not have to execute in serial unless you've set it up that way with dependencies. After that, you practiced with side inputs in your lab and how to create windows of data even for batch data sets. Lastly, you saw how to create and save new Dataflow templates for your team to use and where you can see Google's premade templates in our public GitHub.

### Quiz - [Serverless Data Processing with Dataflow](https://www.cloudskillsboost.google/course_templates/53/quizzes/509067)

#### Quiz 1.

> [!important]
> **Match each of the Dataflow terms with what they do in the life of a dataflow job:
<table style='border: 1px solid black'><tr style='border: 1px solid black'><td style='border: 1px solid black'>Term</td><td style='border: 1px solid black'>Definition</td></tr><tr style='border: 1px solid black'><td style='border: 1px solid black'>__ 1. Transform</td><td style='border: 1px solid black'>A. Output endpoint for your pipeline</td></tr><tr style='border: 1px solid black'><td style='border: 1px solid black'>__ 2. PCollection</td><td style='border: 1px solid black'>B. A data processing operation or step in your pipeline</td></tr><tr style='border: 1px solid black'><td style='border: 1px solid black'>__ 3. Sink</td><td style='border: 1px solid black'>C. A set of data in your pipeline</td></tr></table>**
>
> - [ ] C<br>B<br>A
> - [ ] B<br>A<br>C
> - [ ] A<br>C<br>B
> - [ ] B<br>C<br>A

#### Quiz 2.

> [!important]
> **Which of the following statements are true?
(Select all 2 correct responses)**
>
> - [ ] Side-inputs in Dataflow are a way to export data from one pipeline to share with another pipeline
> - [ ] Dataflow transforms support both batch and streaming pipelines
> - [ ] Map operations in a MapReduce can be performed by Combine transforms in Dataflow
> - [ ] Dataflow executes Apache Beam pipelines

## Manage Data Pipelines with Cloud Data Fusion and Cloud Composer

This module shows how to manage data pipelines with Cloud Data Fusion and Cloud Composer.  

### Video - [Module introduction](https://www.cloudskillsboost.google/course_templates/53/video/509068)

- [YouTube: Module introduction](https://www.youtube.com/watch?v=wr8BwLkSDLg)

Person: In this module, we will discuss how to manage data pipelines with Cloud Data Fusion and Cloud Composer. Specifically, we will look at how you can use Cloud Data Fusion to visually build data pipelines and how you can use Cloud Composer to orchestrate work between Google Cloud services.

### Video - [Introduction to Cloud Data Fusion](https://www.cloudskillsboost.google/course_templates/53/video/509069)

- [YouTube: Introduction to Cloud Data Fusion](https://www.youtube.com/watch?v=vUqw62xOF-c)

Let’s start with an introduction to Cloud Data Fusion. Cloud Data Fusion provides a graphical user interface and APIs that increase time efficiency and reduce complexity. It equips business users, developers, and data scientists to quickly and easily build, deploy, and manage data integration pipelines. Cloud Data Fusion is essentially a graphical no code tool to build data pipelines. Cloud Data Fusion is used by developers, data scientists, and business analysts alike. For developers, Cloud Data Fusion allows you to cleanse, match, remove duplicates, blend, transform, partition, transfer, standardize, automate, and monitor data. Data scientists can use Cloud Data Fusion to visually build integration pipelines, test, debug, and deploy applications. Business analysts can run Cloud Data Fusion at scale on Google Cloud, operationalized pipelines, and inspect rich integration metadata. Cloud Data Fusion offers a number of benefits. Integrate with any data - through a rich ecosystem of connectors for a variety of legacy and modern systems, relational databases, file systems, cloud services, object stores, NoSQL, EBCDIC, and more. Increase productivity - If you have to constantly move between numerous systems to gather insight, your productivity is significantly reduced. With Cloud Data Fusion, your data from all the different sources can be pooled into a view like in BigQuery, Spanner, or any other Google Cloud technologies, allowing you to be more productive faster. Reduce complexity - through a visual interface for building data pipelines, code free transformations, and reusable pipeline templates. Increase flexibility - through support for on-prem and cloud environments, interoperability with the Open source software CDAP. At a high level, Cloud Data Fusion provides you with a graphical user interface to build data pipelines with no code. You can use existing templates, connectors to Google Cloud, and other Cloud services providers, and an entire library of transformations to help you get your data in the format and quality you want. Also, you can test and debug the pipeline and follow along with each node as it receives and processes data. As you will see in the next lesson, you can tag pipelines to help organize them more efficiently for your team, and you can use the unified search functionality to quickly find field values or other keywords across your pipelines and schemas. Lastly, we’ll talk about how Cloud Data Fusion tracks the lineage of transformations that happen before and after any given field on your dataset. One of the advantages of Cloud Data Fusion is that it's extensible. This includes the ability to templatize pipelines, create conditional triggers, and manage and templatize plugins. There is a UI widget plug-in as well as custom provisioners, custom compute profiles, and the ability to integrate to hubs.

### Video - [Components of Cloud Data Fusion](https://www.cloudskillsboost.google/course_templates/53/video/509070)

- [YouTube: Components of Cloud Data Fusion](https://www.youtube.com/watch?v=130K-Pc2VjQ)

The two major user interface components we will focus our attention on in this course, are the Wrangler UI for exploring data sets visually, and building pipelines with no code, and the Data Pipeline UI for drawing pipelines right on to a canvas. You can choose from existing templates for common data processing tasks like Cloud Storage to BigQuery. There are other features of Cloud Data Fusion that you should be aware of too. There's an integrated rules engine where business users can program in their pre-defined checks and transformations, and store them in a single place. Then data engineers can call these rules as part of a rule book or pipeline later. We mentioned data lineage as part of field metadata earlier. You can use the metadata aggregator to access the lineage of each field in a single UI and analyze other rich metadata about your pipelines and schemas as well. For example, you can create and share a data dictionary for your schemas directly within the tool. Other features, such as the microservice framework, allow you to build specialized logic for processing data. You can also use the Event Condition Action (ECA) Application to parse any event, trigger conditions, and execute an action based on those conditions.

### Video - [Cloud Data Fusion UI](https://www.cloudskillsboost.google/course_templates/53/video/509071)

- [YouTube: Cloud Data Fusion UI](https://www.youtube.com/watch?v=YUfhAan9z8Y)

person: Managing your pipelines is easiest when you have the right tools. We'll now take a high level look at the Cloud Data Fusion UI as you saw in the component overview. Here are some of the key user interface elements that you will encounter when using Data Fusion. Let's look at each of them in turn. Under Control Center is the section for applications, artifacts and a data set. Here you could have multiple pipelines associated with a particular application. The Control Center gives you the ability to see everything at a glance and search for what you need, whether it's a particular data set, pipeline or other artifact, like a data dictionary, for example. Under the Pipeline section, you have a developer studio. You can preview, export, schedule a job or project. You also have a connector and a function palette and a navigation section. Under the Wrangler section, you have connections, transforms, data quality, insights and functions. Under the Integration metadata section, you can search, add tags and properties and see the data lineage for field and data. The Hub allows you to see all the available plugins, sample use cases and prebuilt pipelines. Entities include the ability to create pipelines, upload an application, plugin, driver, library and directives. There are two components in Administration, management and configuration. Under management, you have services and metrics. Under configuration, you have namespace, compute profiles, preferences, system artifacts and the REST client.

### Video - [Build a pipeline](https://www.cloudskillsboost.google/course_templates/53/video/509072)

- [YouTube: Build a pipeline](https://www.youtube.com/watch?v=Pv2LPlAaixs)

>> Now that we've looked at the components in the UI, we'll discuss the process of building a data pipeline. A pipeline is represented visually as a series of stages arranged in a graph. These graphs are called DAGs, or Directed Acyclic Graphs, because they flow from one direction to another, and they cannot feed into themselves. Acyclic simply means not a circle. Each stage is a node. And as you can see here, nodes can be of a different type, you may start with a node that pulls data from Cloud Storage, then passes it on to a node that parses a CSV. The next node takes multiple nodes, has an input and joins them together before passing the joined data to two separate data sink nodes. As you saw in our previous example, you can have multiple nodes fork out from a single parent node. This is useful because you may want to kick off another data processing work stream that should not be blocked by any processing on a separate series of nodes. You can combine data from two or more nodes into a single output in a sink. In Cloud data fusion, the studio is the user interface where you author and create new pipelines. The area where you create nodes and chain them together in your pipeline is your canvas. If you have many nodes in a pipeline, the Canvas can get visually cluttered, so use the mini map to help navigate around a huge pipeline quickly. You can interact with the canvas and add objects by using the Canvas control panel. When you're ready to save and run the entire pipeline, you can do so with the pipeline actions toolbar at the top. Don't forget to give your pipeline a name and description, as well as make use of the many pre-existing templates and plugins so you don't have to write your pipeline from scratch. Here, we've used a template or data pipeline batch, which gives us the three nodes you see here to move data from a Cloud storage file, process it in a wrangler and output it to BigQuery. You should make use of preview mode before you deploy and run your pipeline in production to ensure everything you run will run properly. While a pipeline is in preview, you can click on each node and see any sample data or errors that you will need to correct before deploying. After deployment. You can monitor the health of your pipeline and collect key summary stats of each execution. Here, we're ingesting data from Twitter and Google Cloud and parsing each tweet before loading them into a variety of data sinks. If you have multiple pipelines, it's recommended that you make liberal use of the tags feature to help you quickly find and organize each pipeline for your organization. You can view the start time, the duration of the pipeline run and the overall summary across runs for each pipeline. You can quickly see the data throughput at each node in the pipeline simply by interacting with the node. Note the compute profile used in the Cloud. Clicking on a node gives you detail on the inputs, outputs and errors for that given node. Here, we are integrating with the speech to text API to process audio files into searchable text. You can track the individual health of each node and get useful metrics like records out per second, average processing time, and Max processing time, which can alert you to any anomalies in your pipeline. You can set your pipelines to run automatically at certain intervals. If your pipeline normally takes a long time to process the entire data set, you can also specify a maximum number of concurrent runs to help avoid processing data unnecessarily. Keep in mind that Cloud data fusion is designed for batch data pipelines. We'll dive into streaming data pipelines in future modules. One of the big features of Cloud data fusion is the ability to attract the lineage of a given field value. Let's take this example of a campaign field for double click data set and track every transform operation that happened before and after this field. Here, you can see the lineage of operations that are applied to the campaign field between the campaign dataset and the double click dataset. Note the time this field was last changed by a pipeline run and each of the input fields and descriptions that interacted with the field as part of processing it between datasets. Imagine the use cases if you've inherited a set of analytical reports and you want to walk back upstream all of the logic that went into a certain field. Well, now you can.

### Video - [Explore data using wrangler](https://www.cloudskillsboost.google/course_templates/53/video/509073)

- [YouTube: Explore data using wrangler](https://www.youtube.com/watch?v=MpkqVec7F6Q)

person: We've discussed the core components, tools and processes of building data pipelines. Now we'll look at using Wrangler to explore the data set. So far in the course, we have focused on building new pipelines for our data sets. That presumes we know what the data is and what transformations need to be made already. Oftentimes, a new data set still needs to be explored and analyzed for insights. The Wrangler UI is the cloud data fusion environment for exploring new data sets visually for insights. Here, you can inspect the data set and build a series of transformation steps called directives to stitch together a pipeline. Here's what the Wrangler UI looks like. Starting from the left, you have your connections to existing data sets. You can add new connections to a variety of data sources like Google cloud storage, BigQuery or even other cloud providers. Once you specify your connection, you can browse all of the files and tables in that source. Here, you can see a cloud storage bucket of demo data sets and all the CSV files of customer complaints. Once you've found an example data set like customers.csv here, you can explore the rows and columns visually and view sample insights. As you explore the data, you might want to create new calculated fields, drop columns, filter rows or otherwise wrangle the data. You can do so using the Wrangler UI by adding new directives to form a data transformation recipe. When you're happy with your transformations, you can create a pipeline that you can then run at regular intervals.

### Video - [Lab Intro: Building and executing a pipeline graph in Cloud Data Fusion](https://www.cloudskillsboost.google/course_templates/53/video/509074)

- [YouTube: Lab Intro: Building and executing a pipeline graph in Cloud Data Fusion](https://www.youtube.com/watch?v=Tjk92FB66eo)

- Now it's time for you to practice building and executing a pipeline graph in Cloud Data Fusion. In this lab, you will connect Cloud Data Fusion to a couple of data sources, apply basic transformations, join two data sources, and write data to a sink.

### Lab - [Building and Executing a Pipeline Graph with Data Fusion 2.5](https://www.cloudskillsboost.google/course_templates/53/labs/509075)

This tutorial shows you how to use the Wrangler and Data Pipeline features in Cloud Data Fusion to clean, transform, and process taxi trip data for further analysis.

- [ ] [Building and Executing a Pipeline Graph with Data Fusion 2.5](../labs/Building-and-Executing-a-Pipeline-Graph-with-Data-Fusion-2.5.md)

### Video - [Orchestrate work between Google Cloud services with Cloud Composer](https://www.cloudskillsboost.google/course_templates/53/video/509076)

- [YouTube: Orchestrate work between Google Cloud services with Cloud Composer](https://www.youtube.com/watch?v=oU_g-_RWfro)

person: The next big task for managing data pipelines is to orchestrate the work across multiple Google Cloud services. For example, if you have three cloud data fusion pipelines, and two ML models that you wanted to run in a certain order, you need an orchestration engine. In this module, we'll look at using Cloud Composer to help out with tasks like that. Cloud Composer will control the Google Cloud services that we need to run. What Cloud Composer is simply a serverless environment on which an open source workflow tool runs. That workflow tool is called Apache airflow, which is an open source orchestration engine. The heart of any workflow is D.A.G. As you saw with cloud data fusion, You're also building D.A.G.s with Apache Airflow as you see here. What's happening in this particular D.A.G. are four tasks that update our training data, export it, retrain our model, and we deploy it. You can tell your D.A.G. to do pretty much anything you need it to do. Here, it's sending tasks to BigQuery, cloud storage, and Vertex AI. But yours could orchestrate among four completely different services.

### Video - [Apache Airflow environment](https://www.cloudskillsboost.google/course_templates/53/video/509077)

- [YouTube: Apache Airflow environment](https://www.youtube.com/watch?v=7blDDS8BqDU)

person: Let's preview the actual Cloud Composer environment. Once you use the command line or Google Cloud web UI to launch a Cloud Composer instance, you'll be met with a screen like this. Keep in mind that you can have multiple Cloud Composer environments, and with each environment, you can have a separate Apache Airflow instance, which could have zero to many DAGs. An important note here is that sometimes you'll be required to edit environment variables for your workflows, like specifying your specific Google Cloud project account. Normally, you will not do that at the Cloud Composer level but on the actual Apache Airflow instance level. Again, generally, you're only on the Cloud Composer page here to create new environments before you launch directly into the Airflow web server. To access the Airflow admin UI, where you can monitor and interact with your workflows, you'll click on the link underneath Airflow webserver. The second box you see is the DAGs folder, which is where the code of your actual workflows will be stored. The DAGs folder for each airflow instance is simply a cloud storage bucket that is automatically created for you when you create your Cloud Composer instance. Here is where you upload your DAG files written in Python, and bring your first workflow to life in Airflow.

### Video - [DAGs and Operators](https://www.cloudskillsboost.google/course_templates/53/video/509078)

- [YouTube: DAGs and Operators](https://www.youtube.com/watch?v=32rPE4kq6TQ)

>> Now that you're familiar with the basic environment setup, it's time to discuss your primary artifact, which is your DAG and the operators you're using to call whichever services you want to send tasks to. First, airflow workflows are written in Python, you'll have one Python file for each DAG. For example, here we have simple _load_dag.py in our DAG folder Cloud Storage bucket, and you can see a preview of what the DAG file looks like. Don't worry about reading the code, we'll go into that later. It's sufficient enough for now to just know that there are a series of user created tasks in each DAG file that invoke predefined operators, like this task, which uses the data flow Python operator and is given the task ID of Process Delimited and Push. We'll go over creating a DAG file and its components a little later. Once you've uploaded the Python file to the DAG's folder, you can navigate back to the airflow web server, and under DAGs, you'll see the DAG you created with code represented visually as a directed graph with nodes and edges. You'll remember that the Python code that defined a task we called Process Delimited and Push is now a node in our graph here. Let's explore a bit more of the airflow Web UI. You can see that this particular workflow is called GCS to BigQuery Triggered and it has three tasks when it runs. One, process delimited and push. I just happen to know from that Python file you saw earlier that this task invokes a Dataflow job to read in a new CSV file from a Cloud Storage bucket, processes it and writes the output to BigQuery. Two, success move to completion, which moves the CSV file from an input Cloud Storage bucket to a process store completed Cloud Storage bucket for archiving. Or three, if the pipeline fails partway, the file is moved to the completion bucket but tagged as failure. This is an example of a DAG which isn't strictly sequential. There, a decision is made to run one node or a different one based on the outcome of the parent note. But regardless of the size and shape of your workflow DAG, one common thread for all workflows is the common operators used. If the DAG itself is how to run the workflow, first do step one, then either move to step two or three, the operators specify what actually gets done as part of the task. In this simple example, we're calling on the Dataflow Python operator and the general Python operator. Those are by no means the only operators. So let's pause here and look at all the operators at our disposal to achieve our goal of automatic retraining and deployment of our ML model. Airflow has many operators which you can invoke the tasks you want to complete. Operators are usually atomic in a task, which means generally you only see one operator per task. This list of all services that airflow can orchestrate to is taken directly from the Apache Airflow documentation. Let's take a look at the ones that are likely most relevant to us as data engineers. As you might have guessed, we'll certainly be making use of the BigQuery operators since our workflows depend on the data that is fed into them through Cloud storage and BigQuery. Here's a list of the specific operators that we can invoke in a task to call on the BigQuery service for querying and other data related tasks. You'll be mainly working with the first three in this course, but I encourage you to skim the resource link on all the operators so you can get a feel for what is possible. Once we have our training data in a good place, the next logical step in our workflow is to retrain and redeploy our model. In the same DAG file, after the BigQuery operators complete, we can make a service call through a Vertex AI operator to kick off a new training job and manage our model like incrementing the version. You might have noticed that your airflow DAG can have operators that send tasks out to other Cloud providers. This is great for hybrid workflows where you have components across multiple Cloud platforms, or even on premise. Apache Airflow is open source and continually adds more operators to other services. So be sure to check out the list and the documentation periodically if you're waiting for a new service to be added. Here, you see four tasks: T1, T2, T3, and T4, and four operators corresponding to four Google Cloud services. The names should look familiar, and you can probably start to guess what this pipeline does at a high level just by reading them in order. The first two are concerned with getting fresh model data from a BigQuery dataset and into Cloud Storage for consumption by our ML model later in the pipeline. In the lab you're going to work on later, the dataset will be the one you're already familiar with, the Google Analytics news articles sample dataset. Let's see the parameters the BigQuery operator takes. The BigQuery operator allows you to specify a SQL query to run against a BigQuery dataset. In this quick example, we're parsing in a query which returns the top 100 most popular Stack Overflow posts from the BigQuery public dataset for a specified date range you see there in the Where clause. Notice anything different about the filters in the Where clause? Yes, they are parameters. In this case, for max date, and min date, you can parameterize pieces of the SQL statements like what we did here to only return posts for January 2018 with min query date, and max query date. What is really powerful is that you can even make the parameters dynamic instead of the static ones shown here, and have them be based on the DAG schedule date, like macros.ds_adds-7, which is a week before the DAG scheduled run date. The next two operators handle retraining the model by submitting a job to the machine learning engine and then deploying the updated model to App Engine. At the end of almost all DAG files, you'll find the actual order in which we want these operators to run. This is what the D in DAG is for, Directed. For our example, T2 or task two won't run until T1 has completed. This is what gives our graph the dependencies of a workflow. I can probably guess what you're thinking at this point, you could build some cool branching of multiple child nodes per upstream parent, and that is totally possible. Just don't forget to comment your code so you know where one branch begins, where it's going, and what tasks are involved. As a tip, after you load your DAG file into the DAG folder, you can see the visualization of your DAG in the airflow UI as a directed graph, a Gantt chart or a list if you want. Reviewing the visual representation of the ordered tasks will help you confirm your tasks are ordered properly.

### Video - [Workflow scheduling](https://www.cloudskillsboost.google/course_templates/53/video/509079)

- [YouTube: Workflow scheduling](https://www.youtube.com/watch?v=JIg6KFisCFM)

>> Now that you're familiar with the Cloud Composer and Apache Airflow environments, and the basics of building a list of tasks for Google Cloud services in your DAG, it's time to discuss a really important topic: workflow scheduling. As we hinted at earlier, there are two different ways your workflow can be run without you sitting there manually clicking run DAG. The first and most common is a set schedule or periodic run of a workflow, like once a day at 6:00am, or weekly on Saturdays. The second way is trigger based. Like if you wanted to run your workflow whenever new CSV data files were loaded into a Cloud Storage bucket, or if new data came in from a Pub Sub topic you've subscribed to, then navigate to the DAGs tab to view the existing workflows that you have Python DAG files for. Here we have two DAGs, the bottom one composer sample simple greeting has a daily schedule. But why is this top DAG missing a schedule? How would it ever get run? The answer is the fact that it's not on a set schedule at all. It's event driven. The driver of when this workflow runs is a Cloud function that we create. In the next lesson, we'll actually create our own Cloud function that watches a Cloud Storage bucket for new CSV files. If you want to go the regular scheduled route, you can specify the schedule_interval in your DAG code, like what you see here. By the way, clicking on the schedule of one day here in the UI won't allow you to edit it there, but instead, will take you to the history of all the runs for that workflow. As you saw earlier in this course, there are two general patterns for ETL workflows: event triggered or push. As in, you push a new file to Cloud Storage and your workflow kicks off, or pull, which is where airflow at a set time could look in your Cloud storage folder, and take all the contents that are found for its workflow run. We can use Cloud Functions to create our event driven or push architecture workflow. I mentioned triggering on events within a Cloud Storage bucket, which you can also trigger based on HTTP requests, Pub Sub, FireStore, Firebase, and more as you see here. Generally, push technology is great when wanting to distribute transactions as they happen. Stock tickers and other types of financial institution transactions are very important when it comes to push technology. How about disasters and notification? Again, important. For ML workflows, where your upstream data doesn't arrive at a regular pace, like get all the transactions at the end of each day, consider experimenting with a push architecture. Your final lab, since it's based on regular Google Analytics, news article data will be a pull architecture, but I've added in an optional lab for you to get practice with Cloud Functions and event-driven workflows for those interested. So let's talk through it more now. For our example, let's assume we have a CSV file or a set of files loaded to Cloud Storage. So we'll choose a Cloud storage trigger for our function. Then we specify an event type, finalize, create new files, and a bucket to watch. As part of the Cloud function, we need to create the actual function we want called in JavaScript. The good news is most of this code for triggering airflow DAGs in a function is all boilerplate for you to copy from as a starting point. Here, we specify a name for our function called Trigger DAG, then we tell it where your airflow environment is to be triggered and which DAG in that airflow environment. In this case, it's looking for one called GCS to BigQuery triggered. Keep in mind, you can have multiple workflows or DAGs in a single airflow environment, so be sure you specify the correct DAG underscore name to trigger. Then we have a few constants that we are provided, which construct the airflow URL that we're going to trigger a post request to, as well as who's making the request and what the body of the request is. Lastly, the Trigger DAG function makes the actual request against the airflow server to kick off a workflow DAG. Once you have the Cloud function code ready in your index.js file and the metadata about the function in package.json, which contains code dependency and versioning information, you still need to specify which function you actually want executed. In this case, we created one called Trigger DAG. So we just copy that down. I'll also save you about 20 minutes of frustration and tell you that the function to execute box is case sensitive. So all capital letters, D-A-G is different from capital D, lowercase A and G. And there you have it. Your Cloud function has been created, and is actively watching your Cloud storage bucket for file uploads. But how can you be sure everything is working as intended? For that, check out the next topic on monitoring and logging.

### Video - [Monitoring and Logging](https://www.cloudskillsboost.google/course_templates/53/video/509080)

- [YouTube: Monitoring and Logging](https://www.youtube.com/watch?v=iL1S0G1jFmE)

person: By this point, we've got our environment set up with our DAGs running at a predefined schedule or with triggered events. The last topic we'll cover before you practice what you've learned in your labs is how to monitor and troubleshoot your cloud functions and Airflow workflows. One of the most common reasons you'll want to investigate the historical runs of your DAGs is in the event that your workflow simply stops working. Note that you can have it auto retry for a set number of attempts in case it's a transient bug, but sometimes you can't get your workflow to run at all in the beginning. In the DAG runs, you can monitor when your pipelines run and in what state, like success, running or failure. The quickest way to get to this page is clicking on the schedule for any of your DAGs from the main DAGs page. Here we have five successful runs over 5 days for this DAG, so this one seems to be running just fine. Back on the main page for DAGs, we see some red, which indicates trouble with some of our recent DAG runs. Speaking of DAG runs, you'll note the three circles below, which indicate how many runs passed, are currently active or have failed. It certainly doesn't look good for 268 runs failed and zero passed for this first DAG. Let's see what happened. We click on the name of the DAG to get to the visual representation. It looks like the first task is succeeding, judging by the green border, but the next task, success-move-to-completion is failing. Note that the lighter pink color for the failure-move-to-completion node means that node was skipped. So reading in to this a bit, the CSV file was correctly processed by Dataflow in the first task, but there was some issue moving the CSV file to a different cloud storage bucket as part of task two. To troubleshoot, click on the node for a particular task and then click logs. Here you will find the logs for that specific Airflow run. I search for the word error and then start my diagnosis there. Here, this was a pretty simple error where it was trying to copy a file from an input bucket to an output bucket, and the output bucket didn't exist or was named poorly. Another tool in your toolkit for diagnosing Airflow failures is the general Google Cloud logs. Since Airflow launches other Google Cloud services through tasks, you can see and filter for errors for those services in Cloud Logging as you would debugging any other normal application. Here I filtered for Dataflow step errors to troubleshoot why my workflow is failing. It turns out that I had not changed the name of the output bucket for the CSV file, so after the file was processed by Dataflow as part of step one, it dumped the completed file back into the input bucket, which triggered another Dataflow job for processing and so on. You might be wondering, if there's an error with my cloud function, my Airflow instance would never have been triggered or issued any logs at all since it was unaware we were trying to trigger it, and you're exactly right. If you're using cloud functions, be sure to check the normal Google Cloud Logs for errors and warnings in addition to your Airflow logs. In this example, each time I upload a CSV file to my cloud storage bucket hoping to trigger a cloud function and then my DAG, I get an error message that includes expected to export function named trigger DAG. Remember way back when I said cloud functions were case sensitive? Looking for a function with capital DAG doesn't exist if it's capital D, lowercase A and G, so be sure to be mindful when setting up your cloud functions for the first time.

### Video - [Lab Intro: An Introduction to Cloud Composer](https://www.cloudskillsboost.google/course_templates/53/video/509081)

- [YouTube: Lab Intro: An Introduction to Cloud Composer](https://www.youtube.com/watch?v=x93JrhUOnGo)

Person: Let's now take some time to practice what you learned in this module by creating an environment in Cloud Composer, running a DAG in Airflow, and analyzing the results. The objectives of this lab are to use the Cloud Console to create a Cloud Composer environment, view and run a DAG in the Airflow web interface, and view the results of the wordcount job in storage.

### Lab - [An Introduction to Cloud Composer 2.5](https://www.cloudskillsboost.google/course_templates/53/labs/509082)

In this lab, you create a Cloud Composer environment using the GCP Console. You then use the Airflow web interface to run a workflow that verifies a data file, creates and runs an Apache Hadoop wordcount job on a Dataproc cluster, and deletes the cluster.

- [ ] [An Introduction to Cloud Composer 2.5](../labs/An-Introduction-to-Cloud-Composer-2.5.md)

### Quiz - [Manage Data Pipelines with Cloud Data Fusion and Cloud Composer](https://www.cloudskillsboost.google/course_templates/53/quizzes/509083)

#### Quiz 1.

> [!important]
> **Cloud Data Fusion is the ideal solution when you need**
>
> - [ ] to build visual pipelines
> - [ ] a data warehousing solution
> - [ ] low-latency and high throughput processing of streaming data
> - [ ] to reuse spark pipelines

## Course Summary

Course Summary

### Video - [Course Summary](https://www.cloudskillsboost.google/course_templates/53/video/509084)

- [YouTube: Course Summary](https://www.youtube.com/watch?v=nBgzJ9UA7Mo)

person: You've made it to the end of this course on building batch data pipelines. Let's recap what you've learned. In this course, we covered the different methods of loading data into your data lakes and warehouses. Remember ELT versus ETL and when to use each. ELT of extract, load, transform is a common pattern for when the transformations you want on the data set are minor and then can be handled after you load the data. An example is loading the data into BigQuery first and then doing SQL on the raw data and storing the transformed version as a new table. On the other hand, ETL, or transforming before loading, is common when you have data that needs more complex transformations, where the sheer volume of data makes it better to do those transformations before loading. That's where you would want to use a batch pipeline like Dataflow. Next, we discussed how to transform data in BigQuery using SQL, and you saw the common operations you can perform to ensure your structured data is ready for insights. Lastly, you practiced building batch pipelines in the serverless way with Dataflow. In our Hadoop module, we discussed dataproc in detail. You learned that you can lift and shift your existing Hadoop workloads to the cloud with no code changes, and they will just work. However, once in the cloud, there were additional optimizations you could make, like using cloud storage for cluster storage instead of HDFS for greater efficiency and cost savings. We then used Dataflow to build batch data pipelines using Dataflow templates and by writing them ourselves. Recall that the fundamental unit of logical data in the pipeline is the PCollection, which stands for a parallel collection. Dataflow will automatically split up your data set into many pieces and farm out the processing across as many worker VMs as it needs to complete the job. Dataflow is a serverless application, which means you will have some control over the maximum number of workers, the actual processing work and the autoscaling of workers up and down as the demand requires. In our module on managing data pipelines with Google Cloud, we discussed two new tools: Cloud Data Fusion and Cloud Composer. Recall that Cloud Data Fusion allows data analysts and ETL developers to wrangle data and build pipelines in a visual way. The technology then builds and executes the pipelines on a runner. With Cloud Data Fusion, you also get access to the lineage of each data field, which is the series of any transformation logic that happened before and after the field reaches your data set. The second tool we covered is Cloud Composer as a workflow orchestrator. Cloud Composer is managed Apache Airflow and allows you to, at a high level, command Google Cloud services in a DAG to perform complex operations. These DAGs can be user scheduled or event driven with cloud functions. Recall the example where when a new CSV was uploaded to our cloud storage bucket, a cloud function was triggered to start a Dataflow pipeline for processing and then sync the data into BigQuery. Congratulations on completing building batch data pipelines on Google Cloud. Building resilient streaming analytics systems on Google Cloud is the third course of the data engineering on Google Cloud core series and is covered next. We hope to see you there.

## Course Resources

PDF links to all modules

### Document - [Building Batch Data Pipelines on Google Cloud](https://www.cloudskillsboost.google/course_templates/53/documents/509085)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
