---
id: 874
name: 'Developing Applications with Google Cloud: Foundations'
type: Course
url: https://www.cloudskillsboost.google/course_templates/874
date_published: 2025-01-14
topics:
  - Databases
---

# [Developing Applications with Google Cloud: Foundations](https://www.cloudskillsboost.google/course_templates/874)

**Description:**

In this course, you learn the fundamentals of application development on Google Cloud. You learn best practices for cloud applications, and how to select compute and data options to match your application use cases. You're introduced to generative AI and how it's used to help build applications. You learn about authentication and authorization, application deployment, continuous integration and delivery, and monitoring and performance tuning for your applications running in Google Cloud. Using lectures and hands-on labs, you learn how to get started building and running applications on Google Cloud.

**Objectives:**

* Discuss best practices for application development in the cloud.
* Understand how to choose the appropriate data storage option for application use cases.
* Use authentication and authorization to secure an application.
* Describe use cases for the different Google Cloud compute options used for running applications.

## Introduction

Welcome to "Developing Applications with Google Cloud: Foundations." This course introduces you to best practices for cloud applications and how to select compute and data options to match application use cases. You learn about Google services for continuous integration and delivery, how to deploy your applications, and how to monitor your applications as they run in Google Cloud. You also learn how Google Cloud helps you build authentication, authorization, security, and artificial intelligence into your applications. Through a combination of lectures, hands-on labs, and supplemental materials, you will learn the fundamentals of application development in Google Cloud.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/874/video/520594)

* [YouTube: Course Introduction](https://www.youtube.com/watch?v=XOpfqe8zHhM)

Running your applications in the cloud can provide many benefits, including scalability, high availability, global access, and decreased operational costs. Cloud application development can also involve a multitude of processes and tools. Google Cloud provides many services and products that can help you streamline the application development process and run your applications efficiently and securely. In the Developing Applications with Google Cloud: Foundations course, learn the fundamentals of application development on Google Cloud. You learn best practices for cloud applications, and how to select compute and data options to match application use cases. You learn about Google services for continuous integration and delivery, how to deploy your applications, and how to monitor your applications as they run in Google Cloud. You also learn how Google Cloud helps you build authentication, authorization, security, and artificial intelligence into your applications. Hi, my name is Mike, and I’m a Technical Curriculum Developer here at Google Cloud. This course introduces you to the application development process and services provided by Google Cloud. This content is for application developers, architects, and cloud engineers who want to build applications by using the many powerful platforms and services. Through a series of video lectures, quizzes, and hands-on labs, you get an introduction to the application development process provided by Google Cloud. You learn best practices for application development in the cloud, understand how to choose the appropriate data storage option for application use cases, use authentication and authorization to secure an application, and will be able to describe use cases for the different Google Cloud compute options used for running applications. Welcome, and let's get started!

## Best Practices for Cloud Application Development

This module introduces best practices for developing applications that run in the cloud.

### Video - [Best Practices for Cloud Application Development (1)](https://www.cloudskillsboost.google/course_templates/874/video/520595)

* [YouTube: Best Practices for Cloud Application Development (1)](https://www.youtube.com/watch?v=_mOlaggw3IQ)

Welcome to Developing Applications with Google Cloud: Foundations, Module 1: Best Practices for Cloud Application Development. Before we discuss the details of Google Cloud application development, we look at some best practices for application development in the cloud. Applications that run in the cloud must be built to handle: Global reach: Your application should be responsive and accessible to users across the world. Scalability and high availability: Your application should be able to handle high traffic volumes reliably. The application architecture should use the capabilities of the underlying cloud platform to scale elastically in response to changes in load. Security: Your application and the underlying infrastructure should implement security best practices. Depending on the use case, you might be required to isolate your user data in a specific region for security and compliance reasons. Here are some best practices for managing your application code and environment. First, store your code in a code repository in a version control system such as Git. Using a code repository will enable you to track changes to your source code and set up systems for continuous integration and delivery. Second, do not store external dependencies such as JAR files or external packages in your code repository. Instead, depending on your application platform, explicitly declare your dependencies with their versions and install them by using a dependency manager. For example, in a Node.js application, you can declare your application dependencies in a package.json file and later install them by using the npm install command. Third, separate your configuration settings from your code. Do not store configuration settings as constants in your source code. Instead, specify configuration settings as environment variables. Passing configuration by using environment variables lets you modify settings between development, test, and production environments, which ensures that the tested code is used in production. Instead of implementing a monolithic application, consider implementing or refactoring your application as a set of microservices. Most applications are changed significantly over time. In a monolithic application, the codebase might become bloated. It can be difficult to determine all code that needs to change when implementing a new feature. Packages or components of the application can have tangled dependencies. The entire application must be deployed and tested even if a change is only made to a small part of the codebase. Monolithic applications increase the effort and risk when feature changes and bug fixes are done. Microservices let you restructure your application components in relation to your business boundaries. Refactoring a monolithic application into microservices may require significant time and effort, but the benefits gained can be worth the cost. The codebase for each microservice is modular. It's easy to determine where code needs to be changed. Each service can be updated, tested, and deployed independently without requiring its customers to change simultaneously. Each service can be scaled independently, depending on the load. Ensure that you evaluate the costs and benefits of optimizing and converting a monolithic application into one that uses a microservices architecture. Remote operations can have unpredictable response times and can make your application seem slow. Keep the operations that happen in the user thread at a minimum. Perform backend operations asynchronously. Use event-driven processing where possible. For example, if your application processes uploaded images, you can use a Cloud Storage bucket to store the uploaded images. You can then implement a Cloud Run function that is triggered whenever a new image is uploaded. The Cloud Run function processes the image and uploads the results to a different Cloud Storage location. Design application components so that they are loosely coupled at runtime. Tightly coupled components can make an application less resilient to failures, spikes in traffic, and changes to services. An event or message queue can be used to implement loose coupling, perform asynchronous processing, and buffer requests if traffic spikes. You can use an Eventarc trigger as an event queue or a Pub/Sub topic as a message queue. The order and inventory services are loosely coupled, which allows them to operate independently. In the context of HTTP API payloads, consumers of HTTP APIs should bind loosely with the publishers of the API. In the example, the email service retrieves information about each customer from the Customer service. The Customer service returns the name, age, and email address of the customer in its payload. To send an email, the email service should only reference the name and email fields in the payload. It should not attempt to bind with all fields in the payload. This method of loosely binding fields allows the publisher to evolve the API and add fields to the payload in a backward compatible manner. Implement application components so that they do not store state internally or share state. Accessing a shared state is a common bottleneck for scalability. Design each application component so that it focuses on compute tasks only. This approach allows you to use a Worker pattern to add or remove additional instances of the component for scalability. Application components should start up quickly to enable efficient scaling, and then shut down gracefully when they receive a termination signal. For example, if traffic to your application can vary significantly, you can use Cloud Run for your application, and scale capacity based on traffic. The Cloud Run services process the incoming requests but don't store or share state, so they can be shut down easily when traffic is reduced. Data should be persisted in a separate database like Firestore. When accessing services and resources in a distributed system, applications need to be resilient to temporary and long-lasting errors. Resources can sometimes become unavailable due to transient network errors. In this case, applications should implement retry logic with exponential backoff and fail gracefully if the errors persist. Exponential backoff helps ensure that applications don't make the problem worse by overloading the backend or network. The Cloud Client Libraries retry failed requests automatically. When services are down with long-lasting errors, the application should not generate traffic or waste CPU cycles attempting to retry the request. In this case, applications should implement a circuit breaker and handle the failure gracefully. For errors that are propagated back to the user, consider degrading the application gracefully instead of explicitly displaying the error message. For example, if the recommendation engine is down, consider hiding the recommendations section instead of displaying error messages every time the page is displayed.

### Video - [Best Practices for Cloud Application Development (2)](https://www.cloudskillsboost.google/course_templates/874/video/520596)

* [YouTube: Best Practices for Cloud Application Development (2)](https://www.youtube.com/watch?v=GVJSccB7Juo)

Caching content can improve application performance and lower network latency. Cache application data that is frequently accessed or that is computationally intensive to calculate each time. When a user requests data, the application component should check the cache first. If data exists in the cache, the application should return the previously cached data. If the data is not in the cache or has expired, the application should retrieve the data from backend data sources and recompute results. The application should also update the cache with the new value. Memorystore is Google Cloud's fully managed in-memory service for caching using Redis or Memcached. In addition to caching application data in a cache, you can also use a content delivery network, or CDN, to cache web content. Cloud CDN uses Google's global edge network to serve content closer to users, which accelerates your websites and applications. Static content can be served from Cloud Storage buckets, services and functions running on Cloud Run, or Compute Engine virtual machine instance groups. Implement API gateways to make backend functionality available to consumer applications. Apigee is a platform for developing and managing APIs. By fronting services with a proxy layer, Apigee acts as a facade for your backend service APIs, and provides security, rate limiting, quotas, analytics, and more. If you have legacy applications that cannot be refactored and moved to the cloud, consider implementing APIs. Each consumer can then invoke these modern APIs to retrieve information from the backend instead of implementing functionality to communicate by using outdated protocols and disparate interfaces. You can minimize your effort for user administration by delegating identity management. You can delegate user authentication to Google or other external providers like Facebook or GitHub. Identity Platform provides a drop-in, customizable authentication service for user sign-up and sign-in. In addition to external providers, Identity Platform supports other authentication methods like email/password, SAML, OpenID Connect, and multi-factor authentication. Firebase Authentication with Identity Platform provides backend services, easy-to-use SDKs, and ready-made UI libraries to authenticate users to your app. With federated identity management, you do not need to implement, secure, and scale a proprietary solution for authenticating users. Treat your logs as event streams. Logs constitute a continuous stream of events that keep occurring as long as the application is running. Do not manage log files in your application. Instead, write to an event stream like stdout and let the underlying infrastructure collate all events for later analysis and storage. With this approach, you can set up logs-based metrics and trace requests across different services in your application. Logging by writing to stdout works especially well for serverless compute options like Cloud Run and Cloud Run functions. With Google Cloud Observability, you can set up error reporting, logging and logs-based metrics, and monitor applications running in a multi-cloud environment. Implement a strong DevOps model with automation by using CI/CD pipelines. Automation helps you increase release velocity and reliability. With a robust CI/CD pipeline, you can test and roll out changes incrementally instead of making large releases with multiple changes. This approach enables you to lower the risk of regressions, debug issues quickly, and roll back to the last stable build if necessary. CI is continuous integration, and CD is continuous delivery or continuous deployment. In a continuous integration system, developers commit code changes made in their own feature branch into a shared branch in a code repository. When an update is made, the application build is triggered and automated testing confirms that the new update hasn't broken existing unit and integration tests. In a continuous delivery system, the code change validated during continuous integration is automatically stored in a code repository. This validated codebase is ready to be deployed to production by the operations team. Continuous deployment takes this one step further by automatically deploying the validated changes to production. Only a failed test prevents the change from being deployed to production. Continuous deployment means that changes and fixes are available in production faster, and automated deployments reduce the work of your operations team. There is no manual process for validating the release to production, though, so your application, tests, and CI/CD pipeline must be well designed to prevent issues in production. When you create a CI/CD pipeline in Google Cloud, Cloud Build can detect repository commits, trigger a build, and run unit tests. The build system produces deployment artifacts that can be stored in Artifact Registry. Cloud Deploy can automatically trigger the deployment of builds to test environments or directly to production. You can automatically execute integration, security, and performance tests and then deploy successful builds to your production environment. When rolling out builds to the production environment, consider performing blue-green deployments or canary testing. These types of deployment strategies help ensure that unexpected issues with the new build do not affect most users. Consider using the strangler pattern when rearchitecting or migrating large applications. In the early phases of migration, you might replace smaller components of the legacy application with newer application components or services. You can incrementally replace more features of the original application with new services. A strangler facade can receive requests and direct them to the old application or new services. As your implementation evolves, the legacy application is “strangled” by the new services and no longer required. This approach minimizes risk by allowing you to learn from each service implementation without affecting business-critical operations. The term “strangler” comes from strangler vines. The vines seed and start growing on the upper branches of fig trees, gradually enveloping the tree. As you create your own applications in the cloud, always consider these best practices. By following best practices of cloud development, you will set up your applications for success.

### Quiz - [Quiz: Best Practices for Cloud Application Development](https://www.cloudskillsboost.google/course_templates/874/quizzes/520597)

#### Quiz 1.

> [!important]
> **Which of the following is an anti-pattern when designing a cloud application?**
>
> * [ ] Cache application data and frontend content
> * [ ] Perform asynchronous operations
> * [ ] Use APIs and API gateways to connect to legacy systems in applications
> * [ ] Embed configuration settings in your source code

#### Quiz 2.

> [!important]
> **For transient network issues, what error-handling approach should you implement?**
>
> * [ ] Retry with exponential backoff
> * [ ] Implement a circuit breaker
> * [ ] Retry constantly until the call succeeds
> * [ ] Display the error to the user

## Getting Started with Google Cloud Development

This module introduces the different Google Cloud tools that you will use to develop your applications.

### Video - [Overview](https://www.cloudskillsboost.google/course_templates/874/video/520598)

* [YouTube: Overview](https://www.youtube.com/watch?v=D-oMjti6t_U)

Welcome to Developing Applications with Google Cloud: Foundations, module 2: Getting Started with Google Cloud Development. Google Cloud provides many platforms that you can use to build your applications. Your apps can benefit from many powerful services provided by Google Cloud. In this module, you learn how to access these services for your apps and scripts. You learn about Cloud APIs and the Google Cloud SDK, which let you programmatically include these features in your apps. Cloud Client Libraries provide an optimized developer experience by using the natural conventions and styles of each supported language. You also learn about Cloud Code, which will help you develop your Google Cloud applications within integrated development environments.

### Video - [Cloud APIs and the Google Cloud SDK](https://www.cloudskillsboost.google/course_templates/874/video/520599)

* [YouTube: Cloud APIs and the Google Cloud SDK](https://www.youtube.com/watch?v=JUDuV1gUlHY)

Cloud APIs and the Google Cloud SDK are used to interact with Google services. Cloud APIs provide programmatic interfaces to Google Cloud services. You can use a Google Cloud resource or service in your application by calling a corresponding Cloud API. Cloud APIs let you use powerful features like compute, networking, storage, and machine learning in your applications. Cloud APIs can be called by using HTTP requests with JavaScript Object Notation, or JSON, payloads. They can also be called by using Google Remote Procedure Call, or gRPC, requests. gRPC is an open source, remote procedure call framework that can be run anywhere and uses an efficient binary request structure. To call Cloud APIs, the caller must supply application credentials. These credentials are validated to ensure that an application is allowed to access your Google Cloud project and resources. The Google Cloud SDK is used to interact with Google Cloud products and services. The SDK features are in two categories: command-line tools and language-specific Cloud Client Libraries. These tools and libraries use Cloud APIs to communicate with Google Cloud.

### Video - [The Google Cloud CLI](https://www.cloudskillsboost.google/course_templates/874/video/520600)

* [YouTube: The Google Cloud CLI](https://www.youtube.com/watch?v=Y1VqzmQR724)

Next, we look at the Google Cloud CLI. The Google Cloud Command Line Interface, or gcloud CLI, provides tools to manage Google Cloud services from the command line or in automated scripts. These tools provide the functionality of the Cloud APIs in an easy-to-use command-line interface. They automate the process of sending credentials in the Cloud API calls and combine multiple Cloud API calls when required to complete a single common task. You can use the gcloud CLI to perform most tasks allowed by the Cloud APIs. For example, you can manage virtual machines or deploy applications to run on Google Cloud. The gcloud CLI includes many command-line tools. For example, gcloud interacts with Google Cloud services, gsutil manages Cloud Storage buckets and objects, and bq runs queries and manages data in BigQuery. Other command-line tools can be used to manage Kubernetes, emulate Google Cloud services like Firestore or Pub/Sub, or create infrastructure by using Terraform. The gcloud CLI tool lets you perform most common tasks on Google Cloud, including creating and managing resources for many services. This example command, gcloud compute instances list, shows the Compute Engine virtual machine instances for your project. The gcloud CLI tool also lets you manage the Google Cloud CLI tools. The command "gcloud components list" describes each of the CLI components. Each component is listed as not installed, update available, or installed at the latest version. To install a component at the current CLI version, use the "gcloud components install" command. For example, to install the Kubernetes cluster manager kubectl, you'd run the command "gcloud components install kubectl." You can update your version of the Google Cloud CLI with the command "gcloud components update." Next, we discuss the gcloud, gsutil, and bq CLI tools. Cloud Storage provides reliable, secure, and highly performant object storage. The gsutil command can be used to create and manage buckets and objects. gcloud storage is now the preferred command line tool for managing Cloud Storage. gcloud storage performs better than gsutil, and its usage is similar to other gcloud commands. gcloud storage buckets creates, lists, deletes, and manages access control lists for buckets. gcloud storage objects manages objects and their access control lists. gcloud storage commands can be used to copy, move, list, and delete objects. This gcloud storage command example copies a file from your local machine into a Cloud Storage bucket. bq is a command-line tool for BigQuery, Google Cloud's serverless, highly scalable, and cost-effective data warehouse. bq can be used to manage datasets, tables, and other BigQuery entities, but its primary purpose is running queries. This example query searches for all occurrences of a word in a sample public dataset that contains the complete word index of Shakespeare's works.

### Video - [Cloud Client Libraries](https://www.cloudskillsboost.google/course_templates/874/video/520601)

* [YouTube: Cloud Client Libraries](https://www.youtube.com/watch?v=CTBZ7xiCdsQ)

Next, we discuss the Cloud Client Libraries. Using a Cloud Client Library is easier than making direct API calls. The Cloud Client Libraries are the recommended method for accessing Google Cloud resources from your applications. The Cloud Client Libraries provide an optimized developer experience by handling low-level communication with the server, including authentication. They also provide retry logic for transient network failures. These libraries use the natural conventions and style for each of the supported languages. Many libraries also give you performance benefits by automatically calling the gRPC Cloud APIs. Cloud Client Libraries are available for many of the most popular programming languages. Supported languages include Python, Node.js, Java, Go, PHP, Ruby, C++, and the . NET languages, including C#. If your application uses any of these languages, you will probably want to use the corresponding Cloud Client Library. Here's an example of using the Python Cloud Client Library to create a Cloud Storage bucket. Every package provides a client that interacts with an API. Your application runs with a particular identity, which is typically a service account. This example imports the Cloud Storage client library, instantiates the client by using the default credentials provided by the service account, and creates a cloud bucket. The Cloud Client Libraries let you easily manage your Google Cloud resources by using the natural style of the language you have chosen. You can download and install the Google Cloud SDK on Linux, macOS, and Windows. The Google Cloud SDK is initialized by running 'gcloud init.' After it's initialized, you can immediately start using the Google Cloud SDK. You can install and manage SDK components and use the gcloud CLI interactive shell, which provides prompt completion and suggests command options. You can also script gcloud CLI commands to automate your processes.

### Video - [Cloud Shell and Cloud Code](https://www.cloudskillsboost.google/course_templates/874/video/520602)

* [YouTube: Cloud Shell and Cloud Code](https://www.youtube.com/watch?v=CHsV7j3U1Nk)

Finally, we look at Cloud Shell and Cloud Code. Cloud Shell is a free admin machine with browser-based command-line access that is used from the Google Cloud console. It provides you with a temporary virtual machine instance that has 5 GB of persistent disk storage. When you start Cloud Shell, it provisions a Compute Engine virtual machine that runs a Debian-based Linux operating system. Cloud Shell instances are provisioned on a per-user, per-session basis. The instances persist only while your Cloud Shell session is active and terminate after an hour of inactivity. When a new instance needs to be provisioned, it retains the persistent disk that was used with the previous instance. The Google Cloud SDK comes pre-installed in Cloud Shell with built-in authorization to your Google Cloud projects and resources. Cloud Shell comes with a built-in code editor, based on Theia, that you can use to browse directories and view and edit files within your VM. You can use Cloud Code to help you develop your cloud applications in your favorite integrated development environment, or IDE. Cloud Code is a set of IDE plugins that make it easier to create, deploy, and debug cloud applications for Google Cloud. Cloud Code is available for the Cloud Shell Editor, Visual Studio Code, and the JetBrains IDEs, which include IntelliJ for Java and PyCharm for Python development. Cloud Code streamlines common workflows within the IDE by merging non-trivial tasks into a simple user interface inside your IDE. Cloud Code integrates with Secret Manager, which is Google Cloud's service for securely storing passwords, keys, certificates, and other sensitive data. This integration lets you manage your sensitive data within the IDE. You can also manage Cloud APIs from the IDE. You can browse the available Cloud APIs and see Cloud Client Library documentation specific to your programming language. You can also find and copy code samples that use the Cloud APIs. Cloud Code for Kubernetes lets you develop your Kubernetes applications in your IDE. You can run and debug your applications in a local cluster or on Google Kubernetes Engine (GKE). Cloud Code's Kubernetes Explorer provides you with an easy way to visualize and manage your Kubernetes resources within the IDE. You don't need to remember the associated CLI commands. For example, you can right-click on a Pod and stream its logs or open an interactive terminal. If you have developed or deployed Kubernetes applications, you know that YAML configuration files for Kubernetes can be complex and require detailed schemas. Cloud Code's YAML authoring assistance simplifies the process of creating and editing these configuration files by providing autocomplete and inline documentation. Cloud Code also works with Cloud Run, Google Cloud's fully managed serverless product with autoscaling that scales down to zero. You can use Cloud Code to develop your Cloud Run service and then use the Cloud Run Emulator to run and debug it locally. When you're ready to deploy your service, you can deploy it from the IDE. You can also use Cloud Run Explorer to manage your Cloud Run services from the IDE. The gcloud CLI includes local emulators for several of the Google Cloud services that you can use in your applications. You can use the "gcloud beta emulators" commands to install and manage emulators. You do not need to change your application code when you switch between using a local emulator and the Google Cloud service. When you set specified environment variables, Cloud Client Libraries used by your application will automatically connect to the local emulator instead of the Google Cloud service. The local emulators let you develop your code without requiring a connection to the corresponding services, and you will not consume project resources. Local emulators are currently available for Bigtable, Datastore, Firestore, Pub/Sub, and Spanner. Cloud Workstations provides fully managed and secure cloud-based development environments for Google Cloud. Instead of requiring your developers to install software and run setup scripts, you can create a workstation configuration that specifies your environment in a reproducible way. Developers can access fast and consistent development environments anytime and anywhere, using a browser, SSH, or a local IDE. Cloud Workstations supports any code editors and applications that can be run in a container. IT admins can easily provision, scale, manage, and secure cloud development environments for all developers on the team. Environments are consistent no matter where the developers are located or what type of computer and network they use. Cloud Workstations runs on ephemeral Compute Engine VMs, and can be started or stopped on demand or when the IDE is idle to improve cost savings. The IDE runs on customer owned VMs and persistent disks inside the customer VPC, ensuring that the developer machines and code are secure.

### Quiz - [Quiz: Getting Started with Google Cloud Development](https://www.cloudskillsboost.google/course_templates/874/quizzes/520603)

#### Quiz 1.

> [!important]
> **Which of the following statements about Cloud Code are true? Select two.**
>
> * [ ] Cloud Code's YAML authoring assistance provides autocomplete and inline documentation for Docker files.
> * [ ] Cloud Code is only available in the Cloud Shell editor.
> * [ ] Cloud Code is an integrated development environment for creating cloud applications.
> * [ ] Cloud Code integrates with Secret Manager to securely store sensitive data.
> * [ ] Cloud Code works with both Cloud Run and Kubernetes applications.

#### Quiz 2.

> [!important]
> **Which of the following statements about Google Cloud Client Libraries are true? Select two.**
>
> * [ ] Cloud Client Libraries are provided in all languages that can be used on Google Cloud.
> * [ ] Cloud Client Libraries should only be used when you cannot call Cloud APIs directly.
> * [ ] Cloud Client Libraries include the Cloud SDK.
> * [ ] Cloud Client Libraries support a language's natural conventions and styles.
> * [ ] Cloud Client Libraries handle retry logic and authentication.

#### Quiz 3.

> [!important]
> **Which command-line tools does the Google Cloud SDK include? Select two.**
>
> * [ ] bq
> * [ ] ruby
> * [ ] gRPC
> * [ ] node
> * [ ] gcloud

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/874/video/520604)

* [YouTube: Summary](https://www.youtube.com/watch?v=3JGqkGi6am8)

To summarize, here's what we learned in this module: The Google Cloud APIs provide programmatic interfaces to Google Cloud services. The Google Cloud SDK provides a simpler interface when used in scripts or on the command line. When you're ready to write your application, use the Cloud Client Libraries for your chosen programming language to interact with Google Cloud services. Cloud Shell provides a free virtual machine that can be used to manage your Google Cloud projects and resources. Use Cloud Code to develop your applications in your favorite IDE.

## Data Storage Options

This module compares the data storage and database services provided by Google Cloud.

### Video - [Overview](https://www.cloudskillsboost.google/course_templates/874/video/520605)

* [YouTube: Overview](https://www.youtube.com/watch?v=3Rcg7cx1Sj0)

Welcome to Developing Applications with Google Cloud: Foundations. Module 3: Overview of Data Storage Options. Applications can have various types of data. For example, if your application is a social image sharing site, you have to store image files, high volumes of user messages, and transactional data. You generally need to cache frequently accessed data. And you want to collect, query, and analyze all the data to gather business intelligence about your users and product usage patterns. Google Cloud offers managed services that you can use for each type of data. In this module, you learn about services such as Cloud Storage, Firestore, Bigtable, Cloud SQL, Spanner, and BigQuery. You learn the ideal use cases for each data storage option, as well as use cases for which the option might not be suitable. This knowledge helps you choose the data storage option that meets specific use cases in your applications.

### Video - [Cloud Storage and Databases](https://www.cloudskillsboost.google/course_templates/874/video/520606)

* [YouTube: Cloud Storage and Databases](https://www.youtube.com/watch?v=3ifgYPbxXiQ)

First, we look at the different managed storage services that might be useful in your applications. You have a full range of cost-effective storage services to choose from when developing with Google Cloud. No one size fits all, and your choice of storage and database solutions will depend on your application and workload. Cloud Storage is a unified object storage that lets you serve, analyze, and archive data anywhere in the world. Objects are accessed by using HTTP requests, including ranged GETs to retrieve portions of the data. The only key is the object name. There is object metadata but the object itself is treated as unstructured bytes. The scale of the system allows for serving large static content and accepting user uploaded content including videos, photos, and files. Objects can be up to 5 TB each. Cloud Storage is built for availability, durability, scalability, and consistency. It's an ideal solution for hosting static websites and storing images, videos, objects and blobs, and any unstructured data. Firestore is a fast, fully managed, serverless, NoSQL document database built for automatic scaling, high performance, and ease of application development. Firestore provides features such as: A strongly consistent storage layer, A hierarchical collection and document-based data model, Real-time updates and offline features, and Mobile and Web client libraries. Firestore is built to scale and takes advantage of Google Cloud’s powerful infrastructure, with automatic scaling, in response to your application's load. Firestore is an excellent choice for mobile and web apps, especially those that require flexible data storage or have external user accounts. The data model for Firestore supports flexible, hierarchical data structures. Store your data in documents, organized into collections. Documents can contain complex nested objects in addition to subcollections. Bigtable is a high-performance, NoSQL database service. It’s a sparsely populated table that can scale to billions of rows and thousands of columns. Bigtable can store terabytes to petabytes of data. Bigtable is built for fast key-value lookup and supports consistent sub-10ms latency. Due to the fast lookup and write speed of Bigtable, it's great for storing user behavior. Bigtable is ideal for operational and analytical applications and is ideal for storing large amounts of single-keyed data and performing MapReduce operations. Bigtable offers seamless scaling: changes to the deployment configuration are immediate, so there’s no downtime during reconfiguration. Bigtable supports the open source industry standard HBase API. Cloud SQL is Google Cloud’s managed relational database service. With Cloud SQL, Google manages replication, failover, and backups of your databases so you can focus on your MySQL, PostgreSQL, or SQL Server-compatible applications. Cloud SQL lets you easily configure replication and backups to protect your data. You can replicate a primary instance to one or more read replicas. A read replica is a copy of the primary that reflects changes to the primary instance in almost-real time. You can enable automatic failover to make your database highly available. The Cloud SQL Auth Proxy works by having a local client, called the proxy, running in the local environment. An application communicates with the proxy by using the standard database protocol for your database. The proxy uses a secure tunnel to communicate with its companion process running on the server. Cloud SQL Auth Proxy provides secure access to your Cloud SQL instances without you having to configure allowed IP addresses or SSL certificates. Cloud SQL is ideal for web frameworks, applications requiring structured data, and online transaction processing (OLTP) workloads. It is ideal for applications using MySQL, PostgreSQL, or SQL Server, with minimal refactoring required for migration to Google Cloud. AlloyDB is a fully managed, high performance PostgreSQL database service from Google Cloud. It combines the best of Google with PostgreSQL. Historically, a PostgreSQL database runs on a single VM with an attached disk. Scaling this type of architecture is difficult. With AlloyDB, Google separates the compute and storage for the database. Google databases like Bigtable and Spanner have traditionally used this same approach, allowing them to scale extremely well. AlloyDB gains the same benefits, allowing the database to scale while maintaining high performance for both reads and writes. AlloyDB is four times faster than standard PostgreSQL for transactional workloads. The Columnar Engine of AlloyDB results in 100 times faster performance than standard PostgreSQL for analytical queries, with zero impact on performance when running business intelligence, reporting, and hybrid transactional and analytical processing workloads. AlloyDB has full PostgreSQL compatibility and high availability with automatic scaling. Google manages replication, failover, and backups of your databases so you can focus on your applications. The AlloyDB Auth Proxy, which is similar to the Cloud SQL Auth Proxy, provides secure access to the database without having to configure allowed IP addresses or SSL certificates. AlloyDB is ideal for applications requiring high performance and PostgreSQL compatibility. The predictable and high performance is perfect for applications that want to perform a mix of transactional and analytical processing. Spanner is Google Cloud’s fully managed relational database service that offers both strong consistency and horizontal scalability. It's designed for mission-critical online transactional processing, or OLTP, applications. Spanner provides automatic, synchronous replication for high availability. Spanner is built for multi-region replication and offers one of the highest SLAs in the industry: 99.999%. Spanner is ideal for applications with relational, structured, and semi-structured data that require high availability, strong consistency, and transactional reads and writes.

### Video - [BigQuery, Memorystore, and Product Comparisons](https://www.cloudskillsboost.google/course_templates/874/video/520607)

* [YouTube: BigQuery, Memorystore, and Product Comparisons](https://www.youtube.com/watch?v=2Bir-3Jpwzc)

Next, we look at BigQuery and Memorystore, and then compare the storage services. BigQuery is a fully managed, serverless enterprise data warehouse for analytics. BigQuery has built-in features like machine learning, geospatial analysis, and business intelligence. BigQuery can scan terabytes in seconds, and petabytes in minutes. It's a great solution for Online Analytical Processing, or OLAP, workloads, for big data exploration and processing, and for reporting with Business Intelligence tools. Applications that you run on Google Cloud can achieve high levels of performance by using either Redis or Memcached without the burden of managing complex deployments. Memorystore supports both of these highly scalable, available, and secure open source caching engines, and is fully protocol compatible with each engine. Memorystore is ideal for scalable web applications, gaming, and stream processing, where a distributed in-memory data store allows for fast, real-time access or processing of data. As a fully managed service, provisioning, replication, failover, and patching are all automated. You can also monitor instances and set up alerts with Cloud Monitoring. Memorystore can be protected from the internet by using VPC networks and internal IP addresses. Memorystore also integrates with Identity and Access Management (IAM). Here are the Google Cloud storage options at a glance. To choose the right storage option for your application, it’s important to understand what a product is and isn’t ideal for by design. This slide includes a simple description of the products and use cases that are ideal for each product. Use cases that are not ideal for each product are also listed. Other considerations for choosing a storage option for your application include read/write latency, typical size of your data, and storage type. When you're designing the database needs of your applications, remember that you are not limited to a single database. Choose the database that is most suitable for each use case. Size limits are per database, so you can exceed the size limits shown here by splitting your data into multiple databases. Refer to the table or Google Cloud documentation to identify the best storage option for your application.

### Video - [Lab Overview](https://www.cloudskillsboost.google/course_templates/874/video/520608)

* [YouTube: Lab Overview](https://www.youtube.com/watch?v=Tvmq2U2nOKI)

In this lab, "Storing Application Data in Firestore and Cloud Storage," you create a bookshelf application written in Python. You update the application to use Cloud Client Libraries to store book data in a Firestore database. You also use Cloud Client Libraries to upload book cover images to a Cloud Storage bucket.

### Lab - [Developing Applications with Google Cloud: Storing Application Data](https://www.cloudskillsboost.google/course_templates/874/labs/520609)

In this lab, you create a bookshelf application written in Python. You update the application to use Cloud Client Libraries to store book data in a Firestore database. You also use Cloud Client Libraries to upload book cover images to a Cloud Storage bucket.

* [ ] [Developing Applications with Google Cloud: Storing Application Data](../labs/Developing-Applications-with-Google-Cloud-Storing-Application-Data.md)

### Quiz - [Quiz: Data Storage Options](https://www.cloudskillsboost.google/course_templates/874/quizzes/520610)

#### Quiz 1.

> [!important]
> **You're building a banking application that is expected to have a very large number of users around the world. When users make a deposit, they want to see the result of this deposit reflected immediately when they view their balance. What data storage option is ideal for storing account balance information for users?**
>
> * [ ] Bigtable is ideal because it supports low-latency read/write access.
> * [ ] Firestore is ideal because it lets you develop a mobile app.
> * [ ] Spanner is ideal because it supports strongly consistent reads in addition to horizontal scalability, low latency, and high throughput.
> * [ ] Cloud SQL is ideal because it's a relational database that supports transactions.

#### Quiz 2.

> [!important]
> **You have a very large database that you're using for complex queries in a suite of business intelligence applications. You want to move the data to a fully-managed solution. Which option is ideal for such use cases?**
>
> * [ ] Bigtable
> * [ ] Firestore
> * [ ] Cloud SQL
> * [ ] BigQuery

#### Quiz 3.

> [!important]
> **A restaurant in your neighborhood wants to put up a website that displays static information including a menu, restaurant hours, and location on a map. What is the best solution for serving the website's content on Google Cloud?**
>
> * [ ] Serve the website's content from Bigtable.
> * [ ] Serve the website's content from a web server running on a Compute Engine instance.
> * [ ] Serve the website's content from a Cloud Storage bucket.
> * [ ] Serve the website's content from Firestore.

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/874/video/520611)

* [YouTube: Summary](https://www.youtube.com/watch?v=lQzJcydNmsY)

Google Cloud has a rich set of services to enable you to store, query, and manage different types of application data. Cloud Storage is a managed service for storing unstructured data. It's ideal for hosting static websites and storing large objects like videos or photos. Firestore is a fully managed, scalable, and serverless document database that scales to meet any demand. It offers a great developer experience for mobile and web applications, with support for live synchronization, offline support, and transactions. Bigtable is a high-performance NoSQL database service, ideal for fast access to very large amounts of data with high read and write throughput. It's ideal for large analytical and operational workloads. Cloud SQL is a fully managed database service that helps you set up, maintain, manage, and administer your MySQL, PostgreSQL, and SQL Server databases. It's very useful for migrating existing applications to Google Cloud without having to refactor all of your data access. AlloyDB is a fully managed, high performance PostgreSQL database service that is useful for applications that require a mix of transactional and analytical processing. Spanner is Google Cloud’s fully managed relational database service offering both strong consistency and horizontal scalability. It's ideal for low-latency transactional systems, especially those systems with global scale. BigQuery is Google Cloud's serverless enterprise data warehouse that scales with your data. It's a great solution for OLAP workloads. Memorystore is a fully managed, scalable, secure, and highly available in-memory service for caching application data. With all of these options, you can find managed services to handle all of your application's data needs.

## Handling Authentication and Authorization

This module explains how authentication and authorization are added to your cloud applications.

### Video - [Overview](https://www.cloudskillsboost.google/course_templates/874/video/520612)

* [YouTube: Overview](https://www.youtube.com/watch?v=EWO1IpTQ3g8)

Welcome to Developing Applications with Google Cloud: Foundations, module 4: Handling Authentication and Authorization. It can be daunting to implement user authentication and authorization from the beginning in your application. Writing your own code to secure user data leaves you vulnerable to attacks. With Identity and Access Management (IAM) and Identity Platform authentication, you have a simple and secure solution for authenticating and authorizing your application users. In this module, you learn about IAM principals and how to specify the resources they can access and the operations they can perform on these resources. You learn how to use service accounts and other methods to authenticate and authorize applications to invoke Google Cloud APIs, and when to use each method. We discuss how you can use Identity-Aware Proxy to control access to your application. The Firebase SDK makes it really easy to implement federated identity management. You learn how to use the Firebase SDK to validate users against credentials stored in Identity Platform, Google Cloud's enterprise-grade access management and identity platform. Finally, you learn about Secret Manager, and how it can be used to securely store credentials that are required by your application.

### Video - [Authorization with Cloud IAM](https://www.cloudskillsboost.google/course_templates/874/video/520613)

* [YouTube: Authorization with Cloud IAM](https://www.youtube.com/watch?v=f7Mrts6uUaY)

First, we review IAM authorization. IAM lets you manage access control by defining who (the principal) has what access (role) for which resource. You can grant more granular access to Google Cloud resources by using the security principle of least privilege, which means that you should only grant access to resources that are necessary. You can specify who has access to your resources with IAM principals. A Google Account represents a developer, an administrator, or any other person who interacts with Google Cloud. A service account is an account for an application or compute workload instead of an individual end user. When you run code that's hosted on Google Cloud, the code runs as the account you specify. You can create as many service accounts as needed to represent the different logical components of your application. A Google group is a named collection of Google Accounts and service accounts. Every Google group has a unique email address that's associated with the group. Google Groups are a convenient way to apply access controls to a collection of users. You can grant and change access controls for a whole group at once instead of one at a time for individual users or service accounts. Google Groups don't have login credentials, and you cannot use Google Groups to establish identity to make a request to access a resource. A Google Workspace account represents a virtual group of all Google Accounts that it contains. Google Workspace accounts are associated with your organization's internet domain name, such as example.com. Like Google Groups, Google Workspace accounts cannot be used to establish identity, but they enable convenient permission management. A Cloud Identity domain is like a Google Workspace account, because it represents a virtual group of all Google Accounts in an organization. However, Cloud Identity domain users don't have access to Google Workspace applications and features. Like Google Workspace accounts, a Cloud Identity domain cannot be used to establish identity. Google groups, Google Workspace accounts, and Cloud Identity domains are convenient ways to apply access policies to collection of users. You can grant access to principals for a Google Cloud resource. Some examples of resources are projects, Compute Engine instances, Cloud Storage buckets, or Artifact Registry repositories. Permissions determine what operations are allowed on a resource. In the IAM world, permissions are represented in the form of . ., for example, pubsub.subscriptions.consume. You cannot assign a permission to a user directly. Instead, you grant the user a role. A role is a collection of permissions. When you grant a role to a user, you grant them all the permissions that the role contains. In this example, all users in the Google group named "staff" are granted the InstanceAdmin role on project_a. Each user would have all permissions contained in the role. IAM supports three different types of roles. Basic roles are highly permissive roles with broad access. For example, the viewer role gets read-only access to all resources in a project. Because basic roles are so broad, they are usually not recommended for production environments. Predefined roles provide granular access to specific Google Cloud resources. Predefined roles are created and maintained by Google. An example role is run.invoker, which lets the user invoke Cloud Run services. Custom roles are user-defined and typically maintained for specific needs. You should create and use custom roles when existing predefined roles are too permissive for your use case. Because you have full control over the permissions given, custom roles can help enforce the principle of least privilege. The principle of least privilege says that you should provide each principal exactly the permissions necessary for it to do its job. You can grant multiple roles to the same user.

### Video - [Authenticating to Google APIs and Service Accounts](https://www.cloudskillsboost.google/course_templates/874/video/520614)

* [YouTube: Authenticating to Google APIs and Service Accounts](https://www.youtube.com/watch?v=juYoUBws2hA)

Next, we discuss authentication in Google Cloud. IAM enables authorization for Google Cloud services, specifying what you're permitted to do. Authentication proves that you are who you say you are. You prove your identity by presenting some kind of credential. If you are an application developer on Google Cloud, you might need to use or create different types of authentication. Your application might need to authenticate to Google and allow access to Google services and resources. You might need to call apps hosted on Google Cloud services, like Cloud Run and Cloud Run functions. Usually, you want only authenticated callers to access your apps. You might also need to authenticate end users for your application. When you call a Google Cloud service, you're typically making an API call. There are three ways to authorize API calls to Google services. An API key is a string of characters that identifies the application. Using an API key associates the request with a Google Cloud project for billing and quota purposes. A compromised API key will allow full and long-lasting access to the API, so API keys are typically appropriate only for low security, read-only APIs. Most Google APIs do not accept API keys. A user account represents a person. The account is identified by the email address of the person. The act of logging in uses the email address and a separate credential, usually a password, to create an OAuth token. The token allows limited access to the API, based on the user's permissions, and expires after a period of time. OAuth tokens are typically more secure than API keys. A service account represents a workload or application, and is identified by its unique email address. An OAuth token for a service account provides access to the API based on the roles attached to the service account. We focus first on service account authentication. A service account acts as the identity for an application or a compute workload. The service account is used by your application to call a Google API or service so that users aren't directly involved in the authentication process. Each service account is identified by its unique email address. Service accounts enable authorization because you can assign specific IAM roles to a service account. Service accounts are authenticated by using an RSA private/public key pair. There is no password associated with a service account, so you can't use a service account to log in with a browser. You can assign specific IAM roles to a service account to provide the level of access required by the application. Unlike user accounts, service accounts do not have passwords. Instead, service accounts use RSA key pairs for authentication. The private key for a service account can be downloaded as a service account JSON file. In the early days of service accounts, downloaded service account keys were typically used whenever you needed to authenticate as an application. If you know the private key of a service account's key pair, you can use the private key to request an access token. The resulting access token can be used to interact with Google Cloud APIs on the service account's behalf. For this reason, having access to the private key is similar to knowing a user's password. Service account keys can become a security risk if not managed carefully. There are risks associated with using service account keys. The first risk is credential leakage. Consider what would happen if a developer committed a private key to a public code repository. A bad actor could use this key to gain access to resources in your environment. Another risk is privilege escalation. If a bad actor gets access to a service account key, they could use the key to escalate their own privileges. For example, the bad actor could use a service account with permission on a database to give themselves access to the database. Even after you manage to detect the danger and change the service account key, escalated privileges would remain. A third risk is identity masking. By authenticating as the service account, a bad actor might conceal their identity and actions. The best way to mitigate these risks is to avoid using downloaded service account keys and use other methods to authenticate service accounts whenever possible.

### Video - [Choosing an Authentication Method](https://www.cloudskillsboost.google/course_templates/874/video/520615)

* [YouTube: Choosing an Authentication Method](https://www.youtube.com/watch?v=aA6cgqgXOaQ)

There are multiple ways to authenticate to Google Cloud APIs from an application. This diagram will help you choose which method you should use based on your situation. The first question that you need to answer is whether your application is running on Google Cloud. If your app is running on Google Cloud, and your app is not running on Google Kubernetes Engine (GKE), there are two recommended methods for authentication. For development work in a local environment, you should use the "gcloud auth application-default login" command to let the application use your user credentials. For applications that are not running in a local development environment, attach a service account directly to the compute or serverless instance on Google Cloud. If your application is running on GKE, Workload Identity is the best solution. Workload Identity allows workloads in your GKE clusters to impersonate IAM service accounts to access Google Cloud APIs. If you're not running your application on Google Cloud, you need to decide whether federation is possible. You're able to set up Workload Identity Federation for workloads running in other clouds or running on-premises, as long as the cloud provider or on-premises deployment can generate an ID token. Workload Identity Federation lets you exchange an external provider token for a Google Cloud access token. With that access token, you can impersonate a regular service account and its permissions without requiring a service account key. If federation is not possible, your last resort is to use service account keys. Take great care to secure any service account keys that are used. Cloud Client Libraries use Application Default Credentials, or ADC, to find your application's credentials and let you access Google Cloud APIs. ADC uses a specific process to search for the application credentials. Your code will not need to change when you move it from your local development environment to a Google Cloud service like Cloud Run or GKE. Apps not running in Google Cloud can also use the same code. ADC checks credential locations in the following order: The existence of a GOOGLE_APPLICATION_CREDENTIALS environment variable is checked first. If the environment variable is set, ADC will use the service account file at the path specified by the environment variable. If the environment variable isn't set, ADC next checks a well-known location for user credentials that are set using the gcloud CLI. Finally, ADC will use an attached service account. Some services let you attach service accounts to a specific resource on the service. For example, Cloud Run and Cloud Run functions allow the attachment of a service account to a specific service or function respectively. ADC will use the service account attached to the service or function. If there is not a service account attached to the specific resource, ADC will use the default service account for the service being used, like Compute Engine, GKE, Cloud Run, or Cloud Run functions. If no credentials are found during all of these steps, an error will be thrown. The example shows a Python function that automatically finds the credentials. The code makes an authenticated API request without specifying credentials, so ADC will search for the credentials. Returning to our diagram, we explore each of the authentication methods. When you're using a local development environment, you can use the gcloud CLI command 'gcloud auth application-default login.' This command generates a JSON file containing user-access credentials and places it in the well-known location for ADC to find. These user-access credentials allow the application to make API calls with your user identity. The JSON file is secure, does not include your password, and is time-bound. If you have used the gcloud CLI before, you might be familiar with a similar command, 'gcloud auth login.' 'gcloud auth login' uses your credentials when you run gcloud CLI commands, but 'gcloud auth application-default login' uses your credentials when calling from your code. The preferred way to call Google Cloud APIs from applications running on Google Cloud serverless products, or Compute Engine, is to use attached service accounts. When you run on a Compute Engine VM, replace the default service account for the VM with a service account created specifically for the application. That service account should be given only the privileges needed by your application. Similarly, when you deploy a service to Cloud Run or create a function on Cloud Run functions, attach a service account to the service or function. Attaching a user-managed service account is the preferred way to provide credentials to ADC for production code that runs on Google Cloud. If you're running your application in GKE, Workload Identity is the recommended way to access Google Cloud APIs in a secure and manageable way. Workload Identity allows a Kubernetes service account in your GKE cluster to act as the IAM service account when your application calls Google Cloud APIs. Using Workload Identity lets you assign distinct, fine-grained identities and authorization for each application in your cluster. Workload Identity will automatically exchange Kubernetes service account tokens for IAM tokens when calling Google Cloud APIs. The first step for using Workload Identity is to enable Workload Identity on your cluster. You then allow the Kubernetes service account to impersonate the IAM service account you have created. ADC will manage the rest. Sometimes you need to run applications outside of Google Cloud, whether you're running multi-cloud or on-premises. Identity federation for your workloads lets you grant on-premises or multi-cloud workloads access to Google Cloud APIs without requiring a service account key. Traditionally, applications running outside Google Cloud have used service account keys for access. Service account keys are powerful credentials which can present a security risk when they are not managed correctly. Workload identity federation removes the need for service account keys for external applications that use an identity provider that supports OpenID Connect. An OpenID Connect ID token can be exchanged for a short-lived Google Cloud access token, which allows impersonation of an IAM service account. This process allows the external app to call Google Cloud services without having to secure or rotate a service account key. When you can't use federation, you might need to use a service account key to provide external applications with access to Google Cloud APIs. Using service account keys really is a last resort. If you must use service account keys, make sure you're using the best security practices. You should upload a public key for your service account instead of downloading a private key created by Google. You can use your own infrastructure to generate a public and private key pair, and then upload the public key to Google and securely deliver the private key to your runtime environment. This leaves less chance of error when handling your private key. Another best practice for service account keys is to never embed them in program source code or binaries. Keys can be extracted from binaries, and source code might be hosted in repositories, which makes keys more likely to be compromised. Finally, ensure that your service accounts follow the principle of least privilege. Grant only the minimum set of permissions necessary for the associated application. If the service account key is somehow compromised, you will limit the access for the bad actor.

### Video - [Other Authentication/Authorization Methods](https://www.cloudskillsboost.google/course_templates/874/video/520616)

* [YouTube: Other Authentication/Authorization Methods](https://www.youtube.com/watch?v=-CDyFeRnx3o)

Now we discuss some other authentication and authorization methods for applications. Your applications might need to access resources on behalf of a user. Examples of use cases where you might want to access resources on behalf of a user include: Your application needs access to BigQuery datasets that belong to your application users, or Your application needs to authenticate as a user to create projects or resources on their behalf. You can use the OAuth 2.0 protocol to access resources on behalf of a user. When your application requests access to the resources, the user is prompted for consent. If consent is provided, the application can then request credentials from an authorization server. The application can use those credentials to access resources on behalf of the user. Another way to provide access to users is to use Identity-Aware Proxy, or IAP. IAP controls access to your cloud applications running in your Google Cloud project. IAP verifies a user's identity and determines whether that user should be given access to the application based on configuration. The developer does not need to write code to control access. IAP lets you establish a central authorization layer for applications accessed by HTTPS. IAP helps you adopt an application-level access control model instead of relying on VPNs, network level firewalls, or complex authorization code in your applications. Firebase is an app development platform that helps you build mobile applications. Firebase Authentication provides features to help you add authentication and identity management to your mobile apps. Firebase Auth supports authentication using passwords, phone numbers, and popular federated identity providers like Google, Apple, and GitHub. Firebase Auth provides drop-in auth components that handle UI flows for sign-up and signing in and edge cases like account recovery. SDKs and ready-made UI libraries make development easy. After a successful login, you can access the user's profile and use the provided token in OAuth 2.0 and OpenID Connect flows for backend services. Identity Platform and Firebase Authentication offer similar functionality. Both let you easily sign users in to your apps by providing backend services, SDKs, and UI libraries. However, Identity Platform offers additional capabilities designed for enterprise customers. Identity Platform supports signing in with OpenID Connect and SAML authentication. Other enterprise features such as multi-factor authentication and integration with Identity-Aware Proxy are also supported.

### Video - [Using Secret Manager](https://www.cloudskillsboost.google/course_templates/874/video/520617)

* [YouTube: Using Secret Manager](https://www.youtube.com/watch?v=ytxv5EKdR74)

Finally, we discuss Secret Manager. To authenticate with other services and applications, many applications require credentials such as API keys, passwords, or certificates. These credentials must be stored securely. Developers often think about storing the credentials in flat files. Access becomes incredibly easy, but security can be difficult. Not everyone should be able to see this information, so you'd have to use the file operating system to restrict access. However, using flat files can lead to secrets being scattered across your organization's cloud and on-premises infrastructure. Secret Manager provides a secure and convenient way to store sensitive information. You can store, manage, and access secrets as binary blobs or text strings. Only users and applications with the appropriate permissions can access the secret. Keeping all of your secrets in a single place, and using IAM to determine who has access, simplifies secure management of your secrets. Now we take a quick look at some of the features of Secret Manager. The name of a secret stored in Secret Manager is global, but the secret data can optionally be stored in a specified region. Secrets can be versioned. Each version can have different secret data. There is no limit on the number of versions that can be stored. Versions cannot be modified, but they can be deleted. Secret Manager also follows the principle of least privilege. Secrets are created at the project level, and only project owners start with permission to create and access secrets within the project. Other roles must be explicitly granted IAM permissions to access secrets. When Cloud Audit Logs are enabled, every interaction with Secret Manager, including reads and updates, generates an audit entry. This auditing lets you verify that only the appropriate users and apps are accessing your secrets. Secret Manager manages server-side encryption keys on your behalf by using the same hardened key management systems used for Google's own encrypted data. Secret Manager also integrates with Cloud Key Management Service, or Cloud KMS. You can have Cloud KMS encrypt the version for a secret before storing it. After retrieving the version, Cloud KMS will also be needed to decrypt it. Next, we look at how Secret Manager works. First, we create a secret. You can use the Google Cloud console, the gcloud CLI, or code to create the secret. In this example, we create the secret by using a gcloud command. The command shown here creates a secret named "my-secret." The data file parameter takes the secret from standard input, passed in using the piped echo command. The replication policy is set to automatic, which lets the secret payload be stored in any region. A replication policy of "user-managed" lets you specify the regions where the secret might be stored. You can also retrieve secrets using the Google Cloud console, the gcloud CLI, or code. In this example, we use Python code to retrieve the secret. The Secret Manager Python SDK can be used to create, update, delete, or access secrets. The first step is to import the secretmanager library and create a client. Secrets are accessed by resource name, which includes the project ID, the ID or name of the secret, and the version you want to access. Versions are ordinal numbers: 1, 2, 3, and so on. You can also use the keyword "latest" to access the latest version for the secret. Retrieving the secret is as easy as passing the name to the library function, and decoding the payload. This process provides a secure method for using sensitive data within your applications.

### Quiz - [Quiz: Handling Authentication and Authorization](https://www.cloudskillsboost.google/course_templates/874/quizzes/520618)

#### Quiz 1.

> [!important]
> **How should you authenticate to Google Cloud APIs from your production application that is deployed to Cloud Run?**
>
> * [ ] Attach a service account to your application.
> * [ ] Use a service account key.
> * [ ] Use workload identity federation.
> * [ ] Use "gcloud auth application-default login."

#### Quiz 2.

> [!important]
> **Your photo-sharing application requires user login. You don't want to build a custom user authentication system that stores usernames and passwords. What is the best way to authenticate your users?**
>
> * [ ] You can use OAuth 2.0 to access resources on behalf of a user.
> * [ ] You can use Identity-Aware Proxy to provide application-level access.
> * [ ] You can leverage federated identity management by using Firebase Authentication.
> * [ ] You can give employees read permissions to critical resources in the project.

#### Quiz 3.

> [!important]
> **Your enterprise has an online expense reporting application. Employees must be able to access the application without having to log into the corporate VPN. How can you enable this type of access?
**
>
> * [ ] You can use OAuth 2.0 to access resources on behalf of a user.
> * [ ] You can give employees read permissions to critical resources in the project.
> * [ ] You can leverage federated identity management by using Firebase authentication.
> * [ ] You can use Identity-Aware Proxy to provide application-level access.

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/874/video/520619)

* [YouTube: Summary](https://www.youtube.com/watch?v=t6K3dESnkG0)

We began the discussion by reviewing authorization with Identity and Access Management (IAM). IAM lets you manage access control by defining who has what access for which resource. Application access to Google Cloud APIs may be controlled using service accounts. Your application assumes the identity of the service account to invoke Google Cloud APIs. You can create one or more service accounts to restrict access to different resources in your application. You also learned about risks associated with service account keys. A diagram helped explain how to decide which authentication method is best for your application. Identity-Aware Proxy, or IAP for short, lets you control access to your application. It verifies the user’s identity and checks whether that user should be allowed to access the application. End-users simply use an internet-accessible URL to access IAP-secured applications. No VPN is required! Firebase Authentication and Identity Platform let you add application users, and we discussed the differences between them. Finally, we looked at Secret Manager, which can help you securely store sensitive information for use in your applications.

## Adding Intelligence to Your Application

This module discusses how pre-trained machine learning APIs and generative AI can improve your cloud applications.

### Video - [Overview](https://www.cloudskillsboost.google/course_templates/874/video/520620)

* [YouTube: Overview](https://www.youtube.com/watch?v=eyA-ZEpFSfM)

Welcome to Developing Applications with Google Cloud: Foundations, module 5: Adding Intelligence to Your Application. Machine learning is about teaching machines to recognize patterns like humans do. Although even a two-year-old can easily distinguish between an apple and an orange, it's difficult to teach a computer to do the same thing. Google has developed pre-trained machine learning models and made them available to you as easy-to-use Google Cloud APIs. Now, with just a few lines of code, you can add artificial intelligence, or AI, to your own application. In this module, we explore Google's pre-trained machine learning APIs for vision, speech, video intelligence, and natural language processing. Generative AI takes artificial intelligence a step further. Generative AI is a type of artificial intelligence that creates new content based on what it has learned from existing content. Generative AI can make applications more powerful and the development experience more efficient. This module will introduce you to generative AI, and explain why developers should care about it.

### Video - [Using Pre-Trained Machine Learning Models](https://www.cloudskillsboost.google/course_templates/874/video/520621)

* [YouTube: Using Pre-Trained Machine Learning Models](https://www.youtube.com/watch?v=ukmx_YPTATk)

Pre-trained learning models can help you add powerful features to your applications. Google Cloud offers several pre-trained machine-learning (ML) models that you can use to add intelligence to your application. The Vision API lets you perform complex image detection. The Speech-to-Text and Text-to-Speech APIs enable developers to convert audio to text and text to audio. The Cloud Translation API lets you translate an arbitrary string into any supported language. The Cloud Translation API is highly responsive. Websites and applications can use the Cloud Translation API for fast, dynamic translation of text from a source language to a target language. The Cloud Natural Language API lets you extract information about entities that are mentioned in text documents, news articles, or blog posts. You can use the API to understand sentiment about your product on social media, or parse intent from customer conversations. The Video Intelligence API lets you search video files to extract and label entities at the shot, frame, or video level. The API annotates videos stored in Cloud Storage and helps you identify key entities in your video and when they occur within the video. AutoML on Vertex AI enables users with limited ML expertise to train high-quality models specific to their business needs. AutoML on Vertex AI lets you train models on images, tabular data, text, or videos without writing any code. You can also use your own data to build and train your own custom ML models by using frameworks like TensorFlow, PyTorch, and Vertex AI. It's really easy to invoke the REST APIs to implement machine learning in your application, no ML knowledge is required. In this example, we are using the Vision API to process an image that's stored in Cloud Storage. We invoke the REST API and send it a JSON request, and we receive a JSON response with attributes that describe the image. Let's take a look at a few examples now. The Vision API can categorize objects under labels and perform optical character recognition, or OCR. The Vision API can detect landmarks, logos, faces, and explicit content. For example, the Vision API can analyze faces and return information about emotions and head wear. In the wedding picture, the API accurately returns the emotional expressions on the faces in the picture. In the picture of the Sphinx, Vision API correctly detects that the image is from the Sphinx in Las Vegas and not the Sphinx in Egypt. The Speech-to-Text API enables developers to convert audio to text. It handles 110 languages and variants to support your global user base. You can transcribe the text of users dictating to an application’s microphone, enable command-and-control through voice, transcribe audio files, and more. Here's an example of how Google uses machine learning. Google’s conference room systems perform occupancy detection by using motion detection with the VC camera. Every 30 seconds, the system sends a Pub/Sub notification indicating whether motion was detected or not. It also sends a Pub/Sub notification when a call starts or ends. If motion is detected between 6 and 8 minutes after the meeting start time, the room counts as occupied. Otherwise, it's empty, and is available for someone else to reserve.

### Video - [Intro to Generative AI](https://www.cloudskillsboost.google/course_templates/874/video/520622)

* [YouTube: Intro to Generative AI](https://www.youtube.com/watch?v=6EV69036PPg)

One type of artificial intelligence that you can use for your applications is generative artificial intelligence, or generative AI for short. Generative AI is a type of artificial intelligence that creates new content based on what it has learned from existing content. We call this type of learning "training." A statistical model is created by using the existing content. You can provide an input, called a prompt, to the model, which can predict an expected response. New content can be generated based on the expected response. How does AI generate new content? It learns from a massive amount of existing content such as text, images, and audio. Training results in the creation of a “foundation model.” The most popular type of foundation model is a large language model, or LLM. LLMs are trained on text data only, but other types of foundation models might be trained on other types of data, like images or programming code. The foundation model can then be used directly to generate content and solve general problems, such as content extraction or document summarization. The model can also be trained further with new datasets in your field to solve specific problems, such as financial model generation or healthcare consulting. This training results in the creation of a new model that is tailored to your specific needs. How is generative AI different from traditional programming and other types of machine learning? In traditional programming, you have to specify the rules, then the machine acts on them and returns the answers. For example, using traditional programming, you might specify these attributes of a cat: type: animal legs: 4 ears: 2 fur: yes likes: yarn, catnip However, writing these algorithms is difficult because it’s impossible to implement all possible rules. So you need a new method: machine learning with neural networks. With machine learning, you feed the machine data and answers and let it discover the rules itself. For example, you train the machine on many pictures of cats and other animals. The machine learns the pattern and predicts whether a new picture is a cat. However, this type of learning is typically in a narrow field to solve a specific task. What if you want a machine to develop some fundamental intelligence to solve general problems? Generative AI aims to solve this problem. With generative AI, you feed a machine a huge amount of multimodal data. The machine learns a seemingly endless number of concepts and develops foundation models like an LLM. So when you ask the machine “what’s a cat,” it can give you everything it learned about a cat. So, what are large language models? Large language models refer to large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes. What does large mean? Large has two meanings. First is the enormous size of the training dataset, sometimes at the petabyte scale. Second it refers to the number of parameters, which now reaches billions and even trillions. Parameters are essentially the memories and knowledge that the machine has learned during model training. Parameters determine the ability of a model to solve a problem, such as predicting text. General-purpose means that the models are sufficient to solve common problems. The models work due to the commonality found in a human language, regardless of the specific tasks you are trying to do. This leads to the last point: pre-trained and fine-tuned. A large language model can be pre-trained for general purpose use with a large dataset. Later, it can be fine-tuned for a specific purpose by using a much smaller dataset. What are the potential use cases of generative AI? Generative AI can create content and bring your thoughts and visions to life. It can: Generate stories or poems based on prompts that you provide, or improve images based on instructions. Creation of content is an important benefit of generative AI, but it doesn't end there. Generative AI can summarize knowledge. Such as: Automatically summarizing video, audio, and paragraphs, or generating questions and answers based on the content. Generative AI can do search and discover for you. For example, it can: Search for a document, or Discover products based on desired features. Generative AI can also automate workflows. For instance, it can: Extract and label contracts, or classify feedback and create support tickets. You can use generative AI to create powerful and compelling applications. Generative AI will revolutionize how applications are developed. You will code your apps with help from your own generative AI-powered coding assistant. Features will include: Code generation. Generate code based on a natural language description of the desired code. Automatically generate unit tests for a piece of code or ask your assistant to optimize code. Documentation. The assistant can add comments to your code or generate release notes based on the changes. Code explanation. Ask your assistant to explain what the code does, and how it does it. Fixing code. Your AI assistant will be able to find bugs in your code, and then fix them. Code completion. As you type your code, the context of your code will be used to finish the line of code you're writing. Or your code editor might suggest code for the entire function. Code translation. Take code written in one coding language, and have your assistant translate it into another, while adhering to coding conventions of the new language. The Vertex AI Codey APIs, powered by the Codey foundation model, can assist with code generation, code chat, and code completion to provide these features. Gemini will provide you with this assistant to help you write code faster and more efficiently.

### Video - [Lab Overview](https://www.cloudskillsboost.google/course_templates/874/video/520623)

* [YouTube: Lab Overview](https://www.youtube.com/watch?v=Ni-v8XndA_k)

In this lab, "Adding User Authentication and Intelligence to Your Application," you enhance the bookshelf application by using OAuth to authenticate users. You use Secret Manager to store sensitive application data. Finally, you use the Cloud Translation API to translate the description of the book to the preferred language of the logged in user.

### Lab - [Developing Applications on Google Cloud: Adding User Authentication and Intelligence to Your Application](https://www.cloudskillsboost.google/course_templates/874/labs/520624)

In this lab, you add user authentication and user profiles to a Python application. You also use Secret Manager to store sensitive application data, and use a Google Cloud machine learning API to add a feature to the application.

* [ ] [Developing Applications on Google Cloud: Adding User Authentication and Intelligence to Your Application](../labs/Developing-Applications-on-Google-Cloud-Adding-User-Authentication-and-Intelligence-to-Your-Application.md)

### Quiz - [Quiz: Adding Intelligence to Your Application](https://www.cloudskillsboost.google/course_templates/874/quizzes/520625)

#### Quiz 1.

> [!important]
> **How can you invoke pre-trained machine learning APIs (such as the Vision API or Natural Language API) from your application?**
>
> * [ ] Use a gcloud CLI command.
> * [ ] Use the REST API.
> * [ ] Use the Google Cloud console.
> * [ ] Use TensorFlow.

#### Quiz 2.

> [!important]
> **You're developing an application that labels entities in video before storing the files. Which API should you use?**
>
> * [ ] Cloud Translation API
> * [ ] Cloud Natural Language API
> * [ ] Video Intelligence API
> * [ ] Vision API

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/874/video/520626)

* [YouTube: Summary](https://www.youtube.com/watch?v=wYFB04v4pfs)

We began this discussion by looking at the machine learning services, tools, and pretrained models that help you add intelligence to your applications. Google Cloud provides pretrained models for vision, speech, video intelligence, and natural language processing. You can use these pretrained models or train your own models without being a machine learning expert. You can then use these models to build exciting features in your applications. Google Cloud also provides tools like Vertex AI that can help you use your own data to build customized models. Using these models in your apps is as simple as making API calls. We also discussed generative AI, and how it differs from traditional programming. You learned about large language models and we discussed generative AI use cases for applications and application development.

## Deploying Applications

This module discusses how to build and deploy applications on Google Cloud.

### Video - [Overview](https://www.cloudskillsboost.google/course_templates/874/video/520627)

* [YouTube: Overview](https://www.youtube.com/watch?v=yHLewecWuKQ)

Welcome to Developing Applications with Google Cloud: Foundations, module 6: Deploying Applications. To run reliable services, you must have reliable release processes. When you have many engineers building separate application components, it's crucial that they are able to run unit tests, integration tests, and other tests quickly. With most user-facing software, teams want to release software often with new features and bug fixes. To enable high release velocity, build, test, and release processes must be automated as much as possible. In this module, you learn the components of a continuous integration and delivery pipeline. You also learn how to build container images for your application by using Cloud Build, and how to push your images to Artifact Registry.

### Video - [Continuous Integration and Delivery](https://www.cloudskillsboost.google/course_templates/874/video/520628)

* [YouTube: Continuous Integration and Delivery](https://www.youtube.com/watch?v=i8885AZgHJg)

A continuous integration and continuous delivery pipeline provides a stable, repeatable process for building and deploying your applications. Continuous integration occurs when developers commit their changes into a feature branch in a code repository, and a build service like Cloud Build is automatically triggered. Rules that you have established guide the generation of your application containers and executables. The application's artifacts are then stored in a repository like Artifact Registry. Continuous delivery is triggered when changes are pushed to the main branch in the repository. The build system builds the code and creates application images. The deployment system, like Cloud Deploy, then deploys application images to Cloud Run or GKE in the staging environment to automatically run integration and performance tests. If all tests pass, the build is tagged as a release candidate build. You can manually approve a release candidate build. This approval can trigger deployment to production environments as a canary or blue-green release. You can monitor the performance of your application in the production environment by using Cloud Monitoring. If the new deployment functions optimally, you can switch over all of your traffic to this new release. But if you discover problems, you can also quickly roll back to the last stable release. The continuous deployment workflow varies slightly in that there is no manual approval process. The deployment system automatically deploys release candidates to the production environment. When you first get started, this diagram shows a simple CI/CD pipeline that you might build. This type of pipeline provides an efficient and consistent build and deploy process. However, it's very important to consider security throughout the continuous integration and delivery process. Google Cloud provides several services that will help secure your builds and deployments. Google Cloud's Software Delivery Shield is a fully managed, end-to-end software supply chain security solution that protects every step of your CI/CD process. The Assured Open Source Software service lets you incorporate open Java and Python source packages that have been verified and tested by Google. These packages are built using Google's secure pipelines, and the packages are regularly scanned, analyzed, and tested for vulnerabilities. Google actively finds and fixes vulnerabilities in these packages. Cloud Build can import your source code and verified open source packages and execute your build on Google Cloud infrastructure. Cloud Build maintains verifiable metadata about the build, helping you to verify that a built artifact was created from trusted sources by a trusted build system. Artifact Registry lets you store, secure, and manage your build artifacts, and Artifact Analysis proactively detects vulnerabilities for artifacts in Artifact Registry. Artifact Analysis provides integrated on-demand and automated scanning for base container images and Maven and Go packages in containers. When you push a Java project to Artifact Registry, Artifact Analysis scans for vulnerabilities within the open source dependencies used by your Maven artifacts. After the initial scan, Artifact Analysis continuously monitors the metadata for scanned images in Artifact Registry for new vulnerabilities. Cloud Deploy automates delivery of your applications to a series of target environments in a defined sequence. It supports continuous delivery directly to Cloud Run and GKE, with one-click approvals and rollbacks. It also displays security insights for deployed applications. Binary Authorization helps establish, maintain, and verify a chain of trust along your software supply chain. Binary Authorization collects attestations, which are digital documents that certify that an image was built by successfully executing a specific, required process. Binary Authorization ensures that an image is deployed only when the attestations meet your organization's policy, and it alerts you when policy violations are found. GKE and Cloud Run also contribute to your pipeline security. GKE can assess your container security and give active guidance around cluster settings, workload configuration, and vulnerabilities. It can evaluate your GKE clusters and workloads and provide you with actionable recommendations to improve security. Cloud Run also contains a security panel that displays security insights about your builds and vulnerabilities in running services. Software Delivery Shield helps you fully secure your CI/CD pipeline.

### Video - [Containers and Building Application Images](https://www.cloudskillsboost.google/course_templates/874/video/520629)

* [YouTube: Containers and Building Application Images](https://www.youtube.com/watch?v=Yp0w73a98Pw)

Now we discuss containerized applications and how they can be built on Google Cloud. Containers are a preferred method for packaging and deploying your applications. Here's a quick refresher on Containers. Container-based virtualization is an alternative to hardware virtualization, as used in traditional virtual machines. Virtual machines are isolated from one another in part by having each virtual machine have its own instance of the operating system. But operating systems can be slow to boot and can be resource-heavy. Containers respond to these problems by using built-in capabilities of modern operating systems to isolate container environments from one another. A process is a running program. In Linux and Windows, the memory address spaces of running processes have long been isolated from one another. Popular implementations of software containers build on this isolation. They take advantage of additional operating-system features that give processes the capability to have their own namespaces and limit other processes’ access to resources. Containers start much faster than virtual machines and use fewer resources, because each container does not have its own instance of the operating system. Instead, developers configure each container with a minimal set of software libraries to do the job. A lightweight container runtime does the plumbing jobs needed to allow that container to launch and run, calling into the kernel as necessary. The container runtime also determines the image format. Containers can be deployed to both Cloud Run and GKE. So what does a container provide that a virtual machine does not? First, containers provide a clear separation of responsibility. Developers can focus on the application logic and any dependencies that are required for the application. The IT operations teams that will deploy and manage the application don't need to worry about application details like software versions and configurations. Second, containers provide workload portability. Containers are lightweight and can run virtually anywhere, from a developer's laptop to a VM running on-premises or in any cloud. The same application that is tested by a developer on their laptop and tested in an integration environment can run in the production environment. This workload portability simplifies promotion of the app during the development lifecycle and lets you move workloads between clouds and data centers with minimal effort. Third, containers provide application isolation. Containers virtualize CPU, memory, storage, and network resources at the operating system level. Applications are effectively running in their own environments, which lets containerized applications running on the same hardware use different versions of dependencies without affecting each other. By abstracting just the OS instead of the whole virtual computer, a container can "boot" in a fraction of a second. A virtual machine typically takes a minute or more to boot. The container image for your application is a complete package that contains the application binary and all the software required for the application to run. When you deploy the same container image in your development, test, and production environments, your application will perform the same way in each environment. Cloud Build is a fully managed service that lets you set up build pipelines to create a Docker container image for your application and push the image to Artifact Registry. You don't need to download build tools and container images to a build machine or manage your own build infrastructure. By using Artifact Registry and Cloud Build, you can create build pipelines that are automatically triggered when you commit code to a repository. In Cloud Build, you can create a build trigger that is executed based on a trigger type. A trigger type specifies whether a build should be triggered based on commits to a particular branch in a repository or commits that contain a particular tag. You create a build configuration file that specifies the steps in the build pipeline. Steps are analogous to commands or scripts that you execute to build your application. Each build step is a Docker container that's invoked by Cloud Build when the build is executed. The step name identifies the container to invoke for the build step. The images attribute contains the name of the container image to be created by this build configuration. Cloud Build lets you use different code repositories, tag container images to enable searches, and create build steps that perform operations like downloading and processing data. The build configuration can be specified in a YAML or JSON format. Cloud Build mounts your source code into the “/workspace” directory of the Docker container associated with a build step. The artifacts produced by each build step are persisted in the “/workspace” folder and can be used by the following build step. Cloud Build automatically pushes the built container image to Artifact Registry. In Artifact Registry, you can view the status and history of builds for a given container. Cloud Build can also publish build status notifications to Pub/Sub. You can subscribe to these notifications to act based on build status or other attributes.

### Quiz - [Quiz: Deploying Applications](https://www.cloudskillsboost.google/course_templates/874/quizzes/520630)

#### Quiz 1.

> [!important]
> **Which of the following statements about continuous integration and delivery are accurate? Select two.**
>
> * [ ] Continuous delivery is a workflow that is triggered when changes are pushed to the main branch in the repository.
> * [ ] To benefit from continuous integration and delivery developers must use GitHub.
> * [ ] Continuous integration is a developer workflow in which developers pull from the main branch and commit their changes into a feature branch in a source code repository.
> * [ ] Any continuous integration and delivery pipeline built on Google Cloud will automatically provide best security practices.
> * [ ] If all tests pass, builds generated from continuous integration in a feature branch can be released on a production environment.

#### Quiz 2.

> [!important]
> **How can Cloud Build and Artifact Registry help you build a continuous integration and delivery pipeline? Select two.**
>
> * [ ] Each build step in a Cloud Build configuration specifies how to build an application container image.
> * [ ] With Cloud Build, the artifacts produced by each build step are persisted in the /workspace folder and can be used by the following build step.
> * [ ] You can install Cloud Build on Google Kubernetes Engine. GKE will autoscale depending on the number of builds.
> * [ ] By using Artifact Registry and Cloud Build, you can create build pipelines that are automatically triggered when you commit code to a repository.
> * [ ] When you commit code to a repository, you must start a build manually using a gcloud CLI command.

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/874/video/520631)

* [YouTube: Summary](https://www.youtube.com/watch?v=Qq_g-M5KGTo)

In this module, you learned how continuous integration and delivery can be used to build a repeatable process for building and deploying your applications. We also discussed how to build and maintain container images by using Cloud Build and Artifact Registry.

## Compute Options for Your Application

This module discusses the compute options available for running your applications in Google Cloud.

### Video - [Overview](https://www.cloudskillsboost.google/course_templates/874/video/520632)

* [YouTube: Overview](https://www.youtube.com/watch?v=ZavaI-GZznk)

Welcome to Developing Applications with Google Cloud: Foundations, Module 7: Compute Options for Your Application. Google Cloud has a range of compute options that you can use to run your applications. You can choose a platform that matches the needs of your application, including the level of control that you need for the infrastructure. Having more control over the infrastructure usually leads to a greater operational burden. If you use Cloud Client Libraries in your application to work with Google Cloud services, you can usually move to another platform without reworking your application. In this module, you learn about which use cases are most appropriate for each compute option and how to decide between them. Google Cloud provides a wide range of platforms on which to run your applications. Compute Engine lets you create virtual machines, or VMs, that mimic the servers you might have used in a traditional data center. Virtual machines are highly flexible: they let you run the same wide range of applications you can run on physical hardware, but now on Google's infrastructure. Google Kubernetes Engine, or GKE, is Google Cloud's managed service for running containers and managing the virtual machines used to run them. With GKE, a cluster of virtual machines is created for running your containerized applications. When you deploy applications to the cluster, GKE manages the scaling and security for the cluster and applications, reducing the operational costs of running your applications. Cloud Run is a fully managed serverless platform that also runs containerized applications. Unlike with GKE, all infrastructure management for Cloud Run is abstracted away. Cloud Run automatically scales up and down from zero almost instantaneously, depending on traffic. You only pay when your code is running. Cloud Run functions also lets you run your code in the cloud with zero management of servers or containers. Write your event-driven code, and Google Cloud manages everything, scaling from zero to planet-scale. So, which compute option should you use to run your applications? Well, it depends.

### Video - [Compute Engine](https://www.cloudskillsboost.google/course_templates/874/video/520633)

* [YouTube: Compute Engine](https://www.youtube.com/watch?v=a1cIuXiIqNg)

First, we start with Compute Engine. Compute Engine is the most flexible option for running your applications, but it requires the most operational effort to manage. With Compute Engine, you create high-performance virtual machines and then install and run your applications on them. Compute Engine supports predefined machine types for popular configurations but also lets you create custom machine types to customize CPU and memory for your VMs. Compute Engine lets you create and attach persistent disks and local SSDs. These disks can be accessed like physical disks in a desktop or server. Unlike typical physical disks, Compute Engine disks can be increased in size while they are running. The performance and throughput of persistent disks increase when they increase in size. Compute Engine provides preemptible VMs that are ideal for large compute and batch jobs. If capacity must be reclaimed, Google Cloud can terminate preemptible VMs. For applications that can handle these interruptions, preemptible VMs are available at a discount of at least 60% compared to standard VMs. You can run your choice of operating system on your VMs, including Debian, CentOS, Ubuntu, and various other flavors of Linux or Windows. You can also use a shared image from the Google Cloud community or bring your own operating system. Use Compute Engine when you want full control of your infrastructure. Compute Engine lets you create highly customized VMs for specialized applications that have unique compute or operating system requirements. You can install and patch software that runs on a VM. You can create managed instance groups of VMs based on an instance template and configure global load balancing and autoscaling of the managed instance groups. Compute Engine can perform health checks and replace unhealthy instances in an instance group. Compute Engine can also autoscale the number of instances based on the traffic volume in specific regions. Compute Engine offers you the most flexibility to configure your resources for the specific type of application that you need to run. You can install and run any third-party licensed software on Compute Engine. You can attach graphics processing units (GPUs) and Tensor Processing Units (TPUs) to Compute Engine VMs to accelerate parallel processing and machine learning workloads. You can use Compute Engine for applications that require TCP network protocols other than HTTP or HTTPS. Compute Engine is ideal for lift-and-shift migrations. You can move virtual machines from your on-premises data center, or another cloud provider, to Google Cloud without changing your application.

### Video - [Google Kubernetes Engine](https://www.cloudskillsboost.google/course_templates/874/video/520634)

* [YouTube: Google Kubernetes Engine](https://www.youtube.com/watch?v=IVnLa0R3y74)

Next is Google Kubernetes Engine. Kubernetes is a leading open source platform for deploying, scaling, and operating containers. Kubernetes, first developed at Google, is now a Cloud Native Computing Foundation project with a large and active community. Kubernetes provides you with a framework to run distributed containerized systems resiliently and at scale. It manages many operational tasks, such as scaling application components, providing network abstractions, orchestrating failovers, rolling out deployments, storage orchestration, and management of secrets and configurations. A Kubernetes cluster contains control plane and worker nodes. The nodes in a cluster are the machines—virtual or physical—that run your applications. The Kubernetes control plane manages the worker nodes and the Pods in the cluster. A Pod is a group of containers that share networking and storage resources on the node. Google Kubernetes Engine, or GKE, is a managed Kubernetes service on Google infrastructure. GKE helps you deploy, manage, and scale Kubernetes environments for your containerized applications on Google Cloud. More specifically, GKE is a component of the Google Cloud compute offerings that facilitates bringing your Kubernetes workloads into the cloud. For an unmanaged cluster, you need to manage most of the operational aspects of the cluster yourself. GKE handles much of this operational effort for you automatically by eliminating many of the infrastructure tasks required to create and manage a Kubernetes cluster. With GKE, Google manages most of your cluster tasks. Google manages the control plane, scaling of Pods, node patching and upgrades, and the monitoring, availability, and reliability of the cluster. By default, you manage the underlying nodes and node pools, including provisioning, maintenance, and lifecycle management. You're also responsible for selecting the security and networking configuration for your cluster. This level of management is the standard mode for GKE. GKE Autopilot is a mode of operation in which the entire cluster’s infrastructure is managed for you, including control plane, node pools, and nodes. By managing the cluster infrastructure, Autopilot helps reduce operational and maintenance costs while improving resource utilization. Autopilot is a fully managed Kubernetes experience that lets you focus on your workloads instead of the management of the cluster's infrastructure. Autopilot automatically implements GKE hardening guidelines and security and networking best practices and blocks less safe practices. GKE Standard mode provides customers with advanced configuration flexibility over the cluster infrastructure. GKE Autopilot mode lets Google provision and manage the entire cluster and underlying infrastructure. You can use different modes for different clusters, depending on how much infrastructure control you need. GKE is fully managed, which means that you don’t have to provision the underlying resources. GKE uses a container-optimized operating system to run your workloads. Google maintains this operating system, which is optimized to scale quickly with a minimal resource footprint. When you use GKE, you start by directing the service to instantiate a Kubernetes cluster for you. The GKE auto-upgrade feature, when enabled, ensures that your clusters are always automatically upgraded with the latest stable version of Kubernetes. The virtual machines that host your containers in a GKE cluster are called nodes. Auto-repair can automatically repair unhealthy nodes for you. It performs periodic health checks on each node of the cluster. If a node is determined to be unhealthy and requires repair, GKE will drain the node, thus allowing workloads to gracefully exit. It will then recreate the node. GKE and Kubernetes both support the scaling of workloads within a cluster. GKE also supports scaling of the cluster itself. GKE uses Cloud Monitoring and Cloud Logging to help you monitor and understand your applications’ performance and behavior. GKE seamlessly integrates with many parts of Google Cloud. With Cloud Build, you can use private container images that you have securely stored in Artifact Registry to automate the deployment of your workloads. Identity and Access Management lets you control access by using accounts and role permissions. GKE is integrated with Virtual Private Clouds, or VPCs, which lets you use Google Cloud's networking features. And finally, the Google Cloud Console provides insights into GKE clusters and their resources, thus letting you view, inspect, and delete resources in those clusters. GKE supports any application runtime that you can package as a Docker image. GKE is ideally suited for containerized applications, including third-party containerized software. You can run your container image on Kubernetes in a hybrid or multi cloud environment. This feature is especially helpful when some parts of your application run on-premises and other parts run in the cloud. You can use GKE to run containerized applications that use network protocols other than HTTP and HTTPS. Managing the infrastructure for a Kubernetes environment can be complex. GKE simplifies many of the operational tasks associated with provisioning and managing the infrastructure. With GKE, Google Cloud persistent disks are automatically provisioned by default when you create Kubernetes persistent volumes to provide storage for stateful applications. GKE automatically provisions Google Cloud network load balancers when you deploy Kubernetes network load balancer services, and provisions Google Cloud HTTP and HTTP(S) Load Balancing when you configure Kubernetes Ingress resources. This auto-provisioning feature eliminates the need to configure and manage these resources manually. GKE has support for Google Cloud Observability, which provides integration with tools for troubleshooting and application and service monitoring. GKE simplifies cluster deployment and scaling. You can describe the compute, memory, network, and storage resources that you want to make available across all the containers required by your applications. GKE will provision and manage the underlying Google Cloud resources automatically. You can either deploy fixed-size clusters or configure your clusters to automatically scale. Autoscaling adds or removes compute instances in response to changes in the resource requirements of the containers that run inside the cluster. You deploy and manage your containerized application for GKE the same way you would for any other Kubernetes environment. You can use the “kubectl” command to perform most operational tasks. Although you can deploy ad hoc resources directly by using kubectl commands, the recommended best practice is to use YAML manifest files to define configurations. These files define the properties of the containers that are used for the components in your applications. Manifest files can also define the network services, security policies, and other Kubernetes objects that are used to deliver resilient, scalable, containerized applications. Applications can be deployed by using Deployments, where Kubernetes continually ensures that a specified number of replicas for a Pod or set of Pods is running. The deployment shown here is for stateless components. You can also use StatefulSets for applications where you need persistent storage. You can also use YAML manifests to define a range of other resource types. As a part of a continuous integration and delivery (CI/CD) pipeline, you can generate a new Docker image for each code commit. The CI/CD pipeline can automatically deploy the image to development, test, and production environments. Cloud Build, Artifact Registry, Cloud Deploy, and GKE can be used to create a strong CI/CD system.

### Video - [Cloud Run](https://www.cloudskillsboost.google/course_templates/874/video/520635)

* [YouTube: Cloud Run](https://www.youtube.com/watch?v=VGfuG_q-vco)

Next is Cloud Run. Cloud Run is a fully managed compute platform that allows you to run request or event-driven stateless workloads without having to worry about servers. It abstracts away all infrastructure management such as provisioning, configuring, and managing servers, so you can focus on writing code. It automatically scales up and down from zero, almost instantaneously, depending on traffic, so you never have to worry about scale configuration. Cloud Run also charges you only for the resources you use, rounded up to the nearest 100 milliseconds, so you never pay for overprovisioned resources. Many serverless platforms add constraints around support for languages and libraries, or even restrict the way you code. Cloud Run enables you to write code your way by letting you easily deploy any stateless containers that listen for requests or events delivered over HTTP. Containers offer flexibility and portability of workloads. With Cloud Run, you can build your applications in any language, using whatever frameworks and tools you want, and deploy them in seconds. You can use a single command to deploy containerized applications. After that, Cloud Run horizontally scales your container image automatically in order to handle received requests, then scales it down when demand decreases. You only pay for the CPU, memory, and networking that are consumed during request handling. The Cloud Run developer workflow is a straightforward three-step process. First, use your favorite programming language to write your application. This application should listen for web requests. Second, build and package your application into a container image. Finally, deploy the container image to Cloud Run. When you deploy your container image, you get a unique HTTP(S) URL. Cloud Run starts your container on demand to handle requests and dynamically adds and removes containers to ensure capacity to handle all incoming requests. When you build a container image, you have full control over how the software is built and how every file is added. Sometimes you need this control. However, building an application is difficult enough already, and sometimes you just want to turn source code into an HTTPS endpoint. You want to ensure that your container image is secure, well configured, and built in a consistent way. With Cloud Run, you can choose either approach. You can use a container-based workflow or a source-based workflow. When you use the source-based approach, you deploy your source code instead of a container image. Cloud Run then builds your source and packages the application into a secure container image for you automatically. Cloud Run uses buildpacks to automatically build container images. Buildpacks transform your application source code into container images that can run on any cloud. Google Cloud buildpacks are also used for the internal build system for Cloud Run functions and App Engine. Cloud Run can be used for many use cases. Cloud Run is an excellent choice for data processing applications. Because you pay for usage—not provisioned resources—you only pay when data is being processed. Cloud Run can also host small or large web applications and web APIs. Cloud Run scales up when necessary and scales down to zero when it's not in use. Jobs that must be run in response to Pub/Sub or Eventarc events are good candidates for Cloud Run. Cloud Run jobs work differently from HTTP Cloud Run services. A Cloud Run job doesn't listen for and serve HTTP requests. There is no need to listen on a port or start a web server. Instead, the job is executed as a one-off task or as part of a workflow. You can also use Cloud Scheduler to run a job on a regular schedule. When a Cloud Run job finishes, the job exits. Each job can be composed of a single task or multiple independent tasks. Because multiple tasks within a job are independent, the tasks can be run in parallel. Each task execution is aware of its task index, which is provided to the task in an environment variable. In addition, tasks that fail can be automatically retried. The maximum number of parallel tasks can be specified so that you don't overwhelm backend services with too many concurrent tasks. Each task within a job runs a single container image. This container runs to completion. Like Cloud Run services, Cloud Run jobs run on a fully serverless platform. You don't need to manage any infrastructure to run your jobs.

### Video - [Cloud Run functions](https://www.cloudskillsboost.google/course_templates/874/video/520636)

* [YouTube: Cloud Run functions](https://www.youtube.com/watch?v=9sQf8tkXt3Y)

Next, we look at Cloud Run functions, previously known as Cloud Functions. With Cloud Run functions, you can develop an application that is event-driven, serverless, and highly scalable. Each function is a lightweight microservice that allows you to integrate application components and data sources. Cloud Run functions is ideal for microservices that require a small piece of code to quickly process data in response to an event. Cloud Run functions is priced according to how long your function runs, the number of invocations, and the resources that you provision for the function. In this example, all components of the application use fully managed services: Cloud Storage, Pub/Sub, Cloud Run functions, the Vision API, and the Cloud Translation API. These services scale automatically depending on the volume of incoming data, and required compute capacity. This scalability and reliability allows you to focus on your application code. Cloud Run functions is appropriate for many use cases. You can use Cloud Run functions for lightweight extract-transform-load, or ETL, operations or for processing messages that are published to a Pub/Sub topic. Cloud Run functions can also serve as a target for webhooks, which allow applications or services to make direct HTTP calls to invoke microservices. Any lightweight functionality that is run in response to an event is a candidate for Cloud Run functions. Cloud Run functions lets you focus on code. Let’s look at Node.js as an example. When you use the Node.js runtime, your function's source code must be exported in a Node.js module. You do not need to upload zip files with packaged dependencies. You can specify any dependencies for your Node.js Cloud Run function in a package.json file. The Cloud Run functions service automatically installs all dependencies before running your code. You can use Cloud Client Libraries to programmatically interact with other Google Cloud services. Cloud Run functions currently supports Node.js, Python, Go, Java, . NET, Ruby, and PHP.

### Video - [Comparisons](https://www.cloudskillsboost.google/course_templates/874/video/520637)

* [YouTube: Comparisons](https://www.youtube.com/watch?v=Iv1Ps-zTFAY)

Now we compare the platforms to understand their relative strengths. After we’ve talked about those platforms, the question remains: where should I run my applications? And the answer is still “it depends.” The first question that you might ask is “how much infrastructure control do I need?” If you want to lift-and-shift legacy systems to the cloud, or you have specific licensing requirements that depend on specific hardware, you might need to use Compute Engine. If you can run containers and have hybrid systems on multiple clouds or data centers, or if you have applications that are not HTTP-based, Google Kubernetes Engine might be the right choice. If you want to run stateless containers but not manage the infrastructure at all, Cloud Run might be the best choice. And if you just need to write event-driven functions to connect cloud services, Cloud Run functions is probably the correct choice. Gaining more control over the infrastructure requires more operational effort. When you create a Compute Engine virtual machine, you control the updates for the operating system and software. With GKE, Google manages the virtual machine nodes for your cluster, but you still manage the size of the cluster and decide how to scale each application within the cluster. Cloud Run and Cloud Run functions are serverless. For those platforms, you just need to deploy your application, and Google manages the infrastructure and autoscaling. Another question is “how are my teams structured?” If your teams are mostly developer-focused, Cloud Run and Cloud Run functions are probably best for you. If you have both developers and operations teams, you might still use Cloud Run and Cloud Run functions when appropriate. You might also decide to use Google Kubernetes Engine to integrate with your hybrid systems and have more control over your workloads. GKE also lets you use GPUs, TPUs, and non-HTTP network protocols, which are not available for Cloud Run and Cloud Run functions. If you're modernizing your applications over time, you might need to manage Compute Engine VMs that have been migrated from on-premises data centers. Your operations team must be able to manage the health and security of these virtual machines. Pricing for the platforms is different, which might affect your choice as well. Compute Engine and Google Kubernetes Engine charges are based on the usage of dedicated VMs. The charges are predictable, and these platforms can be ideal when you require consistent capacity for your applications. Cloud Run and Cloud Run functions are pay per use, which can result in significant savings, especially when your traffic patterns are inconsistent. This sketch note, titled "Where should I run my stuff," provides a visualization of the relative strengths of the platforms. In general, as you move toward the left, you have more control over your application and infrastructure, but managing that infrastructure requires more operational cost and effort. The two platforms on the right are serverless, which requires much less operational burden and lets you focus on the application instead of the infrastructure. Different applications have different requirements, and you can choose the platform that best meets your requirements. You don't need to standardize on a single platform, even within a single application. Larger applications might benefit from using multiple platforms that allow them to solve each problem with the correct tool. Applications that don't have unusual infrastructure requirements can typically be run on multiple platforms. If you use the Cloud Client Libraries in your code, moving code from one platform to another is usually easy. Containerized applications can be run on both Cloud Run and Google Kubernetes Engine. When you don't want to manage the containers, you can deploy your code to Cloud Run functions or deploy to Google Kubernetes Engine or Cloud Run by using buildpacks. The ease of running the same application on different platforms means that you don't have to worry too much about your decision. For example, you might initially decide to run your application on Cloud Run, with Google managing the infrastructure and scaling. Later, when your application starts getting more traffic, you might decide to run the same application on Google Kubernetes Engine for more control. We've talked about four platforms, but haven't yet talked about App Engine. App Engine is a fully managed serverless compute option used to build and deploy low latency, highly scalable web applications. App Engine supports two environments: standard and flexible. The App Engine standard environment runs your code in a sandbox, and doesn’t require you to build containers. However, your applications must be written with specific versions of supported languages. The standard environment responds well to spikes in traffic by scaling up within seconds and scaling down to zero after 15 minutes of inactivity. You pay based on the computing instances that are running your application, but do not need to manage those instances. You pay nothing when your application scales down to zero. The standard environment is a reasonable choice for non-containerized applications with spikes in traffic. The App Engine flexible environment requires you to create a container for your application, but you gain flexibility by doing so. Your code can be written by using any languages and libraries. The flexible environment is better for applications that have sustained traffic, because it scales up and down much slower than the standard environment does, and cannot scale to zero. Your application will always need to run in at least one instance. Like the standard environment, you pay for all computing instances that are used by the flexible environment. Cloud Run provides many of the best features of both App Engine environments. Cloud Run applications are required to run in containers. If you do not need custom containers, buildpacks can automatically create the containers for you. Cloud Run can scale up and down almost immediately in response to traffic spikes. Unlike App Engine, you only pay for Cloud Run when you are processing requests, rounded up to the nearest tenth of a second. This pricing model can result in significant cost savings for your application. App Engine is fully supported, and it works well for creating web applications and web APIs, but Cloud Run is often a better choice for these use cases. Returning to the question "Where should I run my applications?", the best answer is that you should run each application on the platform that best fits its requirements. Most applications written with the Cloud Client Libraries can be easily moved from platform to platform, so you can change your decision later. If you do not have complex infrastructure requirements, start with a serverless platform that lets you focus on the application instead of the infrastructure. If you later want more control over the infrastructure, you can move your application to a platform that requires more operational effort but provides the needed control or flexibility.

### Quiz - [Quiz: Compute Options for Your Application](https://www.cloudskillsboost.google/course_templates/874/quizzes/520638)

#### Quiz 1.

> [!important]
> **Your code needs to create a thumbnail of an image in response to a Pub/Sub event. Which of the following execution environments should you consider?**
>
> * [ ] Google Kubernetes Engine
> * [ ] App Engine (flexible environment)
> * [ ] Compute Engine
> * [ ] Cloud Run functions

#### Quiz 2.

> [!important]
> **You have an application running on virtual machines in another cloud provider. You want to move this application into Google Cloud with as few changes as possible. Which execution environment should you consider?**
>
> * [ ] Compute Engine
> * [ ] Google Kubernetes Engine
> * [ ] Cloud Run functions
> * [ ] Cloud Run

#### Quiz 3.

> [!important]
> **Your application is containerized, requires automatic scaling, and accepts requests over TCP. You do not want to manage infrastructure. Which would be the ideal execution environment for your application?
**
>
> * [ ] Cloud Run
> * [ ] Cloud Run functions
> * [ ] Google Kubernetes Engine with Autopilot
> * [ ] Compute Engine

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/874/video/520639)

* [YouTube: Summary](https://www.youtube.com/watch?v=aobbFMwGJFc)

Google Cloud offers a range of compute options depending on the needs of your application and the level of operational control that you require. We compared the benefits of Compute Engine, GKE, Cloud Run, and Cloud Run functions. Cloud Client Libraries are the recommended way to programmatically interact with Google Cloud services. With this approach, you can move your application to a different platform when your needs change.

## Monitoring and Performance Tuning

This module discusses the application use cases of the services in Google Cloud Observability.

### Video - [Overview](https://www.cloudskillsboost.google/course_templates/874/video/520640)

* [YouTube: Overview](https://www.youtube.com/watch?v=U75vD6CxyKg)

Welcome to Developing Applications with Google Cloud: Foundations, Module 8: Monitoring and Performance Tuning. Google Cloud Observability combines metrics, logs, and metadata, whether you're running on Google Cloud, Amazon Web Services, on-premises infrastructure, or a hybrid cloud. You can quickly understand service behaviors and issues, from a single comprehensive view of your environment, and act if needed.

### Video - [Google Cloud Observability](https://www.cloudskillsboost.google/course_templates/874/video/520641)

* [YouTube: Google Cloud Observability](https://www.youtube.com/watch?v=1o5iEIPPe7w)

We start by looking at the components of Google Cloud Observability. The integrated managed services in Google Cloud Observability help you manage your running services and applications. Cloud Monitoring provides visibility into the performance, availability, and overall health of cloud applications. You can collect metrics, events, and metadata from Google Cloud services and applications, and create alerting policies to provide timely awareness of problems in your applications. Cloud Logging is a fully managed service that ingests application and platform log data, as well as custom log data from GKE environments, VMs, and other services inside and outside of Google Cloud. Cloud Logging lets you search and filter your logs, and provides you the ability to troubleshoot and analyze log data for your applications. Error Reporting counts, analyzes, and aggregates errors produced in your running cloud services. You add the error reporting library to your application, and then report an error when it occurs. The Error Reporting dashboard displays a summary of each error found and the number of occurrences of each error, and notifications can be sent when a new or resolved error occurs. Cloud Trace is a distributed tracing system for Google Cloud that collects latency data from applications and displays it in near real-time in the Google Cloud console. Cloud Trace helps you understand how long it takes your application to handle incoming requests and analyze latency across services in the application. Cloud Profiler uses statistical techniques, and low-impact instrumentation that runs across all production application instances to provide a complete picture of an application’s performance without slowing it down. Cloud Profiler helps you identify and eliminate potential performance issues. Cloud Monitoring helps increase reliability by giving users the ability to monitor Google Cloud and multi-cloud environments to identify trends and prevent issues. With Cloud Monitoring, you can reduce monitoring overhead and improve your signal-to-noise ratio, letting you detect and fix problems faster. Why should application developers care about monitoring their applications? Monitoring is the foundation of application reliability. With Cloud Monitoring, you can build custom dashboards and use out-of-the-box dashboards to answer basic questions about your application. You can monitor your apps and infrastructure, whether they are running in Google Cloud, on-premises, or in other clouds. These dashboards let you monitor the health of your infrastructure and applications and easily spot anomalies. Monitoring over time also lets you see trends in application usage patterns. How large is my database, and how fast is it growing? How quickly is my daily active user count growing? Which features of my application are used the most? Monitoring also can tell you what needs urgent attention. Is the application broken, or will it soon break? Alerting on metric trends or limits lets you be notified of issues and fix them before they cause catastrophic failures. Monitoring can also be useful for conducting retrospective analysis. The latency of our application increased dramatically overnight -- what else happened around the same time? Is there a condition that I can alert on to let me fix the issue before it affects users? At a minimum, there are some key metrics that you should capture for your applications. You should create application dashboards that include the four golden signals: Latency, traffic, errors, and saturation. Latency is the amount of time that it takes to serve a request. Make sure to distinguish between the latency of successful and unsuccessful requests. For example, an HTTP error that occurs due to a loss of connection to a database or another backend service, might be solved really quickly. However, because an HTTP 500 error indicates a failed request, including 500 errors in your overall latency might result in misleading metrics. Traffic is a measure of how much demand is placed on your system. It's measured as a system-specific metric. For example, web server traffic is measured as the number of HTTP or HTTPS requests per second. Traffic to a NoSQL database is measured as the number of read or write operations per second. Errors indicate the number of failed requests. Criteria for failure might be anything like an explicit error, such as an HTTP 500 error, or a successful HTTP 200 response but with incorrect content. It might also be a policy error. For example, your application promises a response time of one second, but some requests take over a second. Saturation indicates how full your application is, or what resources are being stretched and reaching target capacity. Systems can degrade in performance before they achieve 100% utilization, so make sure to set utilization targets carefully.

### Video - [Logging and metrics](https://www.cloudskillsboost.google/course_templates/874/video/520642)

* [YouTube: Logging and metrics](https://www.youtube.com/watch?v=6U0NKwXytWE)

Cloud Logging is one of the tools that can be most valuable for app developers. Logs and metrics will help you understand how your application is working. A robust system of logging is crucial for developer productivity and to help you understand the state of your application. Cloud Logging is a real-time log-management system with storage, search, analysis, and monitoring support. Cloud Logging automatically collects logs from Google Cloud resources. You can also collect logs from your applications. Logging is an important tool for developers. Logs can help you troubleshoot your application. Using the Logs Explorer you can view individual log entries and find related log entries. Logging details about the flow of calls within your application helps you understand how your application is working, or why it's not working as intended. Logs can also be used to analyze application performance. For example, you can use the Log Analytics interface to perform aggregate operations on your logs. With this interface, you use SQL to query your log data and help you understand performance. You can configure Cloud Logging to notify you when certain kinds of events occur in your logs. You can create a log-based alert to send a notification when a particular pattern appears in a log entry. Alternatively, you might want to monitor trends or the occurrence of events over time. For these situations, you can create a log-based metric. Log-based metrics are suitable whenever you want to count the occurrences of a message and be notified when the number of occurrences crosses a threshold. Metrics can also be used to observe trends in your data, and receive a notification if the values change in an unacceptable way. You can install the Ops Agent on Compute Engine instances to stream logs and metrics from third-party applications into Cloud Logging. The Ops Agent uses Fluent Bit for high throughput logging, and the OpenTelemetry Collector for metrics. Ops Agent automatically collects logs from standard system log locations like /var/log/syslog. File-based logs can also be configured, with customizable paths and refresh interval. Ops Agent also supports flexible processing of logged data. You can configure Ops Agent to parse text logs into structured logs, modify log entries by removing, renaming, or setting fields, or exclude logs based on labels and regular expressions. Ops Agent also supports collection of standard system metrics without any configuration. Collected system metrics include CPU, disk, memory, network, and processes. Curated third-party application metrics can also be collected. Apache Tomcat, Apache Web Server, and nginx are examples of third-party applications that are supported by Ops Agent. Cloud Logging is pre-configured in other compute environments. Cloud Run and Cloud Run functions have built-in support for logging. Logs written by applications are automatically sent to Cloud Logging. You can also enable logging on Google Kubernetes Engine by enabling the Observability for GKE integration for your cluster. Kubernetes logs are not stored permanently within GKE. For example, GKE container logs are removed when their host Pod is removed. System logs are periodically removed to free up space for new logs, and cluster events are removed after one hour. Sending logs to Cloud Logging ensures that log entries are kept as long as you deem necessary. For Cloud Run and Cloud Run functions, you can simply write to stdout and stderr, and the logs will automatically be delivered to Cloud Logging. Text strings are put into the textPayload field of the log entry. Other fields, like the time the log was received, the resource that produced the log entry, and the log name, will also be logged with a text message. It's recommended that you use structured logs instead of text logs. Text logs do not have a log level, so it can be hard to find the really important content inside the text logs. Fields within structured log data can be searched using queries, which makes log analysis much easier. For a structured log entry, you log a single line of serialized JSON, which is placed in the jsonPayload field of the log entry. When you use structured JSON data, some special fields are stripped from the jsonPayload and written to the corresponding field in the generated log entry. For example, you can set the log level by specifying a severity field. The message property will be used as the main display text of the log when it’s provided. In this example, the first line would create a text log entry. The rest of the code shows an example of how to create a structured log entry. The structured entry uses special fields to create labels, the log severity, and a component field within the log entry. Another tool you might want to consider for logging and alerting is Prometheus. Prometheus is an open source systems monitoring and alerting toolkit. It can monitor services running on VMs and Kubernetes. It’s a very popular solution for monitoring Kubernetes workloads and clusters, and alerting when the workloads and clusters are not healthy. Prometheus collects and stores metrics as time series data. This time series data can help you see trends in your application metrics. Prometheus also provides a powerful query language, PromQL, which can be used to create dashboards and alerts from your metrics. Prometheus provides many benefits, but it can be difficult to manage and scale Prometheus infrastructure. One solution to this problem is to use Google Cloud Managed Service for Prometheus. Google Cloud Managed Service for Prometheus is a fully managed Prometheus solution that removes the burden of manually managing and operating Prometheus at scale. The solution is also multi-cloud and cross project. All of your applications, whether running in Google Cloud, other clouds, or on-premises, can be managed in a single pane of glass using Cloud Monitoring. There are many data collectors available. Managed collectors are recommended for all Kubernetes environments, including GKE. For managed collectors, operation of Prometheus is fully handled by the Kubernetes operator. Self-deployed collectors are drop-in replacements for the normal Prometheus binary. OpenTelemetry Collectors and Ops Agent can also both collect metrics that can be used with Prometheus. Managed Service for Prometheus supports any query UI that can call the PromQL query API, including Cloud Monitoring and Grafana. Over 1500 free metrics available in Cloud Monitoring can also be queried, even without sending data to Managed Service for Prometheus. Managed Service for Prometheus also provides a fully cloud-based alerting pipeline for your Cloud Monitoring and Prometheus metrics.

### Video - [Error Reporting](https://www.cloudskillsboost.google/course_templates/874/video/520643)

* [YouTube: Error Reporting](https://www.youtube.com/watch?v=rm8pXa6QgJs)

Error Reporting notifies you of errors in your application and helps you investigate the cause. Error Reporting counts, analyzes, and aggregates the crashes in your running cloud services. A centralized error management interface provides sorting and filtering capabilities, and shows error details such as timing, occurrences, first and last-seen dates, and number of affected users. You can opt in to receive email and mobile alerts when new errors are found. Application error events are processed and displayed in the interface within seconds. Errors are either reported by the Error Reporting API or are inferred to be errors when Error Reporting inspects log entries for common text patterns such as stack traces. Error events are intelligently grouped into error groups and deduplicated by analyzing stack traces, helping you understand the different errors you’re encountering regardless of the number of occurrences. You can see your application's top or new errors in a clear dashboard. Stack traces are parsed and displayed in a way that helps you focus on what matters. The stack trace for an error event is displayed along with a list of the occurrences within the error group, so it’s easy to investigate similar error events at the same time. You might need to enable Error Reporting, depending on where your service is running. Error Reporting is automatically enabled for Cloud Run and Cloud Run functions. You can enable Error Reporting on Google Kubernetes Engine by adding the cloud-platform access scope when you create the cluster. Apps running on Compute Engine, will report error events by granting the VM's service account the Error Reporting Writer role. Error Reporting can be used in many popular languages, including Go, Java, Node.js, PHP, Python, Ruby, or . NET. You can use client libraries, the REST API, or send errors with Cloud Logging. Here’s an example of using the client library to report errors in a Node.js app. After including the error-reporting library, you just need to create a client and a new error event, add the details to the error event, and then report the error. The error event is reported asynchronously so that your code can continue processing without waiting for delivery of the error event. To see your errors, open the Error Reporting page in the Google Cloud console. By default, Error Reporting shows you a list of recently occurring open and acknowledged errors in order of frequency. Errors are grouped and de-duplicated by analyzing their stack traces. Error Reporting recognizes the common frameworks used for your language and groups errors accordingly. You can sort errors based on number of occurrences or by first and last seen. You can also link an issue tracker link to an error group. Selecting an error entry opens an error details page. On this page, you can examine information about the error group, including the number of occurrences over time, specific error events, and stack traces. To view the log entry associated with a sample error, click View Logs from any entry in the Recent samples panel, and you will be taken to Logs Explorer.

### Video - [Managing Performance](https://www.cloudskillsboost.google/course_templates/874/video/520644)

* [YouTube: Managing Performance](https://www.youtube.com/watch?v=lgMSnn8FtBE)

Google Cloud provides other tools that can help you manage the performance and stability of your application. Cloud Trace is a distributed tracing system that collects latency data from your applications and displays it in the Google Cloud console. You can track how requests propagate through your application and receive detailed near-real time performance insights. Cloud Profiler is a statistical, low-overhead profiler that continuously gathers CPU usage and memory-allocation information from your applications. It attributes that information to the source code that generated it, which helps you identify the parts of your application that are consuming the most resources. First, we look at Cloud Trace. Cloud Trace is a distributed tracing system that collects and analyzes latency in your applications. It helps you understand how long it takes your application to handle incoming requests, and how long it takes to complete operations like RPC calls when handling the requests. Latency data is reported on a per URL basis, letting you focus on the operations that are showing the most latency. Cloud Trace can help you identify changes in performance. Cloud Trace automatically creates a daily report that compares the previous day's performance with the performance from the same day of the previous week for the top endpoints. You can also create a custom analysis report and select which traces are included in the report. There are two ways to send trace data to Cloud Trace. The first is automatic tracing. Some configurations support automatic tracing. Latency data for incoming and outgoing HTTP requests from Cloud Run and Cloud Run functions is automatically collected. However, latency data within the services is not collected. You can also instrument the application for tracing, which provides more detailed information than is collected for automatic tracing. You might choose to instrument applications that are running on Cloud Run or Cloud Run functions. When it’s available for your language, OpenTelemetry, and the associated Cloud Trace exporter, are the recommended solution. You can also write custom methods to send tracing data by using the Cloud Trace API, or use the Cloud Trace client libraries. A tracing client collects timing data and sends it to Cloud Trace. You then use the Google Cloud console to view and analyze the data. A trace describes the time that it takes to complete a single operation. A span describes the time that it takes to complete a sub operation within the trace. A trace consists of one or more spans. Visualization of traces and spans can help you track down performance issues in your applications. The Trace explorer page lets you find and examine individual traces in detail. The scatter plot displays a dot for each request in the selected time interval. The (x,y) coordinates for a request correspond to the time and latency of the request. You can filter the shown requests by request attributes like methods or status codes. To explore a trace, you click a dot in the scatter plot. The Trace details pane displays details about this trace and the spans contained within the trace. The dots on a span indicate events that were annotated during completion of the span sub operation. The visual indication of latency can help you determine which parts of your application are causing the performance issues. Understanding the performance of production systems is notoriously difficult. Attempting to measure performance in test environments often fails to replicate the characteristics of the production environment. Cloud Profiler is a statistical, low-overhead profiler that continuously gathers CPU usage and memory-allocation information from your production applications. Cloud Profiler attributes that information to the source code that generated it, which helps you identify the parts of your application that are consuming the most resources.

### Video - [Lab Overview](https://www.cloudskillsboost.google/course_templates/874/video/520645)

* [YouTube: Lab Overview](https://www.youtube.com/watch?v=TovE-3zZPt8)

In this lab, "Deploying and Maintaining Your Application," you containerize your bookshelf application and deploy it to Cloud Run. You also explore tools in Google Cloud Observability that can help you maintain the health of your application.

### Lab - [Developing Applications on Google Cloud: Deploying and Maintaining Your Application](https://www.cloudskillsboost.google/course_templates/874/labs/520646)

In this lab, you containerize a Python application and deploy it to Cloud Run. You also explore tools in Google Cloud's operations suite that can help you maintain the health of your application.

* [ ] [Developing Applications on Google Cloud: Deploying and Maintaining Your Application](../labs/Developing-Applications-on-Google-Cloud-Deploying-and-Maintaining-Your-Application.md)

### Quiz - [Quiz: Monitoring and Performance Tuning](https://www.cloudskillsboost.google/course_templates/874/quizzes/520647)

#### Quiz 1.

> [!important]
> **You want to set up monitoring for your mission-critical application. What signals should you monitor in your dashboards?**
>
> * [ ] Contrast, Latency, Traffic, Errors
> * [ ] Saturation, Latency, Throttling, Errors
> * [ ] Saturation, Latency, Traffic, Errors
> * [ ] Security, Latency, Throttling, Errors

#### Quiz 2.

> [!important]
> **Users are encountering errors in your application. You want to view the stack trace to determine where the error occurred. Which service would help you view the stack trace?**
>
> * [ ] Cloud Logging
> * [ ] Cloud Trace
> * [ ] Cloud Monitoring
> * [ ] Error Reporting

#### Quiz 3.

> [!important]
> **You want to stream logs into Cloud Logging from third-party applications running on Compute Engine instances. What service should you consider?**
>
> * [ ] Ops Agent
> * [ ] Error Reporting
> * [ ] Cloud Trace
> * [ ] Cloud Monitoring

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/874/video/520648)

* [YouTube: Summary](https://www.youtube.com/watch?v=4vEhStsM77s)

The services in Google Cloud Observability help you manage your running services and applications. Cloud Monitoring can identify trends and help you fix issues in your applications. It's recommended that you monitor the four golden signals for your applications: latency, traffic, errors, and saturation. Logs in Cloud Logging can help you to understand the current state of your application. You can use Error Reporting to analyze and aggregate the crashes in your running cloud services. Cloud Trace and Cloud Profiler are used to analyze and improve the performance of your applications in production environments.

## Course Resources

Links to module PDFs

### Document - [Course Resources](https://www.cloudskillsboost.google/course_templates/874/documents/520649)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
