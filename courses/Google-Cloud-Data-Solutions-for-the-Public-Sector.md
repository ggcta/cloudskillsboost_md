---
id: 501
name: 'Google Cloud Data Solutions for the Public Sector'
datePublished: 2023-01-19
topics:
- Analytics Hub
- Data Pipelines
- Google Cloud Services
type: Course
url: https://www.cloudskillsboost.google/course_templates/501
---

# [Google Cloud Data Solutions for the Public Sector](https://www.cloudskillsboost.google/course_templates/501)

**Description:**

Enterprise data sharing made easy with Dataplex and Analytics Hub

Learn how to share data securely in your lakehouse with minimized data duplication and more data governance through Dataplex and Analytics Hub - enterprise data management made easy.

Creating Data Pipelines with Data Fusion

In this session, we will explore using Data Fusion to create code-free point and click pipelines that can ETL high-volumes of data with support for popular data sources, including file systems and object stores, relational and NoSQL databases, and SaaS systems.

**Objectives:**

- Summarize the features and benefits of Cloud Data Fusion and describe how it is used as a data integration tool.
- Create and deploy data pipelines using Cloud Data Fusion.
- Share data using BigQuery, Dataplex, and Analytics Hub.

## Introduction to Data Fusion

This module introduces the concept of data integration, and describes Data Fusion's platform and architecture.

### Video - [Module Introduction](https://www.cloudskillsboost.google/course_templates/501/video/359698)

- [YouTube: Module Introduction](https://www.youtube.com/watch?v=ThzF9hdyiBk)

Aaron: Thank you, guys, for joining-- and gals for joining. I appreciate you taking your time to talk with us today. I am Aaron Pestel-- you can see my ugly mug there on the photo-- data management data analytics specialist. Like John said, I have spent a fair amount of time with Data Fusion, favorite technologies to work with. So if you do have any questions, feel free to interrupt. Go ahead and raise your hand or speak out. If you raise your hand, I think John will call on you so I can make sure that I see you. We're going to spend about an hour, roughly. I've got a little bit of slides, but we're going to spend more of the time actually in the demo working with the product, since it's very user-driven technology. So today, agenda, high-level, we'll talk about why Data Fusion exists, sort of what the problem space is trying to solve. We'll introduce the technology a bit itself, talk a little bit about the platform and the architecture. Then, like I mentioned, we'll spend the majority of the time in the demo.

### Video - [Data Integration Overview](https://www.cloudskillsboost.google/course_templates/501/video/359699)

- [YouTube: Data Integration Overview](https://www.youtube.com/watch?v=HhaYnpnfxgU)

Aaron Pestel: So the overall platform-- it's probably not a surprise to you that more and more organizations are trying to leverage data analytics and artificial intelligence to make decisions. There was a recent survey of CIOs. They were asked, what's going to be the most important thing for you in the next three to five years? Is it SaaS offerings? Is it Database as a Service? Is it cloud technology, security? What is it? And far and away, the top two are data analytics and AI/ML. Because they know that if they're not using those to make decisions, they're going to have other organizations that are making better decisions, and they're going to fall behind from that perspective. In order to do that, though, to make those decisions with data analytics and AI/ML, you have to have access to the data. And unless your organization is different than the majority of the organizations that I end up seeing, the data is all over the place. It's in various different systems. It's in mainframe for this group, and it's in CSVs for this group, and Oracle RAC for this group, and PostgreSQL for that group, and Salesforce for that group. And trying to get all that data collected requires an integration strategy, some kind of a technology or a process or a program that allows you to pull all of that data into a commonplace, or at least a way that it can be accessed commonly so that we can do analytics on it and so that we can run AI/ML models. While that's important to be able to integrate that data, it's very hard. I even find this, just working with the technology-- like John said, I've worked with Data Fusion quite a bit. But it almost seems like every source that comes in is a new thing to learn. So strong with Data Fusion and databases and then somebody needs to connect to Salesforce. Well, now you've got to be strong with Salesforce. Then somebody needs to connect to SAP. Then somebody needs to connect to some other system I've never even heard of, and this REST API, and that REST API. So the technical skills to pull all that together, if your tool can't handle it for you, is very challenging. A lot of the times, customers have point solutions that are out there that just pull from one data set to another data set, or from this data set to the other, from mainframe to CSV, or PostgreSQL to BigQuery. And they've got all these different point solutions. Some have invested in expensive, monolithic systems. You're thinking, like, Informatica and other very large, very legacy systems that have been around for a long time, do a lot of functionality, but are also fairly expensive and heavy to lift. And in the end, we end up with a bunch of data silos, fragmented data that we can't get together into a unified view to do the analytics with. That is the domain the Data Fusion is trying to fit in and try to fulfill.

### Video - [Data Fusion Overview](https://www.cloudskillsboost.google/course_templates/501/video/359700)

- [YouTube: Data Fusion Overview](https://www.youtube.com/watch?v=A9Y6GhOvEpA)

Aaron: At the highest level, its goal is to be that enterprise integration service that pulls data from any number of heterogeneous sources or other sources, external sources, and pulls it into a data lake or a data mart. In Google speak, that generally means pulling it into, like, GCS, a data-lake type approach, or pulling it into BigQuery, more of a structured data, where they can run BQML, machine-learning models, and analytics and analytic applications, like Looker or Data Studio on top of that. So that's generally what it is, creating those connectors to external sources, and pulling them into these data lakes or data warehouses that we can do analytics on. The sources that are available there-- I think now hundreds of sources. I've not actually counted them all. You can see them on the website or in the product. But obviously, all the technologies within Google, you can connect to, so pulling data from CSVs or Avro files or Parquet files in GCS, pulling it from Pub/Sub or from BigQuery or from Cloud SQL or Bigtable or any number of the Google Cloud sources. We can also pull from any database that has JDBC sources. This is probably the most common that we see on-prem. Customers have Oracle or Oracle RAC or SQL server, and they want to be able to connect to that and pull that data into something like BigQuery so they can do analytics on it. But we'll also connect to object stores. We'll connect to applications, SAP, ServiceNow, Salesforce, various others, even streaming data-- Kafka, Pub/Sub, Confluent. And then we've put this other thing, which actually gets used quite a bit, something like an HTTP source. So often, a customer will come out and say, hey, I to connect to XYZ system. And I've never even heard of XYZ system. But you go look at the XYZ system, and it has a REST API. And so our HTTP source will often be able to connect to that REST API, use something like OATH to do authentication, and then pull that data across. Once we connect to the source, we have pipelines that securely pull that data, we can do transformations. So if you don't want to transform, you just want to copy the data across, we can do that. But if you want to transform it-- maybe removing columns, masking columns, cleansing data, wrangling, or data prep, as we call it-- you can do that within Data Fusion as well. And then the end of the pipeline puts it into a target, or what we'll call in the tool, when you see the demonstration, a sync. More often than not, that's cloud storage or BigQuery. But it can be things like Dataplex, which is our lakehouse, or other technologies too. We have some customers storing data in Salesforce. We have some customers storing data in Postgres. So it's not that it only can go to BigQuery or cloud storage, but that's the most common destination. Some of the ways that Data Fusion does this is with the code-free pipelines. So there are other technologies-- and I'll show you a chart here in a minute that talks about some of them. You may have heard of Dataflow, which is another data pipeline technology within BigQuery. Its strength is being code-centric. So it's using a Beam API that's typically written in Java or Python. There's a couple other languages too. So if you're in an environment where you've got lots of developers, you like writing code, you don't like clicking on boxes and creating pipelines, graphically, you want to write code, then data flow is a great tool for you from that perspective. Data Fusion is trying to fill this more of a free- or a low-code pipeline. So we're going to create some pipelines today connecting to Salesforce, connecting to cloud storage, and you won't write any lines of code. That's not to say you can't if you had some complex transform. There are [AUDIO OUT] transforms and Python transforms where you can write code. But the intent with Data Fusion is that for most things, you do not need to write code. Another interesting thing with Data Fusion is that it tries to unify the data preparation with the pipelines. So there are a lot of tools in this space that do what we call EL, Extract Load. So they'll connect to a given source, and they'll put the data in the sync, but they don't transform. They don't mask any of the data. They don't change any of the data or cleanse any of the data wrangling. They just move it from one to the other. Those are our sort of pipelining tools. And then there are tools that sometimes specialize on data prep, or data preparation. They may be great at opening up a random CSV file, looking at the data, changing data formats, filling null fields, changing syntax, things of that nature. But they probably aren't great at pulling from bunches of different sources or running big pipelines. They really focus on the data preparation. And Data Fusion tries to concatenate the two of them, both a very strong, scalable ELT and ETL model, but also strong data preparation custom within that pipeline that can allow you to connect to various different sources.

### Video - [Platform](https://www.cloudskillsboost.google/course_templates/501/video/359701)

- [YouTube: Platform](https://www.youtube.com/watch?v=7YlHaIH5Ih0)

Aaron: So let's talk a little bit about the platform and the architecture. And we're just about ready to get to the demo. This is one of the charts that I really like and I really don't like at the same time. I really like that you can see the breadth of technologies that exist within Google's analytics platform. Sometimes, it's useful just to look through these and say Pub/Sub. Do I know what that is? Data stream. Do I know what that is? And go through each one of those and use that as almost like a checklist for learning about the technologies that exist within BigQuery. There's a lot of great technologies, all of these which could be an hour and a half session or longer. But the thing I don't like about it is sometimes you think it's too complicated. Or my goodness, do I have to use all of these products to actually do something? And the answer is no. A lot of the customers that I work with use two products on this page, they use Data Fusion and they use BigQuery. And so they use Data Fusion to pull from the various sources, whatever those sources are, transform the data, they stick it into BigQuery, they run analytics on it, they do BigQuery ML for machine learning models, and those are the only two technologies that they use. And so if you're looking for a simple type environment like that, you can do that. You don't have to have all of these different pieces. But it is good to know that those pieces do exist. If you're looking for more business intelligence graphing and technology for making your applications more analytics focused, we have Looker tools and other things like that can enhance it. But you don't have to use those to use Data Fusion. The components of Data Fusion. The bread and butter is what we call ETL or ELT, and that's this-- see a small pipeline here. It'll be bigger, obviously, when we go to the demo. But you create a source, you create a sync, you maybe do transforms in the middle, and you run that pipeline or you schedule that pipeline. That is the heart of Data Fusion. When we're doing this, you'll see that I'll pick Salesforce sources or other sources. If you open your Data Fusion instance, you might say, I don't see all the plug-ins that Aaron had. What's going on here? Then you probably need to go to the Connector Hub and we'll show that as well. It's basically like the Google Play store or like the Apple Store of Data Fusion where there's all the plug-ins that are out there that you can deploy and use in your pipeline if you want to, and those are added to regularly. And then, also the Wrangling. So if I'm creating a pipeline and I have a transform in the middle that I want to graphically play with, there's the Wrangler tool that will help with that. And we'll demonstrate that today as well. Also it's important to know-- and you should probably remember this for a hoot in case you're trying to win one of these prizes-- everything, or almost everything that you can do in any of these GUIs, deploying instances, deploying pipelines, running pipelines, monitoring them, monitoring them, checking the logs can be done through REST services as well. So it's great to go through and demo it like we're doing it today. But as you get into production and you want to automate it, make sure that your environment is set up the same every time, make sure pipelines are deployed the same every time, checking the pipelines into more of a CI/CD approach, there are REST APIs that make all of that possible so that you can automate those experiences for a better overall system. This all runs on a core platform that handles the logging so you can look at the logging for all of your pipelines. Which ones have run, monitor them, see how long they've run, how many rows of data they scheduled across, scheduling the pipeline. So some might be run every morning at 8:00, some might be run once a day or once a week. Some of them might run and then trigger another pipeline. So that sort of general scheduling can be done in Data Fusion as well. If you need more advanced scheduling, like I'm going to run these five pipelines and then this pipeline and then these three pipelines, something like a composer environment, which we have composer operators for, give you more of an orchestration of those pipelines, but there is some basic scheduling built into Data Fusion. And then, these technologies run under the companies on Google Kubernetes, Dataproc, which is where your pipelines run, and also BigQuery. More and more of the pipelines that can be get pushed down to BigQuery to make the running of those pipelines more efficient and optimized. Another interesting thing and one of the unique things about Cloud Data Fusion is that it's based on an open source technology, CDAP. CDAP app stands for Cask Data Application Platform. It was an open source project that was created by a company called Cask, which was then acquired by Google a few years ago and it became the basis of Cloud Data Fusion. So if you can give me a little bit of liberty here, maybe think of CDAP almost as like PostgreSQL or as MySQL or MariaDB, sort of an open source technology that exists. And think of Google Cloud Data Platform or Google Cloud Data Flow or Data Fusion as, basically, the Cloud SQL, or the managed hosted version of that. So it gives you a few advantages. If you, for some reason, you're still going to have to have some stuff running in Amazon or in Azure or on prem and you absolutely want to use the same pipeline and ETL tool, you can take this open source CDAP and deploy it anywhere that you want to. It's not something that Google supports, just like we wouldn't support your custom deployment of Postgres or MySQL. But the open source project is out there and available to you. You're not locked into Google when you create pipelines with Data Fusion. Flexible execution. So we're going to talk in a second about the Tenant project and creating the pipeline. And then, we're going to talk about running the pipeline, which is a separate step. When you run that pipeline, you can define where that pipeline is going to execute. Generally, that will run on Dataproc. But really, it can run in any environment where you have Yarn and Spark available. So for example, there's a provisioner down here for Hadoop. Let's say that you have a Hadoop cluster on prem or you have a Hadoop cluster in Amazon with Amazon EMR and you want to run the pipeline there. You can actually use CDF to create the pipeline-- Data Fusion to create the pipeline. And then, run it on Amazon EMR or run an on prem in Hadoop or run it on a local Dataproc instance that we spin up. Far and away, the most common thing that we see is the standard Dataproc instance where you run the pipeline, it starts up a fresh talk instance, deploys the pipeline, runs it. And once the pipeline is done, it shuts down that Dataproc instance. Gives you very consistent results every time. And so that's what most people use, but there are other provisioners that you can use if you'd like to.

### Video - [Architecture](https://www.cloudskillsboost.google/course_templates/501/video/359702)

- [YouTube: Architecture](https://www.youtube.com/watch?v=_m3oO_iCbb4)

Aaron: Moving down to the architecture. So I want to touch on this because this is a common thing that comes up. This diagram is a little busy. For now, ignore this top piece that says Host Project and Service Project, and ignore Project down here and just look at the two yellow boxes at the bottom left here where it says Customer Project and Tenant Project. So a tenant project is a project that's owned and managed by Google, but it's dedicated or set aside to a specific tenant. In other words, to a specific customer. So when you go and launch a Data Fusion instance, which we're going to do here in a second, or show you how to do it in a second, it's not actually going to run that instance in your project, even though you'll see a line that says that instance in your project. But under the covers, that will have created a new project that's managed and hosted by Google that has things like GKE and all the guts that run underneath of Data Fusion underneath that CDAP deployment that gets managed for you, and that runs in this tenant project which is in its own tenant VPC. And then, when you're clicking in the GUI, that's all happening inside this tenant project. And when we say deploy, then when we deploy, generally, that will create a Dataproc instance in your customer project or in your customer VPC, launch Dataproc, the pipeline will get pushed to there, and it will run there. So that tenant project versus that customer project is an important thing to understand. Initially, in the demos we'll do today, you may not even notice it, but once you start trying to connect that tenant project, like to get a schema and try to connect that to your Oracle on-premise environment or connect it to other things like that, you'll have to deal with that VPC and peering that, or making sure that that tenant project can access the resources you need it to access. It's documented fairly well, so I don't want to scare you with it, but I just want to make sure you understand that there is this tenant project concept and customer project. Every time I get a question that says, I can't get Data Fusion to connect to XYZ, the first question is, is it something in the tenant project? Like, is it your studio or [AUDIO OUT] connect, or is it your actual running pipeline and Dataproc connect. The other thing I'll mention here before we get right to the demo, there are public versions of Data Fusion or public IP instances and there are private IP instances. So if you're connecting mostly to public sources, like BigQuery, Amazon S3, Redshift, things that generally are SAP, Salesforce, things that would generally have public IP addresses, you'll probably create a public IP instance and connect that way. But if your Data Fusion is connecting to more private resources, like an on-prem Oracle database that's VPNed in, or Cloud SQL, or some other type of thing, or SQL server, or some other type of a private IP resource, you'll generally create a private IP instance and then you will peer that back to your customer project so that the tenant project can see those private resources. So those are the two deployment models. If you need both, you can just deploy a private instance, and then there's documentation for how to set up a gateway such that your private instance could get out to Salesforce or some public source as well. But it is kind of important to understand the private IP instance versus the public IP instance.

### Quiz - [Quiz: Creating Pipelines with Data Fusion](https://www.cloudskillsboost.google/course_templates/501/quizzes/359703)

#### Quiz 1.

> [!important]
> **Data Fusion can help you do what with your data?**
>
> - [ ] Form insights from the data.
> - [ ] Collect data.
> - [ ] Process data.
> - [ ] Store data.

#### Quiz 2.

> [!important]
> **Which is a common challenge of data integration?**
>
> - [ ] Data integration is expensive.
> - [ ] It's often not prioritized by organizations.
> - [ ] Data integration solutions are not available.
> - [ ] Data integration solutions do not offer enough functionality.

#### Quiz 3.

> [!important]
> **Which is an example of a target?**
>
> - [ ] BigQuery.
> - [ ] A data lake.
> - [ ] ETL processing.
> - [ ] Google Drive.

#### Quiz 4.

> [!important]
> **How does Cloud Data Fusion help you accelerate time to insight?**
>
> - [ ] It integrates with Looker Studio to generate real-time insights.
> - [ ] It stores a large amount of structured and unstructured data.
> - [ ] It accelerates the data intake process.
> - [ ] It pulls data into data warehouses, data lakes, and data marts on Google Cloud.

#### Quiz 5.

> [!important]
> **What is a tenant project?**
>
> - [ ] A project that's managed and hosted by Google but set aside for a specific tenant.
> - [ ] An experimental project in Data Fusion.
> - [ ] A project that's owned by the tenant.
> - [ ] A project that's owned by the customer.

## Creating Data Pipelines using Data Fusion

This module demos Data Fusion and shows how to create a data pipeline.

### Video - [Demo: Creating Data Pipelines with Data Fusion](https://www.cloudskillsboost.google/course_templates/501/video/359704)

- [YouTube: Demo: Creating Data Pipelines with Data Fusion](https://www.youtube.com/watch?v=_4wOzPIC64s)

Aaron: So when you come into Google Cloud platform-- I'm going to take this demo, like, from scratch. So like, if you just went and got a trial account today, I want you to be able to do what we're doing here. And then we'll get into some of the more advanced features. But I want to kind of have a clean, simple pass through. So if you don't know where Data Fusion is, you can search for Data Fusion this. Spell it right. And you'll see the link come up. You can also find it from your hamburger menu up here. Click on Data Fusion. When you click on Data Fusion, you probably won't see three instances here because you won't have started one or started any like I have started here. So what you'll do is you'll click Create Instance. This is going to create that tenant project that we spoke about. So once I get done clicking Go, it's going to create this project that Google manage. It's going to deploy CDAP in it, and it's going to give me back a URL to access it. That's what I'm creating here. So I'll call this myinstance. I can pick the region I want to put it in. I can pick whatever version of the product I want to use. 6.6 is the most current. I can pick an edition. So Developer edition-- if it's just me doing some light playing, I might pick Developer. If I'm doing a little bit more development, I might get away with Basic. Generally, you'll want to use Enterprise. Under the covers, this is basically how big-sized or how largely sized your tenant project is. So if you end up picking something like Developer and start wrangling large files or running lots of pipelines, your resources in that tenant project are going to run out, versus with Enterprise, you'll have more resources to do more work, have multiple users using it, et cetera. So you pick the edition. And then under Advanced Options, you pick what we'd said with private IP or public IP. So if you're needing to connect to something in your VPN or something with a private IP like a database, you'll have to enable private IP. Specify the network that you want to. If you're just connecting to public resources, like we'll be doing in our demo today, you could just leave that unchecked. And then click Create, and it will start creating your instance. That's it. Takes-- depending on the region and things-- maybe 10 or 15 minutes for it to set up that whole project, deploy everything, and get it back ready, and then you'll get a line like you'll see here. So to go into that instance, we'll click View Instance. And this will take us into a web app that exists on the tenant project, which is CDF that you'll come to know and love as you start working with it. This is the landing screen. We're going to go through all this in a second. But I want to just step right to let's create a simple pipeline. So we'll come into the technology that's called Studio. Studio is sort of our canvas where we define our pipelines. And it looks like you might expect from a canvas. So we can pick sources. In this case, we're going to pick a GCS, which is our Google Cloud Storage source. We've got an avro file out there, but we could use Parquet. We could use CSV. We could use JSON, any number of other files too. And we'll pick a sink that we're going to put it in. So in this case, we're going to put it into BigQuery. We're going to drag the two together like that and connect them. And now we've created our pipeline. The only thing we have yet to do is specify the details of which GCS object or objects and which BigQuery table we want it to go into. So we'll click on GCS. And the first thing we'll need to do is give it a reference name. So we'll call this mygcssource. The reason that we have a reference name is because we track how the metadata. So as we get through in a second, you'll see that we can define which sources went to which sinks, and you can sort of keep track of that from a lineage perspective. So that's why we put a reference name in there, and it has to be unique. The path-- if we know it, we can type it in, but there's lots of little shortcuts in here to help you with that. So we can bring this up, and we can go into Cloud Storage if I didn't remember which path it was that I was trying to pull from. And I believe I have an Avro file in one of my buckets that's called Austin 311 data. Here it is. So this is an avro file. So we're going to select this. Of note, I'm picking one file here. You don't have to pick one file. Let's say that you had 30 files or a folder of files. You can put wildcards in there, so it'll pull all files that match that. So it's not limited to just a single file. You can pull in a whole folder's worth of files if you want to as part of this pipeline. We then need to specify the format. Here we're avro, but you can see all the other types of formats that are supported for the GCS source. And we can then say Get Schema. This will go and introspect that file, open it up, and kind of figure out what the different columns are in this particular file and show that to us. And so we'll see that over here in our Output Schema as soon as it finishes introspecting. So this is a fairly large schema. There's probably 20 columns there. You'll notice that it determined whether they were string columns or timestamp and long and double in different formats or different types. It will do that as well with a CSV. CSV doesn't, obviously, have the schema embedded in it. But it will check-- I think there's a certain number of rows that you can configure it to check-- that it will look for to try to determine-- sample size of 1,000-- to try to determine if a specific column is in string column or is a number column or a timestamp so that it can pick out that schema from that CSV. And you can also create the schema yourself if you want to. But generally, the introspection is good. That's it. We can then click Validate to make sure that we've set everything we need to, everything that's required. It'll tell us there's no errors found. If there was an error, we'd get a little error message up at the top. So now we've specified the GCS source. Now we need to specify a little bit about the BigQuery sink. So we also need to specify the name of this for lineage tracking. So we'll call this BigQuery sink for demo. And we could specify a different project if we were putting it in a different project. We're going to use the same project. And I'll put cdf_training as my dataset. And then I can specify a table name. This table can exist if you've already created it and you're sort of re-adding to this table, or it can create the table from scratch, which is what we'll do here. So let's call this austin_311_training. And that's it. So I gave it a dataset name, which is sort of like a schema in BigQuery. I gave it a table name. It already knows the schema because it pulled that across from the source. And you'll see down here that it's already got the output schema of what that table is going to be created with. And I can validate it. And if it comes back green-- [AUDIO OUT] did, we're good. Now we haven't-- Person: [INAUDIBLE] Aaron: [INAUDIBLE], go ahead. Person: Quick question. That Austin table in BigQuery-- did it already exist or will it create it? [INTERPOSING VOICES] Aaron: In this case, it does not exist, so it's going to automatically create it. But if it did exist, that would be fine as well. And in fact, there are some other options on here-- we come down here. So I can say I want to insert into this, upsert into this table, or update the table based on the values that are coming in. And I can further say, do I want to truncate the table? So let's say this is more of a staging table I'm loading into. I can have the pipeline automatically truncate it before it loads the next set of data in. Or if I leave this false, it would just append to it. And I can also specify Update Table Schema. So let's say that the data that I got was going to be constantly adding columns, it will actually update the columns-- list of columns on the table and allow me to do some schema changes there as part of that pipeline. There are some limits there. We can't, like, rename columns and things like that. But if your data has new columns that are showing up on occasion, you can click Update Table Schema and it will automatically update that schema when new columns come across the wire. Person: Oh, great. That's super [INAUDIBLE]. Aaron: Very nice. OK. We can preview this. So if I want to see what does my data actually going to look like, I would click on Preview and then Run. And this won't actually write to BigQuery. It'll just show me the data that's coming across. That's especially useful when I'm transforming to make sure that my transforms did what I thought they wanted to do. But in this case, we're just going to save it. So we're going to call this GCS to BigQuery training. And then we're going to deploy it. So think of deploying as sort of the committing of it. And when it's in this Data Fusion Studio nondeployed state, I can edit it. I can change things. I can change the properties. Add more nodes to my pipeline. But once I deploy it, it sort of becomes fixed. If I want to change it now, I need to make a copy and remake it. And that's because of the lineage capability. If I could just go and change this, then I can't really prove my lineage anymore because I might have changed this deployed pipeline. So now that it's been deployed, I can do several things with it. I can configure which Dataproc cluster I want to run on. I can specify certain parameters to it. I can set up pipeline alerts, various different things. I can schedule it to happen once a day, or based on a cron job syntax, or run once an hour. Or I can just, in this case, simply run it. Now, when I click Run, this is where we leave the tenant project. So everything here is running in that tenant project that we talked about in the Google managed project in the Google managed VPC. And now it's launching-- or what we call over here provisioning-- a Dataproc instance in my customer project. Once that Dataproc instance has started up, it's going to take this graphical pipeline and-- to the other person's question that asked the question-- convert that, basically, into a Spark application, push that Spark application to Dataproc, and run it. And then once it runs there, it would-- once it finishes, it'll be done. And it will take that Dataproc instance that it's created and shut it down so that you don't have to pay for it anymore. Just to show you, let's go over here to Dataproc and let's see if that cluster is started. And sure enough, it's CDAP GCS to BigQuery and then basically a unique identifier beyond it. But this is the cluster that's being started on Dataproc in my project so that that pipeline can run on it. One thing I will say that kind of comes up, if you're running normal pipelines, like Data Fusion is really designed for large pipelines, large amounts of data, you don't even think about it taking a minute or two to start the Dataproc cluster or to deploy it. But if you're trying to load, like, five records you may be like, this is-- why is it taking a minute to start Dataproc just to run five records? And I think it's important to kind of understand what Data Fusion is trying to be. For example, as an analogy, if you're trying [AUDIO OUT] 1,000 people from the East Coast to the West Coast, the fastest way to do that is with a jet or several jets. Nobody would question it. Everybody would say a jet is fast. But if you're trying to go from your living room to the grocery store, using a jet is probably not going to be the fastest way. You may still be able to make it if you kind of fold up the wings and drive on the roads and you'll get there, but it's not quick. Data Fusion is sort of like that jet. It's not designed to be quick. You're not going to run Data Fusion and migrate two records, you know, in half of a second. That's not what it's trying to be. If you're migrating hundreds of records, or thousands of records, or millions of records, or even billions of records, and you want to parallelize that across a large cluster, there's very few technologies that can do that as well and as fast as Data Fusion can do. OK. So while this pipeline is-- oh, go ahead. Was there a question? Person: Yeah, quick question. Diego had another question. So the question is about accessing files within GCS. And what happens if you have-- and correct me if I'm wrong here, Diego-- but what if you had multiple files in a GCS bucket and they had different problems. Can you differentiate those? Or how would you kind of handle that? Aaron: Yeah, that's a good question. So it sort of depends on-- let me go back to-- let me duplicate this so that we can see. It depends on what your goal is with it. So if they're really supposed to be the same and some might have different, then I could go and do something like this where-- say that I have three different types of schemes. And I could take them like this. And I might put a Wrangler in here like this. Boom. Boom. Boom. And boom. OK. And now I'm reading from three different GCS areas. Let's say folders or file name syntax or something that have different. And I'm putting a transform after these two to basically make them the same schema, to add the column that was missing, or to change the type of the column that was different or whatever it was. And then I can pull from all of these and put them into BigQuery. Now, if it's not intended to be the same type of file, like they're completely different schemas and we don't even want them to go into the same BigQuery table, we want this set of files or this file or set of files to go into one table, this set of files to go into one file or table, then we'd either have different pipelines. And probably what we would do is we'd have separate pipelines there. So we'd have one pipeline for GCS to go from here to here or one from here to here. And if you-- and this might be getting into too much detail, but hopefully it'll kind of whet the appetite. If you say, well, there could be hundreds of them, but everything is the same except for the schema, then you can actually delete all of these, and you can templatize it. So I can come into this property and I can say, I'm going to clear the schema so that the schema basically becomes dynamic. And I'm going to specify the file here as a parameter. We call that a macro. So let's say mysource. And I'm going to specify my sink as a macro as well. So this table is another macro-- mytable. And then what we have is called a templatized pipeline where, when I run this, I would specify the file or files I want to use as the source and the file I want to use as this destination. And so I could have one pipeline and run it 100 different times with 100 different parameters, and it would pull all of those files with completely separate schemas and put them in completely separate tables in BigQuery. Does that answer the question? Person: OK, that's really cool. So I could then call that from my application as things change? Aaron: Yes, you could call that from your application or even when you deploy it. Let me deploy this really quickly. So you can see that when you manually run it-- oops. I didn't have a path there. Let me go back to my other one just real quickly. So I'll show you this. Person: That seems super powerful and super scary all at the same time. Aaron: Yes. We do-- I mean-- I guess I shouldn't say who-- but we have an education agency that basically does this. They have a pipeline like this. So this one migrated a million rows. But there's these runtime arguments. And so if I had a macro, then I'd put, like, source_file and I put in sink_table. And then I could specify these. And so when I run them, it would run the same pipeline, but it would be a completely different schema going from one file or set of files to a different table. So then I only have to manage one pipeline and I can run it multiple different ways, pulling different schemas. Person: OK, cool. And then when you were playing with that entire workflow, it got me thinking, you know, as you're doing some of the wrangling and putting those things in. Could you call out to web services like data loss prevention if you wanted to, like, mask things as stuff's flowing through the pipeline? Is that where you would do that? Aaron: Yes. And we're going to get to building some of those things. But yes, absolutely. Let me go back-- Person: OK. Aaron: --here to our studio, and we'll take a sneak peek at that just because that question came up. So lots of different transforms. I don't know if I have the data loss prevention one added in the plugin, but we can do that when we go to hub. But exactly, so if you wanted to do transposing, or field decryption, or masking, or speech to text, and there is a data loss prevention one, all of these sorts of things that you see here are transforms that you can put into the middle of the pipeline. And each one of them obviously has its own parameters, like the data loss prevention would be which fields are you trying to mask, and how do you want to mask them, et cetera? But you can build as complex of pipelines as you want to. They can even sort of branch and go different ways. And we'll see some of that in a minute. But absolutely. That's exactly where you'd add those things, right there in between the source and the sink. Person: Awesome. Thank you. I'll be quiet because the demo looks really good. And just a heads up, probably about 25 minutes, 25 to 30 minutes time check. Aaron: OK. So let me wrap up fairly quick. All right, so now that we've kind of got a pipeline that's running, I wanted to kind of make sure that you were comfortable with the overall thing. When I first started using Data Fusion, I'd sometimes get lost. If you ever get lost, click on the Data Fusion up here in the upper left and it will take you back to this landing page. This is sort of the basis, the entry point of Data Fusion. When we clicked on it originally here, that's where this took to. These are the components within Data Fusion. Replication allows us to do CDC. So if I want to come in here and I want to connect to an Oracle or a SQL server or a MySQL database, do a snapshot of that database. And then as changes are made to that database, as rows are inserted, updated, or deleted, pass those replicated, updated, deleted, or inserted rows through to BigQuery. I can do that here with the CDC capability. We call that replication. Over here, we can manage. So if I want to look at the logs within my tenant project or I want to configure things like namespaces-- namespaces is a way that we can segregate pipelines and users. So if I have three teams, I can put them in separate namespaces so that team one can't see team two's pipelines and team two can't see team three's pipelines. And I can even make it such that person one in team one can only create pipelines but not deploy them. And person two can deploy them but can't create them based on the type of roles that they're going to have. So that's what you can do with namespaces. Compute Profiles-- this is how I set up, like we've talked about in the presentation. If I want to create a new Dataproc instance or use an existing, I would do that here. And then System Preferences is for setting various parameters. I go back to the main landing. Here's the monitoring dashboard, so I can see the pipelines that have run. Ours actually finished. I can go back in time and see pipelines that have run at various times as well. And when you're running lots of pipelines, that becomes even more useful. We talked about lineage. So we can come over to here, and we can see the various lineage for the different pipelines that have run. So if I say I'm interested in this Austin data, I can see that. I can look at that dataset that's this Austin dataset, and I can see the lineage that there are three pipelines that pull from this Austin dataset, and they copy it to these three different sinks. And like I said, these are now kind of fixed. They can't be changed without redoing them, creating a new one, which allows you to make sure that you keep this lineage correct across all of your data that's being transformed. And then we talked about integration. This is where the Studio is. If you go to List, you can see the pipelines you've deployed and edit them. You can also go to Drafts if you have any pipelines that you haven't finished yet. We're going to look at one of those in a second. And then we can go to Wrangle. That'll be the last demo that we do. So let me quick go back to List and the Draft, and I'm going to open this sources so you can see a few of the additional sources that we have. So I've preloaded some of these. BigQuery and GCS, you've kind of seen those. Postgres-- so I've set up a Postgres database connected with a host and a port. And you'll see that I can now specify a query. So I'm querying an internal Postgres table called pg_views, but this could be any table. If I call Get Schema, it'll call out to that Postgres database and get that schema. If I wanted to check from a different table, I could do this. Call Get Schema and it'll get the schema of that other table. Oops, pg_tables, if I could spell. Then it'll get the different schema. And one of the things that's important, especially as you start scaling, is this Split-By. I won't go into all the details, but imagine that you're doing a query here that's going to come back with 500 million rows. If you do nothing else, it'll be one single query that's basically single-threaded. But Data Fusion allows you to split. So we can pick a field to split by and say, instead of doing one ginormous query, let's do 100 queries that each get 1,000 rows apiece. And then that huge pipeline can be parallelized and can run much faster on a large Dataproc instance. We also have Salesforce connectors. So this is connected to a little Salesforce instance that I have. There's an Account instance, or an Account object they call, like a table in Salesforce. So we can reach out to Salesforce, introspect that schema and pull data from that. We can also use SOSQL, which is like a SQL version of Salesforce. And one other thing that I'll just touch on so that you know it exists. Let's say-- like the gentleman that was asking about the multiple CSV files-- let's say that you've got a database schema with 20 tables. And you don't want to create 20 different pipelines. We have a multiple database source and a multisink where I can come in and say, let's just have one source, and let's choose all of these tables, or all the tables except these tables, or let's do these 40 SQL statements. So select from this table, select from this table, select from this table, taking only the columns that you want, et cetera. And then this one pipeline will migrate all of these tables or SQL statements over into a separate table on the sink side. So just various ways of creating more complex pipelines without having to create multiple individual pipelines. The last thing I want to touch on here quickly is Wrangler. So-- Person: Quick question on that last one there. Aaron: Pardon? Person: Quick question on that last one for the multiple databases. Aaron: Yes. Person: Is that going to do a one-time transfer? Or will it do, like, change data capture type stuff? Aaron: It can do either. So if you just set it up by default, it's going to do a one-time transfer. But what a lot of customers will do and what we've helped a lot of customers do is basically add a WHERE clause like, say, WHERE date between last run and today and pull those changes across. And so you can put those kind of WHERE conditions into your source. And so you'll run the pipeline each time, but each time you run it, it will just take the next chunk of data that's changed. As long as there's some sort of a date column or some column within your dataset where you can identify what the new data is, then you can put that there in a WHERE clause. Person: OK, perfect. Thank you. Aaron: Great question. OK, let's come into Wrangler now, and we'll wrap it up with this. I've already got this loaded. So let me close that. And I'm going to go and grab-- I believe I've got this in a BigQuery table. And just to show you some of the data-preparation-type stuff we can do. So apestel. This is an account table. I don't remember where I actually got this from, but it was an example of a dataset where the customer said, you know, it's a bunch of crazy stuff like this. So here's a row, for example, that doesn't have any values. It's all null. So up at the top within Wrangler, which kind of just looks like [AUDIO OUT]. . It's a way of looking at your data. If it's green, it says that all of the data exists. If it's red or if it's partially red, it's basically saying that some of the data exists and some of the data does not exist. You can also click on the Insights table to get further insights, like what are the most common values within your dataset? What percentage of them are null? You can start to sort of group things together into your own graphs, like BillingCity, and let's take a name and then kind of build little graphs or charts comparing that data. And if I had to-- probably should have picked one that had a, like, number of sales or something like that-- and make the different sizes of things so that can just visualize your data and understand how the data is related better with the analytics of this tool. We can also change things. So if I want to come into this and say, I really don't want it to be null. I want to fill all the null cells with something like N/A so people know that it's actually empty. I can do that. If I want to change things to be uppercase, like sometimes people want their billing addresses to always be upper, I can do a format and make that all uppercase. And go and say, just find things and remove them. You know, in my data, I don't want to have this word "customer channel" and "customer direct." It should just be "direct." I can do a find and replace, and I can say customer space that. Replace it with nothing. And so now it'll get rid of all of that. I can do some level of masking-- not to the extent that we can with, like, data loss prevention, but I can do some masking in here. Let's say I've got these phone numbers. And so I will fill null first with 1-2-3, 1-2-3, 1-2-3-4. And then I'll say, I want to change these to only show the last four digits or last four characters. And so now I've masked out the front of the phone number, or the front of the Social Security number, the front the driver's license number, whatever it is that you're wanting to mask. And basically, while I'm doing these steps, all I'm doing is I'm creating this recipe of steps that are going to go into my Wrangler. And when I run the pipeline, every one of these Wrangler steps will run on every single row of data that comes across. So I can clean up the data to make the format that I want to. It would take way too long to go through all the different steps that you can do, but you can delete columns. If I decide I didn't even want that in there, I can delete it. I can split columns. If you pulled your data from a source like Redshift that has spaces in column names, you can reformat the column names or we call cleanse them. Put them all in lowercase. If there were spaces, then put underscores to kind of more standardize them. But lots of ways to kind of get this data in the format that you want to. And then once you've done that, you can create a pipeline. And it will create the same type of pipeline that we just saw. And it will automatically pick from the source that we picked and put that Wrangler step in here. We click on the edited Wrangler, you'll see the directives that we created. And you can customize them there, or click Wrangler to go back to that screen. But do the more complex transform and then put a sink on there of BigQuery or whatever it is that we want to. And now we've got a pipeline that does a more complex transformation. All right, the last thing I'll touch on is what happens if there's an error still in the data? Let's say that I thought the data was all numeric, but somebody slipped in a text field or something like that. Within Wrangler, we can say, when I've hit an error, do I want to fail the whole line, skip the error, or send it to an error port? So I can actually take each row that gets an error and send it to a separate port. And then I'll have these error collectors down here like this. And I can say, any errors I want to go to here. And then I'm going to send them to, like, a redo folder or to Pub/Sub or any other kind of sink that I want. In this case, let's put them to a separate BigQuery table that some analysts might go look at and figure out what's wrong and maybe change things. So I can handle errors in that way. I can also do it for the whole pipeline. So if the whole pipeline succeeds or fails, there's ways that I can configure a pipeline alert and say, you know, if this thing succeeds, send an email. And if it's a success, do this. Or if it's fail, do this. So different alerts that you can do at the pipeline layer as opposed to at the individual rapid layer. And then the last thing-- the final thing-- are these actions. So these get used quite a bit now. Normally, we think of a pipeline is just a source, a sink, and what's in the middle. But the actions can happen before or after. So let's say that I move data into BigQuery, and then I need to run a stored procedure. Or I need to copy the data to a different thing. I can run this action after the pipeline, and it will run this cleanup or push-to-production script. Or if I needed to clean up before the pipeline, I can put actions there as well that will occur before the pipeline. So before I even pull from the sink, it will run this command on BigQuery, or this command on Cloud SQL, or this GCS creator copy. So these actions can be done before or after the pipeline to kind of give you more capabilities within that pipeline. Person: And can those actions include math? Like mathematical? Aaron: Include math. Person: Yeah, like summations of table of columns or anything like that? Aaron: Yes. So basically, this is going to do any SQL-- this particular action is a BQ execute action. There's lots of different types. But you can do anything in SQL. So if you wanted to do a select, or you wanted to do a update table and put kind of a sum-- you know, anything that you could put in SQL, you could put in this command and it would execute that on BigQuery. Person: Thank you. Aaron: Yes, you're very welcome. And this gets used quite commonly. So often, these sinks get used more as almost of a staging area. So the whole pipeline stages into there. And then once it's all been staged, then we might run something else that copies it from a staging area to a final area, or does a final transfer inside of BigQuery, so that it's sort of all atomic. That pipeline fails halfway through, it's only failed into the staging area. We never actually got to the area where we commit it to the final destination. And then we can rerun it and not have any sort of half data migrated, half data not migrated.

### Lab - [Building Realtime Pipelines in Cloud Data Fusion](https://www.cloudskillsboost.google/course_templates/501/labs/359705)

In addition to batch pipelines, Data Fusion also allows you to create realtime pipelines, that can process events as they are generated. Currently, realtime pipelines execute using Apache Spark Streaming on Cloud Dataproc clusters. In this lab you you will learn how to build a streaming pipeline using Data Fusion.

- [ ] [Building Realtime Pipelines in Cloud Data Fusion](../labs/Building-Realtime-Pipelines-in-Cloud-Data-Fusion.md)

## Data Sharing with BigQuery

This module shows how BigQuery can be used to share data within an organization.

### Video - [The Importance of Data Sharing](https://www.cloudskillsboost.google/course_templates/501/video/359706)

- [YouTube: The Importance of Data Sharing](https://www.youtube.com/watch?v=86UFas436-k)

Kenny: Right, what's the most valuable resources these days? Here's the quote. From Economist, they declared it's not oil anymore, it's data. And in 2020, MIT was saying the era of big-data silos is fading. Shared data is the future. That's why I picked data sharing as today's topic. This is what we see in a lot of businesses. And we talk about 360 degree views, not only breaking data silo to see what's going on in our business or in our mission to serve our citizens in the government or institutions, and we also find the whole loop-- the feedback loop take the data, take the insight we got from the data, and send it back to our mission system is also very important. In that process and be able to build the data-- processing data in real time is the key. I start to see more and more our customers pursue to build infrastructure and realize Google Cloud platform to go real-time. Another thing I see is we talk about data driven, we talk about digital transformations in traditional systems. We know we have OLTP. Then, we build a reporting system. Then we call it decision support. Now we call it business intelligence. At this stage, we see there's a new type of application pretty much is driven by your data, and that's called machine learning. For all those processes, we realize we need more and more data. And a very important part is we need to share the data more often. That will help us reduce the value gap between different divisions, different agencies in our organizations. Of course, when sharing the data, security and data governance is the key. So I'm also going to talk about in our two products how we solve those problems. If I dive into more details, I also want to-- you know, if you have questions, please raise your hand and I would like to make this a more interactives. Here's the Google Cloud. Typically, in Google Cloud, this is how we organize resources. So each of our Enterprise customers, you have your organization. We call the domains. Then, under this domain, you have multiple folders that you logically organize your resource, your processing, your data either by your line of business or by your divisions, by your groups. And in this folder, then we have something called projects. Inside a project, that's where you allocate your resource. And project also is the place you can set up your billings. And you also can set up your Id and management. Of course, the identity management and the billing can also be set up at an organization levels.

### Video - [The need for an integrated data platform](https://www.cloudskillsboost.google/course_templates/501/video/359707)

- [YouTube: The need for an integrated data platform](https://www.youtube.com/watch?v=uOndGBvSiL4)

Kenny: This is the platform that I love to use quite often and with a lot of my customers and partners. And what I love about this slide is, on the top, you can see who will interact with your data through the processings. We call it persona. You see we have data engineers. We have data scientists. We have developers. We have data analyst. We also have business users. A lot of people these days, they need access to data efficiently. And also, to second row, you can see that's our data life cycles, right? We collect our data from leftmost-- your mission systems from your partners, from your public data sets, such as sit on the data [INAUDIBLE] dot com, dot gov, or from your Google BigQuery public data sets. Oh, you may also get a data from your devices. For instance, for securities, we have so many firewalls, so many network switch, so tons of those devices, and all LTE devices. So those devices also feed us a lot of data. And what data we got typically, and we ingest our data in two different mode. One is we ingest in a batch mode. Another one we do is a real-time amount. So quite often, as data engineers, we build the pipelines to ingest data from our [INAUDIBLE] data generator on the left, then go into our data platforms, shown here. And next step, you see tons of tools on Google Cloud, like Dataflow, Dataproc, Data Diffusion, and Composers. We do ETL jobs-- Extract, Transform, and Load data-- or load data, extract, or transform. Then we store the data in either relational database or Google Cloud Storage. We call it the buckets. And the buckets, typically, we see structured data, semi-structured data, and unstructured data. In the relational database, like BigQuery, typically, you see structured data. And not on the list but also very important, I see, another resource we have is called Google Healthcare API. That's where we store HL7 medical-related information. That's huge, a lot of information there as well. Once we got a data collected, stored in the BigQuery, next step, we want to analyze data. And a lot of tools we're using and include machine learning in BigQuery, we call BQML. And BigQuery itself has very powerful SQL engines. That SQL engine can access data in BigQuery but can also access data sit on the Google Cloud storages. That's where I want to talk about how we enable that in Dataplex today. So you can see, today, I have some BigQuery highlighted in the blue background. I also have a BigQuery Analytics Hub. That's what I'm going to talk about, demo about, that you see how it works. At last, you can see, at a business user level, or machine learning, data scientist level, that we have a list of tools here, include third-party tools, enables you extract the value from your data. Down at the bottom, you can see across all your data life cycles. We have tool, like data catalog, Dataplex. We have security tools. We have operational tools help you manage the life cycles of your data. And rightmost, after we extract value, we'd like to fit this data insightful information back to your mission systems, drive your mission systems, help you make our life better.

### Video - [BigQuery Overview](https://www.cloudskillsboost.google/course_templates/501/video/359708)

- [YouTube: BigQuery Overview](https://www.youtube.com/watch?v=TRk7W1ynfog)

Kenny: All right, now move on. Just one example. I talked about in Google project-- in Google Cloud, how we organize data in multiple locations, multiple resources, how it is structured. This example, so just in BigQuery. BigQuery we call-- is it regional service? Or we call it the national service. What that mean? In BigQuery, not like our-- some other public cloud providers, you don't specify the cluster size. You don't specify machine types. BigQuery is serverless. OK? I'm going to say that many times. BigQuery is serverless. Simply, once you log onto Google Cloud, if you have a privilege, you can go in and say, I want to enable BigQuery API. That's all you need to do. After you enable BigQuery services, then you can create your data set. At the time when you create your data set, as I showed on this slide, you can see, that's where you determine where your data is going to reside. You can say-- you can set a data location at the US, or you can set your data location as region. For instance, here I choose us-northeast-1. So behind the scenes, BigQuery, Google Cloud, we managing multiple copies of your data. We guarantee your data is durable with 11 nines. 99.9999 percentage. So what does that mean? Basically, behind the scenes, we keep five copies of your data. So we make sure you have highest availability. We also make sure, if something happened-- disaster, for instance-- earthquake, that's one example-- you still have your data. In BigQuery, this is a common, typical concept I just want to quickly go through. You can see the projects. Underneath, you have a project-- you have multiple data sets, sorry. In the project, you have multiple data sets in BigQuery. And each data set is kind of like a database or schema in traditional database. Underneath the data set, you have tables, you have views, you have materialized views, you have a temporary table, you have authorized views. And within the project, you can run the jobs. And we also add new objects. It's called Snapshots. So what a Snapshot does is you-- if you have a database and you want to say, hey, my data looks great and I want to keep my data-- for instance, like, this morning, now, I want to take a Snapshot of my table-- then maybe a month later and something happens, someone accidentally deleted my data, I can go back to my Snapshot and clone my data back, restore my data. OK, so Snapshot is a very effective way to keep one kind of copy of your data. And you don't have to copy the data out, so saves you the space and also the cost. Of course, some people is not satisfied with that as well. Some people say, oh, no, Snapshot is still in the BigQuery. I want something outside of a BigQuery. Yeah, sure, you can do it in the Dataplex. I'm going to talk about it later. We enable data tiering. So you can set up the jobs. You know, if you'd like, you can pull data out from a BigQuery to Google Cloud Storage as extra copy. So just peace of mind. If you feel comfortable to do that way, yes. We not only support you. We help you automate the process in the Dataplex. I don't know people come here today is how familiar you are with Google Cloud BigQuery, so I just showed some of Google Cloud BigQuery user interface. Then I will do the demo, then you can see how those things fly and how those things work. And this is the screen of a BigQuery workspace-- quite widely used. And you can see leftmost on the top, this button allows you add data. You can add a data set. You can add public data. And on the leftmost, there's a list of projects and also, in each project, the list of data sets. In the data set, you have a list of-- you see, I have a list of tables. Then leftmost you see, after the red line, I say Analytics Hub. That's where Analytics Hub reside. If you are using BigQuery already, you might see that feature enabled because BigQuery Analytics Hub is in a preview now. Move over, you can see in this worksheet we have multiple tabs. The tabs will show your schema, your table definition, will also let you put in or type in your SQL statement, cut and paste, whatever. You can run your query. And there's also a control to say, if I'll run that query, what kind of mode I want to run. Also show you, if I want to save my query, or I want to share my table, or I want to schedule my query to run, you can simply, easily, within same user interface, just a single click of a mouse. Beneath this is your query. Then you can see there are four different tabs. You see that one is job information. If you find your query, your query's going to come back quickly, or query can take a longer time. If a query takes a long time, the job information enable you to cancel the job. Also, next line, you can see that your query result come back like a table format. And if you'd like to see in JSON format, you can just click the third tab. The last one I circled with yellow circle there. That's called execution plan. That helps you analyze your query. If you see concern your performance, you can take a look. Then you can actually see under the hood, how does BigQuery query engine works? All of them on the one screen. The line below that you can see is execution details. You can see how long your query has been run. In this case, elapsed time is 10 seconds. Then what's the total time used if we aggregated all the processors together? Then you can see some other execution information. Down the bottom, on same user interface, you can see we also show-- we captured log. So if you are personal with the query you run, there's your personal history and also have a project history. So you can go back to see what job has been run. What that also tells you is you on BigQuery-- you can always go back to History to see who did what when. All right. Move rightmost-- in the corner, you can see, once you put in your SQL statement, BigQuery will automatically check your syntaxes. If you have wrong syntaxes or something you mistyped, that will have a right-- that tells your message-- tells you what's wrong with your query. And if you correct your syntax error, then you're going to see this green checkmark. Then next to your green checkmark, we also estimate this query-- how many bytes, megabytes, gigabytes will cost you. OK, so then you knew, before you run the query, you kind of know, it depends on what kind of price amounts that you do, especially if you do it on demand. This will tells you how many bytes being processed. You know how much it's going to cost you. Then you can see that below this one, you will see you can save the result or you can export the result. Well, I talked too much now. Let's take a look. BigQuery.

### Video - [Demo: BigQuery](https://www.cloudskillsboost.google/course_templates/501/video/359709)

- [YouTube: Demo: BigQuery](https://www.youtube.com/watch?v=zgL8u_9lueY)

Kenny: You can see the same user interface I talked to you folks earlier. And you can see-- this is a live. You can see I'm in these projects. You can see in the project, I have many data sets. And one of the data sets has got API Apaches. And in this data set, I have a table called sales. You can see-- yeah. And also, let me see. I slipped over. Then you can see another console, a tab I have opened. The difference is, this table, you can see a BigQuery unique feature. We have, like, an embed. So you can have a table embedded into another table. In relational database, typically, we call this cluster table. It's quite open in BigQuery, and it does provide efficiencies. This table I have here, this called sales_partition_clustered table. I want to show you the details of this table. You see how big this table is? Can you see it? 1.9 petabytes. You see how many records I have here? Over a trillion records. All right. I showed this as a query SQL statement. I'm going to run the query on this table. This table, as you see, I have a petabytes table. I have a, what, trillion records. Now result come back. How long it takes? Six seconds. I processed 285.9 gigabyte-- bytes. That's how much data we just processed. And you can see the records come back. And you'll see the class table has benefits, so it's sales, sales items, sales line items. So this first record, I have eight line items.

### Lab - [BigQuery: Qwik Start - Console [PWDW]](https://www.cloudskillsboost.google/course_templates/501/labs/359710)

This lab shows you how to query public tables and load sample data into BigQuery using the Web UI. Watch the short videos <A HREF="https://youtu.be/rwZsPjCTkhw">Get Meaningful Insights with Google BigQuery</A> and <A HREF="https://youtu.be/dOpNxH64JIU">BigQuery: Qwik Start - Qwiklabs Preview</A>.

- [ ] [BigQuery: Qwik Start - Console [PWDW]](../labs/BigQuery-Qwik-Start-Console-[PWDW].md)

### Lab - [BigQuery: Qwik Start - Command Line](https://www.cloudskillsboost.google/course_templates/501/labs/359711)

This hands-on lab shows you how to query public tables and load sample data into BigQuery using the Command Line Interface. Watch the short videos <A HREF="https://youtu.be/m0rqccviLNM">Get Meaningful Insights with Google BigQuery</A> and <A HREF="https://youtu.be/dOpNxH64JIU">BigQuery: Qwik Start - Qwiklabs Preview</A>.

- [ ] [BigQuery: Qwik Start - Command Line](../labs/BigQuery-Qwik-Start-Command-Line.md)

### Lab - [Big Data Analysis to a Slide Presentation](https://www.cloudskillsboost.google/course_templates/501/labs/359712)

This lab leverages two Google developer platforms: G Suite and Google Cloud Platform (GCP). It uses GCP's BigQuery API, Sheets, and Slides to collect, analyze and present data.

- [ ] [Big Data Analysis to a Slide Presentation](../labs/Big-Data-Analysis-to-a-Slide-Presentation.md)

### Quiz - [Quiz: Data Sharing with BigQuery](https://www.cloudskillsboost.google/course_templates/501/quizzes/359713)

#### Quiz 1.

> [!important]
> **Your organization is implementing new machine learning processes. How can you use your data as you're building out these processes?**
>
> - [ ] Query the data faster by Importing your datasets directly into a folder.
> - [ ] Create national datasets to gain more granular insights.
> - [ ] Share your data to break silos and enable data-driven decision making.
> - [ ] Ensure that all your data is structured.

#### Quiz 2.

> [!important]
> **You have enabled BigQuery and need to set up a dataset that is region specific. What should you do?**
>
> - [ ] Import your dataset and specify the location in the project.
> - [ ] Create individual tables for each region in your dataset.
> - [ ] Create a project for regional data.
> - [ ] Create a folder for regional data.

#### Quiz 3.

> [!important]
> **You're organizing your resources in Google Cloud, and you need to allocate resources and set up billings. What should you use?**
>
> - [ ] A silo.
> - [ ] A billing account.
> - [ ] A project.
> - [ ] A folder.

#### Quiz 4.

> [!important]
> **The data of your organization has already been processed and stored, and you're ready to start analyzing the data. Which tool should you use?**
>
> - [ ] Data Fusion
> - [ ] Pub/Sub
> - [ ] Dataflow
> - [ ] BigQuery

## Data Sharing with Analytics Hub and Dataplex

This module gives an overview of Analytics Hub and Dataplex, describes data lakes, data warehouses, and the differences between them, and demos both Analytics Hub and Dataplex.

### Video - [Analytics Hub Overview](https://www.cloudskillsboost.google/course_templates/501/video/359714)

- [YouTube: Analytics Hub Overview](https://www.youtube.com/watch?v=-k0kaPYV6aw)

Kenny: Now let's talk about Analytics Hub. What is Analytics Hub? Analytics Hub are the piece of functionality we build on top of BigQuery. And you probably know a lot of this-- a lot of companies, organizations these days will have data owned by different agencies, generated by different applications. And to share those data becomes a headache. We see on BigQuery today, we have 4,500-plus organizations shares over 250-plus petabyte data in the BigQuery these days. And this is what we think is important. We think any data platform which serves you, we want you spend time on your data, extract value, do things useful. And you don't waste the time on maintain the infrastructures. So that's why see the serverless is very important. Another thing we think is also valuable to you is do not duplicate data. Only make the copy of your data if you will update the data or change the data. The reason for that is less duplication, you pay less money. Less duplication, you don't have to maintain the data pipelines. Third thing we see is a lot of times these days, when we're sharing data, we're not only sharing within organizations. We share data with our partners. We share data with third parties. So quite often, we'd like to people who pay the data, who pay the usage, being separate. So that's why in BigQuery-- and not only BigQuery, a lot of other Google products, we decouple compute and storage. We build our product like BigQuery or [INAUDIBLE] Dataflow or Data Fusion using microservice architecture. By decouple them, we enable you bill the user, or usage, or bill by storage or by compute separately. And other thing is we find also very useful in demand is when people come to access data, they want to be able to uniform the access data. They say, hey, I'm in BigQuery. I want to access data in database, but I also would like to access data outside of the database. That's why we have a [INAUDIBLE] query. We have external tables. And to enable you to centrally govern your data, secure your data, that's where Dataplex come help you. I'm going to show you that later. OK. All right. Now let's think about it. One, do we share the photos in a family in old days, right? We have a one-- we have cameras, one or two, and we have family albums that we all share, enjoy, see where we travel, where we go. But at this stage, everyone has their own smartphones, right? And with the smartphone, how do you share the data? How do you share your photos? Think about this. That's where-- same analogy in the business. In each divisions, they have an application. They generate the data. They maintain their data life cycles. How we should let them to share data? None of them, for instance, your family, your wife, husband, son, and daughter, especially between the generations. Think about it. Who gives you access to all their photo libraries? My family-- it's very hard for me to get the data-- share the photos from my son or daughters. What do they do? They usually-- they carve out the album in their Google Photos. They say, hey, Dad. You know, here's a few photos I share with you. So they put up some subset, very small subset, of their photo they share with me. This should be the same way when we share our business data. All right. Here's an example of organizations, right? So today, we have multiple divisions and groups that we have our projects. In the projects, we have staging data. We have curated data. And then we publish data. So when we come to the data share, we typically make the-- publish the dataset. Make them as a shared. I will hold on my curated data. I will hold on my raw data. I probably tightly control my raw data. I don't want other people to peek into my private data, right? Same thing and with some other organizations. So other organizations, when they share, they can have external dataset shared, or they have a public dataset shared, but they all share coming from this, what-- published datasets.

### Video - [Enhanced Data Sharing through BigQuery](https://www.cloudskillsboost.google/course_templates/501/video/359715)

- [YouTube: Enhanced Data Sharing through BigQuery](https://www.youtube.com/watch?v=0RTQMJBzoU0)

Kenny He: That's where Analytics Hub and BigQuery comes. In Analytics Hub, on top of the BigQuery, it allows you from your created data set, attached as a shared data set. Then you create an information exchange hub. In exchange, you add the data set as your listing. So one exchange, you could have multiple listings. Then you have a data publisher. That publisher will make the data shared, put in exchange as a listing. Then you have a subscribers, the people who want to read the data or get access to data. They check on the exchange, check the listing. They search, they found it, they subscribed. When they subscribe, they want to add your data set into their project and their subsequent project as linked data set. Do you make a copy? No. The linked set is just a pointer, point to your shared data set. See, in terms of the cost, the shared data set-- the owner of the data provider project paid the storage of the shared data sets. Then they subscribed the project. When you run a query against the shared data sets, you pay the execution. You pay the query cost. If you want to keep a copy, if you want to make them change and share the data substrate produced, you can create a table from those queries. And then you can add, enrich the data, but then you're going to pay the cost of the storage. Any question about this flow?

### Video - [Exchanges and Roles](https://www.cloudskillsboost.google/course_templates/501/video/359716)

- [YouTube: Exchanges and Roles](https://www.youtube.com/watch?v=BhU5p5whUYs)

Kenny He: And I mentioned about the exchange. I mentioned about the listings. This slide basically just to show that-- see, as publishers in the BigQuery, you can create Analytics Hub. You create exchange. And after you have exchange created, you add data set as your listing. And then some other users, people who interest your data, they can subscribe your exchange, subscribe these listings, by adding your data sets onto their projects list. They get a link to data sets. OK? You can create many exchanges in the Analytics Hub. And you can managing the securities, govern your data, and you're also going to be able to search, that people, they don't know what available out there to search to see what data is available. To manage the whole process, this is in the Analytics Hub analyst-- five roles, we enabled you. One is Analytics Hub admin. And admin is the person who can enable the services, who can designate who is exchange administrators. Exchange administrators can create exchange. Then we also have Analytics Hub publishers. And besides the publishers, of course, we have Hub subscribers. And we also could have Hub viewers. The difference between subscribe and the viewer is the subscriber can view not only schema or structure of data but also run the query, see the records in your table. The viewer can only see the metadata. So if you don't know this person and you want to guard the access, you want to verify this person, then you can just give them viewer access. And they can find what data is available out there, but they don't see the actual records. If they want to see the record, they have to come to you, apply for access. Then you can grant them subscriber access on your shared data set. Any question?

### Video - [Demo: Analytics Hub](https://www.cloudskillsboost.google/course_templates/501/video/359717)

- [YouTube: Demo: Analytics Hub](https://www.youtube.com/watch?v=sheCrKWCf7I)

Kenny: I'm ready to show you how to make this happen. OK, let me shoot a demo. In this demo, this is the flow. First, I have [INAUDIBLE]. I'm going to show how do I discover data, then add data to my data sets. Then I'll show you data publisher, how do I publish my data. And it also as the a Data Hub admin, how do I exchange or change my exchange listings and customize, make them more meaningful? Let me show a slide here. No, not slide. Can you see my screen? John: Yes, sir. Kenny: All right. Thank you. We're back to BigQuery worksheet. But now you're probably familiar this one already. And I'm going to show something more, as you can see. In BigQuery, I have a single workspace, you can see. Analytics Hub is listed here. And also, I mentioned earlier that we can add a data set. This is another way to access Analytics Hub. So I can pin the project, which makes it on top. So I can always have access, or I can explore public data set host in BigQuery. I also can add external data source, or I can explore my Analytics Hub. Let's do that. Now you see in this Analytic Hubs. And I want to see information. For instance, I want to see the healthcare. I don't know what's available out there. So I can hit Enter, in the search. Now you see there's two data sets showed up, one is my FHIR patient. Another one is my Salesforce account, in which I have some interesting data, health care related. Now, if I go to OASH-- this is the demo I gave for other organizations. There, you can see, this is the data set I published. And you can see I have some descriptions here. And I also have the query samples here. You see Run this Query. So in my descriptions, I can just explain what this is. And I can give it a specified term of access. I also can impact some queries, and in the future, will also will enable you-- enable somewhat like a Google Data Studio or Google Looker Blocks in the published areas. And you can see I collected-- I don't know if you still see my screen. Hold on, let me make sure I'm on a-- yep. So I click Run the Query. And I went to another screen. You see in this is screen, what happened is that they should have taken me to my query. I lost the link. Oh, I see why. That's because of security, that URLs-- I did set up that. All right. They didn't show this-- the query itself. this Is another way I can access Analytics Hub. You can see menu. I click Analytics Hub. Then, I can search my listing, and also because I am an Analytics Hub admin. So I can maintain multiple exchanges. You see I have one, two, three, four, five exchanges. And then, I also do search my listing this way as well. So, all right, let's go in my FHIR patients. All right, now let's add the data set. So I have this data set, where I have my FHIR patients, you can see when I add the data sets, I can specify what's my Project names. I can give my data sets name. Let's see, in this case, I'll say I want to call-- I want to-- oops. I want to call it linked_fhir_patients. Good. So you can specify the link to data sets, in which projects of yours. And what's the name of your data set? Then, I just Save. OK, I can go to the linked data, once I saved. All right. Let me go to BigQuery. Let's go to BigQuery to make sure. All right, I switch to this project. These are specified my projects. And I linked the project, you can see. Well, I'd probably refresh this screen. Make sure. Because my Windows, usually, if I don't refresh it-- and now, let me see. There you go. You see, here's my linked data set. Let me open this data set this way. And also, I can use expand it. Then you can see, in my data set, what I have. In my data set, I have multiple tables. I have three machine learning models. And you can see, in this data set, you can see this data set next to my name. It points to-- this is a linked data set. You also noticed, in this data set, as a permissions, I have a released permission. I have these two actions I can take against this data sets. This data set actually is from another data set of mine. Let me also open that data set so that you can do the compare. This is Apigee. This is my data set. OK. you can see the data set opens Apigee. I say, I can create the tables. I can share the data set with some other peoples. I can copy the tables. But in this linked data set, I cannot do those actions. I can only read the data from the linked data sets. And I remember, Sean, the question you had before, enriching my data. And most likely, what we do is your linked data set in your project and another data set, you can run a query. You can join in those two different tables. That's how you enrich your data. OK, I wish I didn't confuse you there. Any questions? John: Kenny, we had another question about can you share specific list of tables via Analytics Hub? Kenny: Yes. John: So instead of that entire data set, could you pick and choose which data to [INAUDIBLE] share? Kenny: Today, you can not take partial of your table in the data that share with them. Today, we cannot. If you have this request demand, let us know. We can put in a new feature request. Today, the shared data, as a whole, we cannot partially cut the shared data set. So you can share half and hide another half. No. John: OK. Is that at the table level? Can you share individual tables? Kenny: At Analytics Hub, we don't do share at the table level. We do share at the data set level. That's what we have today. John: OK, so at the data set level. OK. Kenny: Yes. Yep. John: Great. Thank you. Kenny: No problem. . Let me go back. Yep. All right, now, we saw how we discover data, how we add a data set. Now let me go back to see how we create those exchanges. So you see that the data set I have actually is coming from this exchange, Healthcare Data 01. And for me to create exchange, I just simply click Create Exchange. Then I give the name. I put in a Primary Contact. Primary Contact could be username. Typical is group name. That's Primary Contact should be. OK. Now let me tap into this exchange. In the exchange, now you can see I can create listings. And you saw my listing, FHIR patients. Now, let me show you what the listing-- You saw this before, right? This is where the descriptions, additional details, publish informations. That was a query I put in. If I do Edit, then you can see I'll give the Display Names, Contact. Who is my Primary Contact? Typically, you should be using group names here. Now, who are the providers? And who is publishers? And this is the description. This is the icon. You see I have icon JPG files. And this is where I put in my queries. You see, once I say Run this Query, that's what I put it in query. You see I was not able to run the query. That was because this copy I put in last night. You can see I have a project-- these structures. You can see troubleshoot-- oh, I have table names queries. So this is how you put information. And also, of course, you can preview it. Then you go back, make sure that you can Save it. That's your listing. That's very simple. And in this listing, you-- oh, one thing I forgot to mention, I think, was-- yep. And also, in this listing. the term-- and you see you can put in the tables. You can see on the right side. And there's a whole slew of syntax descriptions. If you're interested, you can-- this is the markdown syntax. This quick reference tells you how to customize this description, very easy to do, pretty much like HTML, but obviously simpler than HTML. All right, let's go back. John: Hey, Kenny? Kenny: Yes, sir. John: There was one question that came in that we might need some clarity on. And then also, I wanted to give you a time check. We're at one hour, so we've got 30 minutes left. The question was, could you enable-- so this goes back to the sharing tables or data sets-- could you create a view that only allows the view of a table? So you only share-- like, I was thinking this would be possible, if I understood what you said before, as you could create a view in a secondary data set. And then share that data set out. Kenny: In the shared-- the data sets, that includes view, yes. You can include a view in your shared data set. But whatever you put in the shared data set, when you share, it has to be a whole. So one way you can do that is, say, in my shared data set, you say I do not want to put it in any table. All my objects in my shared data set is made up of views. Yes, you can do that. John: OK, great. Thanks, Kenny. And can you do-- and I'm not a BigQuery master-- could you create a view of a table that sits in a secondary data set? Kenny: Oh, yeah, yes. You can-- yes. John: I thought you could, but I-- Kenny: Yep. You can create a view from separate data set, not only from your organization. User can pull them from a public data set, from other organizations as well. John: OK. Yeah, so that would be a good workaround to make that work. Kenny: Yep. This is a Create Listing. I just want to show you quickly. I'm kind of slow here. This is where you put your data sets. You put what's your Source Datasets name. And you put in the Icons. You put your Markdown languages All right. No more questions? Any more questions? John: No, I think we're good. Kenny: All right. Thank you. And put all together. You see this slide, on the left, you have data publishers. On the right most, you have data subscribers. And also, we can think this-- you have a project called Analytics Hub. Run Analytics Hub only. Then that could be-- you can think of the way as. That's my future enterprise data warehouse. All the data that's showing are from that project. That's an EDW project. In my EDW projects, they are made up of my Analytics Hub-enabled data exchanges. In the exchange, it might have multiple listings. That way, there's a new way to build your enterprise data warehouse, makes your life easier. Let data owner be the owner, managing all the data lifecycles. And use the one as enterprise data warehouse. You manage control, management access, make everybody happy. This is just some of [INAUDIBLE] possibilities with Analytics Hub. You have a public data exchange. You can Google data exchange. You have a private data exchange. You can have internal exchange, commercial industry. The sky's the limit. Use your own imagination. Happy using building your enterprise data warehouse on the top of Analytics Hub.

### Video - [Lakehouse and Data Lakes](https://www.cloudskillsboost.google/course_templates/501/video/359718)

- [YouTube: Lakehouse and Data Lakes](https://www.youtube.com/watch?v=K1INS_M5haI)

Kenny: All right, let's move on to Lakehouse, data lake. What is data lake? Typically, we see data lake is at a storage level. We put into it, like, a block storage. In a Google Cloud, it will be Google Cloud Storage. We'll have structured data, semistructured data, and unstructured data. That's data lake. And on top of this data-- and we typically have a use case like a web clickstream analysis, sensor data, whole nine yards. Then you can compare the traditional data lake and data warehouses, right? And underneath, the data types are slightly different. And also, usage is slightly different. In data warehouse, we do standard SQL statement. And great-- well, also on the top, we build our BI applications from like, Looker, Dabbler, Power BI, Oracle BI, whatever it is. And the limitation for traditional data warehouse is you have limited support for machine learning in other platforms, but not in the BigQuery because in BigQuery, we fully support machine learning. We have over about 18 different types of models you can build it. You also can import TensorFlow, create a model into BigQuery, apply that. Proprietary system-- yes, that's true for many traditional database to support a data warehouse on a data lake site because they are blocks of storage, whatever the nature of your format is-- CSV, GSI, whatever-- just to use it. On the top, many those people in data lake-- they run map reduce, they run machine learnings. And they do a lot of those kind of things. And one thing-- this is-- we see people emerging. And people will say, we love SQL statement. We want to access both. We'd love to build application on top of both-- you know, relational, data lake access at the same time. That's where Lakehouse comes from. And Dataplex is our tool to enable you to do that, merging BigQuery and data sit on the Google Cloud Storage. In Lakehouse enabled by Dataplex, we not only allow you run SQL in BigQuery. In Lakehouse enabled by Dataplex, we also enable you to run Spark jobs. We also run-- enable you to run Notebook from the Dataplex. So it gives you uniform access to data. Doesn't matter where they are. The advantage is if you love to keep your data on Google Cloud Storage, leave them there. You don't need to put them into BigQuery. You love data in BigQuery, leave them. That's what I said before. We do not encourage you duplicate a data. Waste your money, waste your time. Let's keep where the data is, use them in uniform, access them from anywhere

### Video - [Dataplex](https://www.cloudskillsboost.google/course_templates/501/video/359719)

- [YouTube: Dataplex](https://www.youtube.com/watch?v=nHLYzJ4a3X4)

Kenny: All right, let's jump into Dataplex. Same analogy. And we shared photos. Let's think about smartphone photo pictures as your BigQuery database. Then think about it. At home, you have a cassette, right? You have old albums. You have a VHR. I'm so old. And I don't know if you guys have seen a VHR or not, but a lot of those tapes, we treat them as your data lakes. What is Dataplex? Dataplex is, let us to build a lakehouse to combine all of them together. We can create my lakehouse create these zones. In the zone, I can attach my data assets. OK, As I said, uniform-- all those different storage, different process engines, that's what Dataplex does. And then you can see, Dataplex implements multilayers underneath with multicloud on-premise structured, unstructured, semi-structured. And in the middle-- and a very important piece-- is in Dataplex with uniform metadata. That's the secret sauce. So all BigQuery metadata and all Spark metadata, typically, we store it in a metastore. We uniform them, and Dataplex keeps them in sync. All right, use cases and what is Dataplex helping. Once I have this Dataplaex, I can enable you-- we call it the data mesh. So I can see my data not only in BigQuery, but also see my data on Google Cloud Storage. This is logically how we organize my lakehouse using Dataplex. At the top, I created my lake. Then I created my zones. Once I have a zone, then I can create multiple assets. In the assets, I can attach either BigQuery or GCS buckets into my zone. So my zone and my lake, they're logical structures. And then my assets is kind of like listings in the Dataplex. But different from an analytic hub we talked earlier today is analytic hub only operates within BigQuery. In Dataplex, we're solving different problems. In Dataplex, we merge data in BigQuery and on Google Cloud Storage together. We form the lakehouse. I'll give you one example. It's-- I work with the state government. One thing they-- one mission critical system is the integrated legibility systems. In those integrated legibility systems, they have structured data. They also have a lot of PDF files. And they also have like, driver's licenses, their identity informations. So when they do the auditing, the auditor needs access to all those data. So that's one pain point. A lot of states have the problem to link them together. With Dataplex, we're able to let them to put all the data like into the zone, for instance, for a county. Like in Montgomery County-- I have all my citizens, my people who get government services or need government services put in one zone. Then I have assets. One asset is a form of our BigQuery. It has all the structured data. Then in other assets I have my GCS bucket. It has my semi-structured data. Then in another GCS bucket, I have on my unstructured data. So if you need audit, you come to this zone. You can access all the data at the same times. And in Dataplex, we not only enable you to organize those data logically, merge them together, or mash them together. More importantly, we enable you automatically discover your data, especially in Google Cloud Storage bucket. Let's say you have a particular file. I have CSV files. And other cloud providers, they ask you to create an external table. With Dataplex, only thing you do-- you need to do-- is create a bucket, copy the data in there. Dataplex will automatically discover those data. Then we add this data as external table for BigQuery. You can read it. You can view the data right away. Also coming with Dataplex is called Data Quality tools. So you can describe what is your data validation rules. Then we can automatically check your data schema. We can check your schema drifting. Then we can check if your data is missing or has invalid values. That's a lot of advantage coming with Dataplex. And Dataplex also has a tool we use to enable data lifecycle management, automate a lot of processes. That's the platform we see give us advantage. Now let's show how Dataplex works. OK, Here is a demo flow I want to show you, and I'm going to show again similar with analytic hub. First, I'm going to do some discovery and find a table. Then I'll query this table. And this table actually is an external table on GCS. Then I'm going to show you how to add assets in my Dataplex, how to create my lakehouse, how the uniform access works. All right. Let me--

### Quiz - [Quiz: Data Sharing with Analytics Hub and Dataplex](https://www.cloudskillsboost.google/course_templates/501/quizzes/359720)

#### Quiz 1.

> [!important]
> **Lakehouse merges:**
>
> - [ ] MySQL and structured data.
> - [ ] BigQuery and Dataplex.
> - [ ] Data and security.
> - [ ] BigQuery and data.

#### Quiz 2.

> [!important]
> **How does Google Cloud ensure that data is shared securely and inexpensively?**
>
> - [ ] Centralized governance and security.
> - [ ] Analytics Hub runs on-premises so that you can control your data.
> - [ ] Only you can access your data.
> - [ ] With Analytics Hub, data is duplicated for easier sharing.

#### Quiz 3.

> [!important]
> **You need to unify distributed data to help automate data management. Which Google Cloud tool should you use?**
>
> - [ ] Lakehouse
> - [ ] Data Fusion
> - [ ] BigQuery
> - [ ] Dataplex

#### Quiz 4.

> [!important]
> **What is an exchange?**
>
> - [ ] A way to manage roles in Analytics Hub.
> - [ ] Permission, visibility, and access control.
> - [ ] A collection of dataset listings for sharing and discovery.
> - [ ] A data query.

#### Quiz 5.

> [!important]
> **You need a data storage solution for a Business Intelligence application. Which data storage solution should you use?**
>
> - [ ] Data Lake
> - [ ] Database
> - [ ] Data Bucket
> - [ ] A Data Warehouse

#### Quiz 6.

> [!important]
> **Which is NOT a benefit of using Dataplex?**
>
> - [ ] Efficient Data Ingestion
> - [ ] Integrated Analytics Experience
> - [ ] Centralized Security and Governance
> - [ ] Intelligent Data Management

#### Quiz 7.

> [!important]
> **Which Analytics Hub role creates data assets and publishes exchanges?**
>
> - [ ] Publisher
> - [ ] Administrator
> - [ ] Subscriber
> - [ ] Exchange Administrator

## Course Resources

Student PDF links to all modules

### Document - [Student Slides](https://www.cloudskillsboost.google/course_templates/501/documents/359721)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
