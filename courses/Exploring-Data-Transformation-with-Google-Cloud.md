---
id: 267
name: 'Exploring Data Transformation with Google Cloud'
type: Course
url: https://www.cloudskillsboost.google/course_templates/267
date_published: 2024-10-18
topics:
  - Data Management
  - Data
  - Data Processing
---

# [Exploring Data Transformation with Google Cloud](https://www.cloudskillsboost.google/course_templates/267)

**Description:**

Cloud technology can bring great value to an organization, and combining the power of cloud technology with data has the potential to unlock even more value and create new customer experiences.

"Exploring Data Transformation with Google Cloud" explores the value data can bring to an organization and ways Google Cloud can make data useful and accessible.

Part of the Cloud Digital Leader learning path, this course aims to help individuals grow in their role and build the future of their business.

**Objectives:**

* Examine the value of data and how it affects customer experiences.
* Explore the different Google Cloud data management solutions that are available.
* Learn about the ways Google Cloud products have made data more useful and accessible to a workforce.

## Course Introduction

In this introduction, you'll explore the course goals and preview each section.

### Video - [Course introduction](https://www.cloudskillsboost.google/course_templates/267/video/512683)

* [YouTube: Course introduction](https://www.youtube.com/watch?v=jNmxl3wtyU0)

Business data is not a new term, because organizations have long applied information about performance and operations to make decisions. With traditional methods, though, data analysis can take days or months, and is often incomplete. In addition, specialized teams are often required to produce complex reports. With cloud technology, this doesn’t need to be the case. Data can now be consumed, analyzed, and used at speed and scale never before possible. In fact, organizations can now benefit from cloud technology to ingest data in real time to train machine learning models and to act in ways that benefit their business. You no longer need to be a data scientist or technical expert to perform data analysis. With that in mind, this course, “Exploring Data Transformation with Google Cloud,” is designed to help you understand the value of data and how it affects customer experiences, learn about the different Google Cloud data management solutions that are available, and explore the ways that Google Cloud products have made data more useful and accessible to a workforce. Throughout the course, you’ll be presented with graded knowledge assessments. You must pass these assessments to receive course credit. OK, let's get started!

## The Value of Data

In this section of the course, you'll explore the important role that data plays in an organization's digital transformation. 

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/267/video/512684)

* [YouTube: Introduction](https://www.youtube.com/watch?v=wN47Z9ls4FA)

The word “data” is used a lot in today’s business world. There’s a good reason for that, because capturing, managing, and using data is central to redefining customer experiences and creating new value in almost every industry. In this section of the course, you’ll explore how data generates business insights and drives decision making, basic data management concepts, like databases, data warehouses, and data lakes, how organizations can create value by using their current data, collecting new data, and sourcing data externally, how the cloud unlocks business value from all types of data, including structured data and previously untapped unstructured data, the data value chain, from the initial creation of data through data activation, and the importance that data governance plays in a successful data journey.

### Video - [How data creates value](https://www.cloudskillsboost.google/course_templates/267/video/512685)

* [YouTube: How data creates value](https://www.youtube.com/watch?v=z6UKQbNLz-M)

Data is an essential ingredient for driving innovation and differentiation, and is the key to unlocking value from artificial intelligence. Data powers AI-driven business insights, helps companies make better real-time decisions, and is the basis for how companies build and run their applications. We’re generating more data every day, and the complexity and speed of data arrival are changing the business environment. However, the most valuable insights no longer come just from sales, inventory, and personnel data. They are often hidden across unstructured data points from a myriad of sources and systems. Extracting the insights requires the right blend of tools, skills, and strategy. Some data is easy to capture like financial data, because it can be found in databases and spreadsheets. But other data might not be as easy for example, how your customers engage with you across social media platforms. And after you capture data, how do you store it so that you can gain insights from it? With machine learning, or ML, and artificial intelligence, or AI, organizations can generate insights from data, both past and present, and can also perceive, predict, recommend, and categorize data in new ways. For example, ML lets online retailers who use smart analytics tools to ingest real-time customer behavior data while they surface the best suggestions for particular users. So with every click that the user makes, their website experience becomes increasingly personalized. However, some organizations struggle to remove the barriers that sit between them and their data. According to a report by Accenture titled “Closing the data value gap,” 68% of organizations say they are still unable to realize tangible and measurable value from data. Organizations that want to adapt must determine how to close the gap and support value generation. An intelligent data cloud is the key to unlocking more business value.

### Video - [Unlocking business value from data](https://www.cloudskillsboost.google/course_templates/267/video/512686)

* [YouTube: Unlocking business value from data](https://www.youtube.com/watch?v=7dn8R1i5qy4)

Unlocking the value of data is central to digital transformation. To generate insights, you might need to combine different types of data. However, not all data is created and organized the same way. Data can be categorized into three main types: structured, semi-structured, and unstructured. Structured data is highly organized and well-defined. It’s typically stored in a table with relationships between the different rows and columns, like in a spreadsheet or database. Because structured data is organized in this way, it is easy to analyze. For example, it’s common for organizations to use structured data in customer relationship management tools, or CRMs, as they follow customer behavior patterns and trends. Semi-structured data falls somewhere in between structured and unstructured data. It’s organized into a hierarchy, but without full differentiation or any particular ordering. Examples include emails, HTML, JSON, and XML files. Although this data type doesn’t have a formal structure, it contains tags or other markers that make it easier to analyze than unstructured data. Unstructured data is information that either doesn’t have a predefined data model or isn’t organized in a predefined manner. Categories include: Text, which is the most common, and is often generated and collected from sources like documents, presentations, or even social media posts. Data files, like images, audio files, and videos. And infrastructure activity and performance data, like log files from servers, networks, and applications or output data from Internet of Things (IoT) sensors. Organizations can use unstructured data in many ways. For example, a marketing team might analyze social media posts to identify sentiment toward a brand. Or customer service teams might train automated chatbots to augment support staff by analyzing language in customer communications and providing interactive responses. But in general, unstructured data has historically been difficult to analyze. According to Harvard Business Review, on average less than 1% of an organization’s unstructured data is analyzed or used at all. Until recently, tools to tap the potential of unstructured data were either unavailable or prohibitively expensive and complex. What makes this statistic even more concerning is that, according to Gartner research, unstructured data represents 80% to 90% of all new enterprise data. This reveals a staggering gap between the data being generated and the value that it's providing. But, cloud technology has changed that. With the right cloud tools, businesses can extract value from unstructured data by using machine learning to discover trends, or even using application programming interfaces, or APIs, to extract structure from the data. An example of an API is Google Cloud’s Vision API, which uses machine learning to detect products within a picture and can then even label the picture to describe its contents. Understanding the different types of data available can help organizations define what’s possible with the data solutions they have. One of the transformative powers of the cloud is how it can unlock value from structured and the previously untapped, unstructured data.

### Video - [Data management concepts](https://www.cloudskillsboost.google/course_templates/267/video/512687)

* [YouTube: Data management concepts](https://www.youtube.com/watch?v=nThMEdgYUBA)

Organizations need a modern approach to enterprise data to manage the vast volumes that are produced. The list of options often includes databases, data warehouses, and data lakes. Let’s explore each of these options starting with databases. A database is an organized collection of data stored in tables and accessed electronically from a computer system. Let’s examine two types of databases: relational and non-relational. A relational database stores and provides access to data points that are related to one another. This means storing information in tables, rows, and columns that have a clearly defined schema that represents the structure or logical configuration of the database. A relational database can establish links—or relationships–between information by joining tables, and structured query language, or SQL, can be used to query and manipulate data. Relational databases are highly consistent, reliable, and best suited for dealing with large amounts of structured data. They’re designed for business data processing and storing the online transactional data needed to support the daily operations of a company. A non-relational database, sometimes known as a NoSQL database, is less structured in format and doesn’t use a tabular format of rows and columns like relational databases. Instead, non-relational databases follow a flexible data model, which makes them ideal for storing data that changes its organization frequently or for applications that handle diverse types of data. This includes when large quantities of complex and diverse data need to be organized, or when the data regularly evolves to meet new business requirements. Choosing the right database depends on the use case. Google Cloud relational database products include Cloud SQL and Spanner, while Bigtable is a non-relational database product. We’ll look at these products in more detail later. Let’s explore another data management concept, the data warehouse. Like a database, a data warehouse is a place to store data. However, while a database is designed to capture data for storage, retrieval, and use, a data warehouse is designed to analyze data. A data warehouse is an enterprise system used for the analysis and reporting of structured and semi-structured data from multiple sources. Think of the data warehouse as the central hub for all business data. Business data might include point-of-sale transactions, marketing automation, or even customer relationship management data. Suited for both ad hoc analysis and custom reporting, a data warehouse can help analyze sales and identify trends, because it can store both current and historical data in one place. This capability can provide a long-range view of data over time, which makes a data warehouse a primary component of business intelligence. BigQuery is Google Cloud's data warehouse offering. We’ll explore BigQuery in more detail later. Although data warehouses handle structured and semi-structured data, they’re not typically the answer for how to handle large amounts of available unstructured data, like images, videos, and documents. Unstructured data, which doesn't conform to a well-defined schema, is often disregarded in traditional analytics. A data lake is a repository designed to ingest, store, explore, process, and analyze any type or volume of raw data, regardless of the source, like operational systems, web sources, social media, or Internet of Things, or IoT. It can store different types of data in its original format; ignoring size limits, and without much pre-processing or adding structure. Having this unprocessed, raw data available for analysis prevents unintentionally contaminating the data or adding bias. It also means that the raw data can be enriched by merging it with other data at the same time. This differs from a data warehouse that contains structured data that has been cleaned and processed, ready for strategic analysis based on predefined business needs. Data lakes often consist of many different products, depending on the nature of the data that is ingested. For example, the best Google Cloud products for storing structured data are Cloud SQL, Spanner, or BigQuery. For semi-structured data, the options include Datastore and Bigtable. And for storing unstructured data, Cloud Storage is an option. Data warehouses and data lakes should be considered complementary instead of competing tools. Although both store data in some capacity, each is optimized for different uses. Traditional data warehouse users are business intelligence analysts who are closer to the business and focus on driving insights from data. These users traditionally use the data to answer questions. Data lake users, and also analysts, include data engineers and data scientists. They’re closer to the raw data with the tools and capabilities to explore, mine, and experiment with the data. These users find answers in the data, but they also find questions. As enterprises are increasingly focused on data-driven decision making, data warehouses and data lakes play a critical role in an organization’s digital transformation journey. Democratization of data lets users gain a deeper understanding of business situations because they have more context than ever before. Today, organizations need a 360-degree real-time view of their businesses to gain a competitive edge.

### Video - [The role of data in digital transformation](https://www.cloudskillsboost.google/course_templates/267/video/512688)

* [YouTube: The role of data in digital transformation](https://www.youtube.com/watch?v=ihGk5vJ3Pcs)

Organizations have access to data like never before. This includes both internal information, called first-party data, and external information, which is usually data about customers and industry, often called second or third-party data. As organizations have digitized their operations, many types of business data have become available, including information about their customers. First-party data is the proprietary customer datasets that a business collects from customer or audience transactions and interactions. These datasets might include information about digital interactions, like the length of time a user spends on a web page. Second-party data often describes first-party data from another organization, such as a partner or other business in their supply chain, that can be easily deployed to augment a company's internal datasets. The organization does not directly own this data, but it’s relevant to their business. Finally, there’s third-party data, which are datasets collected and managed by organizations that don’t directly interact with an organization's customers or business. These datasets might come from government, nonprofit, or academic sources, like weather or public demographic data, or from industry-specific sources like analyst reports or industry benchmarking. Third-party data is often shared or purchased on data marketplaces or exchanges, such as the Google Cloud Marketplace. Using external data can greatly increase the value of data by providing new context and insights. Let’s explore an example of how an airline transformed their business through data. Budget airlines don't provide food as part of their service. Instead, they charge customers for meals if they want them. This solution might seem cost-effective, but it can be difficult to estimate the number of meals required onboard. If the airline overestimates the number of meals needed, they risk wasting food and losing revenue. But if they underestimate, they risk selling out of food; providing poor customer service and losing potential revenue. One budget airline in Asia reimagined how they could solve this problem by using data. They began by identifying factors to help estimate stock, such as the size of the plane and the number of passengers. But they soon discovered that estimates based on these factors were inaccurate. This meant having to think about their data differently by analyzing information such as destination, time of flight, and flight connections before and after each journey. Using this information, they uncovered actionable insights. For example, they learned that flights to and from India required 73% more vegetarian meals. With these new insights, the airline was able to predict the number of meals required more accurately, which in turn provided a more positive customer experience and improved the profitability of their food service. This is just one example of how cloud technology can unlock new value by reimagining data. No matter where you are in your company, you too can use data to solve challenges.

### Video - [The data value chain](https://www.cloudskillsboost.google/course_templates/267/video/512689)

* [YouTube: The data value chain](https://www.youtube.com/watch?v=329d80pdlSQ)

When you think about data processing, it's important to place it within the broader context of the data value chain. Imagine data traveling along an assembly line, like a car in a factory. The assembly line progressively adds parts and value to an object that moves along it. Raw data at the beginning of the line is eventually transformed into actions that humans or machines take. Let’s examine the steps in this data value chain. Data genesis is the initial creation of a unit of data; this could be a click on a website, the swipe of a card, a sensor recording from an IoT device, or countless other examples. It’s the raw material that will eventually be turned into an insight ready for action. Data collection brings that initial unit of data to the assembly line through ingestion. The basic function of ingestion is to extract data from the system in which it’s hosted and bring it to a new system. It can have dramatically different requirements based on the volume, velocity, and variety of the raw data that’s required for a given analysis, and how fast the data needs to be analyzed. Data processing is where the collected raw data is transformed into a form that’s ready to derive insights from. The data will likely need to be adjusted, for example, by merging different datasets together. It can be a single-stage operation, or it can be a complex tree of cascading procedures. In our manufacturing process analogy, this phase is where raw materials take the shape of the pre-assembly parts of a manufactured product. Data storage is where the data lands, can be found, and is ready for analysis and action. As with real-world manufacturing, where storage options vary depending on the type of product that is processed, different types of data can be stored in different ways. For example, NoSQL is available for fast reads and writes, data warehousing for fast access to analysis, and object storage for unstructured data. There are also customized options of these standard stores. Data analysis provides direction for business-oriented action. To continue with our manufacturing line analogy, in this stage, inputs from the data processing stage are assembled into a final product. And finally, the last step in the data value chain is data activation. When an analysis is produced, it needs to be pushed to the relevant business procedures and decision makers so that action can be taken and the value chain completed. The most common points of activation are applications that make automated decisions and business intelligence dashboards that guide humans toward better, more informed decisions. In our manufacturing line example, this is the step where a fully produced product is put to its intended use. There is no one way to assemble a data value chain, as there’s no one way to create a real-world manufacturing line. Similarly, as technologies progress, new inputs become available, your workforce evolves, or the desired output changes, the optimal value chain will also change. However, at its core, the value chain principles hold. We want to use raw data to perform actions that benefit the business.

### Video - [Data governance](https://www.cloudskillsboost.google/course_templates/267/video/512690)

* [YouTube: Data governance](https://www.youtube.com/watch?v=m27rFmBegAA)

In the last decade, the amount of data produced has increased exponentially, and the cloud has made it easier to collect, store, and analyze it at a lower cost. Organizations are now challenged to democratize and embed data in every decision, while they also ensure that it’s secure and protected from unauthorized use. An effective data governance program can help implement data directives to achieve this. But what exactly is data governance? Data governance means setting internal standards—data policies—that apply to how data is gathered, stored, processed, and disposed of. It governs who can access certain data and what data is under governance. It also involves complying with external standards set by industry associations, government agencies, and other stakeholders. Data governance focuses on making the data available to all stakeholders across the full lifecycle of the data, in a form that they can readily access and use, in a manner that generates the desired business outcomes through insights and analysis, and if relevant, in a way that conforms to regulatory standards and compliance needs. Data governance brings several benefits. It makes data more valuable. Data governance implements processes to ensure high quality data, and provides a platform that makes it easier to share data securely with stakeholders across the organization. It helps users make better, more timely decisions. Through data governance, users throughout an organization get the data they need to reach and service customers, design and improve products and services, and seize opportunities for new revenues. By democratizing data, organizations can embed data in all decision making. It improves cost controls. Data helps organizations manage resources and operate more effectively. Because they can eliminate data duplication caused by information silos, they don’t overbuy—and have to maintain—expensive hardware. It enhances regulatory compliance. An increasingly complex regulatory climate has made it even more important for organizations to establish rigorous data governance practices. They avoid risks associated with noncompliance and proactively anticipate new regulations. It helps earn greater trust from customers and suppliers. By being in auditable compliance with both internal and external data policies, organizations gain the trust of customers and partners. It helps manage risk. With robust data governance, organizations can reduce concerns about exposure of sensitive data to individuals or systems who lack proper authorization, security breaches from malicious outsiders, or even insiders who access data they don’t have the right to see. It allows more personnel access to more data. Strong data governance provides confidence that the right personnel get access to the right data, and that this democratization of data does not negatively impact the organization. It's possible that organizations without an effective data governance program will suffer from compliance violations. This can lead to fines, poor data quality–which generates lower quality insights that impact business decisions, challenges in finding data–which results in delayed analysis and missed business opportunities, and poorly trained data models for AI–which reduces the model accuracy and benefits of using AI. Every organization needs data governance. As businesses throughout all industries progress on their digital transformation journeys, data has quickly become the most valuable asset they possess.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/267/quizzes/512691)

## Google Cloud Data Management Solutions

In this section of the course, you'll explore Google Cloud data management products and solutions, and learn how they apply to different business use cases.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/267/video/512692)

* [YouTube: Introduction](https://www.youtube.com/watch?v=U2kvXRnvyxY)

Data plays such an integral role in an organization's operations. For this reason, it’s crucial to have an effective way of storing and managing it. Google Cloud offers a wide range of data management products and solutions, each applicable to different business use cases. In this section of the course, you’ll explore: Google Cloud data management options and the differences between them. The different storage classes available with Cloud Storage. How to choose the right storage product to meet the needs of your organization. And ways an organization can migrate and/or modernize their current database in the cloud.

### Video - [Unstructured data storage](https://www.cloudskillsboost.google/course_templates/267/video/512693)

* [YouTube: Unstructured data storage](https://www.youtube.com/watch?v=uecOHbCn1Kk)

Every application needs to store data, like media to be streamed or even sensor data from devices, and different applications and workloads require different storage solutions. Google Cloud offers several core storage products. This list includes Cloud Storage, Cloud SQL, Spanner, BigQuery, Firestore, and Bigtable. Depending on your use case, you might use one or several of these services to do the job. Let’s begin with Cloud Storage, which is a service that offers developers and IT organizations durable and highly available object storage. But what is object storage? Object storage is a computer data storage architecture that manages data as “objects” instead of as file storage, which is a file and folder hierarchy, or as block storage, which is chunks of a disk. These objects are stored in a packaged format that contains the binary form of the actual data, and relevant associated metadata–such as creation date, author, resource type, and permissions–and a globally unique identifier. These unique keys are in the form of URLs, which means object storage interacts well with web technologies. Data commonly stored as objects include video, pictures, and audio recordings. This type of data is referred to as unstructured, which means that it doesn’t have a predefined data model or isn’t organized in a predefined manner, as you might find in a structured database format. Cloud Storage lets customers store any amount of data and retrieve it as often as needed. It’s a fully managed, scalable service that has a wide variety of uses, such as serving website content, storing data for archival and disaster recovery, and distributing large data objects to end users through direct download. There are four primary storage classes in Cloud Storage. The first is Standard storage. Standard Storage is considered best for frequently accessed, or “hot,” data. It’s also great for data that’s stored for only brief periods of time. The second storage class is Nearline storage. This option is best for storing infrequently accessed data, like reading or modifying data on average once a month or less. Examples might include data backups, long-tail multimedia content, or data archiving. The third storage class is Coldline storage. This is also a low-cost option for storing infrequently accessed data. However, as compared to Nearline storage, Coldline storage is meant for reading or modifying data, at most, once every 90 days. And the fourth storage class is Archive storage. This is the lowest-cost option, used ideally for data archiving, online backup, and disaster recovery. It’s the best choice for data that you plan to access less than once a year, because it has higher costs for data access and operations and a 365-day minimum storage duration. Although each of these four classes have differences, it’s worth noting there are several characteristics that apply across all of these storage classes, which include: unlimited storage with no minimum object size requirement, worldwide accessibility and locations, low latency and high durability, a uniform experience–which extends to security, tools, and APIs, and geo-redundancy if data is stored in a multi-region or dual-region. This means placing physical servers in geographically diverse data centers to protect against catastrophic events and natural disasters, and load-balancing traffic for optimal performance. Cloud Storage also provides a feature called Autoclass, which automatically transitions objects to appropriate storage classes based on each object's access pattern. The feature moves data that is not accessed to colder storage classes to reduce storage cost and moves data that is accessed to Standard storage to optimize future accesses. Autoclass simplifies and automates cost saving for your Cloud Storage data.

### Video - [Structured data storage](https://www.cloudskillsboost.google/course_templates/267/video/512694)

* [YouTube: Structured data storage](https://www.youtube.com/watch?v=bv3kk26MKbo)

In the previous lesson, you saw how Cloud Storage is used to store unstructured data. Now let’s explore some Google Cloud data storage products that are suited for storing structured data. Structured data consists of numbers and values that are organized in a predefined format that’s easily searchable in a relational database. Earlier in the course, we mentioned that a relational database stores information in tables, rows, and columns that have a clearly defined schema that represents the structure or logical configuration of the database. Cloud SQL offers fully managed relational databases, including MySQL, PostgreSQL, and SQL Server as a service. It’s designed to transfer mundane—but necessary and often time-consuming—tasks to Google, like applying patches and updates, managing backups, and configuring replications, so you can focus on building great applications. Trusted by thousands of the largest enterprises around the world, organizations that use Cloud SQL obtain various benefits. It doesn't require any software installation or maintenance. It supports managed backups, so backed-up data is securely stored and accessible if a restore is required. It encrypts customer data when on Google’s internal networks and when stored in database tables, temporary files, and backups. And it includes a network firewall, which controls network access to each database instance. Spanner is a fully managed, mission-critical, relational database service that scales horizontally to handle unexpected business spikes. Battle tested by Google’s own mission-critical applications and services, Spanner is the service that powers Google’s multi-billion dollar business. Spanner is especially suited for applications that require a SQL relational database management system with joins and secondary indexes, built-in high availability, which provides data redundancy to reduce downtime when a zone or instance becomes unavailable (the goal is to prevent a single point of failure), strong global consistency, which ensures that all locations where data is stored are updated to the most recent data version quickly, and high numbers of input and output operations per second (tens of thousands of reads and writes per second or more). Both Cloud SQL and Spanner are fully managed database services, but how do they differ? Cloud SQL is a fully managed relational database service for MySQL, PostgreSQL, and SQL Server with greater than 99.95% availability. Database Migration Service (DMS) makes it easy to migrate your production databases to Cloud SQL with minimal downtime. And then there is Spanner, which is a fully managed relational database with unlimited scale, strong consistency, and up to 99.999% availability with zero downtime for planned maintenance and schema changes. This globally distributed, ACID-compliant cloud database automatically handles replicas, sharding, and transaction processing, so you can quickly scale to meet any usage pattern and ensure success of products. When considering which option is best for your business, consider this: if you have outgrown any relational database, are sharding your databases for throughput high performance, need transactional consistency, global data, and strong consistency, or just want to consolidate your database, consider using Spanner. If you don’t need horizontal scaling or a globally available system, Cloud SQL is a cost-effective solution. The final structured data storage solution that we’ll explore is BigQuery. BigQuery is a fully-managed data warehouse. As we’ve already learned, a data warehouse is a large store that contains petabytes of data gathered from a wide range of sources within an organization and is used to guide management decisions. Because it’s fully managed, BigQuery takes care of the underlying infrastructure, so users can focus on using SQL queries to answer business questions, without having to worry about deployment, scalability, and security. BigQuery provides two services in one: storage and analytics. It’s a place to store petabytes of data. For reference, one petabyte is equivalent to 11,000 movies at 4k quality. BigQuery is also a place to analyze data, with built-in features like machine learning, geospatial analysis, and business intelligence. Data in BigQuery is encrypted at rest by default without any action required from a user. Encryption at rest is encryption used to protect data that’s stored on a disk, including solid-state drives, or backup media. BigQuery provides seamless integration with the existing partner ecosystem. Businesses can tap into our ecosystem of system integrators and data integration partners to help enhance analytics and reporting. These integrations mean that BigQuery lets organizations make the most of existing investments in business intelligence, data ingestion, and data integration tools. Industry research shows that 90% of organizations have a multicloud strategy, which adds complexity to data integration, orchestration, and governance. BigQuery works in a multicloud environment, which lets data teams eradicate data silos by using BigQuery to securely and cost effectively analyze data across multiple cloud providers. BigQuery also has built-in machine learning features so that ML models can be written directly in BigQuery by using SQL. And if other professional tools—such as Vertex AI from Google Cloud—are used to train ML models, datasets can be exported from BigQuery directly into Vertex AI for a seamless integration across the data-to-AI lifecycle.

### Video - [Semi-structured data storage](https://www.cloudskillsboost.google/course_templates/267/video/512695)

* [YouTube: Semi-structured data storage](https://www.youtube.com/watch?v=HS9Q2savCRk)

Semi-structured data contains elements of both structured and unstructured data. It does have some defining or consistent characteristics, but generally doesn’t follow a structure as rigid as a relational database. Semi-structured data is easier to organize because it usually contains some organizational properties, such as tags or metadata. An example of semi-structured data is an email message. While the actual content of the email is unstructured, it does contain structured data such as the name and email address of the sender and recipient, the time sent, and so on. Google Cloud offers two semi-structured data storage products, Firestore and Bigtable. Firestore is a flexible, horizontally scalable, NoSQL cloud database for storing and syncing data in real-time. Firestore can be directly accessed by mobile and web applications. Firestore performs data storage in the form of documents, with the documents being stored in collections. Documents support a wide variety of data types, such as nested objects, numbers, and strings. One of Firestore’s main features is automatic scaling. It’s been designed to scale automatically depending on user demand, but retains the same level of performance irrespective of database size. Firestore also provides offline usage through a comprehensive database on users’ devices. Offline data access ensures that applications run without interruption, even if the user gets disconnected from the internet. And then there’s Bigtable, Google's NoSQL big data database service. It's the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail. Bigtable is designed to handle large workloads at consistent low latency, which means Bigtable responds to requests quickly, and high throughput, which means it can send and receive large amounts of data. For this reason, it's a great choice for both operational and analytical applications, including Internet of Things, user analytics, and financial data analysis. When deciding on a storage option, you might choose Bigtable if you’re working with more than 1 TB of semi-structured or structured data, data is fast with high throughput, or it’s rapidly changing, you’re working with NoSQL data, data is a time-series or has natural ordering, you’re working with big data and running batch or real-time processing on the data, or you’re running machine learning algorithms on the data.

### Video - [Choosing the right storage product](https://www.cloudskillsboost.google/course_templates/267/video/512696)

* [YouTube: Choosing the right storage product](https://www.youtube.com/watch?v=uLnawxRHfrg)

So, you’ve learned about the different storage options that Google Cloud offers, but in what scenarios should you use each one? Ultimately, it’s a combination of the data type that needs to be stored and the business need. If data is unstructured, then Cloud Storage is the most appropriate option. You have to decide a storage class: Standard, Nearline, Coldline, or Archive. Or whether to let the Autoclass feature decide that for you. If data is structured or semi-structured, choosing a storage product will depend on whether workloads are transactional or analytical. Transactional workloads stem from online transaction processing, or OLTP, systems, which are used when fast data inserts and updates are required to build row-based records. An example of this is point-of-sale transaction records. Then there are analytical workloads, which stem from online analytical processing, or OLAP systems, which are used when entire datasets need to be read. They often require complex queries, for example, aggregations. An example here would be analyzing sales history to see trends and aggregated views. After you determine if the workloads are transactional or analytical, you must determine whether the data will be accessed by using SQL. So, if your data is transactional and you need to access it by using SQL, then Cloud SQL and Spanner are two options. Cloud SQL works best for local to regional scalability, and Spanner is best to scale a database globally. If the transactional data will be accessed without SQL, Firestore might be the best option. Firestore is a transactional NoSQL, document-oriented database. If you have analytical workloads that require SQL commands, BigQuery might be the best option. BigQuery, Google’s data warehouse solution, lets you analyze petabyte-scale datasets. Alternatively, Bigtable provides a scalable NoSQL solution for analytical workloads. It’s best for real-time, high-throughput applications that require only millisecond latency.

### Video - [Database migration and modernization](https://www.cloudskillsboost.google/course_templates/267/video/512697)

* [YouTube: Database migration and modernization](https://www.youtube.com/watch?v=dQHhvW_kJDE)

Running modern applications on legacy, on-premises databases requires overcoming expensive, time-consuming challenges around latency, throughput, availability, and scaling. With database modernization, organizations can move data from traditional databases to fully managed or modern databases with relative ease. There are different ways that an organization can migrate or modernize their current database in the cloud. The most straightforward method is a lift and shift platform migration. This is where databases are migrated from on-premises and private cloud environments to the same type of database hosted by a public cloud provider, such as Google Cloud. Although this solution makes the database more difficult to modernize, it does bring with it the benefits of minimal upheaval, and having data and infrastructure managed by the cloud provider. Alternatively, a managed database migration allows the migration of databases from SQL Server, MySQL, PostgreSQL, and others to a fully managed Google Cloud database. Although this migration requires careful planning and might cause slight upheaval, a fully managed solution lets you focus on higher priority work that really adds value to your organization. Google Cloud’s Database Migration Service (DMS) can easily migrate your databases to Google Cloud, or Datastream can be used to synchronize data across databases, storage systems, and applications. Let’s look at a real life use case. With 18 fulfillment centers, 38 delivery centers, and a catalog of more than 22 million items, the online retailer Wayfair needed a way to quickly move from their on-premises data centers, which ran on SQL Server, to Google Cloud. This had to be achieved without inconveniencing their team of over 3,000 engineers, their tens of millions of customers, or their 16,000 supplier partners. So, the goal was to lift and shift their workloads as quickly as possible with minimal changes, and then use cloud databases to modernize those workloads. Wayfair chose Google Cloud because of the clear path for shifting workloads to the cloud by using Cloud SQL for SQL Server. Google Cloud provided the flexibility to be deliberate about which engine and product to run Wayfair’s systems on going forward. They liked how they could run SQL Server on virtual machines (VMs), for example, but could also benefit from database offerings like Cloud SQL and Spanner. Now that migration is complete, they also use Google Kubernetes Engine (GKE) and Compute Engine VMs to host the services built by the Google Cloud team. They also use Pub/Sub and Dataflow for sending operational data to their analytical store in BigQuery.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/267/quizzes/512698)

## Making Data Useful and Accessible

In this section of the course, you'll examine how smart analytics, business intelligence tools, and streaming analytics can add value in different business use cases.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/267/video/512699)

* [YouTube: Introduction](https://www.youtube.com/watch?v=dLc8s_ol0Qk)

It’s not always easy for organizations to make smart business decisions based on the data they’ve collected or produced. And too often there can be blockers in place that make analyzing it difficult for part, or all, of a workforce. With Google Cloud, that doesn’t need to be the case. In the final module of this course, you’ll explore: How Looker makes it easy for a workforce to access the data they need, when they need it. How streaming analytics in real time can make data more useful. Two Google Cloud products that modernize data pipelines: Pub/Sub and Dataflow.

### Video - [Business intelligence and insights using Looker](https://www.cloudskillsboost.google/course_templates/267/video/512700)

* [YouTube: Business intelligence and insights using Looker](https://www.youtube.com/watch?v=Q26rwSsKdDg)

When data is in a database, a fair amount of effort and expertise might still be required to uncover insights. This goal can be achieved by using a business intelligence solution. However, the challenge that organizations often face is identifying the right business intelligence solution. Some solutions are too complex and not accessible by those outside the data engineering or data analysis teams. This means other teams have to put in requests and wait for answers, which defeats the purpose of gaining real-time insights. Other solutions let everyone in the business perform their own data analysis, but they can only perform their analysis with a selection of the available data. This means that only a few people, or possibly no one, has a full view of the organization’s business data. Looker is a Google Cloud business intelligence (BI) platform designed to help individuals and teams analyze, visualize, and share data. This includes creating interactive dashboards and reports that are easy to understand and share. By having a reliable authority for business data, anyone on a team can explore it, ask and answer their own questions, and create visualizations. This approach empowers organizations to not just uncover insights but also act on them. Looker supports BigQuery, along with more than 60 different SQL databases. Together, BigQuery and Looker provide rich, interactive dashboards and reports without compromising performance, scale, security, or data freshness. Looker is also 100% web-based, which makes it easy to integrate into existing workflows and share with multiple teams at an organization. So how can Looker be used? Let's explore an example. Diamond Resorts, a global leader in hospitality, offers destinations, events, and experiences to help people recharge, connect, and enjoy. They had previously used a mixture of complex Excel workbooks and legacy BI tools to track important metrics. Each business unit operated and ran their own siloed data initiatives. As a result, there were: No common view of business or single authority for common metrics Redundant data engineering efforts, because work was never shared or used across the organization And inconsistent project prioritization, because decisions were driven primarily on intuition as opposed to actual data Also, infrastructure did not meet business requirements with: Executive reporting efforts that took months to complete Data that was duplicated across multiple business units without proper governance Multiple reporting tools and data warehouses throughout the business And infrastructure that didn’t support advanced analytics aspirations Diamond Resorts wanted to create a single common cloud-based architecture that was fully-managed; establishing data governance and enabling the business to be more data-driven, while they set the foundation for advanced analytics efforts. They migrated to the cloud and began using Looker to help improve business agility. This decision let them gain access to real-time insights in less than 3 months. It helped them to navigate COVID changes with important operational metrics such as daily booking and cancellations, while it also provided a 360-degree customer view. And in addition to this, manual reporting for the Yield Management team was decreased by hours each day. The chief information officer said, “Projects that we anticipated coming in future years were suddenly ready to be tackled within weeks.” This is just one example of how an effective business intelligence solution can let businesses transform to better serve their customers.

### Video - [Streaming analytics](https://www.cloudskillsboost.google/course_templates/267/video/512701)

* [YouTube: Streaming analytics](https://www.youtube.com/watch?v=rauVGDGY-y8)

Data traditionally is moved in batches. Batch processing often processes large volumes of data at the same time, with long periods of latency. An example is payroll and billing systems that have to be processed on either a weekly or monthly basis. Although this approach can be efficient to handle large volumes of data, it doesn’t work with time-sensitive data that’s meant to be streamed, because that data can be stale by the time it’s processed. Streaming analytics is the processing and analyzing of data records continuously instead of in batches. Generally, streaming analytics is useful for the types of data sources that send data in small sizes, often in kilobytes, in a continuous flow as the data is generated. This results in the analysis and reporting of events as they happen. Sources of streaming data include equipment sensors, clickstreams, social media feeds, stock market quotes, app activity, and more. Companies use streaming analytics to analyze data in real time and provide insights into a wide range of activities, such as metering, server activity, geolocation of devices, or website clicks. Use cases include: Ecommerce: User clickstreams can be analyzed to optimize the shopping experience with real-time pricing, promotions, and inventory management. Financial services: Account activity can be analyzed to detect abnormal behavior in the data stream and generate a security alert. Investment services: Market changes can be tracked and settings adjusted to customer portfolios based on configured constraints, such as selling when a certain stock value is reached. News media: User click records can be streamed from various news source platforms and the data can then be enriched with demographic information to better serve articles that are relevant to the targeted audience. Utilities: Throughput across a power grid can be monitored and alerts generated or workflows initiated when established thresholds are reached. Google Cloud offers two main streaming analytics products to ingest, process, and analyze event streams in real time, which makes data more useful and accessible from the instant it’s generated. Pub/Sub ingests hundreds of millions of events per second, but Dataflow unifies streaming and batch data analysis and builds cohesive data pipelines. A data pipeline represents a series of actions, or stages, that ingest raw data from different sources and then move that data to a destination for storage and analysis. You'll explore these products in more detail in the next section.

### Video - [Pub/Sub and Dataflow](https://www.cloudskillsboost.google/course_templates/267/video/512702)

* [YouTube: Pub/Sub and Dataflow](https://www.youtube.com/watch?v=3inmKZBpfdQ)

One of the early stages in a data pipeline is data ingestion, which is where large amounts of streaming data are received. Data, however, may not always come from a single, structured database. Instead, the data might stream from a thousand, or even a million, different events that are all happening asynchronously. A common example of this is data from IoT, or Internet of Things, applications. These can include sensors on taxis that send out location data every 30 seconds or temperature sensors around a data center to help optimize heating and cooling. Pub/Sub is a distributed messaging service that can receive messages from various device streams such as gaming events, IoT devices, and application streams. The name is short for Publisher/Subscriber, or publish messages to subscribers. After messages have been captured from the streaming input sources you need a way to pipe that data into a data warehouse for analysis. This is where Dataflow comes in. Dataflow creates a pipeline to process both streaming data and batch data. “Process” in this case refers to the steps to extract, transform, and load data, sometimes referred to as ETL. A popular solution for pipeline design is Apache Beam. It’s an open source, unified programming model to define and execute data processing pipelines, including ETL, batch, and stream processing. Dataflow handles much of the complexity for infrastructure setup and maintenance, and is built on Google’s infrastructure. This product allows for reliable auto scaling to meet data pipeline demands. Dataflow is serverless and fully managed. Serverless computing means that software developers can build and run applications without having to provision or manage the back-end infrastructure. For example, Google Cloud manages infrastructure tasks on behalf of the users, like resource provisioning, performance tuning, and ensuring pipeline reliability. And a fully managed environment is one where software can be deployed, monitored, and managed without needing an operations team. You can create this environment by using automation tools and technologies. Using a serverless and fully managed solution like Dataflow means that you can spend more time analyzing the insights from your datasets and less time provisioning resources to ensure that your pipeline will successfully complete its next cycles.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/267/quizzes/512703)

## Course Summary

The course closes with a summary of the key points covered in each section and next steps to continue learning.

### Video - [Course summary](https://www.cloudskillsboost.google/course_templates/267/video/512704)

* [YouTube: Course summary](https://www.youtube.com/watch?v=TGc9bCCTEvM)

This brings us to the end of the Exploring Data Transformation with Google Cloud course. Let’s do a quick recap. In the first section of the course, The Value of Data, you learned how data generates business insights and drives decision making; basic data management concepts, like databases, data warehouses, and data lakes; how organizations can create value by using their current data, collecting new data, and sourcing data externally; how the cloud unlocks business value from all types of data, including structured data and previously untapped unstructured data; about the data value chain, from the initial creation of data to data activation; and the importance that data governance plays in a successful data journey. In the second section of the course, Google Cloud Data Management Solutions, you learned about Google Cloud data management options and the differences between them; about the different storage classes available with Cloud Storage; how to choose the right storage product to meet the needs of your organization; and ways an organization can migrate and/or modernize their current database in the cloud. Finally, in the third section of the course, Making Data Useful and Accessible, you learned how Looker makes it easy for a workforce to access the data they need, when they need it; how streaming analytics in real time can make data more useful; and about two Google Cloud products that modernize data pipelines, Pub/Sub and Dataflow. Now that you’ve had a comprehensive introduction to data transformation, move on to the next course in the series, Innovating with Google Cloud Artificial Intelligence, where you’ll learn about the fundamentals of artificial intelligence and machine learning, selecting Google Cloud AI solutions, and building and using Google Cloud AI solutions. We’ll see you next time!

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
