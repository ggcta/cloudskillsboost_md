---
id: 964
name: 'The Power of Storytelling: How to Visualize Data in the Cloud'
datePublished: 2024-06-05
topics:
- Dashboard
- Data Management
- Data
type: Course
url: https://www.cloudskillsboost.google/course_templates/964
---

# [The Power of Storytelling: How to Visualize Data in the Cloud](https://www.cloudskillsboost.google/course_templates/964)

**Description:**

This is the fourth of five courses in the Google Cloud Data Analytics Certificate. In this course, you'll focus on developing skills in the five key stages of visualizing data in the cloud: storytelling, planning, exploring data, building visualizations, and sharing data with others. You'll also gain experience using UI/UX skills to wireframe impactful, cloud-native visualizations and work with cloud-native data visualization tools to explore datasets, create reports, and build dashboards that drive decisions and foster collaboration.

**Objectives:**

- Explore the importance of data visualization and storytelling in cloud data analysis.
- Identify and translate stakeholder requests to create dashboards and reports via wireframes and design mockups.
- Explain the role and advantages of cloud-native data visualization tools to access, explore, and analyze data.
- Create a report and dashboard to foster collaboration and decision-making.

## Introduction to the power of storytelling: How to visualize data in the cloud

In this module, you'll advance from the basic principles learned in the first course to the next steps of planning powerful visualizations and key concepts to effectively display, present, and communicate data.

### Video - [Introduction to Course 4](https://www.cloudskillsboost.google/course_templates/964/video/486415)

- [YouTube: Introduction to Course 4](https://www.youtube.com/watch?v=a4uwHKc1PqU)

Welcome! In your work as a cloud data analyst, communicating your findings to all different kinds of stakeholders is an essential part of your job. One key form of communication is data visualizations. Visualizing data in the cloud is critical for you and your data team to best understand the data. Then, you can effectively influence stakeholders so they can make the best business decisions. Let’s scope how you might turn seemingly mundane numbers and text into vivid illustrations that provide a business with engaging inspiration through the power of data visualization. Data visualization is the graphical representation of data using charts, graphs, and other visual formats. These visualizations help users interact with the data, understand what it’s telling them, and make data-driven decisions. Before starting this course, you should have some familiarity with how data is stored and accessed on the cloud, and a basic understanding of how it’s transformed for analysis. It’s also helpful to know some SQL, which is Structured Query Language. This is the most common way that humans talk to relational databases. Having some familiarity with querying foundations is important, too. But before we get too deep into it, let me introduce myself. Hi, I’m C.J. Here at Google, I’m a Cloud Customer Engineer. This means that I act as the technical expert on account teams that sell Google Cloud Data and Analytics services to some of the largest retailers in the world. I’ve worked in the Data and Analytics space my entire career. In my current job, I help companies to understand how Google Cloud can help them to leverage their data assets to derive insights and drive business outcomes. As a cloud data analyst, you'll work with a variety of tools that simplify the process of creating data visualizations. But don’t worry! No artistic background is necessary. The tools you’ll learn have a variety of great features designed to help you communicate your data insights effectively. So, we’ll start this section of the course with data storytelling and how to captivate your audience using insights from the cloud. Then, we’ll get into the basics of user experience, or UX, and user interface, or UI, design to create compelling communication. Next, you’ll explore how to translate a data story and create a visualization plan that meets business needs. You’ll start by learning about the implications of data types on design. For example, we’ll cover how data types inform a data analyst’s visual representation choice. You’ll then learn how to examine common design patterns used in business intelligence dashboards. Then, you’ll focus on ways to work with stakeholders to translate stakeholder requests into visualizations that get results. You’ll also apply what you’ve learned about communicating with visualizations by taking your first steps toward accessing, exploring, and reporting on data in the cloud. To accomplish this, you’ll discover how to access a dataset for visualization and explore that data. Then, you’ll learn about dimensions and measures, and model data for visualization, and work with Looker, a visualization tool, to build a simple dashboard. Next up, you’ll work with large datasets and use business data visualization tools that can work on a large scale, integrated security features with robust functionality, that fits within the IT infrastructure. This involves strategies for fostering a data-driven culture among organizations of all sizes, and keeping data organized and secure. Finally, you’ll step into the role of a developer, who works with data modeling languages and advanced tools. You’ll explore the developer environment, model data with a data modeling language, learn tools to write dashboards as code, and use a data modeling language to address complex business needs. Learning how to visualize data in a way that’s helpful and comfortable for both technical and non-technical users is a foundational step for your role as a cloud data professional. Providing visual data that everyone can understand also helps to improve data literacy across your organization. Impressive, right? Ok, let’s get started!

### Document - [Course 4 overview](https://www.cloudskillsboost.google/course_templates/964/documents/486416)

### Video - [C.J.: A day in the life of a data analytics customer engineer](https://www.cloudskillsboost.google/course_templates/964/video/486417)

- [YouTube: C.J.: A day in the life of a data analytics customer engineer](https://www.youtube.com/watch?v=faOo1kEl2RY)

Hi, I’m C.J.! Here, at Google, I’m a Cloud Customer Engineer. This means that I am part of the technical arm of the sales team. I help Google Cloud customers understand how our data and analytics products work, and more importantly, how they can work for them to deliver value for their specific use cases. My primary job responsibilities include building and delivering customer demos, leading proof of concept engagements, hosting technical information sessions, and collaborating with our product teams to relay and interpret customer feedback. This feedback loop is critical in shaping the evolution of our products and delivering a better experience and more value to our customers. A typical day for me may include reading up on and trying out new Cloud products or features. I may also meet with customers to share information or learn more about challenges they are trying to solve. And then, I might collaborate with colleagues to design and build a new demo. This is all in addition to internal tasks I take on to help grow the organization in various ways. This role is very dynamic and that’s one of the reasons I really enjoy it. In my role, I talk a lot about the value of data and the potential value that some of these tools can unlock. At some point, you’ve got to stop talking and take action. I recently had this experience with a customer. After several conversations and a couple of demos, they said, “That’s great, but what can you show me with my data?” I replied, “Give me 30 days.” During those 30 days, I used the techniques and tools discussed in this course to explore and model their data and bring to life several stories, in the form of dashboards, that were hidden in the disparate data sources. The customer was amazed at seeing their data presented in such novel ways. It’s those types of reactions that motivate me to do this work. What’s your motivation? I don’t know everything, but I’ve learned a lot in my career, so for you aspiring cloud data analysts, here are a few pieces of advice. Break tasks into small, manageable pieces. You will undoubtedly be assigned a task that feels daunting at some point in your career. By breaking large tasks into smaller pieces, you ensure you can deliver with confidence. Set realistic expectations for delivery. This ultimately helps both you and your stakeholders be successful. Work iteratively. Let the first draft be just that, a first draft. Get something out there, get feedback, and improve on it. Always be learning. In this field, change is constant. Keeping up with advancements in tools and best practices is key to staying relevant. Know your audience. How much you know is usually less important than how effectively you can communicate it.

### Document - [Helpful resources and tips](https://www.cloudskillsboost.google/course_templates/964/documents/486418)

### Document - [Lab technical tips](https://www.cloudskillsboost.google/course_templates/964/documents/486419)

### Document - [Explore your Course 4 scenario: TheLook eCommerce](https://www.cloudskillsboost.google/course_templates/964/documents/486420)

### Video - [Welcome to module 1](https://www.cloudskillsboost.google/course_templates/964/video/486421)

- [YouTube: Welcome to module 1](https://www.youtube.com/watch?v=Ju1HjaQ1wFE)

Imagine a cloud data analyst that is rising in the ranks within their data team. They’ve just received an exciting opportunity to grow their skills in storytelling with data. Let’s turn to the first page! In this section of the course, you’re going to discover some great strategies to become a storyteller with cloud data. Along the way, you’ll encounter the basic principles of UX and UI design and how those concepts influence your role as a cloud data analyst. This might include creating a dashboard your stakeholders will love interacting with, or designing a simple chart to convey a complex idea. After that, you’ll explore the foundations of visualization layout, organization, color, usability, and accessibility, along with how each of these elements combine to create effective design. Finally, you’ll identify how to communicate through design for both technical and interpersonal concepts to convey interesting and valuable data stories to any audience. All right, let’s begin!

### Document - [Why visualization matters](https://www.cloudskillsboost.google/course_templates/964/documents/486422)

### Video - [Ways that cloud data enhances storytelling](https://www.cloudskillsboost.google/course_templates/964/video/486423)

- [YouTube: Ways that cloud data enhances storytelling](https://www.youtube.com/watch?v=-fC7Ca4sLKc)

A great story needs captivating characters and an intriguing setting. With fiction, we can use our imaginations and experiences to create a thought-provoking message, but to tell a data story we need, well, data! Using cloud data to craft your narrative provides you with more options than working with local data alone. Let’s find out how! Before the introduction of cloud data and cloud data analysis tools, data analysts would only have the ability to work with data that was locally available. Meaning, they would have to collect, track, and manage everything they needed to do their analysis. But today, centralized, organization-wide cloud data warehouses make it possible to capture the entire context, rather than individual data sources in isolation. One of the biggest benefits of working with data in the cloud is access to a greater variety of data sources, including external and public datasets, along with application data, like Salesforce and Google Analytics. Data professionals can harness all of this data to add depth to their analysis and enhance data storytelling. Let’s consider an example with a clothing retailer. Their sale and inventory for each location is kept in a database on-site. Additionally, the marketing team enters all their campaign data locally in Google Sheets. Because the databases are not connected, there’s no way for one store to know the inventory of another, or for the marketing team to learn what brands are selling or not selling. These various and distributed storage locations are called data silos. A data silo is a collection of data held by one group at a central location. Data teams have used data silos as their traditional approach to storing local data. Because data silos store data locally, silos limit access to data organization wide. As a result, data silos make robust data storytelling and collaborating across teams challenging. Plus, data silos can contribute to a lack of data integrity, because they tend to get out of sync and can cause widespread discrepancies. You can think of these as gaps or inconsistencies in your data story’s plot. Now, with cloud-based data analysis, data professionals across the globe can pull information from multiple data sources and combine them into a single, cohesive report or visual that conveys a clear data message. This helps teams craft fascinating data narratives and understand data trends in the context of the broader organization. Returning to the clothing retailer example, the clothing retailer’s cloud data team can use cloud tools to connect each store's database. Then the data team can access all data across the organization. So, when marketing needs to know which shirts are selling in each store and which one’s aren’t, they can use a well-designed report or dashboard that showcases the data for informed decision-making. And when the purchasing department needs to know which brands to buy and which should be discontinued, they can use the visualized data to spot trends and make well-reasoned choices. Meanwhile, the clothing retailer’s store managers are no longer limited to their own sales and inventory data. These managers now can collaborate with other store managers to share data between stores, and foster growth across all of their locations. Say you’re a cloud data analyst working for the clothing retailer. You can take your analysis a step further by connecting to external data sources, like publicly available datasets, social media, third-party tools, and lots more. This flexibility enables you to create even more context, and build even more detailed and expressive stories. For example, your cloud data team can connect to an external database that has information about clothing trends from around the world. This provides context for whether the retailer should spend more time developing or marketing their new collection of sweatshirts, or if they should focus on their existing line of hats instead. The clothing retailer can also utilize the shared data resources to identify opportunities for different store locations to provide better service to their customers. For example, you might find that stores in a specific area aren't benefitting from a promotion because the primary language in that community is different from the marketing language. Because of your analysis, the clothing retailer will be able to better identify how to reach communities that weren’t accessible before by offering marketing promotions in the locally-used languages. Another awesome cloud data feature is running Google Ads reports. For example, you could show your retail client the regions where their ads are clicked the most, along with that region’s historical sales data. This cloud data gives the retailer immediate feedback about which locations are most likely to respond to their advertising efforts. There are so many ways to add value to projects as a cloud data professional! When you incorporate data from multiple sources into your data storytelling, you provide effective and essential context, and thought-provoking data insights. The different teams you’ll work with, like sales and marketing, will thank you!

### Video - [Planning phases of the data visualization workflow](https://www.cloudskillsboost.google/course_templates/964/video/486424)

- [YouTube: Planning phases of the data visualization workflow](https://www.youtube.com/watch?v=AiuGLvfInh8)

As a cloud data analyst, you know that it’s important to only build a data visualization after studying the data and developing a clear narrative plan. After all, planning data visualizations is fun! And creating data visualizations without a narrative plan would be like setting out on a road trip without having directions to your destination, or packing for that trip without knowing what you’ll need when you get there. In this video, you'll learn all about the essential planning phases of the data visualization workflow. You’ll discover unique cloud features that support digital consumption, the user experience, and data governance and security. This understanding will help you develop effective visualizations. All right, let’s begin with digital consumption. First and foremost, digital consumption is all about understanding the medium through which the cloud data team will interact with the data. Next, it’s important to know your audience, who you will share your data visualizations with. But when it comes to digital mediums, you also need to consider how your audience will interact with the data and what they expect from their interactions. As a cloud data analyst, your audience uses information from many places. It’s a digital world! And users expect the information to be current and interactive. They also probably want to filter the data to only display the content that’s important to them. For example, if you build a dashboard for a large company with multiple locations, the team in each location will be most interested in sales insights from their region. Meanwhile, the product team at the central office will need to compare product sales across all locations. Digital users expect their user experience to be intuitive, easy to use with little or no instruction or explanation. So, your goal is to create visualizations that help digital users quickly and instinctively understand the meaning and purpose of what they’re interacting with. An intuitive user experience, along with all of the other key principles of user experience design, help people get comfortable with using digital tools. Finally, utilizing effective data governance and security policies are critical when working with data over the internet. The very nature of pulling information from the internet means opening up the possibility of data breaches, viruses, and malware. So, before building a visualization, it’s critical to have a strategy for keeping your data safe. Let’s return to the example where you’re a cloud data analyst for a clothing retailer. Each store’s sales data includes the names and contact information of the sales staff. This is considered PII, or personally identifiable information, and must be handled with exceptional care. So, when you design a visualization that displays this sales data, you’d want to be sure to include only the numbers, and not any personal data. Additionally, it’s essential to control who has access to information. Maybe an individual store location’s team supervisor has privileges to view the most granular information, but team members using the same dashboard would only have access to summary-level data. You want to make sure that the dashboard displays what its audience needs to know, without including the parts meant only for a few. All right, we’ve now reached our destination, the end of this video! And because we had a plan for exactly where we were going, we reached our destination without any hitches. Keep in mind you want to apply the same principles when designing data visualizations in the cloud. And when you do, you’ll be on the road to success!

### Video - [Dashboard design for effective communication](https://www.cloudskillsboost.google/course_templates/964/video/486425)

- [YouTube: Dashboard design for effective communication](https://www.youtube.com/watch?v=sc9KVnSz0RE)

As you begin to build dashboards, you’ll use a variety of technical skills and tools. But it’s important to keep in mind that the purpose of dashboards isn’t the technology involved, rather it’s the communication. What do I mean by that? You’re going to use all sorts of technical tools, programs, codes, and markup languages to show the data. But your goal with dashboards isn’t to show off your technical skills. Your goal is to communicate the story the data has to tell. To do that, you will end up communicating a couple of different types of concepts. The first is the technical information you convey. At the forefront, dashboards communicate a very specific data story. Think of the different sections of the dashboard —or the different visualizations presented— as chapters in a book. Each section has something to say, but only by putting the pieces together can your dashboard tell the full story. It does this by building on what has come before and providing context for what comes next. Also, each piece of the story has to advance the narrative in some way, or it’s just clutter. Another important aspect is that each insight is consistently understood by all users. In literature, people usually have different reactions to the same story. But in data analysis, everyone must understand the data story in the same way. Next, pay attention to the size of your visuals. Making one graph bigger than others directs the user’s attention to the larger graph. You could do this for visualizations that are more important than the others. It’s also good to keep in mind that you’re not just communicating technical information. There are many other ways that your design communicates non-technical information to your users. For example, the words and tone you use are important. More complex terms communicate a more formal or professional environment, whereas a casual tone sets a more relaxed, informal feel. Likewise, your graphics can range between simple, to sophisticated, to playful depending on the tone you’re trying to set. Also, be sure to align your visuals with your organization’s brand. This means using your design to support the organizational culture and environment through official colors, tone, and graphics. This helps your dashboard become a cohesive part of the corporate tool set. Finally, good design communicates your level of professionalism. An effective dashboard demonstrates all the hard work you invested in ensuring that users can quickly gain the insights they need! Remember that the purpose of dashboards isn’t the technology involved, rather it’s the communication. This will better prepare you to effectively communicate with your users, and create visualizations they’ll understand. Designing your dashboards to look professional and be easy to navigate is important, but don’t forget that it also communicates so much more. Harness that knowledge to become an even more effective and communicative data analyst.

### Document - [Dashboard strategies for answering business questions](https://www.cloudskillsboost.google/course_templates/964/documents/486426)

### Quiz - [Test your knowledge: Become a storyteller with cloud data](https://www.cloudskillsboost.google/course_templates/964/quizzes/486427)

### Video - [The importance of UX/UI design](https://www.cloudskillsboost.google/course_templates/964/video/486428)

- [YouTube: The importance of UX/UI design](https://www.youtube.com/watch?v=fSeqGutxcS4)

The professional world is full of people who specialize in certain jobs or have unique talents. Teachers know all about educating others, musicians read music and produce beautiful sounds on their instruments, and coders create applications for people to interact with and perform activities. It would be a very different situation if we all had to learn how to teach, play the trumpet, and code our own apps! It’s good to know that users can rely on data professionals to create effective user interfaces that lead to positive experiences. And that's what we’re going to learn about in this video! Let’s start with some definitions. The user experience, or UX, is the overall experience a user has with a product or service, especially in terms of how easy or pleasing it is to use. The importance of this concept continues to evolve as technology advances to make human-to-computer interactions easier and more intuitive. And a user interface, or UI, is the means through which a user and a computer system interact. So, everything the user experiences on a screen or device is part of the UI. And, on a foundational level, the UI allows people to interact with computer software and hardware without needing to understand how to program code. Both of these design approaches are about the user and what they need. As a data analyst, you’ll frequently encounter these terms combined in the acronym UX/UI or sometimes the other way around. Either way, these design approaches are key to your work with cloud visualizations. After all, if the solution you give to stakeholders is difficult to learn, cumbersome, confusing, or they just don’t like it, they won’t use it. And the goal of data analysis is to provide people with information they can use, especially with data visualizations! Here’s an example. Consider a website or app you’ve used where you couldn’t find what you wanted, or it took a long time to find, and was a frustrating experience. There might even be a website that you just didn’t like and you couldn’t quite describe why, but you had a negative reaction to it. Now think about a website or app that was visually appealing to you and the information you needed was quick and easy to find. The difference between these two scenarios is all about the UX and UI. Having well laid out, well designed dashboards and reports isn’t just about making data look good. It’s also about ensuring that users understand what options are available to them, how they can access these options, and what insights they can gain. And it’s about keeping your users at the forefront of everything you do as a data analyst. The more you can concentrate on their needs as your ultimate goal, the better you’ll be able to design effective, powerful visualizations that bring data to life. If you want each section of a dashboard to be clickable so users can drill down into more detail, there needs to be an indication that lets users know that’s an option. In this example, there are no indicators in the first row that let the user know the headings are clickable. The second and third rows demonstrate several visual alternatives to indicate a difference between the information section and the other sections. In the second row, the text is bold and that indicates a clickable link. In the third row, the text is underlined and there is an icon that both help to indicate a clickable link. You might also use formatting tools to indicate the ability to act on a particular item, like images, arrows, bolded text, stylized text, and so on. One final point: While the previous UX and UI examples described visual design choices, keep in mind that as a data analyst there are other important aspects of design. For example, some people use other types of interfaces, like screen readers. That’s why it’s essential you consider users with different accessibility requirements in your design. All right, I hope you had a great experience while using this video! Keep investigating UX and UI and considering the many ways that you can enhance the experiences of others when presenting data insights.

### Video - [Design decisions for data visualizations](https://www.cloudskillsboost.google/course_templates/964/video/486429)

- [YouTube: Design decisions for data visualizations](https://www.youtube.com/watch?v=DmpRbdxd_eg)

As you begin developing data visualizations in the cloud, you’ll be confronted with a lot of important design decisions. Examples of visualization decisions include: Is it insightful? Does this layout and organization make sense? Is it accessible and intuitive for everyone to use? Well, as a cloud data analyst, the answer to all of these questions is always: What’s best for the audience? When creating visualizations, everything you do is for the benefit of your audience or users. For example, you could create a dashboard that looks really nice, but if it’s not organized logically, your users will have a hard time using it. And even if your dashboard is well-organized, it won’t be as engaging if your visuals are distracting. Logical organization is so important. A pro tip to organize your dashboard logically is to work backward. Starting with picturing exactly what your audience needs and wants. This information will help you make successful design decisions, because when you know what your audience needs, you can develop your design around their goals and objectives. In the research phase of the data visualization planning process, you’ll learn what your users need. This is also referred to as usability studies or user research. There are a variety of ways to learn what’s important to your users. Which method you choose depends on a variety of factors, like the type of business or industry, the budget, and what tools and resources are available to you. All right, let’s turn our attention to workflows. I’m sure it’s no surprise that you’ll also want to keep your audience in mind as you begin to develop the more technical aspects of your design. The main point with workflows is to only present what’s necessary to achieve the objectives. Consider this scenario. You call a market to check their inventory for a recipe you’re making. You dial the option for the meat department, but instead get routed to the produce department. This is an example of a bad workflow. You can also experience complications in technical dashboard designs. So, whenever possible, work backwards to plan around the methods you uncovered during your usability study. This will help you develop a logical workflow and optimize the user experience. Ok, now, prioritize the information to make sure that you’re only providing your audience with the information they want or need. For example, you might have uncovered a new feature in the dashboard system. But, if it’s not going to add value to your dashboard, don’t implement it. Adding features that aren’t helpful can end up distracting your audience instead of adding value. And that’s a wrap! Keep in mind, when you design using great attention to detail, you’ll capture your audience’s attention and communicate what they need to know. And keep them coming back for more valuable data insights!

### Document - [Foundational concepts of design: Structure and aesthetics](https://www.cloudskillsboost.google/course_templates/964/documents/486430)

### Document - [Foundational concepts of design: Usability and accessibility](https://www.cloudskillsboost.google/course_templates/964/documents/486431)

### Video - [Additional design concepts to consider](https://www.cloudskillsboost.google/course_templates/964/video/486432)

- [YouTube: Additional design concepts to consider](https://www.youtube.com/watch?v=TKLB4x5xUgg)

Hello, and thanks for joining me for this video about turning great data into great visualizations! As a cloud data professional, one of your most important responsibilities is communicating data insights to others. So let’s get started learning about the many factors that influence your design decisions. First, it’s important to consider how the solution you’re developing will be accessed. For example, will your users primarily be on laptops and desktops, mobile devices, or a combination? For example, if your users mostly use mobile devices, there are a lot of implications to consider to ensure your designs work in the mobile world. The display has a limited size, and too many visualizations can be difficult to manage on a small screen. Instead, you might prioritize the most important visualizations that the display can accommodate. Of course, other visualizations may still be important, so you’ll want to find ways to provide access to them through links, tooltips, popups, and menus. Once you implement your solution, another fairly common task is to request user feedback. The first thing to consider here is what method you’ll use to collect feedback. You might include a feedback button that links to an online form or email pop-up. Once you collect the feedback, you can incorporate it into your plans for future development. And then you’ll decide how to respond to the feedback. The second consideration is what type of feedback you want to receive. Are you only interested in being notified of technical issues that need resolution? Or, are you also open to receiving development requests? No matter the answer, you’ll want to consider how to make the process as easy and efficient as possible. All right, this leads us to the final point. One of your key goals as a data professional is to create a solution that doesn’t need any explanation, instruction, or help. Even so, it’s always good practice to provide help features and clear, concise, accurate documentation. This helps ensure you’ve been thoughtful about how users will experience your data visualization. Great data visualizations offer significant value when they convey meaningful stories and provoke thought. In other words, when your data visualizations are effective, you’re playing a valuable part in turning your organization’s data into relevant, actionable insights.

### Video - [Design principles and strategies for dashboards](https://www.cloudskillsboost.google/course_templates/964/video/486433)

- [YouTube: Design principles and strategies for dashboards](https://www.youtube.com/watch?v=ZzK1T319-uU)

Attention to detail is a data professional’s superpower. It's spotting the little things that others might miss, like finding tiny errors in a dataset or noticing the subtle differences between two potential project plans. In this video, you’ll learn how to apply this concept to data visualization design, so you can also be a data superhero! Let’s cover a few strategies that will help you get there. Ok, the first strategy is to keep simplicity and clarity as central design goals. Let’s use dashboards as an example. As a reminder, a dashboard is a data visualization tool that visually displays data in one place. When constructing dashboards, you’ll want to avoid cumbersome menus, layouts, and workflows. The second strategy is to map a structure that’s organized, logical, and intuitive. Also, distribute your visualizations and avoid overcrowding. Dashboards can very quickly become cluttered, making it difficult for users to find what they need. The third strategy is grouping similar data types together. This leads to effective visual inferences that indicate relationships, or connections, between the information. On the other hand, it also helps create visual separations between data that isn't related. This provides important context and meaning for your users. The fourth strategy is to prioritize usability and accessibility principles in your designs. After all, you want everyone to be able to use and understand your dashboards. You can’t rely solely on visuals to convey concepts, as many people use screen readers and other assistive technologies. Accessibility must be an essential part of every data visualization you create. The fifth strategy is to establish a plan for consistently applying colors and textures throughout your dashboard. It’s likely to confuse a user if one red visualization means “warning” and another red visualization means “success.” Labels should all be the same color as well. Unless that color is intentionally signaling a difference between types of items or groups. The sixth strategy is to apply labels and to be intentional when constructing them. Poorly worded labels can cause users to miss helpful information or spend time on the wrong part of a visualization or dashboard. As with many areas of design, consistency is key. For example, maybe users refer to a sales dashboard to forecast and identify trends in the marketplace. The first visual is a bar chart that shows the comparison year over year. Just naming it “Sales” doesn’t help people understand what the chart is and what information they can gather from it. A better title would be: “Year-over-year comparison.” Provide clarity when labeling to ensure your axis labels and legends are clear and consistent. This helps the user understand that years are being compared. Also, you may notice that this image is incomplete. There isn’t a reference for what the numbers in the y-axis represent. Does this reference dollars, units, thousands, or millions? These types of details are essential, so you’ll want to be sure that they’re clearly defined. Here are some pro tips for achieving consistency in your data visualization design. First, document all terms and definitions in a legend. Then, for consistency, ensure that all visualizations use the legend definitions. If you find a need to break this rule, make sure it is clearly noted. And lastly, use the same terminology across all parts of your solution, including department names, groups, and teams. Follow these strategies and tips to become a true data superhero! Your dashboard users will love your attention to detail. And just like a true superhero, you’ll demonstrate how much you value the needs of others!

### Quiz - [Test your knowledge: Basic principles of UI/UX design](https://www.cloudskillsboost.google/course_templates/964/quizzes/486434)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/964/video/486435)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=1gT5fzS4zu4)

Hello advancing cloud data analyst! Congrats on learning how to design and communicate stories with cloud data! In this section, you learned about how to grow your skills in storytelling with cloud data. You can now describe the strategies of user experience, or UX, and user interface, or UI, design, plus how those concepts affect your role as a cloud data analyst. You understand the foundations of layout, organization, color, usability and accessibility, along with how each of these elements interact to create good design. Finally, you now have the know-how to identify the many ways to communicate through design, and share both technical and non-technical concepts with your users. Congrats again on your amazing progress so far!

### Document - [Glossary terms from module 1](https://www.cloudskillsboost.google/course_templates/964/documents/486436)

### Quiz - [Module 1 challenge](https://www.cloudskillsboost.google/course_templates/964/quizzes/486437)

## Methods for visualization planning and design

In this module, you'll differentiate between various types of data and what method of visualization will display the information in the most effective manner. You'll also learn how to translate stakeholder requests into effective, clear dashboards and reports that will provide actionable insights and context. 

### Video - [Welcome to module 2](https://www.cloudskillsboost.google/course_templates/964/video/486438)

- [YouTube: Welcome to module 2](https://www.youtube.com/watch?v=7VlZ4kvbdrY)

Hi, there! Welcome to this section of the program all about visualization planning and design. I’m really looking forward to guiding you through these topics about the impact that you can make on data insights. Together, we’ll explore the many aspects of design that can influence your choices about dashboards and data visualizations. You’ll learn about data type implications on design, which will help you make good decisions about how to present data when creating visualizations. Next, you’ll check out business intelligence dashboard types and consider how to select the right type of dashboard visualizations based on user requirements. Then, you’ll learn more about working with stakeholders, including how to ask questions that unpack stakeholder needs. Finally, you’ll discover how to translate stakeholder requests into a design plan and test its functionality through the use of wireframing. These topics will prepare you for the planning and communication aspects of design, and help you create a design plan that you can implement with confidence. Let’s get started!

### Video - [Data types to consider for visualizations](https://www.cloudskillsboost.google/course_templates/964/video/486439)

- [YouTube: Data types to consider for visualizations](https://www.youtube.com/watch?v=E65b5KXSlBI)

Hi, and welcome to this video all about data types! There are many different types of data, and each one has its own unique properties and purposes. Knowing the type of data you’re working with is important, especially when designing data visualizations, because it helps you communicate data insights more effectively. So, let’s begin by exploring the two basic types of data: categorical data and numerical data. Categorical data —also referred to as qualitative data— is a subjective and explanatory measure of a quality or characteristic. Put in simpler terms, it’s data that describes the categories or qualities of the data being collected. Think of things you can name, like countries, industries, or products. Numerical data —also referred to as quantitative data— is a specific and objective measure, like a number, quantity, or range. So, it’s data that can be quantified, and it describes things you can count, order, or measure, like ages, test scores, and prices. When it comes to data visualizations, understanding whether your data is categorical or numerical affects your decision-making and design processes. For example, categorical data can be displayed in bar, column, and pie charts to show relationships between data. But this won’t work for numerical data, which is best suited to line charts, histograms, scatterplots, box plots, and bubble charts. These visualizations are effective at displaying changes over time, values, distributions, and summaries. Now, there are five common ways to visualize data, based on data type: single-value, comparison, composition, distribution, and relationship visualizations. Let’s consider each of these in detail. First, a single-value visualization is simply a query that generates one value that will be used in a visualization. For example, you could display the average points per game of your favorite football player. As the name suggests, comparison charts compare two or more values. This type of visualization is displayed in bar, column, and line charts. For example, it’s easy to compare the population of the top 10 largest countries with any of these chart types. Next, composition charts compare individual parts to a whole, usually equalling one hundred percent. You’ll usually find this information displayed as pie charts, stacked columns, or bar charts. For example, an age distribution chart shows the percentage of people in each age group for a single country where the total percentage equals 100. Comparison and composition charts are the most commonly used in data visualizations. They’re also the only charts that effectively display categorical data. Next up, distribution charts. These are great for illustrating if and how data correlates among different data points. Two variables correlate if they have a relationship and, therefore, tend to vary together in some way. These charts are very detailed, and are useful for displaying differences in averages, means, medians, minimums and maximums across different variables. Finally, relationship charts also demonstrate correlation. Here, it’s between two or more variables. These relationships are depicted in scatter plots and bubble charts. This bubble chart shows growth rate versus median age as a correlation. Distribution and relationship charts are the only charts that effectively display numerical data. One quick final point: some types of graphs can be used in a couple of different ways. For example, a line chart can show comparison or distribution, depending on how the user should interpret the data. There are tons of programs for designing charts and graphs, so you’ll have your choice of several tools to use when creating these different data visualization types. And as you work to figure out which visuals will be most effective for your audience, always consider the data first!

### Document - [The importance of data type identification](https://www.cloudskillsboost.google/course_templates/964/documents/486440)

### Document - [Visualization techniques for different data types](https://www.cloudskillsboost.google/course_templates/964/documents/486441)

### Document - [Activity: Choose an effective visualization type](https://www.cloudskillsboost.google/course_templates/964/documents/486442)

### Quiz - [Activity Quiz: Choose an effective visualization type](https://www.cloudskillsboost.google/course_templates/964/quizzes/486443)

### Document - [Activity Exemplar: Choose an effective visualization type](https://www.cloudskillsboost.google/course_templates/964/documents/486444)

### Quiz - [Test your knowledge: Data type implications on visualizations](https://www.cloudskillsboost.google/course_templates/964/quizzes/486445)

### Video - [Introduction to business intelligence dashboards](https://www.cloudskillsboost.google/course_templates/964/video/486446)

- [YouTube: Introduction to business intelligence dashboards](https://www.youtube.com/watch?v=Xsxg2UKUNmU)

When it comes to what transportation we use, people's needs vary depending on many things. For example, some people may travel by bicycle or public transit to run a few errands. Other people may use a car. But, if you have a lot of cargo and need to be able to haul heavy loads, you’ll probably need a truck. Just like choosing a way to get around, as a data analyst, you’ll base the type of dashboard you design on what your users need and how they’re going to use the information you present. This video will provide you with many great strategies to address each of these important considerations. Ok, first, there are four basic types of business intelligence dashboards: strategic, operational, analytical, and tactical. A strategic dashboard is a very high-level visualization. Typically, corporate executive users prefer a strategic dashboard to focus on long-term, organizational strategies and key performance indicators, or KPIs. This type of dashboards usually displays corporate financial performance and revenue trends. Next, an operational dashboard is more detailed and is typically helpful for junior-level decision-makers and their teams. These dashboard users are more concerned with shorter timeframes, operational processes, and monitoring performance. So, these dashboards display business metrics like sales activity, marketing performance, or customer support status. Then, an analytical dashboard is helpful for assimilating large amounts of data for historical analysis to identify trends, make comparisons, create predictions, and establish future goals. Most commonly, middle managers or data analysts working on ecommerce, sales, or website analytics will use this type of dashboard. Finally, a tactical dashboard is highly detailed and usually used to analyze social media ads or sales manager KPIs. Tactical dashboards might be used by any of these stakeholders to track initiatives and performance. Here are two things to keep in mind in regard to dashboard selection and user groups. First, each of these types of dashboards might use the same data sources. As a data analyst, you might use dashboards to present that information in different ways, based on your particular users’ needs. Second, you’ll likely encounter certain user groups that need to use more than one type of dashboard, depending on the questions those user groups ask, and this can change over time. While the various dashboards generally have different purposes and goals, they’re also flexible. So you can use visualizations in multiple dashboard types. And you can choose the dashboard style based on a variety of factors, like your user, their job role, and the business goal. Understanding the different dashboard types is essential to your work as a cloud data analyst. Just like picking transportation for quick rides to the store or finding a big truck for moves across the country, the dashboard you design must be driven by user needs and company objectives. Let’s hope you get a lot of mileage out of your dashboard know-how!

### Document - [4 types of business intelligence dashboards](https://www.cloudskillsboost.google/course_templates/964/documents/486447)

### Video - [Benefits of scorecards](https://www.cloudskillsboost.google/course_templates/964/video/486448)

- [YouTube: Benefits of scorecards](https://www.youtube.com/watch?v=1gUl8K44g1k)

While playing sports or a board game, people keep score for a number of reasons. It provides a fun sense of competition that can be a motivating factor for players to play their best, it helps track player progress over time, and it lets the players know who won! Keeping score is also something many businesses do with scorecards. A scorecard is a statistical record used to measure achievement or progress toward a goal. It compares multiple points of data to help stakeholders make decisions that will help them meet objectives. Like dashboards, scorecards are also a type of visualization. But, while the purpose of a dashboard is to monitor progress, the purpose of a scorecard is to track the progress of metrics. Scorecards enable users to easily spot actual results and compare them against goals. This helps them quickly decide if they need to take action. For example, consider a data analyst who is working with data for a call center. The center has a goal to increase revenue by twenty percent this year. A dashboard could highlight their annual sales to date, but it wouldn’t offer any context showing whether they’re on track or know how to adjust if they aren’t. So, the data analyst creates a scorecard that displays the relationship between the call center’s sales goal and the actual, current sales data. This gives them a quick and easy way to determine if they’re on track to meet the twenty percent growth target. And if not, they can adjust their performance. This call center might also like to know how many people are calling in for the first time versus how many have called in before or how many incoming calls have been converted into customers. The data analyst can design another scorecard, which the business can use to compare the data results to department goals. This example demonstrates both the number of calls received and the relationship the data results have to the department’s key performance indicators, or KPI’s for the last complete week. In this instance, the call center data reveals a decrease in its average number of calls, but an increase in the call duration. Comparing current numbers to desired performance also helps the call center management quickly notice any problems and react accordingly. As another example, if call hold times are longer than the target because the number of calls are increasing, then the call center management can hire additional staff to reduce the queue back down to the target range. Or, if the call center notices the number of issues is increasing more than the average, a supervisor can help answer calls for a bit to get things back on track. In these instances, the scorecards have given life to the data; transitioning it from interesting information, to a specific call for action. While dashboards definitely have value on their own, incorporating scorecards in dashboards adds helpful context and valuable insights. Just like a championship sports match or a highly competitive board game, knowing the score helps motivate everyone to perform their best!

### Quiz - [Test your knowledge: Business intelligence visualization types](https://www.cloudskillsboost.google/course_templates/964/quizzes/486449)

### Video - [SMART questions for stakeholders](https://www.cloudskillsboost.google/course_templates/964/video/486450)

- [YouTube: SMART questions for stakeholders](https://www.youtube.com/watch?v=6fpzjtSUtOk)

Some of the most accomplished people throughout history are famous for asking effective questions. The Greek philosopher Socrates was known for his method of inquiry, the Socratic method. It involves asking questions to help people to think critically and draw out new ideas. Physicist Marie Curie, who conducted pioneering research on radioactivity, was always asking why and how. And she bravely asked questions that challenged conventional thinking. Some of these questions even led to the development of X-rays and breakthrough medical treatments. As a data professional, you can also accomplish great things by learning how to ask effective questions. To do this, ask SMART questions, based on the SMART goal methodology. This is a method you can use to determine a question’s effectiveness based on whether the question is specific, measurable, action-oriented, relevant, or time-bound. Make sure each question you ask utilizes at least one of these components to prompt your stakeholders to provide you with the level of detail you need to fully understand the request. Let’s go through each of these now. First, questions should be specific. For example, instead of asking, “What kinds of employees will be using this system?”, you could ask, “Will this solution be used by entry-level teams, project teams, or middle managers?” Even if the answer isn’t one of those options, it will prompt your stakeholder to provide you with a more detailed answer. Next, ask questions that will provide measurable answers. So, if your stakeholder tells you they want to increase sales, this alone is not measurable. Instead, it’ll be necessary to ask questions that lead to more tangible results. Questions like, “What are your current sales?” and “By how much do you want to increase that?” The answers will create specific, quantifiable, and measurable objectives. Now for the “A”: If you’re familiar with SMART goals, you’ll see that this is where SMART questions differ slightly. You’ll want to make sure your questions are action-oriented. This means phrasing your questions in such a way that stakeholders can provide responses that you can act on. For example, the question, “Will you use market and industry benchmarks?” can be answered with just yes or no. And that’s not helpful for building a conversation. But, asking an open-ended question like “Which market and industry benchmarks will you use?” prompts both a detailed response and an opportunity to grow the conversation. Of course, you’ll always want to make sure your questions are relevant. There are certain questions that you’ll ask during every project, including “What problem are you trying to solve?” and “Which key-performance indicators will you use to measure success?” Other questions will vary based on the type of project you’re working on. But either way, ensure they’re relevant to the project and business objectives. Finally, you’ll want to specify the time period to be analyzed. This limits the range of possibilities and ensures you focus on relevant data. So, ask if the data should come from the past quarter or the past two quarters, if you should analyze information from this calendar year or fiscal year. And there’s another way to think about time-bound questions. This involves considering project time constraints. With this in mind, ask questions like “What is the timeframe for this project?” or “What are the top three priorities that need to be accomplished during this timeframe?” Alright, now one last thing to consider. After asking your SMART questions, it’s a good idea to start setting realistic goals and expectations. If your stakeholders ask for a dashboard that includes every possible data point, this is probably unrealistic because you have a limited amount of time and dashboard space. But when you set realistic expectations, you’re more likely to meet your goals and provide your stakeholders the data they need. Asking SMART questions can help you learn new things, solve problems, and be a more effective cloud data professional. One day, it may even lead you to some amazing, life-changing discoveries!

### Document - [[Supplemental] Tips for effective teamwork](https://www.cloudskillsboost.google/course_templates/964/documents/486451)

### Document - [Dashboard alignment with stakeholder needs](https://www.cloudskillsboost.google/course_templates/964/documents/486452)

### Quiz - [Test your knowledge: Strategies for stakeholder alignment](https://www.cloudskillsboost.google/course_templates/964/quizzes/486453)

### Video - [Strategies for translating stakeholder requests](https://www.cloudskillsboost.google/course_templates/964/video/486454)

- [YouTube: Strategies for translating stakeholder requests](https://www.youtube.com/watch?v=llPb4vGMaL8)

You’re probably most familiar with the word translate when it comes to translating words from one language to another. As a data analyst, you’ll hear this word used in a slightly different way, to translate stakeholder requests. As a data professional, this will be a critical component of your job to define stakeholder requirements before you can create tangible visualizations, such as reports and dashboards, to meet stakeholder needs. So, in this video, you’re going to learn exactly how to do that! While the language analogy works well here, maybe a better way to think about this process is as a facilitator. You’re going to take the needs and goals of your stakeholders and turn them into data that their teams can use to make informed, business decisions. Ok, first of all, to translate a stakeholder request, you’ll want to get to know and understand your stakeholders. In this role, you will work with many different people from lots of varied backgrounds and specialities. What are their areas of expertise and experience? Maybe you might like to identify their personality type or how they best like to communicate. It might also help you to identify their comfort level when discussing data-related concepts. Any effort you take to better understand the stakeholder will help improve your ability to communicate with them and deliver the solutions they need. You’ll also ask detailed questions to identify their business requirements. For example, you might find yourself working with someone who likes to stay with what’s familiar to them. They might even be very specific about the request, right down to the exact number of columns a report should have. It seems like they’ve given this request a great deal of thought and know just what they need. But you still want to use SMART questions that are specific, measurable, action-oriented, relevant, or time-bound to get to the root of what they really need or are trying to accomplish. The response to each question is an opportunity for you to better understand their needs and provide them with a solution that is as concise and effective as possible. Asking why —and other effective questions— may also enable you to identify needs that your stakeholder hasn’t considered yet. Keep asking questions to get the full scope of the project. It can be incredibly rewarding to apply translation skills, whether you’re communicating in another language, or collaborating with a stakeholder to determine their business needs. And this is just one more key to unlocking how to be more effective in your role as a data analyst!

### Video - [Introduction to wireframes for stakeholder alignment](https://www.cloudskillsboost.google/course_templates/964/video/486455)

- [YouTube: Introduction to wireframes for stakeholder alignment](https://www.youtube.com/watch?v=HnhT5AsHtlA)

There are many great reasons why professionals create first drafts before they put their plan into action. An architect draws a sketch of a new building design prior to starting construction. A graphic designer provides mockups of new logos before creating the final logo. And a data professional presents a wireframe in advance of actually building a user interface. In all of these cases, a simple example, called a prototype, allows the creator to communicate their ideas and get feedback before investing a lot of time and resources into the project. As a data professional, your role in design goes beyond providing a report or dashboard to your users. You’ll also use design during the planning stages of a project to communicate with stakeholders and ensure product functionality. As a cloud data analyst, you can create these prototypes, also known as wireframes. A wireframe is a visual representation of the structure and functionality of a user interface or product layout. For dashboards, this means creating the basic layout and design of the dashboard. You can use wireframes during project planning phases to ensure that the user interface or product you’re designing is complete, and satisfies the stakeholder request. Wireframing in the early stages of the development process is critical for two main reasons: to align with stakeholder needs and simplify the feedback process. First, visual examples tend to be easier to understand than verbal or written communications. So, when you mock up your vision into a wireframe to share with stakeholders, this helps everyone understand if they’re in alignment. In this way, you’re much more likely to develop a finished product that reflects your stakeholders’ needs. The second benefit of using a wireframe while planning a deliverable is that it speeds up and simplifies the feedback process. Providing stakeholders with easy-to-review materials helps them to quickly and easily review your plan. As a cloud data analyst, there are a variety of wireframe programs that you may use in your work. These programs will help you create products or user interfaces, like websites and dashboards. Additionally, these programs offer a variety of features to aid in your draft design and they highlight how your charts will interact to tell the data story. This helps you’ve a complete and actionable data story. Wireframes are also very effective at helping you improve the accessibility of your design. For example, you can arrange design elements like navigation, images, and text to help ensure users can easily gain insights based on their abilities. Now, you might find yourself working in a situation where you don’t have access to a wireframe program. In that case, there are a variety of other apps that you can use to create a visual draft of your plan, like Google Slides or Google Docs. While these programs won’t have the full functionality of some wireframe programs, you’ll still be able to visually document the general layout and plan for your stakeholders. Here’s the bottom line. No matter which wireframe program you choose to design your prototypes, your wireframes will help you align with stakeholder needs and simplify the feedback process. By sharing your wireframes with stakeholders during the planning process, you’ll receive valuable feedback to help save time and effort while identifying potential challenges, design opportunities, and ways to improve the accessibility of your design.

### Document - [The importance of the user journey](https://www.cloudskillsboost.google/course_templates/964/documents/486456)

### Video - [Lateefat: Meet the needs of cloud customers](https://www.cloudskillsboost.google/course_templates/964/video/486457)

- [YouTube: Lateefat: Meet the needs of cloud customers](https://www.youtube.com/watch?v=AE6DKyISK7w)

One thing that gets me super excited about this industry is that I really love solving new problems and thinking and stretching my brain around them. And because the customers I work with are so varied, I get to stretch myself by thinking about problems and industries and solutions that I wouldn't have ever gotten exposure to without it. My name's Lateefat, I'm a customer engineer here at Google Cloud with a data analytics specialty. And essentially, what I do is help companies build apps and move them to the cloud. So when I was in high school, I took my first statistics course and I was never fantastic about math, I was pretty good, I won't sell myself short, but when I got to university, I added that as a degree major. During that process, I also was a waitress part-time in college, and I realized I liked interacting with people and I also tutored and I was like, I like teaching people about things, especially when they didn't get it before. And so, customer engineering is kind of the melding of all of that. I'm teaching customers about cloud that they didn't know before, I'm teaching them about their own products that they might not have known or sat down to discover. But I'm also being able to implement my statistics knowledge, my data analytics knowledge with people and not just behind a screen all day. To build my skills, the first thing I did was I took an online course and then I also got a professional data engineering certificate for Google Cloud, which is where I work now, as well as really just getting my hands dirty, there's no other way to replace experience. Sometimes that's finding new projects, sometimes that's volunteering for like a local community organization that needs data analytics help. Just anything where you can put yourself in a situation you haven't been in before. By being able to show real time that I could do the work and I had experience that translated, I was able to make my way in. Soft skills that are important for my current job are communication, and by communication, I mean the ability to listen, to understand, as well as communicate complex concepts back to somebody, as well as project management skills. So think about creating a timeline, helping people stick to that timeline and being able to react quickly if anything comes up. If there was something I could say to anybody starting out for the first time, it's okay if you don't understand things, it's okay if it's confusing. But now, three years later, I realize most of the problem was my inability to stop and really think about it and chew on it because I was always like raring to go. And I think you guys will probably all experience this at some point, you just wanna be done so fast, get into your next career so fast. Take the opportunity to understand what's going on and then also recognize that it's okay to try something and then try something else, you'll get there eventually.

### Document - [How to build a wireframe in Google Slides](https://www.cloudskillsboost.google/course_templates/964/documents/486458)

### Document - [Activity: Analyze a wireframe](https://www.cloudskillsboost.google/course_templates/964/documents/486459)

### Quiz - [Activity Quiz: Analyze a wireframe](https://www.cloudskillsboost.google/course_templates/964/quizzes/486460)

### Quiz - [Test your knowledge: Translate stakeholder requests with wireframes](https://www.cloudskillsboost.google/course_templates/964/quizzes/486461)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/964/video/486462)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=fq-BJFhOEoo)

Congratulations on finishing another section of this program! You now know tons of useful information about visualization planning and design! You started off by exploring the impact data types have on your design decisions. And you learned how data types influence the types of visualizations data professionals can create. Then, you considered the various types of business intelligence dashboards, and how keeping user needs in mind can help you select the right type of dashboard visualizations for each particular project. After that, you gained some key insights into working with stakeholders. You discovered how to ask questions that get to the heart of what’s really important by uncovering stakeholder needs. You also identified different ways to use design to translate stakeholder requests into a solid design plan. Finally, you learned how to use wireframes to align with stakeholder needs, simplify the feedback process, and improve the accessibility of your designs. Wonderful job on your progress so far. I can’t wait for you to keep up the momentum in the next section of the course!

### Document - [Glossary terms from module 2](https://www.cloudskillsboost.google/course_templates/964/documents/486463)

### Quiz - [Module 2 challenge](https://www.cloudskillsboost.google/course_templates/964/quizzes/486464)

## Access, explore, and report on data in the cloud

In this module, you'll explore the role of cloud-native data tools in the data analysis workflow, connect to and explore a data set, define dimensions and measures, and build a report to share data insights.

### Video - [Welcome to module 3](https://www.cloudskillsboost.google/course_templates/964/video/486465)

- [YouTube: Welcome to module 3](https://www.youtube.com/watch?v=uyysDLtjGfc)

Hey there, future data pro! Welcome to this section of the program, all about accessing, exploring, and reporting on data in the cloud. We’ll be focusing on the last two stages of the data journey: analyze and activate. You’re going to explore some really amazing data visualization tools and discover how to select the right tool for each phase of the data analytics workflow. You’ll also learn how to access, explore, and analyze cloud data that’s stored in data lakes, data warehouses, and other sources. Along the way, you’ll explore datasets using data visualization tools. Then, you’ll learn about data types, dimensions, and measures to better understand the data model. You’ll also use data modeling to customize how data appears. Next, you’ll model data for visualizations, which includes modifying, blending, and aggregating the data. You’ll also learn how to manipulate and organize data in a way that best yields insights. Finally, you’ll understand report analysis for visualizing and sharing information with stakeholders. You’ll also get direct experience with Looker Studio, where you’ll connect to a data source, explore a dataset, transform fetched data, and create a report. Now that you have a roadmap for what’s coming up, let’s get started!

### Video - [The data journey: Analyze and activate](https://www.cloudskillsboost.google/course_templates/964/video/486466)

- [YouTube: The data journey: Analyze and activate](https://www.youtube.com/watch?v=aZw3mElXjrM)

As you likely know, the data journey consists of five stages: collect, process, store, analyze, and activate. This video is about the last two stages of the data journey: analyze and activate. During the analyze stage, you identify trends and patterns in your data to uncover insights that your users need. Cloud-based data visualization tools provide a variety of features that make it easy to explore and analyze data, such as filtering data, drilling down to uncover more granular insights, and creating custom visualizations. These features help you spot trends and patterns in data, and develop deeper insights. During the activate stage, you present your visualizations to your stakeholders and use the insights from the data to make decisions and take action. Visualizations created with cloud-based data visualization tools can easily be shared with others to inform decision-making. These visualizations can be used to communicate insights from data in a clear and concise way, helping teams make the most of their data. Let’s look at an example of analyze and activate at work. As a data analyst, you can use cloud visualization tools to analyze data about national sneaker sales trends from a publicly available dataset and sales data from the company's own local sales dataset. This allows you to create a single graph that compares the two datasets. This visualization can then be used to activate the data by providing the team with a visual representation of how the different regional sales compare against national averages. The sales team can then use the findings to help them develop more effective sales strategies. There are many benefits to using cloud-based data visualization tools, such as flexibility, time savings, and user friendliness. Cloud-based data visualization tools can be used to analyze and activate data from a variety of sources, including local databases, spreadsheets, public datasets, and cloud-based databases. This enables data professionals to combine multiple data sources with huge amounts of data into a single visualization. Cloud-based data visualization tools can also save hours of manual processing and analysis. These tools empower your users to interact with and explore the data in a user-friendly way. This encourages them to ask more questions, leading them to make data-driven decisions. There are so many benefits to using cloud-based visualization tools to harness data and share compelling data stories. When you use these tools to discover, access, and visualize data, you’ll be creating awesome new opportunities for yourself and your teams!

### Document - [Cloud-based tools for accessing and visualizing data](https://www.cloudskillsboost.google/course_templates/964/documents/486467)

### Video - [Considerations for connecting to cloud data](https://www.cloudskillsboost.google/course_templates/964/video/486468)

- [YouTube: Considerations for connecting to cloud data](https://www.youtube.com/watch?v=7rNaZZAd0Ho)

One of the exciting things about the cloud data profession is the fact that there is so much data available to use. Having more access to data means you can find answers to questions that you may not have been able to before. In this video, we’re going to review some of the different types of data sources and various considerations for connecting to cloud data. As a cloud data analyst, there are four common types of data sources that you will likely use in your work. First, data professionals can collect public data from businesses, government, and private organizations. As the name suggests, this data is publically available. This has the potential to advance education, social causes, economic growth, innovation, business and government accountability, and so much more. Second, the cloud also has all kinds of product-specific data, across industries like retail, non-profit, and finance, that can be accessed by data management systems. Third, software platforms are a great data source. These contain user relationship management, social media, advertising, human resources, and financial system information. Fourth, the cloud also has company-specific sources, like Salesforce, proprietary databases, and spreadsheets. These systems typically have their own unique formats, so it’s important to be aware of those implications for visualization and analysis. As a cloud data analyst, there are many different ways to connect to the different data sources we just covered. There are also a lot of helpful solutions to help you find what you need, including Looker and Looker Studio, to use internet-accessible data in the cloud. Now, no matter where the data comes from, you will need to pay attention to the data freshness. Data freshness refers to how current, or up-to-date, the data within a report or dashboard is. For example, if the data in a dashboard is updated once every 24 hours at midnight, and a user accesses the dashboard at 5 PM, then that information is outdated by 17 hours, and it won’t reflect any of the current day’s data. The importance of data freshness will vary by visualization and business need. In some cases, users access the data infrequently, and so data freshness is not a priority. For example, when company executives review annual numbers in a high-level strategic dashboard, the data typically does not need to be updated regularly. But a sales team’s dashboard with up-to-date sales trends likely does need to be refreshed in near-real-time. Another aspect of working with cloud data tools that you’ll need to consider is data security. This means being aware of user permissioning groups and what they can do. These groups are usually broken down into four types: admin, developer, user, and viewer groups, but may vary by product. Let’s understand each of these. Administrators, sometimes called admins, are the highest level within any system. These users will have the ability to do things like manage the workspace, assign roles, and update the database. This is a very important role! Next, developers work with data modeling languages to create data models and manage dashboards. Then, users generally explore the data and perform functions like creating visualizations, custom fields, and table calculations. Lastly, the most limited role is the viewers. They can access and interact with visualizations that have already been created, but not edit them. To fully understand the various groups and roles, review each specific tool’s documentation. Also keep in mind that every user with more access can do everything those with less access can do. As an example, consider developers who can also perform functions as users do, and they can interact with visualizations, just like viewers. Now, one last thing. When setting user permissions, be aware of not only what users can access, but also when and how. So, make sure you’re prepared to protect company data that users have access to. This entails protecting data from malicious actors —or people who try to access the data without authorization— whether users are on a mobile device in a public place, or using a work laptop at home. As a cloud data professional, there’s a lot to consider when accessing open data from a range of data sources in the cloud. The information that’s available anytime, from anywhere, is truly staggering. But it’s your job to make sure it’s used responsibly and safely. By thoughtfully handling these critical details, you’ll be able to create dashboards with data from publicly available and-or company-specific sources to help your stakeholders make insightful decisions.

### Quiz - [Test your knowledge: Access data on the cloud](https://www.cloudskillsboost.google/course_templates/964/quizzes/486469)

### Video - [The importance of data exploration](https://www.cloudskillsboost.google/course_templates/964/video/486470)

- [YouTube: The importance of data exploration](https://www.youtube.com/watch?v=VVbfDeLYibQ)

When we come across something that’s unfamiliar, like a place we’ve never visited, we may first try to identify details and characteristics to help us better understand the world around us. This can be a landmark, like a building or body of water, or even a street sign! This type of exploration is important when learning about new things, and using them for decision-making and problem-solving. It’s also an essential element of data analytics, and that’s what we’re going to explore in this video. Exploration is what this video is all about, data exploration, that is. As a cloud data analyst, data exploration is the process of understanding a dataset by inspecting its characteristics, identifying patterns, and asking questions. This includes qualities like size, quantity, distribution, and accuracy. Knowing about these attributes can help you comprehend the essential nature of their data. Let’s get into why data exploration is so important. First, it’s essential for you to have an understanding of the data set’s structure. This will give you a better foundation for the size, quantity, and type of data you’ll be working with. Think of this like a filing cabinet. You rely on the labels of the folders to understand what types of files you’re working with, and where they’re located. Data exploration is mostly about defining what a column includes, so it’s important that you correctly label these columns. Using the filing cabinet example, if one folder is named “Finances one,” and another is named “Finances two,” it’s not clear what information is inside. If the folders have very similar names, this can lead to issues when you’re trying to find a specific piece of information. And even with effective labels, it’s still necessary to open the folders to know what they contain. Data exploration is like opening the folders in a filing cabinet to understand what data you have to work with. Data exploration is also an opportunity to experiment with ways to visualize data. It helps you refine your plans for how to create visualizations that are easy to understand. You’ll likely tailor your questions and hypotheses as part of your data exploration. Finally, data exploration enables you to confirm the data quality and the distribution of data values. This benefits you and your data team because you can identify outliers and data errors. And, you can decide to address these issues before you design your visualizations. Now, there are a variety of tools available for exploring data. You can explore the data using code with Python, SQL, and R. Or, if the dataset is small enough, you can use spreadsheet programs like Google Sheets or Excel. You can also explore the data visually with a tool like Looker. Tools like these are great because they help you visually explore the database and data relationships, making it easier to quickly identify potential issues before they become problems. Understanding how to characterize data will help make you a highly effective data professional, especially when it’s time to learn about new datasets. Great work, and thanks so much for joining me on this exploration of exploration.

### Document - [Actionable insights through data exploration](https://www.cloudskillsboost.google/course_templates/964/documents/486471)

### Video - [Nicholaus: Cloud tools for business optimization](https://www.cloudskillsboost.google/course_templates/964/video/486472)

- [YouTube: Nicholaus: Cloud tools for business optimization](https://www.youtube.com/watch?v=QnyREsFH2Qg)

Hi, my name is Nicholaus. I'm a business analyst at Google. Some people call me Klaus. It's kind of an interesting nickname that I've picked up over, mostly because my name has a U in it, so it's Nicholaus. I am responsible for reporting business metrics to leads. It's very specifically operations within Google, so anything that has to do with inventory, anything that has to do with IT interactions, anything that has to do with platform health, et cetera, et cetera. I also do a lot of machine learning type work, so I do a lot of advanced analytics and trying to figure out how we can apply machine learning and AI to find good opportunities within the business to optimize. I grew up in Santa Cruz, California. I struggled in school, did not like school whatsoever. School was not for me. I had a troubling home life. The result of that was me dropping outta school very early, roughly around 15, 16 years old, started washing dishes. I was part of a program that a chef was doing at a hospital, local hospital, great guy, Chef Sean, and he was working with juvenile kids who were at-risk youth, people in and out of the juvenile system, et cetera, and he was teaching them how to do culinary arts work. I found myself at 21, 22 years old, feeling a little bit like I didn't have a choice in the career path that I had chosen, and I wanted to try something different. I didn't know exactly what it was going to be. Maybe I'll try to go back to school just to see what happens. And at the time, I wanted to be a writer and I said, "Maybe I'll be a writer. I'll tell my story." And then I changed my mind. That's the wonderful thing about school is that you can change your mind and you can really just try on different things. I said, "Maybe I'll be a nurse," because I really wanted to help people. I took a class called Mathematics for Healthcare Professionals, and the professor in the class said, "Hey, you seem to have a real aptitude for this." I was able to get a full scholarship to go to New York University, where I double majored in math and computer science. Especially at these types of schools, schools like NYU, these private universities, it was very atypical for someone to be a undergraduate and be my age. I remember a couple times I was mistaken for either a professor or DoorDash delivery person, (laughs) especially with all the tattoos, I stuck out like a sore thumb in there. I arrived at Google in 2016. I moved from New York City to Mountain View at the time, and I was arriving with a very different tool set and a very, very different set of life experiences that the rest of my colleagues had, and that really allowed me to really shine. I think a lot of people that I've met here within the company actually have a lot of interesting stories, and it's actually those interesting stories that makes them qualified to be Googlers. (laughs) My org is related to the cloud somewhat indirectly, but we do use our data warehouse is primarily built with cloud tools, so I use cloud tools constantly, as they apply to business internals within Google. Depending on what you're into, depending on what you like, if you like to work with massive data sets, there's a place for you in the Cloud. If you like to work on use cases and be interfacing with customers and understanding how you can apply cloud tools within a business, embedded in a business, there's a place for you. If you like to work in an advanced analytics space and want to deploy huge scalable, machine learning algorithms, there's a place for you. The Cloud ecosystem and the Cloud universe is really large, and I think if you find a place, if you find something that's interesting to you, that gives you that spark, you can have a really great career. I think that anybody who's going through the process is obviously thinking about, okay, what is the next step for me. If I'm gonna do this, if I'm gonna sacrifice my time to do this certificate, how is this going to affect my future? And I guess what I would say is that the certificate itself is going to not only give you some sort of artifact that's gonna unlock a lot of doors for you when someone from Google or someone from a business is looking at trying to assess your skills, but it's also gonna open a door for you. It's also gonna start to expose you to subject matter that you might not be familiar with before. And that exposure itself is worth something, that exposure to new concepts and new ideas, because it's very associative. I didn't start with a computer science degree. I started with something totally different. And through being open and through exposure to new ideas, I found my way to what I do now, which is data science work and business analytics work. It was only through giving myself that freedom to be exposed to new things that allowed me to find my way to something that I'm truly passionate about today.

### Video - [Dimensions and measures in data models](https://www.cloudskillsboost.google/course_templates/964/video/486473)

- [YouTube: Dimensions and measures in data models](https://www.youtube.com/watch?v=Xmhi-VPrpwo)

Hello, data enthusiast! I can’t wait to take you through this video all about the value of data models and attributes, like dimensions and measures. Let's start with data models. A data model is a concept for organizing data elements and how they relate to one another. It can be visually represented to show how the data is structured and how it relates to each other. Every data source has a data model, even if it's not explicitly defined. For example, consider a simple table of data. This table has a data model that defines the columns and rows of the table. A more complex data source, like a snowflake schema, has a more complex data model that defines the relationships between different tables. As a data analyst, you create data models to structure and organize the dataset and determine how to define data attributes. Dimensions and measures provide a flexible way to explore a dataset’s attributes and ask meaningful questions of the data. And they’re an important part of how cloud data analysts work with visualization tools. But what exactly are data attributes? Well, that brings us to dimensions! Dimensions are unique attributes of the data that help you to describe data. Here's a simple data model made up of rows and columns that contains information about books. The table has dimensions like title, genre, price, date published, and whether the book is in stock. Each row in the table represents a single book, and contains the values for the dimensions that describe that book. These values are lined up so that the dimensions form columns. For example, the title of the book is stored in the title column, the genre of the book is stored in the genre column, and so on. Since dimensions group values together, you can use them to ask questions of the data to gain insights. For example, the bookstore owner could ask about which genres their store has in their inventory by using the "genre" dimension to find that value in each row of data. Dimensions can even be combined to ask more complex questions, like, "What genres are available, and which of the books within each genre are currently in stock?" The bookstore owner would do this by using both the "genre" and "in stock" dimensions. But what if you want to know how many books are in the entire bookstore? For that, you need a measure. Measures are aggregations of one or more dimensions like count or average. Returning to the bookstore dataset example, each row in the data model contains the individual characteristics of a single book. If you want to know how many books are in the entire bookstore, you need to use a measure to perform a mathematical operation: count. Since every row equals one book, you’ll need to count the rows to find out the total number of books in your inventory. One way to do this is by counting the values in a single column, or dimension —like the unique identifier column— to find the total number of rows. Measures can also aggregate dimensions in other ways, like finding the sum or average value of all the books. Dimensions and measures are two of the most important concepts in data analytics, especially when visualizing data. They provide valuable information and are essential for understanding data relationships. Keep investigating the use of dimensions and measures in your data models to maximize the power of any dataset you encounter!

### Document - [Customize dimensions and measures in Looker Studio](https://www.cloudskillsboost.google/course_templates/964/documents/486474)

### Quiz - [Test your knowledge: Explore and organize data in the cloud](https://www.cloudskillsboost.google/course_templates/964/quizzes/486475)

### Video - [Basics of modeling data](https://www.cloudskillsboost.google/course_templates/964/video/486476)

- [YouTube: Basics of modeling data](https://www.youtube.com/watch?v=7LFW4zw3rMc)

Welcome back! As a data analyst, you’ll often find that you need to prepare your data before it’s ready for reports and dashboards. As you prepare your data you need to perform a process called data modeling. There are a couple of reasons why you might need to model data. For example, if you’re working with multiple data sources, you’ll have to do some preparation to bring them together effectively. Another reason to model data is to understand the data better. Data modeling is the process of designing, structuring, joining, and transforming data to prepare it for reporting. Really, it’s a form of manipulating data so that it’s much easier to work with and understand. For example, through data modeling, you can define a join between two datasets and format a column so values are uniform. Or you could define metrics or key performance indicators, also known as KPIs. There are several ways to model data. One option is filtering, which is the process of showing only the data that meets a specified criteria while hiding the rest. So, by filtering data, results are limited to focus on a specific subset of the data. Filtering can either include data that matches specific criteria or exclude data that doesn't. For example, in this movie database, you could filter by genre to only show comedies. This is an example of include. Or, you could filter to show everything except horror, which is an example of exclude. Alright, now, another example of modeling is blending data. Data blending is the process of combining data from multiple data sources to create a single report visualization. This greatly enriches your dataset and gives you more flexibility. While this is a great feature, it's important to note that blends aren’t reusable across reports because they use multiple sources. Ok, another useful data modeling process involves aggregations, like sum and average to group and summarize data. Other aggregation examples include count, minimum, and maximum. Each of these functions changes how the data is displayed within a visualization and can be modified as needed. There’s one final consideration to keep in mind. When modeling, it’s common to encounter a situation in which the data doesn’t appear as you intend it to. When this happens, you’ll need to make some adjustments. For example, if the original column title isn’t clear, then you need to modify it to be more descriptive. You can do this in a single instance by updating the label for the specific visualization. Or, if the change should be universal, then change the label in the data source itself, so it’s clear for all instances of potential use cases. This is also true of more advanced situations, like calculations. You can modify a column within the model to a sum or average for just a specific visualization, or change the calculation in the data source to make the change reusable for future models. Modeling data is an excellent way to get closer to the key details of any dataset. Keep learning and exploring data modeling, and soon you’ll know how to effectively inform your data visualization decisions and prepare to share even greater insights!

### Document - [Aggregate and filter to summarize data](https://www.cloudskillsboost.google/course_templates/964/documents/486477)

### Video - [Data blending for helpful insights](https://www.cloudskillsboost.google/course_templates/964/video/486478)

- [YouTube: Data blending for helpful insights](https://www.youtube.com/watch?v=KvjXSbxWY1w)

Hi there! Welcome to this video that’s all about blending data. You’re going to learn about the exciting ways that data from different sources can be put together for helpful data insights. When you join, merge, aggregate, and connect other important data relationships from separate data tables, the final, fully integrated dataset becomes that much more powerful! Data blending is the process of combining data from multiple data sources to create a single visualization. This means that you can pull data from different locations and combine them into a single table. By putting disparate data together in a single place, you’ll be able to create visualizations for your reports that provide additional guidance to users’ analysis and decision-making. For example, a company could use data from its local database, a public cloud data source, and a software application, then combine them into a single, effective visualization. When data is easily available, users have more information and context. And so user insights will be broader, deeper, more diverse, and more informed. Ok, now let’s go through some important data blending steps and considerations. First, while blending data may seem similar to creating a dataset, there are some key differences. Most importantly, blends are embedded into the report from which they were created. This means that they’re only available to use in that specific report. One way to get around this is to copy the report. Then, the blend will be copied along with it. Second, it’s also important to call out that a blend’s data freshness relies completely on its underlying data sources. In other words, the data will only be as current as its source. Now, there are a variety of business intelligence tools for quick and easy data blending. But in general, the process will go something like this. First, connect to the data sources. Most programs will have the ability to combine more than two data sources. Next, define joins. You join data to combine the data into a single dataset. Keep in mind that each dataset must have at least one column in common. This is how the Business Intelligence, or BI, platform understands those essential data relationships. Finally, clean data by deleting any unnecessary data, correct data entry errors, and design the optimal format. This sets up any analysis project for some very effective graphs, charts, tables, and single value visualizations. Blending is a powerful tool for users to gain greater insights into the data. Blending enables you to create a more comprehensive view of the data, identify patterns and trends, and make insights accessible to users. It’s a valuable solution for data analysts, and it provides all sorts of useful data applications.

### Quiz - [Test your knowledge: Model data for visualization](https://www.cloudskillsboost.google/course_templates/964/quizzes/486479)

### Video - [Introduction to data reports](https://www.cloudskillsboost.google/course_templates/964/video/486480)

- [YouTube: Introduction to data reports](https://www.youtube.com/watch?v=odSkXNwZRck)

As a data analyst, you’ll often use both reports and dashboards when you need to develop data visualizations. They’re the most common visualizations you will create! In this section, you’ll learn more about data reports, how they’re used, their benefits, and some basic differences between reports and dashboards. You’ve likely already spent a bit of time learning about dashboards. But what about reports? Reports is one of those terms that you might think you already understand, but how it's used in data analysis might be a bit different than what you expect. Unlike text-based reports similar to a news article or science publication, a data report is a visualization of detailed business intelligence data for making business decisions. Data reports visually present data insights with a focus to take action on business needs or objectives. Data reports inform and educate, whether it’s a research report presenting the findings of a study, a progress report with project status updates, or a financial report highlighting business performance. Let’s consider the similarities between reports and another visualization type you may be familiar with, dashboards. First, both reports and dashboards are ways to visualize and activate data. They can be built with the same tools and may even look similar. Both can use multiple sources to present data insights. And they make data more accessible to users, who will use them to make decisions with the data, collaborate with colleagues, and share critical insights. Yet, there are important differences between reports and dashboards. Let’s consider these differences and clarify how each is used. The first thing to understand about reports as a data analyst is that they’re usually a curated, static representation. Meaning, unlike dashboards, the data doesn’t change once you generate the report. Think of reports as a snapshot of a specific point in time. In other words, there’s an expiration date to a report’s effectiveness. This is different from dashboards that may be updated in near-real-time. One example of how you can use data reports is for project-specific reporting. Take a large hotel construction project, for example. You could create a report of the construction costs to date, and that information would only be pertinent for that particular point in time. Another key difference is that data reports have more detail than dashboards. Reports typically have more content and more explanation about that content, so they’re much longer. So, while dashboards can fit within the confines of a single screen view, reports might require scrolling through many pages. This combination of length and detail also means it can take users longer to review reports and fully understand what the report includes. This is also a big contrast from dashboards, which are meant to be understood more at-a-glance and without additional context. Both data reports and dashboards have their place within data analytics. Reports are a great tool for users to collaborate and answer specific business questions, especially for one-time questions that require more detail. Dashboards are best for repeated, ongoing, up-to-date data insights. As always, prioritizing your user’s needs will be essential as you determine which type of visualization will work best for each situation.

### Document - [Case study: Benefits of using data reports](https://www.cloudskillsboost.google/course_templates/964/documents/486481)

### Document - [Guide to Looker Studio](https://www.cloudskillsboost.google/course_templates/964/documents/486482)

### Lab - [Create a report in Looker Studio](https://www.cloudskillsboost.google/course_templates/964/labs/486483)

Use Looker Studio to build a report.

- [ ] [Create a report in Looker Studio](../labs/Create-a-report-in-Looker-Studio.md)

### Quiz - [Test your knowledge: Create a report to visualize data insights](https://www.cloudskillsboost.google/course_templates/964/quizzes/486484)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/964/video/486485)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=2ueC15nghw8)

Check out your new cloud data professional skills. You’ve put in great efforts with this section of the program about accessing, exploring, and reporting on data in the cloud! Recently, you spent time identifying the ways in which data visualization tools vary and how to select the best tool for each phase of the data analytics workflow. You also learned how to make the most of cloud data stored in data lakehouses, data warehouses, and other sources. You explored datasets using data visualization tools, and you identified data types, dimensions, and measures in the context of data modeling. Next, you modeled data for visualizations to modify, blend, and aggregate data using data visualization tools. You also considered how to manipulate and organize data in a way that best visualizes insights. Finally, you transitioned into reports to better share data and key insights. You checked out Looker Studio, connected to a data source, explored the dataset, transformed data, and created a report to visualize data insights. Awesome job on your progress. Keep it up!

### Document - [Glossary terms from module 3](https://www.cloudskillsboost.google/course_templates/964/documents/486486)

### Quiz - [Module 3 challenge](https://www.cloudskillsboost.google/course_templates/964/quizzes/486487)

## Enterprise business analytics

In this module, you'll explore needs of enterprise business users including data governance and self-serve analytics. You'll also gain hands-on experience exploring an enterprise data set and building a dashboard to meet a business need. 

### Video - [Welcome to module 4](https://www.cloudskillsboost.google/course_templates/964/video/486488)

- [YouTube: Welcome to module 4](https://www.youtube.com/watch?v=V6J2VzGcUKk)

Greetings, data superstar! I’m happy to welcome you to this section of the program all about using enterprise grade visualization tools used by large, data-driven organizations. You’re about to explore the needs of enterprise users and some cool tools for helping them visualize insights from large datasets. Along the way, you’ll gain practical experience building a dashboard to meet a business need. Next, you'll use visualization tools to explore large datasets. Then, you’ll learn how to work with a real dataset to build a dashboard that meets a specific business need. Finally, you’ll discover ways the cloud data analyst role is evolving thanks to cloud technologies. Alright, it’s time to enter the world of business analytics!

### Video - [Introduction to enterprise-grade visualization tools](https://www.cloudskillsboost.google/course_templates/964/video/486489)

- [YouTube: Introduction to enterprise-grade visualization tools](https://www.youtube.com/watch?v=g-Tg8Q46BZo)

SPEAKER: Enterprise-grade visualization tools can help businesses of all sizes make sense of large amounts of data. In this video, we'll consider what these tools are and how they can be used. But first, let's get a quick definition. An enterprise-grade data visualization tool is a type of data visualization software used by large, data-driven organizations to explore, analyze, and share business analytics. These tools are designed to handle large amounts of data securely, and they make it easy for users of all levels to work with data. Some common features of enterprise-grade visualization tools include-- performance optimization, metadata management, data cataloging, organization-wide metric definitions, self-service analytics, and data governance. Let's dig a little deeper to explore each one. When it comes to data visualization tools, performance optimization matters. Organizations working with high volumes of data need to quickly access information and view reports in near-real time. Enterprise-grade visualization tools are capable of meeting this performance standard, with advanced features that allow users to efficiently access and interpret large amounts of data. Next up, metadata management. Metadata is data about data. It describes data resources and helps users discover, interact with, and learn about the data they're working with. Examples of metadata include the name of the data set, description of a data asset, the user permissions to access the data, and the history of changes made to a data asset. Metadata management is the process of managing, accessing, and organizing metadata. This is essential for businesses that need to find data quickly and collaborate effectively. By managing metadata, organizations can create a standardized way to describe data, which can help teams across the organization collaborate and streamline communication. Next, a data catalog is a centralized inventory of an organization's data assets. Data catalogs collect metadata about these assets, which includes information like the data's meaning, where it came from, its quality, and where it's stored. Data analysts can also use data catalogs to track and manage data assets. Data catalogs list who has access to data, how often it's being used, and if it's being used in accordance with the organization's data policies. Organization-wide metrics are another way enterprise-grade visualization solutions help to make it easier for teams to collaborate and work efficiently. Organization-wide metrics are metrics that are defined and shared across teams. They're typically used to define key performance indicators, or KPIs, like revenue and user satisfaction. Once an organization-wide metric is defined, an entire organization has access to the same metric, helping to ensure everyone in the organization is speaking the same data language and using the same indicators to gauge impact. Let's cover the self-service analytics features that help make it possible to empower users to work and collaborate with data more effectively, as well as make it possible for an organization to empower teams to make the most of their data. Self-service analytics is an approach to business intelligence that allows both technical and non-technical users across an organization to access data, perform ad hoc data analysis, and generate reports. In traditional guided analytics, users rely on data analysts to produce reports. But with self-service analytics, team members from across an organization can directly interact with data and even create their own reports. Finally, as more users interact directly with data, data governance becomes increasingly important for keeping sensitive data safe and ensuring that each person has access to only the data they need. Data governance is a process for ensuring the formal management of a company's data assets. An important feature of enterprise-grade data visualization tools, managing data, is key to ensuring the safe and efficient use of data across an organization, especially when users across the organization are working with and communicating about data. So to recap, enterprise-grade data visualization tools offer a variety of features that make it easy for organizations to make sense of large amounts of data securely. So as you move forward as a cloud data analyst, you're now in a position to know when it's appropriate for the business to recommend enterprise-grade visualization tools.

### Document - [Data governance for safeguarding data](https://www.cloudskillsboost.google/course_templates/964/documents/486490)

### Video - [Comparison of self-service and guided analytics](https://www.cloudskillsboost.google/course_templates/964/video/486491)

- [YouTube: Comparison of self-service and guided analytics](https://www.youtube.com/watch?v=TL_7crPYhwE)

Hi! In this video, you’ll learn about two ways to empower users to make better decisions with data: self-service and guided analytics. Organizations of all sizes are increasingly relying on data-driven decision making, but this has created a new challenge for data teams. More and more people need access to the data, and they want insights fast. To meet this demand, many organizations have turned to self-service analytics. But what is self-service analytics? Self-service analytics is an approach to business intelligence that allows both technical and non-technical users across an organization to access data, perform ad-hoc data analysis, and generate reports. Self-service analytics is often contrasted to a more traditional approach: guided analytics. Guided analytics is an approach to business intelligence where solutions, like reports and dashboards, are created by an analyst or developer to meet a specific business need. Both guided and self-service analytics play an important role in helping organizations deliver meaningful insights to users across teams. The right approach will depend on your specific data needs. Let's check out both. Consider this scenario. You’re hiring a tour guide to take you around Paris. Your tour guide plans out an itinerary for you including the most famous sights, like the Eiffel Tower and the Louvre museum. Because you have an itinerary, you don’t have to worry about details. You can just enjoy the experience. Guided analytics works in a similar way. It provides you with a predefined set of tools and visualizations that help you explore your data and identify insights. With guided analytics, the user identifies a business need that they want addressed with data. The data team then determines the best approach to address the business need and develops a solution. The data team then delivers the solution to the business, ready to help guide their decision-making. Guided analytics is a useful approach because, with the help of an expert guide —the data team—, users can easily access specific, actionable insights. Since the data team is responsible for the data, issues of data governance and privacy are more easily addressed. But, organizations that only use guided analytics often run into two problems. First, handling every request can strain the resources of the data team. They may not have the time or capacity to create solutions for everyone who needs them. Second, waiting for a solution can take too much time, delaying the users’ access to the insights they need for data-driven decisions. Self-service analytics addresses these concerns by allowing business users to interact with and analyze data directly. Instead of hiring a guide, imagine you decide to explore Paris on your own. You have a map of the city and a list of the sights you want to visit. Then, you're free to explore at your own pace and in your own way. Self-service analytics is like a self-guided tour of your data. With self-service analytics, users have direct access to datasets. They can perform ad-hoc analytical tasks, like creating visualizations and running reports. They can explore different ways to utilize the data. This gives users across teams more control over their data analytics and allows them to make data-driven decisions faster. But self-service analytics also has its challenges. One challenge with self-service analytics is that users may only be working with the data in the context of their jobs and responsibilities. This means that they may not be aware of how the data applies to other users, or how it can be used to answer broader questions. Another challenge with self-service analytics is that the self-service solution needs to be user-friendly. If the solution is too difficult to use, users will be less likely to adopt it. That's why it's so important for data teams to work alongside self-service users. They can help users understand the data, identify the right questions to ask, and use the self-service solution effectively. In the end, both self-service and guided analytics can add value and promote data-driven decision-making across the organization. As a cloud data analyst, the approach you choose will depend on the data and needs of your organization and its users.

### Quiz - [Test your knowledge: Enterprise grade data analytics platforms](https://www.cloudskillsboost.google/course_templates/964/quizzes/486492)

### Video - [Visualization tools for enterprise data exploration](https://www.cloudskillsboost.google/course_templates/964/video/486493)

- [YouTube: Visualization tools for enterprise data exploration](https://www.youtube.com/watch?v=I7aKVgzawUI)

Putting together a huge puzzle can be challenging. You start with a bunch of pieces, and it's not immediately clear how they all fit together. So you examine each piece looking for patterns to see how they fit together. But the more you examine the characteristics of each piece, the way to solve the puzzle becomes clear. Data exploration is a lot like putting together a data puzzle. Data exploration is the process of understanding a dataset by inspecting its characteristics, identifying patterns, and asking questions. And when you explore data with a visualization tool, you’ll understand how you can start to answer questions about the data. Let’s check out how this works with an example. This is Arjun. He’s a data analyst at a large clothing company. Because the company’s database is huge, Arjun needs to better understand the data’s structure in order to effectively analyze the data. To do this, Arjun decides to use a data exploration technique called sampling. Sampling is the technique of selecting a segment of a dataset that is representative of the entire dataset in order to better understand its characteristics. Using sampling, Arjun can use a more manageable amount of data to review and start honing in on the specific questions and answers using the dataset. Next, Arjun decides to visually explore the sample of data by graphing the data in a histogram and scatter plot. With this technique, Arjun can spot patterns in the data that may be useful to further analyze the larger dataset. Arjun can also use data visualizations to find outliers and anomalies that may affect data quality, and deal with them prior to performing data analyses. Finally, Arjun decides to go one step further by digging into the details of the data using a technique called data drilling. This allows Arjun to explore the attributes that make up the dataset at a more granular level, which helps Arjun further fine-tune any questions to answer using specific queries. By exploring the data, Arjun is able to put the pieces of a data puzzle together to understand the big picture of the dataset and get more from the data. One important takeaway. As a data analyst, you will not always follow a linear path like Arjun to get the results. Data exploration is actually an iterative process that can vary depending on the data and may even require you to go back and forth between different steps as you learn more about your data. But no matter your approach, data exploration is a critical first step in the data visualization process that will help you understand the data better, save you time and resources, and make sure your analysis is accurate, insightful, and meets your data goals.

### Document - [Insights from a single dataset](https://www.cloudskillsboost.google/course_templates/964/documents/486494)

### Video - [The process of filtering data](https://www.cloudskillsboost.google/course_templates/964/video/486495)

- [YouTube: The process of filtering data](https://www.youtube.com/watch?v=QlOF-G9PSd0)

Hi and welcome to this video on filtering with dimensions and measures. Filtering is the process of limiting the data returned from a query based on specific criteria. It's a powerful tool that can be used to narrow your focus on a specific part of your data. When working with data visualization tools, filtering is usually done on dimensions and measures. Let’s explore how filtering works with each one. Dimensions are unique attributes that help you to describe data. Filtering with dimensions is a great way to narrow down your results and get the insights you need because it lets you limit your results to only the data that has the specific attributes you're interested in. Let’s dig a bit deeper with an example. In this simple 10-row dataset, each row represents a single book. The book attributes include title, author, and price. These attributes are the columns of the dataset. They can also be used to filter the data. To filter on a dimension, you first need to identify the dimension and then set the criteria that must be met. For example, if you want to filter on the "author" dimension, you could set the criteria to return only the author values that match "Chad A." All rows that do not meet this criteria will be filtered out. In the sample dataset, this will return two rows, since Chad A. is the author of "Cache Me if You Can" and "Data Mapping for the Slightly Curious." It's also possible to filter on multiple dimensions, like "author" and "title," using a logical operator. For example, let's say you want to answer the question, "Which rows have both the author name Chad A. and the title 'Cache Me if You Can'?" To answer this question, you would first need to identify the "author" and "title" dimensions. Then, you would use the "and" logical operator to set the criteria so that only the rows where the author value matches "Chad A." and the title value matches "Cache Me if You Can" would be returned. In the sample dataset, this will return only one row because only a single book meets the criteria. Another useful logical operator to use with dimensions is "or". The "or" logical operator is valuable if you want to ask questions like “Which books have an author of Chad A. or have the title of 'To Join or Not to Join?'?" In this case, the dimensions used are “author” and “title,” and the criteria is set to match author values that match Chad A. or titles that match "To Join or Not to Join?" In the sample dataset, this will return three rows, since Chad A. is the author of two books and "To Join or Not to Join?" is the title of one book. When you filter on a dimension, the filtering happens first on the raw data before any calculation. This means that the filter will only be applied to the rows that match the criteria, and the calculations will be performed on the filtered data. But the opposite happens when you filter using a measure. Measures are aggregations of one or more dimensions, like count or average. Measures can be filtered to answer questions like “Which authors have an average book price of more than ten dollars?” In this case, the filtering happens after the calculation. This means that the filter will be applied to the calculated results. Let’s return to the book example. To return the authors that have an average book price of ten dollars or more, first, the rows are grouped by author, then the average is found for each group. Next, these results are filtered based on the criteria, authors with an average book price of ten dollars or more. This returns two authors: Amy T., who has an average price of twelve dollars, and Beatrix P., who has an average price of twenty dollars. Even though the average was found for the other authors, that data is filtered out because it doesn’t meet the criteria. As a cloud data analyst, filtering data using dimensions and measures is a powerful skill that can help you uncover insights in your data. Filtering allows you to focus on the data that's most relevant to you, which can help you answer specific questions and make better business decisions.

### Video - [Data drilling up, down, and through](https://www.cloudskillsboost.google/course_templates/964/video/486496)

- [YouTube: Data drilling up, down, and through](https://www.youtube.com/watch?v=a0WkmaFH22M)

When you take a picture with a camera, you can zoom in on things to focus on details up close, or you can zoom out to get a wider perspective. To do this with data, you can use data drilling. Data drilling is a process that allows users to explore data in more detail by revealing additional levels of information. There are three main ways of data drilling: drill down, drill up, and drill through. But before we dive into how each one works, it’s important to understand dimensional hierarchies. A dimensional hierarchy is a way to define the levels of detail in a dataset that a chart can display. For example, a dimensional hierarchy for time might have the following levels: year, month, week, day, hour. Each level in the hierarchy represents a more granular representation of the data. For example, the "year" level represents the most general level of detail, while the "hour" level represents the most granular level of detail. The order of the levels in the hierarchy is important because it defines how users navigate when they drill into the data. For example, click on the specific “year” level to drill down to the “month” values for that year. By understanding dimensional hierarchies, you can create charts that allow users to drill up and down to explore the data in more detail. Drilling down is a technique that reveals additional levels of detail within a chart by moving down the hierarchy from more general to more granular data. Let’s use an example to explore how drilling down works. Sam, a data analyst at a local community kitchen, looks at a chart that shows the total pounds of food donated by community members each year. To get a more detailed view of the data, Sam uses drill down to view the pounds of food donated by month. The chart now shows the pounds of food donated by month, and each month is represented by a bar. Sam notices that the most food was donated in December, followed by November and October. They also notice that there was a dip in donations in the summer months. By drilling down into the data, Sam is able to get a more detailed view of the pounds of food donated than was evident when looking at the data grouped by year. This information can help Sam to understand how the donations vary over time, and to identify trends and patterns. The opposite of drilling down is drilling up. Drilling up is a technique that reveals fewer levels of detail within a chart by moving up the hierarchy from more granular to more general data. So, back to Sam. Sam reviews a chart that shows the number of people who visit the community kitchen each week. They want to learn how the data changes over a longer period of time, so they drill up to see the data for the entire month. The chart that Sam is viewing shows the number of people who visit each week, so when they drill up, Sam’s moving to the month level. This gives Sam a more holistic view of the data. Drilling up and drilling down are great ways to explore the different levels of detail in a single chart. But if you want to get a more holistic view of data found in multiple charts, use drill through. Drill through is a technique that allows users to navigate to related visualizations. Sam is working with a visualization that shows the different types of food donated, by category, over the last month. One of the categories is cereal, a popular food for community members! Sam wants to know how many pounds of cereal have been used by people who visit the community kitchen, so they click on the cereal label. This takes them to a new report that is pre-filtered to display only information on cereal and helps them quickly find the information they need. This makes it easier for Sam to find the data they are looking for and spot trends in the data. Drilling up, down, and through are all great ways to understand your data from different perspectives. But, like always, the best technique for you will depend on your data and your business needs.

### Quiz - [Test your knowledge: Data exploration of enterprise data sets](https://www.cloudskillsboost.google/course_templates/964/quizzes/486497)

### Video - [Live dashboard features](https://www.cloudskillsboost.google/course_templates/964/video/486498)

- [YouTube: Live dashboard features](https://www.youtube.com/watch?v=7SfXqxPQeFk)

When a traffic event occurs, traffic analysts are on the clock. They need to keep track of a ton of constantly updating indicators from traffic cameras, GPS data, and weather reports to help them predict the flow of traffic and make decisions that will advise people to plan their commutes. A live dashboard is a data visualization tool that provides near-real-time data updates. This can be a valuable tool for traffic analysts who want to keep the traffic moving and commuters informed. But live dashboards are not just for tracking traffic flows. Organizations in many different industries use live dashboards to help them make quick decisions when time is of the essence. Three key features of live dashboards are time-sensitive data, automatic refreshing, and alerting. Let’s break those down. Time-sensitive data, also known as perishable data, is data that must be acted on within a specific time frame, or it loses value. An example of time-sensitive data in action is a stock ticker. A stock ticker is a tool that financial investors use when trading in financial products and stocks. The ticker constantly updates with the latest stock prices. Investors, traders, and brokers monitor this steady flow of information to spot trends in the latest data, and make decisions about whether to buy, sell, or hold. The data in a live dashboard works in a similar way. A live dashboard uses charts and graphs to present time-sensitive data, helping decision-makers identify trends and get answers to urgent business questions. A second key feature of live dashboards is automatic refreshing. Automatic refreshing is a process that enables dashboards to be automatically updated at regular intervals. This gives users an overview of the current state of their metrics at a glance. Next up is alerting. While live dashboards present time-sensitive data that is frequently updated, few teams have the resources to monitor a dashboard all the time. Alerting is a feature that enables dashboard users to receive a notification when predetermined conditions are met or exceeded. Alerting is useful on a live dashboard because it can help data teams detect problems early —even if data team members are not actively monitoring the dashboard— and keep everyone informed when important changes in the data occur. Time-sensitive data, alerting, and monitoring are three features that make live dashboards especially helpful for users to monitor urgent, time-sensitive data, like key performance indicators, or KPIs. Let’s check out how this works with an example. Joe, Zara, and Chang are responsible for their business’ webpage and they use a live dashboard to help keep tabs on their website’s KPIs. Joe monitors the average time spent on each page by the site visitors. One day, Joe notices the time website visitors spend on a page is decreasing. To get another opinion, Joe shares his findings with Zara. Meanwhile, Zara notices page load time increasing. Both insights are based on the most up-to-date data available, so Joe and Zara agree something may be wrong. They decide to share the dashboard with Chang to discuss what to do next. Using the shared dashboard, Chang is able to quickly compare the charts and notices that the changes Joe and Zara observed in the data started at about the same time. Chang agrees that there may be an issue with the website, so the team investigates further. By monitoring their data in near-real-time, the team can spot trends in the data and act quickly. Soon, they’ve found the source of the issue. Now, the data team can apply a fix to keep website traffic flowing. But, what would happen if the team was away at the time? Alerts have them covered. To make sure they get notifications when they're away, the website data team has set up an alert in the dashboard that will notify them when the web traffic exceeds or falls below a certain threshold. This threshold is a value that, if exceeded, indicates that there may be an issue that needs their attention. This way, the team can react quickly to any potential problems, even when they're not actively monitoring the dashboard. Dashboards offer data teams a range of useful applications, from tracking traffic patterns to monitoring website activity. A live dashboard can be a powerful tool that can help teams across an organization have the data they need to make important, time-sensitive decisions and stay on top of trends. In your role as a data analyst, you will be sure to find dashboards a great asset to your work, too.

### Document - [User experiences with dashboards](https://www.cloudskillsboost.google/course_templates/964/documents/486499)

### Document - [Collaborative dashboards](https://www.cloudskillsboost.google/course_templates/964/documents/486500)

### Document - [Dynamic AI-driven dashboards](https://www.cloudskillsboost.google/course_templates/964/documents/486501)

### Document - [Guide to Looker Enterprise](https://www.cloudskillsboost.google/course_templates/964/documents/486502)

### Lab - [Build a dashboard using the Looker Enterprise UI](https://www.cloudskillsboost.google/course_templates/964/labs/486503)

Explore in Looker Enterprise to do in-depth data analysis, integrate insights across different data sources, build actionable data-driven workflows, and create custom data applications to make critical business decisions.

- [ ] [Build a dashboard using the Looker Enterprise UI](../labs/Build-a-dashboard-using-the-Looker-Enterprise-UI.md)

### Video - [The changing role of a cloud data analyst](https://www.cloudskillsboost.google/course_templates/964/video/486504)

- [YouTube: The changing role of a cloud data analyst](https://www.youtube.com/watch?v=AgXSq4a2DPg)

Hi! In this video, you'll learn about the changing role of a cloud data analyst. Organizations have access to so much data that it's revolutionized the way teams across organizations think about and use data to drive decision-making. In the past, data teams were gatekeepers. They managed their organization's datasets and determined how the data could be analyzed. But with the rise of self-service analytics, and the increased use of data-driven decision-making across more teams in an organization, the role of data teams is rapidly changing. Today, instead of gatekeepers, data teams serve as facilitators. Data teams work cross-functionally to ensure the safe, uniform, and efficient use of data for decision-making across an entire organization. The new role has also brought new responsibilities for data teams to improve data literacy across the organization, establish data governance policies and procedures to keep data secure, promote a common data language, and foster collaboration across teams. Let’s dig deeper into these responsibilities. Data literacy is the ability to understand and use data. In many organizations, non-technical employees are being asked to work directly with data. This has increased the need for data literacy across teams. As part of an organization's data team, you might help define the skills employees need to be data literate to fit a specific role and train them to use their data effectively. You may also serve as a resource for teams to help them understand the data and get the most from their data insights. Another way you may help facilitate a data-driven culture in your organization is by working with your team to ensure solid data governance standards are in place. Data governance is a process for ensuring the formal management of a company’s data assets. As more team members have direct access to data, data governance has become an important part of a data team’s responsibilities. One way to govern data is by defining and assigning user roles and responsibilities based on how team members will interact with the data. This makes sure that everyone has the data they need to get their jobs done while also keeping the data safe. As more people work with data across an organization, it's also important to have a common data language. This means that data teams should create definitions so that the entire organization universally understands the terms used to describe data. As part of an organization’s data team, you may be tasked with optimizing and structuring data in a way that promotes the use of a common data language. This will avoid ambiguity and improve data quality. Finally, a data-driven culture is a culture in which both technical and non-technical workers collaborate and feel empowered to make decisions with data. As a cloud data analyst, you will use your data expertise and experience working with various teams to foster collaboration and help teams share data resources, work together with data, and drive decision-making. The role of a data analyst in today's businesses is changing! While an organization’s data team was once thought of as a gatekeeper for the data, today, the data analyst’s role is increasingly becoming one of facilitator. As a cloud data analyst, not only will you work with data directly, you will also work across teams in an organization to help foster a data-driven culture and help others make decisions with data.

### Quiz - [Test your knowledge: Use dashboards to meet business needs](https://www.cloudskillsboost.google/course_templates/964/quizzes/486505)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/964/video/486506)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=pHgGuT7OsEc)

Hi there! You’ve completed another section of this course and are getting even closer to your goal. Great work! As you’ve been discovering, organizations regularly work with high volumes of data. So, they require tools and methods for effectively exploring large datasets, then helping teams realize the full value of that data. With this concept in mind, you found out how to identify enterprise-grade visualization tools’ key features and how they're used to meet business objectives. Then, you discovered why it’s so important to explore large datasets with visualization tools. Next, you learned how to work with a real dataset to build a dashboard that met a specific business need. Finally, you dived into how the role of a cloud data analyst is evolving and the importance to share and collaborate in the modern business environment. Big congrats to you on your progress!

### Document - [Glossary terms from module 4](https://www.cloudskillsboost.google/course_templates/964/documents/486507)

### Quiz - [Module 4 challenge](https://www.cloudskillsboost.google/course_templates/964/quizzes/486508)

#### Quiz 1.

> [!important]
> **A cloud data analyst is using an enterprise-grade data visualization tool to update a visualization. Before they make any changes, the analyst reviews the history of changes made to the data asset. Which enterprise-grade data visualization tool feature is the analyst using?**
>
> - [ ] Metadata management
> - [ ] Organization-wide metric definitions
> - [ ] Data governance
> - [ ] Performance optimization

#### Quiz 2.

> [!important]
> **Which analytics approach allows users to interact with and analyze data directly?**
>
> - [ ] Self-service analytics
> - [ ] Self-guided analytics
> - [ ] Guided analytics
> - [ ] Guide-service analytics

#### Quiz 3.

> [!important]
> **A cloud data analyst is exploring their organization's dataset and wants to identify patterns that can be useful for further analysis. What can they do to achieve this?**
>
> - [ ] Create a histogram and/or a scatter plot from a sample of the dataset.
> - [ ] Focus on understanding the dataset structure first and wait until later to identify patterns.
> - [ ] Combine a sample of the dataset with an external dataset and/or an internal dataset.
> - [ ] Analyze the structure of the data first and then create a sample of the dataset.

#### Quiz 4.

> [!important]
> **A cloud data analyst is working on a visualization for a clothing store. They want to know if their customers are either in Argentina or in Algeria. The dataset that they are using has the following attributes: <span style="background-color:#cccccc;">customer_id,</span> <span style="background-color:#cccccc;">country,</span> <span style="background-color:#cccccc;">age,</span> <span style="background-color:#cccccc;">first_purchase_date.</span> How can the cloud data analyst best answer the question?**
>
> - [ ] By averaging the total number of items purchased in each country.
> - [ ] By filtering the dataset using: <span style="background-color:#cccccc;">country</span> <span style="background-color:#cccccc;">=</span> <span style="background-color:#cccccc;">Argentina</span> <span style="background-color:#cccccc;"><strong>AND</strong></span> <span style="background-color:#cccccc;">country</span> <span style="background-color:#cccccc;">=</span> <span style="background-color:#cccccc;">Algeria.</span>
> - [ ] By counting their customers in each country.
> - [ ] By filtering the dataset using: <span style="background-color:#cccccc;">country</span> <span style="background-color:#cccccc;">=</span> <span style="background-color:#cccccc;">"Argentina"</span> <span style="background-color:#cccccc;"><strong>OR</strong></span> <span style="background-color:#cccccc;">country</span> <span style="background-color:#cccccc;">=</span> <span style="background-color:#cccccc;">"Algeria".</span>

#### Quiz 5.

> [!important]
> **As a cloud data analyst, you want to provide your users with the ability to explore the data from a general view to a granular view. What technique should you use to allow users to explore the data?**
>
> - [ ] Drill down
> - [ ] Drill level
> - [ ] Drill through
> - [ ] Drill up

#### Quiz 6.

> [!important]
> **A cloud data analyst configures a dashboard to gather and process new data every five minutes. Which live dashboard feature is the cloud data analyst using?**
>
> - [ ] Time-sensitive data
> - [ ] Easy configuration
> - [ ] Alerting
> - [ ] Automatic refreshing

#### Quiz 7.

> [!important]
> **One of your responsibilities as a cloud data analyst is ensuring that solid data governance standards are in place. Which of the following strategies is a way to meet this responsibility?**
>
> - [ ] Define and assign user roles and responsibilities that ensure everyone has the data they need while also keeping the data safe.
> - [ ] Control the organization's datasets and decide how the data would be analyzed.
> - [ ] Help team members to understand the data and get the most from their data insights.
> - [ ] Foster collaboration and help teams share data resources and work together.

#### Quiz 8.

> [!important]
> **Your organization decides that users can directly interact with data using an enterprise-grade data visualization tool. What type of analytic approach is the organization implementing?**
>
> - [ ] Self-service analytics
> - [ ] Guided analytics
> - [ ] Ad-hoc analysis
> - [ ] Traditional analytics

#### Quiz 9.

> [!important]
> **A cloud data analyst is building a report and needs to filter the data to find all the customers that have spent more than $150 total on purchases. The data table has the following dimensions: <span style="background-color:#cccccc;">customer_id,</span> <span style="background-color:#cccccc;">country,</span> <span style="background-color:#cccccc;">age,</span> <span style="background-color:#cccccc;">purchase_date,</span> and <span style="background-color:#cccccc;">total.</span> Which of the following filters will the cloud data analyst apply?**
>
> - [ ] Group the rows by customer, then add the total by customer, and finally filter on total > $150.
> - [ ] Count the customers and then filter by a total > $150.
> - [ ] Filter the row by price and then calculate the total per customer.
> - [ ] Aggregate the rows by total and then filter by customer.

#### Quiz 10.

> [!important]
> **A cloud data analyst is preparing a visualization for the marketing department. In the visualization, users will start with a display summarizing all the marketing campaigns. From there, the users can navigate to visualizations with details for each campaign. What technique is the cloud data analyst using?**
>
> - [ ] Drill down
> - [ ] Drill left
> - [ ] Drill up
> - [ ] Drill through

## Explore the developer environment

In this module, you'll learn to model dimensions and measures using LookML and explore the use of LookML to improve the user experience and address complex business needs. This module also covers how to optimize performance.

### Video - [Welcome to module 5](https://www.cloudskillsboost.google/course_templates/964/video/486509)

- [YouTube: Welcome to module 5](https://www.youtube.com/watch?v=d2rO_PkvkDs)

Hello, and welcome to this section of the course all about developing data models and addressing complex business needs! As a data analyst, when you create visualizations with a user interface, you can build key components with a click and not have to worry about writing code. But, to meet more complex business needs, a data modeling language is very helpful. You’ll learn all about that coming up! First, you’ll learn about data team members who work with datasets and data modeling languages. These data team members are known as developers. You’ll explore the developer role and environment, and the tools developers use to work efficiently. Next, you’ll get into data modeling languages and how they help meet business needs. You’ll also gain experience working with a modeling language to define dimensions and measures. Then, you’ll learn more about how developers view and work with dashboards as code. Finally, you’ll explore ways data modeling languages are used to address complex business needs including derived tables and caching. Exploring data modeling languages and the underlying code are great ways to better understand what’s going on behind the scenes when you build a visualization. With this new knowledge, you’ll be able to make your dashboards and reports work even better for your stakeholders. Let’s get started!

### Video - [Bring data tools together with an IDE](https://www.cloudskillsboost.google/course_templates/964/video/486510)

- [YouTube: Bring data tools together with an IDE](https://www.youtube.com/watch?v=sHU33ErELB0)

When working on a project, it's useful to have a user-friendly set of tools that you can use together to help you get the job done efficiently. For example, you’re writing a note to a friend and you want to draft this personal correspondence with a typewriter. You may need to use a dictionary to check the spelling and find and correct any mistakes! This can be time-consuming and error-prone. These days, using a word processing app is a popular way to write a letter. This software often has automated features that can check the grammar, spelling, and formatting! This saves time and prevents mistakes. Like word processing apps, an IDE can help developers get their job done. An IDE, or integrated development environment, is an application that brings together the tools needed for development in a user-friendly environment. In this video, you will learn how to use an IDE to develop your visualization projects efficiently. Let’s dive in with an example. Min is developing a visualization project using a data modeling language. To improve workflow and help save development time, Min uses an IDE that’s integrated into the developer interface of her enterprise-grade visualization tool. To develop the model, Min enters code directly into a code editor. Like a word processor, an IDE’s text editor makes it easier to write in a specific language and includes editing and formatting options. So, when Min enters code in the data modeling language Look ML, the code editor’s autocomplete predicts the code Min may want to use. This saves time and helps Min write more efficiently. Min can also check and fix her syntax quickly thanks to a built-in syntax highlighter that’s designed specifically for the language she is using. An IDE is also useful for debugging code. The IDE’s built-in debugger highlights possible errors in the code and suggests fixes. This makes it easier for Min to track down errors in the code quickly and spot issues that may otherwise be hard to find. This helps Min write cleaner, better functioning code. Min is also able to easily find the metadata she needs because the visualization tool is integrated into Min’s development environment. This gives Min information about the model’s structure and helps Min write code efficiently. Min can also use the IDE to access files for the project, all in one place. This is helpful because it saves Min time and effort, and keeps the project organized. Finally, when Min wants to track code changes and deploy the code, Min can use the version control tools integrated within the IDE. Version control makes it easier for Min to collaborate and share code with other members of the data team for feedback. While the exact features vary depending on the IDE used to develop your project, an IDE is a great way to more efficiently work with code and have all the resources you need to do your job in one place.

### Document - [The elements of LookML](https://www.cloudskillsboost.google/course_templates/964/documents/486511)

### Video - [The benefits of using version control](https://www.cloudskillsboost.google/course_templates/964/video/486512)

- [YouTube: The benefits of using version control](https://www.youtube.com/watch?v=XN7Zk5hy5Nk)

Using a data visualization user interface, or UI, can be a quick and easy way to make changes directly to a deployed dashboard. But, it's not always the easiest way to manage dashboards. When you make changes to the deployed dashboard, it can be difficult to track down any bugs that may be introduced. Additionally, if two or more developers are making changes to the dashboard at the same time, they may accidentally overwrite each other's work. To prevent errors from reaching the deployed dashboard, and to make it easier to collaborate, some development teams use a process called version control. Version control is a process to track changes to your code, data, or other files over time. Using version control can be a great way to manage changes for a complex visualization project. To start, let’s break down the parts of a version control project. First, a repository is a central location for storing and managing the files and history of a project. When developers use version control to store a visualization project, creating a repository is usually the first step. This can be done locally, but repositories are often hosted on a remote server. This repository becomes the central location, or single source of truth, for all the changes developers make to the project. In a repository, developers make all changes to a project in branches. A branch is a working copy of a repository. Like a tree branch that splits off the trunk, a branch in version control splits off the main project code. This allows the branch to grow in its own way, and form an independent development path. When a developer creates a new branch, it's a duplicate of the main project code. The developer can then use this copy to make changes to the code. But, since the branch is independent, any changes the developer makes to the copy do not impact the main project, or other branches. Instead, the changes are local to only that branch. This makes version control a really powerful tool when developers need to collaborate on large-scale projects and want to review changes made to the dashboard before it goes live. Let’s explore an example. Min and Carl are part of a team building a complex dashboard. They've each been assigned an important feature to work on. To start, they each create their own branch in the shared repository. This creates a separate copy of the code that they can work on independently. This way, they can work on their assigned features without impacting anyone else's work. They can also use their branches to collaborate. So, let's say Carl gets stuck and needs help deciding how to implement a feature. Carl shares a branch with Min, who can then comment on Carl's code and provide feedback. Min can help Carl get unblocked and move forward. Min and Carl can also share their branches with other team members to test the code and spot potential bugs, or errors, before the code goes live. This is a great way to collaborate and ensure the code is high quality. When the work in a branch is complete and tested, the changes Min and Carl made can then be merged back into the main project code and deployed to the live project. Using branching, then, allows teams to independently work on features, collaborate, and test their work before the dashboard goes live. Developing complex visualization projects like dashboards can be a lot of work. And if you're working on a team, it can be even more challenging to keep track of changes and make sure everyone is on the same page. But with version control you can track changes over time, collaborate with others, and keep your projects organized.

### Quiz - [Test your knowledge: Getting started with IDEs](https://www.cloudskillsboost.google/course_templates/964/quizzes/486513)

### Video - [Introduction to data modeling languages](https://www.cloudskillsboost.google/course_templates/964/video/486514)

- [YouTube: Introduction to data modeling languages](https://www.youtube.com/watch?v=QxYhDJ7XXk8)

Hey there, and thanks so much for joining me for this video on data modeling languages. First, some quick definitions. A data model is a concept for organizing data elements and how they relate to one another. It can be visually represented to show how the data is structured and how it relates to each other. But what is a data modeling language? A data modeling language is a tool used to create and represent semantic data models. And, to dig a little deeper, a semantic data model is a type of data model that uses everyday language to represent data. This makes it easier for users to understand the meaning of the data and how it relates to each other. As a cloud data analyst, you have many data modeling languages available to build semantic data models, and each one is unique. But, there are three common modeling language features that make data modeling languages a good choice for building data models, regardless of the language you choose: abstraction, modularity, and efficiency. Let’s explore each one. Abstraction is a concept used to understand complex concepts and ideas by focusing on the most essential parts. Working with data can be complex. But data modeling languages use abstraction to reduce this complexity. Using a data modeling language, developers can focus on the essential parts of data modeling: designing a model that is accurate, efficient, and reusable. Having an easy-to-use data model also helps end users interact with data without having to worry about the complexity of accessing it. Instead, they can focus on getting their work done. Let’s take an example to see how this works. As a developer, when using the data modeling language Look ML to create a dimension, you describe the name, type, and the SQL statement needed. But you do not need to know exactly how to implement it. Look ML can help! The underlying Look ML engine uses this definition and determines how it will work. That’s why data modeling languages are easy to learn and use. Plus, when developers use Look ML to define their business logic as user-friendly dimensions and measures, it makes it easier for users to explore the data with confidence. Data modeling languages are also modular. Modularity is a concept that breaks down a system into smaller parts that can be easily separated and reused. For developers, a key benefit of modularity is having parts that can be separated into smaller, self-sufficient chunks. Modularity makes it easier to work on large projects, especially when working as a team. Reusable components can also save time and keep both definitions and projects consistent. For example, when using Look ML, you can create a measure once and then reuse it as many times as you need in the same project or even in another project entirely. Reusing a measure saves developer time, and ensures business users have a single source of truth. Better yet, the measure works the same way each time it's used! Finally, data modeling languages can provide efficiency to your workflow. The ability to reuse measures and even entire data models saves time and effort when developing new systems. Many data modeling languages include built-in validators that allow you to check code syntax and catch errors before the code is deployed. Data modeling languages also allow you to generate documentation for the models. This helps to communicate important information about the data models and other components to data team members who may need to reuse elements that you’ve built. As a developer, the data modeling language you choose will depend on the task at hand and the tools you're using. But no matter which language you pick, data modeling languages are a valuable tool that can help you improve data quality and build more effective models.

### Document - [Define dimensions and measure with LookML](https://www.cloudskillsboost.google/course_templates/964/documents/486515)

### Document - [Guide to LookML](https://www.cloudskillsboost.google/course_templates/964/documents/486516)

### Lab - [Model dimensions and measures using LookML](https://www.cloudskillsboost.google/course_templates/964/labs/486517)

Use Looker to create new measures and dimensions

- [ ] [Model dimensions and measures using LookML](../labs/Model-dimensions-and-measures-using-LookML.md)

### Video - [Data modeling languages for business needs](https://www.cloudskillsboost.google/course_templates/964/video/486518)

- [YouTube: Data modeling languages for business needs](https://www.youtube.com/watch?v=zyv2MIG1UVM)

Hi and thanks for joining me! In this video, you’ll learn ways a data modeling language can be used to address on-the-job problems and help users make the most of their data. Data modeling languages can help improve data quality and increase efficiency. But to be useful to an organization, data modeling languages must help meet business needs. A data modeling language is a tool used to create and represent semantic data models. As a data analyst, you’ll work with enterprise data and use data modeling languages. These languages offer you three primary advantages. They can be used to define fields in the semantic layer in a user-friendly way, create a single source of truth, and build visualizations that meet specific business needs. Let’s examine examples of each benefit. First, you use a data modeling language to add a semantic layer on top of the data. A semantic layer is a set of definitions and logic that helps ensure that everyone in the organization understands the data in the same way. The semantic layer defines fields describing the structure of data. For example, a data team for a large nonprofit can use a data modeling language to define dimensions and measures to create common definitions across their donor data. This means that everyone in the nonprofit who needs to use the data will share the same vocabulary, which can help prevent misunderstandings and errors. It can also make it easier for users across teams to communicate about the data. Second, you can use a data modeling language to easily create a single source of truth so users throughout the organization can access the same data at the same time. For example, a hospital data team can use a data modeling language to build a data model that brings patient data together into one place. Then, using that data model they can create interactive dashboards to allow doctors and nurses from the hospital to have access to all the information they need. This allows the nurses and doctors to share the same information about patients and their care, enabling them to make timely informed decisions and help improve patient outcomes. Third, data modeling languages allow you to build visualizations for a specific purpose that can help lead to data-driven decision making. For example, say a financial services team needs to track the performance of their investments in near-real-time. Using a data modeling language, the data team can define the structure and layout of a dashboard and any interactive features and visualizations to help financial services find the information they need swiftly. The data team can provide financial services a dashboard with visualizations that meets the needs of the financial services team. This dashboard helps financial services access and interact with the information they need to make data-driven decisions quickly, maximize returns, and protect their clients’ investments! Whether you’re part of a non-profit, hospital, or financial services firm, as a cloud data analyst, you will be tasked with using data to meet business needs and help teams collaborate more efficiently with their data. Data modeling languages are a great way to do this because they help teams better communicate and understand data. This keeps everyone on the same page!

### Quiz - [Test your knowledge: Data modeling in the developer environment](https://www.cloudskillsboost.google/course_templates/964/quizzes/486519)

### Video - [Discover dashboards as code](https://www.cloudskillsboost.google/course_templates/964/video/486520)

- [YouTube: Discover dashboards as code](https://www.youtube.com/watch?v=Fh99CC_17-Q)

Welcome to this video on dashboards as code. The dashboards-as-code approach is a hot topic in the data visualization world, and for good reason. Data visualization tools have made it simple for even non-technical users to create dashboards and other visualizations with ease. But as the number of dashboards within an organization grows, dashboards can be a challenge to keep track of! That's where the dashboards-as-code approach comes in. Dashboards as code is an approach to managing dashboards by defining them in code. This makes it easier to track changes in your dashboards, test your features before making them live, and reuse entire dashboards again and again. Why manage dashboards using the dashboards-as-code approach? The simple answer is that the approach enables data teams to deliver visualizations that users can trust because, like a software product, the visualizations do not go live before the data team iterates, reviews, and tests the dashboard. Consider, for example, a user makes changes to a dashboard with a visualization tool’s user interface. Every time the user makes a change, the change goes live immediately. And if the change introduces an error, there's no way to easily revert it back to a previous version. For large teams that work with multiple dashboards, this can make it hard to maintain dashboards and fix mistakes when they happen. In contrast, dashboards as code works with the underlying code that makes up a dashboard, not the user interface. Using a dashboards-as-code approach means that developers build a dashboard using a language of their choice, store the code using version control, and then deploy the dashboard when they’re ready to go live. Then when developers need to change the live version of the dashboard, they make changes to the underlying code in a developer environment. This allows the data team to track and peer review any changes before the changes go live. There are three advantages when developers track changes and review dashboards. First, since developers are tracking changes, they can easily roll a dashboard back to a prior version if a problem arises. Second, developers can easily comment on and review changes to reduce dashboard errors. And since the dashboards are stored as code, developers can reuse dashboards across different tools easily just by importing the code. Third, developers can test and validate features —or even entire dashboards— to make sure dashboards work as intended before going live. Ultimately, this ensures only dashboards of the highest possible quality reach users. It’s also important to call out three potential challenges with the dashboards-as-code approach. First, as a data analyst, the approach can have a steep learning curve if you're not familiar with coding. Second, if you're using a visualization tool that doesn't support dashboards as code, you'll need to find a different tool or learn how to code in a different language. Third, it can be time-consuming to build entire dashboards using code. Dashboards as code is a powerful approach that can help organizations manage their dashboards effectively. But before deciding on a dashboards-as-code approach, it's important to weigh the pros and cons to decide if it's truly a good fit for your organization and data needs.

### Document - [Explore a dashboard file’s LookML code](https://www.cloudskillsboost.google/course_templates/964/documents/486521)

### Quiz - [Test your knowledge: Manage visualizations with dashboards as code](https://www.cloudskillsboost.google/course_templates/964/quizzes/486522)

### Video - [Derived tables for complex data problems](https://www.cloudskillsboost.google/course_templates/964/video/486523)

- [YouTube: Derived tables for complex data problems](https://www.youtube.com/watch?v=WZm0tahy7Jk)

Hey there! In this video, I'll demonstrate how to solve complex data problems using derived tables. As a cloud data analyst, you usually write queries to request specific sets of data from existing tables in a database. But, sometimes, these queries can get complex. And what if the data you need isn't readily available in a table that exists in the database? That's where derived tables come in. A derived table is a query whose results are used as if it were an actual table in the database. But derived tables aren't independent queries. They are nested within an outer query, which means that they cannot be executed on their own. The outer query uses the results of the derived table to create its own results. A derived table’s job is to gather the specific data you need to answer the question posed by the outer query. That data is stored as a virtual table, and the outer query can use the information just like it was a regular, stored table in the database. And once the outer query is finished running, the virtual table is usually deleted, and is not stored in the database. Let’s explore how this works. Liz is a data analyst at a large non-profit. To better understand donor behavior, Liz was asked to identify two types of donors. First, “megadonors,” who gave more than $10,000 total in the past year. Second, “frequent donors,” who donated more than three separate times during the same period, regardless of the amount. But this exact data does not exist in a table in the donors database. So, to get the data she needs and make her query easier to read, Liz decides to use a derived table. First, she identifies the data she needs to run the outer query successfully. Then, she writes the query for the derived table. This query gathers only that information that is useful for the outer query. In Liz's case, the query they write for the derived table groups the donations by donor ID and then aggregates the data to count the number of donations and total amount donated for each donor. Liz then writes an outer query that uses the results of the derived table to find the information. The derived table query is nested inside. When Liz runs the outer query, first the nested query executes and gathers the information needed into a derived table. Then, the outer query uses that information just like it would an actual, stored table to answer a data question. In this case, the outer query uses the information from the derived table to identify both “megadonors” and “frequent donors” over the last year. Once the outer query is completed, the derived table is usually discarded. So, every time the query is executed, the virtual table is created again. Derived tables have both limitations and benefits. First, we’ll cover limitations of derived tables. Because a derived table must usually be created from scratch each time the outer query is run, it can potentially impact performance, especially if the derived table is complex. Derived tables are not usually persistent, meaning they are not stored in the database and not reusable in other queries. Instead, if you want to use the results of a derived table in another query, you need to recreate the derived table as part of that query. But despite the limitations, derived tables offer several benefits for data professionals. Derived tables simplify complex queries and break down complex queries into modular parts. This makes the queries easier to write, read, and execute. You can also use derived tables with enterprise grade visualization tools. This enables users to perform complex calculations on data and even create custom views and reports. Needless to say, derived tables provide a powerful way to work with data using data visualization tools! As a cloud data analyst, derived queries can help you solve complex data problems, and ensure your queries are simple but effective. Although derived tables are a great tool, it's important to use them wisely. Make sure you weigh the tradeoffs between the performance of the derived table and how to best meet the business need with your data.

### Document - [Types of derived tables](https://www.cloudskillsboost.google/course_templates/964/documents/486524)

### Video - [Improve performance with caching](https://www.cloudskillsboost.google/course_templates/964/video/486525)

- [YouTube: Improve performance with caching](https://www.youtube.com/watch?v=XzYR0r1zca4)

When it comes to data visualizations, speed and accessibility matter. Slow load times can hurt users’ ability to make timely decisions with data and meet their business goals. So, to make sure people have access to the data when they need it, cloud data analysts leverage caching. Caching is the process of storing data in a temporary location so it can be accessed more quickly in the future. As a cloud data professional, you can use caching to keep frequently-used data in memory. Caching has three benefits that improve the performance of your visualizations: reduce traffic to the data source, minimize load time, and maintain availability. Let’s explore each of these benefits. The first benefit of caching is that caching stores a query's results in memory for a period of time to reduce traffic to the data source. This reduces the number of queries sent to a data source, reducing traffic. For example, a large warehouse data team uses a dashboard to monitor time-sensitive inventory data. Without caching, every time a data team member interacts with the dashboard and requests information about the inventory, a query is sent to the data source for the specific inventory information. The results return, and refresh and populate the visualization. But this can cause a lot of requests being made to the data source, especially when there's many users or the data is accessed frequently. Plus, the same data is likely to be requested and returned more than once. That’s inefficient! To help reduce traffic to the data source, the data team may enact a caching policy. A caching policy is a set of rules that determine how long cached results are stored in memory and when they are refreshed. With a caching policy in place, the data team decides how often the database should refresh the dashboard data. This reduces the number of queries issued to the database. Here’s how it works. Before sending a new query to the database, the memory is searched to determine if the results have been cached or stored in the memory. If a cached copy of the data is available, it's used for the visualization and no query is issued to the database. If not, then a query is sent to the data source, and the results are used in the visualization and stored in memory for a period of time. So, instead of running a query directly against the database every time the data is requested, a query is run once and then the results are stored in memory where they can be quickly accessed again. The second benefit of caching is that it can help minimize load times. Caching can also minimize the load time of visualizations because the data can be loaded from memory, not the data source. This can save significant time, especially when working with a lot of data. The third benefit of caching is that it can help maintain the availability of data visualizations by storing results in memory. This can be helpful in situations where the data source is unavailable or when the internet connection is unstable. But, while caching is a great way to reduce traffic to the data source, minimize load time, and maintain availability, as a cloud data professional, there are some considerations you should keep in mind. First, caching can lead to stale data. To prevent this, it's important to set a time limit for caching. Meaning, the time frame is long enough that the retrieved data will actually be used in a visualization, but not so long that the data will be out of date. Second, you need to monitor caching. This means you need to make sure users are accessing up-to-date data and the cache is working properly to store data and delete it when it's expired. Caching can help improve the performance of your visualizations, reduce load time on the database, and help make sure the data is always available to your users. But not all caching policies are the same. So, when considering a caching policy, it's important to make sure that the policy works for your organization's data needs.

### Quiz - [Test your knowledge: Address complex business questions and optimize performance](https://www.cloudskillsboost.google/course_templates/964/quizzes/486526)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/964/video/486527)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=lXvKQgYFxT8)

Wow! You've achieved another milestone in your journey to becoming a cloud data analyst. You're really showing your dedication to the field! You’ve learned all about how developers work with data modeling languages to build custom data models and meet complex business needs for their users. You also explored the role of a developer on a data team and the tools developers use to get the job done, including IDEs and version control. And you delved even deeper into data modeling languages and learned about building custom dimensions and measures. You also explored ways developers view and work with dashboards as code. Finally, you discovered ways developers can use a data modeling language to address complex business needs by creating derived tables and enabling caching. Congratulations again! It’s been awesome sharing these lessons with you!

### Video - [Lauren and Andrew: Interview role play](https://www.cloudskillsboost.google/course_templates/964/video/486528)

- [YouTube: Lauren and Andrew: Interview role play](https://www.youtube.com/watch?v=vZmOefTJsr0)

Hi, I'm Lauren. Hi, I'm Andrew. It's time to take a look at another interview that's in progress. In this interview, the questions will cover cloud data analytics topics related to data visualization in the cloud. We hope this will help you know what to expect in your next interview. (no audio) What interests you in cloud data analytics? What are you looking for in your next role? What really interests me about cloud data analytics is really diving into the raw data and telling a story. There is such a gratifying experience from really digging into the levers that happen in real life that you can then see reflected in the data, and then using that to tell a story to then really drive like an actual change to the business or to a process that you're working on. That kind of impact, basically using numbers and code, is something that's always been very interesting to me. So in a new role, I'm definitely looking for a challenging environment where feedback and iteration are abundant. I think the more learning I'm exposed to, the more excited I feel. Imagine this scenario: a stakeholder made a request to create a dashboard to meet key performance indicators. What are the steps you would take to ensure the dashboard could be used by the stakeholder? First, I'd really wanna get familiar with the stakeholder, understand their familiarity with the data, understand their specific business needs, all while using the SMART approach, so specific, measurable, action-oriented, relevant, and time-bound, in hopes of uncovering needs and considerations that that stakeholder hadn't yet thought of in this exploratory process. The positive takeaway would be a visualization that the stakeholder could then use to drive meaningful and impactful change in their project or process. Tell me about a time you created data visualizations. What visualizations did you create, and how did you decide what visualizations to use when representing the data? I decided to create dashboards embedded with pie charts because I took the time to understand my audience, and I realized that they weren't very technical. And so a lot of the metrics and data I needed to present in this visualization needed to be high-level enough and easily filterable so that the stakeholder or the user could be able to take all the insights that they needed to in that. And I specifically chose to use reputable and strong data sources with adequate access control so that the people viewing that dashboard could only see the data that was relevant to them. The takeaway I had was to really focus on the audience or the user, because then, that would, in a sense, retroactively inform all of the visualizations I would end up making. What are some common visualization types, and when would you use them? Some common visualization types are: pie charts, bar charts, line graphs, waterfall charts. And the context of how I would use them would really depend on the stakeholder and their needs. So if we really wanted to represent change over time, that might mean a line graph or a bar chart. If we wanna say what pieces make up a whole, then a pie chart might serve a better purpose in that sense. But it really has to start with understanding the stakeholder. - Great, do you have any questions for me? A question I have is: If you had a disagreement between a junior analyst and a manager, how would you manage it? That's a really great question. It's very important to me that everyone on our team and everyone within our working groups feels comfortable and safe working together. So the first thing I would do is talk to both the junior analyst and the manager separately. I'd gather the facts. There's always two sides to every story. I'd really wanna get a better understanding of what actually happened. I would encourage them to speak with each other privately, but of course, offer my support if they would like me there. Everyone has different working styles. Everyone has different ways of doing things. And my hope is that, through discussion and through open conversation, we could get to the right resolution. In this scenario, Andrew demonstrated how to end responses with positive takeaways. You too can talk about challenges that you faced and end with the positive takeaways that you had.

### Document - [Interview tip: End responses with positive takeaways](https://www.cloudskillsboost.google/course_templates/964/documents/486529)

### Document - [Glossary terms from module 5](https://www.cloudskillsboost.google/course_templates/964/documents/486530)

### Quiz - [Module 5 challenge](https://www.cloudskillsboost.google/course_templates/964/quizzes/486531)

### Video - [Course wrap-up](https://www.cloudskillsboost.google/course_templates/964/video/486532)

- [YouTube: Course wrap-up](https://www.youtube.com/watch?v=4Jc1P7RR6AI)

A huge congratulations on finishing this course! I love telling stories with data! One of the most satisfying parts of my job is hearing customers say, “Wow, I had no idea that was happening” or “Ah, so now it makes sense.” As a cloud data analyst, you’ll have the opportunity to tell your own data stories and teach others to leverage the power of data. Let’s revisit all the cool things you learned in this course. You started by learning about storytelling and how to captivate your audience using insights from the cloud. Then, we got into the basics of user experience, or UX, and user interface, or UI, design to create compelling communication. Next, you learned about the implications of data types on design, including how data types inform design choices, and examined common design patterns used in business intelligence dashboards. Then, you explored ways to work with stakeholders to translate requests into visualizations that get results. You then discovered how to access and explore data for visualizations. You learned about dimensions and measures, and how to model data for visualization. You also learned to work with a visualization tool to build a simple dashboard. Next, you learned how to work with large datasets and use business data visualization tools that can work on a large scale, integrated security features with robust functionality that fits within the IT infrastructure. This involves strategies for fostering a data-driven culture, and keeping data organized and secure. Finally, you explored the developer environment, practiced how to model data with a data modeling language, learn tools to write dashboards as code, and use a data modeling language to address complex business needs. You are now prepared to champion data literacy across your organization by knowing how to visualize data in a way that’s intuitive for both technical and non-technical users. Congratulations again! You’re now ready to move on to the last course!

### Document - [Course 4 resources and citations](https://www.cloudskillsboost.google/course_templates/964/documents/486533)

### Document - [Glossary terms from Course 4](https://www.cloudskillsboost.google/course_templates/964/documents/486534)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
