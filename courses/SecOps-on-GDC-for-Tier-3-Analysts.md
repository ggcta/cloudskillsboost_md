---
id: 1197
name: 'SecOps on GDC for Tier 3 Analysts'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1197
date_published: 2025-01-24
topics:

---

# [SecOps on GDC for Tier 3 Analysts](https://www.cloudskillsboost.google/course_templates/1197)

**Description:**

This course gives you a deep dive into the workflows of Tier 3 analysts.

**Objectives:**

* Describe tasks and tools you will use in workflows for vulnerability management, threat modeling, and security engineering.

## Course Overview

This module introduces the audience, prerequisites, and agenda for the learning pathway.

### Video - [Course 3 Overview](https://www.cloudskillsboost.google/course_templates/1197/video/522078)

* [YouTube: Course 3 Overview](https://www.youtube.com/watch?v=WR6Kh75KF50)

SPEAKER: Welcome to the Google Distributed Cloud, or GDC air-gapped Security Operations Fundamentals series of courses. Security itself is an all-encompassing endeavor. Within each of the three courses that make up this series, you'll focus on a specific aspect of GDC for security operations. The first course provides a high-level overview of security fundamentals on the GDC platform. You'll be introduced to the GDC offering and architecture, the SecOps roles in the GDC security operations center-- or SOC-- and the definitions of the security principles. You'll also learn about the day-to-day processes and tools that you can use to keep the GDC deployment secure, both proactively and reactively. Finally, you will review the default logs, dashboards, and alerts, which are at the core of GDC security monitoring. The second course provides you with a deep dive into the workflows of Tier 1 and Tier 2 security analysts. These workflows are monitoring, intake and incident response. You will go through a variety of video demonstrations that mimic how you would tackle these workflows in GDC. The third course provides you with a deep dive into the workflows of Tier 3 analysts. This workflow will focus on vulnerability management, threat modeling, and security engineering. Once again, you will watch video demonstrations on how you would tackle these activities. To benefit fully from taking this course, you should have these prerequisite skills and knowledge-- prior understanding of SecOps, basic proficiency with Windows and Linux logs, basic understanding of Kubernetes terminology for logging, prior completion of the Google Cloud Fundamentals Core Infrastructure course, or equivalent experience with Google Cloud Services hosted by GDC. Throughout this series of courses, you'll learn about GDC, in context, by exploring how Cymbal Federal, a fictional organization, accomplishes its SecOps goals using the GDC air-gapped platform. Imagine that you have just joined the GDC Security Operations Center team for Cymbal Federal as a security analyst. Congratulations on your new role. Cymbal Federal is a government entity aligned to support the broad mission objectives of the executive branch of government. During a recent review, the operation and application capabilities of Cymbal Federal were designated mission critical. An organization-wide, multi-year, digital transformation program is underway. Cymbal Federal aims to ensure compliance while modernizing its suite of applications with GDC. Here is the SOC organization structure for Cymbal Federal. Cymbal Federal has opted to onboard a threat hunter, a vulnerability manager, and a security engineer as Tier 3 analysts. Cymbal Federal has also been considering introducing an insider threat function, but has decided to focus on proactively securing the platform against external threats first. As a result, Cymbal Federal has opted to rely on existing strict internal processes for safeguarding national security and classified information as the initial mitigation against internal threats. In this course, you will focus on how Cymbal Federal can best leverage threat modeling, vulnerability management, and security engineering at the organization. You will also learn about the additional role of the Tier 3 analyst, as the point of escalation for Tier 2 analysts during advanced incident response. The Cymbal Federal case study attempts to mirror an enterprise-level scenario, and aims to help you understand how the different components of the platform fit together and what their capabilities are. You will revisit this case study throughout the course. This course will offer a variety of videos, presentations, quizzes, and video demonstrations. By the end of this course, you will be able to elevate your cybersecurity expertise as a Tier 3 analyst. Let's get started with advanced incident management for Tier 3 analysts.

## Advanced Incident Response

This module introduces advanced incident response, and discusses the challenges of handling an incident beyond existing runbook processes. You will also learn how to devise an ad hoc plan for incident response, select appropriate tools for an advanced incident response as well as create effective runbook processes.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1197/video/522079)

* [YouTube: Module overview](https://www.youtube.com/watch?v=QeDtdOJtItY)

SPEAKER: Welcome to Advanced Incident Response. In this module, you will learn about advanced incident response that requires tier three expertise. First, you'll gain an understanding of what advanced incident response entails. Then you will learn about the challenges associated with manually managing security incidents that go beyond the scope of existing runbooks. This includes exploring the steps required to create an ad-hoc incident response plan for when unexpected incidents occur. You will learn about the tools that can enhance your incident response capabilities. Finally, you will learn how to develop effective runbook processes to improve your incident response.

### Video - [Advanced incident response for Tier 3 analysts](https://www.cloudskillsboost.google/course_templates/1197/video/522080)

* [YouTube: Advanced incident response for Tier 3 analysts](https://www.youtube.com/watch?v=ZgJ0OOIeYZQ)

SPEAKER: In course 1 and 2 of GDC Airgapped Security Operations Fundamentals, you explored the role of tier 1 and tier 2 analysts during incident response. In this course, you will learn about the responsibilities of tier 3 analysts and how they help manage advanced incidents. Advanced incident response encompasses the use of sophisticated strategies and technologies to address complex cybersecurity threats that go beyond the current runbook processes. Tier 3 analysts offer expertise for in-depth analysis and effective resolution of high-impact incidents. As a tier 3 analyst, you're tasked with unraveling issues that have left both tier 1 and tier 2 teams perplexed. Imagine diagnosing and remedying a network outage caused by an uncommon conflict between the latest firewall firmware and the established network setup. Tier 3 analysts offer expertise that is essential to effectively responding to and recovering from complex threats. Let's quickly explore what a tier 3 analyst can offer. Tier 3 analysts offer specialized knowledge in specific technical domains. This enables you to address problems that demand intricate understanding. A typical scenario could involve using sophisticated forensic tools to salvage data from a damaged database without losing data integrity. Tier 3 analysts also offer problem solving for complex issues. You are the last line of defense against advanced cybersecurity threats. An example is disarming a zero-day exploit in an essential business application before it's exploited by cyber criminals. Tier 3 analysts are exceptionally skilled in root cause analysis. For example, following a significant service interruption, a tier 3 might analyze the event to find a hidden memory leak in third-party middleware. This will prompt tier 3 to take action to prevent similar issues. Tier 3 analysts also have in-depth expertise in strategy and technology development. For example, as a tier 3, you may devise a state-of-the-art defense against ransomware by integrating machine learning to identify unusual network traffic patterns that are indicative of an attack. Tier 3 analysts also offer mentorship and play a key role in elevating the capabilities of tier 2 analysts. For example, you may guide a tier 2 through the intricate troubleshooting of a client's custom application, thereby sharpening a tier 2's technical prowess. Finally, tier 3 analysts play a key role in ensuring stability and security. For example, by establishing a strong security framework for remote connections, you can thwart potential security breaches. You thereby secure the organization's digital resources from unauthorized access.

### Video - [Ad hoc incident response](https://www.cloudskillsboost.google/course_templates/1197/video/522081)

* [YouTube: Ad hoc incident response](https://www.youtube.com/watch?v=cScG_v19IsU)

SPEAKER: In the previous video, you explored the type of expertise that Tier 3 offers. Now, let's consider the challenges associated with advanced incident response. A Tier 3 analyst may often be required to manually attempt to solve an incident that is not covered in existing runbooks. This is known as ad hoc incident response and requires specialized expertise. Numerous challenges exist that can impact the efficiency and effectiveness of resolving advanced incidents for which runbooks don't exist. Understanding these challenges is crucial for anyone looking to improve their incident response processes. Let's take a moment to review these challenges. Ad hoc advanced incident response faces significant challenges due to the lack of preparedness and predefined procedures. This results in inconsistent and potentially delayed actions that may not adequately address or mitigate the sophisticated nature of the threat. Ad hoc incidents are also managed manually, which can result in ineffective communication. Without automated processes, it can be challenging to efficiently share information among team members, stakeholders, and clients. This can result in crucial information being missed or misunderstood, thus further complicating the resolution process. Advanced incidents may also offer reduced visibility because they employ sophisticated techniques like encryption and stealth tactics to evade detection. These methods, including polymorphic malware and legitimate tool exploitation, complicate traditional monitoring, making the malicious activities blend seamlessly with normal traffic. Creating runbooks for advanced incidents can be challenging because these threats often involve novel, sophisticated techniques that evolve rapidly. This makes it difficult to anticipate and document specific response procedures for every potential scenario. This can further slow down response time from Tier 3 analysts. Tier 3 analysts may also struggle to maintain historical data when working on advanced incident response. This, again, is due to the complex and evolving nature of advanced threats, which can overwhelm existing data management and analysis systems. This hinders effective tracking and correlation of incidents over time, which impacts your ability to perform trend analysis, identify recurring problems, and implement preventative measures. Lastly, manual incident response can lead to increased staff burnout. The high-risk nature of the incident, coupled with extended working hours, can significantly stress your team. This can affect morale, as well as overall effectiveness when undertaking incident response. For Tier 3 analysts to address these challenges, they need to adopt a proactive approach. This will be explored in the next video.

### Video - [Using an incident management plan for ad hoc incident response](https://www.cloudskillsboost.google/course_templates/1197/video/522082)

* [YouTube: Using an incident management plan for ad hoc incident response](https://www.youtube.com/watch?v=--WiGPcFZp0)

SPEAKER: In the previous video, you were introduced to the challenges of ad-hoc incident response. Creating a documented plan helps Tier 3 analysts address these challenges. To start, Tier 3 analysts will brainstorm a customized incident response plan, likely with the support of the SOC manager and extended team, which outlines clear next steps, roles, and responsibilities. This incident response plan will be designed based on the incident at hand and to fill gaps that exist in current runbooks. You can follow the general best practice guidelines outlined in the incident management plan to draft this plan of attack. So how do Tier 3 analysts create and use an incident management plan for ad-hoc advanced incident response? While in this case a runbook does not exist, a Tier 3 may choose to refer to the guidelines outlined in the incident management plan. These recommendations can be used as inspiration for deciding on the plan of attack for the advanced incident response. Let's explore the process. First, the incident management plan provides you with information on its own scope, what it aims to achieve, its boundaries, and the types of incidents the plan will cover. This clarity is vital for ensuring everyone involved understands the plan's objectives and limitations. Next, you will want to assign roles and responsibilities. This is crucial, as you will need to establish a defined hierarchy and clarify duties to streamline the incident response process. The response team section of the incident management plan should also explain how you integrate Tier 3 expertise when addressing complex scenarios that demand a high level of technical skill. Then you will craft step-by-step procedures for your incident response plan. The goal of doing this is to create a detailed guide that will walk your team through the entire process, from incident detection to resolution, and include manual strategies for complex scenarios that don't fit the standard mold. Next, your incident response plan will need to outline internal and external communication protocols to ensure timely updates for all stakeholders. You will also need to outline real time collaboration tools to use to keep communication flowing smoothly among team members and external parties. Then, you will need to set up a documentation process for handling incidents, both during and after incident response. This includes outlining how incidents are reported and the structure for post-incident reports. Including documentation, is crucial for analysis and for outlining future improvements to your response strategy. Finally, you'll need to regularly review and update the drafted plan, integrating insights as the incident response progresses. This step ensures that your plan remains relevant and effective while keeping the incident response in motion.

### Video - [Tools for advanced incident response](https://www.cloudskillsboost.google/course_templates/1197/video/522083)

* [YouTube: Tools for advanced incident response](https://www.youtube.com/watch?v=IbDf-37WS24)

SPEAKER: Advanced incident response tools are crucial for improving manual incident response. These tools provide the tier 3 analysts the capabilities they need to adapt and react effectively when facing unforeseen security challenges. When managing advanced incidents, there's a variety of tools at your disposal that can really make a difference. Examples include monitoring tools, alerting tools, ticketing systems, and post-incident analysis tools. These tools are all designed to help you overcome some of the challenges noted in earlier videos, such as delayed response time, ineffective communication, reduced visibility, and difficulties in maintaining historical data. Advanced incident response tools can detect issues and promptly notify you, keep tasks organized, and help you learn from past incidents. This all contributes to a smoother and more effective incident response. Here is a list of common tools you will use for advanced incident response. In the following videos, you'll explore each one in order to understand their function, explore an example, and uncover best practices.

### Video - [Endpoint Detection and Response (EDR) tools](https://www.cloudskillsboost.google/course_templates/1197/video/522084)

* [YouTube: Endpoint Detection and Response (EDR) tools](https://www.youtube.com/watch?v=FrjlbroLyRc)

SPEAKER: In this video, you'll explore endpoint security tools. These tools aim at protecting endpoint devices like desktops, laptops, and mobile phones from potential threats. Endpoint Detection and Response, EDR, tools play a pivotal role in identifying and countering malicious activities. EDR tools offer continuous monitoring and recording of endpoint activities. They enable security teams to swiftly detect, investigate, and respond to threats. EDR tools provide deep visibility into endpoint events and are used by tier 3 analysts to trace an attacker's actions and pinpoint the incident's root cause. It is appropriate to select EDR tools when you need granular insights into suspicious activities on endpoints and require the capability to respond rapidly to identified threats. Let's consider an example. The network at Cymbal Federal includes a variety of endpoint devices, including desktops, laptops, and smartphones. One day, an employee clicks on a phishing email link, unintentionally initiating the download of ransomware onto their laptop. How can EDR tools help? Detection-- as soon as the ransomware begins to execute on the employee's laptop, the EDR tool detects the suspicious behavior. This detection is possible because EDR solutions continuously monitor and record activities on endpoints, looking for patterns or actions that deviate from the norm. Alerting-- the EDR tool immediately alerts the security team about the potential threat and provides detailed information about the suspicious activity. This alert includes specifics about the file's origin, behavior, and which endpoints are affected. Analysis-- security analysts use the EDR tool to quickly dive into the data surrounding the incident. The tool provides a comprehensive timeline of the ransomware's actions, from the initial download to its attempted spread across the network. This level of detail helps identify how the ransomware entered the system, which vulnerabilities were exploited, and whether any other devices are at risk. Response-- the security team will use the detailed insights from the EDR tool to isolate the affected laptop from the network in order to prevent the ransomware from spreading. The security team can then execute a response plan to remove the ransomware from the compromised device. They will leverage EDR's capabilities to reverse the malicious actions and restore affected files from backups. By understanding when to select these tools and following best practices, you can significantly enhance your organization's ability to manage endpoint security, ensuring a robust security posture. When using endpoint security tools, keep the following best practices in mind. Ensure your endpoint security solutions are seamlessly integrated with alert management systems. This is to facilitate a swift response from security and IT teams upon detection of an incident. Implement continuous monitoring practices to detect and respond to threats in real time, thus minimizing potential damage. Keep your endpoint security tools updated and patched to protect against the latest vulnerabilities and threats. Educate your team on the significance of endpoint security, and ensure they are familiar with EDR and procedures for responding to incidents. Develop a comprehensive incident response plan that includes guidelines on how and when to use endpoint security tools effectively during an incident.

### Video - [Security Information and Event Management (SIEM) tools](https://www.cloudskillsboost.google/course_templates/1197/video/522085)

* [YouTube: Security Information and Event Management (SIEM) tools](https://www.youtube.com/watch?v=10tI9UasoeA)

SPEAKER: Next, there are Security Information and Event Management, SIEM, tools. SIEM tools are engineered to analyze security events and alerts in real time. These events and alerts are sourced from an array of devices such as network hardware, firewalls, antivirus systems, and intrusion-detection systems. The primary function of SIEM tools is to collect, aggregate, and normalize log data from these diverse sources in order to unearth patterns that might signal a security incident. It is appropriate to select SIEM tools if your organization needs to analyze and correlate data across various security and network devices. SIEM tools are also the right choice if you require real-time visibility in order to quickly identify and respond to threats. Finally, you may select SIEM tools if you need to comply with industry regulations by maintaining and reviewing security event logs. Let's consider an example. There's been a significant spike in data traffic from a seemingly innocuous internal server at Cymbal Federal. Under normal circumstances, this server exhibits low activity. However, this unusual behavior triggers the SIEM system to generate an alert based on the patterns it has learned to recognize in regards to potential security incidents. How can SIEM tools further assist in incident response? Detection and alerting-- the SIEM tool, having aggregated and analyzed logs from across Cymbal Federal's network, identifies the anomaly and promptly alerts the cybersecurity team. This quick detection is critical as it potentially indicates a compromised server being used to exfiltrate data. Analysis-- the cybersecurity team receives the alert and uses the SIEM tool to dive into the aggregated log data from the server, and the correlated events from the network's firewalls and intrusion detection systems. The SIEM's capability to normalize and analyze data from various sources enables the team to swiftly make sense of the sequence of events leading up to the anomaly. Response-- the team uses the insights from the SIEM analysis to discover that the server has been compromised due to an unpatched vulnerability that is now being used in a data exfiltration attempt. The team immediately isolates the server from the network in order to halt a potential data leak. The team begins remediation efforts. Forensics and improvement-- the team neutralizes the immediate threat. The team then conducts a forensic analysis using the data collected by the SIEM tool in order to understand how the breach occurred and why it was not initially prevented. This analysis helps Cymbal Federal patch vulnerabilities, adjust security policies, and refine the SIEM's alerting thresholds to improve detection and prevent similar incidents in the future. SIEM tools enhance your ability to detect and respond to incidents in real time. They also provide critical insights that can help improve your overall security strategy. Remember, the effectiveness of a SIEM system greatly depends on its configuration and the ongoing engagement of your security team in tuning and managing the system to adapt to new threats and organizational changes. Keep the following best practices in mind. Ensure that your SIEM tool is properly configured to collect data from all relevant sources within your network. This comprehensive view is crucial for effective analysis. Regularly update the rules and definitions within your SIEM tool to keep pace with the evolving threat landscape. Incorporate threat intelligence feeds to automate this process. Use the machine learning and anomaly detection capabilities of your SIEM tool to identify subtle and unusual patterns that simple rule-based systems might miss. Configure your SIEM tool to prioritize alerts based on severity and relevance to your environment. This ensures your security team focuses on the most critical issues first. Make use of the visualization and reporting features of your SIEM tool to regularly review your security posture. This can aid in investigating incidents and demonstrating compliance with regulatory requirements.

### Video - [Vulnerability scanners](https://www.cloudskillsboost.google/course_templates/1197/video/522086)

* [YouTube: Vulnerability scanners](https://www.youtube.com/watch?v=oX3fsAd0uFI)

SPEAKER: Vulnerability scanners are another tool available to tier 3 analysts. These scanners identify vulnerabilities and also help tier 3 analysts prioritize them by offering a ranked list based on potential impact. This prioritization is essential for focusing your efforts on addressing the most critical vulnerabilities first. These scanners identify vulnerabilities and also help tier 3 analysts prioritize them by offering a ranked list based on potential impact. This prioritization is essential for focusing your efforts on addressing the most critical vulnerabilities first. This enables you to optimize your mediation process. Additionally, some vulnerability scanners come equipped with patch management features. These features streamline the process of applying necessary patches and updates to vulnerable systems. It is appropriate to use vulnerability scanners when your organization needs to schedule periodic scans to continuously monitor systems for new vulnerabilities. You should also run scans after deploying new systems or applications, or when you make significant changes to your network. This is to ensure no new vulnerabilities have been introduced. Finally, vulnerability scanners can help you gather the necessary data to demonstrate compliance with various security standards and regulations. Let's consider an example. Cymbal Federal's IT infrastructure is vast and multifaceted, comprising numerous servers, workstations, and network devices. Each of these potentially harbors exploitable vulnerabilities. Cymbal Federal is aware of the risks associated with these vulnerabilities. The company decides to deploy a state-of-the-art vulnerability scanner across its network. How can it help with incident response? Vulnerability identification-- the vulnerability scanner begins its operations by methodically scanning the network. The scanner identifies a wide range of vulnerabilities across various systems. These vulnerabilities include outdated software, misconfigurations, and flaws that could potentially be exploited by attackers. Prioritization of vulnerabilities-- after the scan is complete, Cymbal Federal is presented with a comprehensive report detailing the vulnerabilities. Unlike a simple list, this report ranks the vulnerabilities based on their potential impact on the organization's network security. This list uses criteria such as the exploitability, severity, and criticality of the affected system. This prioritization enables Cymbal Federal to focus resources on addressing the most dangerous vulnerabilities first. Patch management-- the vulnerability scanner employed by Cymbal Federal includes integrated patch management capabilities. This feature automates the process of downloading and applying patches for the identified vulnerabilities. This significantly reduces the window of opportunity for attackers to exploit these flaws. The scanner also schedules post-patch scans in order to ensure that the patches have been successfully implemented, and that no new vulnerabilities have been introduced. By effectively incorporating vulnerability scanners into your cybersecurity strategy, you're taking a proactive step towards identifying and mitigating potential security threats before they can be exploited. This not only enhances your organization's security posture, but also supports compliance efforts and reduces the risk of a successful cyber attack. Keep the following best practices in mind. Ensure your vulnerability scanning encompasses all systems and networks within your organization. This avoids blind spots in your security posture. Assess the context of each vulnerability within your specific environment to prioritize remediation efforts effectively. Not all vulnerabilities pose the same level of risk to every organization. Leverage the capability of vulnerability scanners to automate and schedule scans. This ensures continuous security monitoring without manual intervention. If your vulnerability scanner offers patch management capabilities, integrate these processes to streamline the patching of identified vulnerabilities. Regularly review the reports generated by your vulnerability scanner. Take prompt action on the identified vulnerabilities according to their prioritization.

### Video - [Threat intelligence tools](https://www.cloudskillsboost.google/course_templates/1197/video/522087)

* [YouTube: Threat intelligence tools](https://www.youtube.com/watch?v=sDlZfOy9J14)

SPEAKER: Tier 3 analysts can also use threat intelligence tools. Threat intelligence platforms are designed to furnish your organizations with actionable information. This includes indicators of compromise, IOCs, profiles of threat actors, and tailored advice to counter specific threats. This information is pivotal for preemptive defense strategies. Furthermore, threat intelligence platforms can integrate with other security tools, such as SIEM systems and endpoint security solutions. These combined insights enable you to enhance your overall security posture by informing and automating responses to identified threats. It is appropriate to select threat intelligence platforms if you're looking for advanced warning about emerging threats and vulnerabilities that might impact your organization. Threat intelligence platforms are also a good choice if you need to understand the tactics, techniques, and procedures, or TTPs, of threat actors targeting your sector. Finally, threat intelligence platforms integrate with your existing security systems, providing these systems with the latest threat data. This improves your overall detection and response capabilities. Let's consider an example. Cymbal Federal faces sophisticated cyber threats ranging from state-sponsored espionage to advanced persistent threats, also known as APTs. To bolster defenses, Cymbal Federal integrates a threat intelligence platform into its cybersecurity framework. What does this mean for Cymbal Federal? Gathering actionable intelligence. The threat intelligence platform begins by aggregating data from a variety of sources, including open-source intelligence, forums, and feeds, to track the latest threats. The platforms analyze this data to produce actionable intelligence, such as indicators of compromise, also known as IOCs. These could include suspicious IP addresses, URLs, or malware hashes. Profiling threat actors-- beyond IOCs, the threat intelligence platform provides in-depth profiles of threat actors targeting sectors relevant to Cymbal Federal. This includes their tactics, techniques, and procedures, TTPs. This information enables Cymbal Federal to understand and anticipate the methods that might be used against them. Tailored advice for countermeasures-- the threat intelligence platform offers tailored advice suggesting specific countermeasures and defensive strategies based on the identified threats and Cymbal Federal's unique security posture. This could involve recommendations for configuring firewalls, updating intrusion detection systems, or implementing stronger access controls. Integration with existing security tools-- Cymbal Federal's threat intelligence platform is fully integrated with the organization's existing security infrastructure. This includes the SIEM system and endpoint security solutions. This integration allows for the automated dissemination of intelligence across the network. This enables proactive adjustments to security measures and automates responses to detected threats. By incorporating threat intelligence platforms into your cybersecurity strategy, you empower your organization with the knowledge needed to proactively defend against potential attacks. You also tailor your security measures to counter relevant threats and thus significantly enhance your incident response capabilities. Keep the following best practices in mind-- focus on information that directly impacts your organization and that you can act upon to enhance your security measures. Leverage the capability of threat intelligence platforms to integrate with your existing security tools. This provides a more cohesive and informed response strategy. Participate in industry-specific threat intelligence sharing communities or platforms to benefit from shared knowledge and contribute your own insights. This strengthens collective defense strategies. Ensure that the threat intelligence feeds you subscribe to are regularly updated to reflect the latest threats and vulnerabilities. Educate your security team on interpreting and utilizing threat intelligence effectively, ensuring they can translate insights into actionable defense mechanisms.

### Video - [Intrusion Detection and Prevention Systems (IDPS) tools](https://www.cloudskillsboost.google/course_templates/1197/video/522088)

* [YouTube: Intrusion Detection and Prevention Systems (IDPS) tools](https://www.youtube.com/watch?v=yYLtRJtM2jw)

SPEAKER: Intrusion Detection and Prevention Systems, IDPS, stand out for their critical role in monitoring and safeguarding network traffic and system activities. IDPS tools are engineered to detect and mitigate potential security threats in real-time. These tools employ a variety of detection techniques, such as signature-based detection, relies on known patterns of malicious activity, behavior-based detection looks for deviations from normal activity, indicating potential threats. Anomaly detection identifies unusual patterns that may signal an intrusion. IDPS tools also take automated actions to prevent further damage. These actions can include blocking traffic or terminating connections that are deemed malicious. Whether deployed on premises or in the cloud, IDPS solutions offer flexibility to suit different organizational needs and can be a crucial component of a comprehensive security strategy. You will rely on IDPS tools if you need to continuously monitor network traffic and system activities for potential threats. IDPS tools are also a good choice if you need to automatically block or mitigate detected threats in real time. IDPS tools are ideal for organizations looking to enhance their security posture with advanced threat detection capabilities. Let's consider an example. Imagine that Cymbal Federal is reinforcing its cybersecurity measures to protect sensitive data and critical infrastructure from cyber threats. As part of this initiative, Cymbal Federal decides to implement IDPS tools in order to enhance the monitoring and security of its network traffic and system activities. Let's explore this further. Selection of IDPS tools. Cymbal Federal evaluates various IDPS solutions, considering their detection techniques and deployment options. Cymbal Federal opts for a hybrid approach, integrating both signature-based and behavior-based detection, along with anomaly detection capabilities. This ensures comprehensive coverage against a wide range of threats. Deployment. The selected IDPS solution is deployed across Cymbal Federal's network infrastructure. This includes critical endpoints, network gateways, and cloud environments. This ensures all entry points and data flows are under continuous surveillance. Operational integration. Signature-based detection is configured to monitor for known patterns of malicious activity. It uses a constantly updated database of signatures to recognize established threats. Behavior-based detection is set up to analyze the regular activity patterns within the agency's network. This enables the IDPS to spot deviations that may indicate compromised systems or insider threats. Anomaly detection algorithms are employed to identify unusual network traffic or system behavior that could signal a novel or sophisticated cyber attack. Automated response mechanisms. The IDPS is configured to take immediate automated actions upon detecting potential threats. This includes blocking suspicious network traffic, isolating affected systems, and terminating unauthorized connections. By incorporating IDPS tools into your cybersecurity arsenal, you enhance your organization's ability to preemptively detect and respond to security threats, thus safeguarding your systems and data from potential compromise. Keep the following best practices in mind. Ensure the detection capabilities of your IDPS are up to date with the latest threat intelligence and signatures. This ensures you can effectively counter new threats. Adjust the IDPS settings to match your organization's network behavior, reducing false positives and focusing on relevant threats. Integrate IDPS with SIEM systems, threat intelligence platforms, and other security tools for a coordinated and comprehensive security posture. Periodically review the IDPS logs and the effectiveness of its response actions. This is to ensure optimal performance and to adjust strategies as needed. Equip your team with the necessary skills to manage and respond to IDPS alerts. Ensure they can distinguish between false alarms and genuine threats.

### Video - [Digital forensic tools](https://www.cloudskillsboost.google/course_templates/1197/video/522089)

* [YouTube: Digital forensic tools](https://www.youtube.com/watch?v=KvkpJqOX_W8)

SPEAKER: Finally, digital forensic tools enable you to piece together an attacker's actions, identify what data was compromised, and assess the full extent of the damage caused by an incident. The toolkit for a digital forensic investigator includes the following-- forensic imaging software for creating exact copies of data without altering it, data recovery tools to retrieve lost or deleted data, analysis software to scrutinize the collected data for evidence. You will work with digital forensics tools in the aftermath of a security breach in order to uncover the details of how the breach occurred and the extent of the data compromised. You will also use these tools when there is a need to gather evidence for legal proceedings or compliance audits following an incident. You will also use digital forensic tools to conduct in-depth investigations into complex security incidents that require detailed analysis in order to understand the attacker's techniques and motives. Let's consider an example. You are a Tier 3 analyst at Cymbal Federal, and have been tasked with conducting a digital forensic investigation following a sophisticated cyberattack. This scenario highlights the integral role of digital forensics in understanding and mitigating the impact of security incidents. So how do you use digital forensic tools? Initial assessment-- upon being alerted to the incident, you conduct an initial assessment to understand the scope and potential entry points of the attack. This step is crucial for determining the direction of the forensic investigation. Evidence preservation-- you use forensic imaging software to create bit-by-bit copies of all affected systems and storage devices. This process ensures that you have exact replicas of the data as it existed at the time of the incident. This allows for analysis without risking the integrity of the original evidence. Data recovery-- with the copy secured, you employ data recovery tools to retrieve lost or deliberately deleted data. This step often uncovers hidden information about the attack, such as malware artifacts, tampered files, or logs that the attacker attempted to erase. In-depth analysis-- you use sophisticated analysis software to scrutinize the collected data in detail. This includes examining file signatures, analyzing logs for suspicious activity, and reconstructing the sequence of events leading up to, during, and after the attack. Your goal is to piece together the attacker's actions, identify the methods used to penetrate the network, and determine what data was accessed or exfiltrated. Damage assessment-- based on the evidence gathered, you assess the full extent of the damage. This involves identifying which data was compromised, estimating the impact on Cymbal Federal's operations, and determining whether any sensitive information was leaked. Reporting and recommendations-- you compile a comprehensive report detailing your findings, including the techniques used by the attackers, the vulnerabilities exploited, and the data affected. You also provide recommendations for mitigating the damage and preventing similar incidents in the future. Examples include enhancing security measures, patching identified vulnerabilities, and improving incident response protocols. By incorporating digital forensics into your incident response strategy, you enhance your ability to conduct thorough investigations, derive actionable insights from incidents, and improve your organization's resilience against future attacks. When using digital forensics tools, keep the following best practices in mind. Given the complexity of digital forensics, it's crucial that those who operate these tools have specialized training and expertise to effectively collect and analyze digital evidence. Maintain a clear and documented chain of custody when using digital evidence. This is essential for its admissibility in legal cases. Always follow strict procedures for handling and preserving evidence. Digital forensics tools should be used in conjunction with other incident response tools like SIEM and IDPS. This is to ensure a comprehensive approach to security incident response. Regularly update digital forensics tools and techniques to keep pace with new technologies and threats. Ensure that all collected digital evidence is securely stored and preserved in an unaltered state. Use forensic-grade storage solutions and practices.

### Video - [Advanced incident response tools at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1197/video/522090)

* [YouTube: Advanced incident response tools at Cymbal Federal](https://www.youtube.com/watch?v=zzh12P9nysM)

SPEAKER: Let's consider a final example from Cymbal Federal in order to see advanced incident resolution tools in action. This example will showcase how the various tools can be used to solve an incident. Cymbal Federal faces an advanced cyberattack targeting its network infrastructure. The attack aims to exfiltrate sensitive data and employs a combination of malware, phishing, and exploitation of vulnerabilities. Endpoint Detection and Response, EDR, tools can be employed for real-time monitoring of endpoints, such as desktops and mobile devices. In the case of Cymbal Federal, you can use EDR tools on employee devices to detect the execution of the malicious attachment from the phishing email. You can immediately isolate the affected endpoints and prevent malware from spreading. EDR tools enable you to quickly detect and isolate affected devices, prevent the spread of malware, and aid you in the identification of the attack vector. SIEM systems aggregate and analyze log data from across the network. This includes data from firewalls, antivirus systems, and IDS/IPS. In this example, the SIEM system aggregates logs from across the Cymbal Federal network, including EDR alerts, firewall logs, and IDS alerts. This provides real-time visibility into the attack. SIEM analytics correlate the suspicious activities to pinpoint the attack patterns and alert the Cymbal Federal tier 3 security team. By correlating events and identifying unusual patterns, the SIEM tool will alert you to the broader scope of the attack. This helps you pinpoint anomalies that suggest a coordinated effort to breach the network. After detecting an initial threat, you'll rely on vulnerability scanners to identify and prioritize existing network vulnerabilities that may have been exploited during the attack. In this case, a phishing attack has been detected. Tier 3 analysts use vulnerability scanners to identify instances of the Log4Shell vulnerability across Cymbal Federal systems. Patch management features facilitate the rapid application of necessary updates to mitigate vulnerabilities and strengthen defenses. You use Cymbal Federal's threat intelligence platform to uncover actionable information on Indicators Of Compromise, IOCs, and profiles of the suspected threat actors. This intelligence enables Cymbal Federal to enhance their detection rules and security measures to protect against similar attacks in the future. Cymbal Federal can use IDPS to monitor network traffic and system activities for signs of intrusion. The IDPS detects unusual network traffic patterns indicative of the exploitation attempt of Log4Shell and automatically applies rules to block further exploitation attempts. IDPS uses signature-based, behavior-based, and anomaly detection techniques. This enables Cymbal Federal tier 3 analysts to identify and automatically block malicious traffic and activities. This effectively contains the attack and prevents further damage. Finally, you, as a simple federal tier 3 analyst, undertake a digital forensics investigation to piece together the attackers actions and assess the full extent of the damage. You use forensic imaging software, data recovery tools, and analysis software to create exact copies of affected data, retrieve lost or deleted information, and scrutinize the evidence. This comprehensive analysis enables you to uncover how the attackers breached the network and the data that was compromised. You can later assist in developing strategies to prevent future incidents.

### Video - [Metrics for evaluating incident response](https://www.cloudskillsboost.google/course_templates/1197/video/522091)

* [YouTube: Metrics for evaluating incident response](https://www.youtube.com/watch?v=KganFDelIBg)

SPEAKER: In this video, you will learn how Tier 3 analysts apply their specialized expertise to develop and update runbooks, in order to improve incident response plan effectiveness. As a Tier 3 analyst, you play a pivotal role in both resolving advanced incidents and leveraging those experiences to fortify incident response processes. Your responsibilities include not only the hands-on management of complex cybersecurity threats, but also the critical analysis and integration of those experiences into operational playbooks. After navigating through advanced incidents, you are tasked with updating runbooks, ensuring they reflect the cutting-edge tactics, techniques, and procedures-- TTPs-- that proved effective. Additionally, you harness insights gained from incidents to develop and refine key performance metrics. These metrics are instrumental in measuring the efficacy of incident response strategies. This includes identifying areas for improvement and guiding training initiatives. Your dual focus on both immediate incident resolution and continuous process enhancement is essential for elevating your organization's cybersecurity posture and response readiness. Let's explore the different metrics you can use. The first metric is MTTR, Mean Time To Respond. This is the amount of time it takes for an incident to be resolved from the moment it is reported. This can reveal how efficient your response plan is, and if your alerting tools are effectively mobilizing your response team into action. The MTTR metric indicates the efficiency of the response plan and reflects the effectiveness of alerting tools in mobilizing the response plan. The Mean Time Between Failures, MTBF metric, can help you determine how well your team is identifying and mitigating vulnerabilities within the organization. This metric helps you assess how effectively a team identifies and mitigates vulnerabilities. Next is the Mean Time To Detect, MTTD. This is the average amount of time it takes for an incident to be detected after it occurs. This metric typically reveals the health of your monitoring systems and how well teams can identify critical issues. This metric can be paired with the Mean Time To Acknowledge. Mean Time To Acknowledge, MTTA, is a key performance metric used in various fields, including IT service management and cyber security incident response. This metric measures the average time taken from when an incident or alert is reported, until it is acknowledged by a responsible party. This metric is crucial for assessing the responsiveness of incident response teams or IT support teams. It indicates how quickly an organization can begin addressing an issue or a security threat. The Incident Escalation Rate is how often incidents are escalated to higher level management. This metric measures the frequency of incidents being escalated to higher level management. This can help indicate when there is a need for improvement in the incident response process. High escalation rates may signal issues within the current incident handling procedures. Resolution Accuracy measures the frequency of reopened incidents. This metric enables you to determine how accurately incidents are diagnosed and remediated. It reveals the effectiveness of post-incident reports and how well the team is able to analyze the reports. The Customer Satisfaction metric is paramount to an organization's reputation. It helps identify areas for improvement in incident response. For example, there may be complaints about prolonged resolution times, indicating that response times must be reduced. Understanding and effectively using key performance metrics can significantly enhance an organization's operational efficiency and customer service quality. Metrics like MTTR, MTBF, MTTD, and MTTA are crucial for assessing and improving the efficiency of your incident response team, as well as system reliability and the speed of incident detection and acknowledgment. As a Tier 3 analyst, monitoring these metrics-- along with incident escalation rate, resolution accuracy, and customer satisfaction-- enables you to identify areas requiring improvement. This can contribute to quicker incident resolution, increasingly stable operations, and enhanced customer satisfaction.

## Vulnerability management for Tier 3 analysts

In this module, you will learn to define the role and benefits of vulnerability management and explain how to assess them. You will also learn to describe vulnerability scanning and penetration testing, evaluate when to use which vulnerability tool, and outline best practices for vulnerability management.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1197/video/522092)

* [YouTube: Module overview](https://www.youtube.com/watch?v=9BwAy22lS5o)

SPEAKER: Welcome to Vulnerability Management for Tier 3 Analysts. In this module, you will learn how a tier 3 analyst typically performs vulnerability management and how this could apply to Google Distributed Cloud, GDC, airgapped. First, you will learn what vulnerability management is, why it's important, and the benefits it brings to organizations. Next, you will explore methods and strategies that tier 3 analysts can use to effectively assess vulnerabilities within an IT environment. This includes vulnerability scanning, penetration testing, and deploying specific vulnerability tools based on the context and objectives of your security assessments. Finally, you'll learn about the recommended best practices for efficiently managing vulnerabilities in order to maintain a secure IT environment.

### Video - [Introduction to vulnerability management](https://www.cloudskillsboost.google/course_templates/1197/video/522093)

* [YouTube: Introduction to vulnerability management](https://www.youtube.com/watch?v=V4i2QRo8Alw)

SPEAKER: Vulnerability management is a process that you, as part of the SOC, can use to help find, assess, address, and keep track of security weaknesses in your organization's computer systems and software. These weaknesses, also known as security vulnerabilities, are essentially flaws in your technology that could potentially be exploited by cyber attackers. Attackers can use these vulnerabilities to gain unauthorized access to your systems, devices, networks, databases, or applications, potentially compromising your data and causing various security issues. Within a typical corporate network, vulnerabilities can exist at different levels and in different parts of your technology infrastructure. The role of vulnerability management is to continuously monitor these weaknesses within your corporate environment. This ongoing process involves identifying the most serious vulnerabilities and then organizing efforts to respond to them effectively. Vulnerability management aims to reduce the risk of potential cyber attacks by prioritizing and addressing the most critical vulnerabilities first. Vulnerability management ensures that your organization is proactively taking steps to strengthen its security posture and protect sensitive data. Put simply, vulnerability management helps you maintain the security and integrity of your systems and data by identifying and mitigating potential weaknesses.

### Video - [Vulnerability management systems (VMS)](https://www.cloudskillsboost.google/course_templates/1197/video/522094)

* [YouTube: Vulnerability management systems (VMS)](https://www.youtube.com/watch?v=AFeibSdeh9k)

SPEAKER: A Vulnerability Management System, or VMS, is a critical cybersecurity tool that you can use in your vulnerability management efforts. A VMS is like your cybersecurity watchdog. The VMS uses risk data to identify the most critical security weak points in your digital world. When the VMS finds them, it doesn't just sit there. It takes a good look at what's going on around those weak spots. The VMS considers how your business works, whether there are any cyber threats lurking around, and the risk associated with each vulnerability. Then, the VMS provides you with recommendations on how to make your IT infrastructure and management procedures more secure. Let's explore why VMS is so beneficial. With a VMS in place, you can proactively discover and assess vulnerabilities within your IT infrastructure before cyber attackers have a chance to exploit them. This proactive approach allows you to address these weaknesses promptly. This reduces the chances of a security breach during security events. A VMS provides valuable information to incident response teams. This means that when a security incident occurs, responders have a clear understanding of the organization's vulnerabilities. This knowledge helps security response teams more effectively identify the root cause of the incident, and implement the right countermeasures to contain and mitigate the threat. The VMS also provides regular reports and analytics that detail the status of the organization's vulnerability management efforts. It offers valuable visibility into the organization's security posture, and it allows you and other stakeholders to make informed decisions about security, investments, and risk management strategies. This ensures that your cybersecurity stays robust. Finally, a VMS encourages a culture of continuous improvement. It prompts the organization to regularly review and refine vulnerability management processes. This ongoing improvement ensures that your cybersecurity posture remains strong and adapts to the ever-evolving threat landscape. This helps you stay ahead of potential risks and threats. You should now understand the benefits of a vulnerability management system. In the following video, you'll learn how this relates to an organization's vulnerability management program. A vulnerability management program is an organization's comprehensive strategy to identify, assess, remediate, and evaluate the security vulnerabilities within an organization's network and systems. A vulnerability management system is an overarching approach that includes policies, procedures, and tools to manage and mitigate risks effectively. Think of your vulnerability management program as part of your daily security routine. This program is all about keeping an eye on your cybersecurity day in and day out. Within this program, the VMS plays a critical role as the technological tool that supports the program's broader objectives. The VMS is designed to automate key functions such as the assessment of vulnerabilities, reporting on identified risks, repair or remediation of detected vulnerabilities, and evaluation of remediation efforts to ensure vulnerabilities are effectively addressed.

### Video - [The vulnerability management lifecycle](https://www.cloudskillsboost.google/course_templates/1197/video/522095)

* [YouTube: The vulnerability management lifecycle](https://www.youtube.com/watch?v=ss2mOjNYGK4)

SPEAKER: Vulnerability management follows a cyclical life cycle in which you move from assessment to remediation towards ongoing evaluation of vulnerabilities. This continuous cycle is essential for maintaining robust security and compliance, as well as continuously adapting to new threats. So what are the steps that security analysts follow as an incident moves through vulnerability management lifecycle? You start with asset discovery in order to build and regularly update an inventory of your organization's assets. This includes versions and patch statuses. This sets the baseline for vulnerability identification. Next, prioritize your assets by risk and significance to your operations. Focus your efforts on the most critical assets, then, conduct vulnerability assessments based on asset exposure. Use public vulnerability lists to better understand risks and tailor your cybersecurity strategy accordingly. You will learn more about vulnerability assessments in the next section of this module. Next, a vulnerability assessment report is generated. This report details a list of discoveries concerning existing vulnerabilities and categorizes them based on severity. Your job will be to prioritize them and then undertake asset discovery again in order to determine proactive measures to counter the vulnerabilities. Note that this report may be generated a month after the initial asset discovery. Remediation is undertaken to correct the vulnerabilities that were identified. This ensures the most critical issues are addressed first based on the established prioritization. The cycle concludes with evaluation and verification. This is where you confirm that the vulnerabilities have been resolved and validate the efficacy of the remediation measures. Following this life cycle, you will engage in developing a post-assessment plan. This plan is incorporated into your security strategy. It outlines high-risk assets and suggests remediation assets, such as software updates and patches, along with continuous monitoring to ensure effectiveness. Once integrated into your security strategy, the post-assessment plan serves as a roadmap for immediate remediation actions as well as a guide for improving your long-term security posture. The post-assessment plan enables you to prioritize resources effectively, with a focus on protecting high-risk assets first while also planning for the future. It is also important to note that the post-assessment plan helps foster a culture of continuous improvement and security awareness within the organization. This ensures that both personnel and processes evolve to meet emerging threats. By regularly updating this plan based on ongoing assessments and monitoring, you ensure that your security strategy remains robust, proactive, and adaptable to the changing landscape of cybersecurity threats.

### Video - [Vulnerability management on GDC](https://www.cloudskillsboost.google/course_templates/1197/video/522096)

* [YouTube: Vulnerability management on GDC](https://www.youtube.com/watch?v=AurWmM08Q_g)

SPEAKER: In the context of vulnerability management, GDC leverages the capabilities of Portswigger Burp for thorough web application security testing and Tenable Nessus for comprehensive network vulnerability assessments. This ensures a robust defense against security threats. Portswigger Burp is invaluable for GDC cybersecurity. It's like having a skilled security expert by your side when dealing with web application security. With Burp, you can actively test your web applications for vulnerabilities by simulating various attack scenarios. Examples include cross-site scripting, XSS, and SQL injection. Burp not only identifies weaknesses, but also provides you with detailed reports. This helps you understand and address potential risks effectively. Burp's features make it a go-to choice for ensuring the security of your web services. Tenable Nessus helps you evaluate the security of your networked systems. Nessus scans your distributed cloud infrastructure for vulnerabilities and conducts thorough assessments, pinpointing weaknesses and potential entry points for attackers. You can then use these insights to promptly prioritize and remediate vulnerabilities, which bolsters your overall cybersecurity posture. Nessus plays a critical role in safeguarding your hosted services and data within the GDC environment. Let's take a moment to explore when you would use each tool. You can use Burp if you need to assess the security of your web applications. Burp can identify vulnerabilities such as cross-site scripting, XSS, SQL injection, and more. You can also use Burp during penetration testing exercises when you want to simulate attacks and discover potential weaknesses in your web services. Burp also offers continuous monitoring of your web applications. This ensures your web applications remain secure and protected over time. Tenable Nessus is the right tool for scanning networked systems. Nessus helps identify vulnerabilities across your distributed cloud infrastructure. Nessus is also used to prioritize vulnerabilities based on their severity. This is useful for compliance testing, as you can ensure your systems meet specific regulatory requirements and security standards. To summarize, it's worth noting that Portswigger Burp is the right choice for web application security and penetration testing, while Tenable Nessus is best suited for network vulnerability assessment and compliance auditing within your GDC environment. Now, are there other tools to consider? You may refer to additional tools when your current VMS falls short. Supplementary tools help fill the gaps and enhance your overall cybersecurity posture. Quality and speed-- if you need specialized assessments or faster scanning capabilities. For example, you might turn to specialized vulnerability scanning tools tailored to that specific environment to ensure comprehensive coverage and quicker results. User experience-- if the user experience with your current vulnerability management tool isn't meeting your organization's needs. For example, you could consider supplementary tools that offer a more user-friendly interface or better customization options to align with your team's preferences. Compatibility-- if your existing vulnerability management tool doesn't integrate well with other security tools or your organization's specific technology stack. For example, you might need supplemental tools that bridge the gap and ensure seamless communication between different systems. Cloud support-- if your vulnerability management tool lacks adequate cloud support or doesn't effectively scan cloud-based assets. For example, you may opt for supplementary, cloud-specific security tools that can provide comprehensive coverage in cloud environments. Compliance-- if industries or organizations have strict compliance requirements that go beyond what a standard vulnerability management tool can handle. For example, you might need specialized compliance assessment tools that cater to specific regulatory standards. Prioritization-- if the prioritization features that are often included in vulnerability management tools are not enough. For example, you might require dedicated risk management tools to further refine and prioritize vulnerabilities based on your organization's unique risk appetite and business context. Remediation instructions-- if your primary vulnerability management tool lacks detailed remediation instructions or automation capabilities. For example, you might want to complement it with tools that offer comprehensive guidance on addressing identified vulnerabilities and streamlining the remediation process.

### Video - [Techniques for vulnerability management](https://www.cloudskillsboost.google/course_templates/1197/video/522097)

* [YouTube: Techniques for vulnerability management](https://www.youtube.com/watch?v=TT_wnG5iO4M)

SPEAKER: The vulnerability ecosystem is expansive. Google's engineering team diligently manages vulnerabilities for Google Cloud Datastore, GDC, and frequently releases new patches to safeguard security. This proactive measure by Google highlights the importance of vulnerability management and allows Google to maintain a high level of security across its vast array of products. Google sets the benchmark in the industry for effective and proactive vulnerability management. Within this collaborative framework, Google's Security Operations Center, SOC, plays a crucial role, leveraging real-time data and sophisticated analytics to continuously monitor for threats and potential vulnerabilities. As a tier 3 analyst, you can add significant value by integrating vulnerability functionalities into your practices. This enables you to not just rely on external patches but also actively identify and mitigate vulnerabilities. In the following videos, you will focus on three fundamental techniques that tier 3 analysts can use for effective vulnerability management. Vulnerability assessment. This is where you systematically identify and quantify vulnerabilities in systems. This is an essential skill for pinpointing security weaknesses. Vulnerability scanning. This is where you use automated tools to scan for known vulnerabilities. This enables you to gain insight into potential security risks. Penetration testing. This is where you conduct simulated cyber attacks against systems to uncover exploitable vulnerabilities. This enables you to test your defenses in real-world scenarios. It's important to note that vulnerability assessment, vulnerability scanning, and penetration testing are part of a larger vulnerability management ecosystem. Some practices that are not covered in this course but are equally crucial for a well-rounded cybersecurity skill set include the following. Manual code review, thoroughly reviewing the application source code to detect security flaws. Configuration audit, reviewing system configurations with best security practices to ensure optimal protection. Monitoring vendor advisories, staying up to date with security advisories from vendors so that you are aware of new vulnerabilities and patches. Collaboration with security communities, engaging with security communities and forums to exchange information, learning about emerging threats and discovering mitigation strategies.

### Video - [Vulnerability assessment](https://www.cloudskillsboost.google/course_templates/1197/video/522098)

* [YouTube: Vulnerability assessment](https://www.youtube.com/watch?v=u6rBfqL1FR4)

SPEAKER: In this video, you'll look at vulnerability assessment, one of the three fundamental techniques that tier 3 analysts can use for effective vulnerability management. Vulnerability assessment is a crucial process that helps you identify, classify, and prioritize weaknesses in your organization's computer systems, applications, and networks. This enables you to gain essential knowledge and risk context, which allows you to understand and respond to security threats effectively. Vulnerability assessment leverages numerous automated tools, such as network-security scanners. The results from these tools are then documented in detailed reports. While vulnerability assessment benefits all organizations, it is especially valuable for large enterprises and those that frequently face cyber attacks. As a tier 3 analyst, you will frequently encounter vulnerability assessments as part of your advanced analysis and threat-hunting efforts. Your deep technical expertise enables you to interpret the results of these assessments, prioritize vulnerabilities based on real-world threat intelligence, and develop sophisticated strategies for mitigation and prevention. The vulnerability-assessment process follows six steps. Let's explore each one. You start with vulnerability identification. This is where you discover and create a comprehensive list of vulnerabilities within your IT infrastructure. This process typically encompasses both manual penetration testing, often referred to as pentesting, as well as the use of automated vulnerability scanners. Vulnerability scanners are designed to scrutinize your networks computers, and web applications to pinpoint known vulnerabilities. Vulnerability scanners rely on various sources, including Common Vulnerabilities and Exposures, also known as a CVE glossary. It's important to note that while scanners are effective at uncovering known vulnerabilities, pentesting steps are used to fill in the gaps by discovering unknown vulnerabilities that could potentially be exploited. Once vulnerabilities are identified within your environment, your next task is to uncover the components responsible for these vulnerabilities and determine their root causes. To effectively manage security weaknesses, you will engage in a security-analysis process. This process involves classifying the severity of each vulnerability, identifying potential remediation options, and then aligning these choices with your organization's risk-management strategy. Ultimately, you will need to decide whether to accept, mitigate, or mediate each vulnerability. This ensures a comprehensive approach to bolstering your cybersecurity defenses. Once vulnerabilities have been identified and analyzed, your next step is to prioritize them effectively. This is often achieved by specialized vulnerability-assessment tools, which assign a rank or severity level to each identified vulnerability. A comprehensive risk-assessment report is then generated. This includes considerations like the system's composition, the nature of the data it stores, its impact on business continuity, its susceptibility to attacks or compromise, compliance with regulatory standards, and much more. This meticulous process provides you with the insights necessary to make informed decisions. You can then take the appropriate actions in order to safeguard your organization's cybersecurity posture. Once security issues that are deemed unacceptable have been identified, it's time for action. Remediation involves actively addressing and rectifying the security issues that were uncovered. You'll find that vulnerability-management systems are invaluable during this stage, as they often provide recommended remediation guidance for addressing common security vulnerabilities. For example, your VMS might suggest actions like installing readily available security patches or replacing hardware. Finally, mitigation involves taking steps to either reduce the impact of a potential exploit or minimize the likelihood that a vulnerability can be exploited. The mitigation measures you choose will depend on your organization's unique risk tolerance and budgetary constraints. Common mitigation strategies encompass various approaches, such as introducing new security controls, implementing encryption measures, or even replacing existing software or hardware components. By adapting these strategies to align with your organization's specific needs and circumstances, you can enhance your cybersecurity defenses and better protect your digital assets.

### Video - [Vulnerability scanning](https://www.cloudskillsboost.google/course_templates/1197/video/522099)

* [YouTube: Vulnerability scanning](https://www.youtube.com/watch?v=rUv-9p71648)

SPEAKER: In this video, you'll look at vulnerability scanning, the second of the three fundamental techniques that Tier 3 analysts can use for effective vulnerability management. Vulnerability scanning serves as a pivotal action within the vulnerability assessment process. It serves as the technical execution that detects and catalogs system and network vulnerabilities, which are then analyzed and prioritized in the broader assessment framework. Let's take a closer look. Vulnerability scanning is a cybersecurity practice that systematically identifies and assesses weaknesses within an organization's IT infrastructure. This helps you proactively mitigate potential security risks and maintain a secure environment. Vulnerability scans provide you with detailed reports that help you prioritize and address vulnerabilities effectively. This reduces the risk of cyber attacks and data breaches, while enhancing your overall cybersecurity posture. Vulnerability scanning is critical for maintaining the security and integrity of an organization's systems, networks, applications, and data. So what does vulnerability scanning include? Vulnerability scans can be automated or manual. For automated scans, scanning tools are configured to run regular scans at scheduled intervals. Manual scans may involve Tier 3 expertise to identify more complex or lesser known vulnerabilities that automated tools might miss. Vulnerabilities are typically rated and prioritized based on their severity using the Common Vulnerability Scoring System, CVSS. This scoring system provides a standardized way to assess the potential impact and exploitability of vulnerabilities. Once vulnerabilities are identified, you can conduct a risk assessment to evaluate the potential impact and likelihood of exploitation. This assessment considers factors such as the criticality of affected systems, business impact, compliance requirements, and the organization's risk tolerance. Vulnerability scanning generates detailed reports that include a list of identified vulnerabilities, their severity rating, and recommendations for remediation. These reports serve as a valuable resource for IT and security teams as you prioritize and address vulnerabilities. Remediation efforts may involve applying security patches, reconfiguring systems, updating software, or implementing additional security controls to reduce the risk associated with vulnerabilities. Continuous monitoring ensures real-time detection of new threats and system changes. This facilitates an immediate response to secure the network environment. Compliance requirements dictate the scope, frequency, and methods of vulnerability scans needed to meet industry standards. This ensures data protection and system integrity. Vulnerability scanning involves regular and automated scans that are conducted to account for changes in the IT environment, new vulnerabilities, and evolving threats. This helps you stay vigilant and maintain a proactive security stance. Many industries and organizations have compliance requirements that mandate regularly scheduled vulnerability scanning and remediation efforts. Compliance with standards like PCIDSS, HIPAA, and GDPR often includes vulnerability management as a key component. In summary, vulnerability scanning is a critical component of a robust cybersecurity strategy. It helps you identify and address potential security weaknesses, reduce the risk of cyber attacks, enhance your security posture, and demonstrate compliance with industry regulations.

### Video - [Penetration testing](https://www.cloudskillsboost.google/course_templates/1197/video/522100)

* [YouTube: Penetration testing](https://www.youtube.com/watch?v=KLTfpaEmeQ4)

SPEAKER: In this video, you'll look at penetration testing, the third of the three fundamental techniques that tier 3 analysts can use for effective vulnerability management. While vulnerability scanning identifies potential security weaknesses, penetration testing takes a step further by attempting to breach these vulnerabilities, which provides a practical evaluation of the organization's defense capabilities. Together, vulnerability scanning and penetration testing form a comprehensive approach to identifying and mitigating security risks. Let's take a closer look at penetration testing. With penetration testing, also known as pen testing, you play the role of a simulated attacker in order to actively evaluate your organization's security. Your main goals are to identify vulnerabilities and weaknesses within your systems, networks, or applications, and then provide actionable insights and recommendations to enhance your overall cybersecurity defenses. In the world of cybersecurity, penetration testing is a vital practice to ensure the security of systems, networks, and applications. Penetration testing fulfills numerous objectives, including the following. Identifying your security weaknesses, vulnerabilities, and potential entry points within the systems, networks, or applications you're testing. Assessing your security posture in order to determine how effective your current security controls, policies, and procedures are and safeguarding the environment. Testing your incident response capabilities in order to determine your organization's capability in detecting and responding to security incidents. Maintaining compliance with industry regulations and standards to ensure the organization meets legal and contractual security obligations. This means demonstrating due diligence in identifying and addressing vulnerabilities. Note that after the testing is complete, your role as a tier 3 analyst is to offer actionable recommendations to strengthen the organization's cybersecurity defenses. The approach you'll follow in a penetration test typically includes the following steps. Planning. Begin by defining the scope, objectives, and rules of engagement for the test. This stage sets the foundation for the entire process. Reconnaissance. Gather crucial information about the target environment, including systems and potential vulnerabilities. This phase is akin to gathering intelligence. Scanning and enumeration. This is where you employ various tools and techniques to discover vulnerabilities and identify target systems that may be susceptible to exploitation. Exploitation. You actively attempt to exploit the vulnerabilities you've identified to gain unauthorized access or control. Post-exploitation. After successfully infiltrating a system, you assess the extent of potential damage and identify if sensitive data is at risk of exposure. Reporting. You document your findings, risks, and recommendations in a comprehensive report that guides the organization in addressing vulnerabilities and improving security. As a penetration tester, you'll assess a wide range of vulnerabilities, including the following. Software vulnerabilities. These involve evaluating weaknesses in applications, operating systems, and software components. Network vulnerabilities. Here, your focus will extend to identifying weaknesses in network configurations, firewalls, routers, and related elements. Physical security. This entails evaluating physical access controls such as locks and surveillance systems in order to ensure they can't be easily compromised. Social engineering. This involves testing human vulnerabilities through techniques like phishing or impersonation. Misconfiguration. This refers to incorrect or suboptimal settings in software or hardware setups that can lead to vulnerabilities, performance issues, or system malfunctions.

### Video - [Tools for penetration testing](https://www.cloudskillsboost.google/course_templates/1197/video/522101)

* [YouTube: Tools for penetration testing](https://www.youtube.com/watch?v=0ZZVh19vl-c)

SPEAKER: There are various tools to help you in your penetration testing efforts. In this video, you'll briefly explore each one. Nessus is a widely used vulnerability scanner that assists in identifying potential weaknesses in systems, networks, and applications by conducting comprehensive vulnerability assessments. Metasploit is a versatile exploitation framework that helps penetration testers identify, exploit, and validate vulnerabilities. Metasploit is a crucial tool in ethical hacking and security assessments. Wireshark falls under the category of network analysis tools. Wireshark facilitates network protocol analysis and traffic inspection. It is used to gain insights into network behavior and diagnose network-related issues. Nmap, also known as network mapper, is a prominent port-scanning tool. Nmap is primarily used to discover open ports and services on target systems. This aids in vulnerability assessment and network reconnaissance. Burp Suite is a comprehensive toolkit for web application security testing. It is instrumental in web application testing, scanning, and analysis. Burp is particularly helpful for identifying vulnerabilities in web applications. Hydra is a powerful password-cracking tool. It is commonly used to test authentication security by attempting to crack passwords through various methods. In summary, these tool families play an essential role in penetration testing and cybersecurity assessments, with each tool serving a specific purpose in identifying vulnerabilities and securing systems and networks. GDC offers Nessus and Burp Suite in the default SOC setup.

### Video - [Best practices for a vulnerability management program](https://www.cloudskillsboost.google/course_templates/1197/video/522102)

* [YouTube: Best practices for a vulnerability management program](https://www.youtube.com/watch?v=d4zsI-X4H_Y)

SPEAKER: In previous videos, you learned about the importance of a vulnerability management program within an organization's comprehensive vulnerability management strategy. As you may remember, a vulnerability management program is an overarching approach that includes policies, procedures, and tools to manage and mitigate risks effectively. So what are some practices you can implement into your vulnerability management program? First, start with a thorough preliminary evaluation. It's essential for you to identify all the assets in your IT environment through asset discovery. This step is increasingly challenging due to the complexity of today's hybrid multi-cloud infrastructure and the intricate interactions of modern websites and web applications with internal and third-party resources. To manage these complexities, ensure your vulnerability management tools can accurately inventory all assets, including web applications and endpoint devices, while also considering your organization's risk tolerance to prioritize critical assets effectively. This prioritization helps balance security with operational efficiency. A well-structured vulnerability management program is vital for protecting against potential threats by facilitating the identification, assessment, prioritization, and mitigation of vulnerabilities. Maintaining an up-to-date inventory of all assets, including hardware and software, is crucial for effectively managing vulnerabilities and ensuring your organization security posture is strong. In addition to maintaining a comprehensive asset inventory, you should also focus on continuous vulnerability assessment. This means you need to regularly scan and assess your IT environment for vulnerabilities using automated tools. This is not a one-time activity but an ongoing process to identify new vulnerabilities as they emerge. Continuous assessment will also help you detect vulnerabilities in a timely manner. This ensures that you can take prompt action to address them. Handling extensive lists of vulnerabilities during assessments can be a daunting and labor-intensive task. Vulnerability assessment solutions help you stay organized by enabling you and your security team to prioritize efforts on the most critical vulnerabilities. Vulnerability assessment solutions assign severity ratings based on the Common Vulnerability Scoring System, CVSS, and consider factors such as vulnerability severity, asset criticality, exploitation complexity, and potential impact. This enables you to streamline the prioritization process, making your vulnerability assessments more efficient and focused on the most urgent security concerns. You should also develop and implement a robust patch management process to ensure the timely application of security patches. This process should include testing patches in a nonproduction environment to avoid potential disruptions before they are deployed in the production environment. A vulnerability assessment enables you to develop an effective threat response plan by leveraging insights from your cybersecurity posture. This process involves using findings from automated scans in order to create comprehensive remediation strategies. Such strategies include engaging key stakeholders, outlining specific incident response actions, and implementing proactive measures to prevent future incidents. For vulnerabilities that cannot be immediately patched, it's important to devise alternative mitigation plans, such as implementing compensating controls, like firewall rules, access controls, or configuration changes. This enables you to mitigate risk and protect your digital assets against evolving cyber threats. Your vulnerability management program should also adhere to industry standards and compliance requirements relevant to your organization. Standards such as ISO/IEC 27001 provide you with guidelines on how to manage information security, including steps for vulnerability management. It is also recommended that you integrate vulnerability management into your organization's incident response plan. This ensures that if a vulnerability is exploited, there are predefined strategies so that the incident response team knows how to react and can quickly take action to mitigate the impact. Vulnerability management training for IT staff and users is also recommended. Awareness can significantly reduce the risk of vulnerabilities being exploited through social engineering attacks or poor security practices. It is also recommended that you assess and manage the risks associated with third-party vendors and service providers. It's important that vendors follow similar vulnerability management practices, especially if they have access to your organization's IT environment. For instance, if Cymbal Federal employs external cloud storage solutions, it's essential to conduct a comprehensive security evaluation of these services to ensure they adhere to Cymbal Federal's stringent data protection and security criteria. This helps reduce the risk of data breaches and compliance violations. Finally, it's important to note that vulnerability management is not a set-and-forget process. You should regularly review and update your vulnerability management practices to adapt to new threats, technologies, and organizational changes. This includes learning from past security incidents to improve future vulnerability management efforts.

### Video - [Vulnerability management reports](https://www.cloudskillsboost.google/course_templates/1197/video/522103)

* [YouTube: Vulnerability management reports](https://www.youtube.com/watch?v=KXMX0ISRhcw)

SPEAKER: To conclude this module, let's quickly discuss the outcome of your vulnerability management program, a report. Vulnerability management reports serve as valuable resources for security teams. They include recommendations that enable you to efficiently and comprehensively tackle vulnerabilities through actions such as direct patching, configuration adjustments, or threat mitigation. These reports should prioritize clarity and conciseness to promptly inform you about the discoveries and their significance. A well-structured vulnerability report typically encompasses the following components-- summary, a concise overview that highlights the most critical findings. This offers you a snapshot without overwhelming you with details. Vulnerability list, a visual diagram or list that provides a clear representation of the identified vulnerabilities. This offers you a quick reference. Methodology and tools, an explanation of the tools and methods employed during the assessment, scan, or test. This offers you transparency regarding the process. Findings, the core of the report. This section delves into each vulnerability, detailing its description, severity level, potential consequences if exploited, and comprehensive recommendations for either remediation or mitigation. These well-structured reports ensure that your security team can swiftly grasp the key insights and take effective measures to enhance your organization's cybersecurity posture. By following this approach, you can streamline your response to vulnerabilities and bolster your overall security defenses.

### Video - [Module summary](https://www.cloudskillsboost.google/course_templates/1197/video/522104)

* [YouTube: Module summary](https://www.youtube.com/watch?v=HIRT384vgpI)

SPEAKER: Let's briefly recap what you've learned. In this module, you were introduced to vulnerability management and how as a tier 3 analyst engage in vulnerability management on GDC. You explored the benefits that vulnerability management can offer organizations as well as methods and strategies tier 3 analysts can use to effectively assess vulnerabilities within an IT environment. These include vulnerability assessment, vulnerability scanning, as well as penetration testing. You also explored deploying specific vulnerability tools based on the context and objectives of security assessments. This module concluded with a discussion of recommended best practices for efficiently managing vulnerabilities in order to maintain a secure IT environment. Now that you have a solid understanding of these topics, you're ready to advance to the next module.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1197/quizzes/522105)

#### Quiz 1.

> [!important]
> **What is the primary purpose of vulnerability scanning in an organization's cybersecurity strategy?**
>
> * [ ] To enforce compliance with industry regulations
> * [ ] To simulate real-world attacks
> * [ ] To conduct a risk assessment
> * [ ] To identify and prioritize known vulnerabilities

#### Quiz 2.

> [!important]
> **What is the primary purpose of vulnerability management?**
>
> * [ ] To detect and mitigate security vulnerabilities
> * [ ] To manage employee access to systems
> * [ ] To oversee company emails
> * [ ] To ensure all software is up-to-date

#### Quiz 3.

> [!important]
> **What is the primary benefit of vulnerability management?**
>
> * [ ] Streamlining authentication processes
> * [ ] Reducing the need for IT support
> * [ ] Enhancing resilience against cyber attacks
> * [ ] Mitigating data breach risks

#### Quiz 4.

> [!important]
> **Which best practice focuses on developing strategies for effectively responding to security incidents?**
>
> * [ ] Compliance and Standards
> * [ ] Incident Response Integration
> * [ ] Patch Management
> * [ ] Continuous Improvement

## Threat Modeling

In this module you will learn to define the role and benefits of threat modeling and describe the frameworks that apply to a variety of threats. You will also learn to differenciate between techniques for threat modeling, evaluate different tools and outline best practices.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1197/video/522106)

* [YouTube: Module overview](https://www.youtube.com/watch?v=ulMOhJjNZng)

SPEAKER: Welcome to threat modeling. In this module, you will learn about threat modeling, including its role and benefits. Then, you will explore frameworks that support threat modeling. You will also be introduced to techniques for threat modeling and explore different threat modeling tools. Finally, you will walk through best practices and how to apply them when undertaking threat modeling.

### Video - [The modern threat landscape](https://www.cloudskillsboost.google/course_templates/1197/video/522107)

* [YouTube: The modern threat landscape](https://www.youtube.com/watch?v=_HGXuT79TYM)

SPEAKER: To understand the role of threat modeling, it's important to consider evolving threats in the modern threat landscape. In today's constantly changing digital world, you're constantly faced with new technologies that bring along sophisticated cyber threats. These threats are continuously becoming more complex. This creates challenges for cybersecurity measures. Cyber threats have moved beyond the predictable patterns that security measures once relied on. Attackers are now employing highly adaptable and refined strategies to breach defenses. You're facing tactics such as advanced phishing schemes, zero-day exploits that target unknown vulnerabilities, and sophisticated malware designed to evade detection. Traditional security measures that depend on signatures and predefined rules become outdated almost as soon as they're implemented. Organizations of all sizes are potential targets for cyber threats. Ransomware gangs actively seek out vulnerable servers across all types of organizations. Even individual attackers can pose significant threats. Traditional antivirus software can be likened to using a torch in a vast, dark environment. It offers limited visibility and might not detect more sophisticated cyber threats. Advanced threats are often hidden and challenging to trace. These include malware strains that constantly change their structure and exploit vulnerabilities that haven't been seen before. As a Tier 3 Analyst, you need to thoroughly understand these threats. This means you need to be able to identify, analyze, and comprehend the nature, mechanisms, and potential impacts these threats may have on your digital environment. To do this, you need a multifaceted and proactive approach. This is threat modeling.

### Video - [Introduction to threat modeling](https://www.cloudskillsboost.google/course_templates/1197/video/522108)

* [YouTube: Introduction to threat modeling](https://www.youtube.com/watch?v=Fo8tF79C-bs)

SPEAKER: Threat modeling is the systematic process of identifying and prioritizing potential threats to a system. During threat modeling, you analyze the security of applications, systems, or business processes to uncover vulnerabilities that could be exploited by malicious actors. You then devise strategies to prevent or mitigate such attacks. Threat modeling is typically performed by threat hunters, who are responsible for the design and security of a system. Threat modeling focuses on security during the early stages of development and throughout the project life cycle. Let's explore some key elements of threat modeling. Threat modeling is a proactive security approach. With reactive security, you're essentially waiting for threats to appear, much like watching for enemy ships on the horizon. In contrast, proactive threat modeling is about actively seeking out potential threats in the digital realm before they can affect your systems. This involves delving into anomalies, scrutinizing unusual patterns in log files, and hypothesizing potential attacker behaviors. You're examining every facet of network activity, from clicks and connections to snippets of code, to identify and tackle complex cybersecurity challenges head on. This is the essence of the proactive threat modeling approach, where anticipation and early detection are key to maintaining security. Proactive threat modeling transcends the straightforward application of brute-force techniques and algorithmic solutions. Proactive threat modeling embraces a strategic and intellectual approach in which you leverage logic and hypothesis-driven investigations to outmaneuver potential threats. This method entails a thorough analysis of threat intelligence, internal data, and prevailing security trends to craft hypotheses about potential threats. By scrutinizing attack patterns and aligning known adversary tactics with your own environment, you can pinpoint probable exploitation avenues. The aim is to foresee and forestall the attacker's next move, staying one step ahead in the cybersecurity game. Navigating security without threat intelligence is like wandering through a dense jungle blindfolded. In contrast, proactive hunters arm themselves with the latest navigational tools, maps, and compasses, in a sense. Threat hunters use both internal and external threat intelligence feeds to stay informed about emerging vulnerabilities, attacker tactics, and targeted campaigns. This knowledge shapes hypotheses, steers investigations, and assists in prioritizing the most critical threats. By doing so, you ensure that your modeling efforts are not only efficient but also highly effective, targeting the areas where you can make the most impact. This proactive strategy is the foundation of successful threat modeling. By moving from a stance of reactive defense to one of reactive investigation, you are able to form hypotheses grounded in evidence and intelligence and thus harness the strength of threat knowledge. You reposition yourself as the predator within the digital ecosystem, uncovering hidden threats before they have a chance to cause damage.

### Video - [The threat modeling process](https://www.cloudskillsboost.google/course_templates/1197/video/522109)

* [YouTube: The threat modeling process](https://www.youtube.com/watch?v=8k5QMm8OJtA)

SPEAKER: Threat modeling itself is an ongoing and iterative process that evolves with changes in the system, applications, or organizational environment. Let's take a high-level look at the steps involved in threat modeling. One, threat modeling starts with the identification of critical assets that may be at high risk. Two, for each of these assets, threat modelers identify possible threats. Three, threat modelers conduct a vulnerability analysis and examine the findings. Four, threat modelers then prepare a plan of action to mitigate the threats and eradicate the vulnerabilities. Five, then threat modelers apply planned countermeasures and safeguards. Six, threat modelers then validate if the threat has been mitigated. Seven, if the threat has not been resolved, then a new cycle of analysis and remediation starts. Eight, if the threat has been resolved, then the threat hunter finalizes a comprehensive threat modeling report. Note that the steps from vulnerability analysis onward closely mirror the activities and processes of incident response, from investigation to remediation and recovery. The focus for the rest of this module is on step two, how to identify threats.

### Video - [Threat modeling frameworks](https://www.cloudskillsboost.google/course_templates/1197/video/522110)

* [YouTube: Threat modeling frameworks](https://www.youtube.com/watch?v=9rFPmQG5m50)

SPEAKER: In earlier videos, you learned that threat modeling allows you to systematically identify and assess potential threats and vulnerabilities within your system. This enables you to devise effective strategies to mitigate risks before attackers can exploit them. This proactive approach ensures you're always a step ahead in securing your digital assets. But how do you define what is truly a threat? Threats can take various forms, such as DoS or malware attacks. Threat modeling frameworks offer you a structured way to approach and manage threats. This is necessary to maintain a strategic advantage over cyber adversaries. There are various threat modeling frameworks to choose from. They enable you to conduct your assessments in a methodical manner, ensuring that you are both thorough and accurate. Let's quickly review some of the most impactful frameworks you can use. The MITRE ATT&CK framework serves as a comprehensive catalog that details adversary tactics and techniques based on real world observations. This framework provides a detailed guide to understanding and visualizing how attacks happen, helping to prioritize how to defend against them effectively. The Diamond Model is instrumental in dissecting and gaining an understanding of the intricacies of cyber intrusions. The framework emphasizes the interconnected relationship between four key components, the adversary, the victim, the infrastructure, and the adversary capabilities. The Cyber Kill Chain outlines the stages of a cyber attack from initial reconnaissance to the execution of the objective. This framework provides a structured methodology for breaking down and examining the sequential phases of an attack. By integrating these frameworks into your cybersecurity practices, you transition from a reactive to a proactive stance in threat management. This is because you enhance your ability to systematically identify, analyze, and neutralize cyber threats more efficiently. You are able to better focus on the most probable vectors of attack, and unveil concealed threats with increased precision. These frameworks sharpen your defensive strategies and elevate your overall security posture.

### Video - [Threat modeling frameworks at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1197/video/522111)

* [YouTube: Threat modeling frameworks at Cymbal Federal](https://www.youtube.com/watch?v=p2GGiY-EmxI)

SPEAKER: To stay a step ahead of adversaries, it's crucial to familiarize yourself with their strategies. In this video, you'll consider an example at Cymbal Federal. How can you use threat modeling frameworks to conduct your investigations in a more systematic and exhaustive manner? The processes and practices for the Google Distributed Cloud, GDC Air-Gapped SOC, are built out of the box to match the MITRE ATT&CK framework. As you navigate Splunk or review alerts, you will see that the system helps you categorize threats as soon as possible so that you can match observed activities to known adversary behaviors. The SOC team can also use the Diamond Model to further contextualize the attack, who is behind it, and how it was executed. You and the team combine these insights with those from the MITRE ATT&CK framework to identify group Phantom as exploiting zero-day vulnerabilities, capabilities, to steal sensitive data using a network of compromised servers, infrastructure, to attack Cymbal Federal, victim. You and the SOC team can also use the Cyber Kill Chain to break down the attack into distinct stages. This allows you to identify possible intervention points. For example, you identify the delivery stage, during which attackers deploy a zero-day exploit to gain initial access. This represents a critical opportunity for SOC intervention in which the team can potentially block or at least mitigate the attackers' progress. By using the Cyber Kill Chain framework, you are able to disrupt attacks early.

### Video - [Threat intelligence feeds](https://www.cloudskillsboost.google/course_templates/1197/video/522112)

* [YouTube: Threat intelligence feeds](https://www.youtube.com/watch?v=FWKrOlKKtHI)

SPEAKER: Another technique for effective threat modeling is staying up to date with the latest news in the cybersecurity space. Threat intelligence feeds are vital resources in cybersecurity. These feeds provide real-time or near real-time data about emerging threats, vulnerabilities, malware indicators, and malicious activities. Threat intelligence feeds are used by organizations to enrich their understanding of the threat landscape. This allows organizations to proactively adjust their security measures, respond more effectively to incidents, and enhance their overall security posture. Let's take a moment to walk through some of the most useful threat intelligence feeds and discuss how they contribute to an organization's security strategy. First, there are Open-Source Intelligence, OSINT, feeds. These feeds aggregate publicly available data from various sources, including blogs, forums, websites, and social media. These feeds are a valuable source of information about new vulnerabilities, emerging threat actors, and Tactics, Techniques, and Procedures, TTPs. Examples include the AlienVault Open Threat Exchange, also known as OTX; the Cyber Threat Alliance, known as CTA; and Google's virus tool. Commercial threat intelligence feeds are provided by security companies. These feeds offer more curated and often proprietary information on threats. Commercial intelligence feeds can include detailed analysis, context, and actionable intelligence on malware signatures, IP addresses associated with malicious activity, and more. Examples include CrowdStrike Falcon Intelligence, Mandiant Threat Intelligence, and Symantec DeepSight Intelligence. Industry-specific threat intelligence feeds focus on threats specific to certain industries, such as financial services, health care, or energy. These feeds provide targeted intelligence on tactics and malware used against entities within these sectors. Examples include the Financial Services Information Sharing and Analysis Center, FS-ISAC, for the finance sector, and the Health Information Sharing and Analysis Center, H-ISAC, for health care. Technical threat feeds provide technical details about specific threats, including indicators of compromise, such as malicious IP addresses, URLs, file hashes, and email indicators. These feeds are useful for operational teams in detecting and mitigating attacks. Examples of technical threat feeds include abuse.ch, SANS Internet Storm Center, and CIRCL, Computer Incident Response Center Luxembourg. Now that you've explored some of the more common threat intelligence feeds, let's discuss why they are so useful. Enrichment-- threat intelligence feeds enrich security tools, like SIEM systems, tips, and EDR solutions with up-to-date information. This helps you more efficiently identify and correlate threats. Proactive defense-- by understanding the TTPs and IOCs associated with current threats, organizations can proactively adjust their defenses and mitigate risks before they are exploited. Informing response-- detailed intelligence about a threat's behavior, origin, and impact guides the development of more effective incident response strategies and security policies. Strategic planning-- threat intelligence can inform strategic decisions and guide future investment in security technologies and frameworks that are specifically based on the organization's specific threat landscape. In summary, threat intelligence provides timely and relevant information about potential threats. Threat intelligence feeds allow organizations to stay ahead of adversaries and enhance their ability to detect, respond to, and mitigate cyber threats effectively.

### Video - [Introduction to modeling techniques](https://www.cloudskillsboost.google/course_templates/1197/video/522113)

* [YouTube: Introduction to modeling techniques](https://www.youtube.com/watch?v=fWKaJsTGP3o)

SPEAKER: Threat modeling is an indispensable technique for enhancing the security of your organization. This is particularly true for complex frameworks, such as GDC. So what techniques can you use to model a threat? A variety of methods have been crafted to assist you in pinpointing, evaluating, and neutralizing potential security threats before they become exploitable adversaries. The most common include STRIDE, PASTA, VAST, TRIKE, CVSS, attack trees, security cards, and the Hybrid Threat Modeling Method, HTMM. Each method is uniquely purposed and specifically tailored to address various facets of security assessment and strategic planning. While it's recommended you look at all these model techniques in more detail, the following videos will introduce the top three in more detail, STRIDE, PASTA, and CVSS.

### Video - [Title needs to be changed later](https://www.cloudskillsboost.google/course_templates/1197/video/522114)

* [YouTube: Title needs to be changed later](https://www.youtube.com/watch?v=7hor8H4QzGU)

SPEAKER: STRIDE is an acronym for Spoofing, Tampering, Repudiation, Information disclosure, Denial of service and Elevation of privilege. STRIDE's effectiveness in GDC environments lies in its ability to pinpoint specific threat vectors that could jeopardize cloud resources, user data or service availability. STRIDE's structured approach ensures comprehensive coverage of all security aspects. How does it work? First, STRIDE identifies assets within your system that need protection. These include data, devices, software components, network connections, and users. Then STRIDE establishes boundaries to understand where protection is needed in your system. This includes distinguishing between trusted and untrusted zones. The framework then categorizes potential threats according to the STRIDE model. Spoofing, tampering, repudiation, information disclosure, denial of service, and elevation of privilege. You will see this in action in a moment with the symbol federal use case. Next, STRIDE analyzes each identified threat to understand its potential impact, how it could be exploited, and the likelihood of its occurrence. The model then prioritizes the threats based on their potential impact and the likelihood of their occurrence. This helps focus efforts on the most critical issues. STRIDE then develops mitigation strategies for each high priority threat. This could involve implementing security controls, changing designs, or applying patches. The chosen mitigation strategies are put into practice. This may involve configuring security settings, deploying security solutions, or changing how the systems and data are accessed. Finally, you document the threat model, including identified threats and chosen mitigations. You regularly review and update the model as your system or the threat landscape changes. In the following video, you'll see STRIDE in action at symbol federal.

### Video - [STRIDE at Cymbal Federal.mp4](https://www.cloudskillsboost.google/course_templates/1197/video/522115)

* [YouTube: STRIDE at Cymbal Federal.mp4](https://www.youtube.com/watch?v=0xTh893uhts)

SPEAKER: Let's illustrate the value of STRIDE by returning to the use case at Cymbal Federal. Cymbal Federal has recently migrated its customer data management system to GDC. The organization aims to leverage the distributed nature of GDC for enhanced reliability and scalability. However, Cymbal Federal recognizes the increased sophistication of cyberthreats and their need to reassess their security posture to protect sensitive financial data. To achieve this, Cymbal Federal will be using STRIDE. So how can STRIDE provide Cymbal Federal with comprehensive security? Geopolitical threats offer intelligence on cyberthreats emanating from geopolitical developments. These feeds can help organizations assess the risk of cyber espionage, state-sponsored attacks, or cyber conflicts that could impact their operations. Examples include Stratfor and the Belfer Center's National Cyber Power Index. Let's consider how Cymbal Federal can systematically address each category of threat identified by the STRIDE methodology. Spoofing entity. An attacker attempts to impersonate a legitimate user by stealing credentials through phishing. Cymbal Federal can counter this threat by implementing multi-factor authentication and educating employees about the risks of phishing. Tampering. The attacker seeks to modify financial records during a transaction. Cymbal Federal employs integrity checks and encryption to ensure that data cannot be altered or undetected during transmission or storage. Repudiation. An attacker or legitimate user denies performing a malicious action, such as an unauthorized funds transfer. Cymbal Federal can mitigate this risk by implementing robust logging and monitoring systems that provide irrefutable evidence of all transactions and activities. Information disclosure. Sensitive customer data is exposed due to a misconfiguration in cloud storage permissions. Cymbal Federal addresses this risk by regularly conducting access reviews and employing automated tools to detect misconfigurations in real time. Denial of service, DoS. An attacker aims to overwhelm Cymbal Federal's online banking services, making them unavailable to legitimate users. Cymbal Federal uses GDC's built-in resilience and DDoS protection features to ensure the continuity of services, even under attack. Elevation of privilege. An attacker exploits a vulnerability to gain higher-level access than initially granted. Cymbal Federal prevents this by adhering to the principle of least privilege, regularly updating systems and conducting vulnerability assessments to identify and patch potential weaknesses promptly. As you can see, the comprehensive approach offered by STRIDE ensures that all aspects of the organization's security are covered, from preventing unauthorized access to ensuring the integrity, and availability of its financial services. As a result, Cymbal Federal maintains the trust of its customers and upholds its reputation as a secure and reliable financial institution.

### Video - [Process for Attack Simulation and Threat Analysis (PASTA)](https://www.cloudskillsboost.google/course_templates/1197/video/522116)

* [YouTube: Process for Attack Simulation and Threat Analysis (PASTA)](https://www.youtube.com/watch?v=fSoWsVn7FsA)

SPEAKER: The next modeling technique is the Process for Attack Simulation and Threat Analysis, PASTA. This is a seven-step, risk-centric methodology. PASTA is ideal for GDC, given GDC's focus on providing cloud services that may include custom developed applications. PASTA is particularly relevant given its emphasis on integrating threat modeling within the development process, thus ensuring that applications are secure by design. How does it work? First, PASTA defines the scope of the threat modeling activity, such as the systems, applications and assets to be analyzed. PASTA establishes security objectives and criteria for analysis. Then PASTA breaks down the application or system into its components, data flaws, and functionalities. This is in order to understand potential attack surfaces and security boundaries. PASTA then identifies potential threats for each component and system by considering various attack vectors and threat agents. This is determined through the use of industry frameworks and historical data for comprehensive threat identification. PASTA analyzes the system for existing vulnerabilities, security weaknesses, and misconfigurations that could be exploited by identified threats. PASTA engages in attack modeling based on identified threats and vulnerabilities. PASTA determines the methods attackers could use and the impact of successful exploits. Next, PASTA assesses the risk associated with each potential attack, considers the likelihood of occurrence and the potential impact on the organization. PASTA prioritizes risks based on their severity. Finally, you develop strategies to mitigate or eliminate high priority risks. This may involve implementing security controls, redesigning components, or applying patches. In the following video, you'll see PASTA in action at Cymbal Federal.

### Video - [PASTA at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1197/video/522117)

* [YouTube: PASTA at Cymbal Federal](https://www.youtube.com/watch?v=Anw9sYuZnD4)

SPEAKER: Let's illustrate the value of PASTA by returning to the use case at Cymbal Federal. Cymbal Federal plans to launch a new online banking platform to provide customers with enhanced digital services. This will involve sensitive data such as financial transactions and personal information. Cymbal Federal aims to ensure the platform's security by adopting the PASTA methodology. Let's consider how Cymbal Federal can systematically assess and mitigate the security risks associated with its new online banking platform by using the seven steps of the PASTA methodology. Define objectives. Cymbal Federal identifies the security objectives for its online banking platform, the protection of customer data, and the availability of banking services. This includes the online banking application, underlying cloud infrastructure on GDC, associated APIs, and third-party integrations. Application decomposition. Cymbal Federal maps out the online banking application's architecture, including data flows, user roles, and external dependencies. This is to understand potential attack surfaces. Threat analysis. The team identifies potential threats, such as SQL injection; cross-site scripting, XSS; account takeover; and data breaches that could exploit vulnerabilities within the application or the GDC environment. Vulnerability and weakness analysis. The team uses automated tools and manual testing to assess the online banking platform for vulnerabilities that correspond to the identified threats. The team prioritizes the threats based on their potential impact. Attack modeling. The team conducts simulated attacks to understand how an adversary might exploit the identified vulnerabilities. This helps Cymbal Federal visualize potential attack paths and judge the effectiveness of current security measures. Risk and mitigation analysis. For each identified risk, Cymbal Federal assesses the associated risk and considers the likelihood of its occurrence and potential impact, then prioritizes the risks based on their severity. Develop strategies to mitigate or eliminate high-priority risks. For each identified risk, Cymbal Federal develops mitigation strategies. These include implementing input validation to prevent SQL injection, using secure coding practices, enhancing authentication mechanisms, and employing continuous monitoring solutions. As you can see, the comprehensive approach offered by PASTA not only ensures that the platform is secure by design but also that it aligns with security practices within the organization's risk-tolerance levels. As a result, Cymbal Federal can confidently launch its online banking services and offer customers secure and reliable access to their financial information and transactions.

### Video - [Common Vulnerability Scoring System (CVSS)](https://www.cloudskillsboost.google/course_templates/1197/video/522118)

* [YouTube: Common Vulnerability Scoring System (CVSS)](https://www.youtube.com/watch?v=Bz8q-uJOQyc)

SPEAKER: The final threat modeling technique is the Common Vulnerability Scoring System, CVSS. This methodology provides a way to capture the principal characteristics of a vulnerability and produce a numerical score reflecting its severity. This score can then be used to prioritize response and mitigation efforts based on the risk posed by the vulnerability. CVSS is invaluable for GDC environments due to its ability to standardize the assessment and prioritization of vulnerabilities across distributed systems. This ensures efforts focus on the most critical issues first. How does it work? First, CVSS evaluates the intrinsic qualities of a vulnerability that are constant over time and user environments. These include metrics, such as attack vector, attack complexity, privileges required, user interaction, scope, and impact, confidentiality, integrity, and availability. Next, CVSS adjusts the base score based on factors that change over time but not across user environments. These include the availability of exploits, the level of remediation, and the confidence in the vulnerability description. Finally, CVSS tailors its scope to reflect the impact of the vulnerability on an organization's specific environment. CVSS factors in the mitigations that are in place, the importance of the affected system, and the presence of additional security controls. In the following video, you'll see CVS in action at Cymbal Federal.

### Video - [CVSS at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1197/video/522119)

* [YouTube: CVSS at Cymbal Federal](https://www.youtube.com/watch?v=_efp20toYbs)

SPEAKER: Let's illustrate the value of CVSS by returning to the use case at Cymbal Federal. Cymbal Federal is committed to maintaining the highest security standards to protect its data and services from cyber threats. As part of its routine security operations, Cymbal Federal's security team identifies a series of vulnerabilities within its GDC environment, including software flaws, misconfigurations, and exposure to known exploits. Cymbal Federal has discovered a vulnerability in their online banking application's authentication mechanism. This vulnerability allows an attacker to bypass authentication under certain conditions. Cymbal Federal adopts CVSS to standardize its vulnerability management process. Let's consider how Cymbal Federal can incorporate CVSS into its vulnerability management practices. CVSS scoring. The team applies the CVSS scoring mechanism to each vulnerability they have previously identified. CVSS evaluates the vulnerability in the online banking applications authentication mechanism and calculates a score of 9.0, categorizing it as critical. Prioritization of mitigation efforts. Based on the CVSS scores, Cymbal Federal prioritizes the remediation of vulnerabilities, focusing first on those with the highest scores. The critical authentication vulnerability is slated for immediate remediation due to its high score. Resource allocation. Cymbal Federal efficiently allocates its security resources, directing efforts towards vulnerabilities that pose the most significant risk to its operations, such as the high severity authentication issue. This ensures that Cymbal Federal promptly addresses the most critical vulnerabilities first, thereby lessening potential disruptions to services. By incorporating CVSS into its vulnerability management practices, Cymbal Federal establishes a systematic and quantifiable approach for assessing and prioritizing vulnerabilities. This approach not only enhances the organization's ability to respond effectively to potential threats, but also optimizes its security resource allocation. This ultimately strengthens the security posture of Cymbal Federal's cloud hosted financial services. Through the diligent application of CVSS, Cymbal Federal ensures that its operations remain secure, resilient, and trustworthy for its customers.

### Video - [Selecting a threat modeling technique](https://www.cloudskillsboost.google/course_templates/1197/video/522120)

* [YouTube: Selecting a threat modeling technique](https://www.youtube.com/watch?v=-1EfpoUqkpw)

SPEAKER: In choosing a threat modeling technique, you must first consider what you're trying to achieve. Are you focusing on identifying potential vulnerabilities in software development, assessing the risk to cloud infrastructure, prioritizing vulnerabilities for remediation? By aligning the chosen method with your specific goals, you can ensure that your threat modeling efforts are both effective and efficient. Choosing the most suitable model depends on the particular types of threats you're aiming to model and the objectives driving your threat modeling endeavors. Remember, the goal of threat modeling is not just to identify potential threats but also to enable your organization to make informed decisions about where to allocate resources for the best possible security outcomes. Consider STRIDE for its comprehensive threat categorization, PaSTA for its risk-centric approach to software and services, and CVSS for prioritizing and managing vulnerabilities. Remember, with threat modeling, you're well equipped to address the multifaceted security challenges presented by GDC and can enhance your organization's security posture.

### Video - [SecOps-specific tools for threat modeling](https://www.cloudskillsboost.google/course_templates/1197/video/522121)

* [YouTube: SecOps-specific tools for threat modeling](https://www.youtube.com/watch?v=fWtJuXLosl4)

SPEAKER: In previous videos you learned about threat modeling and useful methodologies. In this video, you'll explore threat modeling tools. SecOp specific tools form the technological backbone of your threat modeling endeavors. These tools empower you to collect, analyze, and visualize data with precision, turning information into actionable insight. With these tools at your disposal, you transform knowledge and methodology into action, ready to confront and neutralize threats with precision. Let's take a look at essential tools for threat modeling. SIEM platforms collect and aggregate vast amounts of security data from across your organization. Think of SIEM as the central hub where all security-related information is processed, making it easier for you to spot trends and anomalies. By integrating threat modeling methodologies into SIEM processes, you can proactively identify potential vulnerabilities and prioritize security measures based on real time insights obtained from the aggregated data. In essence, SIEM bridges the gap between data collection and threat modeling. SIEM provides a comprehensive approach to cybersecurity that helps safeguard against evolving threats. EDR tools offer you deep visibility into endpoint activity, akin to having a high powered microscope that unveils granular activities on each endpoint. This enhanced visibility not only equips you to detect advanced threats, but also enables rapid incident response. You can integrate threat modeling with EDR processes to further enhance your security posture. This is achieved by proactively identifying potential vulnerabilities and prioritizing response actions based on real time insights. EDR facilitates both proactive threat identification and swift incident response to effectively mitigate risks. NTA tools analyze network traffic patterns to pinpoint anomalies, malicious activity, and potential data exfiltration attempts. NTA tools function as vigilant guardians, continuously monitoring the flow of information to intercept threats as they maneuver within or attempt to breach the network perimeter. When you integrate threat modeling methodologies with NTA processes, you can proactively identify vulnerabilities and prioritize response actions based on real time insights derived from network traffic analysis. NTA serves as a proactive defense mechanism, bolstering cybersecurity by providing early detection and response capabilities. Threat Intelligence Platforms, TIPs, collect, aggregate, and curate threat intelligence from various sources. TIPs offer valuable insights into attacker Tactics, Techniques, and Procedures, TTPs. With TIPs, you're not just reacting to threats, you are anticipating them, armed with knowledge of the adversary's playbook. In the following video, you'll see these tools in action by returning to the Cymbal Federal use case.

### Video - [SecOps-specific tools at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1197/video/522122)

* [YouTube: SecOps-specific tools at Cymbal Federal](https://www.youtube.com/watch?v=3bDAS-B9o0c)

SPEAKER: You work at Cymbal Federal and have been tasked with hunting down the subtle traces left behind by adversaries by using IOCs and TTPs as your guide. To do this, you will use SecOps-specific tools. You use SIEM platforms to aggregate security data from across Cymbal Federal. This provides you with a comprehensive view for analysis. During your analysis, you uncover a series of unauthorized login attempts originating from an unfamiliar IP address. Further investigation reveals a pattern of unusual access to sensitive files, suggesting a potential breach orchestrated by external adversaries. You use EDR tools to delve into endpoint activities within Cymbal Federal's network. You discover a stealthy malware strain secretly operating within the Cymbal Federal system. By using EDR tools, you are able to capture anomalous behavior, indicating unauthorized data access and manipulation. This is indicative of an insider threat scenario orchestrated by adversaries seeking to exfiltrate critical information. You use NTA tools to scrutinize network traffic patterns at Cymbal Federal. You detect irregularities that indicate a covert data exfiltration attempt. The NTA alerts reveal a sudden surge in outbound traffic from a seemingly innocuous department server, which is a deviation from typical usage patterns. Further analysis uncovers encrypted communication channels that have been established using suspicious external domains. This highlights a sophisticated cyber espionage campaign orchestrated by external adversaries seeking to siphon sensitive data from the organization. As you can see, by integrating these tools, you sharpen your threat modeling capabilities and ensure Cymbal Federal's digital environment remains fortified against both current and emerging threats.

### Video - [Threat mapping](https://www.cloudskillsboost.google/course_templates/1197/video/522123)

* [YouTube: Threat mapping](https://www.youtube.com/watch?v=Ri5vA7cC3Rg)

SPEAKER: In a previous video, you learned about IoCs and TTPs for threat identification. Let's conclude by learning a little more about these two approaches and how they support threat mapping. IoCs are specific and actionable pieces of information that can directly indicate a compromise. TTPs offer deeper insight into the behavior and methods of attackers. TTPs provide a more strategic view of threat landscapes. IoCs are crucial for immediate detection and swift response to threats, whereas TTPs facilitate a more proactive security posture by enabling you to predict future attacks and mitigate the methods used by attackers. IoCs can quickly become outdated as attackers change their tools and methods. TTPs are more enduring and can remain relevant even as specific threats evolve. After you use threat monitoring methodologies and tools to identify a threat in your system, it's crucial to undertake a process known as threat mapping. Threat mapping is a comprehensive approach in which you understand and document how a specific threat operates within your environment. Threat mapping involves identifying the threats entry point, its movements across your network, the systems and data it has accessed or compromised, and its ultimate objectives. To effectively perform threat mapping, you should do the following. Gather and analyze data. Collect logs, alerts, and other relevant data from your security tools to trace the threats activities. Use threat intelligence. Leverage threat intelligence to understand the TTPs employed by the attacker. This can help in predicting their next moves. Collaborate with your team. Engage with your security team to pull knowledge and insights. This ensures a comprehensive map of the threat. Document everything. Keep detailed records of your findings. Include the tools used by the attacker, the sequence of their actions and any data accessed or exfiltrated. Threat mapping not only aids in mitigating the current threat, but also strengthens your security posture against future attacks. Visualizing the complete attack pathway of a threat throughout your system is fundamental to your ability to identify all vulnerabilities and compromised systems, as well as the severity and sophistication of the threat. This knowledge is necessary to enhance incident response and inform risk management. It also creates a detailed record of the event that can be used for compliance reporting and legal purposes, demonstrating due diligence in your security operations. By thoroughly understanding how an attacker navigates your defenses, you can make informed decisions to enhance your security measures. This ensures a more resilient and robust defense against cyber threats.

### Video - [Best practices for effective threat](https://www.cloudskillsboost.google/course_templates/1197/video/522124)

* [YouTube: Best practices for effective threat](https://www.youtube.com/watch?v=7SploquBoCk)

SPEAKER: Imagine you're tracking a shapeshifter who constantly changes their appearance. That's akin to the challenge you face in staying ahead of evolving cyber threats. To keep pace, you must become a relentless learner, absorbing threat intelligence, attending conferences and diving into new tools and techniques. To keep pace with evolving threats, it is recommended that you leverage resources such as the following. Threat blogs and research papers. These provide insights into the latest attacker tactics, vulnerabilities and emerging malware strains. This keeps you informed on the evolving landscape. Cybersecurity communities, and forums. These offer platforms where you can share knowledge, experiences and best practices with peers. This enriches your strategic approach. Online training courses and certifications. These equip you with specialized skills and deepen your understanding of sophisticated threat modeling methodologies. By actively seeking out new information and staying current with the latest trends, you ensure that your approach to cybersecurity remains sharp and effective. Metrics and measurements serve as your guidepost to evaluate the success of your threat modeling endeavors. Leverage these metrics and measurements to pinpoint areas that need enhancement. Time to detection measures how quickly you can identify and begin investigating a potential threat. This metric indicates the efficiency of your detection processes. Number of validated threats tracks how successful you are at identifying genuine threats. This metric reflects the effectiveness of your modeling strategies. False positives assesses the accuracy of your investigations. This metric helps you identify opportunities to refine your analysis methods. Impact of mitigated threats evaluates the potential damage averted through your successful threat modeling activities. This metric helps you determine the value of your efforts. By setting and measuring these metrics, you gain critical insights into how well your threat modeling approach is working. This enables you to fine tune your strategies, tools and techniques to achieve the best possible outcomes. Before you wrap up this module, it's important to note that no hunter excels in isolation. In the realm of ever evolving cyber threats, your ability to share insights and build a community is vital for collective triumph. You can glean wisdom from the experiences of others, exchange best practices, and unite to address complex challenges through the following. Engaging in online communities and forums. Here you can discuss hurdles, exchange insights, and collaborate on investigations. This enriches your collective knowledge base. Attending industry conferences and workshops. These gatherings allow you to network with fellow hunters, absorb wisdom from experts, and keep abreast of the latest developments and strategies in threat modeling. Contributing to open source threat modeling tools and resources. By sharing your own discoveries and tools, you contribute to the broader cybersecurity ecosystem. This allows you to aid in the development of a more robust communal defense infrastructure. Remember, threat modeling is an ongoing journey of discovery, adaptation, and teamwork. By adhering to these best practices, you'll enhance your capabilities, refine your methodologies, and emerge as a more formidable hunter in the digital wilderness.

### Video - [Module summary](https://www.cloudskillsboost.google/course_templates/1197/video/522125)

* [YouTube: Module summary](https://www.youtube.com/watch?v=dvrcXOp3L5w)

SPEAKER: In this module, you were introduced to threat modeling. You were introduced to the modern threat landscape and explored the importance of adopting a multifaceted and proactive approach when facing threats. You then walked through the threat modeling process, noting that this process closely mirrors that of incident response. The rest of this module focused on how to identify threats. You were introduced to the important threat modeling frameworks, including the MITRE ATT&CK framework, the diamond model, and the cyber kill chain. You also learned how to leverage threat intelligence feeds to stay ahead of threats. This module also revisited the Cymbal Federal use case. And you walked through explored common threat modeling techniques in action. These included STRIDE, PASTA, and CVSS. You concluded the module with an exploration of SecOps-specific tools for threat modeling, best practices, and how to perform threat mapping. Now that you have a solid understanding of these topics, you're ready to advance to the next module.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1197/quizzes/522126)

#### Quiz 1.

> [!important]
> **Which resources are most likely to provide an in-depth technical analysis of emerging vulnerabilities and exploit techniques?**
>
> * [ ] Threat intelligence feeds
> * [ ] Cybersecurity communities and forums
> * [ ] Online training courses and certifications
> * [ ] Threat blogs and research papers

#### Quiz 2.

> [!important]
> **How do tactics, techniques, and procedures (TTP) contribute to threat intelligence?**
>
> * [ ] By limiting access to security controls
> * [ ] By providing detailed insights into adversary behavior
> * [ ] By reducing collaboration among security teams
> * [ ] By increasing ambiguity in threat identification

#### Quiz 3.

> [!important]
> **Which of the following best describes proactive threat modeling?**
>
> * [ ] Relying on traditional antivirus software for protection
> * [ ] Actively seeking out potential threats before they can launch an attack
> * [ ] Conducting security audits after a breach has occurred
> * [ ] Waiting for alerts from security tools before responding to threats

#### Quiz 4.

> [!important]
> **How do indicators of compromise (IoC) contribute to cybersecurity resilience?**
>
> * [ ] By encrypting sensitive data
> * [ ] By promptly detecting and mitigating threats
> * [ ] By blocking all network traffic
> * [ ] By increasing the attack surface

## Security Engineering

In this module, you will learn about the role and benefits of security engineering and explain the principles behind the "secure by design" approach. You will also learn to describe techniques for selecting security controls, as well as identify useful tools to implement and best practices.

### Video - [Module overview.mp4](https://www.cloudskillsboost.google/course_templates/1197/video/522127)

* [YouTube: Module overview.mp4](https://www.youtube.com/watch?v=m-H3IduuaQY)

SPEAKER: Welcome to Security Engineering. In this module, you will learn more about security engineering. First, you will be introduced to the concept of security engineering and learn more about why it is important for security analysts. This includes a high-level exploration of the Secure by Design approach, used during the initial design phase of a project. Next, you will learn about different security controls and the techniques and tools you can use for effective security engineering. Finally, you will discuss some best practices for Tier 3 security engineers.

### Video - [Introduction to security engineering](https://www.cloudskillsboost.google/course_templates/1197/video/522128)

* [YouTube: Introduction to security engineering](https://www.youtube.com/watch?v=qvzB1CirO8A)

SPEAKER: SecOps thrives on a delicate balance between proactive defense and reactive response. Security operations focus on the immediate detection and mitigation of threats while security engineering builds the defenses and fortifications that prevent threats from taking root in the first place. How? Security engineering proactively embeds resilience against cyber threats into the design and implementation of systems. This significantly minimizes vulnerabilities and enhances the organization's defense mechanisms. In SecOps, security engineering is about creating and upholding systems that are inherently secure, with the aim of safeguarding your digital assets against cyber threats. It is your task as a Tier 3 Analyst to design, implement, and manage these systems. Security engineering is important for many reasons. Security engineering forms the foundation for protecting organizational digital assets and information. It contributes to proactive design practices and the implementation of systems that are resilient to cyber threats. This approach minimizes vulnerabilities by embedding security measures across all technology layers. It reduces the risk of data breaches by enabling a robust defense mechanism. It anticipates and prevents security incidents, thus strengthening the organization's overall security posture. Security engineering applies a Secure by Design approach to system design. Secure by Design, emphasizes the importance of considering security during the initial design phase of your project. This approach allows you to identify and mitigate potential vulnerabilities early, thereby reducing the risk and cost associated with addressing security flaws later on. Google Distributed Cloud, GDC, airgapped has been built according to secure by design principles. This provides you and your users with the peace of mind of knowing that the platform is both functional and secure. In the next video, you'll learn more about the Secure by Design approach.

### Video - [The Secure by Design approach](https://www.cloudskillsboost.google/course_templates/1197/video/522129)

* [YouTube: The Secure by Design approach](https://www.youtube.com/watch?v=FbpxRFdoP4I)

SPEAKER: In this video, you'll explore the Secure by Design approach in greater depth. Secure by Design is a proactive defense that involves threat modeling during the early design phase. This is so you can develop a deep understanding of your adversary and identify potential threats, vulnerabilities, and attack vectors that could impact your systems. In addition to threat modeling, Secure by Design embodies a philosophy of building security into every aspect of the system development life cycle. It emphasizes proactive measures such as thorough risk assessment, secure coding practices, and ongoing security testing to create resilient systems that prioritize security from the outset. This approach fosters a security-first mindset, ensuring that security measures are integrated seamlessly into the design and development process from the outset. By doing so, you're not just reacting to threats as they occur. Instead, you're anticipating them, giving you a strategic advantage in safeguarding your assets. In order to proactively fortify defenses, the Secure by Design approach uses threat modeling along with a variety of other tools and techniques. STRIDE and PASTA help you systematically identify and analyze threats. Threat intelligence sources, the MITRE ATT&CK framework, and security advisories provide you with insights into the latest attacker tactics and techniques. Attack simulations and penetration testing are invaluable for testing your defenses against hypothetical and a real-world attack scenarios. These tools and techniques work together to help you align with the Secure by Design philosophy, which emphasizes seamlessly integrating security into every aspect of system development. The Zero-Trust Architecture, ZTA, embodies the Secure by Design philosophy through its foundational mantra, "never trust; always verify." ZTA fundamentally shifts how you think about security, from a traditional perimeter-based model to a model that assumes breaches and verifies every interaction. Let's explore some key principles of ZTA that support the Secure by Design approach. First, there is the principle of least privilege. This means restricting access rights for users to the bare minimum necessary to perform their tasks. By doing this, you limit the potential impact of a breach by ensuring that individuals can only access what they absolutely need. Next is micro-segmentation. This involves dividing your networks into smaller, isolated segments to limit lateral movement and thus contain breaches within a small segment. This helps reduce the overall risk to your organization. There is also continuous authentication and authorization. This means you continuously verify the identity of users and their permissions, thereby ensuring that access rights are still valid at every point of interaction. Finally, implementing robust encryption and access control measures helps protect data integrity and confidentiality. This ensures that, even if data is intercepted, it remains unreadable and secure. Implementing these principles requires careful planning and execution. You must deploy technologies and policies that support dynamic and context-aware security measures. You must also constantly adapt to the evolving threat landscape and specific needs of your organization. By embracing a ZTA, you are protecting your resources against external threats and potential internal threats. You are enabling your security posture to be both resilient and adaptable to change.

### Video - [Secure by Design and SecOps](https://www.cloudskillsboost.google/course_templates/1197/video/522130)

* [YouTube: Secure by Design and SecOps](https://www.youtube.com/watch?v=bOFwUmj_23U)

SPEAKER: In this video, you'll explore how the Secure by Design philosophy fits within the context of DevSecOps. Secure by design is central within the software development life cycle, and recommends that you prioritize security measures early in the development process rather than as an afterthought. For DevSecOps, this means security considerations are woven into every phase of software creation. Let's consider some key practices that DevSecOps can implement throughout the development lifecycle. Empower development and operations teams with the knowledge and tools they need to identify and address security issues proactively. Use tools like Static Application Security Testing, SAST, Dynamic Application Security Testing, DAST, and Software Composition Analysis, SCA, within development pipelines to automatically identify potential vulnerabilities. Implement automated processes to regularly scan your code base and dependencies for known vulnerabilities. This ensures continuous security oversight. Engage in proactive threat modeling and risk assessments to identify and mitigate potential security risks before they become issues. Adopt secure coding standards and conduct thorough code reviews to maintain high security standards and catch vulnerabilities early in the development process. By embracing these practices, you're not just enhancing the security of your software products, you're also streamlining your development process and building a culture of security awareness and responsibility across your teams. This holistic approach to security ensures that organizations can deliver secure, high-quality software at speed, responding effectively to the evolving cybersecurity landscape.

### Video - [Security engineers in the SOC](https://www.cloudskillsboost.google/course_templates/1197/video/522131)

* [YouTube: Security engineers in the SOC](https://www.youtube.com/watch?v=ycOyAkY-6To)

SPEAKER: Automation and orchestration play a pivotal role in modern security engineering because they significantly enhance an organization's capability to optimize the efficiency and effectiveness of its security operations. Automation and orchestration techniques empower security teams to respond to incidents more swiftly, manage complex security environments at scale, and maintain a stronger security posture with fewer resources. In this video, you'll consider what security engineers actually do within the context of the SOC. Security engineers work alongside threat modelers, incident responders, and vulnerability managers. It is the security engineer's task to enhance the security processes within the SOC by updating and expanding the threat identification capabilities. This includes using monitoring tools, like the Google Cloud Security Command Center, and applying SIEM capabilities to analyze security data for suspicious activities. Security engineers will also integrate threat intelligence feeds to stay informed about emerging threats. Security engineers also work with incident responders. You help develop a comprehensive incident response plan that outlines the roles, responsibilities, and procedures for managing security incidents within GDC. Part of your role involves automating response tasks wherever possible in order to streamline operations. You will also regularly conduct drills to identify and implement improvements. Security engineers also assist vulnerability managers. In this case, your responsibilities include conducting regular scans with tools such as the Cloud Security Scanner. This enables you to identify and patch critical vulnerabilities first, and stay up to date with the latest GDC security updates. Finally, in order to ensure robust security policies within GDC environments, your focus as a security engineer should be on developing and enforcing detailed security policies across the GDC infrastructure. As noted, this involves using automation to ensure consistent application of policies. You're also responsible for deploying continuous monitoring tools to maintain adherence to policies, and to schedule regular reviews to ensure processes stay current against emerging cyber threats. In your role, you will also lead efforts to educate all staff through comprehensive training and awareness programs. This education is pivotal in emphasizing the importance of security policies and the role of each staff member in maintaining organizational security. Security engineers also conduct regular audits, security assessments, and engage in penetration testing. This allows you to assess the effectiveness of the security measures in place and to identify areas for improvement so you can adjust your strategies. Increased efficiency. Automating repetitive and time-consuming tasks allows security teams to manage larger environments more effectively. This reduces the time needed to detect and respond to incidents. Consistency and accuracy. Automation reduces human error and ensures that security tasks are performed consistently and accurately. Scalability. Automation and orchestration allow security processes to scale with the organization's growth. Organizations can handle increasing volumes of data and alerts without proportionally increasing the size of their security teams. Faster response times. By automating responses to common threats and orchestrating complex security processes, organizations can significantly reduce the time it takes to contain and mitigate incidents. This helps to minimize potential damage. Improved compliance. Automated compliance reporting and enforcement help organizations adhere to regulatory requirements more efficiently. This is because less manual effort is required. Integrating automation and orchestration into security engineering practices empowers organizations to maintain a robust security posture in the face of evolving cyber threats. How? Let's explore a few examples at a high level.

### Video - [The security engineering workflow](https://www.cloudskillsboost.google/course_templates/1197/video/522132)

* [YouTube: The security engineering workflow](https://www.youtube.com/watch?v=98X7F6x8RsE)

SPEAKER: The security engineering workflow involves sequential steps, starting with gathering security requirements, designing and implementing security controls, monitoring systems, and finally updating documentation. One, first, the security engineer gathers technical and business requirements to inform system design. Two, then security engineers design, implement, test, and document relevant controls in order to create a resilient and adaptable security architecture that is aligned with the security requirements that were identified. Three, following implementation, security engineers need to document requirements, processes, and protocols to ensure that other users have the right resources. Four, finally, security engineers continuously monitor the security systems to ensure that they are behaving optimally. If an update is required, security engineers will once again begin gathering security requirements in order to continuously improve the system.

### Video - [Introduction to security controls](https://www.cloudskillsboost.google/course_templates/1197/video/522133)

* [YouTube: Introduction to security controls](https://www.youtube.com/watch?v=VlnvN1fNYCw)

SPEAKER: In the upcoming videos, you will be introduced to the process of selecting security controls, including the various methodologies and instruments at your disposal as a security engineer. It's important to note that this serves as a foundation, but the selection of security controls will invariably be tailored to fit the specific needs of your project and your unique security requirements. Security controls are the safeguards, or countermeasures, that you implement to protect information technology systems, as well as manage risks to an organization's digital assets. These controls are designed to detect, prevent, reduce, and counteract security risks. Many different types of security controls exist. Deciding on which security controls to use for your project involves following a structured process, in which you must make a series of decisions. This process aims to ensure that the controls you choose effectively mitigate identified risks and align with your organization's overall security strategy and operational requirements. Let's briefly outline this process and the types of choices you'll need to make. In later videos you will learn about the different types of security controls. First, you conduct a comprehensive risk assessment to identify potential threats and vulnerabilities within the system or network. This assessment helps you understand the risk landscape and the impact of potential security breaches on the organization's assets. Based on the risk assessment, you define specific security requirements. This includes considering regulatory compliance needs, industry best practices, and organizational policies that dictate the level of security necessary. Using these security requirements, you evaluate various security control options. This evaluation considers the effectiveness of different controls in mitigating identified risks, including preventive, detective, and corrective measures. You then perform a cost-benefit analysis for each potential control. This analysis weighs the cost of implementing and maintaining the control against the benefit of the risk reduction it offers. You consider both financial implications and the impact on system performance or user experience. You also assess the compatibility of the security control with the existing environment and its ability to integrate with other security measures. Security engineers ensure that the new control does not negatively impact system operations or create conflicts with existing controls. You then consult with key stakeholders, including IT teams, compliance officers, and business unit leaders. You gather input and ensure that the selected control aligns with business objectives and operational workflows. Before full-scale implementation, a pilot test of the chosen security control may be conducted in a controlled environment. This testing phase helps identify any unforeseen issues and assesses the control's effectiveness in a real-world scenario. Based on the outcomes of the evaluation, cost-benefit analysis, compatibility assessment, stakeholder feedback, and pilot testing, you decide on the most appropriate security control to introduce. Finally, the decision and the rationale behind it are documented. Security policies and procedures are updated to reflect the introduction of the new control. Note that following this structured approach ensures that the introduction of a new security control is strategic and effective, addresses the specific security needs of the organization, optimizes resources, and minimizes any adverse impact on operations. Security engineers in the GDC SOC focus on technical security controls and administrative security controls. These can be preventive, detective, or corrective. In the following videos, you'll look at each category in detail.

### Video - [Technical security controls](https://www.cloudskillsboost.google/course_templates/1197/video/522134)

* [YouTube: Technical security controls](https://www.youtube.com/watch?v=VRZD71DVGTE)

SPEAKER: Technical preventive controls are essential for proactively protecting an organization's digital assets. Technical preventive security controls encompass a wide array of mechanisms, including firewalls, encryption, access controls, and antivirus software. These controls work to safeguard the confidentiality, integrity, and availability of data and systems. Technical preventive security controls are useful for reducing the attack surface. They also ensure data privacy and help maintain regulatory compliance. Technical preventive security controls are foundational for building a secure IT environment because they establish barriers to unauthorized actions, encrypt sensitive information, and prevent malware infections. Technical preventive security controls significantly lower the risk of cyber incidents. Let's consider an example in which simple federal implements a series of technical preventive security controls across all its operations. The first one to discuss is threat detection and monitoring. Cymbal Federal deploys an Intrusion Detection System, IDS, to continuously monitor network traffic for signs of malicious activity. Network traffic analysis tools are used to scrutinize data flow patterns for anomalies. These tools support analysts in managing and analyzing traffic patterns for threat modeling. They also assist Tier 1 Analysts in effective incident response. Technical preventive controls also support incident response at Cymbal Federal. Security engineers introduce a new service that automatically isolates affected systems and deploys a blocking IP countermeasure upon detecting an incident. Tier 2 Analysts oversee the automated response actions. They ensure proper execution and coordinate manual interventions when necessary. The patch management system automates the deployment of necessary updates in order to mitigate vulnerabilities that were identified after regular vulnerability scans. This is supervised by vulnerability managers, who ensure that patches and updates are effectively applied to maintain system security. Finally, security policy enforcement. Security engineers ensure compliance with Cymbal Federal security policies by regularly auditing security settings in the Configuration Management tool and access control system. By implementing these technical preventative controls, Cymbal Federal is able to proactively strengthen its cybersecurity defenses, minimize the risk of incidents, and ensure a swift and effective incident response process. You may choose to employ technical detective security controls. These security controls include intrusion detection systems, IDS; Security Information and Event Management, SIEM systems; log analysis tools; and anomaly detection algorithms. Technical detective security controls continuously scan for irregular patterns or known indicators of compromise. This enables you to more effectively identify a breach early in its life cycle. Technical detective security controls also facilitate forensic analysis to understand attack vectors and help inform the improvement of preventive measures. Let's consider how Cymbal Federal enhances its cybersecurity framework by integrating technical detective security controls in all key operational areas. Earlier, you learned how IDS offers network traffic monitoring as a technical preventative measure. However, this also acts as detective control. The SIEM system detects potential security incidents by aggregating and analyzing logs from multiple sources for anomalies. Technical detective security controls are also used to support a new incident response process. Upon detection of a security incident, the SIEM system triggers the automated incident response platform to execute predefined response protocols. For Cymbal Federal, these include isolating affected systems and blocking malicious IP addresses. It is the responsibility of Tier 2 Analysts to manage and refine the automated response actions. This ensures rapid containment and mitigation of incidents. Log analysis tools support vulnerability management by helping to identify security misconfigurations or failed access attempts indicative of vulnerability exploitation attempts. Anomaly detection algorithms use machine learning to detect unusual behavior that could signal an emerging threat. Cymbal Federal Tier 3 Analysts leverage findings from log analysis and anomaly detection to identify and assess new vulnerabilities. They work closely with the security engineers to prioritize and address vulnerabilities through patches or configuration changes. To enforce Cymbal Federal security policies, the SIEM system is then configured to generate alerts on policy violations. In this way, SIEM acts as a continuous audit mechanism, alerting you when there are unauthorized access attempts and deviations from standard operating procedures. Security policy officers then implement corrective measures, such as updating access controls or refining security policies. As seen, by adopting these technical detective security controls, Cymbal Federal is able to significantly improve its ability to detect, respond to, and manage cyber security threats. This proactive approach ensures that Cymbal Federal can maintain a strong security posture, protect sensitive customer information, and comply with financial industry regulations. Next, there are technical corrective security controls. Technical corrective security controls include patch management systems, backup and recovery solutions, and software updates that address security flaws. These controls play a crucial role in the recovery process following a security breach. They are useful for quickly mitigating the impact of incidents, ensuring business continuity, and strengthening system defenses against future attacks. Implementing these technical corrective security controls helps organizations recover from cyber incidents, minimize downtime, and preserve the trust of customers and stakeholders. By focusing on technical corrective security controls, Cymbal Federal enhances its post-incident recovery processes. Cymbal Federal uses advanced, real-time, threat-detection software in its post incident recovery strategy. This software, powered by advanced analytics and machine learning, is able to identify threats as they emerge and analyze the aftermath of incidents. This assists Cymbal Federal to quickly identify exploited vulnerabilities and take swift remediation actions. The deployment of continuous monitoring platforms across the Cymbal Federal network is also pivotal in the recovery phase. These platforms provide ongoing surveillance of system health, user activities and security events. Continuous monitoring post incident is essential for detecting residual threats, thus facilitating a rapid return to normal operations. Threat modelers also play a vital role in both the detection and recovery phases. After an incident, threat modelers assist Tier 1 analysts in dissecting the breach, taking the time to understand the attackers methods and advising on specific corrective actions. This contributes to a comprehensive recovery process and ensures that all signs of the breach are effectively eradicated. Cymbal Federal also uses technical corrective security controls to support incident response in the event of a security breach. If a breach occurs, the automated patch management system is immediately activated to deploy patches for the exploited vulnerabilities across affected systems. At the same time, backup and recovery solutions are used to restore any data that was compromised or lost during the incident. Security engineers oversee the deployment of patches and the recovery process. They ensure all affected systems are swiftly returned to their secure state. Security engineers work in coordination with Tier 2 incident responders who themselves are responsible for the overall incident response strategy. Tier 2 incident responders focus on rapid containment and recovery to minimize operational downtime. Following an incident, software update tools apply the latest security updates and patches to all systems. These tools not only address the vulnerabilities that were exploited, but also strengthen defenses against potential future attacks. Tier 3 analysts assess the security updates for their relevance and urgency. Tier 3 analysts prioritize updates based on the severity of vulnerabilities and the criticality of affected systems. Following an incident, configuration management tools are used to review and adjust system configurations as per updated Cymbal Federal security policies. This ensures that any configurations that may have contributed to the vulnerability are corrected. Security policy officers collaborate with security engineers to update and enforce new security policies across the organization. Together, they ensure that all systems are compliant with the revised policies, thus reducing the risk of future incidents. By implementing these technical corrective security controls, Cymbal Federal enhances its ability to recover from cyber incidents and significantly improves its resilience against future cybersecurity threats. This comprehensive approach to post incident recovery and system strengthening helps symbol federal maintain its commitment to protecting customer data and ensuring the ongoing trust and confidence of its customers and stakeholders.

### Video - [Administrative security controls](https://www.cloudskillsboost.google/course_templates/1197/video/522135)

* [YouTube: Administrative security controls](https://www.youtube.com/watch?v=bFQuemzGMoI)

SPEAKER: Whereas technical controls involve the use of technology to enforce security measures and protect information assets, administrative controls consist of policies, procedures, and guidelines that dictate how data is managed and how human resources are governed to maintain security. Administrative preventive security controls establish a framework of policies, procedures, and guidelines that aim to prevent unauthorized access, data breaches, and other security incidents. These controls help create a secure organizational culture by setting clear expectations for behavior, defining roles and responsibilities, and enforcing compliance with security best practices. Administrative preventive security controls are useful for ensuring that all members of an organization understand their part in maintaining security. This reduces the risk of insider threats and helps ensure compliance with legal and regulatory requirements. Cymbal Federal aims to bolster its security posture by implementing a comprehensive set of administrative preventive controls. Cymbal Federal will focus their efforts on organizational behavior, compliance, and risk management. These measures are led by the Chief Information Security Officer, CISO, as part of an organized team initiative that involves multiple teams in the SOC and in the OC. Let's look at what these Administrative preventive security controls look like at Cymbal Federal. To support threat detection and modeling, Cymbal Federal has established a security awareness and threat intelligence sharing program. This program continuously educates employees on the latest cybersecurity threats and monitoring techniques. The Cyber Security Awareness team also shares regular security briefings and updates across Cymbal Federal. An internal threat intelligence platform is also launched. This provides real-time threat information sharing. To support incident response, the Incident Response Plan, IRP, is continuously iterated upon to include new simulation exercises and drills. This helps prepare employees for potential cybersecurity incidents and ensures that they understand their roles and responsibilities. Security engineers on the incident response team also coordinate training exercises and update the IRP based on lessons learned from drills and real incidents. The vulnerability management team collaborates with infrastructure operator managers to implement a policy for regular security assessments and vulnerability scans across Cymbal Federal's IT infrastructure. Third-party services are also used to perform penetration tests to identify system weaknesses. Finally, security policy guidelines are continuously updated to cover new details related to data protection, access controls, and acceptable use of IT resources. The Security Policy Committee, which consists of representatives from IT, legal, HR, and compliance departments, helps develop, review, enforce, and regularly audit Cymbal Federal security policies. They ensure that employees acknowledge and annually commit to these revised policies. By implementing administrative preventive controls, Cymbal Federal is able to focus its efforts on organizational behavior, compliance, and risk management. The organization is thus able to strengthen its proactive defenses against cyber threats, enhance its incident response readiness, and ensure strict adherence to security policies. This thereby solidifies Cymbal Federal's overall security posture and compliance framework. Cymbal Federal may also choose to implement administrative detective security controls. These controls help identify security incidents as they unfold or after they have occurred. Organizations can use administrative detective security controls to detect unauthorized access, misuse of resources, or non-compliance with established policies. These controls can provide insight into the effectiveness of existing security measures, help identify areas for improvement, and ensure accountability within the organization. Administrative detective security controls help you detect anomalies early. This helps mitigate the impact of security incidents and supports the continuous refinement of security strategies. Cymbal Federal integrates administrative detective controls to improve its security oversight with a focus on the timely detection of security incidents and enhancing accountability. To support threat detection and modeling, the organization launches an employee reporting system that is used to allow employees to report suspicious activities or security concerns. This system is complemented by a centralized incident-tracking dashboard that aggregates and analyzes patterns or emerging threats. Security engineers monitor the dashboard, evaluate reports, and initiate investigations into reported activities. This helps to ensure rapid threat detection and monitoring. Administrative detective security controls are also used to support incident response. Detailed reports are compiled and reviewed when an incident occurs. This helps Cymbal Federal assess the effectiveness of the response and identify areas for improvement. The incident response team is in charge of overseeing the documentation process. They conduct post-incident reviews and update response strategies based on the insights they gain. Security engineers also extend the employee reporting system to support the submission of vulnerability reports. The vulnerability management team then assesses these reports and uses the data to prioritize remediation actions. This ensures that the most critical threats are prioritized first. Finally, Cymbal Federal uses a compliance monitoring system to automatically audit and report deviations from established security policies and standards. The system continuously scans for policy violations, generates compliance reports, and alerts management of any instances of non-compliance. These instances are then reviewed by security policy officers, who then work with relevant departments to enforce policy adherence and address compliance issues. By adopting these administrative detective controls, Cymbal Federal significantly strengthens its security oversight capabilities. This strategic approach enables the timely detection of threats, the efficient management of incidents, the proactive identification of vulnerabilities, and the rigorous enforcement of security policies. Overall, administrative detective controls enable Cymbal Federal to enhance its accountability and resilience against cyber threats. Finally, there are corrective security controls. These controls focus on the actions and procedures that organizations implement to address and remediate security incidents. Organizations can effectively minimize the impact of breaches and quickly restore normal operations by establishing clear incident response protocols, developing comprehensive recovery plans, and conducting thorough post-incident analyses. Administrative corrective security controls are useful for ensuring a structured and efficient response to security incidents. This facilitates quick recovery and helps improve security policies and procedures to prevent the recurrence of similar incidents. Cymbal Federal enhances its post-incident recovery processes by implementing a suite of administrative corrective controls. These focus on efficient incident management, system recovery, and policy refinement. To support threat detection and monitoring. Cymbal Federal designs a post-incident threat analysis program. Following an incident, this program analyzes the threat and supports the adjustment of threat-detection strategies and tools. This helps threat modelers improve future detection and monitoring efforts. Administrative corrective security controls include a review and update process in which incident response procedures are evaluated and enhanced after an incident. This review is undertaken by the incident response team and the incident response coordinator. Together, they ensure that lessons learned are integrated into future response efforts. A vulnerability remediation follow-up program is developed to ensure vulnerabilities exposed by incidents are promptly and effectively addressed. The vulnerability management team works with security engineers to track the remediation of vulnerabilities that were identified during the incident. Together, this team ensures all vulnerabilities are patched or mitigated according to best practices and within agreed-upon time frames. A security policy update and reinforcement initiative is established to revise security policies based on incident findings. This initiative also reinforces policy adherence. New security measures are incorporated, and organization-wide training sessions are conducted to reinforce the importance of adherence to these updated policies. By implementing these administrative corrective controls, Cymbal Federal not only enhances its efficiency in managing and recovering from incidents, but also strengthens its overall cybersecurity framework. This structured approach to post-incident recovery ensures Cymbal Federal is better prepared to protect against and respond to future threats.

### Video - [Tools for security engineers](https://www.cloudskillsboost.google/course_templates/1197/video/522136)

* [YouTube: Tools for security engineers](https://www.youtube.com/watch?v=JwCzKuvdDwI)

SPEAKER: Numerous tools exist to help you implement security controls. In general, these fall into three main categories-- core, observability, and management tools. As a security engineer, you will find yourself using and updating tools within the GDC SOC. So it is important to be familiar with them. Unlike other security analysts in the SOC, you will often look at management tools in order to introduce platform changes following GDC's software development life cycle. ServiceNow will also be important, as you will need to document your work on introducing security controls throughout the process. Documentation is paramount for security engineering too. For all the introduced security controls, you will need to monitor them over time to ensure they are working as expected. Observability tools like Grafana and log-in metric collectors are fundamental. Finally, the security controls you are building aim to fortify the security posture of the SOC, as well as create a more seamless workflow for security analysts. Many of the security controls you'll implement will directly affect the core tools in order to achieve this. Additionally, you may need to introduce new tools to effectively implement the required security controls, as security is always in innovation mode.

### Video - [Security engineering best practices](https://www.cloudskillsboost.google/course_templates/1197/video/522137)

* [YouTube: Security engineering best practices](https://www.youtube.com/watch?v=mWDVR2FtBH8)

SPEAKER: In this video, you'll briefly review a few best practices for an effective security engineering program. The first best practice returns to the Security by Design philosophy and building a Zero Trust Architecture, ZTA. Every time you develop a security control system, remember the mantra "Never trust, always verify." This requires you to verify the identity and security posture of all devices and users, irrespective of their location, before granting access to your network. As noted earlier in the earlier videos, you will also want to keep the following zero trust principles in mind. Design for least privilege access. Only grant access to the resources absolutely necessary for the performance of the user's role. This reduces the potential impact of a breach by limiting access paths and resources that could be exploited. Divide your network into smaller, more manageable zones using micro-segmentation. Taylor and separate access controls for each segment. This helps to effectively contain breaches and limit lateral movement within your network. This granular level of control enhances your security posture by isolating sensitive data and systems from unauthorized access. Introduce more and more capabilities to support the continuous monitoring of the system. This enables you to immediately identify suspicious activities and enables rapid response and mitigation efforts to safeguard your assets. By implementing these principles, you're not just enhancing your security measures, you're also adopting a proactive stance against the ever evolving threat landscape. This helps to ensure your organization's resilience against cyber threats. Another best practice is to embed security in every aspect of your software lifecycle, from continuous education and tool integration to automation, secure coding and risk assessment. Security training instills a security first mindset among teams. This helps ensure that every team member understands the importance of security and their role in maintaining it. The integration of security tools like Static Application Security Testing, SAST, Dynamic Application Security Testing, DAST, and Software Composition Analysis, SCA, into your development pipelines allows you to scan your code base for vulnerabilities. This permits timely remediation before deployment. Automating Security Testing is a key practice within DevSecOps for continuous risk assessment throughout the development process. This significantly reduces the likelihood of vulnerabilities, making it into production. Secure coding practices and thorough code reviews embed security considerations from inception. This enhances software quality. Risk assessment during the design and development phases allows you to anticipate and address potential security issues up front. By embracing these DevSecOps practices, you're not just improving the security of your software products, you're also streamlining your development processes and building a culture of security awareness and responsibility across your organization. This ensures a robust defense against threats right from the start. Finally, it's important to note that you only want to implement necessary security controls. That is because there is a cost of ownership associated with customizing any third-party deployments such as GDC. Any modifications that you introduce require your ongoing maintenance and monitoring. New security controls may result in customizations to the GDC deployment. This customization may not align with the standard support agreements or updates provided by GDC. This means that as new official patches or versions of GDC are released, you will need to make sure that your custom security controls work as expected and do not create conflict. Furthermore, you won't be able to escalate to the GDC engineering team for non-standard issues. Therefore, consideration and planning are crucial when determining your need for non-standard security controls. Carefully balanced security enhancements with the long-term maintenance overheads as part of your assessment.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1197/video/522138)

* [YouTube: Module review](https://www.youtube.com/watch?v=5ymvvv9lxzw)

SPEAKER: In this module, you were introduced to security engineering. You started with a discussion on the importance of security engineering in SecOps and your role as a Tier 3 Analyst. Then you explored the Secure by Design philosophy and learned how this approach fits within the context of DevSecOps. This module also presented a range of technical and administrative security controls that can be used for effective security engineering. You explored each security control in detail and applied them to scenarios at Cymbal Federal. You concluded this module with a brief discussion on tools and best practices for an effective security engineering program. Now that you have a solid understanding of these topics, you're ready to advance to the final module of this course.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1197/quizzes/522139)

#### Quiz 1.

> [!important]
> **What is the guiding principle behind the 'Secure by Design' philosophy?**
>
> * [ ] Integrate security practices and tools after the core design is finalized
> * [ ] Consider security during the initial project design phase
> * [ ] Prioritize security enhancements based on emerging threats during the project lifecycle
> * [ ] Focus on security after project design

#### Quiz 2.

> [!important]
> **What are the three principles of a zero-trust architecture?**
>
> * [ ] Apply continuous monitoring, encrypt data at rest, and segment networks.
> * [ ] Verify explicitly, apply least-privilege access, and assume a breach.
> * [ ] Enable single sign-on, enable multi-factor authentication, and apply end-to-end encryption.
> * [ ] Validate users, protect devices, and segment networks.

#### Quiz 3.

> [!important]
> **GDC manages a database containing member data. This data is classified as "medium sensitivity" based on the potential impact of a breach. Which security control would be most effective in protecting member data from unauthorized access?**
>
> * [ ] Encrypt the database at rest and in transit.
> * [ ] Implement multi-factor authentication (MFA) for all users accessing the database.
> * [ ] Implement role-based access control (RBAC) for accessing the database.
> * [ ] Engage in regular vulnerability scanning for the database server.

## Splunk advanced: Lite management

This module introduces the importance of Splunk management and defines Tier 2 and Tier 3 management tasks in Splunk. You will learn to describe configuration files for Splunk management, use Splunk Web and btool, and address common configuration challenges with Configuration File Management in Splunk Web and btool.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1197/video/522140)

* [YouTube: Module overview](https://www.youtube.com/watch?v=1PYmFiyOelw)

SPEAKER: Welcome to Splunk Advanced Lite Management. Splunk SIEM is the centralized security hub in the SOC. You as a SOC analyst will often find yourself working in Splunk. Given the centrality of its role, you will conclude this course with an overview of Splunk management to help support you in becoming a Splunk power user. First, you will be introduced to Splunk management. This includes the Splunk setup for the Google Distributed Cloud, GDC, airgapped SOC, why Splunk management is so important, and the role of tier 2 and tier 3 analysts in managing Splunk. Then you quickly walk through the different types of configuration files that are available in Splunk management. Finally, you will explore how to administer Splunk using Splunk web and btool. This includes common configuration challenges.

### Video - [Introduction to Splunk management](https://www.cloudskillsboost.google/course_templates/1197/video/522141)

* [YouTube: Introduction to Splunk management](https://www.youtube.com/watch?v=fY49WzC9La4)

SPEAKER: In this video, you'll briefly recap how Splunk is set up for the GDC SOC. Then you'll dive into a discussion of Splunk management. Let's get started. Splunk serves as a unified hub for aggregating and correlating all logs generated throughout the GDC platform. Through custom visualizations and metrics definition, security analysts use Splunk to analyze and visualize security data using a user-friendly interface. The Splunk installation that you have access to has been specifically designed for GDC, following the MITRE ATT&CK framework, as well as other security best practices. Splunk offers a simplified interface and carefully defined pre-built features. These features are optimized to help you focus on what matters most in your day to day work as a security analyst in the GDC SOC. Splunk inputs data from all IT systems and allows you to effortlessly ingest data from all GDC services, as well as all GDC security tools. By correlating this information, you gain a comprehensive view that spots threats wherever they hide. Through Splunk, you gain access to a variety of outputs-- pre-built dashboards and reports for monitoring resource utilization, network activity, and more; automated alerts to keep you informed in real time, ensuring swift action; compliance reporting to meet stringent regulations with confidence. By leveraging these features, you're not just monitoring your environment, you're securing it with the precision and efficiency that today's digital landscape demands. Mastering Splunk is pivotal for conducting your work as a security analyst. First, you need to learn how to professionally use Splunk for in-depth security analysis. Splunk offers you the tools and capabilities to efficiently parse, index, and visualize large volumes of data in real time. This helps you detect anomalies, analyze security threats, and implement proactive defenses. Your ability to use Splunk for in-depth security analysis significantly enhances your organization's security posture and operational resilience. This has been the focus for the course so far. But secondly, you also need to learn how to effectively manage or administer the Splunk platform. Splunk management is the focus for this module. Managing the Splunk platform involves a comprehensive set of responsibilities that ensure the system operates efficiently, securely, and aligns with your organization's data analysis goals. This management encompasses several key activities-- troubleshooting, health checks, administrative support. Let's quickly explore each one. Reactive troubleshooting involves identifying and resolving issues as they arise within the Splunk environment. This is done to minimize downtime and ensure that Splunk remains a reliable source for real-time insights. Troubleshooting could range from addressing search performance problems, fixing data, ingestion errors, or resolving configuration issues. Beyond reacting to issues, you should also perform proactive health checks to prevent problems before they occur. This is done in order to maintain the system's reliability and performance. Health checks include monitoring the performance of the Splunk system, ensuring Splunk is optimized for operation, checking for necessary updates or patches, and verifying the integrity and security of the data being ingested. Your role in managing Splunk will also include routine administrative tasks that are critical for the system's health and security. Administrative support can involve managing access controls to ensure only authorized users can access sensitive data, performing regular backups to safeguard against data loss, and managing licenses and resources based on organizational requirements. In summary, your management of the Splunk platform will encompass a blend of troubleshooting, proactive maintenance, and administrative support tasks to ensure Splunk delivers actionable insights that are accurate. Additionally, the correct and up-to-date management of Splunk supports adherence to regulations like GDPR, HIPAA, and CCPA. It is vital to ensure Splunk operations meet compliance standards while protecting sensitive data from unauthorized access and potential breaches.

### Video - [Who is responsible for managing Splunk?](https://www.cloudskillsboost.google/course_templates/1197/video/522142)

* [YouTube: Who is responsible for managing Splunk?](https://www.youtube.com/watch?v=ibyogkr9HIw)

SPEAKER: All SOC analysts are users of the Splunk platform. Tier 1 analysts are users only. If they spot a possible update that is needed on the Splunk platform, Tier 1 analysts will need to escalate to tier 2 or tier 3 analysts for support. Tier 2 analysts and tier 3 analysts are both users and managers of the Splunk platform. As a tier 2 analyst, you are tasked with basic management of the SIEM. This primarily includes troubleshooting and making minor configuration adjustments through the GUI. Examples include categorization and adjusting error messages. To complete these tasks, you should have a basic understanding of common Splunk errors and how to perform administrative tasks. You should also have knowledge of fundamental troubleshooting techniques and how to adjust configurations using the GUI. Your primary tool will be the Splunk monitoring console for health checks in the GUI. As a tier 3 analyst, specifically as a security engineer, you are tasked with advanced management of the SIEM. This means you are ultimately responsible for the performance of Splunk. You are expected to immediately provide comprehensive support when the system is facing issues. This includes offering complex troubleshooting to solve advanced errors, as well as strategic implementations to address complex configuration changes. Your upscaling extends to using both the GUI and B tool. This requires a more in-depth engagement with Splunk's functionalities and an advanced understanding of Splunk errors and administrative tasks. You also use the Splunk monitoring console for health checks, but with both B tool and GUI. In summary, tier 2 and tier 3 analysts both play an important role in the management of Splunk. Whether you are tier 2 or tier 3, enhancing your proficiency with Splunk light management is critical. Tier 2 management entails tackling light administration tasks such as the following, Configuration settings, managing user roles, troubleshooting to ensure smooth operation and optimization of Splunk. On the other hand, tier 3 management delves into addressing the most intricate and pressing issues. It involves tasks such as the following, fine-tuning configurations, resolving advanced performance issues, implementing custom solutions. While a tier 3 analyst has a much deeper level of expertise in Splunk internals and troubleshooting skills, both analyst tiers require a foundational and shared understanding of Splunk's architecture and management tools. Developing this shared knowledge base is the focus of the rest of this module.

### Video - [Configuration files in Splunk](https://www.cloudskillsboost.google/course_templates/1197/video/522143)

* [YouTube: Configuration files in Splunk](https://www.youtube.com/watch?v=8exNu5Vp1hs)

SPEAKER: In order to effectively manage Splunk, it's important to understand how configuration files define the structure of the platform. The following videos will cover this topic. A Splunk configuration file, which is an ASCII, American Standard Code for Information Interchange, text file, plays a crucial role in how Splunk manages and processes data. These configuration files contain settings and parameters for various objects within the Splunk environment, such as inputs, indexes, sources, and data processing rules. Essentially, ASCII files define how Splunk behaves in specific scenarios, including data ingestion, indexing, searching, and reporting. Each configuration file is associated with a particular aspect of Splunk's functionality. The use of the ASCII format makes these files easy to edit with a standard text editor. This allows administrators and users to manually adjust Splunk's behavior to meet their specific needs. Configuration files offer flexibility so you can fine tune data collection, processing, and presentation. This makes it easier to optimize performance, enhance security, and ensure that the Splunk platform aligns with organizational requirements. Whether you are administering Splunk, using Splunk Web or btool, you are modifying one or more of the configuration files. So it's important to remember that Splunk configuration files are the key to customizing and controlling the Splunk platform's operational aspects. Configuration files provide a powerful tool for administrators to manage and troubleshoot the intricacies of data handling and analysis within their Splunk environments. Splunk uses a diverse array of configuration files to dictate its functionality. These include the management of data inputs, the customization of search behaviors, and more. Configuration files all share the use of the . conf extension, with two notable exceptions. These two exceptions stand out due to their unique file extensions, default.meta and instance.cfg. The default.meta file supports metadata management. This is crucial for managing access controls and permissions within Splunk. This file defines the default sharing and permission settings for knowledge objects like saved searches, dashboards, and reports. The instance.cfg file supports configuration-instance-specific parameters. This file contains configuration settings specific to a particular Splunk instance, such as server name identification and licensing information. These specialized files underscore the flexibility and granularity with which Splunk can be customized to meet an organization's needs. All configuration files are instrumental in adapting Splunk to align with distinct operational requirements, thereby enhancing security, operational efficiency, and regulatory compliance. Note the most commonly used configuration files highlighted here. Let's explore each one. The inputs.conf is crucial for defining how Splunk ingests data. This configuration file specifies the source, type, and method of data collection. Whether you're monitoring files and directories, configuring network data inputs, or setting up Windows event log collections, inputs.conf is where these definitions live. Splunk is a popular platform for searching, monitoring, and analyzing machine-generated big data. The inputs.conf file is a configuration file that tells Splunk to monitor the /var/log/myapp/logs directory for new or modified files. The data ingested from this directory will not be disabled. Disabled equals false. It will be stored in the myapp_logs index, and we'll use myapp_log as the source type. This can help with parsing and formatting data. The props.conf plays a pivotal role in how Splunk formats and handles ingested data. This configuration file includes settings for line breaking, timestamp recognition, character-set encoding, and field extraction at index time. Essentially, props.conf helps Splunk understand the structure of your data. The props.file applies to the myapp_log source type. It disables line merging, SHOULD_LINEMERGE equals false. This ensures that each line in a log file is treated as a separate event. The time_format specifies the timestamp format expected in the log data. This assists Splunk in accurately extracting event times. MAX_TIMESTAMP_LOOKAHEAD tells Splunk how many characters into an event to look for a timestamp. This optimizes the parsing process. The transforms.conf file works in tandem with props.conf to specify how data is transformed. This includes defining field extractions, data anonymization rules, and data filtering before indexing. This configuration file allows for complex manipulations and enrichments of the data. This example defines a field-extraction rule named myapp_extract_fields. The regex parameter uses a regular expression to parse each line of log data, capturing the date, time, log level, and message into named groups. The format parameter maps these groups to field names. write_meta indicates that the extracted fields should be written as metadata, making them searchable within Splunk. Each of these configuration files allows Splunk administrators to precisely control how data is ingested, indexed, and processed. This ensures efficient data management and facilitates powerful data analysis capabilities. The indexes.conf file controls the behavior and properties of indexes in Splunk. This configuration file specifies index locations, storage requirements, retention policies, and other index-specific settings. Effective management of indexes.conf is crucial for optimizing data storage, ensuring data is searchable, and retaining data according to compliance and operational needs. The indexes.conf configuration file defines a custom index named myapp_logs. indexes.conf sets the paths for the hot/warm homePath and cold coldPath buckets using the $SPLUNK_DB variable. The frozenTimePeriodInSecs parameter specifies that data older than 604,800 seconds, or seven days, will be frozen. This means it will be archived or deleted based on your Splunk environment's data-retention policy. The outputs.conf file configures how Splunk forwards data to other destinations, including other Splunk instances or third-party systems. This is essential for setting up distributed Splunk environments, adjusting performance settings, enabling data sharing, data routing, and the configuration of universal forwarders. This example configures Splunk to forward data to two indexers at 192.168.1.10 and 192.168.1.11 on port 9997, with load balancing enabled. The server.conf file is central to configuring system-level settings in Splunk. It includes configurations for general server properties, clustering, licensing, and various system parameters. Modifying server.conf allows administrators to adjust the core aspects of the Splunk environment, impacting how the system operates on a fundamental level. This example sets the server name and a symmetric key for encrypted communication and specifies the use of an enterprise license group. The limits.conf file controls various performance-related settings within Splunk. These include memory usage, search concurrency limits, and timeouts. Adjusting these settings can help optimize Splunk's performance for specific workloads. This example limits search queries to run for a maximum of 300 seconds and allows up to 10 concurrent searches. The alert_actions.conf file defines configurations for alert actions. When an alert is triggered, this configuration determines what actions are taken. This includes sending an email, executing a script, or logging an event. In this example, Splunk is configured to respond when an alert triggers. Here, Splunk sends an email to admin@example.com with a custom subject and a message containing the URL to the search results that triggered the alarm. The authentication.conf file is responsible for managing authentication mechanisms within Splunk. These include user credentials and authentication methods. This configuration file defines how users log in to the system and specify settings for built-in authentication or integration with external authentication providers, like LDAP, Active Directory, SAML, or custom scripts. By adjusting authentication.conf, administrators can enhance security and streamline user access based on organizational policies. This example configures Splunk to use LDAP for authentication. It specifies the LDAP server and binds a distinguished name, DN, for connecting to the LDAP server. The authorized.conf file specifies user roles and permissions. It defines which actions users can perform and what data they can access. authorized.conf is crucial for enforcing security and ensuring that users only have access to the data and functionality relevant to their role. This example sets up an admin role, with the ability to search all indexes, and a user role, with access to only the main index. Now that you know about some of the most frequently used configuration files in Splunk management, let's conclude by exploring how Splunk stores and updates these files. Configuration files are stored in the system or apps directory, depending on their scope and applicability. Configuration files in the apps directory, $SPLUNK HOME/Etc/Apps/appname/local, apply settings that are specific to that app. This means that the configuration files only impact the components and data associated with Splunk. This allows for localized customizations of app-specific behaviors, features, and data-processing rules, without affecting the global Splunk configuration or other apps. Configuration files in the system directory, $SPLUNK_HOME/Etc/System/local, apply globally. These impact the overall Splunk environment and all apps that are installed on that Splunk instance. This distinction enables both broad and targeted configuration of the Splunk platform. When Splunk operates, it consolidates configuration files by merging them into a unified runtime model for each file type. This merging typically occurs during Splunk's startup phase or when searches are executed. Essentially, the merged configuration represents a combination of all relevant files and ensures that each file's settings contribute to the overall configuration. If there are no conflicts or duplicate settings among the merged files, Splunk merges them seamlessly, resulting in a unified configuration without any discrepancies. However, if conflicts arise where multiple files contain conflicting settings, Splunk determines priority based on the context in which the settings are applied. In cases of conflict, priority is assigned based on two main contexts. One, global context-- index-time. Settings defined at the global level, such as those pertaining to index time operations, take precedence over settings specified at the app or user level. These settings influence indexing processes and other global functionalities within Splunk. Two, app/user context-- search-time. Settings specified within the context of a particular app or user, such as those used during search time operations, take precedence in their respective contexts. These settings affect search-time behavior and functionalities within Splunk.

### Video - [Administering Splunk with Splunk Web and btool](https://www.cloudskillsboost.google/course_templates/1197/video/522144)

* [YouTube: Administering Splunk with Splunk Web and btool](https://www.youtube.com/watch?v=3m2xPPumi3E)

SPEAKER: Splunk can be efficiently managed through both the intuitive Splunk web interface for graphical configuration and the powerful btool command line utility. The GUI, graphical user interface, in Splunk offers an intuitive, user-friendly environment for managing configurations, conducting data analysis, and visualizing results. This approach makes Splunk accessible to users across various skill levels. This facilitates easier adoption and the more effective use of Splunk's powerful features for data insights. Splunk Web offers a graphical interface that simplifies the management and configuration of Splunk environments. This approach has several advantages over direct file editing. Splunk Web provides a more intuitive way to navigate configurations, making it easier for users to perform tasks without deep knowledge of Splunk's file structure. The graphical interface reduces the risk of syntax and formatting errors that can occur with direct file editing. Splunk Web offers immediate validation and feedback on configurations. This helps administrators immediately troubleshoot issues. Now let's pivot to learn about Splunk's btool. Splunk's btool is a command line utility that enables you to examine configuration file issues and thus better troubleshoot and understand active configurations within your Splunk environment. The btool allows for the effective diagnosis and management of configurations. This aids in ensuring that Splunk runs smoothly and according to its intended settings. Splunk's btool merges configuration files from various directories and is then able to identify conflicts. How? Btool starts by examining the configuration files spread across Splunk's directory structure. Btool processes these files in sequence. This sequence is dictated by Splunk's hierarchy structure, which prioritizes specific settings over more general ones. Btool simulates the merging process, thereby showing you how settings from different sources are combined. This simulation mimics how Splunk behaves when it launches or when configurations are reloaded. Btool points out which configurations are being overridden, thereby clarifying the precedence of settings and where each setting originates. Btool also flags any conflicts or duplicate settings that might lead to problems in your Splunk setup. This enables you to troubleshoot and resolve issues before they impact your system. Now that you are familiar with Splunk Web and btool, let's quickly compare them. Through GUI, graphical user interface, you can do the following-- totally manage user roles and permissions, do a complete setup and configuration of data inputs, fully manage data indexing settings as well as the search functionality, and report generation, have complete control over user permissions and security settings, configure security settings where GUI options are available and have full deployment management capabilities, partially inspect elements that have corresponding UI components, have direct configuration for some settings, edit configuration files if necessary, use the GUI to partially browse the file system with restricted browsing capabilities, automate certain tasks using the REST API in the GUI. Now let's compare the GUI capabilities with those of btool, command line utility. Btool offers full capabilities to inspect configuration files and understand how settings are applied. To do this, you must possess a comprehensive understanding of the file system structure. You must also have been granted the capability to inspect and troubleshoot automated scripts. Through btool, you gain partial capability to inspect user related configurations. Btool allows limited inspection of data input configurations, index configurations, access control settings, and deployment-related configurations. Btool allows partial capabilities for the inspection of security configurations. Btool is not applicable for direct management in areas such as search and reporting or performance monitoring, which are fully manageable in the GUI. The GUI provides a more user friendly environment with visual tools for managing configurations while btool is powerful for backend inspection and troubleshooting tasks.

### Video - [Best practices and advanced troubleshooting in Splunk](https://www.cloudskillsboost.google/course_templates/1197/video/522145)

* [YouTube: Best practices and advanced troubleshooting in Splunk](https://www.youtube.com/watch?v=aRwsKn32mK8)

SPEAKER: In this video, you'll consider best practices to navigate Splunk more effectively. First, attempt to accurately estimate the amount of data you'll be ingesting into Splunk. This helps prevent any performance bottlenecks. Remember to regularly review and adjust your system's capacity as your data grows. Make sure to normalize your data during the ingestion process. This simplification will make your searching and reporting tasks much more straightforward. Real-time searches can be resource intensive. Instead, schedule your searches to optimize system performance. Implement robust security measures such as role-based access controls and data encryption. These types of practices are crucial to safeguarding your sensitive information. You may encounter Splunk errors during the initial setup phase when making updates or when implementing complex settings. Common configuration errors in Splunk include inputs.conf Syntax Error. These relate to a misconfigured log file path for Cymbal Federal's network monitoring logs. This leads to gaps in data monitoring. Parsing errors in props.conf. This relates to incorrectly defining the time underscore format for Cymbal Federal's transaction logs. This causes inaccurate time based data analysis. Index settings misconfiguration in indexes.conf. This means an inappropriate max size for the security events index was set. This can potentially cause early data rollover and the loss of critical forensic information. Sourcetype mismatches. This means assigning a generic source type to Cymbal Federal's custom application logs. This can result in ineffective log parsing and analysis. Case Sensitivity Issues. This relates to incorrectly capitalizing field names in transforms.conf, meaning names don't match the case used in search queries. This can lead to data being excluded from search results. Invalid Path Specifications in inputs.conf. This means providing incorrect paths to Cymbal Federal's encrypted file shares for data ingestion. This can prevent Splunk from accessing vital operational data. Advanced troubleshooting techniques in Splunk extend beyond basic log analysis. Splunk offers multiple methods to enable deeper investigation into system behaviors and performance. This facilitates the quicker resolution of intricate issues. Splunk Diag, a command line tool for collecting detailed information about your Splunk environment. This is helpful for in-depth analysis. Splunk Dashboard Studio. Use it to create custom dashboards that monitor system health and performance metrics. This aids in the early detection of issues. Custom scripts. Write scripts to automate the monitoring of log files and provide alerts for specific error patterns. This aids in preemptively addressing problems. External monitoring tools. Integrate with network monitoring solutions to provide a broader view of infrastructure health. This can help highlight potential issues that might impact Splunk's capabilities and functionalities. Community and support forums. Leverage the Splunk community and official support forums for insights and solutions to complex issues encountered by others.

### Video - [Splunk management at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1197/video/522146)

* [YouTube: Splunk management at Cymbal Federal](https://www.youtube.com/watch?v=4mhoJ6KDWFU)

SPEAKER: SOC analysts can access the Splunk Web UI for investigation, searching, and reporting on security events. Tier 1 analysts do not have access to the Administer Splunk or its web UI. Now, let's put everything you've learned together and explore an example of Splunk Management at Cymbal Federal. What do you do? Your first step would be the same as with any incident. You check if there is a runbook that provides ad hoc actions. In this case, a runbook exists to troubleshoot and fix the unable to access web interface issue for Splunk. This runbook provides directions for two errors, one, ERR_CONNECTION_TIME_OUT; two, ERR_CONNECTION_REFUSED. It has four parts, one, prerequisites; two, plan of attack; three, Splunk restart; four, escalation. For prerequisites, you need to ensure you are using the correct tooling, a Microsoft Remote Desktop connection and a browser to access the Splunk Web Interface. You also need to ensure you have the right level of access as an OC IT admin. After ensuring you meet the prerequisites, you can move on to the plan of action. The plan of action in the runbook highlights that the access issue is likely due to a firewall setting needing to be fixed. The runbook provides a multi-step, detailed resolution plan. To assess and fix the firewall settings, you need to do the following. Initial diagnosis, log in through RDP to the Splunk Search Head Server, identified cfed-searchhead. Firewall rule check, use PowerShell to run as an administrator. Then, as the admin, check for existing firewall rules that allow web access to Splunk. The command, get dash net firewall rule pipe where dash object bracket dollar sign display name dash EQ, quote, "allows web access," unquote, bracket reveals no active rules. This indicates a likely cause for the access issues. Firewall rule configuration. Since the rule is missing, you proceed to remove any outdated or incorrect rules. You create a new rule specifically designed to allow inbound traffic on port 8,000 for Splunk. This ensures it is enabled for both domain and private networks. Now you can validate if you have access. If the firewall settings have been addressed and the issue persists, the runbook suggests you restart Splunk services to apply the changes. This involves manually stopping any running Splunk processes through the Task Manager, and then, using PowerShell to execute a Splunk service restart. If the issue still persists after remediation, then the runbook suggests escalating the issue following the steps outlined in supp-g0008 for advanced troubleshooting support.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1197/video/522147)

* [YouTube: Module review](https://www.youtube.com/watch?v=asQj9EBviHg)

SPEAKER: This module provided an overview of Splunk management to help support SOC analysts in becoming Splunk power users. You explored the Splunk setup for GDC, learned why Splunk management is so important, and discussed the role of tier 2 and tier 3 analysts in managing Splunk. You walked through the different types of configuration files available in Splunk management and learned about administering Splunk using Splunk Web and btool. You looked at best practices for using and configuring Splunk and then briefly reviewed advanced troubleshooting techniques and common configuration challenges. The module concluded with an example of Splunk management at symbol federal. You are now ready to wrap up the course.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1197/quizzes/522148)

#### Quiz 1.

> [!important]
> **What is the role of inputs.conf in Splunk?**
>
> * [ ] Specifies data inputs
> * [ ] Inputs user role definitions and permissions
> * [ ] Controls the input of alert thresholds and actions
> * [ ] Regulates input display settings in the user interface

#### Quiz 2.

> [!important]
> **What can you manage through the Splunk Web interface (GUI)?**
>
> * [ ] Detailed configuration file inspection
> * [ ] Searching and reporting functionalities
> * [ ] Automated reporting of network configuration changes
> * [ ] Reporting of real-time server hardware utilization

#### Quiz 3.

> [!important]
> **What is the primary focus of Splunk Management?**
>
> * [ ] To streamline data ingestion processes
> * [ ] To customize the appearance of the Splunk Web interface
> * [ ] To enhance search capabilities and performance
> * [ ] To optimize secure and efficient operations to meet analytical objectives

## Resources

Review course content

### Document - [Course slides](https://www.cloudskillsboost.google/course_templates/1197/documents/522149)

### Document - [Additional resources](https://www.cloudskillsboost.google/course_templates/1197/documents/522150)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
