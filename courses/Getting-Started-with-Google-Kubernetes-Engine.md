---
id: 2
name: 'Getting Started with Google Kubernetes Engine'
type: Course
url: https://www.cloudskillsboost.google/course_templates/2
date_published: 2024-12-13
topics:
  - GKE
  - Kubernetes
  - Docker Image
---

# [Getting Started with Google Kubernetes Engine](https://www.cloudskillsboost.google/course_templates/2)

**Description:**

Welcome to the Getting Started with Google Kubernetes Engine course. If you're interested in Kubernetes, a software layer that sits between your applications and your hardware infrastructure, then you're in the right place! Google Kubernetes Engine brings you Kubernetes as a managed service on Google Cloud.

The goal of this course is to introduce the basics of Google Kubernetes Engine, or GKE, as it's commonly referred to, and how to get applications containerized and running in Google Cloud. The course starts with a basic introduction to Google Cloud, and is then followed by an overview of containers and Kubernetes, Kubernetes architecture, and Kubernetes operations.

**Objectives:**

* Discuss the differences among Google Cloud compute platforms.
* Discuss the components and architecture of Kubernetes.
* Identify how Google manages Kubernetes orchestration.
* Create and manage Google Kubernetes Engine clusters by using the Google Cloud console and gcloud/kubectl commands.

## Course Introduction

The course introduction explains the course goals and previews each section.

### Document - [Welcome and getting started guide](https://www.cloudskillsboost.google/course_templates/2/documents/517294)

### Video - [Course introduction](https://www.cloudskillsboost.google/course_templates/2/video/517295)

* [YouTube: Course introduction](https://www.youtube.com/watch?v=bf6Zr6BFfn0)

Welcome to the “Getting Started with Google Kubernetes Engine” course. If you’re interested in Kubernetes, a software layer that sits between your applications and your hardware infrastructure, then you’re in the right place! Google Kubernetes Engine brings you Kubernetes as a managed service on Google Cloud. The goal of this course is to introduce the basics of GKE, as it’s commonly referred to, and how to get applications containerized and running in Google Cloud. Through a combination of videos, quizzes, and hands-on labs, you’ll get comfortable interacting with GKE. The intended audience for this course will have a basic proficiency of command-line tools, Linux operating system environments, and web server technologies such as Nginx. It’s also helpful to have systems operations experience, including deploying and managing applications, either on-premises or in a public cloud environment. The course starts with a basic introduction to Google Cloud, is followed by an overview of containers and Kubernetes, Kubernetes architecture, and Kubernetes operations. Okay, let’s get started!

## Introduction to Google Cloud

The first section of this course introduces cloud computing concepts. Learners explore fundamental terminology, the Google Cloud network, how Google Cloud resources are organized in an hierarchy for management, and the tools available to connect to Google Cloud for allocating, changing, and releasing resources.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/2/video/517296)

* [YouTube: Introduction](https://www.youtube.com/watch?v=vqgw2cSrEEI)

Before you jump into the world of Kubernetes, let’s take some time to introduce you to, or remind you of, some Google Cloud core concepts. In this first section of this course, you’ll explore: The definition of cloud computing. The services Google Cloud offers architects and developers to build solutions. How Google’s powerful global network can power Google Cloud services. How Google Cloud resources are structured and managed. Tools to ensure that your organization doesn't accidentally face a big Google Cloud bill. And four different ways to interact with Google Cloud to commence work. You’ll also get some hands-on practice accessing the Cloud console and Cloud Shell.

### Video - [Cloud computing and Google Cloud](https://www.cloudskillsboost.google/course_templates/2/video/517297)

* [YouTube: Cloud computing and Google Cloud](https://www.youtube.com/watch?v=y265d3oreJw)

Let’s start at the beginning with an overview of cloud computing. Cloud computing is a way of using information technology, IT, that has these five equally important traits. First, customers get computing resources that are on-demand and self-service. Through a web interface, users get the processing power, storage, and network they need with no need for human intervention. Second, customers get access to those resources over the internet, from anywhere they have a connection. Third, the cloud provider has a big pool of those resources and allocates them to users out of that pool. That allows the provider to buy in bulk and pass the savings on to the customers. Customers don't have to know or care about the exact physical location of those resources. Fourth, the resources are elastic–which means they’re flexible, so customers can be. If they need more resources they can get more, and quickly. If they need less, they can scale back. And finally, customers pay only for what they use, or reserve as they go. If they stop using resources, they stop paying. That's it. That's the definition of cloud. Google Cloud offers a variety of services for architects and developers to use to build solutions. Some might sound familiar, like virtual machines, whereas others might represent a totally new paradigm, like Google Kubernetes Engine. A common first request that organizations make of Google Cloud is to run some code in the cloud. Google offers a range of computing services to help fulfill that request. We’ll explore these options next.

### Video - [Google Cloud compute offerings](https://www.cloudskillsboost.google/course_templates/2/video/517298)

* [YouTube: Google Cloud compute offerings](https://www.youtube.com/watch?v=1jv14Tzr4nc)

SPEAKER: As organizations design for the future and start running more compute workloads in Google Cloud, it's important to be aware of the options available. Google offers a range of computing services. Let's explore each. The services are Compute Engine, GKE, App Engine, Cloud Run functions, and Cloud Run. At the end of this lesson, you'll understand why people choose each. The first is Compute Engine. Compute Engine is an IaaS offering, or Infrastructure as a Service, which provides compute storage and network virtually that are similar to physical data centers. Compute Engine provides access to predefined and customized virtual-machine configurations. At the time this training was developed, VMs could be as large as 416 vCPUs with more than terabytes of memory. Virtual machines require block storage, and Compute Engine offers two main choices-- persistent disks and local SSDs. Persistent disks offer network storage that can scale up to 257 terabytes and can disk snapshots for backup and mobility. Alternatively, local SSDs enable high input/output operations per second. Compute Engine workloads can be placed behind global load balancers that support autoscaling. They offer a feature called managed instance groups, with which resources can be defined to automatically deploy to meet demand. Compute Engine costs can be controlled with the help of per-second billing. This means that when compute resources are deployed for short periods of time, like with batch-processing jobs, costs can stay low. Compute Engine also offers preemptible virtual machines, which provide significantly cheaper pricing for workloads that can be interrupted safely. Compute Engine is a popular choice for developers because it provides complete control over infrastructure since operating systems can be customized, and it can run applications that rely on a mix of operating systems. On-premises workloads can easily be lifted and shifted to Google Cloud without needing to rewrite applications or make any changes. And it's the best option when other computing options don't support your application or requirements. Google Cloud's second compute offering, and the focus of this course, is Google Kubernetes Engine. GKE runs containerized applications in a cloud environment as opposed to on an individual virtual machine like Compute Engine. A container represents code packaged with all its dependencies. The third computing service offered by Google is App Engine. App Engine is a fully managed PaaS offering, or Platform as a Service. PaaS offerings bind code to libraries that provide access to the infrastructure-application needs. This allows more resources to be focused on application logic instead of deployment. This means developers can just upload code and App Engine will deploy the required infrastructure. It supports popular languages like Java, Node.js, Python, PHP, C sharp, . NET, Ruby, and Go, and can also be used to run container workloads. App Engine is closely integrated with Cloud Monitoring, Cloud Logging, Cloud Profiler, and Error Reporting. App Engine also supports version control and traffic splitting. App Engine is a good choice for developers that want to focus on writing code, want to focus on building applications instead of deploying and managing the environment, and don't need to build a highly reliable and scalable infrastructure. Some of the most Common App Engine use cases include websites, mobile-app and gaming backends, and a method to present a RESTful API, which is an application-programming interface that resembles the way a web browser interacts with a web server to the internet. App Engine makes them easy to operate. Another compute option offered by Google Cloud is Cloud Run functions. Cloud Run functions is a lightweight, event-based, asynchronous compute solution for creating small, single-purpose functions that respond to cloud events without the need to manage a server or a runtime environment. It executes code in response to events, like when a new file is uploaded to cloud storage. It's also a completely serverless execution environment. Cloud Run functions is often referred to as functions as a service. Simply upload code written in Node.js. Python, Go, Java, . NET core, Ruby or PHP, and Google Cloud will automatically deploy the appropriate computing capacity to run that code. These functions can be used to construct application workflows from individual business logic tasks. Cloud Run functions can also be used to connect and extend cloud services. You're billed to the nearest 100 milliseconds but only while your code is running. Cloud Run functions also provides a perpetual free tier. So many Cloud function use cases can be free of charge. What are common Cloud Run functions? It can be part of a microservices application architecture. It is used to build simple, serverless mobile or IoT backends or integrate with third-party services and APIs. Files uploaded to a Cloud Storage bucket can be processed in real time. Similarly, the data can be extracted, transformed, and loaded for querying and analysis. And it can be part of intelligent applications, such as virtual assistants, video or image analysis and sentiment analysis. And finally, there is Cloud Run. Cloud Run is a managed compute platform that runs stateless containers through web requests or Pub/Sub events. Cloud run is serverless. That means it removes all infrastructure-management tasks so you can focus on developing applications. It's built on Knative, an open API and runtime environment built on Kubernetes, that gives you freedom to move your workloads across different environments and platforms. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously. And it charges only for the resources used. Calculated down to the nearest 100 milliseconds, so you never pay for overprovisioned resources.

### Video - [The Google network](https://www.cloudskillsboost.google/course_templates/2/video/517299)

* [YouTube: The Google network](https://www.youtube.com/watch?v=_mOZS-uZTYQ)

SPEAKER: Behind the services provided by Google Cloud lies a huge range of resources, from physical assets like servers to virtual resources like virtual machines and containers. It all runs on Google's own global network. According to some publicly available estimates, Google's network carries as much as 40% of the world's internet traffic every day. Google's network is the largest network of its kind, and Google has invested billions of dollars over the years to build it. This network is designed to give customers the highest possible throughput and lowest possible latencies for their applications by using more than 100 content caching nodes worldwide. These are locations where high-demand content is cached for quicker access, which allows applications to respond to user requests from the location that provides the quickest response time. Google Cloud's infrastructure is based in five major geographic locations-- North America, South America, Europe, Asia, and Australia. Having multiple service locations is important because choosing where to locate applications affects qualities like availability, durability, and latency. Each of these locations is divided into several different regions and zones. Regions represent independent geographic areas and are composed of zones. For example, London, or europe-west2, is a region that currently comprises three different zones. A zone is an area where Google Cloud resources are deployed. For example, if you use Compute Engine to launch a virtual machine, it will run in the zone that you specify to ensure resource redundancy. You can also run resources in different regions. This is useful for bringing applications closer to users around the world, and also for protection if issues with an entire region occur, such as a natural disaster. Google Cloud currently supports 124 zones in 41 regions, although this number is constantly increasing. You can find the most up-to-date numbers at cloud.google.com /about/locations.

### Video - [Resource management](https://www.cloudskillsboost.google/course_templates/2/video/517300)

* [YouTube: Resource management](https://www.youtube.com/watch?v=6Igov5qBjXc)

Every Google Cloud resource you use must belong to a project. But what is a project? A project is a container for all your Google Cloud resources. It provides a way to organize resources, manage billing, and control access. Each project even has a unique identifier. Google Cloud’s resource hierarchy contains four levels, and starting from the bottom up they are: resources, projects, folders, and an organization node. Resources are at the first level. These represent containers, virtual machines, tables in BigQuery, or anything else in Google Cloud. Resources are organized into projects, which sit on the second level. Projects can be organized into folders, or even subfolders. These sit at the third level. And then at the top level is an organization node, which encompasses all the projects, folders, and resources in your organization. It’s important to understand this resource hierarchy because it directly relates to how policies are managed and applied when you use Google Cloud. Policies can be defined at the project, folder, and organization node levels. Some Google Cloud services allow policies to be applied to individual resources, too. Policies are inherited downward. This means that if you apply a policy to a folder, it will also apply to all of the projects within that folder. Let’s look at the second level of the resource hierarchy, projects, in a little more detail. Projects are the basis for enabling and using Google Cloud services, like managing APIs, enabling billing, adding and removing collaborators, and enabling other Google services. Each project is a separate entity under the organization node, and each resource belongs to exactly one project. Projects can have different owners and users because they’re billed and managed separately. Each Google Cloud project has three identifying attributes: a project ID, a project name, and a project number. The project ID is a globally unique identifier assigned by Google that can’t be changed after creation. They’re what we refer to as being immutable. Project IDs are used in different contexts to inform Google Cloud of the exact project to work with. Project names, however, are user-created. They don’t have to be unique and they can be changed at any time, so they are not immutable. Google Cloud also assigns each project a unique project number. It’s helpful to know that these Google-generated numbers exist, but we won’t explore them much in this course. They’re mainly used internally by Google Cloud to keep track of resources. You can use folders to group projects under an organization in a hierarchy. For example, your organization might contain multiple departments, each with its own set Google Cloud resources. Folders allow you to group these resources on a per-department basis. Folders also give teams the ability to delegate administrative rights so that they can work independently. And when an organization node contains lots of folders, projects, and resources, a workforce might need to restrict who has access to what. To help with this task, administrators can use Identity and Access Management, or IAM. With IAM, administrators can apply policies that define who can do what and on which resources. The “who” part of an IAM policy can be a Google account, a Google group, a service account, or a Cloud Identity domain. A “who” is also called a “principal.” Each principal has its own identifier, usually an email address. The “can do what” part of an IAM policy is defined by a role. An IAM role is a collection of permissions. When you grant a role to a principal, you grant all the permissions that the role contains. For example, to manage virtual machine instances in a project, you must be able to create, delete, start, stop and change virtual machines. So these permissions are grouped into a role to make them easier to understand and easier to manage. So, who is responsible for security in the cloud? It’s a shared responsibility between you and Google Cloud. A general guideline for shared responsibility is that "if you configure or store it, you're responsible for securing it." This means that a cloud provider is responsible for securing the parts of the cloud that it directly controls, such as hardware, networks, and physical security. At the same time, the customer is responsible for securing anything that they create within the cloud, such as the configurations, access policies, and user data. No matter which cloud provider you use, there is shared responsibility.

### Video - [Billing](https://www.cloudskillsboost.google/course_templates/2/video/517301)

* [YouTube: Billing](https://www.youtube.com/watch?v=H8MBuunbLPc)

Let’s explore how billing works in Google Cloud. Billing is established at the project level of the Google Cloud resource hierarchy. This means that when you define a Google Cloud project, you link a billing account to it. This billing account is where you will configure all your billing information, including your payment option. A billing account can be linked to zero or more projects, but projects that aren’t linked to a billing account can only use free Google Cloud services. Billing accounts are charged automatically and invoiced every month or at every threshold limit. Billing subaccounts can be used to separate billing by project. Some Google Cloud customers who resell Google Cloud services use sub accounts for each of their own clients. Now, you’re probably thinking, “How can I ensure that I don’t accidentally face a big Google Cloud bill?” We provide a few tools to help. You can define budgets at the billing account level or at the project level. A budget can be a fixed limit, or it can be tied to another metric; for example, a percentage of the previous month’s spend. To be notified when costs approach your budget limit, you can create an alert. For example, with a budget limit of $20,000 and an alert set at 90%, you’ll receive a notification alert when your expenses reach $18,000. Alerts are generally set at 50%, 90% and 100%, but can also be customized. Reports is a visual tool in the Google Cloud console that lets you monitor expenditure based on a project or services. Finally, Google Cloud also implements quotas, which are designed to prevent the over-consumption of resources because of an error or a malicious attack. This way both account owners and the Google Cloud community as a whole are protected. There are two types of quotas: rate quotas and allocation quotas. Both are applied at the project level. Rate quotas reset after a specific time. For example, by default, the GKE service implements a quota of 3,000 calls to its API from each Google Cloud project every 100 seconds. After that 100 seconds, the limit is reset. Allocation quotas govern the number of resources that you can have in your projects. For example, by default, each Google Cloud project has a quota that allows it no more than five Virtual Private Cloud networks. Although all projects start with the same quotas, you can change some of them by requesting an increase from Google Cloud Support.

### Video - [Interacting with Google Cloud](https://www.cloudskillsboost.google/course_templates/2/video/517302)

* [YouTube: Interacting with Google Cloud](https://www.youtube.com/watch?v=9U6kSWvKTN8)

SPEAKER: Now that you've had a chance to explore how resources in Google Cloud run, are organized, and billed to you, it's time to see how you actually interact with Google Cloud to start your work. You can use four Google products to access and interact with Google Cloud. The Google Cloud Console, the Google Cloud SDK and Cloud Shell, the APIs, and the Google Cloud app. Let's explore each. The first is the Google Cloud Console, which is Google Cloud's graphical user interface, or GUI, and it helps you deploy, scale and diagnose production issues in a simple web-based interface. With the Google Cloud Console, you can easily find your resources, check their health, have full management control over them, and set budgets to control how much you spend on them. The Google Cloud Console also provides a search facility to quickly find resources and connect instances through SSH in the browser. The Google Cloud Console is available from console.cloud.google.com. You'll get some experience with the Cloud Console during an upcoming lab. The second products are the Google Cloud SDK and Cloud Shell. Unlike the Google Cloud Console, you can download and install the Google Cloud SDK locally onto a computer. The Google Cloud SDK is a set of tools that you can use to manage resources and applications hosted on Google Cloud. These include the gcloud tool, which provides the main command line interface for Google Cloud products and services, gcloud storage, which lets you access cloud storage from the command line, and bq, a command line tool for BigQuery. When installed, all of the tools within the Google Cloud SDK are located under the bin directory. Cloud Shell provides command line access to cloud resources directly from a browser. Cloud Shell is a Debian-based virtual machine with a persistent five gigabyte home directory, which makes it easy to manage Google Cloud projects and resources. Each Cloud Shell VM is ephemeral, which means that it will be stopped whenever you stop using it interactively and it will be restarted when you re-enter Cloud Shell. It also provides web preview functionality and built-in authorization for access to Cloud Console projects and resources, including your GKE resources. With Cloud Shell, the Google Cloud SDK gcloud command and other utilities are always installed, available, up to date, and fully authenticated. The Cloud Console's GKE area has a web based interface for administering GKE resources. The Cloud Shell is the place to launch commands to administer those GKE resources. Some of those commands are from the Google Cloud SDK and others will be specific to your workload. Later in this course, you'll learn about the kubectl command, and you can see it being launched from Cloud Shell here. Third way to access Google Cloud is through application programming interfaces or APIs. The services that make up Google Cloud offer APIs so that code you write can control them. The Cloud Console includes a tool called the Google APIs Explorer that shows which APIs are available and in which versions. You can try these APIs interactively, even those that require user authentication. One important point to note is that developers often use APIs to build applications that allocate and manage Google Cloud resources. However, our present focus is on letting Kubernetes manage resources for us. And finally, the fourth way to access and interact with Google Cloud is with the Google Cloud app, which can be used to start, stop, and use SSH to connect to Compute Engine instances and see logs from each instance. It also lets you stop and start Cloud SQL instances. Additionally, you can administer applications deployed on App Engine by viewing errors, rolling back deployments, and changing traffic splitting. The Google Cloud app isn't relevant for the purposes of this course.

### Video - [Lab Introduction: Accessing the Cloud console and Cloud Shell](https://www.cloudskillsboost.google/course_templates/2/video/517303)

* [YouTube: Lab Introduction: Accessing the Cloud console and Cloud Shell](https://www.youtube.com/watch?v=7BNXWDNUwhw)

It’s time to gain some hands-on experience with some of the Google Cloud tools featured in this section of the course. In the lab titled “Accessing the Google Cloud console and Cloud Shell,” you’ll get practice creating buckets, virtual machines, and service accounts from both the Google Cloud console and Cloud Shell. You’ll also get practice executing other commands through Cloud Shell.

### Lab - [Accessing the Google Cloud Console and Cloud Shell](https://www.cloudskillsboost.google/course_templates/2/labs/517304)

Architecting with Google Kubernetes Engine: Introduction to Google Cloud

* [ ] [Accessing the Google Cloud Console and Cloud Shell](../labs/Accessing-the-Google-Cloud-Console-and-Cloud-Shell.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/2/quizzes/517305)

#### Quiz 1.

> [!important]
> **You are considering deploying a solution by using containers on Google Cloud. What Google Cloud solution provides a managed compute platform with native support for containers?**
>
> * [ ] Cloud Run functions
> * [ ] Artifact Registry
> * [ ] Compute Engine autoscaling groups
> * [ ] Google Kubernetes Engine clusters

#### Quiz 2.

> [!important]
> **One of the main characteristics of cloud computing is that resources are elastic. What does that mean?**
>
> * [ ] You share resources from a large pool that enables economies of scale.
> * [ ] You can quickly get more resources when you need them.
> * [ ] Resources can be allocated automatically.
> * [ ] When customers need more resources, they can get more. When they need less, they can scale back.

#### Quiz 3.

> [!important]
> **What Identity and Access Management (IAM) hierarchy structure is best for building an application in Google Cloud?**
>
> * [ ] Create a new organization node for the project, then create all projects and resources inside the new organization node.
> * [ ] Create a new folder inside your organization node, then create projects inside that folder for the resources.
> * [ ] Create new projects and resources inside departmental folders for the resources needed, organized by the component applications.
> * [ ] Create new projects for each of the component applications, then create folders inside those for the resources.

#### Quiz 4.

> [!important]
> **You are developing a new product for a customer and need to be mindful of cost and resources. What Google Cloud tools can be used to ensure costs stay manageable before consumption gets too high?**
>
> * [ ] Configure the billing account for each project associated with the product.
> * [ ] Configure quotas and limits for each product folder.
> * [ ] Set up budgets and alerts at the project level.
> * [ ] Configure the billing account at the product folder level in the resource hierarchy.

## Introduction to Containers and Kubernetes

The second section of this course examines software containers and the benefit they bring to application deployment. Learners explore containers and container images, Cloud Build, Kubernetes, and Google Kubernetes Engine.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/2/video/517306)

* [YouTube: Introduction](https://www.youtube.com/watch?v=QeRfg1do03Q)

Containerization helps development teams move fast, deploy software efficiently, and operate at an unprecedented scale. But what exactly are they, how do they work, and where does Kubernetes come into play? The goal of this second section of the course was designed to help answer those questions. You’ll explore containers, container images, Kubernetes, and Google Kubernetes Engine. You’ll also get hands-on practice with Cloud Build. Let’s get started!

### Video - [Containers](https://www.cloudskillsboost.google/course_templates/2/video/517307)

* [YouTube: Containers](https://www.youtube.com/watch?v=YzMZADRadxM)

So, what is a container? To answer that question, we’ll need to first explore how applications are deployed. Not long ago, the common way to deploy an application was on a local computer. To set one up, you needed physical space, power, cooling, and network connectivity. Then you needed to install an operating system, any software dependencies, and finally, the application. When you needed more processing power, redundancy, security, or scalability, you’d add more computers. And it was very common for each computer to have a single purpose, for example, for a database, web server, or content delivery. This practice wasted resources and took a lot of time to deploy, maintain, and scale. Then came virtualization, which is the process of creating a virtual version of a physical resource, such as a server, storage device, or network. Virtualization made it possible to run multiple virtual servers and operating systems on one local computer. The software layer that breaks the dependencies of an operating system on the underlying hardware and allows several virtual machines to share that hardware is called a a hypervisor. Kernel-based Virtual Machine, or KVM, is one well-known hypervisor. Today, you can use virtualization to deploy new servers fairly quickly. With virtualization, it takes less time to deploy new solutions. Fewer resources are wasted, and portability is improved because virtual machines can be imaged and easily moved. However, an application, all its dependencies, and operating system are still bundled together. It’s not easy to move a VM from one hypervisor product to another, and every time you start a VM, its operating system takes time to boot up. But running multiple applications within a single VM creates another problem: applications that share dependencies are not isolated from each other. The resource requirements of one application can starve other applications of the resources they need. Also, a dependency upgrade for one application might cause another to stop working. You can try to solve this problem with rigorous software engineering policies. For example, you can lock down the dependencies so that no application is allowed to make changes; but this can lead to new problems because dependencies need to be upgraded occasionally. You can also add integration tests to ensure applications work as intended. However, dependency problems can cause novel failure modes that are hard to troubleshoot. Plus, it really slows down development if you have to rely on integration tests to confirm the basic integrity of your application environment. The VM-centric way to solve this problem is to run a dedicated virtual machine for each application. Each application maintains its own dependencies, and the kernel is isolated so one application won't affect the performance of another. The result is that two complete copies of the kernel are running. Scale this to hundreds or thousands of applications and you see its limitations. A more efficient way to resolve the dependency problem is to implement abstraction at the level of the application and its dependencies. You don’t have to virtualize the entire machine, or even the entire operating system–just the user space. The user space is all the code that resides above the kernel, and it includes applications and their dependencies. This is what it means to create containers. Containers are isolated user spaces for running application code. Containers are lightweight because they don’t carry a full operating system, can be scheduled or integrated tightly with the underlying system, which is efficient, and can be created and shut down quickly, because they just start and stop operating system processes and do not boot an entire VM or initialize an operating system for each application. With containers, you can still develop application code in the usual ways–on desktops, laptops, and servers. However, the container can execute final code on VMs. The application code is packaged with all the dependencies it needs, and the engine that executes the container is responsible for making them available at runtime. But what makes containers so appealing to developers? First, they’re a code-centric way to deliver high-performing, scalable applications. Second, containers provide access to reliable underlying hardware and software. With a Linux kernel base, developers can be confident that code will run successfully regardless if it's on a local machine or in production. And if incremental changes are made to a container based on a production image, it can be deployed quickly with a single file copy. This speeds up development. And finally, containers make it easier to build applications that use the microservices design pattern–that is, with loosely coupled, fine-grained components. This modular design pattern allows the operating system to scale and upgrade components of an application without affecting the application as a whole.

### Video - [Container images](https://www.cloudskillsboost.google/course_templates/2/video/517308)

* [YouTube: Container images](https://www.youtube.com/watch?v=VSsCL1vwOMk)

SPEAKER: An application and its dependencies are called an image. And a container is simply a running instance of an image. By building software into container images, developers can package and ship an application without worrying about the system it will run on. But to build and run container images, you need software. One option is Docker. Although this open-source technology can be used to create and run applications in containers, it doesn't offer a way to orchestrate those applications at scale, like Kubernetes does. Later in this course, you'll use Google's Cloud Build to create Docker-formatted container images. A container has the power to isolate workloads, and this ability comes from a combination of several Linux technologies. The first is the foundation of the Linux process. Each Linux process has its own virtual memory address space separate from all others, and Linux processes can be rapidly created and destroyed. The next technology is Linux namespaces. Containers use Linux namespaces to control what an application can see, such as process ID numbers, directory trees, IP addresses, et cetera. It's important to note that Linux namespaces are not the same thing as Kubernetes namespaces, which you will learn more about later in this course. The third technology is Linux cgroups. Linux cgroups control what an application can use, such as its maximum consumption of CPU time, memory, I/O bandwidth, and other resources. And finally, containers use union file systems to bundle everything needed into a neat package. This requires combining applications and their dependencies into a set of clean, minimal layers. Let's explore how this works. A container image is structured in layers, and the tool used to build the image reads instructions from a file called the container manifest. For Docker-formatted container images, that's called a Dockerfile. Each instruction in the Dockerfile specifies a layer inside the container image. Each layer is read-only, but when a container runs from this image, it will also have a writable ephemeral topmost layer. Let's explore a simple Dockerfile. A Dockerfile contains four commands, each of which creates a layer. For the purposes of this training, this Dockerfile has been a little oversimplified for modern use. The from statement starts by creating a base layer, which is pulled from a public repository. This one happens to be the Ubuntu Linux runtime environment of a specific version. The copy command adds a new layer, which contains some files copied in from your build tool's current directory. The run command builds the application by using the make command, and puts the results of the build into a third layer. And finally, the last layer specifies what command you should run within the container when it's launched. When you write a Dockerfile, the layers should start with those least likely to change at the top and the layers most likely to change at the bottom. So I mentioned this Dockerfile example is oversimplified. Let me explain what I meant. Currently, it's not a best practice to build your application in the same container where you ship and run it. After all, your build tools are at best just clutter in a deployed container, and at worst they're an additional attack surface. Today, application packaging relies on a multi-stage build process, where one container builds the final executable image and a separate container receives only what is needed to run the application. When launching a new container from an image, the container runtime adds a new writable layer on top of the underlying layers. This layer is called the container layer. All changes made to the running container, such as writing new files, modifying existing files, and deleting files, are written to this thin writable container layer. And they're ephemeral, which means that when the container is deleted, the contents of this writable layer are lost forever. The underlying container image remains unchanged. So when it comes to application design, this means that permanent data must be stored somewhere other than a running container image. Because each container has its own writable container layer, and all changes are stored in this layer, multiple containers can share access to the same underlying image, and while still maintaining their own data state. This allows container images to get smaller with each layer. For example, a base application image might be 200 megabytes, but the difference to the next point release might only be 200 kilobytes. When building a container, instead of copying the entire image, it creates a layer with just the difference. When running a container, the container runtime pulls down the layers it needs. When updating a container, only the difference needs to be copied. This is much faster than running a new virtual machine. So how can you get or create containers? It's common to use publicly available open-source container images as the base for your own images, or for unmodified use. Google maintains Artifact Registry at pkg.dev, which contains public open-source images. It also provides Google Cloud customers with a place to store their own container images and is integrated with identity and access management, IAM. This allows storing container images that are private to your project. Container images are also available in other public repositories, like the Docker Hub registry and GitLab. Google provides a managed service for building containers called Cloud Build. Cloud Build is integrated with Cloud IAM and was designed to retrieve the source code builds from different code repositories, including Cloud Source Repositories, or Git-compatible repositories like GitHub and Bitbucket. To generate a build with Cloud Build, you must define a series of steps. For example, you can configure build steps to fetch dependencies, compile source code, run integration tests, or use tools such as Docker, Gradle, and Maven. Each build step in Cloud Build runs in a Docker container. From there, Cloud Build can deliver the newly built images to various execution environments, including Google Kubernetes Engine, App Engine, and Cloud Run functions.

### Video - [Lab introduction: Working with Cloud Build](https://www.cloudskillsboost.google/course_templates/2/video/517309)

* [YouTube: Lab introduction: Working with Cloud Build](https://www.youtube.com/watch?v=t7kkv1kyJQc)

Now it’s time to gain some hands-on experience with Cloud Build. In the lab titled, “Working with Cloud Build,” you’ll use provided code to build a Docker container image and a Dockerfile. From there, you’ll upload the container to the Google Cloud Artifact Registry, which is a private Docker repository to securely store and manage Docker images.

### Lab - [Working with Cloud Build](https://www.cloudskillsboost.google/course_templates/2/labs/517310)

Architecting with Google Kubernetes Engine: Working with Cloud Build

* [ ] [Working with Cloud Build](../labs/Working-with-Cloud-Build.md)

### Video - [Kubernetes](https://www.cloudskillsboost.google/course_templates/2/video/517311)

* [YouTube: Kubernetes](https://www.youtube.com/watch?v=oNioR_IRGAg)

So let’s say that your organization has implemented containers, and because containers are so lean, your coworkers are creating them in numbers that exceed the counts of virtual machines you used to have. Let’s also say that the applications that run in the containers need to communicate over the network, but you don’t have a network fabric that lets containers find each other. Kubernetes can help. So, what is Kubernetes? Kubernetes is an open source platform for managing containerized workloads and services. It makes it easy to orchestrate many containers on many hosts, scale them as microservices, and easily deploy rollouts and rollbacks. At the highest level, Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called a cluster. The system is divided into a set of primary components that run as the control plane and a set of nodes that run containers. In Kubernetes, a node represents a computing instance, like a machine. Note that this is different to a node on Google Cloud, which is a virtual machine that runs in Compute Engine. You can describe a set of applications and how they should interact with each other, and Kubernetes determines how to make that happen. Kubernetes supports declarative configurations. When you administer your infrastructure declaratively, you describe the desired state you want to achieve, instead of issuing a series of commands to achieve that desired state. Kubernetes’s job is to make the deployed system conform to your desired state and to keep it there in spite of failures. Declarative configuration saves you work. Because the system’s desired state is always documented, it also reduces the risk of error. Kubernetes also allows imperative configuration, in which you issue commands to change the system’s state. One of the primary strengths of Kubernetes is its ability to automatically keep a system in a state you declare. Therefore, experienced Kubernetes administrators use imperative configuration only for quick temporary fixes and as a tool when building a declarative configuration. Now that you have a better understanding of what Kubernetes is, let’s explore some of its features. Kubernetes supports different workload types. It supports stateless applications, such as Nginx or Apache web servers, and stateful applications where user and session data can be stored persistently. It also supports batch jobs and daemon tasks. Kubernetes can automatically scale containerized applications in and out based on resource utilization. Kubernetes allows users to specify resource request levels and resource limits for workloads. Resource controls help Kubernetes improve the overall workload performance within a cluster. Kubernetes is extensible through a rich ecosystem of plugins and addons. For example, Kubernetes Custom Resource Definitions let developers define new types of resources that can be created, managed, and used in Kubernetes. And finally, because it’s open-source, Kubernetes is portable and can be deployed anywhere–whether on premises or on another cloud service provider. This means that Kubernetes workloads can be moved freely without vendor lock-in.

### Video - [Google Kubernetes Engine](https://www.cloudskillsboost.google/course_templates/2/video/517312)

* [YouTube: Google Kubernetes Engine](https://www.youtube.com/watch?v=VU47vy-dpEo)

What if you started using Kubernetes, but the infrastructure is too much to maintain? This is where Google Kubernetes Engine comes in. Google Kubernetes Engine is a managed Kubernetes service hosted on Google’s infrastructure. It’s designed to help deploy, manage, and scale Kubernetes environments for containerized applications. GKE is fully managed, which means the underlying resources don’t have to be provisioned, and a container-optimized operating system is used to run workloads. Google maintains these operating systems, which are optimized to scale quickly with a minimal resource footprint. Google Kubernetes Engine offers a mode of operation called GKE Autopilot, which is designed to manage your cluster configuration, like nodes, scaling, security, and other preconfigured settings. When you use GKE, you start by directing the service to create and set up a Kubernetes system for you. This system is called a cluster. The GKE auto-upgrade feature ensures that clusters are always upgraded with the latest stable version of Kubernetes. The virtual machines that host containers in a GKE cluster are called nodes. GKE has a node auto-repair feature that was designed to repair unhealthy nodes. It performs periodic health checks on each node of the cluster and nodes determined to be unhealthy are drained and recreated. Just like Kubernetes supports scaling workloads, GKE supports scaling the cluster itself. GKE is integrated with several services: Cloud Build uses private container images securely stored in Artifact Registry to automate the deployment. IAM helps control access by using accounts and role permissions. Google Cloud Observability provides an understanding into how an application is performing. And Virtual Private Clouds, which provide a network infrastructure including load balancers and ingress access for your cluster. And finally the Google Cloud console provides insights into GKE clusters and their resources, and a way to view, inspect, and delete resources in those clusters. Although open source Kubernetes provides a dashboard, it takes a lot of work to set it up securely. With the Google Cloud console, however, there is a more powerful dashboard for your GKE clusters and workloads that you don’t have to manage.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/2/quizzes/517313)

#### Quiz 1.

> [!important]
> **What is the name for the computers in a Google Kubernetes Engine cluster that run workloads?**
>
> * [ ] Nodes
> * [ ] Container images
> * [ ] Control planes
> * [ ] Containers

#### Quiz 2.

> [!important]
> **When using Kubernetes, you must describe the desired state you want, and Kubernetes's job is to make the deployed system conform to that desired state and keep it there despite failures. What is the name of this management approach?**
>
> * [ ] Declarative configuration
> * [ ] Virtualization
> * [ ] Imperative configuration
> * [ ] Containerization

#### Quiz 3.

> [!important]
> **What is significant about the topmost layer in a container? Choose two options.**
>
> * [ ] The topmost layer's contents are ephemeral. When the container is deleted, the contents are lost.
> * [ ] Reading from or writing to the topmost layer requires special privileges.
> * [ ] An application running in a container can only modify the topmost layer.
> * [ ] Reading from or writing to the topmost layer requires special software libraries.

## Kubernetes Architecture

The third section of this course explores the components of a Kubernetes cluster and how they work together. Learners deploy a Kubernetes cluster by using Google Kubernetes Engine, deploy Pods to a GKE cluster, and view and manage different Kubernetes objects.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/2/video/517314)

* [YouTube: Introduction](https://www.youtube.com/watch?v=-KN2QQpxnPw)

Google Kubernetes Engine makes it easy to recognize the benefits of innovation initiatives without getting stuck troubleshooting infrastructure issues and managing daily operations related to enterprise-scale container deployment. But how does Kubernetes expect you to tell it what to do? And what choices do you have for describing your workloads? The third section of the course was designed to answer those questions. You’ll explore: Kubernetes concepts, like Kubernetes object model and the principal of declarative management. A list of Kubernetes components. Google Kubernetes Engine concepts, including the Autopilot and standard modes of operation. And Kubernetes object management. You’ll also get hands-on practice deploying a sample pod in GKE. Let’s get started!

### Video - [Kubernetes concepts](https://www.cloudskillsboost.google/course_templates/2/video/517315)

* [YouTube: Kubernetes concepts](https://www.youtube.com/watch?v=aSi6Aq1bhI0)

To understand how Kubernetes works, it’s important to understand two related concepts. The first concept is the Kubernetes object model. Each item Kubernetes manages is represented by an object, and you can view and change these objects attributes and state. The second concept is the principle of declarative management. Kubernetes needs to be told how objects should be managed, and it will work to achieve and maintain that desired state. This is accomplished through a "watch loop." A Kubernetes object is defined as a persistent entity that represents the state of something running in a cluster: its desired state and its current state. Various kinds of objects represent containerized applications, the resources available to them, and the policies that affect their behavior. Kubernetes objects have two important elements. The first is an object spec for each object being created. It’s here that the desired state of the object is defined by you. The second is the object status, which represents the current state of the object provided by the Kubernetes control plane. By the way, “Kubernetes control plane” is a term to refer to the various system processes that collaborate to make a Kubernetes cluster work. You’ll learn about these processes later. Each object represents a certain type, or “kind,” as it’s referred to in Kubernetes. Pods are the foundational building block of the standard Kubernetes model, and they’re the smallest deployable Kubernetes object. Every running container in a Kubernetes system is in a Pod. A Pod creates the environment where the containers live, and that environment can accommodate one or more containers. If there is more than one container in a Pod, they are tightly coupled and share resources, like networking and storage. Kubernetes assigns each Pod a unique IP address, and every container within a Pod shares the network namespace, including IP address and network ports. Containers within the same Pod can communicate through localhost, 127.0.0.1. A Pod can also specify a set of storage volumes that will be shared among its containers. Let’s explore an example where you want three instances of an nginx Web server, each in its own container, to be always kept running. How can this be achieved in Kubernetes? You’ll recall that Kubernetes operates off of the principle of declarative management, which means that you’ll need to declare some objects to represent those nginx containers, and in this case, those objects should be Pods. From there, it’s Kubernetes’s job to launch those Pods and keep them in existence. Alright, so you’ve given Kubernetes a desired state that consists of three nginx Pods, to be always kept running. We did this by telling Kubernetes to create and maintain one or more objects that represent them. Now Kubernetes will compare the desired state to the current state. For this example, let’s say our declaration of three nginx containers is completely new, meaning the current state does not match the desired state. So Kubernetes, specifically its control plane, will remedy the situation. Because the number of desired Pods running for the object you declared is 3, and 0 are presently running, 3 will be launched. And the Kubernetes control plane will continuously monitor the state of the cluster, endlessly comparing reality to what has been declared, and remedying the state as needed.

### Video - [Kubernetes components](https://www.cloudskillsboost.google/course_templates/2/video/517316)

* [YouTube: Kubernetes components](https://www.youtube.com/watch?v=259yjNVLXJQ)

The Kubernetes control plane is the fleet of cooperating processes that make a Kubernetes cluster work. Although you might only directly work with a few of these components, it’s important to understand what the fleet does and the role they each play. In this section of the course, you’ll get an opportunity to see how a Kubernetes cluster is constructed, part by part. This will help illustrate how a Kubernetes cluster that runs in GKE is easier to manage than one you provisioned yourself. First, a cluster needs computers, and these computers are usually virtual machines. They always are in GKE, but they could be physical computers too. One computer is called the control plane, and the others are called nodes. The node’s job is to run Pods, and the control plane’s is to coordinate the entire cluster. Let’s look at the control-plane components. Several critical Kubernetes components run on the control plane. First is the kube-APIserver component, which is the only single component that you'll interact with directly. The job of this component is to accept commands that view or change the state of the cluster. This includes launching Pods. Next is the kubectl command. The job of the kubectl command is to connect to the kube-APIserver and communicate with it using the Kubernetes API. The kube-APIserver also authenticates incoming requests, determines whether they are authorized and valid, and manages admission control. But it’s not just kubectl that talks with kube-APIserver. In fact, any query or change to the cluster’s state must be addressed to the kube-APIserver. There is the etcd component, which is the cluster’s database. Its job is to reliably store the state of the cluster. This includes all the cluster configuration data,along with more dynamic information such as what nodes are part of the cluster, what Pods should be running, and where they should be running. You’ll never directly interact with etcd, instead the kube-APIserver interacts with the database on behalf of the rest of the system. Next is kube-scheduler, which is responsible for scheduling Pods onto the nodes. Kube-scheduler evaluates the requirements of each individual Pod and selects which node is most suitable. However, it doesn’t do the work of actually launching Pods on nodes (that’s done by another component). Instead, whenever it discovers a Pod object that doesn’t yet have an assigned node, it chooses a node and writes the name of that node into the Pod object. How does kube-scheduler decide where to run a Pod? It knows the state of all the nodes, and also obeys constraints you define regarding where a Pod can run, considering hardware, software, and policy details. For example, you might specify that a certain Pod is only allowed to run on nodes with a specific amount of memory. You can also define affinity parameters, which specify when groups of Pods should run on the same node. Alternatively, you can define anti-affinity parameters, which ensure that Pods do not run on the same node. The kube-controller-manager component has a broader job–it continuously monitors the state of a cluster through the kube-APIserver. Whenever the current state of the cluster doesn’t match the desired state, kube-controller-manager will attempt to make changes to achieve the desired state. It’s called the controller manager because many Kubernetes objects are maintained by loops of code called controllers, which handle the process of remediation. You can use certain Kubernetes controllers to manage workloads. For example, remember our problem of keeping 3 nginx Pods always running? They can be gathered into a controller object called a deployment that runs, scales, and brings them together underneath a front end. Other types of controllers have system-level responsibilities. For example, the Node Controller’s job is to monitor and respond when a node is offline. The kube-cloud-manager component manages controllers that interact with underlying cloud providers. For example, if you manually launched a Kubernetes cluster on Compute Engine, kube-cloud-manager would be responsible for bringing in Google Cloud features like load balancers and storage volumes. Now let’s shift our focus to nodes. Each node runs a small family of control-plane components called a kubelet. You can think of a kubelet as Kubernetes’s agent on each node. When the kube-APIserver wants to start a Pod on a node, it connects to that node’s kubelet. Kubelet uses the container runtime to start the Pod and monitors its lifecycle, including readiness and liveness probes, and reports back to the kube-APIserver. The term container runtime, which was mentioned earlier in this course, is the software used to launch a container from a container image. Kubernetes offers several container runtime choices, but the Linux distribution that GKE uses for its nodes launches containers that use containerd, the runtime component of Docker. And finally, there is the kube-proxy component, which maintains network connectivity among the Pods in a cluster. In open source Kubernetes, network connectivity is accomplished by using the firewalling capabilities of iptables, which are built into the Linux kernel. We saw that the Kubernetes Control Plane is a complex management system. But how is GKE different from Kubernetes? From the user’s perspective, it’s a lot simpler. GKE manages all the control plane components for us. It still exposes an IP address to which we send all of our Kubernetes API requests, but GKE is responsible for provisioning and managing all the control plane infrastructure behind it. It also eliminates the need for a separate control plane. Node configuration and management depends on the type of GKE mode you use. With the Autopilot mode, which is recommended, GKE manages the underlying infrastructure such as node configuration, autoscaling, auto-upgrades, baseline security configurations, and baseline networking configuration. With the Standard mode, you manage the underlying infrastructure, including configuring the individual nodes.

### Video - [GKE Autopilot and GKE standard](https://www.cloudskillsboost.google/course_templates/2/video/517317)

* [YouTube: GKE Autopilot and GKE standard](https://www.youtube.com/watch?v=qcwBpoGFpDo)

Now let’s explore the two available modes of operation, Autopilot and Standard mode, in more detail. At a high level, Autopilot mode optimizes the management of Kubernetes with a hands-off experience. However, less management overhead means less configuration options. And with Autopilot GKE you only pay for what you use. The Standard mode allows the Kubernetes management infrastructure to be configured in many different ways. This requires more management overhead, but produces an environment for fine-grained control. With GKE standard, you pay for all of the provisioned infrastructure, regardless of how much gets used. Let’s examine the benefits and functionality of Autopilot in more detail. Autopilot is optimized for production. As a Google-managed and optimized GKE instance, the job of Autopilot is to create clusters according to battle-tested and hardened best practices. Autopilot defines the underlying machine type for your cluster based on workloads, which optimizes both usage and cost for the cluster and adapts to changing workloads. And without the cluster management overhead, Autopilot lets you deploy production-ready GKE clusters faster. Autopilot also helps produce a strong security posture. Google helps secure the cluster nodes and infrastructure, and it eliminates infrastructure security management tasks. By locking down nodes, Autopilot reduces the cluster's attack surface and ongoing configuration mistakes. Autopilot promotes operational efficiency. Google monitors the entire Autopilot cluster, including control plane, worker nodes and core Kubernetes system components. This way, it ensures that Pods are always scheduled. This level of monitoring allows Google to always keep clusters up to date. Autopilot also provides a way to configure update windows for clusters to ensure minimal disruption to workloads. With Autopilot, Google is fully responsible for optimizing resource consumption. This means you only pay for Pods, not nodes. Now that you’ve seen many of the Autopilot mode benefits, let’s look at some restrictions presented with this operation mode. The configuration options in GKE Autopilot are more restrictive than in GKE Standard. This is because GKE Autopilot is a fully managed service and has a pod-scheduling service level agreement. Autopilot clusters also have restrictions on access to node objects. Features like SSH and privilege escalation were removed and there are limitations on node affinity and host access. However, all Pods in GKE Autopilot are scheduled with a Guaranteed class Quality of Service (or QoS). But this requires a minor configuration change. The GKE Standard mode has the same functionality as Autopilot, but you’re responsible for the configuration, management, and optimization of the cluster. Unless you require the specific level of configuration control offered by GKE standard, it’s recommended that you use Autopilot mode.

### Video - [Object management](https://www.cloudskillsboost.google/course_templates/2/video/517318)

* [YouTube: Object management](https://www.youtube.com/watch?v=Z7l9l1tNfN0)

Let’s finish up this section of the course by exploring Kubernetes object management. It’s important to know that all Kubernetes objects are identified by a unique name and a unique identifier. Recall the earlier example where you wanted the three nginx Web servers to run all the time. The simplest way to achieve this is by declaring three Pod objects and specifying their state. For each, a Pod must be created and an nginx container image must be used. Let’s see how to make this declaration. You define the objects you want Kubernetes to create and maintain with manifest files. These are ordinary text files that can be written in YAML or JSON. You’ll see YAML used in this course. This manifest file defines a desired state for a Pod: its name and a specific container image for it to run. Required fields include: apiVersion, which states the Kubernetes API version used to create the object, kind, which states the object you want (in this case, a Pod), and metadata, which identifies the object name, unique ID, and an optional namespace. If several objects are related, it’s a best practice to define them all in the same YAML file. This makes things easier to manage. We strongly recommend saving YAML files in version-control repositories so it’s easier to track and manage changes and to undo those changes when necessary. It’s also helpful when recreating or restoring a cluster. Cloud Source Repositories is a popular service for this purpose. Let’s look at the details of an object. First, all objects are identified by a name. Names must consist of a unique string under 253 characters. Numbers, letters, hyphens, and periods are allowed. Only one object can have a particular name at the same time in the same Kubernetes namespace. After an object is deleted, however, the name can be reused. Second, every object created throughout the life of a cluster has a unique identifier, or UID, generated by Kubernetes. And third, there are labels. Labels are key-value pairs to tag objects during or after their creation. Labels help identify and organize objects. A way to select Kubernetes resources by label is through the kubectl command. For example, this command can select all the Pods that contain a label called “app” with a value of “nginx.” Label selectors can be used to ask for all the resources that have a certain value for a label, all those that don’t have a certain value, or even all those with a value in a set you supply. Now let’s go back to our example. One way to create three nginx web servers is by declaring three Pod objects, each with its own section of YAML. In Kubernetes, a workload is spread evenly across available nodes by default. So how do you tell Kubernetes to maintain the desired state of three nginx containers? To maintain an application's high availability, you need a better way to manage it in Kubernetes than specifying individual Pods. One option is to declare a controller object. A controller object's job is to manage the state of the Pods. Because Pods are designed to be ephemeral and disposable, they don't heal or repair themselves and are not meant to run forever. Examples include Deployments, StatefulSets, DaemonSets, and Jobs. Deployments are a great choice for long-lived software components like web servers, especially when you want to manage them as a group. In our example, the practical effect of the Deployment controller is to monitor and maintain the three nginx Pods. When the kube-scheduler schedules Pods for a Deployment, it notifies the kube-APIserver. The Deployment controller creates a child object, a ReplicaSet, to launch the desired Pods. If one of these Pods fails, the ReplicaSet controller will recognize the difference between the current state and the desired state and will try to fix it by launching a new Pod. So, this means that instead of using multiple YAML manifests or files for each Pod, you used a single Deployment YAML to launch three replicas of the same container. Within a Deployment object spec, the number of replica Pods, which containers should run the Pods, and which volumes should be mounted the following elements are defined. Based on these templates, controllers maintain the Pod’s desired state within a cluster.

### Video - [Lab introduction: Deploying GKE Autopilot Clusters](https://www.cloudskillsboost.google/course_templates/2/video/517319)

* [YouTube: Lab introduction: Deploying GKE Autopilot Clusters](https://www.youtube.com/watch?v=iznakCr-v7M)

It’s time for some hands-on practice with GKE. In the lab titled “Deploying GKE Autopilot Clusters,” you’ll build and use GKE clusters, then you’ll deploy a sample Pod. Specifically, you’ll use the Google Cloud console to build GKE clusters, deploy a Pod, and then examine the cluster and Pods.

### Lab - [Deploying GKE Autopilot Clusters](https://www.cloudskillsboost.google/course_templates/2/labs/517320)

Getting Started with Google Kubernetes Engine: Creating a GKE Autopilot Cluster via Google Cloud Console

* [ ] [Deploying GKE Autopilot Clusters](../labs/Deploying-GKE-Autopilot-Clusters.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/2/quizzes/517321)

#### Quiz 1.

> [!important]
> **Google Kubernetes Engine offers two modes of operation: Autopilot and Standard mode. Which one of the options below is a use case for using Standard mode.**
>
> * [ ] You want to only pay for Pods and not nodes.
> * [ ] You want machine types based on workloads.
> * [ ] You require SSH access to nodes.
> * [ ] You want to avoid cluster configuration.

#### Quiz 2.

> [!important]
> **When designing an application, you want the containers to be located as close to each other as possible in order to minimize latency. Which design decision helps meet this requirement?**
>
> * [ ] Give the containers the same labels.
> * [ ] Place the containers in the same Namespace.
> * [ ] Place the containers in the same cluster.
> * [ ] Place the containers in the same Pod.

#### Quiz 3.

> [!important]
> **You want to deploy multiple copies of an application in an effort to load balance traffic. How should you deploy Pods to production to achieve this?**
>
> * [ ] Create a deployment manifest that specifies the number of replicas that you want to run.
> * [ ] Deploy the Pod manifest multiple times until you achieve the number of replicas required.
> * [ ] Create a Service manifest for the LoadBalancer that specifies the number of replicas you want to run.
> * [ ] Create separate named Pod manifests for each instance of the application, and deploy as many as you need.

## Kubernetes Operations

The final section of this course introduces the kubectl command, which is the command line utility used to interact with and manage the resources inside Kubernetes clusters. Learners are introduced to the concept of introspection, then get practice deploying Google Kubernetes Engine clusters from Cloud Shell.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/2/video/517322)

* [YouTube: Introduction](https://www.youtube.com/watch?v=MiZkv1nTqwc)

kubectl is a command-line tool to interact with GKE clusters. Because it allows you to manage Kubernetes resources from the command line, it makes it easy to automate tasks and to troubleshoot problems. But how does it work? Does it need special configuration? How can it be used to gather information about the containers, Pods, services, and other engines running within the cluster? The goal of this final section of the course, titled “Kubernetes Operations” was designed to answer those questions. You’ll explore kubectl and how to configure it, and what introspection means and how it can be used to troubleshoot a cluster. You’ll also get hands on practice deploying Google Kubernetes Engine clusters from Cloud Shell. Okay, let’s get started!

### Video - [The kubectl command](https://www.cloudskillsboost.google/course_templates/2/video/517323)

* [YouTube: The kubectl command](https://www.youtube.com/watch?v=Ycs6izxQW7Y)

So, what exactly is the kubectl command and why is it important? kubectl is a utility used by administrators to control Kubernetes clusters. It’s used to communicate with the Kube API server on the control plane. This is important to users, because it allows them to make requests to the cluster and kubectl determines which part of the control plane to communicate with. Within a selected Kubernetes cluster, kubectl transforms command-line entries into API calls and sends them to the Kube API server. However, to work properly, kubectl must be configured with the location and credentials of a Kubernetes cluster. And before kubectl can be used to configure a cluster, it must first be configured. kubectl stores its configuration in a file in the home directory in a hidden folder named . kube, and contains the list of clusters and the credentials that will be attached to each of those clusters. But where do the credentials come from? GKE provides them through the gcloud command. To view the configuration, either open the config file or use the kubectl command: “config view”. Please note that the kubectl config shows the configuration of the kubectl command itself, whereas other kubectl commands show the configurations of cluster and workloads. For example, let’s say an administrator wants to see a list of Pods in a cluster. After connecting kubectl to the cluster with proper credentials, the administrator can issue the kubectl “get pod” command. kubectl then converts this command into an API call, which it sends to the Kube API server through HTTPS on the cluster’s control plane server. From there, the Kube API server processes the request by querying etcd. The Kube API server then returns the results to kubectl through HTTPS. Finally, kubectl interprets the API response and displays the results to the administrator at the command prompt. To connect kubectl to a GKE cluster, first retrieve the credentials for the specified cluster. This can be done with the “get-credentials” gcloud command in any other environment where the gcloud command-line tool and kubectl are installed. Note that both are installed by default in Cloud Shell. By default, the gcloud “get-credentials” command writes configuration information into a config file in the . kube directory in the $HOME directory. If this command is rerun for a different cluster, it’ll update the config file with the credentials for the new cluster. This configuration process only needs to be performed once per cluster in Cloud Shell, because the . kube directory and its contents stay in the $HOME directory. The gcloud command is how authorized users interact with Google Cloud from the command line. If authorized, the gcloud “get-credentials” command provides the credentials needed to connect with a GKE cluster. Although kubectl is a tool for administering the internal state of an existing cluster, it can’t create new clusters or change the shape of existing clusters. That’s done through the GKE control plane, which the gcloud command and the Google Cloud console interfaces to. After the config file in the . kube folder is configured, the kubectl command automatically references this file and connects to the default cluster without prompting for credentials. Now let’s explore how to use the kubectl command. kubectl’s syntax is composed of four parts: the command, the type, the name, and optional flags. The command specifies the action that you want to perform, such as get, describe, logs, or exec. Some commands show information, whereas others change the cluster’s configuration. The TYPE defines the Kubernetes object that the “command” acts upon, like Pods, deployments, nodes, or other objects, including the cluster itself. The TYPE used in combination with “command” tells kubectl what you want to do and the type of object you want to perform that action on. The NAME specifies the object defined in TYPE. The name field isn’t always needed, especially when using commands that list or show information. For example, if you run the command “kubectl get pods” without specifying a name, the command returns the list of all Pods. To filter this list, specify a Pod’s name, such as “kubectl get pod my-test-app” and kubectl will return only information on the Pod named ‘my-test-app’. Some commands let you append flags to the end, which you can think of as a way to make a special request. For example, to view the state of a Pod, use the command “kubectl get pod my-test-app -o=yaml”. Also, it’s worth mentioning that telling kubectl to produce a YAML output can be helpful for other tasks related to Kubernetes objects, for example, recreating an object in another cluster. Flags can also be used to display additional information. For example, run the command “kubectl get pods -o=wide” to display the list of Pods in “wide” format, which reveals Pods in the list. Wide format also displays which node each Pod is running on. The kubectl command has many uses, from creating Kubernetes objects, to viewing them, deleting them, and viewing or exporting configuration files. Just remember to configure kubectl first or to use the --kubeconfig or --context parameters, so that the commands you type are performed on the cluster you intended.

### Video - [Introspection](https://www.cloudskillsboost.google/course_templates/2/video/517324)

* [YouTube: Introspection](https://www.youtube.com/watch?v=lDp3KB6IVCA)

Now that you’ve been introduced to the kubectl command, which is the way to specify the action that you want to perform on a Kubernetes cluster, let’s explore how to debug problems when an application is running. This process is called introspection. It’s the act of gathering information about the containers, pods, services, and other engines that run within the cluster. We’ll start with four commands to use to gather information about your app: get, describe, exec, and logs. A good place to start is with Pods, the basic units of Kubernetes. A simple kubectl “get pods” command tells you whether your Pod is running. It shows the Pod’s phase status as pending, running, succeeded, failed, or unknown, or CrashLoopBackOff. The phase provides a high-level summary, not the comprehensive details about a Pod or its containers. The pending status indicates that Kubernetes has accepted a Pod, but it’s still being scheduled. This means that the container images defined for the Pod have not yet been created by the container runtime. For example, when images are being pulled from the repository, the Pod will be in the pending phase. A Pod runs after it is successfully attached to a node, and all its containers are created. Containers inside a Pod can be starting, restarting, or running continuously. Succeeded means that all containers finished running successfully, or instead, that they terminated successfully and they won’t be restarting. Failed means a container terminated with a failure, and it won’t be restarting. Unknown is where the state of the Pod simply cannot be retrieved, probably because of a communication error between the control plane and a kubelet. And CrashLoopBackOff means that one of the containers in the Pod exited unexpectedly even after it was restarted at least once. This is a common error. Usually, this means that the Pod isn’t configured correctly. To investigate a Pod in detail, use the kubectl “describe pod” command. This command provides information about a Pod and its containers such as labels, resource requirements, and volumes. It also details the status information about the Pod and container. For Pods, the name, namespace, node name, labels, status, and IP address are displayed. For containers, the state–waiting, running, or terminated, images, ports, commands, and restart counts–are displayed. Single command execution using the exec command lets you run a single command inside a container and view the results in your own command shell. This is useful when a single command, such as ping, will do. And finally, the logs command provides a way to see what is happening inside a Pod. This is useful in troubleshooting, as the logs command can reveal errors or debugging messages written by the applications that run inside Pods. The logs contain both the standard output and standard error messages that the applications within the container have generated. The logs command is useful when you need to find out more information about containers that are failing to run successfully. And if the Pod has multiple containers, you can use the -c argument to show the logs for a specific container inside the Pod. There might be a scenario where you need to work inside a Pod, maybe to run a command. Let’s say you need to install a package, like a network monitoring tool or a text editor, before you can begin troubleshooting. To do so, you can launch an interactive shell using the -it switch, which connects your shell to the container that allows you to work inside the container. This syntax attaches the standard input and standard output of the container to your terminal window or command shell. The -i argument tells kubectl to pass the terminal’s standard input to the container, and the -t argument tells kubectl that the input is a TTY. If you don’t use these arguments then the exec command will be executed in the remote container and return immediately to your local shell. Now it’s important to note that it’s not a best practice to install software directly into a container, as changes made by containers to their file systems are usually ephemeral. Instead, consider building container images that have exactly the software you need, instead of temporarily repairing them at run time. The interactive shell will allow you to figure out what needs to be changed to solve a problem, but you should then integrate those changes into your container images and redeploy them.

### Video - [Lab introduction: Deploying GKE Autopilot Clusters from Cloud Shell](https://www.cloudskillsboost.google/course_templates/2/video/517325)

* [YouTube: Lab introduction: Deploying GKE Autopilot Clusters from Cloud Shell](https://www.youtube.com/watch?v=mV-fC3dIlGA)

It’s time for the last hands-on lab of this course. In the lab titled “Deploying GKE Autopilot Clusters from Cloud Shell” you’ll use the command line to build GKE clusters. You’ll inspect the kubeconfig file and use kubectl to manipulate the cluster.

### Lab - [Deploying GKE Autopilot Clusters from Cloud Shell](https://www.cloudskillsboost.google/course_templates/2/labs/517326)

Use this template for lab guides created after April 8, 2020. Report issues to mafaulkner.

* [ ] [Deploying GKE Autopilot Clusters from Cloud Shell](../labs/Deploying-GKE-Autopilot-Clusters-from-Cloud-Shell.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/2/quizzes/517327)

#### Quiz 1.

> [!important]
> **You want to use kubectl to configure your cluster, but first you must configure it. Where does the kubectl command store its configuration file?**
>
> * [ ] The configuration information is entered in Kubectl before executing commands.
> * [ ] kubectl uses the same authorization and credential tokens as the gcloud CLI utilities.
> * [ ] The configuration information is stored in environment variables in the current shell. when required.
> * [ ] The configuration information is stored in the $HOME/.kube/config file.

#### Quiz 2.

> [!important]
> **What command can be used to identify which containers in a Pod are successfully running, and which are failing or having issues?**
>
> * [ ] kubectl logs
> * [ ] kubectl get pod
> * [ ] kubectl exec
> * [ ] kubectl describe pod

#### Quiz 3.

> [!important]
> **Which command can be used to display error messages from containers in a Pod that are failing to run successfully?**
>
> * [ ] kubectl describe pod
> * [ ] kubectl logs
> * [ ] kubectl get pod
> * [ ] kubectl exec -it -- sh

#### Quiz 4.

> [!important]
> **You attempt to update a container image to a new version by using the "kubectl describe pod command," but are not successful. The output of the command shows that the Pod status has changed to "Pending,"the state is shown as "Waiting," and the reason shown is "ImagePullBackOff." What is the most probable cause of this error?**
>
> * [ ] The latest container image has already been deployed.
> * [ ] You specified an invalid container name.
> * [ ] The container image failed to download.
> * [ ] The container image pull policy has been set to "Never."

## Course Summary

The course summary recaps the major concepts learners were introduced to during the course.

### Video - [Course summary](https://www.cloudskillsboost.google/course_templates/2/video/517328)

* [YouTube: Course summary](https://www.youtube.com/watch?v=tANIhUO0XrA)

This brings us to the end of the “Getting Started with Google Kubernetes Engine” course! Let’s do a quick recap. In this first section of the course, you explored: the definition of cloud computing, the services Google Cloud offers architects and developers to build solutions, how Google’s powerful global network can power Google Cloud services How Google Cloud resources are structured and managed, tools to ensure that your organization doesn't accidentally face a big Google Cloud bill, and four different ways to interact with Google Cloud to commence work. In the second section, you learned about: containers, container images, Kubernetes, and Google Kubernetes Engine. In the third section of the course, you were introduced to: Kubernetes concepts like Kubernetes object model and the principal of declarative management, a list of Kubernetes components, Google Kubernetes Engine concepts including the Autopilot and Standard modes of operation, and Kubernetes object management. And finally, in the last section, you examined: the kubectl and how to configure it, and what introspection means and how it can be used to troubleshoot a cluster. If you’re interested in learning more about Google Kubernetes Engine, please go to the next course in this series called “Architecting with Google Kubernetes Engine.” See you next time!

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
