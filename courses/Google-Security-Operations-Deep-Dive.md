---
id: 972
name: 'Google Security Operations - Deep Dive'
type: Course
url: https://www.cloudskillsboost.google/course_templates/972
date_published: 2024-06-11
topics:
  - Data
  - Data Analysis
  - SIEM
---

# [Google Security Operations - Deep Dive](https://www.cloudskillsboost.google/course_templates/972)

**Description:**

Take the next steps in working with the Chronicle Security Operations Platform. Build on fundamental knowledge to go deeper on cusotmization and tuning.

**Objectives:**

* This course dives into all advanced modules of unified Security Operations Platform
* Gain an understanind of enrichment and ingestion of security data
* Learn how to build detecetions and integrations

## Chronicle Security Operations Architecture

Overview of the Chronicle Security Operations Platform Architecture

### Video - [SecOps Architecture](https://www.cloudskillsboost.google/course_templates/972/video/486840)

* [YouTube: SecOps Architecture](https://www.youtube.com/watch?v=K_3JtTrv1MU)

[Music] welcome to the chronicle SEC Ops deep dive course where we will take a deeper look at how to manage your security operations using the chronicle platform Chronicle SEC Ops is a cloud native sas-based platform with two major components Chronicle Sim and Chronicle sore Chronicle Sim was initially launched under the name Chronicle backstory in 2019 any references you may see to that name generally refer to Chronicle Sim Chronicle sore was acquired by Google in 2022 and was formerly named simplify so again this name refers to the chronical sore component of chronical SE Ops as of late 2023 Google launched the unified interface between Chronicle Sim and Chronicle sore in the chronicle SEC Ops platform this course is entirely focused on showcasing this unified platform if you happen to be a legacy customer customer that was provisioned before this you may be navigating to two different interfaces one for chronicle Sim under a backstory do Chronicle do security domain name and one for chronicle SAR under a simplify sore.com address however rest assured this course will be very helpful to you either way the biggest changes in the unified platform are in the main navigation the pages we are showcasing here will otherwise be very similar or identical let's talk about the core components the Sim component focuses on a variety of sim capabilities specifically ingesting normalizing indexing storing correlating searching and visualizing large volumes of data this is necessary in order to produce meaningful detections and insights that our SEC Ops Team can work on top of feeding into the Sim component we have security Telemetry coming from a wide variety of sources on premise or cloud in simple terms this would be your logs feeding into the platform we also have the applied threat intelligence from virus total mandiant and the Google Cloud threat Intel team this is directly applied to your data as well as custom Intel from your threat Intel platform or tip as well as business intelligence from sources like employee directories or cmdb systems such as service now the sore component brings a variety of security orchestration Automation and response capabilities including case management playbooks integration threat Intel enrichment collaboration Incident Management an additional business context now let's talk about what happens to data in that architecture when data comes into the platform first it runs through ingestion every log is duped and stored in the system in its raw form in chronicle an analyst will have access to both the raw and normalized data unlike other systems that perhaps either only store one or the other on premise log sources will typically be sent using the chronical forwarder a lightweight Docker container that can forward logs from sources such as CIS log the chronical ingestion pipelines also handle cloud-based data sources such as Cloud buckets or third party apis after data is ingested Chronicle normalizes and does most of its enrichment upon ingestion meaning data gets transformed into the unified data model using parsers the parsers serve an important function they ensure that the meaningful Fields get normalized so they can be used for correlation Chronicle normalizes upon ingestion utilizing a schema on right model which simply means that the schema or data modeling happens when the data gets ingested the alternative that some other systems apply is a schema on read model that tokenizes the raw data as the analyst searches for it there is a healthy debate on the topic of schema on read versus schema on write but it's important for technical users to understand that chronical Sim made a conscious decision to enforce schema at ingest time time in order to reap the large amount the benefits of having a truly unified data model it's not in the scope of this course to discuss this in detail but some of those benefits include setting a solid foundation for a future of Applied threat Intel and AI what this means in Practical terms is that Chronicle heavily invests in proper parser engineering efforts there are however lightweight mechanisms that allow analysts to operate on unparsed data including raw Log search mechanisms through regular expression now in the normalization stage you can generally expect most logs to be normalized you may have a log source that is custom for example and doesn't yet have a parser these logs will still be available through the raw Log search feature but will not be parsed at or around normalization time some enrichments can be applied an example of that is the geoip enrichment that adds geolocation data to IP addresses that are being normalized next comes the indexing stage for some of the blazing fast searches that go back long periods of time indexing includes a lot of Google magic in the back end but what users end up experiencing is near real-time ingestion and search time over indexed indicators and the ability to apply full indicator matching retroactively at the indexing stage the stitching is also done aliases of different indicators are used to fuse timelines together if they belong to the same user or asset after all this processing the system can now apply detections this is where alerts are produced alerts can be user defined using the Yara L language they can also be automatically produced from Google Cloud threat Intel detections through the curated detections feature all the rules built in or custom start applying to that Pipeline and alerts start streaming into our case management once they hit case management they can be further grouped into cases that provide our analyst with ability to investigate multiple alerts at the same time often achieving efficiencies of 50% or more some cases can also additionally be autoc closed based on enrichments and other information within a Playbook this way the platform creates a manageable workload alerts don't only have to come through Chronicle Sim there are hundreds of Integrations in the sore Marketplace this could be connectors to a legacy Sim connectors to products that produce alerts such as EDR products and many more an integration can have a few things number one actions these are bits of code that can be organized and chained within playbooks for example a mandiant threat Intel enrichment action will take an input of something like a hash and return threat intelligence associated with it actions are written in Python and can utilize the chronicle sore SDK all actions can be found in the IDE number two predefined widgets those are custom visualizations that usually are generated from an action and those are written in HTML JavaScript number three connectors this is a special type of action that runs on a regular basis and is designed to pull data and generate alerts or cases also written in Python there's also the idea of web hooks where systems can push data number four sync jobs those are scheduled jobs that perform any actions that may be necessary this can be cleanup jobs Etc Chronicle also comes with a remote agent think of that as a proxy for on- premise data sources that cannot be exposed to the internet the remote agent can connect to your on premise systems and provide information for actions connectors Etc that are configured to run in that manner we will be getting into details about all of this in upcoming videos last but not least we mentioned that Chronicle SEC Ops is a SAS product but how exactly does that deploy within gcp well with Chronicle you don't have to worry about managing virtual machines or tracking how many CPUs you need the system handles all of this behind the scenes that said gcp is used for a number of different capabilities number one there is a gcp project bound to your chronical instance this manages a number of things including the API pain access to support and some other settings number two you can configure Google Cloud's Workforce identity Federation as an SSO broker this is required for a number of important features number three you can configure details of the ingestion of gcp data into Chronicle including ingestion filters and other settings this concludes today's video stay tuned for the rest of the SEC Ops deep dive course to learn more about different bits of this [Music] architecture

## Ingestion and Enrichment

Deep dive of the ingestion, normalization and enrichment.

### Video - [Ingestion](https://www.cloudskillsboost.google/course_templates/972/video/486841)

* [YouTube: Ingestion](https://www.youtube.com/watch?v=dTX4O6p5k7k)

[Music] video number two ingestion with Google Chronicle hello and welcome to today's video where we will review ingestion with Google Chronicle since Chronicle is now a unified instance we'll review methods to ingest data in both Sim and sore we'll start with log ingestion on the Sim side Sim ingestion there are five primary ways to ingest feeds into Chronicle Sim direct via gcp only for Google cloud data cloud storage API feeds the forwarder and lastly pushing data via the chronicle API method number one cloud storage in this method you can send logs from cloud data sources to Chronicle by using the feed UI under settings feeds with this method you can use Amazon S3 Google cloud storage and Microsoft Azure blob storage as your sources Let's do an example using Amazon S3 as our source click add new and from the drop down for Source type we'll select Amazon S3 for log type we'll select AWS cloud trail click next on this page you'll be asked to First enter a region this method has the option to autodetect a region or specify one we'll select autodetect under S3 URI we'll put in our S3 URI for where these logs reside you then have the option of picking whether this is a single file directory or a directory which includes subdirectories we'll select directory which includes subdirectories this will make it select all the folders under our AWS cloud trail folder and the files within those folders next we'll select the source deletion option you can make it so sources are never deleted after ingestion deleted and then the directories are emptied or delete the files upon transferring we'll select never delete files as we're not worried about saving Cloud space for now we'll now enter our access key for the am user and then the secret key as well each Source type will prompt for the type of authentication it requires in this case a secret key you can also select a namespace namespaces are a more advanced topic and are used in cases where it's possible to have IP collisions between different feeds this is not relevant here so we'll leave this blank under ingestion labels let's click add label and select the label environment with the value production this will make it so that these logs in particular from a WS cloud trail are labeled as production logs as I want to be able to differentiate as I add more logs labels are just arbitrary key value pairs that can be used to add useful annotations to our data from there we click next and then submit this is just a demo though so we will not add these logs to our environment however it should look something like this you can identify that there are various statuses depending on log types the completed status type allows us to test whether the log successfully ingested which we can handle via doing a search in chronicle and seeking out that log type method number two API feeds with this method you can easily add data into Google Chronicle via a few predefined sources that Google has already configured with this method you only need to pick the data source from the list of supported sources and then enter your input parameters and again you can select a label the input parameters change depending on the type of log you are ingesting for the sake of this demo and simplicity We'll add a log type from Anomaly by selecting The Source type of third party API and then the log type of anomaly click next here we need to pick the username for our account and the secret let's input that now we'll not add an asset namespace but we will add an ingestion label again we'll pick environment for label and production for Value like our example using cloud storage click next and submit again and wait for the magic to happen again once the status of completed is displayed you can query the data and look for the new data source added in this case anomaly method number three direct via gcp this method is available only if you are using Google Cloud platform this method is simple and Chronicle enables you to store search and examine the aggregated security information for your Enterprise going back for months or longer in accordance to your data retention period under settings let's move to our tab labeled Google Cloud platform here you will see a button that states go to Google Cloud platform this will take you to your Google Cloud console and you'll be presented with the following popup on this popup you'll be asked to enter your onetime Chronicle access code which can be given to you by your Chronicle customer representative you'll then be asked to consent to the disclaimer and following that acceptance simply click connect Chronicle and you're connected this method includes the following log types Cloud audit logs cloud cloud DNS Cloud firewall Cloud intrusion detection system Cloud load balancing Cloud SQL Windows event logs Linux Sy log Linux Symon Zeke Google kubernetes engine audit D and apy please note that direct Integrations also exist with other products that send data directly to Chronicle internally those products typically use the ingestion API popular Integrations include tanium and cribble method number four for forer Chronicle Sim forwarder is a software component that runs on a machine or device on your network such as a server Chronicle Sim forwarder can collect log data and network interface packets and forward that data to your Chronicle Sim instance each deployed Chronicle Sim forwarder requires a forwarder configuration file a forwarder configuration file specifies various settings that Define how to transfer data to your Chronicle Sim instance such as data compression a forwarder configuration file also specifies one or more collect configurations each collector configuration specifies the collector's ingestion mechanism for example file CFA pcap Splunk CIS log or web proxy log type and other settings you can use many collectors on the same forwarder to ingest data from a variety of mechanisms and log types for example you can configure a forwarder with two CIS log collectors listening for pan firewall and Cisco ASA firewall data on separate ports respectively we can create manage and download forwarder configuration using the chronicle user interface let's do that now under settings click the forwarders tab from here we want to create a forwarder configuration file to be used with our forwarder it's important to note that this tutorial will only go over the configuration file creation and not the creation of the forwarder itself refer to Chronicles documentation for more details on how to create and manage your forward let's click add new forwarder we'll give the forwarder a name for this demo we'll call it demo clicking on the carrot below configuration values reveals a number of options available to us upload compression asset namespace labels and filters you can also toggle the server settings button to configure timeouts and your HTTP settings of your server for this demo we won't use these settings for now click submit it will let us know that we are creating a forwarder and we'll move on to making a collector confirm and let's move to the next step from here we'll name our collector we'll call it demo collector and from the drop- down we'll pick a log type in this case Windows DHCP again clicking on configuration values and advanced settings will reveal a number of other options available to us but we will not select these or dis buffer for now for collector type we'll select file here we will need to enter the full path of the file as it resides on the forwarder let's enter our full path there and then select submit and then confirm great as you can see we have the forwarder setup and clicking the three dots to the right we have the option of adding a new collector editing the forwarder configuration deleting the forwarder cloning the forwarder and lastly and most importantly downloading the forwarder you'll want to select down download and use this configuration file by uploading it to your respective forwarder method number five pushing data via the chronicle API this final method of ingestion of logs using Google Chronicle is an advanced topic by using the chronicle API you can construct your own custom scripts and ingest data into Google Chronicle however this method will not be covered during this video but we recommend you reference the chronical documentation to learn more this covers the primary ways to just data using Chronicle Sim we went over five methods direct via gcp only for Google cloud data cloud storage API feeds the forwarder and lastly custom scripts via the chronicle API let's move on to the sore side of things where we'll learn about connectors web Hooks and Integrations sore ingestion for this section of the course we'll start with connectors Chronicle sore uses connectors to ingest alerts from a variety of data sources into the platform a connector is one of the items in an integration package which can be downloaded through the chronical Marketplace connectors are configured from Source settings ingestion connectors here is where we can add edit or remove connectors however we first need to add an integration if we're beginning with a new connector let's navigate to The Marketplace from the side panel and select that let's search for an integration that we want to add for this demo we'll select jira click on the search bar and look for jira let's click the download icon which will install jira a popup will come up stating that this ontology configuration is already defined in the system it wants to know if we want to keep our current mapping or use the new one let's use our current ontology and click confirm we needed to First install this integration in order to select it from the connector's page it's important to note that not all Marketplace Integrations come with a connector the connector is where you will ingest data into your sore environment whereas the integration gives you options for actions for your playbook for example slack has an integration in the marketplace so you can make playbooks using actions specific to the product but it has no connector since jira is a sophisticated product that handles project management and Bug tracking it contains a lot of data you'll want to use the data from that platform in chronicle sore so we can use it to construct playbooks with custom actions and metadata from jira if we click on the document icon next to the Installation icon we can see a brief description of the integration a link for more info and even the actions which will be added these actions are what is used when designing playbooks with this integration scrolling down we can also see the type of jobs we can run the connector and the various managers included with this the release notes portion shows us the updates pushed to this integration these are handled automatically great now that we have that installed let's return to the connectors under sore settings ingestion connectors let's click the plus icon at the top of the connectors menu and select our new jira connector from the list take note that on this page there is mention of a remote connector and no agents configured with the option to install agent this is where we could alternatively install a remote agent to pull data and run actions on a remote Network or site however we will not be going over this during this course but we encourage you to review the documentation should you want to use this feature click create a new menu appears with a lot of information at the top is a toggle to turn the connector on or off once we configure it as well as the name of the connector and the description below the parameter section we see the word mandatory with asterisks next to mandatory fields in order to set up this connector let's do that now we first need to select an environment for this to run on so we'll select default environment we can edit the product field and event field name but we'll leave this as is for now we'll enter our API root URL for our jira instance as well as the username and API token great the rest of these options can be configured as needed but we'll do a basic setup for now let's click save we'll then toggle on the connector and again hit save the next tab is where you can test whether the connector is working by clicking on run connector once you can test whether you receive a call back from the Connected app and it will display the output with success or any potential errors the next tab is logs which you can enable by toggling the enable log collection C this will allow you to receive streaming log updates of this connector the final section we need to configure is the integration details within the environment we just assigned in this case the default environment let's go to response Integrations setup click on default environment and let's scroll down until we find the jira integration click on the gear icon to the right of the instance and we'll again enter in some Vital Information we'll again enter the URL for the API route for jira the username and the API token let's click save great we can also click test here to see whether or not the integration is firing properly with this final step we are now able to select jira actions to use while constructing playbooks the last area we'll briefly look at is under sore settings ingestion web hooks web hooks are a lightweight solution for pushing alerts from your organization into the platform cases with alerts ingested by web hooks appear in the platform with the same information as cases with alerts inest Ed using connectors Google recommends using either a connector or a web hook but not both from the same Source in order to avoid duplicates using web hooks is recommended for scenarios where more basic mapping logic is required for situations where Advanced mapping logic is required Google recommends using connectors because it provides more advanced and flexible mapping options we will not configure this for our demo but advise you to seek out the sore documentation should you want to use this feature to review there are five primary methods for ingesting data into chronical Sim cloud storage API feeds direct via gcp forwarder and pushing data via the chronical API the video demonstrates how to use each method with detailed steps emphasizing their unique features and configuration requirements additionally it covers chronical sore ingestion focusing on connectors and web hooks explaining how to integrate and manage these tools for efficient data ingestion and Playbook Construction in the sore environment join us on our next video where we'll go over normalization of [Music] data

### Video - [Normalization of Data](https://www.cloudskillsboost.google/course_templates/972/video/486842)

* [YouTube: Normalization of Data](https://www.youtube.com/watch?v=7KMqjw7-xtw)

[Music] video number three normalization of data hello and welcome to today's topic of normalization of data and Chronicle seops with Chronicle we discuss normalization in a number of different areas the first one is normalization of raw events being ingested in chronical Sim the second one is normalization of alerts coming into chronical sore in a lot of cases this will be coming from Chronicle Sim but they can also come from a number of different other alerting sources such as a monitored mailbox for user reported fishing attempts let's begin by talking about normalization of raw logs this is when the Raw event from a log Source gets normalized into the chronical data model you have heard us refer to udm or unified data model a lot and most of the data will be normalized into udm udm is the data model reserved for event data think of it as things that that have timestamps like a user login there is another data model in chronicle Sim called The Entity data model this is reserved for context or state not events this could be something like the user job title State can be active in perpetuity like a Windows host is a Windows host and will never be a Linux host or in most cases it can be active in a Time range until the state expires or another event changes it a user can be an analyst until they get promoted to senior analyst and from that point on they will be a senior analyst context is its own topic and deserves a separate video for today let's focus on normalization of events specifically with udm udm has a tree like structure and Loosely follows a sentence model you have these things called principal Target SRC Observer if you pivot over to the udm reference guide in the documentation and scroll to the top level schema of udm you will find them referred to as nouns it helps to think think about events as statements that have nouns and verbs user logged into computer user downloaded document successfully here our nouns are going to contain the details of who or what a noun can have a bunch of other properties underneath a host name one or more IP addresses a username Etc we will go through examples of that but in the process of normalization you can always refer to the complete guide and reference let's walk through a practical example of the parsing of a log we've already seen here you can see the network connection log to many good news.com in this case this is a format typical to a proxy solution such as zscaler the two most important nouns are principal and Target if you're talking about something like a network connection Alice with this client IP connects to many goodne news.com our principal have the properties principal. user. userid equals Alice principal. iip equals 10.0.3 228 and the user and our Target will be the destination target. hostname equals many goods.com you can expect the out-of-the-box parsers to do this by default as we've seen throughout the fundamentals training another noun is the Observer noun think of this as Alice connected to many good news as observed by This zscaler Server observers may not be super important to security analysts but they are definitely relevant for security Engineers to make sure that all the firewalls proxies and logging servers are reporting what they need to report next we'll discuss parsers parsers can be found in the settings Sim settings parser section A parser is default custom or it has an extension which will be marked here Chronicle has a very large parser team that constantly creates and updates parsers you can see some of these have pending updates you can view all pre-built parsers or view the pending updates of a pre-built parser Chronicle by default will use the pre-built parser but you can extend it using parser extensions or overwrite it with a completely custom parser for reference on the supported out ofth box parsers you can navigate to the parser documentation from the menu select supported log types and default parsers this will take you to a very long an Ever growing list of the supported defaults for standard log sources we always recommend trying to use the default parser the parser team continuously monitors for changes in source of formats and other issues should you need to customize a little bit you can use a parser extension a parser extension is a postprocessor that basically applies the default parser and then allows you to either add more values to it or override specific values if a default parser is not available or your Source format is fundamentally custom and different then you may have to create a parser from scratch you can do this with the log stash like syntax in chronicle called CBN or config based normalization but we will leave this for a more advanced discussion for this tutorial we will create a basic parser extension let's pull up a log we'll go into investigation and search for a sample crowd strike log for a file being opened the specific crowd strike event type we're looking for is file open info we can look for that in my udm lookup or we simply use the AI prompt generator to look for crowdstrike file open info events clicking search gives us a number of samples to work with if we open this survey. XLS example we can see the example of a file open info event our normalized event has a number of fields here including these enriched fields and we can see our raw log here let's do something with the share access field this is really the mode that the file is accessed as read only write only read write execute Etc you can see a numeric representation of that where seven means execution access from there we can go directly to our parser and are prompted between creating an extension viewing the default or creating a custom parser this parser is pretty well tested from The Chronicle Community however for this demo we just wanted to Showcase extensions now we're in an interface where we can interact with this raw log our raw log will be on the left and our parser will be on the right you can see there are two ways of writing extensions the easier more visual mapping mode and the advanced mode the easy mode works for Json XML and key value this log is in Json so we should do that for now the advanced mode allows you to use the full log stash like syntax provided by Chronicle CBN and can be used for advanced regular expressions and other processing clicking on preview udm output we can see exactly how this parses this is without the enrichments just the unenriched parsed fields we can edit the logs and see how different fields end up for instance if we change the event platform from wind to Mac you can see how our platform field changes our share access field is already parsed in security res. detection fields. Val so people can already search for it for the purpose of this exercise let's parse this again but in a different field if we look at our udm reference and navigate to the file object we can see that there is a stat mode field that is perfect for this it's an integer that is designed to store the file mode going back to our parser all we have to do is pick our source field and then search for the correct udm field autocomplete will help even if we just search for mode principal. file. stats. mode shows as the first suggested field but we actually want to put this in Target this is the file that is being opened so it makes sense that it's our Target not our principal just selecting that option ensures that this field is added we can choose whether to append values or replace values this would be relevant if there was already a value in stat mode for example but now since it's a new field it will always append it click on preview and you can see the result parsed nicely here the last step we have here is to click on validate this will pull data from the historic logs in chronicle and make sure there's no unexpected errors after all we only tested with one log we can see that our validation went smoothly and 100% of log lines were successfully normalized we are ready to submit this extension now that you have written your first parser extension let's discuss the second area normalization of alerts in so we've seen that in so we have an entity graph and each alert that comes through sore whether it's through Sim or a direct integration to another product we want to extract as as many of those fields as needed to automate further playbooks here the process is also Visual and fairly straightforward let's say we're in this virus found event when you open this most Integrations you install will automatically try to parse out entities you see the process name the file path the file hash Etc if you look at events you can see the raw Fields pulled from the actual alert along with many other fields if you want to modify What fields are getting parsed you can just go to the event definition let's use this risk name as an example you can see that the string is a little strange security products do this sometimes all we have to do is close this menu and click on the gear icon to configure the event mappings we're then taken to the mapping of exactly this event you can see how destination host name and all these other fields are being parsed let's move up a level instead of all the virus found Fields let's look at all the semantic fields or even on the top level we can look at all the events forwarded by our arcite connector you can see how for these events threat signature is simply lacking otherwise this would have been highlighted in green and you would see the value we still want to use the threat signature field if it's present but if not we want to use this risk name field as an alternative field we can see how it greens out for us and it renders the value iar test file which is a standard test file a pen tester or someone like that would use to test the AV engine now that we have this value parsed out we can probably put this in a Playbook somewhere there we can also automatically handle the triage check if our av quarantined or blocked it raise a ticket to our av team if they didn't and close as false positive we can even automate those notifications bragging to our red team that we caught them if that's the case that's our Deep dive into the aspects of normalization in the next video we'll discuss enrichment how you add fields that are not in the normal log or alert thanks for [Music] watching [Music]

### Video - [SecOps Enrichment](https://www.cloudskillsboost.google/course_templates/972/video/486843)

* [YouTube: SecOps Enrichment](https://www.youtube.com/watch?v=hu6iIJFtXDo)

[Music] enrichment in chronicle SEC Ops welcome to another episode in the chronicle secops Deep dive course today we are going to talk about how chronical Sim and Chronicle sore enrich raw data in order to create a story of Events first let's talk about enrichment in chronicle Sim this slide shows how enrichment builds up at the bottom you have raw data used for retention compliance and a forensic track record on top of that you have the normalized data in udm the unified data model this is used for search detection and dashboarding next you have the enriched data this is the data in the entity data model or entity graph this includes context about users assets and more and at the very top you have Global context things that Google knows this includes intelligence we will discuss the following types of enrichment number one stitching of identifiers how does chronicle know how to unify entities and ensure that the system knows what each user and host are doing number two enrichment of fields how can Chronicle enrich a variety of fields including IP addresses number three ingestion of enrichment sources ioc feeds context feeds and more number four outof thebox graph based enrichments Chronicle Sim was designed for enrichment this was built into the platform from its beginning where one of the first key features was connecting events together these events were connected together based on IP to host name to MAC address correlation which is commonly driven by DHCP logs this is what we refer to as stitching or building an alias and it's an incredibly powerful tool first it's worth mentioning that Chronicle performs in model stitching here's a useful example let's say we have a raw log that was created due to a user called Alice connecting to the website many good news.com these logs happen very often and it's normal for a Sim to collect them what isn't normal though is for a Sim to collect contextual information such as this information below describing our user Alice and providing her email employee ID and Department this is a type of log that is typically collected once a day and it doesn't change often but it's not a normal log it's a stateful log that's why we are expanding on our concept of normalization as this type of log doesn't get parsed into udm instead it gets parsed into the entity graph which as a data model doesn't follow the sentence model of user did thing with a result but it follows a more descriptive model of Alice is a user with username of Alice and email address of Alice comp.com this is a technical detail and for the most part you wouldn't be building parsers into the entity graph because a lot of them are already built but it's good to know this exists this often motivates technically Adept customers who usually have custom applications or systems of record these systems maintain context which their analysts typically have to manually review repeatedly this type of data can easily be ingested into Chronicle back to the example the data in the enrichment isn't passively stored waiting to be used by an analyst in a lookup of sorts it actually gets appended and flattened into the model now every single login network connection Etc initiated by Alice knows her email employee ID and job title at any point in time imagine how powerful this in model stitching can be when writing alerts further down the track let's go through a quick demo of this if we search for a user we already know for example an employee called mic Ross by typing user equals mic Ross we will get an overview of the graph metadata going into the Legacy view will also give us a nice rendering of the graph here you can see the graph not only maintains entities but relationships such as what databases Mike has access to you can expect the new udm search to have this type of visualization soon as well going back to our original udm search view you can see that user Mike Ross has 2fa disabled denoted by this label going into the events section you can see this with the e as an enriched field now we can easily ask the question what is the activity of users that have 2fa disabled and are contractors in the same Contracting Department Department that query result will pull all events now stitching any user whether they have the username mic Ross or this window security identifier here on the filter or the display name of Mike Ross it will be shown here which allows us to consolidate events from many sources again our stitching Source may come in once a day but the stitching occurs inside the data model which means you can now write detections that for example check if two-factor authentication is disabled now there is other inline enrichment that applies with the most popular being IP addresses Chronicle provides geolocation enrich data for external IP addresses to enable more powerful Rule detections and greater context for investigations for example Chronicle might use an external IP address to enrich the event with information about the country such as the United States a specific State such as Alaska and the network the IP address is in such as the ASN and carrier name here is an example where we can find that the IP address of many goods.com is hosted out of Saudi Arabia and you can see this visibly in the data model the enrichment happens inline as the events get ingested other examples of possible inline enrichment includes virus total data users of the Enterprise Plus Edition benefit from having all this Rich virus total data directly in line with the event some enrichment is calculated as a powerful metric behind the scenes let's look at the question of how we may know many good news.com is bad just by using context let's pull up many good news.com first we have who is Data this is accessible to us in rules the two most critical metrics here are first and last scene as well as prevalence these are computed in the back end and provide Great Signs in this case many goods.com is low prevalence or rare in the environment this is a signal we can use in detections we also see indicator matches and indicator feeds are another useful signal these are all examples where enrichment sources have been ingested and are available for use in rule creation if you search the chronical documentation for chronical enrichment you will find a number of other examples this includes Google Cloud threat intelligence around existing tour noes known benign executables Etc now you don't have to manually maintain IP lists of all the tour nodes as you can just tap into that intelligence we will wrap up this section with an example of a rule that uses enrichment in the fundamentals course we saw an example of a correlation between two events a process launch and a network connection what about correlation between enrichment and an event well if the enrichment is inline like an IP address or a job title you can easily do this in fact we already demonstrated it when looking for users that have tofa disabled but what if the enrichment is not in line after all not everything can be enriched in line otherwise these enriched udm events will become infinite let's say we look for logins but we also want to add that they are coming from tour exit notes you can see an example of this rule here in the event section we have an event login as well as an entity if that IP address is found in our graph in the list of known tour nodes then we will just alert all we had to do is mention the feed name that was provided to us in the enrichment documentation the Google Cloud threat Intel team will maintain this list for us this is an example of outof the-box graph-based enrichments now that we've discussed enrichment in Chronicle Sim let's discuss how to enrich even further with chronical SAR when events come into sore case management they also get modeled into entities if we go back to our case we can see how that occurred when the entity many good news.com came in it was enriched with a bunch of these details details if you remember we've already discussed such as Sim enrichment as well as enrichment that came in through a Playbook like these mandiant enrichments entity enrichments can also be useful Boolean flag like is suspicious there are a few different ways to apply enrichments into the sore data model one is we can do it right when the alerts are being ingested for that we have to navigate to our event mapping the easiest and fastest way to do that is through the event Itself by clicking on the gear icon this then leads us into a configuration which should be familiar from the SEC Ops fundamentals course here not only can we map the event identifier such as the username or user email for Steve Watson but also add a number of enrichments if we click on the menu and choose add enrichments you will see a list of available fields and we can select any of them to enrich our user you can see here that intern is selected but department is not we can easily add that to our enrichments another way to do that is through playbooks if we go to any playbooks and pull up the actions you will find there a number of actions to manipulate entity enrichments you can Mark entities as suspicious and en Rich entities from Json and more this way in the course of Playbook execution you can add important details from your entities it's worth mentioning that this is part of one of the powerups if you navigate to MarketPlace you will find a number of these powerups that are essentially different Playbook tools and actions that will make your life easier enrichments will allow you to manipulate playbooks a lot better and these are the actions that we saw in this example wrapping up we've looked looked at log level enrichments of data in chronicle Sim this makes it a lot easier for us to determine what happened conduct searches based on context build context aware detections and more we also determined that these enrichments also exist in sore where our system becomes smarter over time about the entities we deal with and we can manage these enrichments very easily thank you for watching and until next [Music] time [Music]

## Building Detections

How to build a detection with YARA-L

### Video - [Building Detections with YARA-L](https://www.cloudskillsboost.google/course_templates/972/video/486844)

* [YouTube: Building Detections with YARA-L](https://www.youtube.com/watch?v=0TUncXVTyXc)

[Music] video number five deep dive into building detections with Yara L welcome to today's video where we look into Chronicles detection language Yara L this tutorial aims to follow the new to Chronicles blog series by Google Cloud principal security strategist John Stoner and we strongly recommend diving into this series as you explore this topic further today we're going to be working within the rules engine under detections rules our goal today is to explore the capabilities of the rules engine this video post is the tip of the iceberg in regards to the rules engine as there's many different operators that users can utilize for deeper exploration look into the documentation or follow the new to Chronicle blog posts the rules engine in chronicle uses a language called Yara L to build detections for those not familiar with Yara it is the language that Google's virus Total Team developed for use with malware samples and it's been a practitioner favorite for over a decade for hunting and exploring malware binaries the original Yara syntax was designed for binaries but Chronicle took some of the concepts of Yara and adapted it to be used for logs and we call it Yara l or Yara for logs Yara and Yara L are not directly portable to one another as one is designed for logs and one for binary so they operate on completely different data sets Yar L can be used for detection as events are being ingested and it can be used to look retrospectively across historical data this provides analysts with flexibility to develop new detections and use historical data to validate and test rules as well as uncover threats that were not previously known if you want to learn more about yl check out the white paper explaining its design let's navigate to our first sample rule to review Yar L has a fixed format of sections used when creating rules while there are six sections as of today only three are required to get started let's focus just on the components needed to write a single event rule in future posts we will cover the additional areas as we build increasingly robust rules the three required sections of any Yar L rule are the meta events and condition sections meta contains the metadata associated with the rule itself events contain the ud M fields and their Associated values that are being evaluated to determine if criteria exist for the rule to fire condition specifies the circumstances that would cause the rule to fire this is really simple for a single event rule but gets a lot more fun for multi-event rules let's take a look at an example to see how this works in this example we want to detect adversaries who are searching the Windows registry for credentials that may be stored there this rule is based on the miter attack technique unsecured credentials credentials in registry this rule was originally written by the team at sock Prime and was converted from Sigma it was adapted to reflect the current technique and sub technique in The Meta section we have a number of fields defined including the author of the rule a description of what the rule does and the severity of the rule we added additional specificity around the attack tactic and technique that we are attempting to detect this metadata can be seen when viewing the rule detections this view provides that summary information at the top of the page and the detections that fired the event section contains the criteria that needs to be observed within the logs for the detection to fire each item in the event section that is being evaluated will be proped by a variable name in our example you can see the string dollar sign selection one is proped to our udm Fields this event variable is arbitrary but establishing a common taxonomy for your organization is important as you build out your rules you may wonder if this name is even relevant it's not since we're working with a single event rule but as we start introducing multiple events into our rules having that common taxonomy will become important three operators are available to connect criteria together they are the following and or and not by default and is assumed in the absence of other operators in fact in the example above with one piece of criteria on each line Yara L reads this as dollar sign selection 1. metadata. event type equals process launch and dollar sign selection one. targetprocess file. path equals forrex SL parenthesis can also be used to group items together the final two lines in our example contain two different registry queries and the existence of either one will trigger our rule to fire we can bind them with parenthesis and separate them with the operator or the not operator provides a way to exclude certain criteria from consideration if we wanted to exclude a specific user or specific host from consideration the not operator could be proped to the event criteria search criteria can take the form of a string match as we see in the first line regular expression matching is also possible Chronicle uses re2 there are functions for regular Expressions that are subject for a deeper dive but for now we can use forward slashes around a value to denote a regular expression very similar to how this is done in the udm search we can add period asterisk to handle variability in spaces file extensions or other unpredictability within the values that we are searching for in our criteria because the forward slash is used to bound our search term and denote a regular expression back slashes are used as Escape characters to indicate that forward slashes within the string are to be treated as a forward slash and not as the end of a regular expression if we wanted to loosen our Search terms to find greater variability in our events we could add an additional period asterisk between our values we can also add the modifier no case to the end of our expression to insulate ourselves against adversaries mixing their case in their commands the final mandatory section and the last one we will talk about today is the condition section now for a single event rule the condition section is kind of boring it is basically going to be dollar sign selection one or whatever the variable we are using for the event itself when we get to multi-event rules this will get more interesting with that we have a single event rule but before deploying it perhaps we want to test it against our events to ensure we are getting detections that we expect Chronicle has us covered with the test rule capability within the rule editor simply specify the time range and click run test and the rule will run run against our data during the time period specified and return any detections we can also drill into both the raw and udm event to see the underlying data and can further refine our rule as needed you can now do the regular operations with this rule including saving it and enabling it at the time of the creation of this video Chronicle has a limit of 1,000 single event rules but this limit is continuously expanding so check your interface or documentation for current numbers any rules over that limit can still be saved and used but cannot be enabled live now let's talk about multi-event rules and searches building multi-event searches is pretty straightforward because at its heart it adds one additional section to our Yara L rule if you recall all rules have meta events and condition sections multi-event rules add a match section to the rule at its most basic the match section is going to group events by one or more Fields across a Time window that you specify when working with multiple events you need to establish commonality between the events to bind them together together for example if you are looking for two kinds of events on the same system you might establish the host name as the common field in other cases you might look for both the username and the host name previously we introduced event variables which are proped to each field in the event section of the rule with the addition of the multi-event rule we are going to also introduce placeholder and match variables that we will use within our rule another New Concept that we are going to cover are time Windows time Windows help Chronicle constrain the the amount of data that is being looked at to determine if a rule should fire or not there are two different kinds of time Windows the first is a hop and the second is a sliding window the default hop window takes into account all events that match during the time window but it does not take into account the order of them sliding windows will focus on a specific event order but this does have a performance impact the Syntax for sliding Windows is slightly different when I say different I mean it takes into account an initial trigger event that you can think of as a stake in the ground that other events must occur before or after for the rule to trigger with these Concepts laid out let's look at an example let's say you want to build a rule to detect temporary accounts being utilized specifically the use case is that you want to detect when an account has been created used to login and then deleted for our purposes we will Define our threshold for these events to occur within 4 hours but this time range can be adjusted based on what you define as a temporary account the match condition time range can range from 1 minute to 48 hours so there is a good deal of flexibility to work with the event section will include our event criteria for each of our events account creation user login and account dele fortunately we can use the metadata. event type enumerated values of user creation user login and user deletion to denote each one of these events if you are looking at a specific application or need to tighten the event event criteria additional criteria can be included unlike in the single event rule we need to differentiate these three events from one another so our event variables are going to be unique from one another we could use dollar sign E1 or dollar sign selection or any other name to describe each event but a concise descriptive variable name will pay dividends down the road that is foreshadowing so stick with me we will use dollar sign create dollar sign login and dollar sign delete to describe each of these three events the second line of criteria under each of the three metadata. event type Fields is what is called a placeholder variable the idea behind the placeholder variable is to take the value of a field in an event in this case the target. user. userid field and write it to a variable that can be used elsewhere in our rule this is done for each of the three events because we will need to link them together you might be thinking does this field need to be the same field in all events the answer there is no it can be any field but in this example it is the same field the final pieces of criteria in the event section are the two less than or equal to statements that are comparing times these two lines of criteria allow us to order the create event before the login event and the login event before the delete event you might be thinking what about the talk of sliding Windows why don't you just use that well there is no reason you couldn't use it but because you are looking for a specific order across all events you can enforce greater control by saying create comes first then login and then deletion based on the event timestamp you can just collect all the events in the window and evaluate it together with our event criteria specified we are going to move to the match section the match section is where we will use our match variable now the match variable looks like the placeholder variable from the event section that's because it is the same but this is where it is used to join our distinct events together if we had multiple fields to join we would have have multiple placeholder variables defined in the event section and multiple match variables separated by a comma in the match section to complete the match section we add the required word over followed by our time window of 4 hours that we are using for our window for all three of these events to occur within the condition section is a bit more robust than what we saw in a single event Ro but it is still pretty straightforward our condition is that we need at least one event of each type create login and delete to exist for our rule to fire this is why we have all three event variables in the condition separated by the operator and when we save and test our rule we get detections that look like this if we click the greater than sign next to the detection we get this our first event is the detection itself and the expansion contains the underlying events that triggered the rule to Fire and the detection to be created if we look back at our event section we can see that we have our metadata event type values in the purple boxes ordered now check out the black boxes under the detection column notice that they say create login and delete we've seen these terms before that's right they are the event variables by using brief but descriptive terms for event variables our analysts can benefit from that when looking at the detection and quickly understanding the context around the detections and the underlying events they are looking at with that it's time to wrap this up we now have a rule to detect creation login and deletion of a temporary account and hopefully a better understanding around how you can create your own multi-event rules and start exploring the syntax further once again make sure to review the documentation for syntax specifics this is also a topic extensively covered in the new to Chronicle blog series that will layer in more details to this using the outcome sections interesting functions and more until next time where we will dive deeper into customizing response capabilities [Music]

## Marketplace and Settings

Deep dive into Marketplace and Integrations, Building Integrations and Settings

### Video - [Marketplace & Integrations](https://www.cloudskillsboost.google/course_templates/972/video/486845)

* [YouTube: Marketplace & Integrations](https://www.youtube.com/watch?v=gYU0vn4uG9o)

[Music] video number six Marketplace Integrations and creating your first integration welcome to the chronicle SEC Ops Deep dive course in this video we will take a deep dive into the marketplace and Integrations and build our very first integration you can navigate to the marketplace in the navigation on the left hand side Marketplace allows you to inst company Integrations Integrations published by the community and custom Integrations that you have built in the IDE the marketplace also contains a repository for predefined use cases powerups that enhance Playbook capabilities and analytics that provide valuable insights there are three types of Integrations in the marketplace number one chronical Integrations number two Integrations published by the community these have been validated by Google and will appear with user details next to them number three custom Integrations these are Integrations which you have created and which are only displayed on your Marketplace you can refine your search by selecting the type of integration and or status from the drop-down menus located on the top left of the Integrations tab you can also filter your search for an integration by its status installed not installed and available upgrade Integrations that have not been installed yet will have a download icon on the bottom right of the box click the icon to successfully install the integration and load the code into the integrated Dev environment or IDE custom Integrations will not show the download icon as they are installed via the IDE all Integrations need to be configured and saved each integration is called an instance once configured you can see these instances when navigating to response integration setup on the left of this page are the environments in which you can configure an instance the shared instances provides a container where you can can configure instances that can be used in all environments or you can add different instances for each environment each configured integration will have different parameters defined by the integration developer let's see an example of an integration being configured we're going to go ahead and download the Azure active directory integration if we click on the details we will find the actions available here additionally there's usually a helpful documentation link on how to set up your third party app in this case we would need to do an Azure ad app registration which we have already done and we've configured the proper permissions follow the documentation here for the app you're trying to install as there's specific permissions that apply for various different Integrations now if we go back to our setup clicking on the gear icon will allow us to configure and test this integration as you can see the parameters requested are typical Microsoft graph parameters that were already described in the documentation we are quickly going to put those parameters here after that we can go ahead and test the integration by clicking test this will use a special action in the integration called ping that will return success if our configuration is successful now we are ready to use our integration in playbooks one quick thing to show is how to test Integrations in the IDE under response you will find the IDE the integrated development environment or IDE for short is a framework for viewing editing and testing code it allows you to view the code of commercial Integrations and to create custom Integrations from scratch or by duplicating commercial Integrations code in addition this is the place to manage Import and Export custom Integrations we can expand our collapse all accordion tabs and you will find one tab for each integration this corresponds to every integration installed from the marketplace Chronicle Community or custom at the very top we will find our Azure active directory integration and underneath you will find the code for every single action manager or connector in a little bit we will build our first custom integration where we will explain some of those for now you can use this as a place to inspect how various Integrations are built and see the code for everything even the outof the-box Integrations you can also Import and Export Integrations or individual items the gear icon with a play button denotes an action let's click on one of these actions such as the list users action you can see that each action is nothing more than python code with some special syntax and some abilities to set up inputs and outputs without going through every line of this you will notice there is a details tab with a list of parameters these correspond to the parameters that A playbook developer can specify as a reminder these parameters can be anything static values inputs from the case inputs from other actions Etc playbooks are the way to chain those together the best way to understand an action is by testing it with some manual values you can easily do that in the testing tab here we have to define a few values first when you test any action you have to specify scope this is because most actions work on entities as opposed to trying to force soar admins to manually specify inputs and outputs The Entity model is normalized and we've covered it to a greater extent in the normalization and enrichment classes these entities and other test details will be picked up from our test case remember that when you are in any case you have the option to ingest an alert as a test case this becomes useful here we can select any open test case from the drop down here we get to pick an integration instance here we only configured the default instance however if you have multiple instances you can easily toggle between them the rest of the fields are not mandatory so we will leave them blank and default for now all we have to do is hit play and our integration will run and return the needed results you can expand upon the Json results of this where we see our first user Adele if we had switched the Sorting order to be descending instead and hit the play button again you would find a reverse alphabetical sorting of our users the debug output tab provides us with useful debugging messages here for instance you will see the outputs of print and debug statements in the code this now gives you a general idea of how to browse and test existing Integrations manually if you're trying to understand them but what if you have to build a new integration or action let's put all of this information in action by building a simple hello world integration that takes the input of a name and outputs a simple hello greeting to our user in the left navigation navigate to response IDE click the plus icon and select integration enter a name and click create the integration will be created and listed on the left hand side with a gear icon to the right that designates it as a custom integration clicking the gear icon will bring up the integration settings where the icon description python dependencies and integration parameters can be defined some of these details will also show up in our Marketplace note that script dependencies are python libraries that the custom integration will need to import dependencies can be added as wheel files tarballs gunzip format or python files every integration runs in its own virtual environment so feel free to add different versions of libraries even if one is already installed on the system now that our integration is created let's create an instance of it which means set it up we will find our integration in the marketplace as a custom integration pressing on the gear icon will allow us to view the details where we can add a description if we like we can't use the test function yet because we haven't implemented a ping action you can see this indicated in an error here we will not do that right now but that's covered in the documentation this was sufficient to set up our default instance we can set up more instances in our settings but let's leave that as is for now let's go back and finish our action now if you go back to the IDE you can create a number of things including connectors jobs managers and actions managers are not strictly necessary for an integration to function but they are a great idea especially for Integrations that involve third party tools managers are essentially rappers that contain the API Logic for the third party tool in our case we just want to do an action we will give that a name then we will choose our hello world integration from the drop-down and choose the type sync for the action this means it will execute when the Playbook calls it versus an async action that will wait for a response you can see the boilerplate code here we can even test it within the testing tab once again we just have to choose an entity or entities choose a test case and an instance and click run this integration has no inputs and it has just a boilerplate output message in the debug output tab you will find more details our action actually went through all enti ities attached to our test case and printed its identifiers now let's create an input for name for this we will create a parameter by going to overview and clicking the plus under parameters we will type in the parameter name and add a description we will also make this parameter mandatory under type we will select string you have a number of other interesting options there for other types of actions since it's a required field we also have to provide a default value let's click save and now we have a parameter now let's look at the code this is very standard python with a few libraries there is a decorator on our main function called output Handler which we need in order to ensure playbooks can take outputs from this action then we instantiate an object of type simplify action this has a number of interesting functions and you can look into a lot more details when exploring the SDK in this example we only see a few methods such as the one on this line extracting the parameters this is helpful example code of how to extract integration ation parameters and action parameters we don't have any integration parameters those are usually API keys and other integration details so we will comment that line out but we do have one action parameter name so let's modify this line to extract it we will change our output message from this static string to a formatted hello string adding the name this can use any python syntax result values can be used if there's some Json resulting from the action but let's keep this simple for now we can also comment out the code printing entities this is not very helpful for us right now we can hover over the text and press control or command plus K and C similar to some of the idees that you may be used to we will leave a logging message here and just end our action using a simplify do end statement returning our result let's go back to test this in the testing tab we can now enter name Natasha and once we hit play the integration will return hello Natasha let's see this in action in a Playbook we will go to response playbooks and click on the plus symbol to create a Playbook enter a folder for this and select the environment we are now in an empty Playbook every Playbook needs to have a trigger so we'll just select all for our test we can see our action in the actions tab when we search for Hello World you can search for the integration name or the action name once you drop the action and edit it you see our parameter in there we can put in a static value or use any of the parameters available to us for example the case assigned user from this menu if we turn on our simulator we can now choose any of the test cases in our instance to test on you can see the result of our action there this wraps up our basic action you can find more advanced examples of building a first integration in the documentation in our next class we will focus on expanding this action by using some of the SDK and building a custom View for the to render our greeting in a widget thanks for watching and until next [Music] time

### Video - [Building Integrations](https://www.cloudskillsboost.google/course_templates/972/video/486846)

* [YouTube: Building Integrations](https://www.youtube.com/watch?v=jhYcLBz2Cpc)

[Music] video number seven building Integrations working with the SDK and customizing views hello and welcome to the chronicle SEC Ops Deep dive course in our last lesson we went through building a simple hello world integration today we are going to expand a little bit more into advanced development Concepts into our IDE as well as discuss building custom HTML views if you remember we built this simple action called hello world now instead of it only returning a greeting we actually want this to return two values the greeting as well as a value return from the system such as how many comments does a case currently have in the code we will make a few changes first we are going to extract out our greeting where we Define greeting equals hello plus the name we are passing now we're going to make this more interesting and return a Json with our action not just a simple output as that is designed for status messages anyway let's change our message to something like successfully retrieved greeting we're going to create a variable here called Json result and assign a dictionary of values currently just the greeting variable here under the value with the key greeting if we go to our testing tab then specify all entities a test case an integration in stand and hit play we will see the result of that before we're able to test it we'll add one more thing there is a notion of a script result this can include things like entity reports Etc but the simplest one is a Json result we can add that by saying simplify do result. add result Json and passing our Json object the various script result methods are described in the SDK reference we will review in a minute hitting play you see that we now actually have not just the output message but also the result Json including our greeting variable A playbook can now use the outputs of this action next let's talk about how to get more interesting data in that Json here there are two concepts sore sdks and sore apis sdks are a bit easier to use but have more limited functions the restful apis offer a much broader capabilities within chronical sore you can find reference to the SDK by going into the reference section of the chronicle documentation there under s SDK you will find a number of different sdks the camplify action class that we already saw has a lot of useful methods such as adding comments adding attachments etc for instance if we want to count the case comments we can just use the get case comments method let's try that in our IDE we will add one more variable called comments here we will call simplify doget case comments since we just want the number of the comments we can call the function Len to get the size of the array again standard python can apply here just add this variable to case result by adding it to the Json and we suddenly have the number of comments nicely displayed here in the Json result we can run this test again by hitting play and this will return the latest data with our newly added comment you may be thinking what if the SDK doesn't support the action that I need the good news is that there is a huge variety of apis available in fact every action we've done in soar behind the scenes is calling those apis so chances are anything you can do through the UI you can do through the API Chronicle sore comes with automatically generated Swagger docks that have a huge library of apis now these will be restful apis which you can test directly by authorizing things with an API key that you can generate within the advanced sore setting section you can then browse a number of sections with a variety of apis for instance if we wanted to test the get case exists API call we would just click on try it out give it a parameter in this case a case ID such as 100 and we would see the request and response in our case true methods here are marked with get post delete Etc you can use that reference to build an integration with a separate thirdparty application using an API key the Swagger docs provide details of the HTTP URL method and body that you need to pass what is nice about actions is that they handle authentication internally this means you don't need an API key you can just write your API call inside an action code as there are helpful methods here to initiate a session that handle authentication internally we won't be adding that to our hello world example but you can see countless examples of that in other Integrations if you would like to explore for example if we go to our tools integration you will see add comment to entity as a method this will refer to one of those API endpoints in this case V1 entities ad note you can see here how we are using the simplify do session. poost method and passing the URL the API route and adding the Json data no need to use API keys or anything like that using those apis is a bit more advanced than a deep dive course but we wanted to make you aware of their existence as they are a powerful method of customizing anything within your sore instance going back to our action example let's quickly recap now we have an action that takes a name as an input and returns a greeting and a total number of comments associated with a case and we want to incorporate that into a Playbook one thing that we strongly recommend all developers do is incorporate an example of Json output with each of their actions that return Json this will just make it a lot easier on playbook developers and you will see why a little bit later in the this video to do this we can go under the details Tab and toggle the include Json result button then using the Json icon we can open a menu and choose to import Json sample and upload a sample of what our expected result should look like it should probably look something like this now the last thing is we want to change this return value and simply pass true this means that this action has a result click save and our action is ready for action we want to wrap up our hello world example with a HTML widget for this we're going to go to response playbooks and edit the Playbook we already created in the previous video if we run the simulation again we will get our greeting as well as the total comment in the new Json that's great we can use this in future actions or we can also use this in our view let's create a custom view here by clicking on the ad view button we will give this a name and assign it to some roles you can have multiple views for different roles but this is just a simple example we don't have any predefined widgets here so leaving that checked wouldn't do much but we can do that anyway now we can drag and drop various widgets we can do a simple Json response widget that will simply display the Json if we navigate to configuration we can easily find the data source clicking on the brackets allows us to pull inputs from content text or from the Playbook itself we can see our Json result is now being displayed but we're not implementing sore to stare at Raw Json so let's create a styled HTML widget drag the widget into the view and click on the gear icon here in the configuration you can immediately start writing HTML but you can also select from one of the presets which are great simple examples of widgets you can use let's use this preset called score which will visualize a gauge chart this generates some HTML CSS and JavaScript code and you can put any custom HTML code in this window you can set the widget width to be 33% 50% or 100% of the view and in the advanced settings you can even render it conditionally if we expand our HTML code we can see a nice preview under the code you can see how changing the values actually changes the widgets now instead of this header we want to show our greeting for this will plug a variable by clicking on the square brackets icon this pulls up a menu of possible variables to pull in we're really interested in the result of our action you can find that Under The Playbook section and under the hello world action if you look at the Json result option there is an icon that allows us to browse that result this is where that sample Json comes into play we can just select our greeting from the Json and once this is in a case this will render our greeting now now let's do the same with our total number of comments and just show engagement we will enter a variable here and look for the proper field in the Json result additionally we will make the maximum engagement be 10 on that scale just for the example we are now ready to test our Playbook we can go back to our cases and just simulate a new case like this failed login case for example going in we will find our widgets we will see a hello tier one as this is the default assignment and we will find no comments now let's make a few changes to this let's add a comment perhaps and assign the case to Tatiana now we can navigate to the playbooks Tab and replay the Playbook using this replay icon this replays all our actions and once complete we can return to the overview tab running this action will change the way the widgets are displayed you will see both the Json result and the HTML widget with the new name and the updated number of comments in the gauge that's it for this class hopefully by now you understand enough to know how playbooks actions Integrations and Views operate and have confidence in the ability to extend the system for many different use cases until next time when we will conclude our course with more resources on developing on top of the chronicle SEC Ops [Music] platform [Music]

### Video - [Settings](https://www.cloudskillsboost.google/course_templates/972/video/486847)

* [YouTube: Settings](https://www.youtube.com/watch?v=1zzj92LOJTE)

[Music] video number eight settings hello and welcome to today's video where we'll review the settings feature using Google Chronicle with the introduction of the unified instance the settings tab is now merged with options to customize and edit your sim settings and or your sore settings all in one convenient place in this video we'll take a highle look at each page and discuss the details of each settings feature Sim settings let's start with the SIM settings by clicking on the settings tab Sim settings the first page that pops up is the profile page where you can view the details of the current profile you're logged into under user details the IDP user ID and group is listed in addition to the roles assigned which covers which permissions your profile currently has which in this case is an administrator the organization details lists your customer ID gcp project number and the gcp project ID clicking the associated link will take you to your gcp console for the project assigned to this instance moving on the users and groups page is where you will predictably assign users and groups the main page separates out the user group type and assigned role clicking the assign new button brings up a popup where you can assign groups and users to different roles you can easily change roles by clicking on the arrow and changing to a new Ro based on the drop- down menu lastly at the top right you'll notice another drop- down menu this is where you can Define the default role for new and unassigned users and groups the roles page is where you can view roles and their Associated permissions remember these are Sim settings and do not affect access to Features within the sore environment there are four roles administrator which has full access editor which has the ability to edit but with reduced Ed permissions from administrators the viewer role which can only view specific Sim information but cannot edit it and lastly viewer with no detect access which can view as well but with no access to detect information the feeds page is an important page as this is where you will add the majority of your data feeds into Chronicle the feeds are labeled by name status Source type and log type you can add a new feed by clicking the add new button at the top which brings up a popup where you can Define the parameters of the feed you're ingesting we won't go over this now since we already did in the ingestion video of this course the forwarders page is where you will set up and download the configuration information for your forwarder forwarders are used to send data to Chronicle from Windows and Linux devices typically on premise although they can be cloud-based the main page gives information on the forwarder name config ID and the date it was last updated expanding the menu by clicking the arrow icon will give you you further information about The Collector as well clicking the add new forwarder page will bring up a popup where you can configure your forwarder however again this was previously covered in ingestion so we'll move on the parsers page is where you will view the information about all of your parsers it is labeled by the log type parser type last updated status validation and update columns since it's possible to have many parsers you can use the search feature to search through the names of your parsers to quickly access them you can also filter based on whether the parser is active inactive pre-built pre-built extended custom or custom extended lastly clicking the create parser brings up a popup where you can search for a log source and continue with a raw log custom parser or start with an existing pre-built parser the next tab is the Google Cloud platform page where you can connect Chronicle to Google Cloud platform to manage Google Cloud assets simplify login with a single set of credentials and log chronical activity in Google Cloud audit logging clicking the go to Google Cloud platform button will again open your Google Cloud console and allow you to continue with setting this up the final tab in the Sim settings section is Google workspace this is where you have the option to connect workspace and Chronicle to detect Insider risk in your environment you'll need to enter your Google workspace customer ID and then click generate token from there you can click the go to Google workspace admin and log in to your workspace admin console to finish the setup sore settings the next major section of the settings tab is the sore settings tab click on the navigation pane to the left and select sore settings group number one organization this first group is named organization this group is where you will handle your environments permissions license management roles and rebranding we'll start with user user management where you can see a list of all the users currently assigned to your sore instance further you can use the search bar to find users based on strings that match the fields listed on this page if you click on a user and then the pencil icon you can edit the respective user as it brings up a popup with all the relevant fields and information for that user clicking the plus icon brings up the same popup but with blank information as this is where you will add new users to the instance click cancel and let's continue to environments the environments page is similar to the users page although this is where you will Define environments for your sock tenants data just like the user management page you can search add and edit environments with the same functionality however this page has an additional feature click on the ventilated Kebab menu icon and you'll see a few options add Dynamic parameters export Dynamic parameter values and import parameter values expor ing the parameters will simply export the environment metadata into a CSV file you can also import parameters based off of a CSV file formatted within the same way lastly add Dynamic parameters brings up a menu where you can Define different parameter values based on the environments this is a new and helpful feature that allows you to do things like create playbooks according to those parameter values using placeholders instead of only using the default parameters values this way you can have logic for different environments for different use cases next is the permissions page this page is powerful in managing your instance as it gives you the ability to create permission groups with privileges for each module in the sore environment looking at this first permission group admins we predictably see that the group has access to all modules and features in chronicle sore however the basic group for example only has access to dashboards search cases entity Explorer homepage report ports and SLA we can add more permissions simply by toggling each module on or off and clicking the checkbox for what permissions we wish to grant for that group click save to ensure the settings are retained you can also copy permissions by clicking on the duplicate icon should you want to start with a template that you have already configured lastly clicking the ad permission group allows you to name and define an entire permission group similar to how we edited the one in our previous example you will have to select a landing page in order to be able to save any settings you make moving on the license management page is your One-Stop shop to keep track of your system license status and anticipated expiration you can view critical information like the validity of your license the system version which also lists your customer ID the limitations of your system and the modules for your system which lists which modules are active or disabled based on your license the roles page is where you can view edit and create security team roles and cases access this is where you will Define what roles have access to what additional roles like the ceso RO here that has access to administrator Tier 1 tier 2 tier three and sock manager roles like user management and environments the search addition and edit features all work the same you can select a role to be default by clicking on a roll and then the edit button and selecting set as default and then saving the rebranding page is where you can choose to display your company logo on the header of each page and or on all exported reports simply click on header Andor reports and then upload an image by clicking the browse button and then apply at this time only PNG files between 120 to 3,000 pixels with a maximum size of 5 megabytes are accepted group number two case data the next group in Source settings is named case data here you can configure and customize case information and data including tags and Views we'll Begin by starting on the tag page this page is where you will manage tags that are automatically added to cases let's click on the plus icon to add a new tag condition we'll need to add a name for the tag so let's pick demo from there we need to select a tag condition the condition which is matched to automatically add the tag to the case we can select between entities product rule generator and vendor let's keep it simple and select product the dropdown allows you to pick products already in the environment or Define your own let's stick with other and enter the value demo app click save this condition will make it so that anytime a case comes through with the product name demo app it will automatically be tagged with demo great let's move on the case stages is where you define case management stages for handling cases according to Sock methodology currently you can see there are six stages triage assessment investigation incident Improvement and lastly research click on the plus icon and let's add one more we'll name this final with an order of seven meaning it will be the last stage in the case management steps click add from case stages we'll move into case close root cause here you can Define root causes for closing a case whether it was malicious or not and what was the actual case you can see there are a number of root causes here like external attack or human error with external attack being labeled as malicious and human error being labeled as not malicious let's add our own by clicking the plus icon we'll name the root cause testing and the reason as maintenance click add great this is now a case closure cause we can use while testing to make sure our test cases are not bundled up into real threats moving on the case name page is where you can select the hierarchy for case names as alerts are generated as as it stands now our hierarchy is first to create the name of the case based on the rule name which triggered the case next it will select the first tag in the case if there is no rule name to add lastly it will choose the product Alert in the absence of either of the previous two you can set the hierarchy for up to five names and add new ones by clicking on the brackets and selecting the field from the popup menu for example we can select alert. event Name by searching for name and then selecting that field click save the final page is views we covered this in-depth in this course earlier with playbooks and Views so we'll just cover the overview here the first tab selected is default case view here you can edit the default case view for all cases and select from General Widgets or predefined widgets coming from a specific product like security Command Center the second tab default alert view is simply where you define the default views for individual alerts within a case group number three Advanced the advanced group is where you will make changes to advanced settings there are a number of these that don't Naturally Fit together so let's go over each page one by one the API Keys page is where you will generate API keys to interact with Chronicles rest API like many of the pages before this has the same basic features of search addition and edit options clicking on the plus icon brings up a popup where you autogenerate an API key be sure you click the copy to clipboard icon at the end of the key to write it down as it will be obfuscated after creation you can fill out the relevant Fields here such as application name the permission group environments and sock rolls be sure to hit save to keep these settings the audit page is your single pane of glass into usage and activity from users using the sore environment there is an audit log that can be filtered by various Fields by clicking on the drop downs above the main menu screen you can further export this log by clicking the export icon and Export this log as a CSV lastly you can see a feature at the top of the page that gives you important stats about user activity the general page is where you can configure various settings to fit your needs there are only two main sections here cross environment policy and data retention be sure to hit save should you make any changes to these widgets localization is the next page and is where you will manage and configure the default time zones and datetime formats for all users in the platform clicking Advanced configuration carrot allows you to edit the display of date and time formats as a bonus if you click on your profile in the top right corner you can also make these same changes in the popup the alert grouping page is where you will manage preferences for alert grouping into cases you can make General changes like the max alerts grouped into a case time frame for grouping alerts and whether or not to group entities and Source grouping identifiers in the same case you can also make changes to rule hierarchy and overflow settings for cases IDP group mapping is where you can map IDP groups to permission groups you can also select the default access level for users with IDP groups that are not mapped in the mapping above remote agents is the next page where you can set up and manage remote agents the remote agent is a python based application which is installed on remote environments and provides the ability to pull data and run actions on a remote Network or site the purpose of the agent is to be connected and accessible to the required security products in the remote environment there are two ways to deploy the agent Docker and installer this is a lengthy subject and we recommend you reference the sore documentation to better understand remote agents and their configurations the email settings page is the most straightforward page it's where you can select to manage email addresses from which all system emails will be sent you can select Chronicle SMTP which is automatically configured or customer configuration which you must manually configure and fill out lastly and importantly we have the support access page this can be enabled and you can specify access to Google support by selecting the required permission levels and time period group number four data configuration the fourth grouping on the sore settings page is data configuration this section is short but important let's start with the properties metadata page here you can view property definitions for data ingested from your data sources the configuration for this page is similar to the majority of the other Pages we have reviewed so far in this video the next page is the statistics page where you can Define case statistics according to desired properties and their appearance group number five ontology the ontology grouping is entirely focused around the visualization of visual families while using sore it begins with the page ontology status this page is where you manage and configure visual family matches to specific products and events you can see a series of stats at the top for product event types and events assigned to default families looking below you can configure the visual families for each Source by clicking the gear icon at the far right of the sources by clicking the check box to the left of each Source you can also export as many sources and mappings as you like by clicking the export icon which exports as a zip file containing Json for the mappings you can also import an ontology assuming you have a Json for it as well the visual families page is where you can manage edit and create visual amilies tied to families with descriptions you can import export copy edit and add new visual families as well group number six environments group number six focuses on environments and the numerous settings available to configure the first page is networks here you can Define organizational networks in the system you can add networks here according to the name and CDR format and assign priorities and environments within your organization domains is the next page and similar to network you can Define organizational domains the settings here are simpler allowing you to add a domain name and the associated environment you want to map it to next up is the custom lists page you can Define lists of special interest for example hosts holding confidential information or users who have come under suspicion these custom lists can be selected as triggers for playbooks you can also choose to download a template with sample data use this format to add lists categories and then import the information back in this can be useful if you are a new customer and want to import existing information into the platform email templates is next and where you can Define email templates to use in email actions manual or by playbooks adding a new template brings up a menu where you can select the name content and Associated environments for the template email HTML templates is similar to email templates but is different only in that you can provide custom h HTML code in the content of the email template block lists is the next section here you can create a block list of items these are composed of entities that the system will not group alerts by or entities which should not be displayed in the system you only need to include the entity identifier type and environment the SLA page is a powerful feature as it allows you to set slas for resolving cases and alerts according to specific SLA triggers these can be config figured when you add a new SLA and consist of the SLA type period time unit time to critical period time to critical period unit and lastly environment the final section of the environments group is requests you can Define requests for end users to select in the homepage screen the requests can be handled either manually by an analyst or by using a Playbook thereby automating the request process and turning the platform into an internal ticketing system between different teams such as it to the sock or from an mssp to an end user each request enters the platform as a case with the label request on it to clearly Define it examples of such requests can be anything from blocking malicious IPS to optimizing Sim rules and even onboarding a new user group number seven ingestion the final group is ingestion again this section is specifically for ingestion with soar so it only includes connectors and web hooks we covered these more in depth in this video series under the ingestion section so we will only briefly review the first page is the connectors page where you can create manage and edit new connectors that feed data into your sore tenant following this section there is the web hooks page here you can create web hooks to thirdparty applications and configure them to push alerts from your organization into the platform we covered so much to review the settings page is now unified into one widget but separated by sore and Sim there are an abundant amount of options available to you so you can configure your tenant the way you want until next time where we will do a Chronicle SE Ops fundamentals and deep dive [Music] retrospective

## Overview

Overview of the course

### Video - [Retrospective](https://www.cloudskillsboost.google/course_templates/972/video/486848)

* [YouTube: Retrospective](https://www.youtube.com/watch?v=_9Q-jrUAN2A)

[Music] video number nine Chronicle SE Ops fundamentals and deep dive retrospective congratulations if you are watching this video that means you've gone through extensive fundamentals and deep dive training this video is intended to look back and strengthen our learnings first we will do a look back into the SEC Ops fundamentals course and then review material of our more advanced chronical SE Ops deep dive course this exercise will help you confidently go into your chronical work understanding the big picture video number one introduced us to the unified security operations platform where we set some important foundations we learned how the chronical security operations platform is a game changer built to analyze massive amounts of data enabling quicker threat detection and response we learned about the newly launched integrated experience between Sim and SAR we we learned how that integration is presented and what the high level architecture is in terms of providing a unified interface and applied threat intelligence we also got a quick tour of the new navigation that encompasses Sim and sore capabilities all in one place stepping through each one of the navigation options video number two is where we started getting a foundation by looking at case management and investigating a case that included a suspicious Excel macro connecting to a suspicious domain this video started a three-part miniseries of the investigation of that case and the various features you can utilize case management represents the single pain of glass that our analysts will use to look at operationally relevant data and alerts here are the top takeaways using Chronicle for investigation Chronicle transforms into your investigative Command Center here you'll navigate through high severity alerts and deep dive into case analysis starting your day in the cases page tailored to your role you can start from a case summary dive into into alerts understand the context and utilize playbooks for efficient investigation it's about connecting the dots quickly and accurately customizing case view customization is key with Chronicle you can filter cases to fit your needs customize widgets views and design your investigative workbench we also discussed the idea of how cases correspond with alerts and how different alerts can group together in one case the importance of entities we introduced the idea of entities and entity graph how that modeling helps us be more effective in understanding threats and applying response duet AI we showed the new AI summary widget that can supercharge your investigations in our third video we took a deeper dive into alert View and discussed how Chronicle deals with alerts and actions some key takeaways include number one the alert investigation process can be streamlined when investigating an alert alert you can have a number of visuals and possible actions to take which can visually map and crystallize the process number two alert views are highly customizable the system can adapt to show the information you need at the right time for the right person and can update as new actions get executed number three integrating external enrichment and intelligence is easy in this video we showcase how insights from virus total or mandiant are Incorporated in order to assist our analyst with a decision video number four concluded our three-part miniseries digging into the case page and showcased some Advanced capabilities here are top takeaways number one alerts can consist of one or more events that are correlated and each event can be mapped into the entity model to further enhance investigation and response number two playbooks are much more than just scripts they're sophisticated tools for chaining actions enrichments and decisions together for unified case management they automate repetitive tasks provide decision-making guidance and ensure every step of the investigation is recorded and actionable number three the alert graph the alert graph is your road map to understanding complex relationships within alerts use it to Pivot seamlessly to the SIM for a more granular investigation uncovering deeper insights up until video number five we spent all the time within the sore capability much like a junior analyst would in video number five we pivoted into the Sim cap ability with deeper search and investigation capabilities all within the unified platform some of the things you will learn here is number one how to conduct a udm search we introduce the unified data model and showcase some ability to search through the data and craft your first searches number two how to customize a search we dive deeper into the filtering pivoting summarization and search customization capability number three productivity tips what are some ways to rapidly create share and discover searches to make your investigation process smarter and faster in video number six we introduced the detection capability these are some of the things you went over number one how to manage detections in chronicle access rule definitions manage alerts and understand the impact of each detection to harden your security operations number two how to create rules creating rules is straightforward utilize Yar L to Define custom rules and monitor their performance through the rules dashboard keeping your security posture adaptive and resilient number three introduction to the Yara L syntax we provide a brief foundational introduction on the Yar L syntax using the rule that we explored in detail in previous videos in video number seven we look into curated detections and how you may go about building your first rule topics there include number one curated detections we dove into curated detections a repository of pre-made rules crafted by our experts these rules Target specific threats providing a solid foundation for your security strategy number two Rule enablement and tuning customizing these rules is simple yet powerful you can enable broad or precise rules and review them based on miter attack mapping categories and tune them through rule exclusions number three AI assisted rule creation who says your first rule needs to be handwritten we showcase a way to use the duet AI capabilities to describe your security concern and let Chronicles AI craft a rule for you in video number eight we introduced data visualization and dashboards this included an overview of the following topics number one default dashboards what are some of the default reports and dashboards that Chronicle comes out of the box with number two operating with dashboards you can import export email schedule delivery and do a number of other functions and you have access to a personal and shared custom dashboard number three building your first dashboard we went through the process of creating a first dashboard with a sample visualization exploring the data model and essential functions within this video number nine is where we switched gears to response and Automation and started building a Playbook some highlights include number one Integrations how to download Integrations from the marketplace and automatically inherit a number of outof the-box tools we also discussed how to configure an integration number two building your first Playbook with an integration with mandiant threat intelligence this included configuring The Playbook chaining actions together and configuring input parameters and modeling entities as a part of that number three testing A playbook using the powerful simulator capabilities you can test your playbook as you are developing it in video number 10 we conclude our Playbook development by building a simple view with some predefined widgets and wrap up with some topics around dashboarding reporting and additional resources number one custom View views custom views can include a variety of predefined built-in or custom widgets that gear the experience of the analyst for the task at hand number two dashboarding and Reporting SAR includes a lot of capabilities to build dashboards and reports on critical metrics number three additional resources useful reference guides and materials for a new chronical SE Ops user to continue the journey after which we covered the second course Chronicle SEC Ops Deep dive which further deepened our understanding in video number one we went through the platform architecture number one how exactly do Sim and SAR operate together what are some details and key capabilities of each component number two how does the data flow through the platform and What stages does it go through number three and how does it all work within the bounds of gcp video number two is where you will find a deep dive into ingestion this includes number one ingestion from on Prem sources through through forwarders and remote agents and how you can go about deploying them number two ingestion from cloud sources through feeds and connectors and the various flavors of that number three other forms of ingestion that may be supported to assist you in bringing your data into the platform video number three is where we dive deeper into normalization we go into specifics of the following number one the unified data model what it is how it is structured and how data gets normalized number two parsers we show showcase the parser management capabilities and build our very first parser extension number three entity mapping we showcase how to map entities inside of SAR video number four covers the enrichment capabilities how does chronicle enrich with things that are not in the logs here you will find out number one the various types of enrichments that exist and how enrichment Works number two what is the entity graph and how different sources can add context to the graph and how you may build rules to take advantage of that number three how to add enrichments inside of sore and get more contextual information video number five is where we dive deeper into the Yar L syntax here you can expect a lot of detailed information about number one anatomy of a Yar L rule what are the different components of a Yar L Rule and the various sections number two building a single event rule how to build your first single event rule along with some practical examples and syntax exploration number three building a multi-event rule how does the system deal with grouping correlation and time windowing in the context of multi-event rules video number six begins a two-part minseries to prepare us for building our first integration here topics include number one overview of the chronical Marketplace what are the different types of Integrations and how to install them number two anatomy of an integration a walkthrough of a sample Microsoft integration including setting it up and testing it three building a hello World integration in this video we go through a simple example of building an integration from scratch with python video number seven concludes this minseries with additional Advanced features including number one enhancing our hello world integration with more data including data pulled through the API or SDK number two discussing the documentation for our apis and sdks and examples of how you may use them number three building our first HTML and JavaScript based View video number eight walks us through the settings section of sim and sore and covers number one sim settings where you can configure rback feeds parsers forwarders and integrate your environment with gcp number two sore settings where you handle users environments permissions and branding of your personal environment additionally we explored the different settings which allow for more customization while handling cases and playbooks you did it you made it to the end that was a lot of material to review good luck on your continued journey in exploring Chronicle and transforming your security operations through Google's intelligent Cloud native [Music] technology

## QUIZ

Skills assessment

### Quiz - [Assessment](https://www.cloudskillsboost.google/course_templates/972/quizzes/486849)

#### Quiz 1.

> [!important]
> **What of the following statements is NOT true?**
>
> * [ ] Every Integration has a Connector
> * [ ] Integrations can be found in the Marketplace
> * [ ] Integrations can be configured for each environment
> * [ ] Every Action is associated with an Integration

#### Quiz 2.

> [!important]
> **Fill in the blank: We can use the ___ action to return if our configuration is successful.**
>
> * [ ] Ping
> * [ ] Callback
> * [ ] Send
> * [ ] Test

#### Quiz 3.

> [!important]
> **Which of the following is NOT one of the three mandatory sections required for writing rules?**
>
> * [ ] meta
> * [ ] condition
> * [ ] output
> * [ ] events

#### Quiz 4.

> [!important]
> **What feature allows you to connect to your on premise system to provide information for actions, connectors, etc?**
>
> * [ ] Remote Agent
> * [ ] Connector Feed
> * [ ] Streaming Unifier
> * [ ] Remote Link

#### Quiz 5.

> [!important]
> **In what section would we find the "author" of a rule?**
>
> * [ ] match
> * [ ] condition
> * [ ] events
> * [ ] meta

#### Quiz 6.

> [!important]
> **Which of the following is NOT a method for ingesting logs using Cloud Storage?**
>
> * [ ] Amazon S3
> * [ ] Google Cloud Storage
> * [ ] IBM Cloud Object Storage
> * [ ] Microsoft Azure Blob

#### Quiz 7.

> [!important]
> **What does CBN stand for in the context of custom parsers?**
>
> * [ ] Config Based Normalization
> * [ ] Config Binary Normalization
> * [ ] Context Binary Normalization
> * [ ] Context Based Normalization

#### Quiz 8.

> [!important]
> **What section of SOAR settings would you go to if you wanted to see when your License will expire?**
>
> * [ ] License Management
> * [ ] Tenant Management
> * [ ] Console
> * [ ] SOAR Configuration

#### Quiz 9.

> [!important]
> **Fill in the blank: A rule with only this condition: "Condition: $selection1" is an example for a ___ event rule.**
>
> * [ ] Single
> * [ ] Double
> * [ ] Conditional
> * [ ] Multi

#### Quiz 10.

> [!important]
> **What character(s) in this REGEX snippet is used to handle variability in spaces, file extensions, or other unpredictability within the values that we are seraching for in our criteria? "$selection1.target.process.command_line = /reg.*query HKLM \/f password \/t REG_SZ \/s/"**
>
> * [ ] \/
> * [ ] $
> * [ ] /s/
> * [ ] .*

#### Quiz 11.

> [!important]
> **Fill in the Blank: The rules engine in Chronicle uses a language called ___ to build detections.**
>
> * [ ] YARA-L
> * [ ] JSON-L
> * [ ] SQL-Detect
> * [ ] UDM

#### Quiz 12.

> [!important]
> **Which of the following is NOT a method of ingestion using Chronicle SIEM?**
>
> * [ ] Cloud Storage
> * [ ] Third-party API
> * [ ] Websockets
> * [ ] Forwarder

#### Quiz 13.

> [!important]
> **Fill in the blank: The ___ provides a container where you can configure instances that can be used in all environments, or you can add different instances for each environment.**
>
> * [ ] Required Instances
> * [ ] Community Instances
> * [ ] Organizational Instances
> * [ ] Shared Instances

#### Quiz 14.

> [!important]
> **Which of the following components is NOT something provided by an integration using SOAR?**
>
> * [ ] Actions
> * [ ] Connectors
> * [ ] Predefined Widgets
> * [ ] Parsers

#### Quiz 15.

> [!important]
> **Fill in the blank: ___ are a repository of pre-made rules crafted by our experts.**
>
> * [ ] Default Detections
> * [ ] Primary Detections
> * [ ] OOTB Detections
> * [ ] Curated Detections

#### Quiz 16.

> [!important]
> **How do you separate multiple syslog collectors on the same foward when ingesting data?**
>
> * [ ] By proxy
> * [ ] By file path
> * [ ] By storage device
> * [ ] By port

#### Quiz 17.

> [!important]
> **Fill in the blank: Besides custom actions, we can also build custom ___ to display information and widgets for cases and alerts.**
>
> * [ ] References
> * [ ] Displays
> * [ ] Dashboards
> * [ ] Views

#### Quiz 18.

> [!important]
> **What section of SOAR settings would you go to if you wanted to customize your company logo displayed?**
>
> * [ ] Marketing
> * [ ] Rebranding
> * [ ] Header and Logo
> * [ ] Visuals

#### Quiz 19.

> [!important]
> **Fill in the blank: ___ are tools used to further build upon custom integrations.**
>
> * [ ] API and UDM
> * [ ] Enrichments and Fields
> * [ ] API and SDK
> * [ ] Mapper and SDK

#### Quiz 20.

> [!important]
> **What section of SIEM settings would you go to to find your Customer ID?**
>
> * [ ] Feeds
> * [ ] Users
> * [ ] Profile
> * [ ] Parsers

#### Quiz 21.

> [!important]
> **Fill in the blanks: UDM has a ___-like structure and loosely follows a ___ model.**
>
> * [ ] Leaf, Conversational
> * [ ] Column, Paragraph
> * [ ] Web, Subject
> * [ ] Tree, Sentence

#### Quiz 22.

> [!important]
> **When using Chronicle SIEM, what classification of information are things like "Employee ID" or "Department?**
>
> * [ ] Unified
> * [ ] Statistical
> * [ ] Organizational
> * [ ] Contextual

#### Quiz 23.

> [!important]
> **What of the following applied threat intelligence platforms is not used with Google Chronicle?**
>
> * [ ] VirusTotal
> * [ ] Mandiant
> * [ ] Google Endpoint Detection and Response
> * [ ] Google Cloud Threat Intelligence Team

#### Quiz 24.

> [!important]
> **Fill in the Blank: "Is Suspicious" is an example of a boolean flag for ___.**
>
> * [ ] Context Enrichments
> * [ ] Field Enrichments
> * [ ] Data Enrichments
> * [ ] Entity Enrichments

#### Quiz 25.

> [!important]
> **Fill in the blank: Chronicle SIEM was designed for ___.**
>
> * [ ] Analytics
> * [ ] Playbooks
> * [ ] Enrichment
> * [ ] DevOps

#### Quiz 26.

> [!important]
> **What section of SOAR would we navigate to if we wanted to build a custom action?**
>
> * [ ] Playbooks
> * [ ] Cases
> * [ ] IDE
> * [ ] Settings

#### Quiz 27.

> [!important]
> **What feature would you use if you wanted to amend a default parser?**
>
> * [ ] Parser Configurer
> * [ ] Parser Wrapper
> * [ ] Parser Extension
> * [ ] Parser Handler

#### Quiz 28.

> [!important]
> **What data model is reserved for context or state, not events?**
>
> * [ ] Contextual Data Model
> * [ ] State Data Model
> * [ ] Entity Data Model
> * [ ] Unified Data Model

#### Quiz 29.

> [!important]
> **What can we use to constrain the amount of data that is being looked at to determine if a rule should fire or not?**
>
> * [ ] Time Parameter
> * [ ] Event Window
> * [ ] Time Window
> * [ ] Conditional Match

#### Quiz 30.

> [!important]
> **Which of the following is NOT a type of Integration in the Marketplace?**
>
> * [ ] Custom Integrations
> * [ ] Community Integrations
> * [ ] Chronicle Integrations
> * [ ] Mandatory Integrations

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
