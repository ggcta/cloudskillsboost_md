---
id: 17
name: 'Production Machine Learning Systems'
type: Course
url: https://www.cloudskillsboost.google/course_templates/17
date_published: 2024-09-02
topics:
  - Machine Learning Models
  - Machine Learning
  - Machine Learning Model Training
---

# [Production Machine Learning Systems](https://www.cloudskillsboost.google/course_templates/17)

**Description:**

This course covers how to implement the various flavors of production ML systems— static, dynamic, and continuous training; static and dynamic inference; and batch and online processing. You delve into TensorFlow abstraction levels, the various options for doing distributed training, and how to write distributed training models with custom estimators.

This is the second course of the Advanced Machine Learning on Google Cloud series. After completing this course, enroll in the Image Understanding with TensorFlow on Google Cloud course.

**Objectives:**

* Compare static versus dynamic training and inference
* Manage model dependencies
* Set up distributed training for fault tolerance, replication, and more
* Export models for portability

## Introduction to Advanced Machine Learning on Google Cloud

This module previews the topics covered in the course and how to use Qwiklabs to complete each of your labs using Google Cloud.

### Video - [Advanced Machine Learning on Google Cloud](https://www.cloudskillsboost.google/course_templates/17/video/504859)

* [YouTube: Advanced Machine Learning on Google Cloud](https://www.youtube.com/watch?v=e5qXbD6h6j0)

Welcome to the Advanced Machine Learning on Google Cloud specialization. In this specialization, you will learn how to apply machine learning at scale and how to build specialized machine-learning models for images, sequences, and recommendations. The first course is on building production machine learning systems. We look at what to consider when building production machine learning models and provide an overview of static training, dynamic training, static inference, dynamic inference, and distributed training using TensorFlow 2.0, which uses the Keras API. The second course is all about building image models. You will learn about convolutional neural networks and build image classification models of various types. The third course is on building sequence models. Sequence models are used to address a variety of applications, including financial time series prediction, speech recognition, music generation, sentiment classification, and machine translation. We’ll focus on models used in natural language problems, like text classification and translation. Finally, we end the specialization with building real-world recommendation systems. This brings together all the concepts that were covered in both the previous specialization and in this one.

### Video - [Welcome](https://www.cloudskillsboost.google/course_templates/17/video/504860)

* [YouTube: Welcome](https://www.youtube.com/watch?v=4WC0Rd3lbI4)

Welcome to Production ML Systems, the first course in the Advanced Machine Learning on Google Cloud specialization. This course focuses on building production machine learning models and the considerations behind them. We'll be covering what ML architectures are composed of and the why and how of making good systems design decisions. Real-world production ML systems are large ecosystems, of which the model code is just a small part. The rest consists of code that performs critical functions, like data extraction, feature engineering, monitoring, and a serving infrastructure. This course is devoted to exploring the characteristics that make for a robust ML system beyond its ability to make good predictions. In the first module, Architecting Production ML Systems, we’ll explore what an ML system should be able to do and the components that take responsibility for those actions. In module two, Designing Adaptable ML Systems, you’ll see how change can affect an ML system and what can be done to mitigate those effects. In module three, Designing High-performance ML Systems, we’ll explore how to optimize the performance of an ML system by choosing the right hardware and removing bottlenecks. And finally, in module four, Building Hybrid ML Systems, you’ll learn about the technology behind hybrid systems that allows you to run your workloads on the cloud, on the edge using mobile devices, or on-premises.

## Architecting Production ML Systems

This module explores what else a production ML system needs to do and how to meet those needs. You review how to make important, high-level, design decisions around training and model serving need to make in order to get the right performance profile for your model.

### Video - [Architecting ML systems](https://www.cloudskillsboost.google/course_templates/17/video/504861)

* [YouTube: Architecting ML systems](https://www.youtube.com/watch?v=36U-DGvmQtE)

Person: Welcome to "Architecting ML Systems," the second module of the Production Machine Learning Systems course. In this module, we'll explore what makes up an architecture and why and how to make good systems design decisions. Let me ask you a question. You'll recall from earlier in this specialization, we showed how time is distributed among the different tasks necessary to launch an ML model, and surprisingly, modeling accounted for far less than most people expect. The same is true with respect to the code. So the answer is that ML model code typically accounts for about 5% of the overall code base. ML models account for such a small percentage because keeping a system running in production requires many more actions than just computing the model's outputs for a given set of inputs. In this module, you'll see what else a production ML system needs to do and how you can meet those needs. Upon completing this module, you should know how to choose an appropriate training and serving paradigm, serve ML models scalably, and design an architecture from scratch. And although our focus is on Google Cloud, it's important that you always try to reuse generic systems-- many of which are open-source frameworks-- when possible. What's true of software frameworks like TensorFlow, Spark, or Apache Beam is also true of the underlying infrastructure on which you execute them. Instead of spending time and effort provisioning infrastructure, you can use manage services such as Dataproc, AI Platform, or Dataflow to execute your Spark, TensorFlow, and Beam code.

### Video - [Data extraction, analysis, and preparation](https://www.cloudskillsboost.google/course_templates/17/video/504862)

* [YouTube: Data extraction, analysis, and preparation](https://www.youtube.com/watch?v=6Ao8rjt2Png)

person: After you define the business use case and establish the success criteria, the process of delivering an ML model to production typically involves several steps, which can be completed manually or by an automated pipeline. The first three steps deal with data. Data must be ingested, which means it's extracted from a raw data source. With data extraction, you retrieve the data from various sources. Those sources can be streaming, in real time, or batch. For example, you might extract data from a customer relationship management system, or CRM, to analyze customer behavior. This data might be structured where the file is in a CSV, TXT, JSON, or XML format. Or you might have an unstructured data source with images of your products or text comments from chat sessions with your customers. You might have to extract streaming data from your company's transportation vehicles that are equipped with sensors transmitting data in real time. If the data you want to train your model on or get predictions for is structured, you might retrieve it from a data warehouse, such as BigQuery, or you can use Apache Beam's IO module. In this data flow example, we're loading data from BigQuery, calling predict on every record, and then writing the results back into BigQuery. In data analysis, you analyze the data you've extracted. For example, you can use exploratory data analysis, or EDA. This involves using graphics and basic sample statistics to explore your data, such as looking for outliers or anomalies, trends, and data distributions. This step helps you identify those features that can aid in increasing the predictive power of your machine learning model. The way changes in the distribution of your data could affect your model might not be apparent, so let's consider a scenario. In this scenario, an upstream data source encodes a categorical feature using a number, such as a product number. One day, the product number and convention changes, and now the customer uses a totally different mapping with some old numbers and some new numbers. How would you know that this had happened? How would you debug your ML model? The output of your model would tell you whether there's a drop in performance, but it won't tell you why. The raw inputs themselves would appear valid because you're still getting numbers. In order to recognize this change, you would need to look at changes in the distribution of your inputs. In doing so, you might find that earlier, the most commonly occurring value was four. In the new distribution, four might never occur, and the most commonly occurring value might be ten. Depending on how you implemented your feature columns, these new values might be mapped to one component of a one-hot encoded vector or to many components. If, for example, you used a categorical column with a hash bucket, the new values would be distributed according to the hash function. And so one hash bucket might now get more and different values than before. If you used a vocabulary, the new values would map to OOV buckets. But what's important is that for a given tensor, its relationship to the label before and its relationship to the label now are probably very different. So after you've extracted and analyzed your data, the next step in the process is data preparation. Data preparation includes data transformation and feature engineering, which is the process of changing or converting the format, structure, or values of data you've extracted into another format or structure. Most ML models require categorical data to be in a numerical format, but some models work either with numerical or categorical features, while others can handle mixed type features. For example, here are three types of preprocessing for dates using SQL in BigQuery ML: Where we are extracting the parts of the date into different columns, year, month, day, etc., extracting the time period between the current date and columns in terms of years, months, days, etc., and extracting some specific features from the date, name of the weekday, weekend or not, holiday or not, etc. Now, here is an example of the day of week and hour of day queries extracted using SQL and visualized as a table in Data Studio. Please note that for all non-numeric columns other than timestamp, BigQuery ML performs a one-hot encoding transformation. This transformation generates a separate feature for each unique value in the column.

### Video - [Model training, evaluation, and validation](https://www.cloudskillsboost.google/course_templates/17/video/504863)

* [YouTube: Model training, evaluation, and validation](https://www.youtube.com/watch?v=lK0OtgibhAY)

- The next step in the workflow is choosing a model that you'll train. In model training, you implement different machine learning algorithms with the prepared data to train various ML models. Essentially, model training is the process of feeding an ML algorithm with data to help it identify and learn good values for the feature set. So you use your data to incrementally improve your model's predictive ability. Model evaluation aims to estimate the generalization accuracy of a model on future, unseen, or out-of-sample data. Although training a model is a key step in the pipeline, how the model generalizes on unseen data is an equally important aspect that should be considered in every machine-learning pipeline. This means that we need to know whether the model actually works and, consequently, whether we can trust its predictions. Could the model be merely memorizing the data it's fed with, and therefore unable to make good predictions on future samples or samples that it hasn't seen before, such as the test set? Model evaluation consists of a person or a group of people evaluating or assessing the model with respect to some business-relevant metric like AUC, area under the curve, or cost weighted error. If the model meets their criteria, it is moved from the assessment phase to development. For example, in the development phase, you may want to modify hyperparameter values to increase the model's performance, which correlates to improvements in evaluation results. After development, the model is then ready for a live experiment or real-world test of the model. In contrast to the model evaluation component, which is performed by humans, the model validation component evaluates the model against fixed thresholds and alerts engineers when things go wrong. One common test is to look at performance by slice. Let's say, for example, business stakeholders care strongly about a particular geographic market region. An alert can be set to notify engineers when the accuracy by country begins to skew downward. The model evaluation and validation components have one responsibility: to ensure that the models are good before moving them into a production environment.

### Video - [Trained model, prediction service, and performance monitoring](https://www.cloudskillsboost.google/course_templates/17/video/504864)

* [YouTube: Trained model, prediction service, and performance monitoring](https://www.youtube.com/watch?v=SZgy7j0CBd0)

person: The output of model validation is a trained model that can be pushed to the model registry. The machine learning model registry is a centralized tracking system that stores linage, versioning, and related metadata for published machine learning models. A registry may capture governance data required for auditing purposes, such as who trained and published a model, which datasets were used for training, the values of metrics measuring predictive performance, and when the model was deployed to production. It's a place to find, publish, and use ML models or model pipeline components. Machine learning uses data to answer questions. So prediction, or inference, is the step where we get to answer the questions we posed, whether they are about a business problem or an academic research problem. The trained model is served as a prediction service to production. It's important to note that the process is concerned only with deploying the trained model as a prediction service, for example, a microservice with a REST API, instead of deploying the entire ML system. For example, Google's AI Platform Prediction service has an API for serving predictions from machine learning models. In this particular example, AI Platform Prediction retrieves the trained model and saves it as a PKL in Cloud Storage. PKL is the standard method of serializing objects in Python. Trained models deployed in AI Platform Prediction service are exposed as REST endpoints that can be invoked from any standard client that supports HTTP, such as JupyterLab Notebook. The AI Platform Prediction service can host models trained in popular machine learning frameworks, including TensorFlow, XGBoost, and Scikit-Learn. As a best practice, you need a way to actively monitor the quality of your model in production. Monitoring lets you detect model performance degradation or model staleness. The output of monitoring for these changes then feeds into the data analysis component, which can serve as a trigger to execute the pipeline or to execute a new experimental cycle. For example, monitoring should be designed to detect data skews, which occur when your model training data is not representative of the live data. That is, the data that you used to train the model in the research or production environment does not represent the data that you actually have in your live system, and this leads to model staleness. To understand other performance metrics, you can configure Google's Cloud monitoring to monitor your model's traffic patterns, error rates, latency, and resource utilization. This can help spot problems with your models and find the right machine type to optimize latency and cost.

### Video - [Training design decisions](https://www.cloudskillsboost.google/course_templates/17/video/504865)

* [YouTube: Training design decisions](https://www.youtube.com/watch?v=hWsCLoWMdRE)

person: One of the key decisions you'll need to make about your production ML system concerns training. Here's a question. How is physics unlike fashion? If we assume that science is about discovering relationships that already exist in the world, the answer is that physics is constant whereas fashion isn't. To see some proof, just look at some old pictures of yourself. Now you might be asking, why is this relevant? Well, when making decisions about training, you have to decide whether the phenomenon you're modeling is more like physics or like fashion. When training your model, there are two paradigms: static training and dynamic training. In static training, we do what we did in the last specialization. We gather our data, we partition it, we train our model, and then we deploy it. In dynamic training, we do this repeatedly as more data arrives. This leads to the fundamental trade-off between static and dynamic. Static is simpler to build and test, but will probably become stale. Whereas dynamic is harder to build and test, but will adapt to changes. And the tendency to become or not become stale is what we alluded to earlier when we contrasted physics and fashion. If the relationship you're trying to model is constant, like physics, a statically trained model may be sufficient. If, in contrast, the relationship you're trying to model is one that changes, like fashion, the dynamically trained model might be more appropriate. Part of the reason the dynamic is harder to build and test is that new data may have all sorts of bugs in it. And that's something we'll talk about more deeply in a later module on designing adaptable ML systems. Engineering might also be harder, because we need more monitoring, model rollback, and data quarantine capabilities. Let's explore some use cases and think about which sort of training style would be most appropriate. The first use case concerns spam detection. And the question you should ask yourself is, how fresh does spam detection need to be? You could do this as static, but spammers are a crafty and determined bunch. They will probably discover ways of passing whatever filter you impose within a short time. So dynamic is likely to be more effective over time. What about Android voice to text? Note that this question has some subtlety. For a global model, training off-line is probably fine. But if you want to personalize the voice recognition, you may need to do something online, or at least different, on the phone. So this could be static or dynamic, depending on whether you want global or personalized transcription. What about ad conversion rate? The interesting subtlety here is that conversions may come in very late. For example, if I'm shopping for a car online, I'm unlikely to buy for a very long time. This system could use dynamic training. Then regularly going back at different intervals to catch up on new conversion data that has arrived for the past. So in practice, most of the time you'll need to use dynamic, but you might start with static because it's simpler. In a reference architecture for static training, models are trained once and then pushed to AI platform. Now for dynamic training, there are three potential architectures to explore: Cloud Functions, App Engine, or Dataflow. In a general architecture for dynamic training using Cloud Functions, a new data file appears in Cloud Storage, and then the Cloud Function is launched. After that, the Cloud Function starts the AI Platform training job. And then the AI Platform writes out a new model. In a general architecture for dynamic training using App Engine, when a user makes a web request from a dashboard to App Engine, an AI Platform training job is launched, and the AI Platform job writes a new model to Cloud Storage. From there, the statistics of the training job are displayed to the user when the job is complete. The Dataflow pipeline is also possibly invoking the model for predictions. Here, the streaming topic is ingested into Pub/Sub from subscribers. Messages are then aggregated with Dataflow, and aggregated data is stored in BigQuery. AI Platform is launched on the arrival of new data in BigQuery, and then an updated model is deployed.

### Video - [Serving design decisions](https://www.cloudskillsboost.google/course_templates/17/video/504866)

* [YouTube: Serving design decisions](https://www.youtube.com/watch?v=GnchpIlslHw)

Person: Just as the use case determines appropriate training architecture, it also determines the appropriate serving architecture. In designing our serving architecture, one of our goals is to minimize average latency. Just as in operating systems where we don't want to be bottlenecked by slow disk IO, when serving models, we don't want to be bottlenecked by slow to decide models. Remarkably, the solution for serving models is very similar to what we do to optimize IO performance. We use a cache. In this case, instead of faster memory, we'll use a table. Static serving then computes the label ahead of time and serves by looking it up in the table. Dynamic serving in contrast computes the label on demand. There's a space-time trade-off. Static serving is space intensive, resulting in higher storage costs because we store precomputed predictions with a low, fixed latency and lower maintenance costs. Dynamic serving, however, is compute intensive. It has lower storage costs, higher maintenance, and variable latency. The choice of whether to use static or dynamic serving is determined by considering how important costs are with regard to latency, storage, and CPU. Sometimes it can be hard to express the relative important of these three areas. As a result, it might be helpful to consider static and dynamic serving through another lens, peakedness and cardinality. Peakedness in a data distribution is a degree to which data values are concentrated around the mean, or in this case, how concentrated the distribution of the prediction workload is. You can also think of it as inverse entropy. For example, a model that predicts the next word based on the current word, which you might find in your mobile phone keyboard app would be highly peaked because a small number of words account for the majority of words used. In contrast, a model that predicted quarterly revenue for all sales verticals in order to populate a report would be right on the same verticals. And with the same frequently for each. And so, it would be very flat. Cardinality refers to the number of values in a set. In this case, the set is composed of all the possible things we might have to make predictions for. So, a model predicting sales revenue given organization division number would have fairly low cardinality. A model predicting lifetime value given a user friendly e-commerce platform would have high cardinality because the number of users and the number of characters of each user are probably quite large. Taken together peakedness and cardinality create a space. When the cardinality is sufficiently low, we can store the entire expected prediction workload. For example, the predicted sales revenue for all divisions in a table and use static serving. When the cardinality is high because the size of the input space is large and the workload is not very peaked, you probably want to use dynamic training. In practice though, a hybrid of static and dynamic is often chosen, where you statically cache some of the predictions while responding on demand for the long tail. This works best when the distribution is sufficiently peaked. The striped area above the curve and not inside the blue rectangle is suitable for a hybrid solution, with the most frequently requested predictions cached and the tail computed on demand. Let's try to estimate training and inference needs for the same use cases that we saw in the previous lesson. The first use case is predicting whether an e-mail is spam. What inference style is needed? Well, first we need to consider how peaked the distribution is. The answer is not at all. Most e-mails are probably different, although they may be very similar if generated programmatically. Depending on the choice of representation, the cardinality might be enormous, so this would be dynamic. The second use case is Android voice-to-text. This is, again, subtle. Inference is almost certain on line because there's such a long tail of possible voice clips. But maybe with sufficient signal processing, some key phrases like, "Okay, Google," may have pre-computed answers. So, this would be dynamic or hybrid. And the third use case is shopping ad conversion rate. The set of all ads doesn't change much from day to day, assuming users are comfortable waiting for a short while after uploading their ads. This could be done statically. And then the back script could be run at regular intervals throughout the day. This would be static. In practice, you'll often use a hybrid approach. You might not have realized it, but dynamic serving is what we have learned so far. Think back to the architecture of the systems we've used to make predictions. A model that lived in AI Platform was sent one or more instances and returned predictions for each. If you want to build a static serving system, you want to make three design changes. First, you need to change your call to AI Platform from an online prediction job to a batch prediction job. Second, you need to make sure that your model accepted and passed through keys as input. These keys will allow you to join your request to prediction at serving time. And third, you write the predictions to a data warehouse like BigQuery, and create an API to read from it. Although the details for each of these instructions are beyond the scope of this lesson, we've provided links in the course resources on submitting a batch prediction job, enabling pass-through features in your model, and loading data into BigQuery.

### Video - [Designing from scratch](https://www.cloudskillsboost.google/course_templates/17/video/504867)

* [YouTube: Designing from scratch](https://www.youtube.com/watch?v=nYAiWZx8EOk)

person: In this section, we'll apply what we've learned to a new use case. Let's pretend the head of a municipal transit system has contacted you to build a system that predicts the traffic levels on roads. As part of your preparation for this task, you're trying to thoroughly understand the business constraints in order to make the appropriate system design tradeoffs. The data available consists of sensors deployed all over the city, which record whenever a car passes by. For each sensor, we know where it is. We also know the characteristics of the road that it's on. What sort of training architecture is appropriate? Well, you should ask yourself, what is the relationship between the features and the labels? Is it more like physics or fashion? In this case, it's more like fashion trends. Cities are very complex systems. If a train stops service, people will still need to get home. Technology's also always changing. On demand taxi services have reshaped urban transit in ways we didn't anticipate a decade ago. And there are also episodic changes, like sports events and parades, for example. For dynamic relationships, we need to use dynamic training. Which sort of serving architecture is appropriate? Well, you should ask yourself, is the distribution of prediction requests likely to be more peaked or less peaked? In this case, it's likely to be more peaked. The distribution of demand is peaked, because it's likely to be dominated by the request for the most heavily trafficked roads. Is the cardinality of the set of all prediction requests likely to be low, moderate, high, or perhaps need more info? In this case, you'll need more info. Why is that? What does it depend upon? Consider historical traffic data, problem framing, or the variance of traffic levels. In this case, the answers are both historical traffic data and the problem framing, but not the variance of traffic levels. The reason that the cardinality depends upon the framing of the problem is that we don't know whether the task is to make predictions for every minute, hour, or day. And similarly, we don't know how big a region of space each prediction should correspond to. It could be anything from a few feet to a few blocks. As we learned in the first specialization, machine learning is all about generalization-- the leap of faith into unseen input. And what we don't know is whether our users wanna generalize in space, i.e. by making predictions far away from the sensors, in time, by making predictions in the future with finer granularity than the historical data, or both. In all likelihood, you'd start conservatively, which corresponds to a lower, rather than a higher, cardinality. Variance of traffic levels wouldn't matter, because that's a label and not a feature.

### Video - [Using Vertex AI](https://www.cloudskillsboost.google/course_templates/17/video/504868)

* [YouTube: Using Vertex AI](https://www.youtube.com/watch?v=uU9FlOKKnKI)

As you’ve seen from previous videos, the machine learning ecosystem requires decision making at every stage. You need to determine how to handle and prepare data, and also how to design, build, evaluate, train, and monitor a model’s performance. Decisions around workflow processes, how to implement or execute those processes, and the management of the workflow itself are required to solve machine learning problems. One of the most interesting details about the ML ecosystem is that ML code accounts for only a small percentage of it. To keep a system running in production requires a lot more than just computing the model’s outputs for a given set of inputs. This means that each component of the ML ecosystem requires not only decisions and processes, but also people. According to the International Data Corporation, in 2020, a lack of staff with the right expertise, a lack of production-ready data, and a lack of an integrated development environment were reported as primary reasons that machine learning technologies fail. So, how do you ensure success for your machine learning or AI use case? And how can you or your team prepare and manage your data, build your models, deploy them into production, and then manage them? A solution is to use a unified platform that brings all the components of the machine learning ecosystem and workflow together. And in this case, that platform is Vertex AI. Vertex AI brings together the Google Cloud services for building ML under one unified  user interface and application programming interface, or API. With Vertex AI, you can access a dashboard, datasets, features, labeling tasks, notebooks, pipelines, training, experiments, models, endpoints, batch predictions, and metadata. Let’s take a closer look at the datasets, notebooks, training, and models sections of the Vertex AI navigation bar that help you to prepare your data and build and deploy your models. Vertex AI has a unified data preparation tool that supports image, tabular, text, and video content. Uploaded datasets are stored in a Cloud Storage bucket that acts as an input for both AutoML and custom training jobs. Let’s explore an example where you have sample source data from a BigQuery table about movies and their features. First, there is a movie_id column header that can map to an entity type called movie. For each movie entity, features include an average_rating, title, and genres. The values in each column map to specific instances of an entity type or features, which are called entities and feature values. The update_time column indicates when the feature values were generated. In the featurestore, the timestamps are an attribute of the feature values, not a separate resource type. If all feature values were generated at the same time, you don’t need to have a timestamp column. You can specify the timestamp as part of your ingestion request. When you use the data to train a model, Vertex AI examines the source data type and feature values and infers how it will use that feature in model training. This is called the  transformation for that feature. If needed, you can specify a different supported transformation for any feature. After you prepare your dataset, you can develop models using Notebooks. Notebooks is a managed service that offers an integrated and secure JupyterLab environment for data scientists and machine learning developers to experiment, develop, and deploy models into production. Notebooks enable you to create and manage virtual machine (VM) instances because they come pre-installed with the latest data science and machine learning frameworks. They also come with a pre-installed suite of deep learning packages, including support for the TensorFlow and PyTorch frameworks. Either can be configured for CPU-only or GPU-enabled instances. With regard to security, Notebooks instances are protected by Google Cloud authentication and authorization and are available using a Notebooks instance URL, which is part of the metadata of the VM. Now let’s shift our focus to training. With Vertex AI, you can train and compare models using AutoML or custom code training,  with all models stored in  one central model repository. Training pipelines are the primary model training workflow in Vertex AI, which can use training pipelines to create an AutoML-trained model or a custom-trained model. For custom-trained models, training pipelines orchestrate custom training jobs and hyperparameter tuning in conjunction with steps like adding a dataset or uploading the model to Vertex AI for prediction serving. Custom jobs specify how Vertex AI runs custom training code, including worker pools, machine types, and settings related to a Python training application and custom container. Alternatively, hyperparameter tuning searches for the best combination of hyperparameter values by optimizing metric values across a series of trials. Both custom jobs and hyperparameter tuning, however, are only used by custom-trained models. They are not used by AutoML models. Next up are models. Models are built from datasets or unmanaged data sources. Many different types of machine learning models are available with Vertex AI. The right choice will depend on the use case  and your level of experience  with machine learning. A new model can be trained, or an existing model can be imported. After the model has been imported into Vertex AI, it can be deployed to an endpoint and then used to request predictions. AutoML can be used to train a new model with minimal technical effort. It can be used to quickly prototype models  and explore new datasets before  investing in development. For example, you might use AutoML to determine the good features in a dataset. Generally speaking, custom training is used to create a training application optimized for a targeted outcome, because it allows for complete control over training application functionality. You can target any objective, use any algorithm, develop your own loss functions or metrics, or carry out any other customization. And finally, let’s explore Endpoints. Endpoints are machine learning models made available for online prediction requests. An endpoint is an HTTPS endpoint that clients can call to receive the inferencing (scoring) output of a trained model. They can provide timely predictions from many users, for example, in response to an application request. They can also request batch predictions if immediate results aren’t required. Multiple models can be deployed to an endpoint, and a single model can be deployed to multiple endpoints to split traffic. You might deploy a single model to multiple endpoints to test out a new model before serving it to all traffic. Either way, it’s important to emphasize that a model must be deployed to an endpoint before that model can be used to serve online predictions. To make that happen, you must define an endpoint in Vertex AI by giving it a name and location and deciding whether the access is Standard, which makes the endpoint available for prediction serving through a REST API. This has been a brief introduction to Vertex AI, Google Cloud’s unified ML platform. For more information, please see cloud.google.com/vertex-ai.

### Video - [Lab introduction: Structured data prediction](https://www.cloudskillsboost.google/course_templates/17/video/504869)

* [YouTube: Lab introduction: Structured data prediction](https://www.youtube.com/watch?v=BWs9E5LlLDc)

person: This lab provides hands-on practice using Google Cloud's AI platform. You'll start by creating a BigQuery dataset. Then export the data into a GCS bucket and train using Cloud AI Platform. And finally, deploy the trained model using Cloud AI platform.

### Lab - [Structured data prediction using Vertex AI Platform](https://www.cloudskillsboost.google/course_templates/17/labs/504870)

In this lab you train, evaluate, and deploy a machine learning model to predict a baby’s weight.  You then send requests to the model to make online predictions.  This lab is part of a series of labs on processing scientific data.

* [ ] [Structured data prediction using Vertex AI Platform](../labs/Structured-data-prediction-using-Vertex-AI-Platform.md)

### Quiz - [Quiz: Architecting production ML systems](https://www.cloudskillsboost.google/course_templates/17/quizzes/504871)

#### Quiz 1.

> [!important]
> **Which type of training do you use if your data set doesn't change over time?**
>
> * [ ] Dynamic training
> * [ ] Static training
> * [ ] Online training
> * [ ] Real-time training

#### Quiz 2.

> [!important]
> **Vertex AI has a unified data preparation tool that supports image, tabular, text, and video content.   Where are uploaded datasets stored in Vertex AI?**
>
> * [ ] A Google Cloud database that acts as an output for both AutoML and custom training jobs.
> * [ ] A Google Cloud Storage bucket that acts as an input for both AutoML and custom training jobs.
> * [ ] A Google Cloud Storage bucket that acts as an output for both AutoML, custom training jobs, serialized training jobs.
> * [ ] A Google Cloud database that acts as an input for both AutoML and custom training jobs.

#### Quiz 3.

> [!important]
> **Which type of logging should be enabled in the online prediction that logs the stderr and stdout streams from your prediction nodes to Cloud Logging and can be useful for debugging?**
>
> * [ ] Cloud logging
> * [ ] Request-response logging
> * [ ] Container logging
> * [ ] Access logging

#### Quiz 4.

> [!important]
> **Match the three types of data ingest with an appropriate source of training data.**
>
> * [ ] Streaming batch (Dataflow), structured batch (BigQuery), stochastic (App Engine)
> * [ ] Streaming (BigQuery), structured batch (Pub/Sub), unstructured batch (Cloud Storage)
> * [ ] Streaming (Pub/Sub), structured batch (BigQuery), unstructured batch (Cloud Storage)

#### Quiz 5.

> [!important]
> **When you use the data to train a model, Vertex AI examines the source data type and feature values and infers how it will use that feature in model training. This is called the ________________for that feature.**
>
> * [ ] Transformation
> * [ ] Transmutation
> * [ ] Translation
> * [ ] Duplication

#### Quiz 6.

> [!important]
> **What percent of system code does the ML model account for?**
>
> * [ ] 25%
> * [ ] 5%
> * [ ] 90%
> * [ ] 50%

#### Quiz 7.

> [!important]
> **In the featurestore, the timestamps are an attribute of the feature values, not a separate resource type.**
>
> * [ ] False
> * [ ] True

#### Quiz 8.

> [!important]
> **What is the responsibility of model evaluation and validation components?**
>
> * [ ] To ensure that the models are good before moving them into a production/staging environment.
> * [ ] To ensure that the models are good after moving them into a production/staging environment.
> * [ ] To ensure that the models are not good before moving them into a staging environment.
> * [ ] To ensure that the models are not good after moving them into a staging environment.

### Document - [Readings: Architecting production ML systems](https://www.cloudskillsboost.google/course_templates/17/documents/504872)

## Designing Adaptable ML Systems

In this module, you learn how to recognize the ways that our model is dependent on our data, make cost-conscious engineering decisions, know when to roll back our models to earlier versions, debug the causes of observed model behavior and implement a pipeline that is immune to one type of dependency.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/17/video/504873)

* [YouTube: Introduction](https://www.youtube.com/watch?v=Rr86vy4UTzk)

Welcome to Designing Adaptable ML systems. In this module, we'll explore how to: recognize the ways that a model is dependent on data, make cost-conscious engineering decisions, know when to roll back a model to an earlier version, debug the causes of observed model behavior, and implement a pipeline that is immune to one type of dependency. In the 16th century, John Donne famously wrote in one of his poems that no man is an island. He meant that human beings need to be part of a community to thrive. In software engineering terms, we would say that few software programs adopt a monolithic island-like design. Instead, most software today is modular, and depends on other software. Modular programs are more maintainable, as well as easier to reuse, test, and fix because they allow engineers to focus on small pieces of code rather than the entire program. Containers make it easier to manage modular programs. A container is an abstraction that packages applications and libraries together so that the applications can run on a greater variety of hardware and operating systems. This ultimately makes hosting large applications better. To learn more about Kubernetes, Google's open source container orchestration software, check out the Getting Started with Google Kubernetes Engine course. But what if there was no way to identify a specific version of a library, and you had to rely on finding similar libraries at run-time? Furthermore, what if someone else got to choose which version got run and they didn't know or really care about your program? There would be no way of knowing what the run-time behavior would look like. Unfortunately, this is precisely the case for machine learning, because the run-time instructions, for example, the model weights, depend on the data that the model was trained on. Additionally, similar data will yield similar instructions. And finally other people including other teams and our users create our data. Just like in traditional software engineering, mismanaged dependencies, say, code that assumes one set of instructions will be called when another end up being called instead, can be expensive. Your models’ accuracy might go down or become unstable. Sometimes, the errors are subtle and your team may end up spending a large proportion of its time debugging. The good news is that with a better understanding of how to manage data dependencies, many problems can be either detected quickly or circumvented entirely.

### Video - [Adapting to data](https://www.cloudskillsboost.google/course_templates/17/video/504874)

* [YouTube: Adapting to data](https://www.youtube.com/watch?v=GjU2U3qPqXY)

When it comes to adapting to change, consider which of these four is most likely to change? An upstream model, a data source maintained by another team, the relationship between features and labels, or the distributions of inputs. The answer is that all of them can, and often do, change. Let's see how this happens, and what to do about it with a couple example scenarios. Let’s say that you've created a model to predict demand for umbrellas that accepts as input an output from a more specialized weather prediction model. Unbeknownst to you and the owner, this model has been trained on the wrong years of data. Your model, however, is fit to the upstream model’s outputs. What could go wrong? One day, the model owners silently push a fix and the performance of your model, which expected the old model’s distribution of data, drops. The old data had below-average rainfall and now you’re under-predicting the days when you need an umbrella. Here’s another scenario. Let’s say your small data science team has convinced the web development team to let you ingest their traffic logs. Later, the web development team refactors their code and changes their logging format, but continues publishing the old format. At some point, they stop publishing in the old format but they forget to tell your team. Your model’s performance degrades after getting an unexpectedly high number of null features. To fix this problem, first, you should stop consuming data from a source that doesn't notify downstream consumers. Second, you should consider making a local version of the upstream model and keeping it updated. Sometimes, the set of features that the model has been trained on include many that were added indiscriminately, which may worsen performance at times. For example, under pressure during a sprint, your team decided to include a number of new features without understanding their relationship to the label. One of them is causal, while the others are merely correlated with the causal one. The model can't distinguish between the two, and takes both into account equally. Months later, the correlated feature becomes decorrelated with the label and is thus no longer predictive. The model’s performance suffers. To address this, features should always be scrutinized before being added, and all features should be subjected to leave-one-out evaluations to assess their importance.

### Video - [Changing distributions](https://www.cloudskillsboost.google/course_templates/17/video/504875)

* [YouTube: Changing distributions](https://www.youtube.com/watch?v=a-6wWJWxB9k)

Earlier you saw how, in the context of ingesting an upstream model, our model’s performance would degrade if it expected one input but ingested another. The statistical term for changes in the likelihood of observed values like model inputs is changes in the distribution. And it turns out that the distribution of the data can change for all sorts of reasons. For example, sometimes the distribution of the label changes. We've looked at the natality dataset in BigQuery and tried to predict baby weight. Baby weight has actually changed over time. It peaked in the 1980s and has since been declining. In 1969, babies weighed significantly less than they did in 1984. When the distribution of the label changes, it could mean that the relationship between features and labels is changing as well. At the very least, it's likely that our model's predictions, which will typically match the distribution of the labels in the training set, will be significantly less accurate. However, sometimes it's not the labels, but the features, that change their distribution. For example, say you've trained your model to predict population movement patterns using postal code as a feature. Surprisingly, postal codes aren't fixed. Every year, governments release new ones and deprecate old ones. Now as a ML practitioner, you know that postal codes aren't really numbers. So you've chosen to represent them as categorical feature columns, but this might lead to problems. If you chose to specify a vocabulary, but set the number of out of vocab buckets to 0, and didn’t specify a default, then the distribution may become skewed toward the default value, which is -1. And this might be problematic because the model may be forced to make predictions in regions of the feature space which were not well represented in the training data. There's another name for when models are asked to make predictions on points in feature space that are far away from the training data, and that’s extrapolation. Extrapolation means to generalize outside the bounds of what we’ve previously seen. Interpolation is the opposite. It means to generalize within the bounds of what we’ve previously seen. Interpolation is always much easier. For example, let’s say that the model got to see the yellow data and not the gray data. The blue line reflects a linear regression on the yellow data. Predictions in the yellow region are interpolated and reasonably accurate. In contrast, predictions in the gray region are extrapolated and are increasingly inaccurate the farther we get from the yellow region. You can protect yourself from changing distributions using a few different methods. The first thing you can do is be vigilant through monitoring. You can look at the descriptive summaries of your inputs and compare them to what the model has seen. If, for example, the mean or the variance has changed substantially, then you can analyze this new segment of the input space, to see if the relationships learned still hold. You can also look to see whether the model’s residuals, that is the difference between its predictions and the labels, has changed as a function of your inputs. If, for example, you used to have small errors at one slice of the input and large in another, and now it’s switched, this could be evidence of a change in the relationship. Finally, if you have reason to believe that the relationship is changing over time, you can force the model to treat more recent observations as more important by writing a custom loss function, or by retraining the model on the most recent data.

### Video - [Lab: Adapting to data](https://www.cloudskillsboost.google/course_templates/17/video/504876)

* [YouTube: Lab: Adapting to data](https://www.youtube.com/watch?v=gUMFSpx_Ba8)

When leading a team of engineers, many decisions are informed by technical debt and other sorts of cost-benefit analyses. The best teams get very high rates of return on their investments. With that in mind, let’s consider a few scenarios. Let’s imagine that you’re the leader of a team of engineers and you are nearing the end of a code sprint. One of the team’s goals for the sprint is to increase performance on the model by 5%. Currently, however, the best performing model is only marginally better than what was around before. One of the engineers acknowledges this but still insists that it’s worth spending time doing an extensive ablation analysis, where the value of an individual feature is computed by comparing it to a model trained without it. What might this engineer be concerned about? The engineer might be concerned about legacy and bundled features. Legacy features are older features that were added because they were valuable at the time. But since then, better features have been added, which have made them redundant without our knowledge. Bundled features on the other hand, are features that were added as part of a bundle, which collectively are valuable but individually may not be. Both of these features represent additional unnecessary data dependencies. In another scenario, another engineer has found a new data source that is very much related to the label. The problem is that it’s in a unique format and there’s no parser written in Python, which is what the codebase is composed of. Thankfully, there is a parser on the web but it’s closed source and  written in a different language. The engineer is thinking  about the model performance. Something in the back of your mind seems wrong. What is it? It's the smell. No, really! There's a concept called code smell and it applies in ML as well. In this case, you might be thinking, "I wonder what introducing code that we can't inspect and are unable to easily modify into our testing in production frameworks will do."

### Video - [Right and wrong decisions](https://www.cloudskillsboost.google/course_templates/17/video/504877)

* [YouTube: Right and wrong decisions](https://www.youtube.com/watch?v=vQfPD7MrLNc)

Some decisions about data are a matter of weighing cost versus benefit, like short-term performance goals against long-term maintainability. Others, though, are about right and wrong. For example, let’s say that you’ve trained a model to predict “probability a patient has cancer” from medical records and that you’ve selected patient age, gender, prior medical conditions, hospital name, vital signs, and test results as features. Your model had excellent performance on held-out test data but performed terribly on new patients. Any guesses as to why? It turns out the model was trained using a feature that wasn’t legitimately available at decision time, and so, when the model was deployed into production, the distribution of this feature changed and it was no longer a reliable predictor. In this case, that feature was ‘hospital name’. You might think, hospital name, how could that be predictive? Well, remember that there are some hospitals that focus on diseases like cancer. So, the model learned that ‘hospital name’ was very important. However, at decision time, this feature wasn’t available to the model, because patients hadn't yet been assigned to a hospital, but rather than throwing an error, the model simply interpreted the hospital name as an empty string, which it was still capable of handling thanks to out-of-vocabulary buckets in its representations of words. We refer to this idea where the label is somehow leaking into the training data as data leakage. Data leakage is related to a broader class of problems we’ve seen before in the last specialization, where we talked about models learning unacceptable strategies. Previously, we learned that when there’s class imbalances, a model might learn to predict the majority class. In this case, the model has learned to use a feature that wouldn’t actually be known and which cannot be plausibly causally related to the label. Here’s a similar case. A professor of 18th century literature believed that there was a relationship between how an author thought about the mind and their political affiliation. So, for example, perhaps authors who used language like “the mind is a garden” had one political affiliation and authors who used language like “the mind is a steel trap” another. What if we were to naively test this hypothesis with machine learning? Some people tried that and they got some unexpected results. Here’s what they did. They took all of the sentences in all of the works by a number of 18th century authors, extracted just the mind metaphors and set those as their features and then used the political affiliations of the authors who wrote them as labels. Then, they randomly assigned sentences to each of the training, validation, and test sets. And because they divided the data in this way, some sentences from each author were distributed to each of those three sets. And the resulting model was amazing! But suspiciously amazing. What might have gone wrong? One way to think about it is that political affiliation is linked to that person. And if we wouldn't include ‘person name’ in the feature set, we should not include it implicitly either. When the researchers changed the way they partition the data and instead partitioned it by author instead of by sentence, the model's accuracy dropped to something more reasonable.

### Video - [System failure](https://www.cloudskillsboost.google/course_templates/17/video/504878)

* [YouTube: System failure](https://www.youtube.com/watch?v=KBA_mTUUyfc)

Here’s another slightly different scenario. You've trained a product recommendation model based on users’ click and purchase behavior on your ecommerce site. On Black Friday, your server responsible for transactions and payments goes down whilst the web server remains up and running, so the model thinks that no one who clicks is buying anything. It's impossible to have models unlearn things that have already been learned, but one thing you can do is roll back the model's state to a time prior to the data pollution. Of course, in order to do this, you will need infrastructure that automatically creates and saves models as well as their meta information. Here’s another scenario. You’ve trained a static product recommendation model which alone will determine which products users see when they are on the home page and when they are viewing individual products. The model works by using purchasing behavior of other users. After deploying it, user session time and conversion rate initially increase. But, in the months that follow the release of the model, conversion rate and user session time steadily decline to slightly below the levels they were at before the launch of the model. What went wrong? Well, your model is not updating to new users, new products, and new patterns in user preference. Because the model only knows about your older products, it continues to recommend them long after they’ve fallen out of favor. Ultimately, users simply ignored the recommendations altogether, and made do with the site’s search functionality. This “cold start” problem is common for this sort of recommendation model. We’ll talk more about recommendation systems later in the course. The solution here is to dynamically retrain your model on newer data, and also to understand the limits of your model. Here’s one other scenario. You’ve deployed a statically-trained fraud detection model and its performance starts off good but quickly degrades. What’s gone wrong here? In adversarial environments, where one party is trying to beat another, it’s particularly important to dynamically retrain the model, to keep up with the most recent strategies.

### Video - [Concept drift](https://www.cloudskillsboost.google/course_templates/17/video/504879)

* [YouTube: Concept drift](https://www.youtube.com/watch?v=PHdM4hsi4K0)

Why do machine learning models lose their predictive power over time? You’ll recall that machine learning models, such as neural networks, accept a feature vector and provide a prediction for our target variable. These models learn in a  supervised fashion where a set of feature vectors with  expected output is provided. The traditional supervised learning assumes that the training and the application data come from the same distribution. You’ll also remember that  traditional machine learning algorithms were developed  with certain assumptions. The first is that instances  are generated at random according to some probability distribution D. The second is that instances are  independent and identically distributed. And the third assumption is that D is stationary with fixed distributions. Drift is the change in an entity with respect to a baseline. In the case of production ML models, this is the change between the real-time production data and a baseline data set, likely the training set, that is representative of the task the model is intended to perform. If your model were running  in a static environment, using static or stationary data, for example data whose statistical properties do not change, then model drift wouldn’t occur and your model would not lose any of its predictive power because the data you’re predicting comes from the same distribution as the data used for training. But production data can diverge or drift from the baseline data over time due  to changes in the real world. There are several types of drift in ML models. Data Drift or change in probability of X P(X) is a shift in the model’s input data distribution. For example, incomes of all applicants increase by 5%, but the economic fundamentals are the same. Concept drift, or change in  probability of Y given X, is a shift in the actual relationship between the model inputs and the output. An example of concept drift is when macroeconomic factors make lending riskier, and there is a higher standard  to be eligible for a loan. In this case, an income level that was earlier considered creditworthy is no longer creditworthy. Prediction drift, or change in the predicted value of Y given X, is a shift in the model’s predictions. For example, a larger proportion of credit-worthy applications when your product was launched in a more affluent area. Your model still holds, but your business may be  unprepared for this scenario. Label drift or change in the predicted value of Y as your target variable is a shift in the model’s  output or label distribution. Data drift, feature drift, population, or covariate shift are all names to describe changes in the data distribution of the inputs. When data shift occurs, or when you observe that the model performs worse on unknown data regions, that means that the input data has changed. The distribution of the variables is meaningfully different. As a result, the trained model is not relevant for this new data. It would still perform well on the data that is similar to the “old” one. The model is fine on the “old data”, but in practical terms, it became dramatically less useful since we are now dealing with a new feature space. Indeed, the relationships between the model inputs and outputs have changed. In contrast, concept drift occurs when there is a change in the relationship between the input feature and the label, or target. Let’s explore two examples of concept drift which highlight the change in the relationship between the input feature and the label. In this first example, stationary supervised learning, historical data is used to make predictions. You might recall that in supervised learning, a model is trained from historical data and that data is used to make predictions. This second example is supervised learning under concept drift, where a new, secondary data source is ingested to provide both historical data and new data to make predictions. This new data could be in batch or real time. Whatever the form, it’s important to know that  the statistical properties of the target variable may change over time. As a result, an interpretation of the data changes with time, while the general distribution  of the feature input may not. This illustrates concept drift, where the statistical properties  of the class variable, the target we want to predict, changes over time. In this supervised learning  classification example, when the distribution of the label changes, it could mean that the relationship between features and labels is changing as well. At the very least, it’s likely that our model’s predictions, which will typically match  the distribution of the labels on the data on which it was trained, will be significantly less accurate. Let’s take a look at one other example. This one deals with streaming data. Data flows continuously over  time in dynamic environments, particularly for streaming data, such as e-commerce, user modeling, spam emails, fraud detection, and intrusion. Changes in underlying data occur due to changing personal interests, changes in population, or adversary activities, or they can be attributed to a  complex nature of the environment. In this example, a sensor’s measurement can drift due to a fault with the sensor or aging, changes in operation  conditions or control command, and machine degradation as a result of wearing. In these cases, the distribution of the feature inputs and the labels or targets may change, which will impact model performance  and lead to model drift. There is no guarantee that future data will follow similar distributions of past  data in a stream setting. Accordingly, concept drift  at time t can be defined as the change of joint  probability of X and y at time t. The joint probability can be decomposed into two parts as the probability of X times  the probability of y given X . Let’s be a little more careful here with the definition of concept drift. Let’s use X to denote a feature vector and y to denote its corresponding label. Of course, when doing supervised learning, our goal is to understand the  relationship between X and y. We will define a concept as a description of the distribution of our observations. More precisely, you can think of this as a joint probability distribution of our observations. However, this concept could depend on time! Otherwise concept drift  would be a non-issue, right? We’ll use the notation  probability of X and y at time t when we want to consider the probability of X and y at a specific time. Now it’s easy to give a more rigorous description of concept drift. Simply put, concept drift occurs when the distribution of our observations shifts over time, or that the joint probability distribution we mentioned before changes. We can break down this distribution into two parts using properties of joint distributions. First we have the distribution  of the feature space, probability of X, and then what we can think of as a description of our decision boundary, the probability of y given X. If the drift is occurring  for the decision boundary, then we call this decision boundary drift. Likewise, if the drift is  occurring for the feature space, we call this feature space drift. Of course, both can be happening at the same time, and this can make it complicated to understand where the changes are happening! OK, we’ve veered off in a little bit more of a technical direction here, so what’s the point we’re trying to make? Concept drift can occur due to shifts in the feature space and /  or the decision boundary, so we need to be aware of these during production. If the data is changing, or if the relationship between the features and the label is changing, this is going to cause issues with our model. There are different types of concept drift. Let’s wrap up this video by taking  a quick look at four of them: In sudden drift, a new concept occurs within a short time. In gradual drift, a new concept gradually replaces an old one over a period of time. In incremental drift, an old concept incrementally changes to a new concept over a period of time. And in recurring concepts, an old concept may reoccur after some time.

### Video - [Actions to mitigate concept drift](https://www.cloudskillsboost.google/course_templates/17/video/504880)

* [YouTube: Actions to mitigate concept drift](https://www.youtube.com/watch?v=YS1jFHpixfQ)

As previously mentioned, both data drift and concept drift lead to model drift. Examples of data drift can include concepts of the change in the spamming behavior to try to fool the model, a rule update in the app, in other words, a change in the limit of user messages per minute, selection bias, and non-stationary environment, training data for a given season that has no power to generalize to another season. eCommerce apps are good examples of potential concept drift due to their reliance on personalization, for example, the fact that people’s preferences ultimately do change over time. Sensors may also be subject to concept drift due to the nature of the data they collect and how it may change over time. Movie recommendations, again similar to eCommerce apps, rely on user preferences and they may change. Demand forecasting heavily relies on time, and as we have seen, time is a major contributor to potential concept drift. So, what if you diagnose data drift? If you diagnose data drift, enough of the data needs to be labeled to introduce new classes and the model retrained. What if you diagnose concept drift? If you diagnose concept drift, the old data needs to be relabeled and the model retrained. Also, for concept drift, you can design your systems to detect changes. Periodically updating your static model with more recent historical data, for example, is a common way to mitigate concept drift. You can either discard the static model completely or you can use the existing state as the starting point for a better model to update your model by using a sample of the most recent historical data. You can also use an ensemble approach to train your new model in order to correct the predictions from prior models. The prior knowledge learnt from the old concept is used to improve the learning of the new concept. Ensembles which learnt the old concept with high diversity are trained by using low diversity on the new concept. Remember, concept drift is the change in relationships between the model inputs and the model output. After your diagnosis and mitigation efforts, retraining or refreshing the model over time will help to maintain model quality. As the world changes, your data may change. The change can be gradual, sudden, and seasonal. These changes will impact model performance. Thus, machine learning models can be expected to degrade or decay. Sometimes, the performance drop is due to low data quality, broken data pipelines, or technical bugs.

### Video - [TensorFlow data validation](https://www.cloudskillsboost.google/course_templates/17/video/504881)

* [YouTube: TensorFlow data validation](https://www.youtube.com/watch?v=2V-F7iG3pwk)

There are three phases in a pipeline. Data is ingested and validated, a model is trained and analyzed, and the model is then deployed in production. In this video, we’ll provide an overview of TensorFlow Data Validation, which is part of the ingest and validate data phase. To learn more about the train and analyze the model phase, or how to deploy a model in production, please check out our ML Ops Course. TensorFlow Data Validation is a library for analyzing and validating machine learning data. Two common use-cases of TensorFlow Data Validation within a TensorFlow Extended pipelines are validation of continuously arriving data and training-serving skew detection. The pipeline begins with the ExampleGen component. This component takes raw data as input and generates TensorFlow examples, it can take many input formats, for example CSV, TF Record. It also splits the examples for you into Train/Eval. It then passes the result to the StatisticsGen component. This brings us to the three main components of TensorFlow Data Validation. The Statistics Generation component, which generates statistics for feature analysis. The Schema Generation component, which gives you a description of your data. And the Example Validator component, which allows you to check for anomalies We’ll explore those three components in more depth in the next video, but first let’s look at our use cases for TensorFlow Data Validation so that we can understand how these components work. There are many reasons and use cases where you may need to analyze and transform your data. For example, when you’re missing data, such as features with empty values, or when you have labels treated as features, so that your model gets to peek at the right answer during training. You may also have features with values outside the range you expect or other data anomalies. To engineer more effective feature sets, you should identify: Especially informative features, redundant features, features that vary so widely in scale that they may slow learning, and features with little or no unique predictive information. One use case for TensorFlow Data Validation is to validate continuously arriving data. Let’s say on day one you generate statistics based on data from day one. Then, you generate statistics based on day two data. From there, you can validate day two statistics against day one statistics and generate a validation report. You can do the same for day three, validating day three statistics against statistics from both day two and day one. TensorFlow Data Validation can also be used to detect distribution skew between training and serving data. Training-serving skew occurs when training data is generated differently from how the data used to request predictions is generated. But what causes distribution skew? Possible causes might come from a change in how data is handled in training vs in production, or even a faulty sampling mechanism that only chooses a subsample of the serving data to train on. For example, if you use an average value, and for training purposes you average over 10 days, but when you request prediction, you average over the last month. In general, any difference between how you generate your training data and your serving data, the data you use to generate predictions, should be reviewed to prevent training-serving skew. Training-serving skew can also occur based on your data distribution in your training, validation, and testing data splits. To summarize, distribution skew occurs when the distribution of feature values for training data is significantly different from serving data and one of the key causes for distribution skew is how data is handled or changed in training vs production.

### Video - [Components of TensorFlow data validation](https://www.cloudskillsboost.google/course_templates/17/video/504882)

* [YouTube: Components of TensorFlow data validation](https://www.youtube.com/watch?v=no0qLRColnw)

TensorFlow Data Validation is a library for analyzing and validating  machine learning data, for which there are three components: The Statistics Generation component, the Schema Generation component, and the Example Validator component. The StatisticsGen component generates features statistics and random samples over training data, which can be used for visualization and validation. It requires minimal configuration. For example, StatisticsGen takes as input a dataset ingested using ExampleGen. After StatisticsGen finishes running, you can visualize the outputted statistics. In an example using taxi data, the StatisticsGen component has generated data statistics for the numeric features. For the trip_start_hour feature, it appears there is not that much data in the early morning hours. It appears that the trip_start_hour column, where the time window is between 2:00am to 6:00am, has data missing. This helps determine the area we need to focus on to fix any data-related problems. We’ll need to get more data, otherwise, the prediction for 4:00am data will be overgeneralized. Let’s look at another example, this time using a consumer spending score dataset. Here we are generating statistics for both a training dataset, that may have arrived on day one, and a new dataset, that may have arrived on day two. These statistics are being generated from a Pandas DataFrame. You can also generate statistics from a CSV file or a TF.Record formatted file. By comparing both datasets, you can analyze how big of a difference there is between the two, and then determine if that difference matters. StatisticsGen generates both numeric and categorical features. In this example, our dataset has two numerical features, Work_Experience and Family_Size. Our dataset also has three categorical features: Graduated, Profession, and Spending_Score. Notice that for our categorical features, in addition to seeing the  number of missing values, we also see the number of unique values. TensorFlow Data Validation can also help you identify unbalanced data distributions. For example, if you have a dataset you are using for a classification problem and you see that one feature has a lower percentage of values than the other, you can use TensorFlow Data Validation to detect this “unbalance”. The most unbalanced features will be listed at the top of each feature-type list. For example, the following screenshot shows Work_Experience with zero has a data value, which means that 31.54% of the values in this feature are zeroes. There are a number of  StatisticsGen data validation checks that you should be aware of. These include: Feature min, max, mean, mode, and median, feature correlations, class imbalance, check to see missing values, and histograms of features, for both numerical and categorical. The SchemaGen TFX pipeline component can specify data types for feature values, whether a feature has to  be present in all examples, allowed value ranges, and other properties. A SchemaGen pipeline component will automatically generate a schema by inferring types, categories, and ranges  from the training data. In essence, SchemaGen is looking at  the data type of the input, is it an int, float, categorical, etc. And if it is categorical then  what are the valid values? It also comes with a visualization tool to review the inferred schema and fix any issues. In this example visualization, "type" indicates the feature datatype, "presence" indicates whether the feature must be present in 100% of examples or not, so whether it’s required or optional. "Valency" indicates the number of values required per training example. "Domain" and "Values" indicates the feature domain and its values. In the case of categorical features, single indicates that each training example must have exactly one category for the feature. The ExampleValidator pipeline component identifies anomalies in training and serving data. It can detect different classes of anomalies in the data and emit validation results. The ExampleValidator pipeline component identifies any anomalies in the example data by comparing data statistics computed by the StatisticsGen pipeline component against a schema. It takes the inputs and looks for problems in the data, like missing values, and reports any anomalies. As we’ve explored, TensorFlow Data Validation is a component of TensorFlow Extended, and it helps you to analyze and validate your data. Data validation checks include identifying feature correlations, checking for missing values, and identifying class imbalances.

### Video - [Lab Introduction: Introduction to TensorFlow Data Validation](https://www.cloudskillsboost.google/course_templates/17/video/504883)

* [YouTube: Lab Introduction: Introduction to TensorFlow Data Validation](https://www.youtube.com/watch?v=GTbByKuCMz0)

This lab provides a hands-on introduction to TensorFlow Data Validation. You’ll begin by reviewing the different TensorFlow Data Validation methods, then continue on to generate statistics, visualize statistics, infer a schema, and, finally, update a schema.

### Lab - [Introduction to TensorFlow Data Validation](https://www.cloudskillsboost.google/course_templates/17/labs/504884)

This lab is in introduction to TensorFlow Data Validation (TFDV), a key component of TensorFlow Extended. This lab serves as a foundation for understanding the features of TFDV and how it can help you understand, validate, and monitor your data.

* [ ] [Introduction to TensorFlow Data Validation](../labs/Introduction-to-TensorFlow-Data-Validation.md)

### Video - [Lab Introduction: Advanced Visualizations with TensorFlow Data Validation](https://www.cloudskillsboost.google/course_templates/17/video/504885)

* [YouTube: Lab Introduction: Advanced Visualizations with TensorFlow Data Validation](https://www.youtube.com/watch?v=stXBTBcxfMI)

This lab demonstrates how TensorFlow Data Validation can be used to investigate and visualize a dataset. You’ll begin with steps to install TensorFlow Data Validation, then continue on to compute and visualize statistics, infer a schema, check evaluation data for errors, check for and fix evaluation anomalies, check for drift and skew, and finally, freeze a schema.

### Lab - [Advanced Visualizations with TensorFlow Data Validation](https://www.cloudskillsboost.google/course_templates/17/labs/504886)

This lab illustrates how TensorFlow Data Validation (TFDV) can be used to investigate and visualize your dataset. That includes looking at descriptive statistics, inferring a schema, checking for and fixing anomalies, and checking for drift and skew in our dataset. It is important to understand your datasets characteristics, including how it might change over time in your production pipeline. It is also important to look for anomalies in your data, and to compare your training, evaluation, and serving datasets to make sure that they are consistent.

* [ ] [Advanced Visualizations with TensorFlow Data Validation](../labs/Advanced-Visualizations-with-TensorFlow-Data-Validation.md)

### Video - [Mitigating training-serving skew through design](https://www.cloudskillsboost.google/course_templates/17/video/504887)

* [YouTube: Mitigating training-serving skew through design](https://www.youtube.com/watch?v=a_BgeuMOx00)

We’ve talked about training-serving skew a number of times in previous videos, but always at a high level. Let’s look at it now in a little more detail. Training-serving skew refers to differences in performance that occur as a function of differences in environment. Specifically, training-serving skew refers to differences caused by one of three things: A discrepancy between how you handle data in the training and serving pipelines. A change in the data between when you train and when you serve, or a feedback loop between your model and your algorithm. Up until now, we’ve focused on the data  aspect of training-serving skew but it’s also possible to have inconsistencies that arise after the data have been introduced. Say, for example, that in  your development environment, you have version 2 of a library, but in production you have version 1. The libraries may be functionally equivalent but version 2 is highly  optimized and version 1 isn’t. Consequently, predictions might be significantly slower or consume more memory in production than they did in development. Alternatively, it’s possible that version 1 and version 2 are functionally different, perhaps because of a bug. Finally, it’s also possible that different code is used in production versus development, perhaps because of recognition of one of the other issues, but though the intent was  to create equivalent code, the results were imperfect.

### Lab - [Vertex AI: Training and Serving a Custom Model](https://www.cloudskillsboost.google/course_templates/17/labs/504888)

In this lab, you will use Vertex AI to train and serve a TensorFlow model using code in a custom container.

* [ ] [Vertex AI: Training and Serving a Custom Model](../labs/Vertex-AI-Training-and-Serving-a-Custom-Model.md)

### Video - [Diagnosing a production model](https://www.cloudskillsboost.google/course_templates/17/video/504889)

* [YouTube: Diagnosing a production model](https://www.youtube.com/watch?v=-FVz8Q7gnu0)

In this section, we’ll be putting our learnings into practice by diagnosing a production model. Let’s say you’ve architected an ML system to predict the demand for widgets. Your streaming purchase orders arrive in Pub/Sub and are fulfilled asynchronously, but let’s ignore fulfillment and focus on demand prediction. Dataflow processes this stream and using windowing functions, aggregates purchase orders over time. It then uses the output of  these windowing functions and passes that to an ML model that lives in Cloud AI Platform, where the data are joined against historical purchase data that lives in a data warehouse like BigQuery. The model returns a predicted demand for a particular time. The predictions returned by the model are both written to storage and sent  to the purchasing system. The purchasing system determines what’s in the inventory, and the inventory is also logged in the data warehouse. The model is retrained daily, based on actual inventory, sales, and other signals. The model is deployed to Cloud AI Platform for training, and then the model can then be invoked on any new or updated data within the Dataflow. One day, your product manager has the idea to add a credit rating to each purchase order. You’re the head of the machine learning engineering team. Do you think it should be added? You get an email from the head of the business unit saying that he’s just noticed that sales are down significantly. The warehouse manager, who is copied on the email, says that inventory storage costs are also down significantly. The room is suddenly getting quite warm. What could have happened here? This is a great example of a feedback loop! What might have happened was that the model started under-predicting demand, perhaps because of some corrupted historical data or an error in the pipeline. Once demand started to go down, product turnover started to creep up. If this problem went unnoticed for a while, the model might have learned to keep zero inventory all the time! In addition to being a great reminder that humans need to stay in the loop, it’s also a reminder that we are often optimizing for something other than what we ultimately care about. In this case, we were optimizing for matching predicted demand when what we cared about was minimizing carrying costs in order to maximize profits. Here’s another scenario. One of your salespeople just shared some amazing news. By leveraging their contacts at one of Megacorp’s many regional divisions, they signed Megacorp to a five-year deal and it’s the biggest contract yet! Great, you think, not realizing that this could have implications for your model’s performance. How can these be related? It all depends on how the sales orders come in, and how independent the divisions actually are. If the divisions are entirely dependent, because there’s actually  just one purchasing decision, split up by division, and these orders come in separately, your model may still treat  these orders as independent, in which case it would look much more compelling as evidence of an uptick in demand. The solution here would be to add some aggregation by company ID in your pipeline before computing other statistics. Okay, let’s look at one last scenario. Your warehouse manager emails you and tells you that the warehouse flooded and they’ve had to scrap a  large portion of the inventory. They’ve ordered replacements from purchasing, but it will be four days before those arrive and unfulfilled orders in the  meantime will have to wait. You realize that you have the  skills to address this problem. What do you do? You stop your automatic model deployment process. The reason you do so is because data collected during this period will be contaminated. Since the products will show  as out of stock on the website, customer orders will be low.

### Quiz - [Quiz: Designing adaptable ML systems](https://www.cloudskillsboost.google/course_templates/17/quizzes/504890)

#### Quiz 1.

> [!important]
> **Which of the following models are susceptible to a feedback loop? Check all that apply.**
>
> * [ ] A book-recommendation model that suggests novels its users may like based on their popularity (i.e., the number of times the books have been purchased).
> * [ ] A housing-value model that predicts house prices, using size (area in square meters), number of bedrooms, and geographic location as features.
> * [ ] A university-ranking model that rates schools in part by their selectivity (the percentage of students who applied that were admitted).
> * [ ] An election-results model that forecasts the winner of a mayoral race by surveying 2% of voters after the polls have closed.
> * [ ] A traffic-forecasting model that predicts congestion at highway exits near the beach, using beach crowd size as one of its features.
> * [ ] A face-attributes model that detects whether a person is smiling in a photo, which is regularly trained on a database of stock photography that is automatically updated monthly.

#### Quiz 2.

> [!important]
> **Gradual drift is used for which of the following?**
>
> * [ ] A new concept that occurs within a short time
> * [ ] An old concept that incrementally changes to a new concept over a period of time
> * [ ] A new concept that rapidly replaces an old one over a short period of time
> * [ ] An old concept that may reoccur after some time

#### Quiz 3.

> [!important]
> **Suppose you are building an ML-based system to predict the likelihood that a customer will leave a positive review. The user interface that customers leave reviews on changed a few months ago, but you don't know about this. Which of these is a potential consequence of mismanaging this data dependency?**
>
> * [ ] Change in model serving signature
> * [ ] Losses in prediction quality
> * [ ] Change in ability of model to be part of a streaming ingest

#### Quiz 4.

> [!important]
> **What is training skew caused by?**
>
> * [ ] The prediction environment is slower than the training environment.
> * [ ] The Cloud Storage you load your data from in the training environment  is physically closer than the Cloud Storage you load your data from in the production environment.
> * [ ] Your development and production environments are different, or different code is used in the training environment  than in the development environment.
> * [ ] Starting and stopping of the processing when training the model.

#### Quiz 5.

> [!important]
> **Which component identifies anomalies in training and serving data and can automatically create a schema by examining the data?**
>
> * [ ] Data identifier
> * [ ] Data validation
> * [ ] Data transform
> * [ ] Data ingestion

#### Quiz 6.

> [!important]
> **Which of the following tools help software users manage dependency issues?**
>
> * [ ] Maven, Gradle, and Pip
> * [ ] Monolithic programs
> * [ ] Modular programs
> * [ ] Polylithic programs

#### Quiz 7.

> [!important]
> **What is the shift in the actual relationship between the model inputs and the output called?**
>
> * [ ] Concept drift
> * [ ] Prediction drift
> * [ ] Data drift
> * [ ] Label drift

### Document - [Readings: Designing adaptable ML systems](https://www.cloudskillsboost.google/course_templates/17/documents/504891)

## Designing High-Performance ML Systems

In this module, you identify performance considerations for machine learning models.



Machine learning models are not all identical. For some models, you focus on improving I/O performance, and on others, you focus on squeezing out more computational speed.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/17/video/504892)

* [YouTube: Introduction](https://www.youtube.com/watch?v=6aSCPLL0B3I)

Laurence: Hi, I'm Laurence, and I'm a developer advocate on Google Brain, focused on TensorFlow. In this course, you're gonna be learning the considerations behind architecting and implementing production machine learning systems. Now one key consideration of this, of course, is performance. In this module, you'll learn how to identify performance considerations for machine learning models. Now machine learning models are not all identical. For some models, you'll be focused on improving IO performance, and on others, you'll be focused on squeezing out more computational speed. Depending on what your focus is, you will need different ML infrastructure. Whether you decide to scale out with multiple machines, or scale up on a single machine with a GPU or TPU. Sometimes you might even need to do both by using a machine with multiple accelerators attached to it. Now it's not just a hardware choice. The hardware you select will also inform your choice of a distribution strategy.

### Video - [Training](https://www.cloudskillsboost.google/course_templates/17/video/504893)

* [YouTube: Training](https://www.youtube.com/watch?v=rlhb-SHGdp8)

Person: We will start by talking about what "high performance" means in this context, and providing a high-level overview of distributed training architectures. So what does "high-performance machine learning" mean to you? Does it mean powerful? The ability to handle large data sets? Or maybe does it mean doing it as fast as possible? The ability to train for long periods of time? Achieving the best possible accuracy? There's so many things. But one key aspect is the time taken to train a model. If it takes six hours to train a model on some hardware or software architecture, but only three hours to train the same model to the same accuracy on a different hardware/software architecture, I think we'll all agree that the second architecture is twice as performant as the first one. Now, notice that I said "train the model to the same accuracy." Throughout this module, we will assume that we're talking of models that have the same accuracy, or RMSE, or whatever your evaluation measure is. Obviously, when we talk about high-performance machine learning models, accuracy is important. We aren't just going to consider that in this module. The rest of the courses in this specialization will look at how to build more accurate ML models. And there, we'll be looking at model architectures that will help us get to a desired accuracy. Here in this course, we'll look solely at infrastructure performance. Besides the time to train, there is one other aspect-- budget. You often have a training budget. You might be able to train faster on better hardware, but that hardware might cost more, so you may have to make the explicit choice to train on slightly slower infrastructure. So when it comes to you training budget, you have three considerations-- three levers that you can adjust. These are time, cost, and scale. Now, how long are you willing to spend on the model training? This might be driven by the business use case. If you're training a model every day so as to recommend products the next day, then your training has to finish within 24 hours. Realistically, you'll also need time to deploy, to AB test, and all that. So your actual budget might be only 18 hours. So then, how much are you willing to spend on model training in terms of computing costs? This, too, is a business decision. You don't want to train for 18 hours every day if the incremental benefit of this is not sufficient. Scale is another aspect of your budget. Models differ in terms of how computationally expensive they are. Even keeping to the same model, you have a choice of how much data you're going to train on. Generally, the more data, the more accurate the model. But there are diminishing returns to larger and larger data sizes, so your time and cost budget may also dictate the data set size. Similarly, you often have a choice between training on a single, more expensive machine, or multiple cheaper machines. But to take advantage of this, you may have to write your code somewhat differently, and that's another aspect of scale. Also, you have the choice of starting from an earlier model checkpoint and training for just a few steps. Typically, this will converge faster that training from scratch each time. This compromise might allow you to reach the desired accuracy faster and cheaper. In addition, there are ways to tune performance to reduce the time, reduce the cost, or increase the scale. In order to understand what these are, it helps to understand that model training performance will be bound by one of three things: Input/output, which is how fast you can get data into the model for each training step, the CPU, which is how fast you can compute the gradient in each training step, and memory-- how many weights can you hold in memory so that you can do the matrix multiplications in memory? Or do you use the GPU or TPU? Your ML training will be IO-bound if the number of inputs is large, heterogenous, requiring parsing, or if the model is so small that the compute requirements are trivial. This also tends to be the case if the input data is on a storage system with very low throughput. Your ML training will be CPU-bound if the IO is simple, but the model involves lots of expensive computations. You will also encounter this situation if you're running a model on underpowered hardware. Your ML training might be memory-bound if the number of inputs is really large or if the model is complex and has lots of free parameters. You'll also face memory limitations if your accelerator doesn't have enough memory. So knowing what you're bound by, you can look at how to improve performance. If you're IO-bound, look at storing the data more efficiently on a storage system with higher throughput, or parallelizing the reads. Although it's not ideal, you might also consider reducing the batch size so that you're reading less data in each step. If you are CPU-bound, see if you can run the training on a faster accelerator. GPUs keep getting faster, so move to a newer generation processor. And if you're using Google Cloud, you also have the option of running on TPUs. Even if it's not ideal, you might consider using a simpler model, a less computationally expensive activation function or simply just train for fewer steps. If you are memory-bound, see if you can add more memory to the individual workers. Again, this may not be ideal, but you could also consider using fewer layers in your model. Reducing the batch size can also help with memory-bound ML systems.

### Video - [Predictions](https://www.cloudskillsboost.google/course_templates/17/video/504894)

* [YouTube: Predictions](https://www.youtube.com/watch?v=PaKvzTWOJ68)

person: We've talked about the time to train, but there is another aspect to performance: predictions. During inference, you'll have performance considerations as well. If you're doing batch prediction, the considerations are very similar to that of training. You're concerned with things such as time. How long does it take for you to do all of your predictions? And this might be driven by a business need as well. So, for example, if you're doing product recommendations for the next day, you might want recommendations for the top 20% of users precomputed and available in, say, five hours if it takes 18 hours to do the full training. You'll also want to consider cost. "What predictions are you doing, and how much do you precompute" is going to be driven by cost considerations. And then there's scale. Do you have to do all of this on a single machine, or can you distribute it, say, to multiple workers? What kind of hardware is available on these workers? Do they, for example, have GPUs? If you are doing online prediction, the performance considerations are quite different. This is because the end user is actually waiting for the prediction. So let's take a look at how it's different. You typically cannot distribute the prediction graph. Instead, you carry out the computation for one end user on one machine. However, you almost always scale out the predictions onto multiple workers. Essentially, each prediction is handled by a microservice, and you can replicate and scale out the predictions using Kubernetes or App Engine. Cloud ML Engine predictions are a higher-level abstraction, but they are equivalent to doing this. The performance consideration is not how many training stamps you can carry out per minute, but how many queries you can handle per second. The unit of this, queries per second, is often called QPS. That's the performance target that you need to hit. When you design for higher performance, you want to consider training and performance separately, especially if you will be doing online predictions. As I kind of suggested in my line about precomputing batch predictions for the top 20% of users and handling the rest of your users via online prediction, performance considerations will also involve striking the right balance, and ultimately, you will know the exact trade-off-- is it 20% or 10% or 25%?-- only after you build your system and start to measure things. However, unless you plan to be able to do both batch predictions and online predictions, you will be stuck with a solution that doesn't meet all of your needs. The idea behind this module and this course in general is so that you're aware of all of the possibilities. Once you're aware that it can be done, it's not actually all that difficult to accomplish. The technical part is usually quite straightforward, especially if you're using TensorFlow on a capable Cloud platform.

### Video - [Why distributed training is needed](https://www.cloudskillsboost.google/course_templates/17/video/504895)

* [YouTube: Why distributed training is needed](https://www.youtube.com/watch?v=yrQXH9dV1Hk)

Person: In this module, we'll explore how to run a distributed training job with TensorFlow. We'll begin with understanding why distributed training is needed. Then we'll explore distributed training architectures. And lastly, we'll provide an overview of TensorFlow distributed training strategies. Deep learning works because datasets are large. Notice that the X-axis here is logarithmic. For every doubling in the size of the data, the error rate falls linearly. A more complex model also helps-- that is, the jump from the blue line to the orange line. But more data is even more helpful in this situation. As a consequence of both of these trends, in terms of larger data sizes and more complex models, the compute required to build state-of-the-art models has grown over time. This growth is exponential as well. Each Y-axis tick on this graph shows a 10x increase in computational need. AlexNet--which started the deep learning revolution in 2013-- required less than 0.01 petaflops per second-day in compute per day for training. By the time you get to Neural Architecture Search-- the learn-to-learn model published by Google in 2017-- you need about 100 petaflops per second-day, or 1,000 times more compute than you needed for AlexNet. The growth in algorithm complexity and data size means that with complex models and large data volumes distributed systems are pretty much a necessity when it comes to machine learning. Training complex networks with large amounts of data can often take a long time. This graph shows training time on the X-axis plotted against the accuracy of predictions on the Y-axis when training an image recognition model on a GPU. As the dotted line shows, it took around 80 hours to reach 75% accuracy. If your training takes a few minutes to a few hours, it will make you productive and happy, and you can try out different ideas fast. If the training takes a few days, you can still deal with that by running a few ideas in parallel. If the training starts to take a week or more, your progress will slow down because you can't try out new ideas quickly. And if it takes more than a month, well, that's probably not even worth thinking about. And this is no exaggeration. Training deep neural networks such as ResNet-50 can take up to a week on one GPU. A natural question to ask is how can you make training faster? You can use a more powerful device such as a TPU or GPU accelerator. You could optimize your input pipeline. Or you can try out distributed training. In the next video, we'll explore distributed training architectures.

### Video - [Distributed training architectures](https://www.cloudskillsboost.google/course_templates/17/video/504896)

* [YouTube: Distributed training architectures](https://www.youtube.com/watch?v=DTV9MeOrO64)

person: In the previous video, we gave an overview of why distributed training is needed. Let's now take a look at distributed training architectures. Before we get into the details of how to achieve this scaling in TensorFlow, let's step back and explore the high-level concepts and architectures in distributed training. Let's say you start training on a machine with a multi-core CPU. TensorFlow automatically handles scaling on multiple cores. You may speed up your training by adding an accelerator to your machine, such as a GPU. Again, TensorFlow will use this accelerator to speed up model training with no extra work on your part. But with distributed training, you can go further. You can go from using one machine with a single device, in this case a GPU, to a machine with multiple devices attached to it. And finally, to multiple machines, possibly with multiple devices each, connected over a network. Eventually, with various approaches, you can scale up to hundreds of devices. And that is, in fact, what we do in several Google systems. Simply stated, distributed training distributes training workloads across multiple mini-processors, or worker nodes. These worker nodes work in parallel to accelerate the training process. Their parallelism can be achieved via two types of distributed training architecture. Let's explore both, starting with the most common, data parallelism. Data parallelism is model-agnostic, making it the most widely used paradigm for parallelizing neural network training. In data parallelism, you run the same model and computation on every device, but train each of them using different training data samples. Each device computes loss and gradients based on the training samples. Then you update the model's parameters using these gradients. The updated model is then used in the next round of computation. You'll recall that a gradient simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning. There are currently two approaches used to update the model using gradients from various devices, synchronous and asynchronous. Note that in this illustration, p0 and p1 refer to the individual parameters, while PS0 and PS1 denote parameter server zero and parameter server one. In synchronous training, all of the devices train their local model using different parts of data from a single, large mini-batch. They then communicate their locally calculated gradients, directly or indirectly, to all devices. In this approach, each worker device computes the forward and backward passes through the model on a different slice of input data. The computed gradients from each of these slices are then aggregated across all of the devices and reduced, usually using an average, in a process known as Allreduce. The optimizer then performs the parameter updates with these reduced gradients, thereby keeping the devices in sync. Because each worker cannot proceed to the next training step until all the other workers have finished the current step, this gradient calculation becomes the main overhead in distributed training for synchronous strategies. Only after all devices have successfully computed and sent their gradients, so that all models are synchronized, is the model updated. After the model weights are updated, splits from the next mini-batch are sent to each worker device. That is, devices trained on non-overlapping splits of the mini-batch. In asynchronous training, no device waits for updates to the model from any other device. The devices can run independently and share results as peers, or communicate through one or more central servers known as parameter servers. Thus, in an asynchronous parameter server architecture, some devices are designated to be parameter servers and others as workers. Devices used to run computations are called worker devices, while devices used to store variables are parameter devices. Each worker independently fetches the latest parameters from the parameter servers and computes gradients based on a subset of training samples. It then sends the gradients back to the parameter server, which then updates its copy of the parameters with those gradients. Each worker does this independently. This allows it to scale well to a large number of workers, where training workers might be preempted by higher priority production jobs, or a machine may go down for maintenance, or where there is asymmetry between the workers. This doesn't hurt the scaling, because workers are not waiting for each other. The downside of this approach, however, is that workers can get out of sync. They compute parameter updates based on stale values, and this can delay convergence. Given these two broad strategies, the asynchronous parameter server approach and the synchronous Allreduce approach, which should you choose? Well, there isn't one right answer, but here are some considerations. The asynchronous parameter server approach, in which the parameter servers contain fewer features, consume less memory, and can run just a cluster of CPUs, is great for sparse models, as it shards the model across parameter servers, and workers only need to fetch the part they need for each step. For dense models, the parameter server transfers the whole model each step, and this can create a lot of network pressure. Therefore, the synchronous Allreduce approach should be considered for dense models which contain many features and thus consume more memory. In this approach, all machines share the load of storing and maintaining the global parameters. This makes it the best option for dense models, like BERT, Bidirectional Encoder Representations from Transformers. When a model is too big to fit on one device's memory, you can divide it into smaller parts on multiple devices and then compute over the same training samples. This is called model parallelism. Model parallelism feeds or gives every processor the same data, but applies a different model to it. Think of model parallelism as simply multiple program, same data. Model parallelism splits the weights of the net equally among the threads. And all threads work on a single mini-batch. Here, the generated output after each layer needs to be synchronized, i.e. stacked, to provide the input to the next layer. In this approach, each GPU has different parameters and computation of different parts of a model. In other words, multiple GPUs do not need to synchronize the values of the parameters. Model parallelism needs special care when assigning different layers to different GPUs, which is more complicated than data parallelism. The gradients obtained from each model and each GPU are accumulated after a backward process, and the parameters are synchronized and updated. However, a hybrid of the data and model parallelism approaches is sometimes used together in the same architecture. Now that you've been introduced to some of the different distributed training architectures, in the next video we'll take a look at four TensorFlow distributed training strategies.

### Video - [TensorFlow distributed training strategies](https://www.cloudskillsboost.google/course_templates/17/video/504897)

* [YouTube: TensorFlow distributed training strategies](https://www.youtube.com/watch?v=jNnj6KUgRoA)

Person: Distributed training is particularly useful for very large data sets because it becomes very difficult-- and often unrealistic-- to perform model training on only a single hardware accelerator such as a GPU. TensorFlow's distribution strategies make it easier to seamlessly scale up heavy training workloads across multiple hardware accelerators, be they GPUs or even TPUs. But in doing so, you may face challenges. For example, tf.distribute. Strategy can help with these and other potential challenges. It is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. There are four TensorFlow distributed training strategies that support data parallelism. The list includes: We'll cover each strategy in more depth in the videos that follow.

### Video - [Mirrored strategy](https://www.cloudskillsboost.google/course_templates/17/video/504898)

* [YouTube: Mirrored strategy](https://www.youtube.com/watch?v=tbmlZ1T1sxM)

person: Mirrored strategy is the simplest way to get started with distributed training. You can use mirrored strategy when you have a single machine with multiple GPU devices. Mirrored strategy will create a replica of the model on each GPU. During training, one minibatch is split into n parts, where "n" equals the number of GPUs, and each part is fed to one GPU device. For this setup, mirrored strategy manages the coordination of data distribution and gradient updates across all of the GPUs. Let's look at an image classification example where a Keras ResNet model with the functional API is defined. First, download the Cassava dataset from TensorFlow datasets. Then add a preprocess_data function to scale the images. From there, map, shuffle, and prefetch the data, and then define the model. Let's create the strategy object using tf.distribute. MirroredStrategy. Next, let's create the model with variables within the strategy scope. These variables include the model spare_categorical_crossentropy for loss, the Keras optimizer, and metrics variables to compute accuracy. The last change you'll want to make is to the batch size. When you carry out distributed training with the tf.distribute strategy API and tf.data, the batch size now refers to the global batch size. In other words, if you pass a batch size of 64 and you have two GPUs, then each machine will process 32 examples per step. In this case, 64 is known as the global batch size, and 32 is the per replica batch size. To make the most out of your GPUs, you'll want to scale the batch size by the number of replicas. From there, map, shuffle, and prefetch the data. You then call model.fit on the training data. Here we're going to run five passes of the entire training dataset. Now let's explain what actually happens when we call model.fit before adding a strategy. For simplicity, imagine you have a simple linear model instead of the ResNet 50 architecture. In TensorFlow, you can think of this simple model in terms of its computational graph or directed acyclic graph, here referred to as a DAG. Here the matmul op takes in the x and W tensors, which are the training batch and weights, respectively. The resulting tensor is then passed to the add op with the tensor b, which is the model's bias terms. The result of this op is ypred, which is the model's predictions. Now this is an example of data parallelism with two GPUs. The input batch, x, is split in half, and one slice is sent to GPU 0 and the other to GPU 1. In this case, each GPU calculates the same ops, but on different slices of the data. For more information on optimization, please refer to the guide titled, "Optimize TensorFlow GPU performance using the Profiler" at tensorflow.org/guide/profiler.

### Video - [Multi-worker mirrored strategy](https://www.cloudskillsboost.google/course_templates/17/video/504899)

* [YouTube: Multi-worker mirrored strategy](https://www.youtube.com/watch?v=QIuJDnsbP3E)

person: Multi-worker mirrored strategy is very similar to mirrored strategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to mirrored strategy, it creates copies of all variables in the model on each device across all workers. If you've mastered single-host training and are looking to scale training even further, then adding multiple machines to your cluster can help you get an even greater performance boost. You can make use of a cluster of machines that are CPU only or that each have one or more GPUs. Like its single-worker counterpart, mirrored strategy, multi-worker mirrored strategy is a synchronous data parallelism strategy that can be used with only a few code changes. However, unlike mirrored strategy, for a multi-worker setup, TensorFlow needs to know which machines are part of the cluster. In most cases, this is specified with the environment variable "TF_CONFIG". In this simple TF_CONFIG example, the "cluster" key contains a dictionary with the internal IPs and ports of all the machines. In multi-worker mirrored strategy, all machines are designated as "workers", which are the physical machines on which the replicated computation is executed. In addition to each machine being a worker, there needs to be one worker that takes on some extra work, such as saving checkpoints and writing summary files to TensorBoard. This machine is known as the "chief", or by its deprecated name, "master". Conveniently, when using AI Platform training, the TF_CONFIG environment variable is set on each machine in your cluster, so there's no need to worry about this setup. As with any strategy in the tf.distribute module, step one is to create a strategy object. Step two is to wrap the creation of the model parameters within the Scope of the strategy. This is crucial, because it tells mirrored strategy which variables to mirror across the GPU devices. And the third and final step is to scale the batch size by the number of replicas in the cluster. Since we've already covered training with mirrored strategy, the previous steps should be familiar. The main difference when moving from synchronous data parallelism on one machine to many is that the gradients at the end of each step now need to be synchronized across all GPUs in the machine and across all machines in the cluster. This additional step of synchronizing across the machines increases the overhead of distribution. With multi-worker mirrored strategy, the data needs to be sharded, meaning that each worker is assigned a subset of the entire dataset. If autosharding is turned off, each replica processes every example in the dataset, which is not recommended. Therefore, at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. This sharding happens automatically with tf.data.experimental. AutoShardPolicy. By default, the policy is set to AUTO, which will shard your data depending on whether it is file-based or not. Saving the model is slightly more complicated in the multi-worker case, because there needs to be different destinations for each worker. The chief worker will save to the desired model directory, while the other workers will save the model to temporary directories. It's important that these temporary directories are unique in order to prevent multiple workers from writing to the same location. Saving can contain collective ops, so all workers must save, not just the chief.

### Video - [TPU strategy](https://www.cloudskillsboost.google/course_templates/17/video/504900)

* [YouTube: TPU strategy](https://www.youtube.com/watch?v=UiupJeCSquI)

Person: Similar to MirroredStrategy, TPUStrategy uses a single machine where the same model is replicated on each core with its variables synchronized-- mirrored-- across each replica of the model. The main difference, however, is that TPUStrategy will all-reduce across TPU cores, whereas MirroredStrategy will all-reduce across GPUs. tf.distribute. TPUStrategy lets you run your TensorFlow training on Tensor Processing Units--TPUs. TPUs are Google's specialized ASICs designed to dramatically accelerate machine learning workloads. TPUs provide their own implementation of efficient all-reduce and other collective operations across multiple TPU cores which are used in TPUStrategy. You'll also need a variable called "strategy." But this time, you will choose the tf.distrubute. TPFStrategy method. Because TPUs are very fast, many models ported to the TPU end up with a data bottleneck. The TPU is sitting idle, waiting for data for the most part of each training epoch. TPUs read training data exclusively from Google Cloud Storage-- GCS. And GCS can sustain a pretty large throughput if it is continuously streaming from multiple files in parallel. Following best practices will optimize the throughput. With too few files, GCS will not have enough streams to get max throughput. With too many files, time will be wasted accessing each individual file. Let's summarize the distribution strategies using code. Our base scope is a Keras Sequential model. Now, to improve training, we can use the MirroredStrategy. Or for faster training, the MultiWorkerMirroredStrategy. And for really fast training, the TPUStrategy.

### Video - [Parameter server strategy](https://www.cloudskillsboost.google/course_templates/17/video/504901)

* [YouTube: Parameter server strategy](https://www.youtube.com/watch?v=gSCHwdCN310)

person: Earlier, we explored the asynchronous parameter server architecture. A parameter server training cluster consists of Workers and ParameterServers. Variables are created on ParameterServers, and they are read and updated by Workers in each step. By default, Workers read and update these variables independently without synchronizing with each other. The TensorFlow parameter server strategy introduces a central coordinator. The Coordinator is a special task type that creates resources, dispatches training tasks, writes checkpoints and deals with task failures. You can create your parameter server strategy object just like you would for the other strategies. Note that you will need to parse in the ClusterResolver argument, and if training with AI platform, this is just a simple TFConfigClusterResolver. Using model.fit with parameter server training, requires that the input data be provided in a call of an object that takes a single argument of type TF distribute input context and returns TF data dataset. We then need to wrap our dataset function in tf.keras.utils.experimental. DatasetCreator. The code in dataset_fn will be invoked on the input device which is usually the CPU on each of the Worker machines. When using parameter server strategy, it is recommended that you shuffle and repeat your dataset and parse in the steps per epoch argument to model.fit.

### Video - [Lab Introduction: Distributed Training with Keras](https://www.cloudskillsboost.google/course_templates/17/video/504902)

* [YouTube: Lab Introduction: Distributed Training with Keras](https://www.youtube.com/watch?v=5zPotyAmkSQ)

This lab provides guidance on how to use distributed training with Keras. You’ll begin by defining a distribution strategy and setting an input pipeline, then continue on to create a Keras model, define callbacks, and finally, train and evaluate a model.

### Lab - [Distributed Training with Keras](https://www.cloudskillsboost.google/course_templates/17/labs/504903)

The tf.distribute.Strategy API provides an abstraction for distributing your training across multiple processing units. The goal is to allow users to enable distributed training using existing models and training code, with minimal changes.

* [ ] [Distributed Training with Keras](../labs/Distributed-Training-with-Keras.md)

### Video - [Training on large datasets with tf.data API](https://www.cloudskillsboost.google/course_templates/17/video/504904)

* [YouTube: Training on large datasets with tf.data API](https://www.youtube.com/watch?v=m9Ecd6THJv0)

person: Data is one of the most crucial components of your machine learning model. Collecting the right data is not enough. You also need to make sure that you're putting the right processes in place to clean, analyze, and transform the data as needed so that the model can take the most signal from that data as possible. And models which are deployed in production especially require lots and lots of data. This is data that likely won't fit in memory and can possibly be spread across multiple files or may come from an input pipeline. The tf.data API enables you to build those complex input pipelines from simple reusable pieces. For example, the pipeline might be a structured data set that requires normalization, feature crosses, or bucketization. An image model might aggregate data from files in a distributed file system, apply random skewness to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a look-up table, and then batching together sequences of different lengths. The tf.data API makes it possible to handle large amounts of data, read it in different file and data formats, and perform those complex transformations. The tf.data API introduces the tf.data. Dataset abstraction that represents a sequence of elements in which each element consists of one or more components. For example, in an image pipeline an element might be a single training example with a pair of tensor components representing the image and its label. There are two distinct ways to create a dataset. A data source constructs a dataset from data stored in memory or in one or more files. Or a data transformation constructs a dataset from one or more tf.dataset objects. Large datasets tend to be sharded or broken apart into multiple files, which can be loaded progressively. Remember that you train on mini batches of data. You don't even have the entire dataset in memory. One mini batch is all you need for one training step. The dataset API will help you create input functions for your model that load data in progressively throttling it. There are specialized dataset classes that can read data from text files like CSVs, TensorFlow records, or fixed length record files. Datasets can be created from many different file formats. Use TextLineDataset to instantiate a dataset object, which is comprised of-- as you might guess-- one or more text files. TFRecordDataset, TFRecord files, FixedLengthRecord Dataset is a dataset object from fixed length records or one or more binary files. For anything else you can use the generic dataset class and add your own decoding code. Okay, let's walk through an example of TFRecordDataset. At the beginning the TFRecord op is created an executed. It produces a variant tensor representing a dataset which is stored in the corresponding pipeline object. Next, the Shuffle op is executed using the output of a TFRecord op and its input connected the two stages of our input pipeline so far. Next, the user define function is traced and passed as attributes to the Map operation along with the Shuffle dataset variant input. Finally, the Batch op is created an executed creating the final stage of our input pipeline. When the for loop mechanism is used for numerating the elements of the dataset, the iterable method is evoked on the dataset, which triggers the creation and execution of two ops. First an anonymous iterator loop is created and executed which results in the creation of an iterator resource. Subsequently this resource along with the Batch dataset variant is passed into the MakeIterator op initializing the state of the iterator resource with the dataset. When the next method is called it triggers creation and execution of the IteratorGetNext op passing in the iterator resource as the input. Note that the Iterator op is created only once but executed as many times as there are elements in the input pipeline. Finally, when the pipeline iterator object goes out of scope the DeleteIterator op is executed to make sure that the iterator resource is properly disposed of, or to state the obvious, properly disposing of the iterator resource is essential as it is not uncommon for your iterator resources to allocate say hundreds of megabytes to gigabytes of memory because of internal buffering.

### Video - [Lab Introduction: TPU-speed Data Pipelines](https://www.cloudskillsboost.google/course_templates/17/video/504905)

* [YouTube: Lab Introduction: TPU-speed Data Pipelines](https://www.youtube.com/watch?v=574DXfBmW2Q)

In this lab, you’ll get practice loading data from Google Cloud Storage with the Tensorflow Dataset API, to feed a TPU. You’ll start by using the Tensorflow Dataset  API to load training data, then use the TF Record format to load training data from Google Cloud Storage.

### Lab - [TPU Speed Data Pipelines](https://www.cloudskillsboost.google/course_templates/17/labs/504906)

TPUs are very fast. The stream of training data must keep up with their training speed. In this lab, you will learn how to load data from GCS with the tf.data.Dataset API to feed your TPU.

* [ ] [TPU Speed Data Pipelines](../labs/TPU-Speed-Data-Pipelines.md)

### Video - [Inference](https://www.cloudskillsboost.google/course_templates/17/video/504907)

* [YouTube: Inference](https://www.youtube.com/watch?v=ILZuFizU8hw)

Laurence: So far, we have looked at training performance. Now we'll take a look at performance when it comes to predictions. So how do you obtain high performance inference? Well, you need to consider several aspects. There's the throughput requirements. How many queries per second do you need to process? There's latency requirements. And that means how long a query actually takes. And then there's costs. And that's in terms of infrastructure and in terms of maintenance. There are essentially three approaches to implementing this. Using a deployed model, which is REST or HTTP API for streaming pipelines, using Cloud ML Engine batch prediction jobs for batch pipelines, or using Cloud Dataflow direct-model prediction, which can be used for both batch and streaming pipelines. So let's take a look at the third option, and we'll delve into it a bit. And this will help clarify our terminology as well. We're using the word "batch" differently from the word "batch" in ML training. Here we're using "batch" to refer to a bounded dataset. A typical batch data pipeline reads data from some persistent storage, either a data lake, like Google Cloud Storage, or a data warehouse like BigQuery. It then does some processing and writes it out to the same or a different format. The processing carried on by Cloud Dataflow typically enriches the data with the predictions of an ML model. Now there are two options to do this. Either by using a TensorFlow SavedModel, and loading it directly into the Dataflow pipeline from Cloud Storage, or by using TensorFlow Serving, and accessing it via an HTTP endpoint as a microservice either from Cloud ML Engine, as shown, or using Kubeflow running on a Kubernetes engine. So far, we've used the HTTP endpoint approach, but for performance reasons, you might want to consider the SavedModel approach as well. So what option gives the best performance for batch pipelines? Well, as usual, this depends on the aspect that's most important to you. In terms of raw processing speed, you'll want to use Cloud ML Engine batch predictions. The next fastest is to directly load the SavedModel into your Dataflow job and then invoke it. The third option, in terms of speed, is to use TensorFlow Serving on Cloud ML Engine. But if you want maintainability, the second and third options reverse. The batch prediction is still the best. I mean, what's not to love about a fully managed service? But using online predictions as a microservice allows for easier upgradability and dependency management than loading up the current version into the Dataflow job. This graph is from an upcoming solution. See httpsCloud.Google.com/solutions. By the time this video is available, the solution might already have been published. A streaming pipeline is similar, except that the input dataset is not bounded. So we read it from an unbounded source, like a pub/sub, and we process it with Dataflow. You have two options of SavedModel or TensorFlow Serving here as well, with TensorFlow Serving hosted on Cloud ML Engine. For streaming pipelines, the SavedModel approach is the fastest. Using minibatching, as we recommended earlier in the module on implementing Serving, helps reduce the gap between the TensorFlow Serving HTTP endpoint approach, supported by Cloud ML Engine, and directly loading the model into the client. However, the Cloud ML Engine approach is much more maintainable, especially when the model will be used for multiple clients. Another thing to keep in mind is that as the number of queries per second keeps increasing, at some point the SavedModel approach will become infeasible, but the Cloud ML Engine approach should scale indefinitely.

### Quiz - [Quiz: Designing high-performance ML systems](https://www.cloudskillsboost.google/course_templates/17/quizzes/504908)

#### Quiz 1.

> [!important]
> **If each of your examples is large in terms of size and requires parsing, and your model is relatively simple and shallow, your model is likely to be:**
>
> * [ ] CPU-bound, so you should use GPUs or TPUs.
> * [ ] Latency-bound, so you should use faster hardware
> * [ ] I/O bound, so you should look for ways to store data more efficiently and ways to parallelize the reads.

#### Quiz 2.

> [!important]
> **Which of the following indicates that ML training is CPU bound?**
>
> * [ ] If I/O is simple, but the model involves lots of complex/expensive computations.
> * [ ] If you are running a model on accelerated hardware.
> * [ ] If you are running a model on powered hardware.
> * [ ] If I/O is complex, but the model involves lots of complex/expensive computations.

#### Quiz 3.

> [!important]
> **What does high-performance machine learning determine?**
>
> * [ ] Training a model
> * [ ] Reliability of a model
> * [ ] Deploying a model
> * [ ] Time taken to train a model

#### Quiz 4.

> [!important]
> **For the fastest I/O performance in TensorFlow… (check all that apply)**
>
> * [ ] Optimize TensorFlow performance using the Profiler.
> * [ ] Prefetch the data
> * [ ] Read in parallel threads.
> * [ ] Read TF records into your model.

### Document - [Readings: Designing high-performance ML systems](https://www.cloudskillsboost.google/course_templates/17/documents/504909)

## Building Hybrid ML Systems

Understand the tools and systems available and when to leverage hybrid machine learning models.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/17/video/504910)

* [YouTube: Introduction](https://www.youtube.com/watch?v=ZMtqMI7SFX4)

Lak: Hi, I'm Lak, and I lead the team that's putting together this course in this specialization. In this module, we will look at building hybrid machine learning models You will learn how to build hybrid machine learning models, and how to optimize TensorFlow graphs for mobile. Let's start by discussing a technology called Kubeflow, which helps us build hybrid cloud machine learning models. But why are we discussing hybrid in the first place? Why would you need anything other than Google Cloud? Google Cloud is a great place to do machine learning. You have access to ready-made models like the Vision API, Natural Language API, et cetera. The key aspect of these models is that they are trained on Google's massive datasets. Sometimes, though, ready-to-run models like the Vision API don't quite fit. And in that case, you might want to train a family of models using your own images and your own labels to customize and add to the Vision API. That is called Auto-ML, and that is possible only on the cloud. But even if you're building custom machine learning models, you have reason to do machine learning on GCP. While TensorFlow is open source, a serverless execution environment like Cloud ML Engine allows your data scientists to not have to worry about infrastructure. Plus, of course, the integration with distributed cloud storage and serverless and BigQuery make the overall development experience a lot better than if you had to provision and manage all of that infrastructure yourself. So far in this series of courses, we have assumed that you are in a cloud-native environment. And so we prototyped and developed our code using Cloud Datalab. And then once we had the code working on a small sample of data, we submitted the training job to Cloud ML Engine to operate on the full data set. We also served out the model using Cloud ML Engine so that we didn't have to worry about infrastructure. There are times, however, when you cannot be fully cloud native. What kinds of situations? You may not be able to do machine learning solely on the cloud. Perhaps you are tied to on-premises infrastructure, and your ultimate goal is to move to the public cloud, but it's gonna take a few years. Perhaps there are constraints about being able to move your training data off your on-premise cluster or data center. So you have to make do with the system that you have. Or maybe the data that is being produced is produced by a system that is running on a different cloud. Or the model predictions need to be consumed by an application on some other cloud. So you need a multi-cloud solution, not a solution that is solely GCP. Or maybe you are running machine learning on the edge and connectivity constraints force you to have to do your predictions on the edge-- on the device itself. And so you have to do inference on the edge. This is a very common situation if you're doing Internet of Things. So here, for example, is Cisco's hybrid cloud architecture. Cisco partnered with Google Cloud to bridge their private cloud infrastructure and their existing applications with Google Cloud Platform. So notice the use of Google Kubernetes Engine to manage their container deployments. Kubernetes is a container orchestration system that was designed and then open-sourced by Google. So why Kubernetes? Using Kubernetes, it is possible to orchestrate containers whether they are running on-prem or on the cloud. Any cloud. So one possible solution to retain the ability to move fast, minimize your infrastructure management needs, and still retain the ability to move or burst to GCP is to use Kubernetes. Specifically, a project called Kubeflow helps you migrate between cloud and on-prem environments. Kubeflow is an open-source machine learning stack built on Kubernetes. On Google Cloud, you can run Kubeflow on Google Kubernetes Engine--GKE. However, you can run Kubeflow on anything from a phone, to a laptop, to an on-prem cluster. Your code remains the same. Some of the configuration settings change, but that's it.

### Video - [Machine Learning on Hybrid Cloud](https://www.cloudskillsboost.google/course_templates/17/video/504911)

* [YouTube: Machine Learning on Hybrid Cloud](https://www.youtube.com/watch?v=w5q0HHSu7GA)

Lak: In order to build hybrid machine learning systems that work well both on-premises and in the cloud, your machine learning framework has to support three things: composability, portability, and scalability. So let's take composability first. When people think about machine learning, they think about building a model, training a model. TensorFlow, PyTorch, NumPy, et cetera. But the reality is 95% of the time is spent not building a model. It's all the other stuff. Each machine learning stage-- data analysis, training, model validation, monitoring-- these are all independent systems. Everyone has a different way to handle all these boxes. And so when we say "composability," it's about the ability to compose a bunch of microservices together and the option to use what makes sense for your problem. But now that you've built your specific framework, you want to move it around. And that's where we get into portability. The stack that you use is likely made up of all these components-- and probably lots more. And all those microservices I detailed earlier only touch a small number of them. But you do it. You configure every stage in the stack, and it's finally running. What's this good for? What happens next? Think about the machine learning workflow. Remember that you did all of this just so that you could develop the model. We'll call that "experimentation." But once you have the code running, what do you need to do? That's right-- you need to train the model on the full dataset. You probably can't do it on the small setup on which you did all your initial development. So you start up a training cluster, and you have to do it all over again. All the configuration, all the libraries, all the testing-- you've got to repeat it for the new environment. And then, chances are you've got to do it once again to move it from on-premises to the cloud. Because remember, we said we want a hybrid environment. A machine learning model that maybe it helps you train on the cloud and predict on the edge, or train on the cloud and predict on-premises. The point is that you have to configure the stack over and over again for each environment that you need to support. Maybe at this point you're thinking, [scoffs] "That doesn't matter to me. "I never have to change environments. I'll only use one environment. " Wrong. So portability--it's essential. And then of course, you've got to do it again when your inputs change. Or your boss calls you and tells you to train faster by training on more machines. You inevitably find that you have to change environments over and over again. Also, your laptop-- it counts as environment number one. And you don't do production services on your laptop. So you need portability. So composability, portability, finally--scalability. You always hear about Kubernetes being able to scale. And that's true, but scalability in machine learning means so many more things. Accelerators-- GPUs, TPUs, et cetera-- disks, skillsets-- software engineers, researchers, data engineers, data analysts, data scientists, different skillsets-- teams across the org-- because there's teams that are gonna be building the experiments, teams that are gonna be using the experiments, teams that are gonna be monitoring the machine learning models. So accelerators, disks, skillsets, teams, experiments. So that's what we think of when we think of machine learning in a hybrid cloud environment-- composability, portability, scalability.

### Video - [Kubeflow](https://www.cloudskillsboost.google/course_templates/17/video/504912)

* [YouTube: Kubeflow](https://www.youtube.com/watch?v=juJ2ylUfnWk)

Person: Welcome back. In this module, you will learn how to build hybrid cloud machine learning models with Kubeflow and how to optimize TensorFlow graphs for mobile. To begin, let's explore Kubeflow, an open-source machine learning platform designed to enable the use of machine learning pipelines to orchestrate complicated workflows running on Kubernetes. Kubeflow helps build hybrid cloud machine learning models. But why are we discussing hybrid in the first place? Why would you need anything other than Google Cloud? So far in this course, we've focused on cloud-native environments, which involve prototyping and developing code using an AI platform notebook. When that code is working on a small sample of data, the training job is submitted to AI Platform to operate on the full data set. There are, however, scenarios when cloud-native-- or conducting machine learning solely on Google Cloud, is not an option. Let's look at some of these scenarios now. Maybe you're tied to on-premises infrastructure, or other constraints prevent moving your training data from an on-premises cluster or data center. Alternatively, perhaps you require a multi-cloud solution architecture-- one that does not rely solely on Google Cloud. This could be because you are working with data that is produced by a system that is running on a different cloud provider, or because model predictions need to be consumed from another cloud. Or maybe you are in an initial phase of an ML project running machine learning on the edge where you're working on a local developer workstation. Training at scale typically happens in a cloud environment. However, inference and distributed training can happen at the edge. "The edge" means that predictions happen on a smart device, which is common in the Internet of Things. Using Kubernetes, you can orchestrate containers that run either on-premises or in the cloud. And that can be any cloud. Using Kubernetes allows for speed and the ability to minimize infrastructure management needs, all while being able to move or burst to Google Cloud. Kubeflow is the machine learning toolkit for Kubernetes, and it brings several benefits. It makes deploying machine learning workflows on Kubernetes simple, portable, and scalable. It also extends Kubernetes' ability to run independent and configurable steps with machine-learning-specific frameworks and libraries. And because Kubeflow is open source, it can run on Google Kubernetes Engine which is part of Google Cloud. However, Kubeflow can actually run on anything, whether it's a phone, a laptop, or an on-premises cluster. Regardless of where it's run, the code remains the same. Some of the configuration settings just change.

### Video - [Lab Introduction: Kubeflow Pipelines with AI Platform](https://www.cloudskillsboost.google/course_templates/17/video/504913)

* [YouTube: Lab Introduction: Kubeflow Pipelines with AI Platform](https://www.youtube.com/watch?v=EH-iMWf9svM)

This lab provides hands-on practice installing and using Kubeflow Pipelines to orchestrate Google Cloud services in an end-to-end ML pipeline. To begin, you’ll create a Kubernetes cluster and install Kubeflow Pipelines. Next, you’ll launch an AI Platform Notebook. From there, you’ll create and run an AI Platform Pipeline. And finally, you’ll run a Python function-based pipeline.

### Lab - [Running Pipelines on Vertex AI 2.5](https://www.cloudskillsboost.google/course_templates/17/labs/504914)

In this lab, you learn how to utilize Vertex AI Pipelines to execute a simple Kubeflow Pipeline SDK derived ML Pipeline.

* [ ] [Running Pipelines on Vertex AI 2.5](../labs/Running-Pipelines-on-Vertex-AI-2.5.md)

### Video - [TensorFlow Lite](https://www.cloudskillsboost.google/course_templates/17/video/504915)

* [YouTube: TensorFlow Lite](https://www.youtube.com/watch?v=c7lJZE44MHQ)

Lak: TensorFlow supports multiple mobile platforms including Android, iOS and Raspberry Pi. So in here, we're gonna focus on mobile devices. Mobile TensorFlow makes sense when there's a poor or missing network connection, or where sending continuous data to a server would be too expensive. The purpose is to help developers make lean mobile apps using TensorFlow, both by continuing to reduce the code footprint, and by supporting quantization and lower-precision arithmetic that make the models smaller. You can build a TensorFlow shared object on Android using Android Studio using a continuous integration tool called Bazel. And for iOS, there is CocoaPod integration as well. And it's all relatively simple. So let's take a look at how you can use a TensorFlow API. The Android inference library integrates with TensorFlow for Java applications. So this library is a very thin wrapper from Java to the native implementation so that the performance impact is not very high. So at first, you create TensorFlowInferenceInterface, opening the model file from the asset in the APK, and then you set up an input feed using the Feed API. And on mobile, the input data tends to be retrieved from various sensors like the camera, et cetera. And then you run the inference, and you fetch the result using the fetch method. So all of these are blocking calls, so you typically run them in a worker thread instead of the main thread because an API call takes time. And even though we've talked primarily about prediction on mobile, a new frontier is confederated learning. The idea is you continuously train the model on the device, and then you combine the model updates from a federation of user devices to update the overall model. The goal is for each user to get their customized experience because there's model training happening on the device, but still retain privacy because it's the overall model update that goes back to the cloud.

### Video - [Optimizing TensorFlow for mobile](https://www.cloudskillsboost.google/course_templates/17/video/504916)

* [YouTube: Optimizing TensorFlow for mobile](https://www.youtube.com/watch?v=ZpgAZEcEadY)

person: Let's look at a second scenario where hybrid models are necessary. Earlier, we explored how more and more applications are combining machine learning with mobile applications. Take Google Translate, for example, which is composed of several models. It uses one model to find a sign, another model to read the sign, using optical character recognition, a third model to translate the sign, a fourth model to superimpose the translated text, and a fifth model to select the best font to use. ML allows you to add some intelligence to your mobile apps, such as image and voice recognition, translation, and natural language processing. You can also apply machine learning to gain smarter analytics on mobile-specific data. For example, to detect certain patterns from motion sensor data or GPS tracking data. This is all because ML can extract meaning from raw data. So if you want to perform image recognition with your mobile app, the easiest way is to send the raw image to the cloud and let the cloud service recognize the objects in the image. However, if you have a neural network algorithm running on your mobile app, you can get labels of the objects and send them to the cloud. It's a more efficient way to collect the object labels on the cloud service. Now let's say you perform motion detection with your mobile app. In this case, you can run a neural network algorithm to extract a feature vector from the sensor data. The numbers in the feature vector represent the signatures of each motion. This means you don't have to send the raw motion data to a cloud service. Also, by applying machine learning to mobile apps, you can reduce network bandwidth and get faster response times when communicating with cloud services. It's important to note that you often can't use the microservices approach for mobile devices, because they can add unwanted latency. You can't delegate to a microservice as you can when running in the cloud, so you'll now want a library, not a process. In these types of situations, it's best to train models in the cloud and carry out predictive modeling on a device. This means embedding the model within the device itself.

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/17/video/504917)

* [YouTube: Summary](https://www.youtube.com/watch?v=eaxS7PH22KU)

Lak: In this module, we showed you two technologies-- Kubeflow and TensorFlow Lite-- that are important in hybrid machine learning systems. Kubeflow gives you composability, portability, and scalability, while preserving the ability to run everywhere. Specifically, Kubeflow offers portability and composability between your on-premises environment and Cloud ML Engine. The tradeoff is that Kubeflow is not serverless. You will have to do cluster management. Still, retaining the ability to move to cloud and serverless at some point in the future, all for some fraction of your workload, provides flexibility. The presence of Kubeflow also limits lock-in. You can always take your models off Google Cloud and you have a way to continue training and serving those models. TensorFlow Lite makes specific compromises to enable machine learning inference on low-power or under-resourced devices. For example, you can convert variable nodes into constant nodes, which streamlines your model because constant nodes are embedded in the graph itself. However, you sacrifice maintainability and portability since you cannot resume training from that model graph. Another compromise you might make is to use a less-accurate model on the device. Perhaps you quantize the nodes, or you use a smaller model. Of course, we hope that you choose to train and serve machine learning models on Google Cloud, so you don't have to make these compromises, or manage all this infrastructure, or train on low-power devices. But if business and real-world considerations require you to be able to train or serve machine learning models outside a cloud environment, it's good to know that you have these options. So Kubeflow and TensorFlow Lite are good to know about to have in your back pocket when such situations arise.

### Quiz - [Quiz: Hybrid ML systems](https://www.cloudskillsboost.google/course_templates/17/quizzes/504918)

#### Quiz 1.

> [!important]
> **To copy the input data into TensorFlow, which of the following syntaxes is correct?**
>
> * [ ] inferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);
> * [ ] inferenceInterface.feed(floatValues, 1, inputSize, inputSize, 3);
> * [ ] inferenceInterface.feed(inputName, floatValues, inputSize; inputSize);
> * [ ] inferenceInterface.feed(inputName, floatValues, 1, inputSize, 3);

#### Quiz 2.

> [!important]
> **A key principle behind Kubeflow is portability so that you can:**
>
> * [ ] Convert your model from CUDA to XLA.
> * [ ] Migrate your model from TensorFlow to PyTorch.
> * [ ] Move your model from on-premises to Google Cloud.

#### Quiz 3.

> [!important]
> **Which of these are reasons that you may not be able to perform machine learning solely on Google Cloud? Check all that apply.**
>
> * [ ] You need to run inference on the edge.
> * [ ] You are tied to on-premises or multi-cloud infrastructure due to business reasons.
> * [ ] TensorFlow is not supported on Google Cloud.

#### Quiz 4.

> [!important]
> **Which of the following determines the correct property of Tensorflow Lite? Select TWO correct answers.**
>
> * [ ] Lower precision arithmetic
> * [ ] Higher precision arithmetic
> * [ ] Quantization
> * [ ] Increased code footprint

### Document - [Readings: Hybrid ML systems](https://www.cloudskillsboost.google/course_templates/17/documents/504919)

## Summary

PDF links to all modules

### Video - [Course summary](https://www.cloudskillsboost.google/course_templates/17/video/504920)

* [YouTube: Course summary](https://www.youtube.com/watch?v=OAbXCUPH8HY)

Person: This bring us to the end of the Productions ML Systems course-- the first course in the specialization. Before you go, let's quickly recap what you've learned. In the first module, Architecting Production ML Systems, we explored what an ML system should be able to do and the components that take responsibility for those actions. We also introduced two decisions that system architects will have to make: Whether to conduct dynamic or static training, or even conduct dynamic or static inference. In module two, Designing Adaptable ML Systems, you saw how change can effect an ML System and what can be done to mitigate those effects. In module three, Designing High-Performance ML Systems, we explored how to optimize the performance of an ML System by choosing the right hardware and removing bottlenecks. And finally, in module four, Building Hybrid ML Systems, you learned about the technology behind hybrid systems that allows you to run your workloads on the cloud, on the edge using mobile devices, or on-premises. We encourage you to continue to the next course, Image Processing and Generation with Google Cloud, where we'll explore convolutional networks transfer learning, and Tensor Processing Units. Thanks for learning with us.

### Document - [Production Machine learning systems - readings](https://www.cloudskillsboost.google/course_templates/17/documents/504921)

### Document - [All quiz questions and answers](https://www.cloudskillsboost.google/course_templates/17/documents/504922)

## Course Resources

PDF links to all modules

### Document - [Architecting Production ML Systems Course Resources](https://www.cloudskillsboost.google/course_templates/17/documents/504923)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
