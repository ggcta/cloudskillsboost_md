---
id: 33
name: 'Architecting with Google Kubernetes Engine: Production'
datePublished: 2025-01-02
topics:
- IAM
- GKE
- CI/CD
type: Course
url: https://www.cloudskillsboost.google/course_templates/33
---

# [Architecting with Google Kubernetes Engine: Production](https://www.cloudskillsboost.google/course_templates/33)

**Description:**

In this course, you'll learn about Kubernetes and Google Kubernetes Engine (GKE) security; logging and monitoring; and using Google Cloud managed storage and database services from within GKE.

This is the second course of the Architecting with Google Kubernetes Engine series. After completing this course, enroll in the Reliable Google Cloud Infrastructure: Design and Process course or the Hybrid Cloud Infrastructure Foundations with Anthos course.

**Objectives:**

- Define Identity and Access Management roles for GKE, as well as Kubernetes pods security policies.
- Understand how logging is implemented and Kubernetes, and how GKE extends that basic functionality using Google Cloud Observability.
- Use Google Cloud Managed Storage Services with GKE.
- Use CI/CD with GKE.

## Production Course Introduction

In this introduction, you'll explore the course goals and preview each section.

### Document - [Production Course introduction](https://www.cloudskillsboost.google/course_templates/33/documents/519801)

## Access Control and Security in Kubernetes and Google Kubernetes Engine

In this section of the course, you'll learn about Kubernetes security, focusing on authentication and authorization. You'll explore Kubernetes Role-Based Access Control (RBAC) and its integration with IAM to secure GKE clusters. You'll also learn how to configure Workload Identity, secure GKE with Pod Security Standards and Pod Security Admission, and implement RBAC within your GKE environment.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/33/video/519802)

- [YouTube: Introduction](https://www.youtube.com/watch?v=_7h8RRoFiEY)

Earlier in this course, you learned how to run workloads within a GKE cluster. Now you’ll focus on securing your cluster and applications by establishing appropriate access control mechanisms. This is essential for ensuring that only authorized users can perform specific actions within your GKE environment, thereby protecting your valuable resources. In this section titled, “Access Control and Security in Kubernetes and Google Kubernetes Engine”, you’ll explore Kubernetes authentication and authorization; examine Kubernetes role-based access controls and how it works with IAM to secure GKE clusters; configure Workload Identity to access Google Cloud services from within GKE; secure GKE with Kubernetes Pod Security Standards and Pod Security Admission; and implement Role-Based Access Control with GKE.

### Video - [Authentication and authorization](https://www.cloudskillsboost.google/course_templates/33/video/519803)

- [YouTube: Authentication and authorization](https://www.youtube.com/watch?v=RIDIPKqg1xw)

Within Kubernetes, user access falls into two categories: individual user accounts and Kubernetes service accounts. Whereas normal user accounts represent human users, Kubernetes service accounts manage credentials for applications running within the cluster. In GKE, user accounts are typically defined through a Cloud Identity domain, which provides centralized management and fine-grained control, as compared to using individual consumer Google accounts. But then there is also Identity and Access Management, or IAM, which is a core Google Cloud service that lets you link permissions to user accounts and groups. We’ll explore this in more detail soon. To simplify managing service accounts in GKE clusters, Google Cloud offers Workload Identity, a feature that bridges the gap between Kubernetes service accounts and IAM service accounts in Google Cloud, which enables access to other Google Cloud services for applications that run within your cluster. Workload Identity offers a secure way to authenticate Pods by keeping service account credentials outside the pods themselves. This prevents attackers from accessing these credentials even if a Pod is compromised. Once a Pod starts, it automatically receives its service account credentials from the GKE metadata server, allowing it to authenticate itself to other Google Cloud services securely. The GKE metadata server is a subset of the Compute Engine metadata server endpoints required for Kubernetes workloads. Furthermore, Workload Identity simplifies Pod authentication by eliminating the need to manage credentials within the Pods. This enhances both security and convenience for Kubernetes deployments and makes it a best practice for any deployment requiring access to other Google Cloud services. Now once an account is authenticated, there are two main ways to authorize which actions it can perform: with IAM and Kubernetes role-based access control, or RBAC. Think of IAM as the gatekeeper for Google Cloud resources beyond your Kubernetes clusters. It lets you define precise permissions for individual users or groups, which allows them to perform specific actions at the project or cluster level. Kubernetes RBAC controls access within your cluster, offering fine-tuned roles with granular permissions for resources at the cluster and namespace levels. To ensure comprehensive access control within your GKE environment, it’s essential to have a synchronized approach involving both IAM and Kubernetes RBAC. IAM acts as the outermost layer of security and defines who can access and modify the overall configurations of your GKE clusters, whereas Kubernetes RBAC governs who can interact with the individual Kubernetes objects residing within a cluster. It’s also crucial to adhere to the principle of least privilege, a cornerstone of security best practices. This means granting each user or group only the minimum set of permissions necessary to perform their specific tasks, minimizing potential risks. Now your workloads might get requests from outside the cluster, and each request must be authenticated before it’s acted upon. In Kubernetes, the API server listens for remote requests by using HTTPS on port 443. GKE manages end-user authentication for you. It will authenticate users to Google Cloud, set up the Kubernetes configuration, get an OAuth access token for the cluster, and keep the access token up-to-date. In Google Cloud, it’s best practice to authenticate using OpenID connect tokens.

### Video - [Kubernetes role-based access control](https://www.cloudskillsboost.google/course_templates/33/video/519804)

- [YouTube: Kubernetes role-based access control](https://www.youtube.com/watch?v=56IeTyPM6Ag)

Kubernetes Role-Based Access Control, or RBAC, is a built-in security mechanism that allows for granular control over user permissions within a Kubernetes cluster. Within GKE, RBAC works alongside IAM to strengthen security. While IAM manages access at the GKE and cluster levels, RBAC provides control over individual Kubernetes resources inside the cluster. Kubernetes RBAC can control who can do what with Kubernetes objects. Subjects, the who, are users, groups, or service accounts authorized to interact with the Kubernetes API server. Verbs, the what, define the specific actions allowed for each resource, such as creating, modifying, or viewing. And resources encompass the Kubernetes objects the subjects can access, like Pods, Deployments, or persistent volumes. Think of it as defining who gets the keys, what doors they can unlock, and what actions they can perform inside. By combining these elements, two types of RBAC API objects can be created, roles and RoleBindings. Both applied at the cluster or namespace level, roles connect API resources and verbs and RoleBindings connect roles to subjects. To use RBAC to grant users access to specific Kubernetes namespaces, you need to create roles and RoleBindings. First, you need to create the roles with proper permissions in each namespace. Then you need to create RoleBindings to bind subjects to the appropriate roles. So, how does this work in practice? Let’s explore how a manifest defines an RBAC role at the namespace level. A manifest includes fields of the apiVersion, kind, metadata, and rules, which are permissions granted by the role. Using the kind field, Role can be used to define a namespace-level role. Note that you can only define a single namespace for a role. Under the rules field, you need to define the permissions granted by the role. This includes apiGroups, which are API groups of resources. Leave this field empty for the core API group. The resources field specifies what resources the role can access, like “Pods”, and the verbs field specifies the permitted actions on those resources. Whereas RBAC Roles are defined at the namespace level, RBAC ClusterRoles are defined at the cluster level. In practice, this means defining the kind field of a manifest file with ClusterRole and leaving the metadata field blank, indicating a cluster-wide scope. Let’s explore some rule sets that specify the resources and verbs covered by a Role or ClusterRole. A resourceName is used to limit the scope to specific instances of a resource and the verbs specified as patch and update. A subresource ‘logs’ can be added to the resources list to specify access to Pod logs also. The combination of verbs get, list and watch are the standard read-only verbs. You should typically assign them as a unit, all or none. And nonResourceURLs can be used to specify get and post actions for non-resource endpoints. This form of rule is unique to ClusterRoles. After ClusterRoles are defined, RoleBindings and ClusterRoleBindings can be attached. To attach a RoleBinding, start by defining the namespace where it applies, then carefully specify the users, groups, or service accounts that will receive the permissions, paying attention to case sensitivity in their names. With RBAC in GKE, you can control access for various user types: individual Google accounts, service accounts tied to Google Cloud projects, and identities associated with your workloads through Workload Identity. All these entities are identified by their email addresses. To connect roles to users or groups, use the roleRef field within a RoleBinding. Specify both the role type–Role or ClusterRole–and its name. Remember, even ClusterRoles bound within a RoleBinding only grant permissions within the specific namespace, not the entire cluster. To identify both namespaced and non-namespaced resources, use the kubectl api-resources command, and specify --namespaced=true for namespaced resources and --namespaced=false for cluster-wide resources. Before creating Roles or ClusterRoles, you can verify the resource scope–namespace or cluster-wide–using kubectl api-resources to list "NameSpaced" resources. Typically, you should manage cluster-level resources with ClusterRoles and NameSpaced resources with Roles. For permissions spanning multiple namespaces, you'll likely need to use ClusterRoles.

### Video - [Workload Identity](https://www.cloudskillsboost.google/course_templates/33/video/519805)

- [YouTube: Workload Identity](https://www.youtube.com/watch?v=1zTZrQ19284)

GKE Workload Identity simplifies how containerized applications access Google Cloud services. Instead of managing service account keys in a GKE environment, containers can directly authenticate by using their dedicated credentials. This enhances security and streamlines access control for deployments. So, how do you use the Workload Identity feature? When you enable Workload Identity on the first cluster in a Project, GKE automatically creates the pool by using the format "PROJECT_ID.svc.id.goog". Any new node pools that you create have Workload Identity enabled by default. Containers deployed to those node pools are then able to use its service account credentials to authenticate to Google Cloud services, either through the Google Cloud Command Line Interface or by using the Application Default Credentials library. To use Workload Identity on individual node pools, it must first be activated at the cluster level. It’s useful to know that Autopilot clusters enable Workload Identity by default. A couple of commands are available to enable Workload Identity on clusters. The first is the gcloud container clusters create command, which can be used to enable Workload Identity on a new standard cluster. And second, there is the gcloud container clusters update command, which can be used to enable Workload Identity on an existing standard cluster. Before you migrate any applications to use Workload Identity, it’s a best practice to create a new node pool. After Workload Identity is enabled, you need to configure your applications to authenticate to Google Cloud using Workload Identity. You need to assign a Kubernetes service account to the application, and then configure that Kubernetes service account to act as an IAM service account. Now let’s explore the steps involved in configuring an application to use Workload Identity. The first step is to create a new namespace to use for the Kubernetes service account. You can use the kubectl create namespace command. Next you need to create a new Kubernetes service account for your application to use, as opposed to using an existing Kubernetes service account in any namespace, including the default service account. To accomplish this, use the kubectl create serviceaccount command. From there, create an IAM allow policy that references the Kubernetes ServiceAccount. To do this, use the add-iam-policy-binding gcloud command. You can use any IAM service account in any project in your organization. Be sure to grant permission to the specific Google Cloud resources that your application needs to access. The next step is to grant the required roles to the Service Account. For example, the role of Storage Object Viewer. And finally, add the name of the Service Account and identify which specific nodes the Pod should be scheduled to run on. This can be applied to your cluster by using a kubectl apply command.

### Video - [Kubernetes control plane security](https://www.cloudskillsboost.google/course_templates/33/video/519806)

- [YouTube: Kubernetes control plane security](https://www.youtube.com/watch?v=pI2LzujVh2E)

The Kubernetes control plane, which is the brain of a cluster, is critical for maintaining the health and operation of containerized applications. Securing it is paramount, because any compromise can grant attackers access and control over an entire cluster. In GKE, Google manages all of the control plane components. This includes the API server (kube-apiserver), etcd (distributed key-value store), and the controller manager. Each GKE cluster has a dedicated root certificate authority (CA) for security, and Google internally manages the root keys for these CAs to ensure their integrity. Secure communications between the control plane and nodes in a cluster relies on the shared root of trust provided by the certificates issued by this CA. Let’s explore how GKE implements security features on the control plane. The first is that GKE isolates etcd databases from other cluster services by using a dedicated certificate authority for each cluster. This means that even if a service is compromised, the trust and security of the etcd databases remain independent, which prevents potential breaches from spreading. Then there are kubelets, which are the primary Kubernetes agents located on nodes. Kubelets maintain secure communication with the API server through trusted certificates issued by the cluster's root CA. This mechanism safeguards the confidentiality and integrity of interactions between Kubelets and the API server. So, how does this work? When a new node is created, it’s provisioned with a shared secret. A "secret" refers to a special type of Kubernetes object used to securely store sensitive information, such as credentials or encryption keys. This secret enables the kubelet on each node to initiate certificate signing requests to the cluster's root CA. Nodes then acquire client certificates upon joining the cluster and obtain fresh certificates when renewal or rotation is required. An important control plane security feature is credential rotation, which is the practice of regularly rotating credentials for GKE control plane components and nodes to minimize the window of opportunity for attackers who might exploit compromised credentials. How frequently you rotate credentials will depend on your organization's security policies. For example, if your cluster manages high-value assets, you might want to rotate credentials more often, but not so often that it causes disruption to your cluster. The credential rotation process in GKE involves several steps. The first step is to create a new IP address for the cluster control plane. The second step is to issue new certificate credentials to the control plane with the new IP address. The third step is to update the nodes to use the new IP address and credentials. The fourth step is to update all API clients outside the cluster to use the new credentials. And the final step to complete the rotation is to remove the old IP address and old credentials. It’s important to note that to prevent your cluster from entering an unrecoverable state if your current credentials expire, GKE will automatically start a credential rotation within 30 days of your current CA expiry date. Now let’s explore a couple of the commands that can help complete these steps. To initiate a credential rotation, use the start-credential-rotation gcloud command, which will create new credentials and issue them to the control plane. To complete a rotation, use the complete-credential-rotation gcloud command. This command configures the control plane to serve only with the new credentials. Please know that during credential rotation, the cluster API server may experience a brief interruption, temporarily pausing new Pod creation. Running Pods will remain unaffected, which ensures ongoing operations. However, to avoid deployment delays, consider scheduling Pod creation before or after the rotation process. Whereas credential rotation helps secure your GKE cluster, it’s important to make additional security considerations. Consider fine-tuning permissions by granting the cluster's service account minimum privileges through IAM, disabling outdated access by turning off legacy metadata APIs for extra security, and hiding sensitive data by enabling metadata concealment to protect hidden credentials. A layered approach to security is key for robust GKE cluster protection.

### Video - [Pod security](https://www.cloudskillsboost.google/course_templates/33/video/519807)

- [YouTube: Pod security](https://www.youtube.com/watch?v=RkQIEXuajGs)

To ensure a robust and secure GKE environment, Pod security is paramount. Google Kubernetes Engine offers two primary mechanisms for achieving this. The first is Pod Security Standards (PSS) and the second is Pod Security Admission (PSA). Pod Security Standards are a set of predefined security configurations for Pods, which offers a flexible framework for securing Kubernetes clusters. These predefined security policies range from permissive to highly restrictive, so you can match your security measures to your unique needs. GKE supports three predefined Pod Security Standards levels. The first level is privileged, which grants unrestricted access to the system. While powerful, it can also introduce significant security risks. It’s primarily used for Pods that require elevated privileges for tasks like debugging or system administration. The second level is baseline, which offers a balance between security and flexibility, and prevents known privilege escalations while allowing common Pod operations. It’s recommended for most production workloads. And the third level is restricted, which enforces the most stringent security measures, severely restricting Pod behavior to minimize potential security risks. It’s most suitable for Pods that handle sensitive data or run in highly sensitive environments. Now let’s shift our focus to the other way GKE ensures Pod security, which is with Pod Security Admission. Pod Security Admission is a Kubernetes admission controller specifically designed for GKE that uses Pod Security Standards to simplify and automate Pod security enforcement. Pod Security Admission functions as a gatekeeper, intercepting Pod creation requests and evaluating the Pod's security context against the specified PSS policy for the namespace. If the Pod doesn’t comply with the policy, the request is rejected, preventing the Pod from being created. This ensures that only Pods that adhere to the defined security standards are allowed to run within the cluster. Combining Pod Security Standards and Pod Security Admission presents a list of advantages for GKE Pod security. Let’s explore that list. PSS makes security easier by offering ready-to-use policies, saving you the time of writing your own security rules. This makes security management less complicated and less prone to mistakes. PSA enforces consistent security by guaranteeing that all Pods follow PSS policies, keeping your cluster secure and preventing breaches. By enforcing stricter Pod security measures, the attack surface is reduced, minimizing potential vulnerabilities and making it more difficult for attackers to exploit weaknesses. And finally, adherence to PSS policies can help organizations meet compliance requirements, which ensures that their Kubernetes environments align with industry standards and regulatory mandates. Now that you know the benefits of PSS and PSA in GKE, let’s look at how you actually implement them. The first step is to enable the PodSecurityPolicy feature in your GKE cluster. This feature provides the foundation for PSA to enforce PSS policies. Next, define the appropriate PSS policy for each namespace based on its security requirements. This requires selecting the appropriate level of restrictiveness–privileged, baseline, or restricted–for each namespace. And finally, assign the chosen PSS policies to each respective namespace. After the PSS policies are applied, it’s important to regularly monitor Pod security compliance and adapt policies as needed. This ensures that Pods remain compliant with the defined security standards, and it allows for policy adjustments to be made when needed.

### Lab - [Securing Google Kubernetes Engine with IAM and Pod Security Admission](https://www.cloudskillsboost.google/course_templates/33/labs/519808)

Architecting with Google Kubernetes Engine: Securing Kubernetes Engine with IAM and Pod Security Admission

- [ ] [Securing Google Kubernetes Engine with IAM and Pod Security Admission](../labs/Securing-Google-Kubernetes-Engine-with-IAM-and-Pod-Security-Admission.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/33/quizzes/519809)

## Google Kubernetes Engine Logging and Monitoring

In this section of the course, you'll learn how to monitor and log your Kubernetes applications using Google Cloud's observability tools. You'll configure Google Cloud Observability to monitor the availability and performance of your applications and gain hands-on experience inspecting logs using both the kubectl command and the Google Cloud Observability tools. You'll then learn how to configure GKE-native monitoring and logging, allowing you to proactively identify and troubleshoot issues within your Kubernetes clusters.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/33/video/519810)

- [YouTube: Introduction](https://www.youtube.com/watch?v=Y-kgN4qackE)

Logging and monitoring are crucial for maintaining stable and healthy GKE deployments. They provide deep visibility into cluster and application behavior, helping you proactively identify any issues, debug them quickly, and optimize resource utilization. In this section of the course, titled “Google Kubernetes Engine Logging and Monitoring,” you’ll explore how Google Cloud Observability is used to manage the availability and performance of your GKE resources; examine the benefits and features available with Cloud Logging and Cloud Monitoring; inspect logs with the kubectl command; inspect logs with Cloud Logging and Logging Agents; and get hands on practice configuring GKE-Native Monitoring and Logging.

### Video - [Google Cloud Observability](https://www.cloudskillsboost.google/course_templates/33/video/519811)

- [YouTube: Google Cloud Observability](https://www.youtube.com/watch?v=en5EYa0qIJI)

If you've ever worked with on-premises environments, you know that you can physically touch the servers. If an application becomes unresponsive, someone can physically determine the cause. In the cloud though, the servers aren't yours—they belong to the cloud provider—and you can’t physically inspect them. So the question becomes: how can you know what's happening with your applications? The answer is: by using Google’s integrated observability tools. Observability involves collecting, analyzing, and visualizing data from various sources within a system to gain insights into its performance, health, and behavior. To achieve this, Google Cloud offers observability tools for monitoring, logging, and diagnostics. It’s a unified platform for managing and gaining insights into the performance, availability, and health of applications deployed on Google Cloud. Let's explore some of the managed services that constitute Google Cloud Observability. The first is Cloud Logging, which collects and stores all application and infrastructure logs. With real-time insights, Cloud Logging can help to troubleshoot issues, identify trends, and comply with regulations. Cloud Monitoring provides a comprehensive view of your cloud infrastructure and applications. It collects metrics from your applications and infrastructure, and provides insights into their performance, health, and availability. It can also be used to create alerting policies to notify you when metrics, health check results, and uptime check results meet specified criteria. Graphs produced in Cloud Monitoring provide visual indication for when events have occurred, such as performance bottlenecks or service dependencies. Cloud Monitoring can also be integrated with many third-party products. Google Cloud Managed Service for Prometheus is Google Cloud's fully managed, multi-cloud, cross-project solution for Prometheus metrics. It lets you globally monitor and alert on your workloads, using Prometheus, without having to manually manage and operate Prometheus at scale. Cloud Trace helps identify performance bottlenecks in applications. It collects latency data from applications, and provides insights into how they’re performing. Cloud Profiler identifies how much CPU power, memory, and other resources an application uses. It continuously gathers CPU usage and memory-allocation information from production applications and provides insights into how applications are using resources. And finally, there’s Error Reporting, which counts, analyzes, and aggregates the crashes in running cloud services in real-time. A centralized error management interface displays the results with sorting and filtering capabilities. A dedicated view shows the error details, which include a time chart, occurrences, affected user count, first and last-seen dates, and a cleaned exception stack trace. Error Reporting supports email and mobile alerts through its API. Google's integrated observability tools offer valuable insights into the performance and health of applications and infrastructure in the cloud. Let’s refer back to two important terms that were mentioned when introducing Cloud Monitoring, events and metrics, and take a moment to differentiate between the two. A metric represents a value that can be monitored, such as CPU or disk usage. They might be “gauge values,” which fluctuate up or down over time, or “counters,” which are values that increase over time. Monitoring metrics can help identify bottlenecks, as they’re specific to a resource. And then there are events, which represent occurrences that happen to a cluster, Pod, or container. Examples include the restart of a Pod, node, or service; when the number of deployments in a cluster is scaled up or down; or when an application responds to a request. Events typically report successes, warnings, or failures, while metrics report numerical values.

### Video - [Cloud Logging](https://www.cloudskillsboost.google/course_templates/33/video/519812)

- [YouTube: Cloud Logging](https://www.youtube.com/watch?v=U36FseRo_V4)

Logging with Google Kubernetes Engine is the process of collecting and storing a record of events that occur in a GKE cluster. Often thought of as a passive form of systems monitoring, logging collects events logs that can be used, for example, to identify patterns or perhaps help track down the root cause of a system failure. Google Kubernetes Engine (GKE) logs and logs produced by Cloud Logging, both provide information to analyze your Google Cloud infrastructure. However, there are some key differences between the tools. Cloud Logging is a fully managed logging service that can be used to collect logs from a variety of sources, including Google Cloud services, like GKE, on-premises infrastructure, and third-party applications. Cloud Logging can be used to consolidate all of your logs in one place and analyze them for trends. Alternatively, a GKE log is a record of information about cluster events, container events, Pod events, node events, audit events, and general system details. The main types of GKE logs are: System, Workloads, API server, Scheduler, Controller Manager. You can find more information on the contents of each log type in the Google Cloud documentation. The /var/log directory in GKE is the default location for storing logs generated by applications and system processes that run on GKE nodes. This directory is not meant for long-term storage of logs, because it might be deleted when the node is deleted or when the disk on which it is stored runs out of space. For long-term storage and analysis of logs, it is recommended to use Cloud Logging. The first step to inspecting a log is to view it, for which there are two options. The first option is to use the kubectl command to display logs directly from a Pod. This option is quick, but it’s worth noting that logs aren’t saved to any database and may be lost if containers are restarted or deleted. The second option is to use Cloud Logging, which provides a single interface to review logs from containers, nodes, and other system services. This allows more options to broaden your visibility of GKE events to help correlate issues. Cloud Logging is a paid-for product, but offers a free tier that stores the first 50 gibibytes of log data for free for 30 days. This storage is in a bucket named _Default, which stores Data Access audit logs and Policy Denied audit logs. Admin audit logs are held for 400 days in a storage bucket named _Required, which stores logs including Admin Activity audit logs, System Event audit logs, and Login Audit logs. For longer retention options, user-defined log buckets can be created, and can have a retention period of up to 3650 days. Logs can also be exported to BigQuery.

### Video - [Cloud Monitoring](https://www.cloudskillsboost.google/course_templates/33/video/519813)

- [YouTube: Cloud Monitoring](https://www.youtube.com/watch?v=CqWQMZggDA0)

Monitoring and logging are often coupled to provide a complete picture of the system status and past trends. Monitoring can also reveal trends and patterns that can help size and scale systems over time. When it comes to monitoring Kubernetes, it’s less about watching the clusters and nodes, and more about monitoring your application’s current state—and anything that might be impeding it. If you’re able to accurately monitor the state of these services, including the throughput and latency, you’re more likely to identify performance bottlenecks. For example, through aggregated logging and debugging, you can diagnose application code issues. In Kubernetes, monitoring can be broken into two domains. One domain is the cluster, which involves monitoring the cluster-level components like individual nodes, kube-APIserver, and kube-controller-manager. The second monitoring domain is the Pods, including the containers and the applications that run inside them. Cluster monitoring refers to the cluster services, nodes, and other infrastructure elements. This can be accomplished using the monitoring in the Cloud Console, or through Cloud Monitoring, using health checks and dashboards. Pod Monitoring can be divided into several sub-categories. System metrics include container deployments, instances, health checks, and state. Container-specific metrics such as the resource consumption. And application-specific metrics, which are designed by the application developer and exposed to a monitoring solution. This is an ideal use case for Google Cloud Managed Service for Prometheus. Users can export their metrics as Prometheus metrics to access the advanced monitoring features of Prometheus. Unlike traditional server monitoring, where you specify a hostname to monitor, the abstraction layers in Kubernetes—well, containers in general—require a different approach. Instead of having a specific hostname or IP address to be monitored, all resources in Kubernetes are labeled. These labels provide a logical approach to organizing resources, and also make it easier to monitor specific systems or subsystems by selecting a combination of different labels. In Cloud Monitoring, as well as other tools, you can filter logs by label. A Kubernetes label consists of a key and a value. This means that if you apply a label with the key defined as “environment” and the value defined as “production” to all the components of your production environment, you can use that label in Cloud Monitoring. Cloud Monitoring provides visibility into the overall health of your GKE cluster and the Pods it contains through custom dashboards. Key features include the ability to use uptime checks to monitor the availability of Kubernetes resources within a cluster as well as control plane metrics, along with other Google Cloud resources that are being used. Integration with Cloud Logging to leverage custom metrics and even monitor your logging-based custom metrics. The ability to use custom metrics to create alerting policies, which can improve the signal-to-noise ratio of your alert messages. Cloud Monitoring is optimized for Kubernetes, because it lets you observe your entire Kubernetes environment to see metrics, logs, traces, events, and metadata in one place. Imagine that your application has an error. With Cloud Monitoring, you can analyze infrastructure metrics, application metrics, or even logs directly from a single dashboard. It supports multi-cluster monitoring within the Google Cloud environment, along with other cloud environments and on-premises Kubernetes environments. And it offers extended direct support to Prometheus. Cloud Monitoring also provides direct integration to Google Cloud Managed Service for Prometheus, which is an open-source monitoring and alerting toolkit that collects and stores time-series data. Cloud Monitoring can only monitor what it can see. So, to extract more information from Cloud Monitoring, you need to add more information. That’s where Prometheus can help. Prometheus can provide detailed metrics about Kubernetes components, including metrics from within the applications running in your Pods, and then expose those metrics to Cloud Monitoring, which provides you with much more granular detail than Cloud Monitoring alone.

### Video - [Inspecting logs with the kubectl command](https://www.cloudskillsboost.google/course_templates/33/video/519814)

- [YouTube: Inspecting logs with the kubectl command](https://www.youtube.com/watch?v=6XBlW1VYBPI)

Let’s explore how to inspect Kubernetes logs with the kubectl command. Regarding Kubernetes-native logging, logs from Kubernetes system components, such as kubelet and kube-proxy, are stored in the /var/log directory of each node’s file system. Messages written by each container to its standard output and standard error are also logged in the /var/log directory. This allows log messages to be captured and centrally managed. Let’s explore some common kubectl commands that can be used to produce GKE logs. To view the logs of a specific Pod, run the kubectl logs command, followed by the name of the Pod. If you don't know the exact name of the Pod, or if the Pod is part of a Deployment, you can run the kubectl get pods command to get a list of all the available Pods. When troubleshooting an issue, you might only need to inspect the most recent log from a specific Pod or container. To limit the number of logs returned, add the --tail option to the kubectl logs command. By adding –tail=20, the output would be limited to the 20 most recent lines. But there might be instances when you don’t actually know how many log messages to retrieve, but you do know the problem occurred in the last few hours. To restrict the output based on time, you can add the --since option, followed by the time period. For example, --since=3h would return all logs for a specific Pod or container from the last 3 hours. Now let’s say you need to view a previous instantiation of a container before it crashed. This can be accomplished by adding the --previous option. If your Pod has multiple containers, you’ll need to specify which container’s log the kubectl command should return. These native Kubernetes logs, and the compressed archives aren’t persistent. If, for any reason, a Pod is evicted or restarted, logs and the archives associated with that Pod are lost. As a result, there’s a need to store logs outside a container, Pod, or node. This is called cluster-level logging. As mentioned earlier, Kubernetes itself doesn’t offer any log storage solution, but it does support various implementations. In GKE this is handled for you by the integration with Cloud Logging. When a container starts on a node, a log file is created. Node-level logging implements log archiving using a rotation mechanism. As the container runs, events happen and the log file grows. Either once per day, or when the log file reaches 100 MB (whichever comes first) the logrotate utility creates a new log and compresses the old log file, saving it into an archive. It then deletes all but the five most recent compressed log archives. This ensures that logs don’t consume all of the available storage on the node. If a container restarts, the default behavior of kubelet is that it keeps one terminated container with its logs. However, all log events are streamed to Cloud Logging, which retains all log event data in the _default bucket for 30 days by default. If your organization requires you to retain log data for longer than 30 days, configure Cloud Logging to export the data to long-term storage such as BigQuery or Cloud Storage.

### Video - [Inspecting logs with Cloud Logging and Logging Agents](https://www.cloudskillsboost.google/course_templates/33/video/519815)

- [YouTube: Inspecting logs with Cloud Logging and Logging Agents](https://www.youtube.com/watch?v=Hl5_9Z-07PA)

Cloud Logging is a fully managed service that stores, searches, analyzes, monitors, and alerts on logging data and events from Google Cloud. It’s designed to automatically scale and ingest terabytes of log data per second. An event describes an operation that takes place in a cluster. Some examples include deleting a Pod, scaling a Deployment, or creating a container. Events are stored as API objects on the cluster control plan, and because events are only stored temporarily in Kubernetes, GKE deploys an event exported in the cluster control plane to capture events and push them into Cloud Logging. GKE integration with Cloud Logging is enabled by default, though it can be disabled on a cluster if required. Cloud Logging agents are preinstalled on nodes and pre-configured to export log data to Cloud Logging. Cloud Logging also offers an API that can be used to write a custom log and push it to Cloud Logging. Additionally, logs can be filtered by using Cloud filter language, either in the Logs Viewer console or directly through the Cloud Logging API. GKE installs a logging agent on every node of a cluster, and this agent collects and exports container logs and system component logs to the Cloud Logging backend. The node logging agent used by Cloud Logging is FluentBit. FluentBit is a log aggregator that can display logs, adding helpful metadata, and then continuously export those logs to Cloud Logging. FluentBit uses DaemonSet, as that ensures that every node in a cluster runs a copy of a specific Pod. The configuration of the FluentBit agent is managed through ConfigMaps. This increases the scalability of the implementation by separating the application (the FluentBit DaemonSet) from the configuration elements (the ConfigMap).

### Lab - [Configuring GKE-Native Monitoring and Logging](https://www.cloudskillsboost.google/course_templates/33/labs/519816)

Architecting with Google Kubernetes Engine: Configuring GKE-Native Monitoring and Logging Engine native Monitoring and Logging

- [ ] [Configuring GKE-Native Monitoring and Logging](../labs/Configuring-GKE-Native-Monitoring-and-Logging.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/33/quizzes/519817)

## Using Google Cloud Managed Storage Services with Google Kubernetes Engine

In this section of the course, you'll explore the storage and database options available for your Kubernetes applications on Google Cloud. You'll compare managed and self-managed storage, learn about Cloud Storage for Kubernetes, and get an overview of Google Cloud's managed database services. You'll then learn how to securely connect to Cloud SQL from your GKE clusters and gain hands-on experience integrating Cloud SQL with Google Kubernetes Engine.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/33/video/519818)

- [YouTube: Introduction](https://www.youtube.com/watch?v=dmGvnq-v8bQ)

In Google Kubernetes Engine, you can build your own storage solutions by attaching volumes to containers. However, these solutions require you to manage and protect your data. Another option is to lean on Google to manage the availability and performance of your data by using Google Cloud managed storage services. In this section, titled “Using Google Cloud managed storage services with Google Kubernetes Engine,” you’ll contrast managed storage services with self-managed storage; identify Cloud Storage use cases; compare Google Cloud managed database services; explore Cloud SQL Auth Proxy and learn how it connects to Cloud SQL from within GKE; and use Cloud SQL with Google Kubernetes Engine.

### Video - [Using Google Cloud Services](https://www.cloudskillsboost.google/course_templates/33/video/519819)

- [YouTube: Using Google Cloud Services](https://www.youtube.com/watch?v=ctqQxjomjl0)

Google Kubernetes Engine offers several storage options for applications in your cluster, including storage abstractions like Volumes and PersistentVolumes. Let’s say you’re building and deploying a MySQL server as a container. To store the database files, you use a GKE Volume based on Persistent Disks. The disadvantage of this approach is that you are responsible for managing the application lifecycle and building a resilient and reliable service. Another option is to use Google Cloud’s fully managed database and storage services. Google Cloud offers relational database, non-relational database, and object storage services that help remove operational management burden. To access managed storage services, applications running in a GKE Kubernetes cluster must be capable of communicating with Google Cloud APIs. After the relevant API is enabled, the application will use granted credentials for authentication and authorization to perform tasks. To help, Workload Identity can be used to simplify and secure process management on your cluster. Every action in Google Cloud must be authenticated and authorized. Workload Identity uses dedicated credentials to directly authenticate, which eliminates the need to manage a collection of service account keys for each Google Cloud API you want to use. And both GKE and IAM have service accounts to provide an additional layer of security. IAM Service Accounts, which are defined within Google Cloud instead of GKE, are used for Google Cloud API authentication and authorization for applications. After Workload Identity is enabled on your cluster, an IAM policy can be used to bind an IAM service account to a GKE service account. From there, you can annotate the Kubernetes ServiceAccount object with the name of the IAM service account, and assign the GKE service account to the workloads that will use the Google Cloud APIs. The IAM service account must have an IAM role that matches your application’s required permissions. Separate service accounts improve security and simplify the management, monitoring, and auditing of API requests. For example, if you need to revoke API access for a specific application, you can delete the service account associated with that application instead of having to revoke access to a shared service account, which would impact multiple applications. Instead of using the default Compute Engine service accounts, you can create a dedicated service account for each application that interacts with Google Cloud services, then grant each service account only the specific IAM roles and permissions it needs. Separate service accounts also provide protection in the event of a security breach. If an attacker notices many service accounts, the value of each will appear to be less valuable.

### Video - [Using Cloud Storage](https://www.cloudskillsboost.google/course_templates/33/video/519820)

- [YouTube: Using Cloud Storage](https://www.youtube.com/watch?v=CCIZOBKq5Nc)

Objects in GKE are groups of bytes, and Cloud Storage is an object storage service that can store those bytes. The structure and semantics, however, are determined by the application. Cloud Storage is commonly used for media hosting, such as serving images for a website or streaming music and videos. For example, a mobile app deployed on GKE might need to access images from Cloud Storage buckets. Cloud Storage can also be used as a data lake for analytics and machine learning workloads, such as genomics and data analytics. Cloud Storage can store unstructured and structured files and data, but it is not optimized for querying. It’s also not designed to be a file system, so it doesn’t replace Persistent Volumes. There are four primary storage classes in Cloud Storage. The first is Standard Storage. Standard Storage is considered best for frequently accessed, or “hot,” data. It’s also great for data that’s stored for only brief periods of time. The second storage class is Nearline Storage. This is best for storing infrequently accessed data, like reading or modifying data on average once a month or less. Examples might include data backups, long-tail multimedia content, or data archiving. The third storage class is Coldline Storage. This is also a low-cost option for storing infrequently accessed data. However, compared to Nearline Storage, Coldline Storage is intended for reading or modifying data, at most, once every 90 days. And the fourth storage class is Archive Storage. This is the lowest-cost option, used ideally for data archiving, online backup, and disaster recovery. It’s the best choice for data that you plan to access less than once a year, because it has higher costs for data access and operations and a 365-day minimum storage duration. Cloud Storage offers robust access control features to protect your data. Before it’s written to disk and stored, your data is encrypted on the server-side with either customer-supplied or customer-managed encryption keys. Cloud Storage can also be integrated with Pub/Sub, a fully-managed real-time messaging service, to notify applications whenever changes are made to a Cloud Storage bucket. And you can also enable versioning for Cloud Storage. After Google Cloud APIs are turned on for your applications, they can be used to access Cloud Storage. Please note that your application will need to use a service account to access the service you want to use. Cloud Storage ensures any data written is instantly available for reading. This is ideal for Kubernetes applications where different Pods need to quickly and reliably share information.

### Video - [Using Google Cloud Databases](https://www.cloudskillsboost.google/course_templates/33/video/519821)

- [YouTube: Using Google Cloud Databases](https://www.youtube.com/watch?v=yfQwvAWTF24)

Google Cloud's database services primarily serve two purposes. The first is to support online application data storage and retrieval, and the second is to provide an analytics database optimized for data mining and trend analysis. Google Cloud offers relational and non-relational database options for applications. SQL-based relational databases store and provide access to data points that are related to one another. This includes built-in mechanisms to ensure the consistency and integrity of your database structure. A non-relational database, sometimes known as a NoSQL database, is less structured in format and prioritizes flexibility over built-in data integrity enforcement. This means your application needs to take on the task of ensuring data quality. Let’s explore five Google Cloud database services– Bigtable, Firestore, Spanner, Cloud SQL, and BigQuery. Bigtable, Google Cloud’s NoSQL database service, is designed to handle massive workloads at consistent low latency and high throughput. Bigtable powers many Google services like Search, Analytics, Maps, and Gmail, and is an excellent option for operational and analytical applications. When choosing which storage option to use, consider Bigtable for storing and accessing more than one terabyte of semi-structured or structured data; high throughput or rapidly changing data; data transactions where strong relational semantics are not required; time-series data or data with natural semantic ordering; running asynchronous batch or synchronous real-time processing on big data; or running machine learning algorithms on the data. Bigtable can interact with other Google Cloud services and third-party clients. APIs let you read and write Bigtable data by using data service layers like Managed VMs, HBase REST Server, or Java Servers with the HBase client. This data is commonly used by applications, dashboards, and data services. Bigtable supports both streaming and batch data ingestion. For real-time data, you can use popular frameworks like Dataflow Streaming Engine, Spark Streaming, or Storm, and for batch processing, Hadoop MapReduce, Dataflow, and Spark are options. It’s worth noting that often processed data, whether summarized or newly calculated, is written back to Bigtable or another database for further use. Now let’s switch your focus to Firestore, which is a flexible, scalable non-relational database commonly used for mobile, web, and server development. Firestore can process NoSQL queries to retrieve a single or multiple documents within a collection that matches your criteria. Queries can be filtered and sorted in Firestore, and are indexed by default, so query performance is dependent on the result set instead of the dataset. Firestore syncs data across all connected devices, but it’s also optimized for simple, one-time queries. It uses data caching to enable an application to write, read, monitor, and query data even while a device is offline. Next up is Spanner, which is a SQL-based, fully managed, scalable relational database. It’s the same database that powers Google’s mission-critical applications and services. Spanner has high availability, is globally consistent, and is capable of performing tens of thousands of read/write operations per second. We’ll discuss the other relational database, Cloud SQL, shortly. And finally, there is BigQuery, which is Google Cloud’s fully managed data warehouse offering. It provides a data warehousing backend for modern business intelligence solutions, and is used to guide management decisions. Because it’s fully managed, BigQuery manages the technical aspects of storing structured data, such as compression, encryption, replication, performance tuning, and scaling. With BigQuery, you can integrate, transform, analyze, and visualize your data by using Google and third-party tools. BigQuery excels at storing and analyzing massive amounts of event and sensor data generated by IoT (Internet of Things) applications. Its optimization for 'write-once, read-many' data patterns makes it ideal for handling this specific type of workload. And it even separates storage and compute costs so you only pay for query execution, not for data at rest.

### Video - [Using Cloud SQL and SQL Auth Proxy](https://www.cloudskillsboost.google/course_templates/33/video/519822)

- [YouTube: Using Cloud SQL and SQL Auth Proxy](https://www.youtube.com/watch?v=II0T_087_3c)

Cloud SQL is a fully managed database service that simplifies the process of setting up, maintaining, and scaling relational databases. Available for MySQL, PostgreSQL, and Microsoft SQL Server, Cloud SQL is designed to hand off mundane, but necessary and often time-consuming, tasks to Google—like applying patches and updates, managing backups, and configuring replications—so your focus can be on building great applications. And then there is the Cloud SQL Auth Proxy, which is a tool designed to streamline and secure connections to your Cloud SQL database instances. It acts as a middleman between your application and your database, eliminating the need to manage complex network configurations or expose your database directly to the internet. In-transit traffic is automatically encrypted, and authentication is handled with SQL. All Pods that compose your application can reliably access the database, even if they're dynamic. In GKE, Cloud SQL Auth Proxy is set up as a “sidecar” container in the same Pod that contains your application. Your application can communicate with the Cloud SQL Auth Proxy container using the localhost network address. To complete this setup, you'll first need to enable the necessary API. This includes Cloud SQL API, and sqladmin API. Next, create an IAM service account. And third, use Workload Identity to link the IAM service account to a Kubernetes service account. Be sure to use Secrets to securely provide credentials to your Pods.

### Video - [Comparing storage options](https://www.cloudskillsboost.google/course_templates/33/video/519823)

- [YouTube: Comparing storage options](https://www.youtube.com/watch?v=l0h1Hcf_8OE)

So, you’ve learned about the different storage options that Google Cloud offers, but in what scenarios should you use each one? Ultimately, it’s a combination of the data type that needs to be stored and the business need. If data is unstructured, then Cloud Storage is the most appropriate option. You have to decide on a storage class: Standard, Nearline, Coldline, or Archive. Or whether to let the Autoclass feature decide that for you. If data is structured or semi-structured, choosing a storage product will depend on whether workloads are transactional or analytical. Transactional workloads stem from online transaction processing, or OLTP systems, which are used when fast data inserts and updates are required to build row-based records. An example of this is point-of-sale transaction records. Then there are analytical workloads, which stem from online analytical processing, or OLAP systems, which are used when entire datasets need to be read. They often require complex queries, for example, aggregations. An example here would be analyzing sales history to see trends and aggregated views. After you determine if the workloads are transactional or analytical, you must determine whether the data will be accessed by using SQL. So, if your data is transactional and you need to access it by using SQL, then Cloud SQL and Spanner are two options. Cloud SQL works best for local to regional scalability, and Spanner is best to scale a database globally. If the transactional data will be accessed without SQL, Firestore might be the best option. Firestore is a transactional NoSQL, document-oriented database. If you have analytical workloads that require SQL commands, BigQuery might be the best option. BigQuery, Google’s data warehouse solution, lets you analyze petabyte-scale datasets. Alternatively, Bigtable provides a scalable NoSQL solution for analytical workloads. It’s best for real-time, high-throughput applications that require only millisecond latency.

### Lab - [Using Cloud SQL with Google Kubernetes Engine and Workload Identity](https://www.cloudskillsboost.google/course_templates/33/labs/519824)

Connect a GKE cluster to Cloud SQL using Workload Identity and a sidecar proxy.

- [ ] [Using Cloud SQL with Google Kubernetes Engine and Workload Identity](../labs/Using-Cloud-SQL-with-Google-Kubernetes-Engine-and-Workload-Identity.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/33/quizzes/519825)

## Using CI/CD with Google Kubernetes Engine

In this section of the course, you'll discover the benefits of Continuous Integration and Continuous Delivery (CI/CD) for streamlining your development and deployment workflows. You'll learn what CI/CD is, why it's important, and how it can optimize application releases. You'll explore CI/CD tools supported by Google Cloud and learn Google's best practices for building CI/CD pipelines on Google Kubernetes Engine.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/33/video/519826)

- [YouTube: Introduction](https://www.youtube.com/watch?v=udPO7TvY88o)

The concept of a development pipeline for testing and deploying new iterations of software is not a new one. With Google Cloud, using continuous integration and continuous deployment pipelines, commonly referred to as simply CI/CD, helps create efficient workflows for maintaining and updating applications. In this final section of the course titled, “Using CI/CD with Google Kubernetes Engine,” you’ll explore CI/CD; examine how to construct a CI/CD pipeline; identify CI/CD products offered by Google Cloud and third parties; learn about best practices for using CI/CD on Google Cloud; and create a continuous delivery pipeline by using Cloud Deploy and Skaffold.

### Video - [What is CI/CD?](https://www.cloudskillsboost.google/course_templates/33/video/519827)

- [YouTube: What is CI/CD?](https://www.youtube.com/watch?v=zLwb_ylCriY)

So, what is CI/CD, and why is it a useful software development technique? Continuous integration and continuous delivery, which is what CI/CD stands for, is a software development practice that involves automating the process of building, testing, and deploying code changes. This allows developers to deliver updates to users more frequently and reliably. Let’s define each term. Continuous integration refers to the practice of frequently merging code changes into a central repository. This repository then undergoes automated builds and tests to ensure the code remains functional after each integration. And then there is continuous delivery, which is a method where teams produce software in short cycles and use processes that ensure the reliable release of software to production at any time. This helps minimize the risk of loss of service quality. For some companies, this means pushing releases to production several times a day. Now that you know what CI/CD stands for, let’s explore some of the benefits. For starters, CI/CD can help detect bugs sooner, which can prevent them from turning into larger problems later in the development cycle. For example, if all of your users have the same version of the application, developers can test updates on a percentage as small as 1%. Feedback can be used from that 1% to identify and fix any bugs or anomalies in the new update, so that when you roll the update out to the other 99% of users, very few bugs will be visible to them. CI/CD can also help improve code quality, because frequent integration and testing can encourage developers to write cleaner and more robust code. It can help enhance collaboration since teams work on a shared codebase. And finally, CI/CD can also help reduce the risk of regressions, because merging code frequently minimizes the chances of breaking existing functionalities.

### Video - [Constructing a CI/CD pipeline](https://www.cloudskillsboost.google/course_templates/33/video/519828)

- [YouTube: Constructing a CI/CD pipeline](https://www.youtube.com/watch?v=h67lMdAPsxA)

Now that you’ve been introduced to what CI/CD is, let’s explore how to construct and implement a CI/CD pipeline. A pipeline represents the stages involved in getting an application’s codebase ready to be released to production. Pipelines can be triggered manually or automatically. Let’s explore each stage of the pipeline. The first stage is build. During the build stage, the codebase is checked out to a specific version, and artifacts, for example Docker container images, are built. The deploy stage is next. This is when artifacts are deployed into a test environment that replicates the production environment. After the application has been deployed, it’s moved to the test stage. During this stage the application is rigorously tested to ensure its quality is high. The most common types of tests are unit, functional, integration, vulnerability, and performance. And finally, if the application passes every test, it moves on to the approve stage. During this stage, developers will decide if a pipeline should proceed to the production environment. Now it’s important to understand that there is no single version of a CI/CD pipeline, yet rather a variety that represents different types of implementations, which range from manual to almost completely automated. With manual pipelines–in other words, one that has no CI/CD–each developer is responsible for integrating and deploying all of the code that they write. They must also ensure that the new code does not conflict with any other developer’s code. Another less manual version of a pipeline is a packaged tool with a fixed set of built-in automation. This type of pipeline is completely reliant on a CI/CD tool. While these tools generally allow for very specific customization, it can make development difficult to frequently maintain and review. It’s not suitable for complex systems. Then there is dev-centric, or developer-centric, CI/CD, which centers the pipeline around the system’s code repository. An example of this would involve using GitOps, which is a methodology that stores both the code and configuration files in a source repository and then uses that source repository as the authority for an application. Within a Kubernetes environment, manifest and YAML files are stored and used to define the deployments for an application. To create these types of pipelines in Google Cloud, you can use Cloud Build. And the final pipeline example you’ll explore is ops-centric, or operations-centric, CI/CD, which prioritizes automating operational tasks like infrastructure management, monitoring, and incident response to ensure smooth and reliable software delivery. By streamlining operations, developers can free up time to focus on code. Spinnaker is a tool commonly used to implement ops-centric CI/CD, which is designed to handle very large deployments, with possibly thousands of instances. The cost of managing Spinnaker can be justified based on the scale of the deployment.

### Video - [CI/CD tools available in Google Cloud](https://www.cloudskillsboost.google/course_templates/33/video/519829)

- [YouTube: CI/CD tools available in Google Cloud](https://www.youtube.com/watch?v=xb0aFiMwXE0)

The Google Cloud Marketplace has a wide range of tools to help users design and create complex infrastructures, applications, and CI/CD pipelines. Let’s explore some of them. Jenkins is one of the oldest open-source continuous integration servers, and remains the most popular option in use today. Spinnaker is also an open-source tool. It’s a multi-cloud, continuous delivery platform that helps users release software changes with high velocity and confidence. CircleCI is a continuous integration and delivery platform designed to make it easy for teams of all sizes to rapidly build and release quality software at scale. GitLab CI is also a continuous integration tool. It’s built into GitLab, a platform for Git repository hosting and development tools. Then there is Drone, which is a modern CI/CD platform built with a container-first architecture. Running builds with Docker is at the core of Drone's design. And then there are specific tools owned and operated by Google, including Artifact Registry, Cloud Build, and Cloud Deploy. In CI/CD pipelines, artifacts are the outputs produced at each stage. They’re typically compiled code, test results, and deployment packages. Artifacts are essential for reproducible builds and consistent deployments, because they ensure that the software delivered is the same, regardless of the environment or time. Google’s Artifact Registry is a fully-managed, secure, and scalable artifact repository for storing, managing, and distributing build artifacts. Artifact Registry can be used to store artifacts of any type, including source code, binaries, and Docker images. Artifact Register features help make it easy for users to manage their artifacts while getting the benefits of Google Cloud’s infrastructure. Features include: Version control, which can be used to track changes over time, which is helpful when debugging or if you need to roll back to a previous version of an artifact. Retention policies, which provide a way to automatically retain artifacts for a designated period of time, or alternatively, automatically delete artifacts after a certain period of time. This can help reduce the risk of data loss, or free up storage space. Access controls, which can be used to define who can view or download artifacts, which helps protect sensitive artifacts from unauthorized users. And webhooks, which are automated notifications sent to a specific URL, that let users know when an artifact is created, updated, or deleted. This is useful for keeping track of artifact changes. In addition, some important, general information to know about Artifact Registry is that it’s built on top of Google Cloud Storage, which is a highly scalable and reliable storage platform; it supports a variety of artifact formats, including AAR, APK, JAR, POM, and ZIP; it can be integrated with a variety of build tools, including Gradle, Maven, and Bazel; and it’s available in all Google Cloud regions. Next up is Cloud Build. In a CI/CD pipeline, the build stage refers to when raw source code is transformed into a deployable package. The process typically involves fetching dependencies, compiling the code, and packaging the application. To help with this process, Google offers Cloud Build, which is a service that can execute builds on Google Cloud infrastructure, an on-premises environment, or a combination of the two. Cloud Build can import source code from a variety of repositories or cloud storage spaces, execute a build to defined specifications, and produce artifacts, such as container images. Cloud Build can be customized to automatically start the build process in response to specific changes in your code. This automation is achieved through pre-defined triggers, which define the specific events that will initiate a build. The build process starts by mounting a common volume named /workspace. Then, the source code of an application is added to this volume. Cloud Build creates each of the build steps as independent containers. This allows each content to operate on the common workspace. This environment reduces the need for redundant data stores. To help deploy and manage complex Kubernetes applications, Cloud Build has built in support for tools like Docker, Git, kubectl, and Helm. Cloud Build can also interact with other Google Cloud products, such as Artifact Registry, Cloud Storage, and GKE. And finally, there is Cloud Deploy, which is a Google Cloud service used to automate the deployment of applications to Google Kubernetes Engine. Cloud Deploy plays a crucial role in the CI/CD pipeline. While CI/CD focuses on automating the entire software delivery process, Cloud Deploy specifically handles the continuous delivery (CD) stage, seamlessly connecting the build artifacts to the deployment environment. It provides a single place to view and manage all of your deployments, and can be integrated with other Google Cloud services, such as Cloud Monitoring and Cloud Logging. Cloud Deploy can deploy applications from many sources, including source code repositories, such as GitHub and Bitbucket; container images, such as Docker and Google Artifact Registry, and Helm charts. And Cloud Deploy also supports many deployment methods. The list includes blue/green deployments, rolling updates, and Canary deployments. Cloud Deploy is a powerful and easy-to-use tool that can help you improve the reliability, scalability, and security of Kubernetes deployments.

### Video - [Best practices for using CI/CD on Google Cloud](https://www.cloudskillsboost.google/course_templates/33/video/519830)

- [YouTube: Best practices for using CI/CD on Google Cloud](https://www.youtube.com/watch?v=Nfo2d-CcIHc)

The continuous integration and continuous delivery development technique, or CI/CD, helps software development teams ensure that code is of high quality and can be deployed to production reliably and quickly. Google offers a variety of tools and services to help teams implement CI/CD pipelines, including Artifact Registry, Cloud Build, and Cloud Deploy. So, before you set off and start building CI/CD pipelines, you might consider some best practices. Use a managed artifact repository. Google’s Artifact Registry is a managed, secure, and scalable artifact repository that can be used to store, manage, and distribute build artifacts. This can help to improve the efficiency and reliability of your CI/CD pipeline. Use a build service. Google’s Cloud Build is a service that can execute builds on Google Cloud infrastructure, an on-premises environment, or a combination of the two. This can help to improve the scalability and flexibility of your CI/CD pipeline. Use a deployment service. Google’s Cloud Deploy is a service that helps deploy and manage applications on Google Kubernetes Engine (GKE). This can help to automate and simplify the deployment process, and can also help to improve the reliability and scalability of your applications. Automate your pipeline. Take every opportunity to automate your CI/CD pipeline. Doing so can help reduce the risk of human error, and can also help to improve the efficiency of your pipeline. Monitor your pipeline. Monitoring your CI/CD pipeline helps ensure that it’s running smoothly and meeting your expectations. This can help you identify and troubleshoot any problems, and can also help you improve the performance of your pipeline. Taking the time to implement these best practices can help ensure the efficiency, reliability, and scalability of your CI/CD pipeline on Google Cloud.

### Lab - [Continuous Delivery with Google Cloud Deploy](https://www.cloudskillsboost.google/course_templates/33/labs/519831)

Create a delivery pipeline using Google Cloud Deploy, create a release for a basic application, and promote the application through a series of Google Kubernetes Engine targets

- [ ] [Continuous Delivery with Google Cloud Deploy](../labs/Continuous-Delivery-with-Google-Cloud-Deploy.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/33/quizzes/519832)

## Production Course Summary

The course closes with a summary of the key points covered in each section.

### Video - [Course summary](https://www.cloudskillsboost.google/course_templates/33/video/519833)

- [YouTube: Course summary](https://www.youtube.com/watch?v=3lwCBBDCgp8)

This brings us to the end of the “Architecting with Google Kubernetes Engine” course. Well done! At this point, you’ve discovered just how powerful GKE is, and we hope that you feel prepared to harness that power and leverage it in your own applications. We covered a lot of topics, so let’s take a moment to review what you’ve learned. The first section of the course was all about workloads in GKE. You learned what Deployments are, and how to configure, manage, and update them; how to use Jobs and CronJobs; how to scale clusters automatically and manually; and how to configure node and Pod affinity. Next, you learned about Pod and cluster networking in GKE, including service creation and load balancer configuration; container-native load balancing; and GKE networking. The third section of the course covered storage abstractions, where you explored running and maintaining pods using StatefulSets; using ConfigMaps to decouple configuration from Pods; managing and storing sensitive access and authentication data; and configuring persistent storage. From there, you moved on to authentication, authorization, and security in GKE, where you learned how to secure GKE clusters with Kubernetes RBAC and IAM; configure Workload Identity; use Pod Security Standards and Pod Security Admission to secure GKE; and implement Role-Based Access Control. Then we journeyed further into the world of monitoring in GKE in the fifth section, and you learned about Google Cloud Observability. You examined how to configure observability tools to monitor and manage performance availability; inspect Kubernetes logs; monitor system performance; create dashboards and alerts; and configure GKE-native monitoring and logging. And you didn’t stop there! In the sixth section of the course, you explored Google Cloud managed storage service options, where you learned to contrast managed storage services with self-managed storage; identify use cases for Cloud Storage for Kubernetes application; and compare the range of Google Cloud managed database services. You also explored Cloud SQL Auth Proxy and used Cloud SQL with GKE. Finally, in the last section of the course, you learned about CI/CD pipelines. This section covered Continuous Integration and Continuous Delivery pipelines, and how they can optimize app releases; code management in a source repository; and best practices for CI/CD pipelines. Whew, you made it! Pat yourself on the back, take a break, and when you return, you’ll be ready to get started building your own containerized applications on GKE. See you next time!

## Course Resources

Student PDF links to all modules

### Document - [Course Resources](https://www.cloudskillsboost.google/course_templates/33/documents/519834)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
