---
id: 865
name: 'BigQuery for Data Analysts'
type: Course
url: https://www.cloudskillsboost.google/course_templates/865
date_published: 2025-02-04
topics:
  - Dataform
---

# [BigQuery for Data Analysts](https://www.cloudskillsboost.google/course_templates/865)

**Description:**

This course is designed for data analysts who want to learn about using BigQuery for their data analysis needs. Through a combination of videos, labs, and demos, we cover various topics that discuss how to ingest, transform, and query your data in BigQuery to derive insights that can help in business decision making.

**Objectives:**

* Introduce BigQuery, Google Cloud’s enterprise data warehouse, and review its features that make BigQuery a great option for your data analytics needs.
* Learn how to analyze large datasets in BigQuery with SQL
* Clean and transform your data in BigQuery with SQL
* Ingest new BigQuery datasets, and discuss options for external data sources
* Review visualization principles, and use Connected Sheets and Looker Studio to viualize data inights from BigQuery
* Use Dataform to develop scalable data transformation pipelines in BigQuery

## Course Introduction

This module introduces the course agenda.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/865/video/523339)

* [YouTube: Course Introduction](https://www.youtube.com/watch?v=OWMrIiqVWzM)

welcome to the course big query for data analysts this course is designed for data analysts who want to learn about using big query for their data analysis needs through a combination of videos labs and demos we cover various topics that discuss how to ingest transform and query your data in big query to derive insights that can help in business decision making so let's take a look in the first module we look at analytics challenges faced by data analysts and compare big data on premises versus on the cloud we then introduce big query which is Google Cloud's Enterprise data warehouse and review its features that make big query a great option for your data analytics needs and finally we'll learn from Real World use cases of companies transformed through analytics on the cloud the second module is all about exploring your data with SQL or structured query language we go from very simple select statements to more complex queries that explore various data sets of course not all data is in the form that we need it to be which is why in module 3 we will discuss principles about data integrity and then we look at how to use SQL to clean prepare and transform your data the last section of this module also briefly introduces other products like data prep cloud data Fusion data flow data proc and data form that can help with data preparation and transformation module 4 talks about ingesting and storing data into big query native storage we discuss when to use extract and load versus extract load and transform versus extract transform and load approaches for loading data into big query we also cover external data sources where you can run your query in big query but the data is hosted outside of big query module five is where all that hard work around ingesting cleaning preparing and transforming your data comes to fruition as you get to visualize insights from your data by building insightful dashboards and reports we start off with a little visualization Theory and some best practices and then look at tools like looker studio and connected sheets that can connect to Big query and help create impactful visualizations to capture and convey your insights allos SQL is a powerful query language programming languages such as python Java or R provides syntaxes and a large array of built-in statistical functions that data analysts might find more expressive and easier to manipulate for certain types of data analysis such tools include open-source web-based applications like Jupiter notebooks and so we discuss these as well creating maintaining and versioning SQL pipelines is a lot of hard work and many times data analysts have to use multiple tools to achieve this so in the next module we introduce data form a new product that offers a unified and to and experience to develop Version Control and orchestrate SQL pipelines in big query and finally we introduce big query Studio big query Studio brings an endtoend analytics experience in a single purpose-built platform and so in the module we start off by discussing the motivation behind building it then we discuss its features and capabilities in more detail ending with a demo and a Hands-On lab we are excited for you to learn about big query and what it can do for your data analytical workloads so let's jump into the first module

## BigQuery for Data Analysts

In the first module, we look at analytics challenges faced by data analysts and compare big data on-premises versus on the Cloud. We then introduce BigQuery, which is Google Cloud's enterprise data warehouse, and review its features that make BigQuery a great option for your data analytics needs. And finally, we'll learn from real-world use cases of companies transformed through analytics on the Cloud.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/865/video/523340)

* [YouTube: Module overview](https://www.youtube.com/watch?v=Y9FFxjbPS2k)

Welcome to module 1, “BigQuery for Data Analysts” In this module, we will highlight analytics challenges faced by data analysts and compare big data on-premises versus on the Cloud. Next, we discuss BigQuery, which is Google Cloud’s enterprise data warehouse, and review its features that make BigQuery a great option for your data analytics needs. And finally, we’ll learn from real-world use cases of companies transformed through analytics on the Cloud.

### Video - [Data analytics on Google Cloud](https://www.cloudskillsboost.google/course_templates/865/video/523341)

* [YouTube: Data analytics on Google Cloud](https://www.youtube.com/watch?v=dHAfytQsOuI)

Let’s start by looking at common analytics challenges faced by you, the data analyst. Data analysts, data engineers, and data scientists all analyze data sets to gain insights. However, they have different roles: Data analysts primarily Gather and interpret data to solve problems and help companies make decisions. They use SQL, and static modeling techniques, to summarize data, build reports and visualizations, and derive insights. Data engineers Build systems for collecting, validating, and preparing data. They develop and maintain data pipelines. Data scientists Use dynamic techniques like machine learning to gain insights about the future. It is important to understand that they may or may not work in silos. For instance, a data analyst might partner up with a data engineer to work on sourcing the data and ingesting it into the system where analysis will take place. And it is not uncommon that these roles are sometimes performed by the same person. Which is why, although the focus of this course is on data analysts, we will cover topics like ingestion and storage, and give a little preview of doing some machine learning in BigQuery. Two of the most common barriers a data analyst will run into are either too much data or data that is not connected together. 3 themes are usually common: Querying: where the queries take too long or have very complicated logic. Infrastructure: where you see issues with scalability, maintenance, etc. Storage: where capacity may be a concern, or pricing could be an issue, or maybe not all data is centralized. Again, it is imperative to note that data engineers and data scientists also face similar challenges when it comes to storage, infrastructure, and working with the data. Well, the good news is, Google Cloud can help you with storage and infrastructure. It has also been designed with scalability in mind, an actual global elastic cloud, so that you can focus on query writing and data analysis to unlock those insights even faster. We will discuss that briefly in the rest of this lesson as we compare on-premises servers and infrastructure with Google Cloud. Let’s start with storage. In December 1981, the cost of a 10-megabyte hard drive was $300,000. In 2023, a 1-terabyte hard drive is less than $20. Storage has become so much cheaper. Although hard drives are cheap, they’re not the only thing you need to have to query big data. You will also need: Computing power Networking Admin and hardware teams to maintain and upgrade your infrastructure. And, not to mention, software and software license costs. But even after managing all that, how do you find the needle in the haystack of data? Traditional databases are not up to the challenge of data of this scale. If you look at most big data projects, and really look at where time and money is being spent, you see that most of the time isn’t spent getting insights from the data. It’s spent on the care and feeding of the machinery—managing infrastructure, manipulating data, monitoring, and performance. Whereas what you really want to spend your time on is data analysis. We realized this ourselves at Google several years ago, and so we started developing systems that: 1) scale with your data growth even as it explodes 2) are managed so that you aren’t wasting time on dealing with all of the underlying complexities 3) are just generally magically awesome so you can get back to data insights, and not data management. The message we are trying to convey is: Don’t build it yourself, when you can leverage Google’s massive infrastructure. And speaking of infrastructure…. Here is a map of Google Cloud’s network infrastructure. The number of regions and zones is continually increasing. An up-to-date view can be found at: https://cloud.google.com/about/locations/. The network and points of presence are available in the network tab at that URL. The edge points of presence are the locations where Google networks are connected with internet service providers to allow users to connect. The Google Cloud network strongly distinguishes Google from other cloud service providers. The points of presence allow Google to provide very low-latency network performance. [The map is correct at the time of publishing this deck : September 2023] The other big benefit is, of course, scalability. If you manage your own infrastructure, you still pay for power for unused CPUs on-premises (especially if you have storage disks and CPUs on the same servers). But with Cloud, you can use the same processing power as having 120 machines to process 1 TB in 1 second, just as if you had one machine process the query in 120 seconds. And the scaling up and down is automatic, so there is no guesswork for demand—if you need more, the cloud scales up elastically and then goes away when not in use. One of the key features of Google Cloud is the decoupling of storage and compute power. With an on-premises setup, typically both storage and compute are co-located right on that server. Which means that you pay for the ability to use processing power even when processing is not happening. With Google Cloud, the separation of compute and storage ensures that you pay only for the resources you use. So if you are only using storage space, well, that’s what you get billed on. BigQuery combines all these great features (that we just discussed) and powerful analytic tools to help data analysts focus on data instead of resource management. BigQuery is Google Cloud’s fully managed enterprise data warehouse that helps you manage and analyze your data. BigQuery's serverless architecture lets you use SQL queries to answer your organization's biggest questions with zero infrastructure management. BigQuery's scalable, distributed analysis engine lets you query terabytes in seconds and petabytes in minutes. BigQuery maximizes flexibility by separating the compute engine that analyzes your data from your storage choices.

### Video - [From data to insights with BigQuery](https://www.cloudskillsboost.google/course_templates/865/video/523342)

* [YouTube: From data to insights with BigQuery](https://www.youtube.com/watch?v=msgGpqAzgfg)

In this section, we will highlight the five common tasks of any data analyst and discuss how BigQuery can help with these tasks. In an earlier section, we briefly described the role of a data analyst, and compared it with other data roles like the data engineer and data scientist. Ideally, the data analyst wants to be spending more time doing the last two tasks shown here—analyzing and visualizing data—and we will spend a considerable amount of time discussing these topics. But the data analyst may also have to deal with ingesting the right data, and that data may need some cleaning and transforming before you can analyze it, and you may want to store it in a certain way for faster retrieval. And so we will spend some time discussing these themes as well. Now, each of these tasks has its own challenges. For instance, analysis may get harder because queries are running slow, and that could be because of the huge volume of data you are working with, or the logic may be rather complicated, resulting in a lot of transformations that are done on the fly. All these make it harder for data analysts to get useful, actionable, timely insights from their data. In this course, we look at how BigQuery can help you with each of these data analyst tasks. In the next few slides, we highlight key features of BigQuery that not only make it a great solution for your analytical workloads, but also do it in a scalable, economical, and impactful way. So let’s take a look. With BigQuery, you get the benefit of Google datacenter–backed infrastructure that is fully managed. The best part is that you don’t need to spend your time optimizing the specific hardware and networking. Your job as a data analyst is to focus on asking great questions of your data and hunt down interesting insights. Now let’s expand on specific features of BigQuery. BigQuery is a fully-managed, enterprise data warehouse, which provides near real-time interactive analysis of massive datasets. It runs on Google’s fully managed, secure, high-performance infrastructure. And by “NoOps”, we mean no administration for performance and scale. The data is replicated across multiple data centers, giving you peace of mind. And you only pay for the storage and processing you use. Secure your data through Access Control Lists (ACLs) and Identity and Access Management (IAM). And rest assured that data is encrypted in transport and at rest. Auditing is important for many users and stakeholders. Google Cloud Audit Logs track Admin Activity, Data Access, and System Events. So you can also know “who did what, where, and when?” in BigQuery, When it comes to scalability, BigQuery’s virtually unlimited data storage and processing power leads to a highly parallel/distributed process model, which means very fast queries. Flexible With streaming ingestion of real-time data, BigQuery makes the data available for analysis even more quickly, without you having to wait for all the data to be available. And you have the flexibility to mash up your data across diverse datasets/projects, including a huge collection of public datasets that BigQuery makes available to you. Easy to use Data is stored in denormalized tables (simple schemas) There is columnar storage for high performance It requires no indexes, keys, or partitions It has a familiar SQL interface and intuitive UI There is nested and repeated field support for schema flexibility And it supports open standards, so analysts can use their preferred tools There are four ways to interact with BigQuery: the BigQuery web UI, the command-line interface (CLI), the REST API, and third-party GUI SQL editors. Since this course focuses on using BigQuery for data analysis, you spend most of the time using the BigQuery web UI. In the hands-on labs, you learn how to examine tables, quickly build queries with a few simple mouse clicks, determine how much the query will process, and so on, all through the BigQuery Web UI. You can also use the CLI to execute queries and explore BigQuery features. The CLI contains a robust set of commands that provide you the flexibility to run commands and queries interactively. A third option is the REST API, which is the programmatic interface that programming languages like Java and Python use to communicate with BigQuery. The service receives HTTP requests and returns JSON responses. Both the web UI and the CLI use this API to communicate with BigQuery. And finally, there’s many GUI SQL editors like Looker, Appsmith, Retool, Cogniti, superQuery, PopSQL, Zing Data, and many more that can interface with BigQuery. Note that the REST API and the GUI SQL editor tools are beyond the scope of this course. If you have had a chance to complete the recommended prerequisite course “Introduction to Data Analytics on Google Cloud,” you will remember that BigQuery is a two-in-one service. The storage layer provides an economical and efficient option for your data storage needs, and has tools to help with the ingestion process, with support for many data formats from a variety of data sources. The analysis engine is optimized to run analytic queries on large datasets, including terabytes of data in seconds and petabytes in minutes. Understanding its capabilities and how it processes queries can help you maximize your data analysis investments. It is worth mentioning that BigQuery also supports querying data that resides outside of BigQuery storage. And we will talk about this in a later module. Here is a quick recap from the “Introduction to Data Analytics” course, on how resources are organized in BigQuery Storage. Remember, tables reside in a dataset, and datasets exist within a project. Of course, you can store other resources like views, routines, etc in the dataset, which we will discuss in a later module. So you can think of a dataset as a collection of tables, views, and so on.

### Video - [BigQuery demo](https://www.cloudskillsboost.google/course_templates/865/video/523343)

* [YouTube: BigQuery demo](https://www.youtube.com/watch?v=hGWp2AngYtY)

Google BigQuery has numerous Public Datasets that anyone can query. Let’s run a SQL query on one such dataset to see how fast we can scan and process 10 Billion rows. Refer to https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/data-to-insights/demos/wikipedia-10-billion.sql Google BigQuery has numerous public datasets that anyone can query. Let’s run a SQL query on one such dataset to see how fast we can scan and process 10 billion rows. Refer to https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/data-to-insights/demos/wikipedia-10-billion.sql Refer to https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/data-to-insights/demos/ folder Demo: wikipedia-10-billion.sql

### Video - [Real-world use cases of companies transformed through analytics on Google Cloud](https://www.cloudskillsboost.google/course_templates/865/video/523344)

* [YouTube: Real-world use cases of companies transformed through analytics on Google Cloud](https://www.youtube.com/watch?v=6jHwX5_lpyU)

In this lesson, we will examine real-world use cases of companies where moving compute and storage to the cloud has enabled them to scale their data analysis. While these customer success stories involve multiple Google Cloud products, we will focus on how their use of BigQuery transformed their operations through faster analysis and useful insights. After a technical evaluation in 2018, MLB (Major League Baseball) decided to migrate their enterprise data warehouse from Teradata to Google Cloud’s BigQuery. They successfully completed a proof of concept in early 2019, and ran a project to fully migrate from Teradata to BigQuery from May 2019 through November 2019. With the migration complete, MLB has realized numerous benefits in migrating to a modern, cloud-first data warehouse platform. BigQuery made it easy to securely share datasets with any G Suite user or group with the click of a button. Users can access BigQuery’s web console to immediately review and run SQL queries on data that is shared with them. They can also use Connected Sheets to analyze large data sets with pivot tables in a familiar interface. In addition, they can import data from files and other databases, and join those private datasets with data shared centrally by the data engineering team. With BigQuery’s on-demand pricing model, MLB was able to run side-by-side performance tests with minimal cost and no commitment. These tests involved taking copies of some of their largest and most diverse datasets and running real-world SQL queries to compare execution time. As MLB underwent the migration effort, BigQuery cost increased linearly with the number of workloads migrated. By switching from on-demand to flat-rate pricing using BigQuery Reservations, they were able to fix their costs and avoid surprise overages, and share unused capacity with other departments in their organization, including their data science and analytics teams. [Source: https://cloud.google.com/blog/products/data-analytics/mlb-moves-to-bigquery-data-warehouse ] Customer-first service means shipping direct, having the right tool in store, teaching new skills, and anticipating customer needs, everywhere. The Home Depot empowers its associates with Google Cloud’s BigQuery by providing timely data to help keep 50,000+ items stocked at over 2,000 locations, to ensure website availability, and provide relevant information through the call center. Migrating to Google Cloud, The Home Depot’s engineers built one of the industry’s most efficient replenishment systems—then figured out how to get more done, using BigQuery for streaming application performance monitoring. The Home Depot runs more than 600 projects in Google Cloud, and analyzes 15 petabytes of data in BigQuery. [Source: https://cloud.google.com/customers/featured/the-home-depot ] Let’s summarize the key things we looked at in this module. We started by highlighting analytics challenges faced by data analysts and compared big data on-premises versus on the cloud. Next, we talked about BigQuery, which is Google Cloud’s enterprise data warehouse, and reviewed the features that make BigQuery a great option for your data analytics needs. And finally, we looked at some real-world use cases of companies transformed through analytics on the cloud.

## Exploring and Preparing Your Data with BigQuery

This module is all about exploring your data with SQL, or structured query language. We go from very simple select statements to more complex queries that explore various datasets.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/865/video/523345)

* [YouTube: Module overview](https://www.youtube.com/watch?v=MabsGi1eSAc)

Welcome to module 2: Exploring and preparing your data with BigQuery In this module we will start by looking at common data exploration techniques when working with BigQuery datasets. Next, we focus on writing SQL (or structured query language) in BigQuery. Both the examples in the lessons, as well as the practice through the hands-on labs in this module, use the BigQuery public datasets and the course dataset. If you have experience in SQL, that is great and some of the queries we cover here will be rather familiar. But if you are new to SQL, don‚Äôt worry because we start right from the basics and move quickly to more advanced query constructs with aggregates, functions, unions, and joins in later lessons.

### Video - [Common data exploration techniques](https://www.cloudskillsboost.google/course_templates/865/video/523346)

* [YouTube: Common data exploration techniques](https://www.youtube.com/watch?v=mv0i47XkYpQ)

Let's talk about your different options that are available for exploring your datasets. There are three primary options for exploring a dataset: Write some SQL and use it in a SQL editor like the BigQuery UI Use a Data preparation or BI tool: Examples include Dataprep, SQL Runner in Looker, Looker Studio, Cloud Data Fusion‚Äôs Wrangler tool, and so on Visualize your raw data with many other third-party tools SQL syntax hasn‚Äôt changed all that significantly since the 1980s and, out of the three options listed, is the most core to your skillset as a data analyst. But that said, as a data analyst, you're not necessarily limited to just using SQL and the BigQuery WEB UI. There are data preparation and visualization tools that you can use as well, and we will briefly cover them later in this course. In the rest of this module, we will focus on using the SQL approach in the BigQuery Web UI. Why? Because SQL is a very good skill to have, almost an imperative skill to have as a data analyst. And it's one of the fastest ways you can interact with data inside of BigQuery. And of course, because it's fun. Exploring a dataset through SQL is more than just writing good queries. You need to know what destination you‚Äôre heading towards and the general layout of your data. Good data analysts will explore how the dataset is structured even before writing a single line of code. Ask yourself these questions: What type of data am I interested in? For example, financial, nonprofit organizations, etc. What specifically about that data am I interested in? It could be organization revenue, perhaps. How do I want to receive the results? Say you want it sorted by the highest revenue first. Next, think of the dataset. What datasets do I have available to me? Do I need to find and upload my own? How is the data structured? Are there multiple tables? Important fields? How much data is there to explore? And finally, think of the SQL query: How do I translate my question into a SQL query? Is my data clean? What fields should I focus on? Do I need to perform any aggregations? Do I need data from multiple tables? These steps are essential to writing good queries that will look through data in the right places, and give you the insights you need in a fast, efficient manner. And we will go through all these steps in the rest of the module as we write queries to get insights from our datasets.

### Video - [Analyze large datasets with BigQuery](https://www.cloudskillsboost.google/course_templates/865/video/523347)

* [YouTube: Analyze large datasets with BigQuery](https://www.youtube.com/watch?v=8UIUjN-hIzw)

Let’s quickly talk about some of the datasets we will run our SQL queries on. A public dataset is any dataset that is stored in BigQuery and made available to the general public through the Google Cloud Public Dataset Program. The public datasets are datasets that BigQuery hosts for you to access and integrate into your applications. We will use a few of these datasets in this course as we show you how to build and run SQL queries in the BigQuery UI. One of the hardest parts about making a course like this is finding a practice data set that any analyst can relate to and want to query for insights. In this course, we're excited to bring you a real products and ecommerce dataset with over a million site hits in a year of transaction records from Google's own online store, where you can buy Google-branded merchandise. These transactions have been recorded through Google Analytics and made publicly available through BigQuery. With the knowledge of BigQuery and SQL that you'll practice throughout this course, you'll be able to answer real-world ecommerce questions, like: What is the total number of transactions generated per device browser? Which customers have added items to their carts but then abandoned them? How do I set up cohorts or a segment of my customers based on their behavior so I can target each one of them in a more personalized way? Or even what keywords and referring partner sites have led our visitors to our ecommerce site, and what pages did they visit while they were there? The great part about working with this dataset is that all the queries and the insights that you derive here are going to be directly applicable to your own Google Analytics datasets. Now, if you're not a fan of ecommerce, the analytical tips and techniques that we'll cover here are sure to be useful in your own queries. Refer to https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/data-to-insights/demos/explore-data-with-sql.sql

### Video - [Query basics](https://www.cloudskillsboost.google/course_templates/865/video/523348)

* [YouTube: Query basics](https://www.youtube.com/watch?v=2FxDOsZoNBg)

For the most part, queries in BigQuery are the same as when using other databases. There is an international standard for SQL, with each version referred to with the term ANSI and the year it was adopted. BigQuery SQL conforms to the ANSI 2011 version of the standards. There are some minor differences specific to the platform, as is common with most SQL-based databases. All the usual SQL constructs like SELECT, WHERE, JOIN, GROUP BY, ORDER BY, etc. are supported. You will find the aggregation functions typical of other databases: COUNT, SUM, MIN, MAX, etc. Some functionality has been added to support features unique to BigQuery: Arrays and structs Geography, and BigQuery ML Let’s quickly review some SQL query basics. The basic construct of a SQL query starts with the SELECT keyword, followed by one or more columns that you want to display in your result. In this select statement, we want to look at the name and revenue columns. Of course you have to specify which table the data resides in, using the FROM keyword. And in this example, it is the sales table. You can also filter your rows by using a WHERE clause. Like in this example, you are only interested in seeing product names whose revenues are above $100,000. Take note that you can filter on multiple columns too. So, you can say where revenue is greater than 100,000, AND the category equals toys. And now you will see only rows that match these 2 conditions. Optionally, you can order or sort your results based on a certain column, like in this case, you order it by revenue. The default is to use the ascending order, which goes from the smallest to the biggest value. But you can use the D E S C suffix to specify descending order. And when you use the WHERE clause and the ORDER BY clause together in the original query, you get rows of data where the revenue is above $100,000, AND the results are sorted from the highest revenue to the lowest revenue. Take note that you can also use ORDER BY on more than one column. So in this example, rows that have the same revenue amount will be further sorted using that category column. Sometimes, you only want to see a subset of rows and you don’t really have any matching conditions yet for your WHERE clause. Maybe you are just exploring the data for now. Like in this example, say you only want to see 10 rows from the table. So you would add the LIMIT keyword, followed by the number 10, which will get that number of rows, and they will be the first 10 rows based on the order the results are sorted in. Also, it’s good practice to comment your SQL queries, because after a while you will become an expert and start writing long complex queries, so it’s always a good idea to have an explanation about what the query does. Now let’s use that SQL knowledge and run some queries on the ecommerce dataset in BigQuery. First, do you know what’s missing in this query? Remember that tables in BigQuery exist within a dataset. So it’s important to specify the dataset name. Tables exist in datasets, and datasets exist within projects. And if you omit the project ID, BigQuery will assume the project is your current one. This could be an issue when, for example, you are trying to access public datasets, as they would be hosted in the bigquery-public-data project. So it’s a good idea to use the fully qualified name using the notation “project ID dot dataset dot table name”. Use back tics (') to get a suggestions list as you type out the fully qualified name of the table, starting with the project ID. Do pay attention to the error messages as they provide good guidance and hints to resolve issues with your queries. Be careful when using the * wildcard, which will select ALL columns of your table. If the intention is to explore the schema of a new table, you can use the Schema tab. Running SELECT * without a LIMIT or WHERE clause on a large table will cost a lot as the entire table, all rows, all fields, are read and returned. You can also explore by looking at all column data for a subset of the rows by using the Preview tab. Sometimes, there may be a genuine need to present all columns, such as in a report. So If you must use SELECT *, be sure to include a LIMIT or a WHERE clause if it applies. Take note that using preview does not incur any query costs. Here is a high-level overview of how BigQuery charges for usage. For more details on the pricing numbers, you can go to https://cloud.google.com/bigquery/pricing Jobs like load batch data, copy, and extract are free. Loading streaming data, however, does incur a cost. And storing data in BigQuery is a separate cost. The BigQuery web UI also has a query validator (on the top right of your SQL query editor window), which provides an estimate of the number of bytes required to process the query. Take note that you get 1 TB per month of query processing at no cost. You can also use the Google Cloud pricing calculator to get estimates for your storage and processing costs. For queries, BigQuery charges based on the number of bytes processed, which is basically the bytes read to complete the query. Recall that traditional data warehouse costs include hardware, licensing, and maintenance. BigQuery is a serverless data analytics platform. You don't need to provision individual instances or virtual machines to use BigQuery. Instead, BigQuery automatically allocates computing resources as you need them. Let’s do a quick recap: List the specific columns in your SELECT statement that you want to see in your results. Be specific about the table name (which means you may want to include the project ID and the dataset as a prefix). And use a LIMIT and/or a WHERE clause when possible. If you run the exact same query twice, you will get the advantage of query cache. The second attempt of the query will try to use results from the previous run of this query, as long as the referenced tables are unmodified. If cached results are returned, you will not be billed for any usage. Results are cached for approximately 24 hours. The default setting in the BigQuery UI is to use cached results. However, caching cannot be enabled when a destination table is selected. We will talk more about destination tables in a later module. BigQuery will also not use cache with a slight change in query. For example, if a space or a blank line space are added, it will not use cache even though it is essentially the same query. Of course, the query cache also won’t make sense if you have non-deterministic elements in your select list. In this example you see the use of the current timestamp function, which returns the time at which the query runs, along with some of the column data. This is also a good example of how you would use a function in your query. We will talk more about functions in a while. Now we did see examples of using using the ORDER BY and LIMIT clauses in previous SQL queries. Combining them in the same query helps you when you need to do “top n” analysis…. like for example the top 10 products by revenue, or top 5 most popular patents, etc. In this example, you are ordering the rows by the timeOnSite column (which shows the amount of time the website visitors spent on the site). AND you have limited the query to show only 10 rows. In other words, the top 10 results. And if your resulting rows have duplicates, like what you see in the result on the left here, then use SELECT DISTINCT to remove duplicates. Another thing you may have noticed is that the timeOnSite column shows durations in seconds. So you can use a simple mathematical formula to change that into minutes. And since you are modifying the original column value into a different denomination, why not call it something else? And so in this example, you can use an alias with the AS keyword. Notice, though, that the results show lot of decimal places for the time-on-site duration in minutes. How can we clean that up a little? Well, you can use the ROUND function that will round up the value to 2 decimal places. Functions are a great tool to use in your SQL queries. They can help perform certain actions on your column data, like calculations or transforms, before displaying them in the output. In the query you just looked at, we used the ROUND function to take 2 inputs: The first input is the value of the time on site duration in minutes, and the second input is the number of decimal places you want to round up the value to. And the result is the duration in minutes, but rounded up to 2 decimal places (as you would have expected). There is a long list of functions that you can use and we will discuss them later. Take a look at this query. Do you have any idea why the query fails? Well, newly defined aliases in the SELECT statement cannot be used yet for filtering rows in your WHERE clause. And the reason for that is that when the query reads the table from disk, it filters for the columns you want returned (which is basically your WHERE clause). At the time of pulling the data from disk, your aliased columns in the select statement are not interpreted yet and are thus not available. So the way to get around this is to use the same calculation or formula or function you had used in the select list, again, in your WHERE clause, like what you see in this example. If you do not want to repeat the calculation in the WHERE clause, consider starting with a sub-query and then filtering (and we will discuss this later). Take note that you CAN use aliased fields in the ORDER BY clause though, like what you see here. You can also use aliases in the GROUP BY and HAVING clauses, which we cover later. Refer to https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/data-to-insights/demos/explore-data-with-sql.sql Demo: explore-data-with-sql.sql Look for section on “Demo using hotkeys” Refer to https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/data-to-insights/demos/explore-data-with-sql.sql Demo: explore-data-with-sql.sql Look for section on “Demo using hotkeys”

### Video - [Lab intro: Exploring an Ecommerce Dataset using SQL in Google BigQuery](https://www.cloudskillsboost.google/course_templates/865/video/523349)

* [YouTube: Lab intro: Exploring an Ecommerce Dataset using SQL in Google BigQuery](https://www.youtube.com/watch?v=WN_FlokOLzU)

In this lab, you explore the ecommerce dataset using SQL in Google BigQuery. This dataset has millions of Google Analytics records for the Google Merchandise Store loaded into a table in BigQuery. More specifically, you will start by accessing the dataset in the BigQuery UI, and explore the table schema and metadata. The lab then walks you through a number of interesting queries like total unique visitors, listing top 5 most viewed products, etc., and also look at ways to identify and remove duplicate information.

### Lab - [Exploring an Ecommerce Dataset using SQL in Google BigQuery](https://www.cloudskillsboost.google/course_templates/865/labs/523350)

In this lab, you learn to use BigQuery to find data, query the data-to-insights public dataset, and write and execute queries.


* [ ] [Exploring an Ecommerce Dataset using SQL in Google BigQuery](../labs/Exploring-an-Ecommerce-Dataset-using-SQL-in-Google-BigQuery.md)

### Video - [Working with functions](https://www.cloudskillsboost.google/course_templates/865/video/523351)

* [YouTube: Working with functions](https://www.youtube.com/watch?v=SrX_f0CSJDY)

We will be introducing a number of functions to you in context over the rest of this module. String manipulation functions are great for changing the string’s format or doing simple transforms, like making every letter uppercase, or pulling the left 5 characters, etc. The CONCAT function, for example, concatenates one or more STRING or BYTES values into a single result. You can add a format function to format the results in a certain way, for example displaying revenue column values with commas, so it reads much better. Here are a few common string functions: The CONCAT function concatenates 2 or more strings together. The ENDS WITH function returns true if the second string is a suffix of the first. Easily convert your strings to lowercase with the LOWER function. The last function here returns true if the value is a partial match for the regular expression. You can find a lot more interesting string and regular expression functions with examples on our documentation site. If you are doing very simple pattern matching for strings, then you can use the LIKE operator in the WHERE clause. This example uses a string function on the name, to render the product name in lowercase, and then finds the word ‘shirt’ anywhere in the name. The use of the LOWER function here is important because we’re unsure whether the product name field is uppercase, lowercase, or a combination of the two. Hence we convert it before comparing it. It is also worth mentioning that while this is a valid approach, it is also a very high-overhead approach as all values of v2 ProductName need to be read and converted to lower case before the LIKE comparison can be processed. Aggregation functions can help perform calculations over a set of values, like the SUM or COUNT or MINIMUM or MAXIMUM. You can see the COUNT function here giving the total number of unique users as it aggregates the total number of distinct visitor IDs. This kind of information is great when looking at website traffic. Can we do more, though? We know that there is country information captured in the table as well. Can we do a total count by country? The answer is yes, and this is where you get to use the GROUP BY clause. So essentially, you are getting the aggregate total, but after grouping the visitors based on a certain column, which in this case, is the country. Now the output is more exciting and insightful as we can see the top 5 countries, in terms of unique visitors to the website. Sometimes we want to filter results after some sort of aggregation. The way to do it is using the HAVING clause. In this example, we want to look at visits per website visitor, but only for those who have visited more than once. And so we use the total count of visits, which is aliased as the records column, in the HAVING clause. You can think of the HAVING clause as the WHERE clause for aggregated columns. Let’s look at a few more functions. For data type conversion, use the CAST function. Maybe you want to display a number column as a string, for example. Or maybe you need to cast a string into a date data type, so that you can pass it to the extract function to get the month from the date string. Here are some of the common BigQuery data types. We have seen most of them in the queries and results we have discussed so far. Take note that this is not the complete list. To learn more, you can go to: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types Casting allows you to treat one data type as another. As mentioned earlier, this is particularly useful if a function expects to see an input in a specific data type, like in the case of the EXTRACT function where you want to display only the month part of a date value. Now sometimes, CAST may not work because you are converting a string column into an integer, and maybe some of the values in the column are not valid. If you tried the CAST function normally, this would error out. But you can use SAFE CAST instead. Now it just produces a NULL for that value instead of throwing an error. And so the question is, what is a null value? Well, just like black holes are the absence of light, NULLs are the absence of data. NULLs may or may not be included in aggregations. By default NULLs are respected. You cannot simply do IF VALUE = NULL since NULLS can never be equivalent to anything, Null is not even equal to Null. So if you want to filter out null values, you can specify IS NOT NULL in the WHERE clause. This example will display only rows that have a valid transaction ID, and discard the ones where the transaction ID value is null. Here is a scenario where you have multiple rows with identical data for multiple columns like the visitor ID, transaction ID, date, etc, and only one column where the data differs, which is the product column. Is there another way to represent this? One solution is to aggregate the various product names into a single cell in the final result. Note the use of the STRING AGGREGATE function that concatenates strings from different rows into a single string. And so here you see one row per transaction ID (because it was one of the grouping columns), and the list of products all combined together into a single string. Another possible solution is to use the ARRAY AGGREGATE function. The function works in the same way by combining all the products together, but presentation-wise it’s a little different. As you can see hare, instead of a really long string of product names mashed up together, here you see an array of product names, which is a lot more readable. The same grouping applies so each transaction ID is only displayed once. This example uses the array aggregation function on the product name and the product quantity, so you can see the amount bought per product, and a nice summary of the number of distinct products per transaction. There are many other different kinds of functions which are not discussed here. Feel free to refer to our documentation site. Let’s wrap up this lesson by talking about the WITH clause. A WITH clause contains one or more common table expressions (or CTE). A CTE acts like a temporary table that you can reference within a single query expression. Each CTE binds the results of a subquery to a table name, which can be used elsewhere in the same query expression. Let’s summarize what you’ve learnt so far in this module. As you have seen, exploring massive datasets with SQL in the BigQuery UI is what the tool does best, and what you will need to master as a data analyst. Use comments to improve the readability of your queries and make best friends with the query validator. It is also worth mentioning that there is an interactive BigQuery SQL translation tool in the BigQuery UI that provides translations between various supported SQL dialects. So, for instance, if you have a workload in Amazon Redshift SQL, this tool can help translate it for BigQuery. Lastly, if you’re looking for other datasets to explore, complete with example SQL queries, search for BigQuery public datasets to see the full list. Let’s test your knowledge with an interactive lab where you get to run queries on the ecommerce dataset.

### Video - [Lab intro: Troubleshooting Common SQL Errors with BigQuery](https://www.cloudskillsboost.google/course_templates/865/video/523352)

* [YouTube: Lab intro: Troubleshooting Common SQL Errors with BigQuery](https://www.youtube.com/watch?v=6yU8YG3zt2s)

In this lab, you get to troubleshoot common SQL errors with BigQuery. This lab steps you through the logic of troubleshooting queries. You troubleshoot and fix broken queries in BigQuery standard SQL. You also use the Query Validator for incorrect query syntax.

### Lab - [Troubleshooting Common SQL Errors with BigQuery](https://www.cloudskillsboost.google/course_templates/865/labs/523353)

In this lab, you use the BigQuery query editor and query validator to troubleshoot common SQL syntax and logic errors.

* [ ] [Troubleshooting Common SQL Errors with BigQuery](../labs/Troubleshooting-Common-SQL-Errors-with-BigQuery.md)

### Video - [Enrich your queries with UNIONs and JOINs](https://www.cloudskillsboost.google/course_templates/865/video/523354)

* [YouTube: Enrich your queries with UNIONs and JOINs](https://www.youtube.com/watch?v=MSffZyVfiL4)

In this lesson, we will talk about using unions and joins to enrich your queries. For this lesson, we will use the temperature readings and the weather station data available through BigQuery’s NOAA public dataset. Since we will use these two table types in our examples, let’s describe them in a little more detail. We have a separate table for all daily weather temperatures since 1929. So there is one table for daily temperature readings in the year 1929, one for 1930, and so on. That’s a lot of tables for us to query and combine (but don’t worry, it won’t be so bad) Our weather station location details (like latitude, longitude, state, station name, etc.) are stored in a single lookup table. Important fields like Country and State are not present in the Daily Temperature table (because of a concept called normalization that we will come to later) but we can look these fields up by joining the tables together. But, before we can link and join the two tables together, we need to first figure out what linking field they have in common. What is our unique identifier for weather stations? Is it USAF (US Air Force Station ID) or WBAN (WEATHER BUREAU ARMY NAVY)? Well, let’s investigate …. In the rest of this lesson, you will learn how to join and union the data across these many many tables for enriched insights. So let’s start with UNION. As mentioned earlier, unions are for appending data from various tables. For example, you want to see temperature recordings for the years 1929 and 1930. A simple way is to select rows from the 1929 temperature table, then select rows from the 1930 temperature table, and UNION the results. UNIONs append more data to your table (vertically). UNION requires the tables or SELECT statements to have identical schema. And the result too will have the same number, type, and order of columns. Here is what the query would look like. Notice the use of UNION DISTINCT, which removes any duplicates. You can use UNION ALL if you don’t have or care about duplicates. Note that when using UNION, the number of columns and the data types of each column must match respectively. Now, what if you want to union tables for 1929 all the way to 2023. Typing all those UNIONs by hand seems tedious. Well, the good news is…. A wildcard table represents a union of all the tables that match the wildcard expression. In this example, the FROM clause uses the wildcard expression to match all tables in the noaa_ gsod dataset that begin with the string gsod, and magically union all of the rows for you. That sounds great. But what if you want to union from 1929 all the way to 2023, but skip some of the years in between? Well, not to fear, there is a trick for that too. Include a collection of tables with the table wildcard, and then filter them with _TABLE_SUFFIX as shown in the example here. Think of this as a special hidden metadata column that you can use in your WHERE clause for filtering. Unions in SQL require careful handling of schemas between tables because things may change over time, and if the number of columns or the data types do not match, the query may fail. Combining all temperature records over the years is great, but we are missing the weather station details, which are captured in a different table. How do we enrich our temperature data with station details? For starters, it may be important to see the weather station data in the results. Beyond that, you can think of doing really insightful aggregations and categorize results by groups to assist in decision making. Essentially, JOINs enrich your dataset by potentially adding fields from two or more tables. So let’s take a look. JOIN requires the same field in each table to connect on. And here is a good place to introduce the concepts of a primary key and a foreign key. Let’s use a simpler example to illustrate this, and we can go back to the weather data example right after this. Say we have an Employee table, with employee information, and a Department table, with department information. The EID column in the Employee table uniquely identifies every employee. So that means there cannot be duplicates or null values. This is what we call a primary key. No two employees can have the same EID. In the same way, the DID in the Department table is the primary key, because each ID is unique and not null. Each ID uniquely identifies a department. But the DID in the Employee table is a foreign key. Which means that the value has to match one of the department IDs in the department table, but the same value may appear a number of times in the employees table, because there could be many employees per department. Let’s go back to the weather data example and see how we can join the 2 tables. Before we can link the two tables together, we need to find our unique row identifier. What is our unique identifier for weather stations? Is it U S A F (or U.S. Air Force) number? Not really. As you can see from the above query, USAF is not unique. One station could possibly have reused this ID over time or one station could have multiple recording devices. The W B A N column also doesn’t look unique by itself. But what about the combination of the two: WBAN and USAF? Yes! If we CONCATENATE the two fields we get a combined unique key showing 30,016 stations. When you join two tables, you basically combine data from separate tables that share a common field into one table. In this example, for each row in the temperature table, you try to join with a row from the weather station table if the station number matches the U S A F number, AND if the W B A N numbers match. That’s called the JOIN condition. Aliases are optional in the SELECT statement if the field names are unique between the tables. JOINS can have multiple linking fields to establish the join on logical keys, like the one shown here. The default JOIN is an INNER join which means the records must exist in both tables for results to be shown. Let’s cover the basic join types now. INNER JOIN is what we have just discussed. This is a join condition based on which the rows are matched and returned. There is also a RIGHT OUTER JOIN that returns all rows from the table on the right side of the JOIN, and all matched rows from the table on the left side of the JOIN. The opposite of that is the LEFT OUTER JOIN. And the FULL OUTER JOIN returns all rows from both sides even if the JOIN condition is not met. Please note that table on the left and right of the join are based on the order the tables are listed following the FROM and JOIN keywords in your select statement. There is one more type of JOIN, the CROSS JOIN, which is essentially a Cartesian product from both sides, meaning that the rows from both tables are combined and returned in your result set. Take note that it is normal for the join key to be unique only on one side of the join. That was the whole point behind the primary key and foreign key discussion earlier. However, sometimes when the data relationships are not fully explored or understood, you may end up missing some join conditions that could explode your resulting rows. Let’s take a look at an example. The query on the left is what we have seen before. The JOIN is based on two conditions, because the combination of the two columns is unique, as we pointed out earlier. The query on the right, however, removes one of the two conditions. Notice how the number of resulting rows just explodes! There are 128 times more rows coming out of the query on the right. Doing a join using a non-unique join key can result in the same output and overhead as explicitly doing a CROSS JOIN. That’s exactly what you see here. And while the effect is like a cross join, this is a standard join with a poorly chosen key. With this dataset, it results in the same output as if the join were a CROSS JOIN. Try to get a good understanding of your data model and relationships. Correctly identifying your primary and foreign key fields is crucial to get the desired results in JOINS. And you can always use multiple join conditions if there is no single unique column, like what we saw in the weather tables example. Let’s summarize what you’ve learnt in this lesson. Understanding when and how to use joins and unions in SQL is a concept that is easy to pick up but takes a while to truly master. The best advice I can give you when starting is to really understand how your data tables are supposed to be related to each other (customer to orders, supplier to inventory) and being able to verify if that is actually true though SQL. Remember: all data is dirty and it’s your job to investigate and interrogate it before potentially polluting your larger dataset with joins and unions. Once you understand the relationships between your tables, use unions to append records to a consolidated table and joins to enrich your results with data from multiple sources. Let’s practice these concepts in our next lab.

### Video - [Lab intro: Troubleshooting and Solving Data Join Pitfalls](https://www.cloudskillsboost.google/course_templates/865/video/523355)

* [YouTube: Lab intro: Troubleshooting and Solving Data Join Pitfalls](https://www.youtube.com/watch?v=bu92NW5Fj3Q)

Joining data tables can provide meaningful insight into your dataset. However, when you join your data there are common pitfalls that could corrupt your results. This lab focuses on avoiding those pitfalls. More specifically, you will create joins between data tables, and work through some join pitfalls by identifying duplicate records and learn when to use each type of JOIN.

### Lab - [Troubleshooting and Solving Data Join Pitfalls](https://www.cloudskillsboost.google/course_templates/865/labs/523356)

In this lab, you explore the relationships between data tables and the pitfalls to avoid when joining them together.

* [ ] [Troubleshooting and Solving Data Join Pitfalls](../labs/Troubleshooting-and-Solving-Data-Join-Pitfalls.md)

## Cleaning and Transforming your Data

In this module, we discuss principles about data integrity, and then we look at how to use SQL to clean, prepare, and transform your data. The last section of this module also briefly introduces other products like Dataprep, Cloud Data Fusion, Dataflow, Dataproc, and Dataform that can help with data preparation and transformation.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/865/video/523357)

* [YouTube: Module overview](https://www.youtube.com/watch?v=BEtHNuUWYFA)

Welcome to module 3, “Cleaning and Transforming your Data”. One of the most critical parts of data analysis happens way before you build your first visualization. And while not necessarily the most glamorous, data preparation and transformation can't be ignored. Mostly you will use SQL and data preparation tools for this crucial step. And we will discuss that in detail in this module. But visualization tools can be a great aid in identifying the data that needs work to make it usable, and we will cover that in a later module. In this module, we're going to cover the elements of what makes a good dataset, and then look at two different ways to process, clean, and prepare your data. The first one, as you might expect, is through SQL queries in the BigQuery UI. But we will also briefly introduce other options that are available for preparing, cleaning, and transforming data, and when to use what.

### Video - [5 principles of dataset integrity](https://www.cloudskillsboost.google/course_templates/865/video/523358)

* [YouTube: 5 principles of dataset integrity](https://www.youtube.com/watch?v=sNVq5iOKLCs)

High-quality datasets will yield high-quality insights. So let’s spend some time in this lesson to talk about the 5 principles of dataset integrity. The majority of data quality lectures will have one central theme: Garbage in, garbage out. So if you want to build amazing machine learning models, you can't feed them with garbage. You've got to feed them with really good data to begin with. The same goes with data analysis. You have to get extremely good, clean data in order to get good, useful, actionable insights. So let's talk about what makes quality data and some of the things that we can do to bring data to that level. High quality data sets follow these strict rules, and there are five of them. They're valid: they conform to your business rules. They're accurate, because they conform to some true objective value. They're complete, meaning that you can see the whole picture and you're not just getting a subset of the data, and no data elements should be filtered, truncated, or lost. They're consistent, meaning that there is harmony across the many data systems, and no contradictory facts. And they're uniform too. Next up, we'll look at an example of each of these. When modern database technology is used to design data-capture systems, validity is fairly easy to ensure: invalid data arises mainly in legacy contexts (where constraints were not implemented in software) or where inappropriate data-capture technology was used (e.g., spreadsheets, where it is very hard to limit what a user chooses to enter into a cell, if cell validation is not used). Data constraints fall into the following categories: Data-type constraints Range constraints Mandatory constraints Set-membership constraints Foreign-key constraints Regular expression patterns Cross-field validation Unique constraints And speaking of uniqueness, what do all these images represent or have in common? Each is a unique means of identification. You have the phone number on the left Car license plate number in the middle And mailbox (or address) Why would it be terrible to have duplicative license plate numbers? Speeding tickets from one person’s car could be fined to someone else! And for that same reason, having multiple physical addresses be the same, or people sharing phone numbers, presents a massive problem. It’s the same with data: having duplicate records or having stale records (like an old address) is not ideal, like we saw in the example of an outer join where the results could unnecessarily multiply with duplicate rows. Now, valid data corresponds to range constraints too. Let’s use an example. Say you've got some dice on the left, and you've captured the rolls of the dice in the table on the right. Which value looks out of place in the table? If you answered 7, you could be right. If you answered 1, you could be right too. It really depends on how many dice you rolled. If you rolled one die, then obviously 7 is out of range. But if you rolled two dice, then the total could be 7. Which then means that 1 is out of range, because the lowest total you can roll with two dice is 2. Range and data type are just 2 kinds of constraints we discussed. There’s a bunch more that can affect the validity of your data, so you need to ensure that those constraints are met too. Moving on: data has to match a source of truth, right? That's what makes it accurate. Say you're in the United States and you're presented with this list. Washington, Alaska, California, Hot Dog, Florida, and Texas, and you're like, “wait a minute, I don't remember a US state called Hot Dog.” Because 'Hot Dog' is a string, so it's a valid data type, but it won't match any value in a known source of truth. There’s a set amount of known U.S. States, 50, and Hot Dog is not in that list. Even a good source of truth could have different values in it. For example, some would choose to include overseas territories like Guam or Puerto Rico, while others might list only states. And so it depends on your source of truth, your standards of accuracy, and even your definitions of terms. Accuracy is very hard to achieve through data-cleansing in general, because it requires accessing an external source of data that contains only true values, and that kind of "gold standard" data is often unavailable. Now take a look at these 2 images. Unless you’re an expert traveller or know the area, it may be hard to tell where we’re looking at from just these two images. Visually, it’s pretty easy to tell when we aren’t seeing the whole picture. Sometimes you need to question the source of the data to get the big picture. And once you get a complete picture of the data, you can easily determine that this is in London! With incomplete data it is hard to get a real sense of what is going on. We inherently trust data because if it comes to us in row and column format, we somehow believe that everything we need is in there if we just query it. Sometimes that’s not true! Consequences of incomplete data can include: Insights that are over-dependent on the small segment of data, like the out-of-context Lamp and Clock Land from the previous frame, or Biased results, or Interpreting patterns that are not there (which we call “overfitting” in machine learning models) Next, we have consistency. The example here introduces a dilemma, where there are potentially two owners for a house. Inconsistency occurs when two data items in the data set contradict each other, e.g., a house address is recorded in two different systems as having two different owners, and only one of them can be correct. In the first table, the house at 123 ABC St has an owner ID of 12. But in the second table, owner ID 12 is associated with a different address, and 123 ABC St is associated with an owner ID of 15. In Database 101, we call this a referential integrity issue. So who actually owns that house? Consistent data ensures harmony across many different systems, so you don’t end up in situations like this. Fixing inconsistency is not always possible: it requires a variety of strategies—e.g., deciding which data were recorded more recently, which data source is likely to be most reliable, or simply trying to find the truth by testing both data items (e.g., calling up the customer). Last but not least, there has to be uniformity in data, which means using the same units of measure in all systems. To illustrate just how important that is, listen to this: On November 10, 1999, NASA unfortunately lost its $125 million Mars Climate Orbiter as it burned up into the surface of Mars. This was because of an issue with one team using the imperial measurement system of feet and yards, and another team that was working on a different part of this system using the metric system. Getting your data to that caliber of quality is important for the reasons we just discussed. The examples we used also highlighted some of the reasons and/or challenges affecting your data quality. For example, your data could have competing or out-of-range data types, or there could be duplicates to deal with. In the next lesson, we focus on how to clean, prepare, and transform data with SQL to get a high-quality dataset. But before we get into that, let’s talk a little about dataset shape and skew. Datasets usually come from different sources, and can be in a variety of different forms or shapes. Sometimes you can have a lot of columns, but not that many rows to analyze. We call that a wide but short dataset. In some cases, this may work. In other cases, maybe you could try asking for more data, or asking for data at a different level of detail. Just to the left of that we see a dataset that's taller than wide, meaning that you have a lot of observations, but only a few columns, even as few as two or three. Sometimes that may be enough, and sometimes you need more than just those columns. And again, it depends on your analytical need. Sometimes, you just have a small dataset (like you see on the very left here). You have an average number of columns and an average number of rows. And you know that's all you've got. Of course the perfect case scenario is you have whatever the right amount of columns is for you and enough records to make judgments and inferences from your data and insights. And again, there's no magic number. You need to make sure that there are actually enough observations to make a justifiable conclusion about your records. This is especially true if you plan to use the data for machine learning…but the same is true for data analysis. The other important attribute of the dataset is the skew, or the distribution of values. For example, you may have an employee table and notice that there are way more people in the California office compared to every other office location. Or maybe the dataset is centered around one value (for example, you have an international business but the data for sales orders is only showing one country). Then you start to question the validity of the data. Did we collect everything or did we miss some rows? Does it correspond to what you were expecting? Are there clear outliers or anomalies in your data? And how would you analyze this in SQL? GROUP BY and ORDER BY results will be rather skewed too. So skew is something that you want to watch out for from an exploration standpoint. But skewed data is not necessarily a bad thing. The important thing with skew is considering if the skew is representative of the objective facts or is an artifact of bad data, like missing the sales data outside of a given area.

### Video - [Clean and transform data using SQL](https://www.cloudskillsboost.google/course_templates/865/video/523359)

* [YouTube: Clean and transform data using SQL](https://www.youtube.com/watch?v=jgBC4bcow3A)

In this lesson, we revisit each of the 5 data integrity rules we covered in the previous lesson, and show how you can use SQL to clean, prepare, and transform your dataset to achieve a high level of quality. Let’s start with validity, meaning that your data needs to conform to business rules and constraints. You can do a lot of things directly within SQL and BigQuery to make this happen. You can specify that a column is allowed to be null. Or, maybe it's a required column, like Name or ID fields that are unique identifiers for rows, and so they should not be null. This can be done when you create new tables, so you can ensure that incoming rows, from upstream systems, will be ingested if they have valid data. Alternatively, you can proactively check whether or not those values are valid after the fact. There's a lot of conditional logic like the IF THEN ELSE and the CASE WHEN statements that you can apply in SQL to test for data validation. Now, even when your data matches the data types, and it meets all of your specifications about whether it's allowed to be null or it’s required, you still want to make sure if your data is actually accurate and makes sense. Sometimes you can use calculated fields to set up a test case to check for values. Or, potentially, you can look it up against an objective reference dataset, like in the case of the 50 states we discussed in the previous lesson. JOINs can be one way of setting this up. Another powerful SQL construct that you can explore is using subqueries or CTEs, and testing the occurrence of values with the IN operator. Completeness can be a hard one. Completeness is exploring the data to see if something’s missing. This means checking for the skew, as we mentioned before, and looking for those null values, and handling those null values, and saying “if it's null, do this” or “if this value isn't present, roll back on this value,” which is what the COALESCE function can help with. You can further enrich your dataset with other datasets, and mash them up together to form a more complete picture. We saw some examples of this when we talked about unions and joins, where we combined multi-year data by doing unions on the various temperature reading tables by year, and adding weather station information using joins, thus getting a bigger and better picture of what the reading says and where it was collected, all available in one result. Depending upon what kind of dirty data or inconsistency you're dealing with, there's a whole host of functions to choose from. Parsing out dates, cleaning up a lot of dates, is a common step many data analysts end up doing to clean their data and make it consistent. String functions like SUBSTRING and REPLACE can be very helpful, and very easily and efficiently resolve inconsistencies when sourcing from a number of tables. And it doesn't necessarily mean you have to replace your raw data table. All these steps are going to be parts of a recipe that continue afterward or downstream. So this logic is repeatable when new dirty data comes into your system. Last but not least: uniformity. You want to make sure that all your data types are similar and that you're comparing apples to apples, especially when it comes to visualizing your results. So always document and comment your code. For example, highlight whether you’re are using the metric or imperial system for measurements. Functions like FORMAT and CAST (that we discussed earlier) can be very useful here to standardise formats across the board. Different time zones are another common issue in imported data. Functions like DATE(), TIME(), and DATETIME() all accept a timezone parameter, so you can easily convert to a standard timezone. And lastly, there are functions available to convert Unix epoch time values to a timestamp format for uniformity. Okay, here's an interesting example. We've got some weather data that we're pulling from NOAA, and we're basically saying, give us all the weather data where the U.S. state is not NULL—meaning that it is present—and limit the top 10 results. So the state has to be present. But now the question is, why does the query still show these blank state values when it's clearly filtered on IS NOT NULL? And the correct answer is, it's because it's not null. It's not null because it's blank string. You can't see it normally. Remember: An empty string or blank string is a valid data value whereas a NULL is the absence of any data. A valid null value in BigQuery looks exactly like what these latitude and longitude columns look like. It will say, N-U-L-L there. Well, then, how could we adjust this query to filter out blanks as well? You would have to update your WHERE clause like how it is shown in the slide, where you also filter out blank strings in addition to null values.

### Video - [Clean and transform data: other options](https://www.cloudskillsboost.google/course_templates/865/video/523360)

* [YouTube: Clean and transform data: other options](https://www.youtube.com/watch?v=2H41UMt0GU8)

In this lesson, we will highlight other options, besides SQL, that are available for you to use to clean, prepare, and transform your data. More specifically, we will talk about Cloud Data Fusion, and its Wrangler tool that helps to create many useful transformations. Next, we will briefly introduce Dataflow, where you can code powerful transformations to be applied to your data. Dataprep is yet another UI based tool that data analysts use to do data cleansing and preparation. And then we briefly look at Dataproc, which could be an option for your ETL workloads. There is also the recently launched Dataform, which is available through the BigQuery console, which we will cover in a later module. We conclude this lesson with recommendations on when to use which product. Cloud Data Fusion is a fully managed, cloud native, enterprise data integration service for quickly building and managing data pipelines. It is essentially a graphical no-code tool to help business users, developers, data analysts, and scientists to quickly and easily build, deploy, and manage data integration pipelines. It is a great UI-based tool to consider when not all data can be exported to Excel and Sheets, or you have inconsistent metadata and no business glossary, or operationalizing the data preparation is a challenge Wrangler is a powerful tool in Cloud Data Fusion that helps you view, explore, and create data transformations on your data. It allows you to transform a small sample (10 MB) of your data in one place before running the logic on the entire dataset. This means you can quickly apply transformations to gain an understanding of how they will affect the entire dataset. Wrangler provides an easy and interactive way to visualize, transform, and cleanse data. It allows the user to view data from multiple locations such as Cloud Storage or Kafka, as well as from databases, and derive new schemas and operationalize the data preparation with a few clicks. Wrangler allows you to connect to your data wherever it resides, and transform it using simple, point-and-click transformation steps. You can create multiple transformations and add them to a recipe. When you are satisfied with the results of your recipe, you can create a data pipeline that includes the source and the Wrangler transformation. In the UI, you can add more plugins to continue transforming your data and add a sink to write the transformed data to a target location like BigQuery. Dataflow is a Google Cloud service that provides unified stream and batch data processing at scale. In this case, “processing” refers to the steps to extract, transform, and load data, or ETL. Dataflow uses a data pipeline model, where data moves through a series of stages. Stages can include reading data from a source, transforming and aggregating the data, and writing the results to a destination. Pipelines can range from very simple to more complex processing. For example, a pipeline might: Move data as-is to a destination. Transform data to be more useable by the target system. Aggregate, process, and enrich data for analysis. Join data with other data. Dataflow is based on the open source Apache Beam project. Apache Beam is an open source, unified model for defining both batch and streaming pipelines. The Apache Beam programming model simplifies the mechanics of large-scale data processing. Using one of the Apache Beam SDKs, you build a program that defines the pipeline. Then, you execute the pipeline on a specific platform such as Dataflow. This model lets you concentrate on the logical composition of your data processing job, rather than managing the orchestration of parallel processing. Apache Beam insulates you from the low-level details of distributed processing, such as coordinating individual workers, sharding datasets, and other such tasks. Dataflow fully manages these low-level details. A pipeline can be run locally on your computer, or remotely on a virtual machine, or by using another service in the cloud. The engine that will be powering your pipeline is specified by the Runner. Each runner has its own configuration and an associated backend service. And Dataflow is one of the runners available in Apache Beam. Dataprep is an integrated partner service that is operated by a third party, Trifacta. Dataprep is built on Dataflow, which means it is auto-scalable to any size and can easily process massive datasets. This also means it is fully integrated with Google Cloud, so there is no infrastructure to deploy or manage. It can access raw data from BigQuery, Cloud Storage, or local files. The data can be in CSV, JSON, or a relational table format. Refined data can be directly ported into BigQuery, Looker Studio, Cloud Storage, or Vertex AI for further analysis and storage. When it comes down to executing the transformations, Dataprep easily scales to handle any data, big or small, by using the power of Cloud Dataflow to execute efficiently and in any region supported by Google Cloud. With your refined data, you can load it into BigQuery and perform interactive queries, visualize it with Looker Studio, or load it into Cloud Storage to train machine learning models with Google Cloud ML service. Here are common use cases where customers have found success using Cloud Dataprep. Data onboarding: Analysts need to integrate external or third-party information into their work, but need to ensure that it conforms to internal structures and formats so that it is compatible with existing systems. Data science and machine learning: Preparing data is not only a challenge for analytics, but also for data science and machine learning projects. You need clean, well-structured data to effectively train models. Marketing analytics: As Google Cloud users have access to tons of marketing data, gaining insight often involves combining & enriching that data with a range of datasets from cross functional sources like sales, finance, or support. This can involve manipulating those datasets into forms that can be joined, aggregated, and cleansed. Dataprep is a graphical user interface (GUI) for creating data transformation steps. You can pick from predefined wranglers to process your data. You can find more information at https://cloud.google.com/dataprep Review the final data flow and then run the job, which will then kick off a new Dataflow job behind the scenes. Use dashboards to monitor your jobs, and dig into the metrics for your transformed datasets. And finally, let’s briefly talk about Dataproc. Dataproc is a fully managed and highly scalable service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. Use Dataproc for data lake modernization, ETL, and secure data science, at scale, integrated with Google Cloud, at a fraction of the cost. With Dataproc, enterprises get a fully managed, purpose-built cluster that can autoscale to support any data or analytics processing job. Dataproc has built-in integration with other Google Cloud Platform services, such as BigQuery, Cloud Storage, Cloud Bigtable, Cloud Logging, and Cloud Monitoring, so you have more than just a Spark or Hadoop cluster—you have a complete data platform. For example, you can use Dataproc to effortlessly ETL terabytes of raw log data directly into BigQuery for business reporting. Now that we have discussed a little bit about each of these products, here is a summary. Cloud Data Fusion (and its Wrangler tool) and Dataprep come with a graphical user interface that can help with the wrangling activities. With Dataflow, you have more flexibility because you can code all your transformations, but that means that you will need to do quite a bit of coding. Dataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataform helps you develop and operationalize scalable data transformation pipelines in SQL, and we will cover that in a later module. The intention in this lesson was to highlight other options, besides SQL, that are available for you to use to clean, prepare, and transform your data. For more details on using each of the products, feel free to refer to our product documentation site.

## Ingesting and Storing New BigQuery Datasets

This module talks about ingesting and storing data into BigQuery native storage. We discuss when to use Extract and Load, versus Extract, Load and Transform, versus Extract Transform and Load approaches for loading data into BigQuery.We also cover external data sources, where you can run your query in BigQuery, but the data is hosted outside of BigQuery.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/865/video/523361)

* [YouTube: Module overview](https://www.youtube.com/watch?v=R9zOMwM0UMk)

Welcome to module 4, Ingesting and Storing New BigQuery Datasets. One of the building blocks of data analysis is creating SQL queries on raw data sources and then saving your results into new reporting tables that you can then query from. In the first lesson, we’ll cover the difference between permanent and temporary data tables and how to store the results of your queries. So far, all the queries we have executed have been on tables or sources that have been previously created and shared via the course dataset or via BigQuery public datasets. In the second lesson, we will look at how to create new datasets and tables, and ingest data into them. And finally, we also cover external data sources, where you can run your query in BigQuery, but the data is hosted elsewhere.

### Video - [Permanent versus temporary data tables](https://www.cloudskillsboost.google/course_templates/865/video/523362)

* [YouTube: Permanent versus temporary data tables](https://www.youtube.com/watch?v=-uQPSP7IgU4)

Let’s start with permanent versus temporary tables in BigQuery. When you run queries in BigQuery, the query results are always saved either to a permanent table (which you can specify) or a temporary table (even if you don't explicitly create one). Many times, data analysts run SQL queries on raw data sources and then save the results into new reporting tables for further analysis in the future. Before you run the query, you can specify a destination table, and choose whether to append or overwrite data in an existing table OR to create a new table if none exists by that name. Now, if you forget to specify a destination table before running your query, the results get copied into a temporary table, that is available for 24 hours. This also means that you can also copy the temporary table to a permanent table by clicking the Save Results button in the results window, and choose BigQuery table. Recall the discussion on query cache from an earlier module. That is basically your temporary table. So same queries in the near future will use cached results when available, assuming: The underlying table has not been modified, and And No CURRENT DATE or similar nondeterministic function is used in the query. A view is a virtual table that’s defined by a SQL query. Think of it as a saved SQL query. So when you select from a view, you are essentially selecting from a SELECT statement, which in turns selects from a table. This means it does not store the results. (Later we’ll talk about materialized views, where the results can be stored.) You can query views in BigQuery using the web UI, the command-line tool, or the API. You can also use a view as a data source for a visualization tool such as Looker or Looker Studio. Essentially, it is pretty much like a table. Because views are not materialized, the query that defines the view is run each time the view is queried. Queries are billed according to the total amount of data in all table fields that are referenced directly or indirectly by the top-level query. You can also create the view by using the CREATE statement, which is a DDL, or Data Definition Language. The CREATE VIEW statement specifies that this query is to be saved as a view. You can use the OR REPLACE keyword phrase to overwrite the view query, if one already exists with the name you’ve specified. Alternatively, you can use the IF NOT EXISTS keyword phrase to create the view only if that name is not already used—so no overwriting any existing view with that name. This discussion would not be complete without showing you the DDL syntax to create a new table schema. This will create an empty table in the specified dataset, using the specified name, with the corresponding columns and data types. And in the future you can use the INSERT statement (which is a DML, or data manipulation language) to insert rows into the table. You can also use a load job from the command line to populate the table with data coming from a CSV file, for instance. In BigQuery, materialized views are precomputed views that periodically cache the results of a query for increased performance and efficiency. BigQuery leverages precomputed results from materialized views and, whenever possible, reads only delta changes from the base table to compute up-to-date results. Materialized views can be queried directly or can be used by the BigQuery optimizer to process queries to the base tables. Queries that use materialized views are generally faster and consume fewer resources than queries that retrieve the same data only from the base table. Materialized views can significantly improve the performance of workloads that have the characteristic of common and repeated queries. In the example on the slide, Step 1 shows how a materialized view is created (and automatically kept fresh). Step 2 shows queries still going to the base table. Note: You can explicitly query the materialized view as well. Step 3 shows queries being executed against the materialized view for faster and more efficient execution. Here are a few things to know about materialized views: You can grant access to a materialized view at the dataset level, the view level, or the column level. You can also set access at a higher level in the IAM resource hierarchy. You can manually refresh a materialized view at any time. You can partition a materialized view if the base table is partitioned as well. By default, materialized views are automatically refreshed within 5 minutes of a change to the base table, but no more frequently than every 30 minutes.

### Video - [Lab intro: Creating new Permanent Tables](https://www.cloudskillsboost.google/course_templates/865/video/523363)

* [YouTube: Lab intro: Creating new Permanent Tables](https://www.youtube.com/watch?v=ICcQFx89gX4)

In this lab, you will learn how to create new permanent reporting tables and logical views from an existing ecommerce dataset. More specifically, you will create tables and access-controlled views using SQL DDL (or Data Definition Language) in BigQuery.

### Lab - [Creating Permanent Tables and Access-Controlled Views in BigQuery](https://www.cloudskillsboost.google/course_templates/865/labs/523364)

This lab focuses on how to create new permanent reporting tables and logical reviews from an existing ecommerce dataset.

* [ ] [Creating Permanent Tables and Access-Controlled Views in BigQuery](../labs/Creating-Permanent-Tables-and-Access-Controlled-Views-in-BigQuery.md)

### Video - [Ingesting new datasets](https://www.cloudskillsboost.google/course_templates/865/video/523365)

* [YouTube: Ingesting new datasets](https://www.youtube.com/watch?v=w9sRDA5HGnU)

In the previous lesson, we learned how to save the results of running SQL queries on BigQuery tables to other, permanent BigQuery tables for further analysis. But what if the source data is not in BigQuery yet? In this lesson, we will discuss the different methods of loading data into BigQuery. And the method you use to load data depends on how much transformation is needed. EL, or Extract and Load, is used when data is imported as-is, where the source and target have the same schema. ELT, or Extract, Load, Transform, is used when raw data will be loaded directly into the target and transformed there. ETL, or Extract, Transform, Load, is used when transformation occurs in an intermediate service before it is loaded into the target. Let’s discuss these in a little more detail. You might say that the simplest case is EL. If the data is usable in its original form, there’s no need for transformation. Just load it. In the hands-on lab at the end of this module, you will get to practice ingesting data from a variety of sources. You can also use BigQuery Data Transfer Service to automate data movement into BigQuery on a scheduled, managed basis. Sometimes, the raw data in the source system is not as usable in its original form, and so you need to apply some transformations to it. One option is to load it into BigQuery and then use all the awesome SQL knowledge you have accumulated so far to build and apply your transformations. The ELT approach based on BigQuery is quickly becoming the center of the data stack. We are seeing a trend where some organizations are slowly moving from an ETL approach, with their pipelines in dedicated ETL tools and environments like Spark, to an ELT approach, with SQL pipelines in BigQuery. In a later module, we will talk about how to use Dataform to develop scalable data transformation pipelines in BigQuery. When EL or ELT is preferred, BigQuery can ingest datasets from a variety of different formats. Once data is inside BigQuery native storage, it is fully managed by the BigQuery team here at Google. And by that, we mean BigQuery manages things like replication, backups, scaling out size, and many more. You also have the option of querying external data sources directly and bypassing BigQuery managed storage. We will talk more about this in the next lesson. And you also have the option to stream records into BigQuery. You can use tools like Dataflow or Data Fusion, or the API, to set up a streaming pipeline. One use case for this is event logging. And one example of high-volume event logging is event tracking. Suppose you have a mobile app that tracks events. Your app, or mobile servers, could independently record user interactions or system errors and stream them into BigQuery. You could analyze this data to determine overall trends, such as areas of high interaction or problems, and monitor error conditions in real time. If you want to learn more about streaming data for building real-time dashboards, please refer to the documentation. Sometimes, the transformations are rather complex, and large enough in number that ETL may be the best option. In such cases, you can always use services like Cloud Data Fusion, Dataflow, or Dataproc to build your pipelines and code the transformations that need to be applied first before the data is loaded into a target destination like BigQuery.

### Video - [External data sources](https://www.cloudskillsboost.google/course_templates/865/video/523366)

* [YouTube: External data sources](https://www.youtube.com/watch?v=UKm78Ku_Uqs)

In this lesson, we will briefly talk about external data sources. An external data source is a data source that you can query directly from BigQuery, even though the data is not stored in BigQuery storage. For example, you might have data in a different Google Cloud database, in files in Cloud Storage, or in a different cloud product altogether that you would like to analyze in BigQuery, but that you aren't prepared to migrate. Broadly speaking, BigQuery has two different mechanisms for querying external data: external tables and federated queries. And we will discuss them in more detail in the next few slides. Take note that we also have products like BigQuery Omni and BigLake tables to access data in AWS buckets and Azure blob storage. These are beyond the scope of this course, but there is a lot of information on our documentation site if you are interested. Use cases for external data sources include the following: Ad-hoc queries where performance is not an issue. Accessing short term, less-frequently accessed data. Accessing constantly changing data (like from a Google sheet). Or joining BigQuery tables with frequently changing data from an external data source. By querying the external data source directly, you don't need to reload the data into BigQuery storage every time it changes. For extract-load-transform (ELT) workloads, loading and cleaning your data in one pass and writing the cleaned result into BigQuery storage, by using a CREATE TABLE ... AS SELECT query. And accessing data in AWS buckets or Azure blob storage with BigQuery Omni remote tables Here is a table that compares external tables versus federated queries. Firstly, the choice will definitely depend on the supported data source. So if you are trying to query data in Cloud Spanner, then federated queries will be the setup you use. Available data sources for external tables include Cloud Bigtable, Cloud Storage, and Google Drive, while available data sources for federated queries include Cloud Spanner and Cloud SQL. For external tables, the table metadata and schema are stored in BigQuery storage, but the data itself resides in the external data source. Whereas for federated queries, getting metadata requires sending a query to the external database because the table metadata is not stored in BigQuery. When it comes to access controls, external tables have more flexibility and support column- and row-level security. BigQuery Omni is a type of external table that allows you to query data across clouds, for data sources in S3 and Azure Blob Storage service. Let’s quickly cover some considerations when working with external tables. Since the tables are located outside of BigQuery, performance is generally slower compared to native tables that reside in BigQuery. Caching and query size estimates are not supported. BigQuery external tables have specific requirements about the region(s) the cloud storage is located in. Please refer to the documentation at https://cloud.google.com/bigquery/docs/locations#data-location for details on this topic. External data source limitations include the following: BigQuery does not guarantee data consistency for external data sources. Changes to the underlying data while a query is running can result in unexpected behavior. Query performance for external data sources may not be as high as querying data in a native BigQuery table. If query speed is a priority, load the data into BigQuery instead of setting up an external data source. The performance of a query that includes an external data source depends on the external storage type. For example, querying data stored in Google Cloud Storage is faster than querying data stored in Google Drive. In general, query performance for external data sources should be equivalent to reading the data directly from the external storage. You cannot run a BigQuery job that exports data from an external data source. You cannot reference an external data source in a wildcard table query. As you have seen, there are a variety of ways to get your new data into BigQuery. We covered loading and storing this data as new permanent tables, which have the benefit of being fully managed. We also looked at querying external data directly and why this may be useful for one-time Extract, Load, Transform jobs. Lastly, you can stream individual records into BigQuery through other products or an API. Next up is our lab where we will practice creating new datasets, loading external data into tables, and running queries on this new data.

### Video - [Lab intro: Ingesting and Querying New Datasets](https://www.cloudskillsboost.google/course_templates/865/video/523367)

* [YouTube: Lab intro: Ingesting and Querying New Datasets](https://www.youtube.com/watch?v=AAPnVlbwC60)

In this lab you will ingest several types of datasets into tables inside of BigQuery. More specifically, you will ingest data from sources like CSV files, Cloud Storage and Google Sheets. Towards the end of lab, you also get to create an external table connection into BigQuery from Google Sheets. Refer to https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/data-to-insights/demos/external-data-query.sql

### Lab - [Ingesting New Datasets into BigQuery](https://www.cloudskillsboost.google/course_templates/865/labs/523368)

This lab focuses on how to ingest new datasets into tables inside of BigQuery.

* [ ] [Ingesting New Datasets into BigQuery](../labs/Ingesting-New-Datasets-into-BigQuery.md)

## Visualizing Your Insights from BigQuery

This module is where all that hard work around ingesting, cleaning, preparing, and transforming your data comes to fruition as you get to visualize insights from your data by building insightful dashboards and reports. We start off with a little visualization theory and some best practices, and then look at tools, like Looker Studio and Connected Sheets, that can connect to BigQuery and help create impactful visualizations to capture and convey your insights. Although SQL is a powerful query language, programming languages such as Python, Java, or R provide syntaxes and a large array of built-in statistical functions that data analysts might find more expressive and easier to manipulate for certain types of data analysis. Such tools include open source web-based applications like Jupyter Notebooks, and so we discuss these as well.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/865/video/523369)

* [YouTube: Module overview](https://www.youtube.com/watch?v=1Rk_92TW3TQ)

Welcome to module 5, Visualizing Your Data Insights from BigQuery. One of the key outputs that data analysts create are insightful reports for their audience. In this data visualization module, we will cover a little visualization theory and some best practices, and then we’ll look at tools like Looker Studio and Connected Sheets, that can connect to BigQuery and help create impactful visualizations to capture and convey your insights. Now, although SQL is a powerful query language, programming languages such as Python, Java, or R provide syntaxes and a large array of built-in statistical functions that data analysts might find more expressive and easier to manipulate for certain types of data analysis. Such tools include open source web-based applications like Jupyter Notebooks, and so we will take some time in the last lesson to talk about this. Recall in an earlier module we mentioned that there are data preparation and visualization tools that you can use as well for exploring data. Visualization tools can not only help to capture and convey your insights in a powerful visual way, but for data analysts who are visual thinkers, they are yet another way to explore data as well. Let’s briefly discuss some common themes in the next lesson.

### Video - [Data visualization principles](https://www.cloudskillsboost.google/course_templates/865/video/523370)

* [YouTube: Data visualization principles](https://www.youtube.com/watch?v=u3uFxj59Bn0)

Let’s start with some data visualization principles. Firstly, visualization tools can help spot hidden trends a lot easier than staring at rows of data. Visualization tools also make it easy to “see” data interactively. Say you start with time-series data, and then you just click into one of the anomalies or spikes in the data and you drill down into those details visually. Then it becomes a natural tool to tell your audiences a cohesive story throughout the flow of your exploration. Building dashboards and reporting summaries is not just aesthetically pleasing, but also allows for conveying of insights in a very fast and effective manner. Think of a billion rows analyzed and reduced by powerful SQL into few rows of grouped aggregates to support decision making. And lastly, if a lot of your data is already in BigQuery, putting a visualization tool on top of it like Looker Studio will naturally get all the performance benefits of having your data and your queries processed in BigQuery, and then displayed and rendered quickly on the front end in a visualization tool. Essentially, you get to build visualizations without writing SQL. No lecture on data visualization will be complete without talking a little bit about visualization theory. Visualization is both an art and a science. So here we're going to get a little bit of the science of what our brains perceive when you look at a beautiful visualization. It would be fair to say that we don’t just see things with our eyes, but we see things with our brains too. Eyes record light, translate it into electrical signals, and pass it to our brains, where the message is processed and understood. In this particular case, we have a stimulus where we have a cat. Your eyes immediately recognize it and say, "Hey, I've seen that before, that's a cute little kitten." And your brain automatically says, "Hey, I've seen a thousand of these before, I immediately know without thinking that it's a cat." Intuition is extremely hard to build in for a computer. Whereas as humans, we're built with a lot of what we call pre-attentive processing, where we can immediately recognize things. Preattentive processing is automatic, unconscious, and you can do it really fast. Now, what does that mean for your data visualizations? It means you can effectively cheat the brain by using common human intuition to not make the brain do a lot of work in the form of attentive processing. So let's take a look at an example together to illustrate this. If you were given 10 seconds to count all the number fives that are present in this visual, then it would be a really hard exercise. Now, if you counted 16 fives in the 10 seconds, then that’s absolutely amazing. But for the majority of us, it's very, very hard to pick out from this noise of numbers here, this very crowded visual, where those fives are. It took longer because you had to use focused attentive processing and scan every single row and value. . Let’s change the visual a little here. Now count the fives. I bet it is a little faster this time round. With the bolded fives, it is a little easier to spot them. Let's think of the theory behind that. When you contrast certain elements, you highlight the focus, and it allows you to treat all the other numbers that aren't fives as background noise that can be safely ignored. We can take this yet a step further and continuously add more of what we call visual encoding on these particular elements that we want to add focus to. It becomes so much easier now with the help of preattentive processing. The visual encoding highlights the relevant information and helps us to understand the message faster using the preattentive space. That leaves the attentive space to focus on other things. Now, there's a variety of methods that you can use to tap into that fast processing time. You can fine-tune with things like the orientation, the shape and the skew, the length of a certain attribute, the size of the mark on the page. Maybe some elements are curved and some aren't. You can add things like a box around it, changing the intensity or the hue, moving around in the positioning or even adding things like motion elements as well. All of these will help the brain focus in on what's important. Let’s look at another example. Which one is a more effective visual? The donut chart is likely the better visual for quick comparison of book characters by gender. Line graphs are mainly for time series, and this graph type makes less sense to use here. How about this example? The horizontal bar chart allows you to have dimensions with valid 0 values (like two books that have not been written yet). Donut charts can get crowded with multiple dimensions. The key message is that your audience is likely only to see the end product of your work. Your visuals and reports often carry more visual weight than your SQL scripts and analysis. One of the core concepts that you will come across in many visualization tools is dimensions versus measures. These concepts are built into how visualization tools create informative and insightful visualizations. This was covered in great detail in the prerequisite “Introduction to Data Analytics” course. So let’s do a quick recap here. Dimensions contain qualitative values (such as names, dates, or geographical data). You can use dimensions to categorize, segment, and reveal the details in your data. Measures contain numeric, quantitative values that you can measure. Measures are aggregations of one or more dimensions (or unique attributes of the data) such as a count or an average. But take note that just because a field is an integer, that does not naturally qualify it as a measure. We will talk about dimensions and measures in a later lesson when we cover Looker Studio. Here's an example of an actual Looker Studio report. The numbers here don’t have to be meaningful to you, but just on the surface you can already see some of the concepts that we just discussed, like visual encoding. Looking here at the histogram, the line chart, and the geographic map, you can see how you’re able to rely on pre-attention processing to understand the big picture quickly. The role of the report is to: Visualize data Restrict data Share with viewers and editors, and To make the data easy to consume and base a decision on.

### Video - [Connected Sheets](https://www.cloudskillsboost.google/course_templates/865/video/523371)

* [YouTube: Connected Sheets](https://www.youtube.com/watch?v=A2d_y4xCc_E)

With Connected Sheets, you can bring the power of Google Cloud and the familiarity of Google Workspace together, so that anyone can analyze data at scale. So let’s take a look. Connected Sheets democratizes big data by providing the ability to access, prepare, analyze, visualize, and share BigQuery data via the simple, familiar interface of a spreadsheet. With that in mind, let’s discuss the main benefits of Connected Sheets: Reduce the burden and reliance on data scientists and other specialized roles to perform analysis. Enable a broader group of users to do things on their own that they previously would have had to request, which has the bonus domino effect of reducing the workload on those specialized workers. Increase the scale of analysis by eliminating the need to use BigQuery directly for massive amounts of data. Google Sheets can handle it. Make use of tools, processes and features that are familiar to a wider range of users. The true data itself remains secure in the original data source. Empower more of the organization to leverage data Enable more of your workforce to analyze data and find valuable insights. Free up data practitioners to focus on higher-value analysis. Increase scale of analysis Analyse billions of rows of data using Google sheets. Familiar, intuitive, and secure Use familiar Sheets tools like pivot tables, charts, and formulas. Avoid the security and performance challenges associated with traditional spreadsheets. In the rest of this lesson, we will discuss how you can easily access large datasets in BigQuery via Sheets, and use familiar Sheets analysis features to generate insights. We will also look at scheduling refreshes to ensure that data is up to date, and sharing the generated insights with secure collaboration options. You can access BigQuery data from within Sheets in a few easy steps: Start with a blank Google Sheet On the top menu, choose Data, then Data Connectors, then Connect to BigQuery Choose a project (this is where the dataset resides, or this could be the public dataset BigQuery project ) Then choose the appropriate dataset Select the table and connect Once the table rows are populated…. you can start analyzing! Another way to set up connected sheets is from the BigQuery web UI. On the Schema tab, using the Export option, you can choose Explore with Sheets, and this will create the connected sheets for you. If you want to only see selected rows based on a query, you can again use the BigQuery UI, run the SQL query, and then use the Explore data dropdown button and select Explore with Sheets. At this point, it is important to differentiate yet another export option that is available in the BigQuery UI. Using the Save results button, you can choose to export the results of a query into a Google Sheet, but without a live connection. Think of it like a copy-paste function where you can simply copy the results over into your spreadsheet. Alright. Once we’ve connected sheets to our dataset, we can leverage the familiar tools we use in Sheets to conduct some analysis, including: Charts Functions Pivot Tables And creating data extracts In this example, we’re creating a pie chart to see what forms of payments are people using for their taxi rides. There are a number of ways to refresh data: Selecting an individual object to refresh, or Refreshing an entire Sheet, or Setting scheduled refreshes Refreshing is manual by default, so data will not change regardless of updates in BigQuery unless Sheets is refreshed too. Take note that for scheduled refreshes, currently the highest available cadence is every one hour. Users don’t encounter the corruptibility and performance challenges associated with using traditional spreadsheets as databases, and the live connection to BigQuery means data is easy to refresh and up-to-date. There’s also less risk because data isn’t stored on individual workstations and the chance of accidental data manipulation is eliminated. Finally, enabling more users to conduct data analysis opens up the world of BigQuery data to Sheets users and therefore the potential for organizations to churn through analysis tasks.

### Video - [Lab intro: Connected Sheets Qwik Start](https://www.cloudskillsboost.google/course_templates/865/video/523372)

* [YouTube: Lab intro: Connected Sheets Qwik Start](https://www.youtube.com/watch?v=ZrwYpZLT6Ms)

In this hands-on lab, you connect a BigQuery dataset to Google Sheets. Once connected, you leverage the familiar tools that you use in Sheets to conduct analysis, including: Charts, Functions and Pivot Tables. You also create data extracts, calculated columns and set up a refresh schedule for your data.

### Lab - [Connected Sheets: Qwik Start](https://www.cloudskillsboost.google/course_templates/865/labs/523373)

Connected Sheets, you can analyze billions of rows and petabytes of data in Google Sheets without specialized knowledge of computer languages like SQL. In this hands-on-lab, you will learn how to connect a BigQuery dataset to Google Sheets and analyze your data.

* [ ] [Connected Sheets: Qwik Start](../labs/Connected-Sheets-Qwik-Start.md)

### Video - [Common data visualization pitfalls](https://www.cloudskillsboost.google/course_templates/865/video/523374)

* [YouTube: Common data visualization pitfalls](https://www.youtube.com/watch?v=T8UH203KEQU)

In this lesson, we discuss a common data visualization pitfall, and share some recommendations. Here is a common issue most analysts have to deal with when designing dashboards. And the main reason is usually where is the data being pulled from. Is it already sitting in a table, and the dashboard is simply retrieving it? Or are you running some complex SQL queries on the fly to compute the results and then populating your dashboard, like in the case of views? Another way to phrase the question is: Where in the data freshness spectrum do you want to be? Storing precomputed results in a table is great, but the data can become outdated over time. So maybe views are a better option, but views can be performance intensive and the query has to complete every time you load the dashboard. Or maybe the best solution is to be somewhere in the middle of the data freshness spectrum, where you create reporting tables that are refreshed over a schedule. That way, you are not getting extremely outdated data, but at the same time you are not running queries on the fly as your dashboard is trying to present information. You can schedule queries to run on a recurring basis. Scheduled queries can include data definition language (DDL) and data manipulation language (DML) statements. You can organize query results by date and time by parameterizing the query string and destination table. And you create multiple scheduled queries based on your visualization needs. When you create or update the schedule for a query, the scheduled time for the query is converted from your local time to UTC. UTC is not affected by daylight saving time. Before we end this lesson, there is one more tool we want to highlight, which is the BigQuery BI Engine. Building real-time analytics dashboards with sub-second query latency can be fraught with complexity or high cost. But, does it really have to be? BI Engine is about enabling data and business analysts to perform interactive analysis in real time at scale. One of the Google Cloud products that helps manage the performance of dashboards is BigQuery BI Engine. BI Engine is a fast, in-memory analysis service that is built directly into BigQuery and available to speed up your business intelligence applications. Historically, BI teams would have to build, manage, and optimize their own BI servers and OLAP cubes to support reporting applications. Now, with BI Engine, you can get sub-second query response time on your BigQuery datasets without having to create your own cubes. BI Engine is built on top of the same BigQuery storage and compute architecture and servers as a fast in-memory intelligent caching service that maintains state. Refer to https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/data-to-insights/demos/scheduled-query.sql

### Video - [Looker Studio](https://www.cloudskillsboost.google/course_templates/865/video/523375)

* [YouTube: Looker Studio](https://www.youtube.com/watch?v=rk5Qmy6sX5g)

Let’s look at another popular visualization tool called Looker Studio. Let’s first distinguish Looker from Looker Studio. Looker is a business intelligence software and big data analytics platform that helps users explore, analyze, and share real-time business analytics. Looker Studio, formerly known as Data Studio, is a web interface that makes it easy to create interactive dashboards and reports from a wide variety of sources, driving smarter business decisions. Data sources act as pipes to connect a Looker Studio report to underlying data. This is the Looker Studio home page. There are two ways to create a new report from scratch. You can either select Blank Report from the template gallery in the middle of the screen. Or, you can click the Create button in the navigation pane on the left of the screen. Note that you can have any or all of these data sources in a single Looker Studio report. In addition to the Google connectors, there is a growing list of Partner connectors to choose from as well. Since Looker Studio reports can be shared, you should be aware of the ramifications of adding a data source. When you add a data source to a report, other people who can view the report can potentially see all the data in that data source. And anyone who can edit the report can use all the fields from any added data sources to create new charts with those fields. To add the data source, click Add to report. Having selected a dataset, you can specify what elements of the dataset you wish to visualize. This includes selecting the Dimensions and Metrics that you want to use from the Data of your dataset. Recall that we talked about dimensions and measures in an earlier lesson. Metrics here are what we referred to as “measures” in our earlier discussion, and may be called measures in other visualization tools. You can select the option to edit the data source, with the pencil icon in front of the data source name, to edit the dataset fields. Alternatively, you can create calculated fields in your Looker Studio report. You can add calculated fields directly to a chart in your report. These chart-specific (also known as "chart-level") calculated fields can do math, use functions, and return results based CASE statements, just like calculated fields in a data source. The CASE statement, for example, defines one or more conditions and returns a result when a condition is met. Easily change your data table view to a chart by clicking Chart in the properties panel and selecting a chart type from the options provided. You can edit the style of your chart, or even change your chart type selection, later. You can also revert back to a data table view. You can also add separate charts by selecting Add a chart from the toolbar. Resize the components on the canvas to arrange data tables and different chart types as required. In the same way that you defined Dimensions and Metrics earlier, do the same for your chart by adding selections from the Data list. Tip: The sequence of the fields under Metric will determine the order in which the data is displayed in the chart. Use the drag feature to easily change the sequence of the fields. Give your report a name. Since Looker Studio is based on Google Drive, note that you can have duplicate file names. Click the View toggle button to view the end-user version of the report. And here is your report. Notice that it looks very similar to when you were editing it, but as a viewer, you can't modify the report. When a viewer mouses over the chart, they are able to view live data. In this example, the viewer is able to see that in the year 2000, there were 31 natural disasters attributable to extreme temperature. Note that users cannot edit your reports unless you give them permission. Looker Studio's vision as a product is to simplify each of the critical steps in producing reports and dashboards. The Looker Studio mantra is "Connect, visualize, share." Sharing in Looker Studio is similar to sharing in Google Drive. Take note that sharing a report does NOT share direct access to any added data sources. These must be shared separately. Refer to https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/data-to-insights/demos/data-studio.md Refer to https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/data-to-insights/demos/data-studio.md

### Video - [Lab intro: Explore and Create Reports with Looker Studio](https://www.cloudskillsboost.google/course_templates/865/video/523376)

* [YouTube: Lab intro: Explore and Create Reports with Looker Studio](https://www.youtube.com/watch?v=zLEBnQ8xC9s)

This lab focuses on how to create new reports and explore your ecommerce dataset visually for insights. More specifically, you will learn how to connect Looker Studio to Google BigQuery data tables, create charts, and explore the relationships between dimensions and measures.

### Lab - [Explore and Create Reports with Looker Studio](https://www.cloudskillsboost.google/course_templates/865/labs/523377)

In this lab, you learn how to connect Looker Studio to Google BigQuery data tables, create charts, and explore the relationships between dimensions and measures.


* [ ] [Explore and Create Reports with Looker Studio](../labs/Explore-and-Create-Reports-with-Looker-Studio.md)

### Video - [Analysis in a notebook](https://www.cloudskillsboost.google/course_templates/865/video/523378)

* [YouTube: Analysis in a notebook](https://www.youtube.com/watch?v=CuJ7f4mEUzs)

Standard software development tools are not very efficient for writing code for data analysis and machine learning. Notebooks are integrated with BigQuery, Dataproc, and Dataflow, making it easy to go from data ingestion to preprocessing and exploration, and eventually model training and deployment. Data analysis and machine learning coding often involve looking at plots, repeatedly executing small chunks of code with minor changes, and frequently having to print output. Running whole scripts iteratively for this task is burdensome. These were some of the issues that motivated the development of notebooks. Notebook environments seamlessly integrate commentary, plots, and code. Rather than having a script that performs a piece of analysis, notebooks organize individually executable pieces of code into cells. Think of Google Docs. How is it different from documents edited in a desktop editor? Another example is filing taxes online. How is the experience different from doing your taxes in a desktop program? There are lots of benefits, but one key aspect is collaboration. You don’t have to email documents back and forth. In the same way, Python notebooks bring the same benefits to data analysis and machine learning. You write the code, create the graph, write some commentary, and send the notebook link to your colleagues. This way, if they want to add one more year of data, they can simply edit the cell, adding more to the graph. Traditional notebooks ran on individual servers, and we have thoroughly discussed challenges with this setup in earlier modules. But when your notebooks are hosted in the cloud, they are always available, and you can develop together quite easily. Let’s talk about how you can set up a Python notebook in Google Cloud. Spinning up a notebook can be done in one click. You may have heard of Jupyter Notebooks. Jupyter Notebooks are basically synonymous to notebooks on Google Cloud Vertex AI. The provided Python environment has a standard machine learning library pre-installed. Notebooks use the latest open source version of the industry standard JupyterLab. Note that notebook instances are treated in the same way as standard Compute Engine instances living in your projects. If you start a notebook, log in to the Cloud Console and navigate to VM instances. You will see your notebook instances under Compute Engine. When you initially set your notebooks hardware, you can use any Compute Engine instance type. You can also easily adjust the underlying hardware, including machine types. To share a notebook within a project, other users can simply “connect” to the VM and work using the URL. Colab is a hosted Jupyter Notebook service that requires no setup to use and provides free access to computing resources, including GPUs and TPUs. Colab is especially well suited to machine learning, data science, and education. Vertex AI now offers an embedded Python notebook built using Colab Enterprise. It provides one-click Python development runtimes. This comes with built-in support for BigQuery DataFrames, which we will cover next. You may be familiar with magic functions in JupyterLabs. Magic functions allow you to execute system commands from within notebook cells. There are magic functions to check the contents of your current directory. You can also define custom magic functions. The BigQuery magic function shown in this slide allows you to execute BigQuery queries. This is useful for checking query format, accuracy, and output. The BigQuery magic function allows you to save the query output to a Pandas DataFrame, so that you can manipulate it further. In this example, we are saving the output of a query to a Pandas DataFrame named df. Pandas is a numeric library for Python, and it displays data in a tabular structure. It also shows you metadata so that you can do things like describe data frames and object types, and generate summary statistics on the columns. This is really powerful, to be able to just pull query results directly into a DataFrame in two lines of code. Keep in mind that you are working in a notebook and there will be a limited amount of memory. So you wouldn't want to go to BigQuery and pull up a big, multi-terabyte or larger dataset. Instead, you’d use a sampling technique to pull a subset of the data. Here is an example of how you can execute a BigQuery query to pull data into two Pandas DataFrame objects. A simple bar plot is then generated from the data frame. Visualizing data is both an art and a science. We’ve just scratched the surface of visualization theory when we discussed preattentive versus attentive processing for quick eye-to-brain understanding. Along the way we saw some terrifically bad ways to visualize data with the wrong charts, and picked up some best practices along the way. We also looked at using Connected Sheets and Looker Studio for visualizing insights, which are the platforms you will be exploring in more depth in your next lab. And finally, we briefly discussed doing data analysis from within a Jupyter notebook where you can go beyond SQL and use Python, Java, or R programming languages. Google Cloud partners and third-party developers have developed multiple integrations with BigQuery, making it easy to load and process data, and make interactive data visualizations.

## Developing scalable data transformations pipelines in BigQuery with Dataform

Creating, maintaining, and versioning SQL pipelines is a lot of hard work. And many times, data analysts have to use multiple tools to achieve this. So in this module, we introduce Dataform, a new product that offers a unified end-to-end experience to develop, version control, and orchestrate SQL pipelines in BigQuery.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/865/video/523379)

* [YouTube: Module overview](https://www.youtube.com/watch?v=4moMi1MoRT0)

Welcome to module 6, Developing Scalable Data Transformation Pipelines in BigQuery with Dataform. In this module, we look at how Dataform offers that unified end-to-end experience to develop, version control, and orchestrate SQL pipelines in BigQuery. We end the module with a demo and you get to practice using Dataform with a hands-on lab.

### Video - [What is Dataform ?](https://www.cloudskillsboost.google/course_templates/865/video/523380)

* [YouTube: What is Dataform ?](https://www.youtube.com/watch?v=IRaK7YkjPmY)

Dataform helps data teams build, version control, and orchestrate SQL pipelines in BigQuery. We are seeing a trend where organizations are slowly moving from an ETL approach, with their pipelines in dedicated ETL tools and environments like Spark, to an ELT approach, with SQL pipelines in BigQuery. Reasons include BigQuery being serverless, so there are no Spark clusters to manage. Also, the availability of analysts with SQL skills is much higher than with Spark or other ETL tool skills. Customers who do ELT centralize their raw data from disparate systems in BigQuery—resulting in thousands of incompatible data tables that need to be transformed before doing any analytics. So we then have various transformation layers, with different levels of data curation that lead to a final layer from which we serve the curated data. This single source of truth layer has the level of quality, completeness, and freshness that we need so that analytical applications, reporting dashboards, and visualization tools can use it to derive insights and make important business decisions. The big challenge, though, is that transforming data is spread across tools and processes, with some of the steps being more manual than we would prefer. It's not easy to reuse SQL statements across different scripts, there's no way to write tests to ensure data consistency, and dependency management requires external software solutions. Documentation and metadata are afterthoughts because they need to be managed in an external catalog. And that’s where Dataform comes in. With dataform, you can develop and operationalize scalable data transformation pipelines in BigQuery using SQL. The framework enables data teams to build a central data model, a single source of truth, with tables that are curated, continuously updated, documented, and tested without managing infrastructure. Dataform’s framework lets you do all the things you want like create table definitions, assert data quality, document tables, and so on, all in a single environment and without additional dependencies. Teams can now collaborate following software development best practices including version control, environments, CI/CD, testing, and documentation. The pipeline execution component empowers data analysts to build production-grade SQL pipelines to manage the data they need—without needing data engineers. Dataform core is part of the open-source Dataform data modeling framework that also includes Dataform CLI. You can compile and run Dataform core locally through Dataform CLI outside of Google Cloud to deploy assets to the following data warehouses: BigQuery Snowflake Redshift Azure SQL Data Warehouse Postgres Since Dataform core is open source, it can be extended with custom integration as well. Let’s wrap up this lesson by talking about SQLX. SQLX is an open source extension of SQL and the primary tool used in Dataform. As it is an extension, every SQL file is also a valid SQLX file. SQLX brings additional features to SQL to make development faster, more reliable, and scalable. It includes functions including dependencies management, automated data quality testing, and data documentation.

### Video - [Getting started with Dataform](https://www.cloudskillsboost.google/course_templates/865/video/523381)

* [YouTube: Getting started with Dataform](https://www.youtube.com/watch?v=kPCbm7sqf-8)

In this lesson, we will discuss how to get started with Dataform, and at the end of this module, you will get some hands-on practice via the lab. Let’s start with the code lifecycle in Dataform, and then we will jump into how to get started with Dataform. After extracting raw data from source systems and loading into BigQuery, Dataform helps you transform it into a well-defined, tested, and documented suite of data tables by writing code. Dataform compiles the SQL workflow code in your workspace to SQL in real time, creating a compilation result of the workspace that you can execute in BigQuery. Dataform uses settings you defined in the dataform.json file to create the compilation result. Dataform integrates with multiple Git providers and enables data analysts and data engineers to collaborate on the same repository. To tailor Dataform code lifecycle to your needs, you can configure the compilation result to influence where and how Dataform executes your SQL workflow. Then, you can manually trigger or schedule executions to influence when Dataform executes your whole SQL workflow or its selected elements. Now let’s take a look at how to get set up so that you can start using Dataform. The first step is to set up a repository. A repository contains a single Dataform project that can be connected to your Git provider. Each Dataform repository houses a collection of SQLX and JavaScript files that make up your SQL workflow, as well as Dataform configuration files and packages. You interact with the contents of your repository in a development workspace. A development workspace is your own editable copy of a repository. In a workspace, you create, edit, or delete the contents of the repository without affecting others who work on the same repository, and then commit and push your changes to the repository. In a development workspace, you can develop SQL workflow actions by using Dataform core with SQLX and JavaScript, or exclusively with JavaScript. Dataform preserves the state of files in your development workspace between sessions. You can link a Dataform repository to a remote Git repository hosted by Git providers like GitHub, GitLab, and Bitbucket Cloud. After you link the repositories, the changes you make in a Dataform development workspace can be pushed to and pulled from the remote Git repository. Each repository will have a dataform.json file that configures basic settings required to compile your Dataform project, like the project ID, table prefix, and schema suffix defaults (which you can override). When creating a repository, the region you set for the repository doesn’t impact where the data in BigQuery is stored. You need to grant certain permissions or roles using IAM in order for your code to access BigQuery datasets, which you will get to do in the lab at the end of this module. Once you do all that initial setup, you will spend most of your time developing and executing your pipelines, starting with declarations. You can declare any BigQuery table type as a data source in Dataform. When creating declarations, you can add descriptions and tags, which will be visible in BigQuery. You can declare dependencies to define a dependency relationship between objects of a SQL workflow. In a dependency relationship, the execution of the dependent object depends on the execution of the dependency object. The next part is the fun, familiar part, which is writing your transformations. Tap into all the SQL discussions we have had so far to build your transformations. And finally, assertions. An assertion is a data quality test query that finds rows that violate one or more rules specified in the query. If the query returns any rows, the assertion fails. Dataform runs assertions every time it updates your SQL workflow and it alerts you if any assertions fail. And once you are ready to execute your workflows, you can use the Executions tab in the workspace to monitor your workflow executions. The figure in the slide shows a failed execution. Take note that although the service account, project ID, and detailed error message are blanked out in this screenshot, they are displayed in the product. In this example, the execution failed because the service account didn’t have the appropriate privileges assigned.

### Video - [Demo](https://www.cloudskillsboost.google/course_templates/865/video/523382)

* [YouTube: Demo](https://www.youtube.com/watch?v=3Z7AFFCq94A)

This demo walks you through how to get started with Dataform, create a Dataform repository and initialize a Dataform development workspace. We will get you familiar with the file structure, where you would add new SQLX files, execute your code and view logs for your workflow executions. To get started, go into the Google Cloud console. And then use the navigation menu to go into BigQuery. Once in BigQuery, you can click on Dataform on the left hand side menu. The first thing you will need to do is to enable the Dataform service. So go ahead and click that. Once enabled, you will get redirected to the Dataform UI. The first thing you want to do here is to create a repository. Recall that a repository contains a single Dataform project that can be connected to your Git provider. Give it a name, and then choose an appropriate region, and also note that there is the default Dataform service account already selected. Let’s go ahead and create the repository. At this point, it is probably a good to copy this service account into your clipboard, so later you can add the appropriate IAM roles (like the BigQuery Job User role for instance) so you have the permissions to run the pipelines in BigQuery. Now, it is in this repository that you will now create a development workspace. This workspace, as you will see in just a bit, is where you can develop and run your SQL workflows. Give it a name and click the create button. Once you go into your development workspace, you will see a few tabs like the Code, Compiled Graph, Executions, Start Execution, and we will look at all that in a bit. But first, let’s initialize the workspace to build the necessary building blocks file structure. Notice these starter files that get populated here in the Files pane. The definitions folder is where you can create your SQLX files. Recall the discussion on declarations in the previous lesson. Here is one example added for you. This code creates a view. We also talked about the dataform.json file which has the configuration that configures basic settings required to compile your Dataform project, like the project ID, table prefix, and schema suffix defaults (which you can override). Alright, lets add a new SQLX file by clicking the dropdown button here, and select create file. Add a name for your file and hit the create file button. The file gets added to the file structure. And then you can start writing your code here. For the purpose of this demo, we will simply add a select statement to create a view. And notice on the far right here how the code gets compiled almost immediately. You can see the object type here which is a view. Ok, now lets go ahead and add another file. Give it a name, click the create file button, and once the file is added to the structure, we can add in the code here to create a table. Notice the code here uses the previously created view to populate the rows in this table. Again the compilation happens almost immediately, and for now don’t worry about the error message here. Notice how the table name gets reflected here. Before we proceed further, let’s add those IAM roles to the service account. In the navigation menu, go to IAM and Admin, and then IAM. Under View By Principals, click on Grant Access. This is where we paste the Dataform service account information we copied earlier when we created the Dataform repository. And we essentially want to add 3 roles here. The first one is the bigquery job user. Next, we add the BigQuery Data Editor. And the final one is the BigQuery Data Viewer. Once that is added, we can now go back to the Dataform UI and navigate back to the repository, and then into the development workspace. And now we are ready to run our code. Using the Start Execution dropdown, we select All Executions, and then at the bottom click on the Start Execution button. To look at the executions, we can either go into the Execution tab on top, or simply click on the details link at the bottom here. You will see the four SQL files listed. Two were created initially, and we had created the other 2 as part of this demo. Clicking on the View Details link here shows the overall status of the run, which says success, and you can see all of the boilerplate stuff that Dataform added to successfully run this code. Clicking on the object here redirects you to the object that was created, and in this case it is the table created with the second SQLX file. Notice the quickstart table in your BigQuery dataset, and if you go into the preview tab here, you will see the table records. That’s the end of this simple demo. There is a lot more you can do with Dataform. Feel free to try out the lab and there is more information on our documentation site. [Not for VO] Use this guide to set up your environment: https://cloud.google.com/dataform/docs/quickstart-create-workflow Also here: https://googlestaging.qwiklabs.com//labs/223827 You can show the UI and SQLx files you created, run an execution, show the logs, and show the result in BigQuery. You can also use another quickstart guide to show more capabilities if you want.

### Video - [Lab intro: Create and execute a SQL workflow in Dataform](https://www.cloudskillsboost.google/course_templates/865/video/523383)

* [YouTube: Lab intro: Create and execute a SQL workflow in Dataform](https://www.youtube.com/watch?v=VRgY_xpe4Mo)

In this lab, you will create and execute a SQL workflow in Dataform to load data in BigQuery. You will first create a Dataform repository and initialize a Dataform development workspace. Next, you will create a SQL workflow that loads data into BigQuery. And then execute it. After executing your SQL pipeline, you will view the execution logs in Dataform.

### Lab - [Create and execute a SQL workflow in Dataform](https://www.cloudskillsboost.google/course_templates/865/labs/523384)

This lab walks you through the process to create and execute a SQL workflow in Dataform to load data in BigQuery.

* [ ] [Create and execute a SQL workflow in Dataform](../labs/Create-and-execute-a-SQL-workflow-in-Dataform.md)

## BigQuery Studio

In this module, we will start off by talking about what BigQuery Studio is, and the reason we built it. Next, we describe in a little more detail all the great capabilities that come with BigQuery Studio. In the end, we wrap up the module with a demo to walk you through the cool features and show you how to use it.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/865/video/523385)

* [YouTube: Module overview](https://www.youtube.com/watch?v=Fpkjf3LOJX4)

welcome to module 7 big query studio in this module we will start off by talking about what big query studio is and the reason we built it next we describe in a little more detail all the great capabilities that come with big Corey studio in the end we wrap up the module of the demo to walk you through the cool features and show you how to use it

### Video - [What is BigQuery Studio ?](https://www.cloudskillsboost.google/course_templates/865/video/523386)

* [YouTube: What is BigQuery Studio ?](https://www.youtube.com/watch?v=-TiuEYTn2r4)

let's talk about what big query studio is and the reason we built it Enterprises know that the shift to AI is now based on recent research organizations that are effective at using data and AI are more profitable than their competitors and see improved performance across a variety of business metrics however many organizations are still struggling to extract the full business value of data and while big query is built on a unified open and intelligent data ecosystem data practitioners still face many challenges some of the key issues include one having to constantly contact switch as they still use multiple disjointed tools two while SQL is extremely powerful it can be unintuitive timec consuming and costly to write and maintain on average data engineers and data scientists spend 25 to 50% of their time writing SQL with data analysts spending even greater time writing SQL code three and finally leveraging new technologies like generative AI traditionally requires learning new Concepts in coding on top of that there is always pressure for more security compliance and governance big query studio is an Evol ution of the big query workspace it is a guey that makes it easy to discover explore analyze and predict data in bigquery to help you accomplish these tasks bigquery Studio leverages many other services across the broader data analytics plus AI platform in one experience that gives data practitioners one editing support for SQL and Python and notebook development powered by collab Enterprise two two resource exploration and Discovery powered by datax three centralized Source control and revision history for Co Asset Management powered by data form four assisted data experiences powered by doit AI big query Studio brings an endtoend analytics experience in a single purpose built platform it provides a unified workspace including a SQL and a notebook interface face powered by collab Enterprise which is currently in preview allowing data Engineers data analysts and data scientists to perform endtoend tasks including data ingestion pipeline creation and Predictive Analytics all using the coding language of their choice big query Studio improves collaboration among data practitioners by extending software development best practices such as cicd version history and Source control control to analytics assets including SQL scripts Python scripts notebooks and SQL pipelines additionally users will be able to securely connect with their favorite external code repositories so that their code will never be out of sync in addition to enabling human collaborations big query studio also provides an AI powered collaborator for contextual chat and code assistance duet Ai and big query can understand the context of each user and their data and uses it to autosuggest functions and code blocks for SQL and python through the new chat interface data practitioners can use natural language to get personalized real-time guidance on performing specific tasks reducing the need for trial and error or searching through documentation

### Video - [Unified analytics](https://www.cloudskillsboost.google/course_templates/865/video/523387)

* [YouTube: Unified analytics](https://www.youtube.com/watch?v=4u40vATdmRA)

with big query Studio you now have a single interface for all your data to AI workflows in big query studio with the scalability of big query in the data science notebook let's discuss this in more detail python notebooks are now natively available in big query studio and listed in the resource hierarchy on the left pane along with SQL queries so for example analytics users like data scientists can now use python in a familiar collab notebook environment for data analysis and exploration at a petabyte scale right inside big query big query Studio's notebook environment supports browsing of data sets and schema autocompletion of data sets and columns and quering and transformation of data furthermore the same collab Enterprise notebook can be accessed in vertex AI for ML workflows such as model training and customization deployment and ml Ops now consider this you have an embedded notebook however the data set you are working with is large raising concerns about memory issues performance Etc you want the query power of big query and the expressibility of python and pandas big query Studio allows you access to Big frames that does just that big frames or big query data frames is an ossf python library that translates the most popular python apis like pandas and scikit learn to Big query sqls for scalable execution on top of bigquery's manage storage and Big Lake tables big frames lets python developers discover describe and understand the data available to them in big query by providing a python compatible API that can automatically scale to Big query size data sets developers can then move from data exploration to a production application by deploying their python code as big query programmable objects such as big query remote stored procedures a bvl model or a big query remote function this is all done from the big frames API and is Unified with bigquery's user permission model making python developers immediately and continually effective when working with the bigquery data platform it can also be extended by Partners so our partner ecosystem can customize bake query for their applications spark jobs on bake query serve the purpose of integrating spark with big query however for customers who prioritize open-source Solutions and don't require big query they can continue to use data proc let's talk about new usability features built right into the interface firstly the interface now makes it easier to retrieve recently accessed resources like notebooks next when it comes to query results you can now sort the results of a query right from the interface then there is an estimate for the sort operation as well and for those of you who prefer a more visual output you can now generate many different types of charts based on your query results right from the interface

### Video - [Asset management](https://www.cloudskillsboost.google/course_templates/865/video/523388)

* [YouTube: Asset management](https://www.youtube.com/watch?v=9sVdFD9Bckg)

in this lesson we will talk about bit query integration with datax and data form bringing asset and code management capabilities to our users for Enterprises that have data that's distributed across data Lakes data warehouses and data Marts using dataplex can help discover data curate data unify data without any data movement organize data based on your business needs and centrally manage monitor and govern data dataplex lets you standardize and unify metadata security policies governance classification and data life cycle management across this distributed data using big query integration with dataplex and its metadata management features you are now able to build out a glance views of data that help you understand the quality of data this integration allows for resource exploration and Discovery through metadata views like lineage quality and profiling so let's take a look at each of these using data profiling you can abild deeper understanding of your data and track data drifts with detailed statistical profiling information right within the big query UI use the data quality tab to build trust and data at scale with built-in rule recommendation serverless execution and alerting view quality metrics for table right in the big query UI data lineage allows tracking of endtoend lineage for big query big lake and big query Omni to understand movement of data Trace Downstream impact Upstream causes for data issues and compliance to policies big query studio is designed to build trust right from the data source guiding users through data understanding issue identification and problem solving data practice practioners benefit from a host of functionalities aimed at elevating data quality so in summary with data lineage you can track the origin and transformations of your data to understand its Evolution with data profiling you can assess data to ensure its quality and integrity and you can use quality constraints to enforce data quality constraints for more reliable analytics using big query integration with d data form and its code management capabilities you can share version and Source control your code within your teams recall that we had discussed data form and Detail in an earlier module

### Video - [Embedded assistance](https://www.cloudskillsboost.google/course_templates/865/video/523389)

* [YouTube: Embedded assistance](https://www.youtube.com/watch?v=5A4BkPzL0E4)

big query studio also provides an AI powered collaborator for contextual chat and code assistance du AI in big query can understand the context of each user and data and uses it to autosuggest functions and code blocks for SQL and python through the new chat interface data practitioners can use natural language to get personalized real-time guidance on performing specific tasks reducing the need for trial and error or searching through documentation so let's take a look duet AI is an umbrella brand used to describe Google's own use of generative AI to build expert assistance into its Cloud products including workspace and Google Cloud platform built on top of Google's leading large Foundation models duet AI is specially trained to help you be more productive on Google Cloud du Ai and Google Cloud leverages the vertex AI platform to provide personal and contextualized AI assistance while keeping your data private and secure duet Ai and Google Cloud provides a new AI powered collaborator to help your developers be more productive and your Ops Team more capable of optimizing infrastructure it also makes the overall Cloud experience intuitive duet Ai and Google cloud is an always on collaborator that offers generative AI powered assistance to a wide range of Google Cloud users including developers data scientists and operators to provide an integrated assistance experience du AI is embedded in many Google Cloud products for the purposes of this course we will only discuss du it Ai and big query but feel free to explore other du AI courses and Labs on our catalog duet Ai and big query can help you work with SQL for instance if you work with SQL queries that other people wrote duet AI in big query can explain a complex query in plain language such explanations can help you understand the query syntax underlying schema and business context for example you might want to understand how data tables and queries are related in a sales data set simply paste the query highight light it and then click on the explain button you will then see an explanation of what the query does in the panel on the right the explanation is great if you have an SQL query to start with but what if you get stuck and need some help with writing the query well the SQL generation feature can help you here you can provide a prompt to generate in SQL query based on your data schema even if you're starting with no code a limited knowledge of the data schema or only a basic knowledge of SQL syntax duet AI in big query can suggest one or more SQL statements simply start off with a pound sign in the query box and type out in natural language what you intend to do then hit enter note that the comment is a single line but wraps around in the display duet Ai and big query will then autogenerate a query for you simply hit the tab button to accept the query and run it or feel free to make changes to the query before you run it doet AI in big query can also help with SQL completion by giving you suggestions as you type your query for example you want to use a certain function but don't remember the exact syntax du AI in big query will prompt you suggestions as you type your query which you can accept or modify we spent the module talking about bigquery Studio let's do a quick recap firstly it provides a unified workspace including both a SQL and a notebook interface allowing data Engineers data analysts and data scientists to perform endtoend tasks including data ingestion pipeline creation and Predictive Analytics all using the coding language of their choice next it allows easy resource exploration and discovery powered by datax with data form included you now have a centralized Source control and revision history for code asset management and finally bit query Studio gives you a cist of data experiences powered by Duit AI

### Video - [Demo](https://www.cloudskillsboost.google/course_templates/865/video/523390)

* [YouTube: Demo](https://www.youtube.com/watch?v=-fq5dWbqy8U)

in this demo we walk you through the new Big query Studio UI and get you familiar with the capabilities we discussed in this module in the cloud console we go to big query and then click on big query Studio to launch it when the UI launches you will see the button to create SQL queries later when we enable required apis you will see another button here to create python notebooks but let's start with du AI in the cloud console the new AI powered collaborator first things first we enable the cloud AI companion API once enabled you can then ask questions of duate AI to get assistance it can be a very general prompt like how do I get started with big query and in the response that is generated it tells you to start with a Google Cloud project which we already have then the API which again is done and next step is the data set creation this is useful information so let's do exactly that right next to the project we follow the steps to create a data set enter a data set ID for it leave everything else default and then click create data set once created back in the dit AI pane we can then ask a more specific question like how do I learn which data sets and tables are available to me in bigquery and the response goes you guidance on how to go about doing that via the console or API as well as using the information schema and it also provides you with the SQL to query the views all right let's close the duet AI pane for now and we will come back to it in just a little bit apart from the duet AI in Cloud console you just saw there is also duet AI in big query which can assist you with more specific big query related tasks so click on the button here and follow the steps to join The Trusted tester program enable the required API and Grant required permissions once all that goes through you click on the duet Ai and big query button again and you see a list of features you can get help on like SQL completion SQL generation and SQL explanation we will look at each of these in action let's start with SQL explanation say you are collaborating with a team of analysts and chance upon one of their rather complex SQL statements you can paste it in the query box highlight the query rightclick and select the explain current selection option the do it AI pane will open on the right and provide you with an explanation of the SQL statement at the top you see an overall explanation of what the query does with a more detailed breakdown below it tells you the data sources being queried and all the other aspects of the query like the join condition how the results are grouped how the results are sorted and the limit Clause followed by a summary too all right that was useful when you have the SQL query but what if you need help in writing one duet AI can help you there as well click here to open the SQL generation widget and write what you intend to query in natural language it may help to be specific with details like tables and aggregations once ready click on the generate button and in the response you see a query written for you based on the prompt you provided once you review it click on the insert button to copy it in the query box you will also see the prompt you provided at the top take note that sometimes you may need to tweak the query a little in this example you will notice that the column name needs some updating so we can easily open up the table verify the actual column name is in fact ID and not product ID so let's close this and make that simple column name update and the query here now let's go ahead and run this query once the results populate we will try to create a chart using these results which is one of the things we talked about in an earlier lesson the line graph does look a little busy so we can always change this to a bar chart feel free to play around with this and change the dimensions and other settings to get the best visual that meet your requirements let's go back to the results and show yet another cool new usability feature we talked about in this module which is the Sorting option right from the UI and once that completes you will see the results sorted on the column you'd selected now let's see how SQL completion Works in action say a start typing a query since SQL completion was enabled duet AI in big query will try to help me complete my query as I type it out like in this example it props me one possible option and if I am happy with it even if you think it is a great starting point and can make changes later simply hit Tab and the suggestion gets copied again tweak it to suit your needs and then run the query all right now let's close the SQL query editor and show you how to work with notebooks in the cloud console we navigate to collab Enterprise under vertex Ai and start by enabling the required apis once enabled let's go ahead and create a new notebook upon creation notice the notebook name now let's navigate back to bigquery studio and no notice that we can now create python notebooks from within the UI because the apis are enabled secondly when we expand the project and look under notebooks we can easily access the notebook just created under vertex Ai and we can create python notebooks right from the big query Studio UI as well which when shared can also be accessed via vertex AI hope you find this demo useful to get started with the new Fe features introduced via big query Studio

### Video - [Lab intro: Analyze Data with Duet AI Assistance](https://www.cloudskillsboost.google/course_templates/865/video/523391)

* [YouTube: Lab intro: Analyze Data with Duet AI Assistance](https://www.youtube.com/watch?v=4bYJD5XKKGQ)

In this lab, you are a data analyst who will use Duet AI and BigQuery to analyze data and predict product sales. More specifically, you will use Duet AI to generate new SQL queries, complete queries, and explain complex queries. The data used in the lab is based on the BigQuery public datasets that contains synthetic ecommerce and digital marketing data.

### Lab - [Analyze data with Gemini assistance](https://www.cloudskillsboost.google/course_templates/865/labs/523392)

In this lab, you'll analyze BigQuery data with assistance from Gemini.

* [ ] [Analyze data with Gemini assistance](../labs/Analyze-data-with-Gemini-assistance.md)

### Lab - [Generate Personalized Email Content with BigQuery Continuous Queries and Gemini](https://www.cloudskillsboost.google/course_templates/865/labs/523393)

In this lab, you learn how to generate personalized email content with BigQuery continuous queries, Gemini, Pub/Sub, and Application Integration triggers.

* [ ] [Generate Personalized Email Content with BigQuery Continuous Queries and Gemini](../labs/Generate-Personalized-Email-Content-with-BigQuery-Continuous-Queries-and-Gemini.md)

## Summary

This module recaps the key topics covered in the course.

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/865/video/523394)

* [YouTube: Summary](https://www.youtube.com/watch?v=paKEtT-LhXY)

we have reached the end of the course we hope that you have enjoyed learning about big query through the combination of videos handon labs and demos let's do a quick recap of what was covered in the modules we started off by looking at analytics challenges faced by data analysts and compared big data on premises versus in the cloud we then introduced Big query which is Google Cloud's Enterprise data warehouse and reviewed its features that make big query a great option for your data analytics needs we also looked at some real world use cases of companies transformed through analytics in the cloud next we looked at a lot of SQL queries that we used on the big query public data sets from simple select statements to complex ones that use functions joins Etc we also shared common pitfalls and the best practices around them the labs from this module provide ided good Hands-On practice and context to the topics covered we continued the discussion on SQL and looked at cleaning preparing and transforming data using SQL but we also looked at other products like data prep cloud data Fusion data flow data proc and data form that can help with data preparation and transformation there are other courses that cover some of these products in more detail if you are interested to learn more in module 4 we discussed how the amount and type of Transformations needed can help determine when to use extract and load versus extract load and transform versus extract transform and load approaches for loading data into big query we also covered external data sources where you can run queries in big query for data that is hosted outside of big query and the Hands-On Labs LED you practice creating permanent tables and ingesting data into big query next we reviewed some data visualization principles through some interesting examples and then looked at various visualization options like looker studio and connected sheets and even looked at running data analysis from python notebooks again the labs in this module offer really good Hands-On practice with the visualizations tools covered next we introduced data form a new product that offers a unified endtoend experience to develop Version Control and orchestrate SQL pipelines in big query the demo and Hands-On lab walked you through the steps on how to get started with data form and building an executing a SQL Pipeline and finally we talked about big query Studio big query studio is an evolution of the big query workspace it is a goey that makes it easy to discover explore analyze and predict data in big query to help you accomplish these tasks big query Studio leverages many other services across the broader data analytics plus AI platform in one experience that gives data practitioners one editing support for SQL and Python and notebook development powered by collab Enterprise two resource exploration and Discovery powered by datax three centralized Source control and revision history for code Asset Management powered by data form four assisted data experiences powered by du AI thank you for completing the course there is a lot more information with examples on our documentation site so please do check it out at https Callin cloud.google.com bigquery slocs

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
