---
id: 985
name: 'Responsible AI for Developers: Fairness & Bias'
datePublished: 2024-12-03
topics:
- Data Governance
- Machine Learning
- Data Modeling
type: Course
url: https://www.cloudskillsboost.google/course_templates/985
---

# [Responsible AI for Developers: Fairness & Bias](https://www.cloudskillsboost.google/course_templates/985)

**Description:**

This course introduces concepts of responsible AI and AI principles. It covers techniques to practically identify fairness and bias and mitigate bias in AI/ML practices. It explores practical methods and tools to implement Responsible AI best practices using Google Cloud products and open source tools. 

**Objectives:**

- Define what is Responsible AI
- Identify Google’s AI principles
- Describe what AI fairness and bias mean
- Explain how to identify and mitigate biases through data and modeling

## Course Introduction

This module introduces the course structure and objectives.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/985/video/515613)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=Wj1b8hOe8Uo)

Hello, welcome to the course, Responsible AI for Developers, Fairness & Bias. This course introduces important topics in responsible AI and Google AI principles. It explores practical methods and tools to implement responsible AI best practices through the use of Google Cloud products and open source tools. In this course, you will learn to define what is responsible AI, identify Google's AI principles, describe what AI fairness and bias mean, and explain how to identify and mitigate biases through data and modeling. Let's begin the course.

## Introduction to Responsible AI

This module provides an overview of Responsible AI, covering Google's AI Principles and sub-topics of Responsible AI. Also, this provides actual case studies of Responsible AI in Google products.

### Video - [AI & Responsibility](https://www.cloudskillsboost.google/course_templates/985/video/515614)

- [YouTube: AI & Responsibility](https://www.youtube.com/watch?v=pkROQ-7HnII)

Welcome to Introduction to Responsible AI. This module will consist of four lessons. You will learn how to describe the importance of responsible AI. List Google's AI principles, describe the best practices for responsible AI development. Identify how responsible AI principles can be applied during product development. Let's start with AI and responsibility. Artificial intelligence, or AI, is everywhere, you may have even heard the phrase generative AI. Most of us already have daily interactions with artificial intelligence. From predictions for traffic and weather to recommendations for TV shows, AI is becoming more common. And new AI systems are continuously being developed at an extraordinary pace. Still, AI is not infallible. No AI model has 100% accuracy, and AI systems are complex. When AI systems are used in real world contexts, they can fail to behave in expected ways, which reduces their realized benefit. Failures in behavior can come from model misuse, like the case of fraudsters who use AI to mimic a CEO's voice. This is because AI models are often under specified. They perform well in the situation in which they are trained, but might not be robust or fair in new situations. There is a common misconception with artificial intelligence that machines play the central decision making role. In reality, it's people who make decisions. People are involved in each aspect of AI development. They collect or create the data that the model is trained on, they design and build these models. They control the deployment of the AI and how it's applied in a given context. Essentially, human decisions are threaded throughout our technology products. And every time a person makes a decision, they are actually making a choice-based on their own values. Whether it's the decision to use generative AI to solve a problem as opposed to other methods. Or anywhere throughout the machine learning lifecycle, that person introduces their own set of values. This means that every decision point requires consideration and evaluation. To ensure that choices have been made responsibly from concept through deployment and maintenance. Because there's potential to affect many areas of society, it's important to develop AI technologies with ethics in mind. When we're talking about new AI technologies and ethics, there's a lot of work in progress on laws, policies, regulations, etc. Unfortunately, there is not a wide global consensus on what these ethics are yet. What we have learned is we can't wait for society to catch up and codify the rules. We have a voice in shaping what those norms will be. It should be noted that ethics is different from laws and policies. Law draws insights from ethics, and ethics inform policy but most ethical norms are not codified. So we need to define what is ethical behavior to put it simply, ethics can be explained in these three ways. Ethics is what we ought to do, not the same as what is actually done or what most people say or think should be done. Ethics is what others can rightly blame us for not doing, even if we suffer no actual punishment. Ethics is what sustains our flourishing together in human society, ethics is an evolved tool for living well as social creatures. Responsible AI doesn't mean to focus only on the obviously controversial use cases though. Without responsible AI practices, even seemingly innocuous or good intent, AI use cases could still cause ethical issues or unintended outcomes. They might not even be as beneficial as they could be. Ethics and responsibility are important, not least because they represent the right thing to do. But also because they can guide AI design to be more beneficial for people's lives. So what is responsible AI? What we can say is that responsible AI requires an understanding of the possible issues, limitations, or unintended consequences. This understanding is aimed at developing AI ethically. There is not a universal definition of responsible AI, nor is there a simple checklist or formula that defines how responsible AI practices should be implemented. Instead, individuals and organizations are developing their own AI foundational principles that reflect their mission and values. While these principles are unique to every organization, if you look for common themes. There is a consistent set of ideas across fairness, interpretability, privacy, and safety. What we found at Google is that following these foundational responsible AI principles leads to developing successful AI. We learned that accountability ensures that your products are beneficial to everyone. Evaluating your AI systems, both when they perform as intended and when they don't, is crucial to building accountable products. Building responsibility into any AI deployment makes better models and builds trust with your customers and your customers' customers. If at any point that trust is broken, you run the risk of AI deployments being stalled or unsuccessful. At worst, it is harmful to the stakeholders that those products affect. Lack of trust in AI systems is a growing barrier to adoption in enterprise. With more organizations selecting enterprise products based on AI commitments and practices, ethical development drives innovation. Empowering AI decision makers and developers to consider ethical considerations enables them to find new, innovative ways to drive your mission forward.

### Video - [Google’s AI Principles](https://www.cloudskillsboost.google/course_templates/985/video/515615)

- [YouTube: Google’s AI Principles](https://www.youtube.com/watch?v=j6aUgLviiqw)

In 2018, Google announced its AI principles. These principles are concrete standards that actively govern our research and product development and affect our business decisions. We incorporated responsibility by design into our products and even more importantly, our organization. Google has been constantly updating them since these principles were announced. Our approach to responsible AI is rooted in a commitment to strive toward AI that is built for everyone, accountable and safe, respects privacy, and driven by scientific excellence. We've developed our own AI principles, practices, governance processes, and tools that together embody our values and guide our approach to responsible AI. Google's AI principles describe our commitment to developing technology responsibly, and work to establish specific application areas we will not pursue. Let's explore each of Google's 7 principles for AI applications and 4 areas you should not pursue. Let's start with the 7 principles for AI applications. Google's first principle states that AI should be socially beneficial. For any project, we should consider a broad range of social and economic factors and proceed only where we believe that the overall likely benefits substantially exceed the foreseeable risks and downsides. Make high quality and accurate information readily available using AI, while continuing to respect cultural, social, and legal norms in the countries where we operate. Evaluate when to make our technologies available on a non-commercial basis. Some examples of the first principal and socially beneficial AI applications could include AI/NL models designed to predict future development of skin cancer in patients. And recommendation engine to suggest online skills training for retail employees, or a drone guidance system for emergency aid airdrops to disaster sites. However, no AI application, no matter how well intentioned, is inherently absolutely beneficial. Its impact depends entirely on how responsibly we design and deploy it. Google's second principle states, AI should avoid creating or reinforcing unfair bias and unjust effects on people, particularly those related to sensitive characteristics such as race, ethnicity, gender, ability, and political or religious belief. Distinguishing fair from unfair biases is not always simple and differs across cultures and societies. Here are a few examples of the second principle, where it's important to avoid applications that create or reinforce unfair bias. They include tech that makes or assists in criminal justice decisions a hiring algorithm that ranks candidate application relevance for recruiters. A machine learning-driven AI designed to flag abusive, offensive, or hate speech. Google's third principle states, AI should be built and tested for safety. For any project, we should avoid unintended results that create risks of harm. Be appropriately cautious, some use case examples where the third principle is especially important include an ML model that explores new strategies and efficiencies in the city power grid. An AI agent that routes calls in an emergency dispatch system. A new ML model that predicts jet engine failure. Google's fourth principle states AI should be accountable to people. For any project, we should provide appropriate opportunities for feedback, relevant explanations and appeal. Introduce appropriate human direction and control. There have been examples of the fourth principal applications, which have failed to be accountable to people. These include a recommendation system that makes fully automated decisions without consent, explanation, and right of appeal, such as credit and insurance decisions. An AI bot that convincingly imitates a human agent. A biometric ID system that is introduced without a user's notice, consent, and ability to opt out. Google's fifth principle states AI should incorporate privacy design principles. For any project, we should give opportunities for notice and consent, encourage architectures that have privacy safeguards provide appropriate transparency and control over the use of data. Here are some examples of fifth principal applications where incorporating privacy design is especially important. A smart refrigerator that learns user purchasing habits, a geolocation app that predicts local foot traffic patterns, a therapy app that processes records of psychological issues. Google's sixth principle, AI should uphold high standards of scientific excellence. For any project, we should work with a range of stakeholders to promote thoughtful leadership in this area, drawing on scientifically rigorous and multidisciplinary approaches. Responsibly share AI knowledge by publishing educational materials, best practices, and research that enable more people to develop useful AI applications. Here are some examples of six principal applications that demonstrate the importance to uphold high standards of scientific excellence. An AI/ML app for emotion detection, an AI/ML app that detects signs of clinical depression, an AI/ML tool that advances deepfake detection. Google's seventh and last principle states, AI should be made available for uses that accord with these principles. Many technologies have multiple uses, so we will work to limit potentially harmful or abusive applications. In addition to these 7 principles, Google also has a commitment as part of its AI principles, to not design or deploy AI in 4 application areas. The first area is technologies that cause or are likely to cause overall harm. Where there is a material risk of harm, Google will proceed only where they believe that the benefits substantially outweigh the risks and will incorporate appropriate safety constraints. The second area, which Google will not pursue is weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people. The third area, which Google will not pursue is technologies that gather or use information for surveillance that violates internationally accepted norms. The fourth and last area, which Google will not pursue is technologies where the purpose contravenes widely accepted principles of international law and human rights.

### Video - [Responsible AI Practices](https://www.cloudskillsboost.google/course_templates/985/video/515616)

- [YouTube: Responsible AI Practices](https://www.youtube.com/watch?v=Tkyfl9GLyDk)

Now let's look at recommended practices for responsible AI. AI is software, and so general best practices for software systems should always be followed when designing AI systems. There are also various considerations unique to machine learning for us to discover. Here are six recommended practices for developing AI with responsible AI principles in mind. Use a human centered design approach and to defy multiple metrics to assess training and monitoring. When possible, examine raw data directly, have awareness of the limitations of your dataset and model, test the AI system to ensure it's working as intended. Monitor and update the system continuously after deployment. Let's discuss the first recommended practice. Use a human centered design approach. The way that actual users experience your system is essential to assessing the true impact of its predictions, recommendations, and decisions. To use a human centered design approach, you should design features with appropriate disclosures built in, clarity and control are crucial to a good user experience. Consider augmentation and assistance. In some cases, it might be optimal for your system to suggest a few options rather than one to the user. Technically, it's much more difficult to achieve good precision at one answer versus precision at a few answers. Model potential adverse feedback early in the design process, followed by specific live testing and iteration for a small fraction of traffic before full deployment. Engage with a diverse set of users and use case scenarios and incorporate feedback before and throughout project development. This builds a rich variety of user perspectives into the project and increases the number of people who benefit from the technology. The next practice is to identify multiple metrics to assess, training and monitoring. The use of several metrics instead of a single one will help you to understand trade offs between different errors and experiences. This means you should define metrics including feedback from user surveys, quantities that track overall system performance and short and long term product health. For example, click through rate and customer lifetime value respectively, and performance sliced across different subgroups. Ensure that your metrics are appropriate for the context and goals of your system. For example, a fire alarm system should have high recall, even if that means the occasional false alarm. Next, the practice of directly examining your raw data. Machine learning models will reflect the data they are trained on so analyze your raw data carefully to ensure you understand it. In cases where this is not possible, like with sensitive raw data, understand your input data as much as possible while respecting privacy. For example, by computing aggregate anonymized summaries, this means that data should be accurate. Ask yourself, does my data contain any mistakes, for example, missing values, incorrect labels, data, and data samples to be representative? Ask yourself, is my data sampled in a way that represents my users and the real world setting? Training serving skews shouldn't happen. The difference between performance during training and performance during serving is a persistent challenge. During training, you may need to adjust your training data or objective function. During evaluation, continue to ensure that evaluation data is as representative as possible of the deployed setting. Data and model should be simple. Ask yourself, are any features in my model redundant or unnecessary? Is my model unnecessarily complex? For supervised systems, consider the relationship between the data labels you have and the items you're trying to predict. Bias should be minimized in your data. First, you want to ensure that your data is fairly representative of the entire population. When you develop AI, you should have awareness of the limitations of your dataset and model. This means you should not mistake correlation for causation. For example, your model might learn that people who buy basketball shoes are taller on average. But this does not mean that a user who buys basketball shoes will become taller as a result. Communicate the scope of the training set. For example, a shoe detector trained with stock photos can work best with stock photos, but has limited capability when tested with user generated mobile device photos. Communicate limitations to users where possible. For example, an app that uses machine learning to recognize specific bird species might communicate that the model was trained on a small set of images from a specific region of the world. By better educating the user, you might also improve the feedback provided from users about your feature or application. Learn from software engineering, best test practices and quality engineering to ensure that the AI system is working as intended and could be trusted. This means you should conduct rigorous unit tests to test each component of the system in isolation. Conduct integration tests to understand how individual NL components interact with other parts of the overall system. Proactively detect input drift by testing that data distributions are not changing in unexpected ways. Use a gold standard dataset to test the system and ensure that it continues to behave as expected by updating it regularly. Conduct iterative user testing to incorporate a diverse set of users' needs in the development cycles. Apply the quality engineering principle of poka-yoke. The principle pushes you to build quality checks into a system so that unintended failures either cannot happen or they trigger an immediate response. For example, if an important feature is unexpectedly missing, the AI system won't output a prediction. Lastly, you should monitor and update the system continuously after deployment. Continued monitoring ensures that your model takes real world performance and user feedback like happiness tracking surveys and the heart framework into account. This means that you should be ready for issues to occur. Any model of the world is imperfect, almost by definition, build time into your product roadmap to let you address issues. Consider both short and long term solutions to issues. Balance short term simple fixes with longer term learned solutions. Analyze the candidate model before deployment, specifically how it differs from the deployed model and how the update will affect the overall system quality and user experience.

### Video - [Case study: Google Flights](https://www.cloudskillsboost.google/course_templates/985/video/515617)

- [YouTube: Case study: Google Flights](https://www.youtube.com/watch?v=962QYr8GO74)

Now, let's look at a case study on Google Flights. Imagine the scene. You were busy at work and suddenly realized you forgot to book the flight for your upcoming trip. Purchasing flight tickets can be difficult. Identifying patterns in flight prices is challenging due to changes in flight prices, inconsistent pricing across sites, and sometimes even pricing breakdowns are hard to understand. Google Flights believes that by putting some of the data and smart AI in the hands of users, it would help them demystify what they need to pay for a certain flight at a certain time. This would potentially save users time, stress, and money. One of the challenges with predictions and a common theme across AI-driven products, is that machine learning predictions can't be 100% right all the time for two reasons. First, the predictions are specific to certain flights to certain places at specific times. And second, we don't have enough pricing data to provide an accurate prediction of whether the price is fair or not. As they thought through how to build a tool, they felt it was important to help users make informed and better decisions by explaining where this data was coming from and what it relates to. Therefore, users were allowed to; assess price goodness today and in the future, track the model's predictions and check them, make confident decisions about when to book while at the same time ensuring that users understand where our data is coming from, view the general trends in flight pricing, have reasonable expectations for the correctness of our predictions. Google Flights started designing a new tool to help users understand whether the prices for a given flight are currently high, low, or typical, and to help users learn market trends for similar trips. At one point, they considered a pivot to a more direct approach, hiding the complicated calculations in the background and giving users a simple conclusion such as, today is a good day to book. But during testing, users expressed that this felt salesy, was upsetting and didn't feel trustworthy. That was not an option because they wanted a tool that only worked if people trusted it. To start setting some guide rails for further iterations, the team created three design principles for price intelligence. Any Google flight price insights surfaced to users would have to be; honest, actionable, and concise, yet explorable. How do you explain the machine learning model output to users in a way that is actionable and compelling, but also accurate? Through the following ways. A price goodness indicator with corresponding descriptions of high, typical, or low. A single-line explanation of the usual price for a trip like the one the user is planning. Prediction text stating whether prices are likely to go up or down. An info icon that opens an explanation bubble with text explaining which data sources were used to compute the insight. At first, a tool showed the likelihood that a price would go up or down in a very specific way. For example, a medium confidence prediction could say, "Prices are unlikely to drop and there's a 75% chance they'll increase by $17 in the next five days." However, this was too much information for a user to process in decision-making. Because medium confidence predictions were confusing and not actionable, the decision was made to use a confidence rating of 90% or higher. Wording was also changed to say likely to go up or not likely to go down. As a result, three strategies were most helpful in the design of price insights in flights. Articulate data sources. Telling the user what data was being used in the AI's prediction helped the product team avoid contextual surprises and privacy suspicion and helped the user know when to apply their own judgment. Experiment with different confidence indicators. Showing model confidence in categorical buckets and visual graphs helped give users relevant information about flight prices in a way that was easy for them to understand, and account for unexpected user behaviors. Conducting user research early and frequently helped anticipate any unintended consequences of detailed explanations. This helped the product team change its communications approach and therefore bolster user trust. In summary, building machine learning products can be challenging. As you build, applying responsible AI principles throughout your project can profoundly influence a user's trust in both your system and the system's machine learning's usefulness in decision-making. We all have a role in how responsible AI is applied. As you saw in this case study, whatever stage in the AI process you are involved with, from design to deployment or application, your decisions have an impact. Remember, it's important that you too have a defined and repeatable process for using AI responsibly.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/985/quizzes/515618)

#### Quiz 1.

> [!important]
> **You have been asked to do a presentation to a stakeholder in your organization on the importance of responsible AI. As you are thinking through your key opening statement, what is the most appropriate statement you can make that describes the importance of having responsible AI? 
**
>
> - [ ] Responsible AI is important for our organization because it ensures that our AI is fail-proof.
> - [ ] Responsible AI is important for our organization because it reduces overall harm that our products may inflict on underrepresented groups.
> - [ ] Responsible AI is important for our organization because it ensures that our AI models  have no limitations.
> - [ ] Responsible AI is important for our organization because it encourages surveillance of others and ensuring that we can be aware of what our competition is doing.

#### Quiz 2.

> [!important]
> **Which statement accurately describes a Google AI principle?**
>
> - [ ] AI should create unfair bias that cannot be easily identified or mitigated.
> - [ ] AI should uphold high standards of operational excellence.
> - [ ] Be accountable to people.
> - [ ] AI should gather or use information for surveillance to ensure the safety of people.

#### Quiz 3.

> [!important]
> **As you think through the development of your AI product, your organization is looking to you to lead and guide the development team around responsible AI best practices. What is a key best practice the team should think through as they develop the AI product?**
>
> - [ ] The team should plan to develop Version 2 of the product once version 1 has been deployed to ensure that they don't lose any time between development cycles.
> - [ ] The team should use an AI centered design approach to ensure that development is rooted in an AI foundation.
> - [ ] The team should limit testing of their AI system to avoid introducing hallucinations into the model.
> - [ ] When possible, the team should hold the capability to access raw data to investigate issues or unintended behaviors.

#### Quiz 4.

> [!important]
> **The data scientist team in your company is working to implement an AI model to assist processing credit card applications. During testing, you notice that the model has inconsistent performance among different sub-group of applicants. What responsible AI best practice should the team apply to the model?**
>
> - [ ] Include the same number of data points from each country or region in the training data.
> - [ ] Designate a vendor team to verify and examine the data so the Product team can focus on model developing.
> - [ ] Ensure the recommendation data the product is pulling from is fairly representative of the entire dataset.
> - [ ] Ensure the recommendation data the product is pulling from gives extra weight on minority groups.

## AI Fairness & Bias

This module focuses on AI Fairness and Bias. It provides various techniques and tools to identify and mitigate biases through data and modeling.

### Video - [Overview of Fairness and Bias](https://www.cloudskillsboost.google/course_templates/985/video/515619)

- [YouTube: Overview of Fairness and Bias](https://www.youtube.com/watch?v=vegnItMS5fk)

Welcome to AI fairness & bias. This module consists of eight lessons. Today, you will learn to, define types of unfair bias in AI, discuss why AI fairness is important for machine learning and difficult to achieve. Identify possible causes of biases in AI systems, recognize AI fairness best practices, and explore more tools and techniques to mitigate bias in data sets and models. Let's start with an overview of fairness and bias. Fairness relates to the second of Google's AI principles. Avoid creating or reinforcing unfair bias, although it's worth noting that the principles often intersect. Let's first define what is bias. Bias is stereotyping, showing prejudice or favoritism towards some things, people or groups over others. It is easy to say, but usually challenging to realize the biases we have because they are hidden inside our own perspectives. Let's do a quick experimentation. Picture a shoe. What kind of shoe did you picture? Here are some examples of a shoe. Which one is closest to what you visualized? Something like a sneaker on the left or a high heel on the right. Although not all biases are harmful, it is important to note that we naturally obtain some tendencies in perceptions, thought processes, and common senses, depending on a group you belong to or information you get. What if you are asked to visualize a scientist? Did you envision a particular gender group or racial group? There are over 100 different types of human biases listed in Wikipedia's catalog of cognitive biases. When developing products for AI systems, it's important to be aware of common human biases that can manifest in your data and in your model. This way, you can take proactive steps to mitigate the effects of biases. Let's look at the five most common biases. Reporting bias occurs when the frequency of events, properties, and or outcomes captured in a data set does not accurately reflect their real-world frequency. This bias can arise because people tend to focus on documenting circumstances that are unusual or especially memorable, assuming that the ordinary is generally accepted. Automation bias is a tendency to favor results generated by automated systems over those generated by non-automated systems, irrespective of the error rates of each. Selection bias occurs if a data set's examples are chosen in a way that doesn't reflect their real-world distribution. Selection bias can take many different forms, such as coverage bias. This occurs when data is not selected in a representative fashion. Non-response bias, or participation bias, this happens when data ends up being unrepresentative due to participation gaps in the data collection process. And sampling bias, which occurs when proper randomization is not used during data collection. Group attribution bias is a tendency to generalize what is true of individuals to an entire group to which they belong. Two key manifestations of this bias are in-group bias and out-group homogeneity bias. In-group bias occurs when there is a preference for members of a group to which you also belong, or for characteristics that you also share. Out-group homogeneity bias occurs when there is a tendency to stereotype individual members of a group to which you do not belong, or to see their characteristics as more uniform. An implicit bias occurs when assumptions are made based on one's own mental models and personal experiences that do not necessarily apply. More generally, a common form of implicit bias is confirmation bias, where model builders unconsciously process data in ways that affirm pre-existing beliefs and hypotheses. In some cases, a model builder might actually keep training a model until it produces a result that aligns with their original hypothesis, this is called experimenters bias. So, how about AI systems? As you know, they usually have multiple steps, including data collection and labeling, training, evaluation, and deployment. Throughout the lifecycle, bias can enter into the system as a systematic error introduced by sampling or reporting procedures. If the data collection has wrong assumptions or implementation, the data set itself ends up having a lot of biases, such as selection bias. Engineers can inadvertently train models by feeding the bias data set, and machine learning models will easily find and repeat the bias patterns and predictions. When building models, bias can also enter in the way model components and logic are defined. Bias can even appear after deployment from feedback loops and model iterations. You can rarely identify a single cause of or a single solution to these bias problems. What happens far more often is the existence of various causes that interact in ML systems to produce problematic outcomes. This results in a range of solutions needed. It's important to note that not all types of bias are necessarily unfair. The definition of fairness is slightly more restrictive. Decisions made by computers after a machine-learning process might be considered unfair if they were based on variables considered sensitive. Examples of such variables include gender, ethnicity, sexual orientation, or disability. With these variables, fairness also encompasses equity and inclusion. It is important to work towards AI systems that are fair and inclusive for all, as the impact of AI increases across sectors and societies. Beyond recommending books and television shows, AI systems can be used for more critical tasks, such as matching people to jobs and partners, identifying fraudulent transactions, or identifying a person who is crossing a street on a red light. So what is the opportunity that exists? AI systems have the potential to be fairer and more inclusive at a broader scale than decision making processes based on ad-hoc rules or human judgments, and the risk that exists. Unfairness in such systems can also have a negative, widescale impact. In all this, AI fairness is actually a difficult task to achieve. First of all, AI models learn from existing data, and an accurate model might learn or even amplify problematic preexisting biases. Second, even with the most rigorous and cross-functional training and testing, it is a challenge to build systems that will be fair across all situations. Third, there is no standard definition of fairness, whether decisions are made by humans or machines. Identifying appropriate fairness criteria for a system requires accounting for considerations such as user experience, cultural, social, historical, political, legal, and ethical. Several of which may have trade-offs, even for situations that seem simple. People may disagree about what is fair, and it may be unclear what point of view should dictate policy, especially in a global setting. Finally, fairness metrics can be incompatible and impossible to satisfy simultaneously. Hence, it's impossible to define a universal metric for fairness. Don't be discouraged, though, it just means that fairness needs to be defined contextually for the given AI problem.

### Video - [Identify Bias - TFDV Tool](https://www.cloudskillsboost.google/course_templates/985/video/515620)

- [YouTube: Identify Bias - TFDV Tool](https://www.youtube.com/watch?v=joLjA5voRog)

Now lets talk about how to identify bias problems in AI systems. There are many possible causes of biases in AI systems. Lets look from these two perspectives. Data and model bias identification always starts with asking good questions. For example, be careful about whether the dataset has proper data distributions. Is it skewed towards a subgroup of the population? Does it have selection bias problems? Also, you should be careful about the proportions. Even if a dataset contains a sufficient number of data points for the minority population, the label proportion can be different from the one of the majority population and you can check for bias issues in different ways. Even after training the model. You can check the model performance in different subsets of the dataset to verify if it doesn't have any harmful biases. You can also check if it doesnt unnecessarily benefit or harm a specific population. To help us identify bias issues, we can leverage a few open source tools TensorFlow data validation is a useful tool for exploring and validating ML data at scale. The what if tool by Google helps you investigate your data and model bias through data and prediction visualization. And helps you apply counterfactual analysis and TensorFlow model analysis is designed to analyze model behavior by applying granular model evaluation. Although the focus here is on the TensorFlow ecosystem tools, the tools mentioned are not limited to the TensorFlow library and are compatible with other frameworks. More importantly, the idea itself is universal, so you can find other alternatives. If you prefer other libraries, you can even implement an alternative on your own after you feel you properly understand what to do. Lets start from TensorFlow data validation or TFDV. TFDV helps you understand data distribution and statistics. Although this tool is not developed for fairness purposes only, it is always the starting point to help you investigate. For starters, with TFDV you can easily calculate summary statistics from a dataset. Although this code example shows you the TF record format which is tensorflows own super efficient binary storage format. You can also directly use a CSB file or a dataframe. All you need is a path to your data. With TFDV you can also calculate statistics by data slices. For example, you can slice data by country and state to help understand data proportion differences in various locations. After you calculate your stats, simply visualize them by using the method. Visualize statistics the dashboard contains two tables, one for numeric features and one for categorical features. The dashboard also has a control panel. The tables and dashboard are separated due to different information being shown for the two data types. Now in the table, a row corresponds to a feature. It contains some calculated statistics about the values of that feature across the entire dataset, along with a number of charts to show the distribution of values. For numerical values, TFDV calculates mean, standard deviation, number of zeros, and the min, median, and max values. The tool automatically tags some stats that might be problematic by bolding them and coloring them red. For example, if a feature had a high number of zero values, also known as null values, it might be automatically tagged. For categorical features, you are given the number of missing values, number of unique values, top or most frequent category, frequency of the top category, and average strength length. Notice this new screenshot and how it is different from the previous screenshot here. Within the same view, you can see a direct comparison of multiple data slices represented by blue and orange bars. With TFDV, you can also automatically infer the schema given stats. This is particularly useful for datasets with lots of features where manually defining the schema can be a tedious task. Instead, automatically generate it using infer schema, then refine as needed. This schema is especially useful when you want to automate data validation steps in an automated mlops pipeline. After defining the ideal characteristics of the dataset, you can automatically check the data issues from the next mlops pipeline execution. Lastly, TFDV lets you easily detect and act on anomalies by using the schema defined. Anomalies can be out of domain unexpected features, a skew of the entire data set, or a proportional gap between multiple data slices. By using this capability in a machine learning pipeline, especially in an automated system, you detect data problems before you train models. Again, although TFDV offers useful functionalities if you prefer, you can also develop a similar data validation system by using different libraries.

### Video - [Identify Bias - What-if Tool](https://www.cloudskillsboost.google/course_templates/985/video/515621)

- [YouTube: Identify Bias - What-if Tool](https://www.youtube.com/watch?v=Qs5CQJt9GzA)

Let's look at the next tools. The what-if tool can be used to visually analyze the interaction of datasets and ML models. With the what-if tool, you can do many explorations by using the data point editor and the performance and fairness tabs. Let's look at each. After loading a model and dataset, the first view you see allows you to visualize inference results using Facets Dive. In this example, each data point is colored by the category that the model predicted for it. Points are laid out top to bottom following the inference score. This means that points at the bottom have a prediction close to zero and very likely belong to the negative label. Points at the very bottom have a prediction close to one and very likely belong to the positive label. With this view, you can easily say that the model is often very confident since most points are close to zero or one and that the model predicts more points belong to the red label. From the facets dye view, you can investigate any data point. In this case, you would probably select difficult use cases such as data points near the decision boundary. To see data point details, click on the data point and a panel appears to the left of the visualization. You can then modify a new value and see if the prediction changes. Example. What if you change the age or the gender group of this data point? You can change a specific feature to see if the prediction is affected by that. For example, by changing the sex feature from male to female, the model prediction was flipped from positive to negative. This what-if analysis is called counterfactual analysis. It helps us understand the model behavior and identify bias problems. You can also apply this method to the entire dataset and quantify the ratio of the flip result when changing a sensitive feature value. This fairness metric is called flip ratio and helps you understand the model's vulnerability. You can also explore the effects of single features for a prediction result through partial dependence plots. This technique can also help us find bias problems, which we will explain how to do with the next interpretability module. The top example shows a partial dependence plot for a numerical value. The second example shows a plot for a categorical value. You can easily see that the model has learned a positive correlation between age and positive prediction. That higher degree is correlated to higher positive prediction. In the data point editor tab, you can find the most similar example to a data point either using L1 or L2 distance. The tool compares the two points side by side, and the green text represents features where the two data points differ. Let's now look at the performance and fairness tab for the last two points. Here, you can view confusion matrices and other metrics overall or per group. You can also slide the classification threshold around and the metrics will be updated accordingly. You also have the ability to automatically calculate the optimal classification threshold given a desired cost ratio. For example, the relative cost of a false positive versus a false negative. The default cost ratio in the tool is one, which means that false negatives and false positives are equally undesirable. Finally, you can test algorithmic fairness constraints by using the slicing capability in the performance and fairness tab. Here, you can analyze model performance for any sensitive feature. For example, if you use sex, you can see that the model is more accurate on females than males in the sense that it has less false positives and false negatives. You can also automatically calculate the optimal classification threshold given a fairness goal, such as demographic parity, which we will discuss in the next section.

### Video - [Identify Bias - TFMA Tool](https://www.cloudskillsboost.google/course_templates/985/video/515622)

- [YouTube: Identify Bias - TFMA Tool](https://www.youtube.com/watch?v=qLITg0UCm8U)

The TensorFlow model Analysis library or TFMA library can help you analyze train model performance from multiple perspectives, including fairness. TFMA is designed to support TensorFlow based models, but can be easily extended to other frameworks such as pandas data frame or scikit learn models. For TF Keras models, it can automatically apply the training metrics at evaluation time and doesn't require additional steps to compute with pre calculated predictions. The results are automatically saved with the model. TFMA supports a wide variety of metrics, including all the standard TensorFlow metrics. This includes regression metrics and classification metrics, and fairness related metrics like flip rate, metric settings can be customized, and you can also define custom metrics entirely. Let's look at TFMA's capabilities in more detail. After training a model, you can first run model analysis on the model. This step will create an evaluation result that can be saved for later use. This result can be visualized by using a TFMA widget that enables you to interactively check model performance metrics. You can also slice the performance by using a sensitive feature such as racial group. With this capability, you can easily check if there is a critical performance gap among different groups. You can also automate the model performance validation process using defined metrics thresholds, for example, in an automated machine learning pipeline. If TFMA finds a critical performance gap, you can stop the model update process before deploying the model. This capability is important when you are building an automated machine learning system. You can compare the performance of two models often used to check the performance improvement of a new model over the preceding model. You can also link the TFMA analysis result to a supplemental library fairness indicators, where you can investigate a model using multiple fairness metrics. The fairness indicator also provides an interactive widget where you can investigate the model performance. This capability can also be incorporated into the what if tool.

### Video - [Mitigate Bias - Data Intervention](https://www.cloudskillsboost.google/course_templates/985/video/515623)

- [YouTube: Mitigate Bias - Data Intervention](https://www.youtube.com/watch?v=3rqxV7ypfaM)

Now we will look at another set of tools and techniques to actually mitigate bias. Let's follow the same format: data and model. You can mitigate bias issues by applying several techniques, although the actual approach depends on the use case, usually, it involves multiple methods. First, let's talk about methods to mitigate biases by underpinning data. Teams should continuously improve and diversify the data sources and in notations included in their training data sets as data gaps can be harmful. This is due to over-representation and/or insufficient training data for specific subgroups that might lead to drastic performance differences across various subgroups. To improve the training data for machine learning models, you should aim for these goals: Ensure all groups have sufficient data representation, ensure data does not contain harmful labels or media that are outside your intended use case, such as pejorative labels or poor quality data, ensure your data does not skew towards harmful biases like stereotypes or bias correlations as it may lead to unintended correlations in your model, and ensure your data does not have unintended biases in the authorship or who collected the data, the way it was described, or who annotated or labeled it, or the content of the data itself, or what was captured. To achieve these goals, here are some techniques that help: Refine your data collection pipeline when you identify bias, especially when associated with data filters in the pipeline, resample the dataset to balance data, augment with more data, like other existing datasets, synthetic data, or new data collection, relabel your data by removing harmful labels, updating labels to current standards, and adding labels that were missed during the initial notation effort. Let's dive into each of the techniques mentioned. First, review and refine the pipeline when you identify bias in your data. When there are filters to include or exclude data in your data collection pipeline, ensure that you think through the implications of what is being filtered in or out, both intended and unintended. Every time you add a filter to your data, you heavily bias it in some way. Refining the pipeline can help achieve higher quality and more diverse data. There are also limitations on refining the pipeline. Best practices about fair data collection depend on features. For example, if you want to collect a fair human face image dataset, you can use the Monk Skin Tone or MST scale, developed by Dr. Ellis Monk in partnership with Google. The MST scale provides a 10-shape scale of skin tones that can be used to evaluate datasets and ML models for better representation. However, this kind of best practice might not be available for some sensitive features, and it can be challenging to define it from the beginning. Next, balance the data by resampling the dataset. For example, you can upsample or downsample the minority or majority group examples with existing data. Resampling can occur at various stages of the ML workflow and often depends on the use of the dataset. The benefit of this technique is that it's relatively cheap, no new data is required if you are upsampling underrepresented groups. However, be aware of a limitation potential to overfit highly underrepresented groups and pay attention to amplification, leading to stereotyping bias. Here is an example of balancing data. Imagine you are training a machine learning model using a dataset with flight attendant data. In this dataset, only a handful of male flight attendants exist in the data. For this case, it would be beneficial to consider downsampling the female attendance group or upsampling the other group. A limitation of this technique is that each individual example could be over-represented. To offset this, it's recommended to explore and collect more data for the minority group and augment with data. Now, augmenting with other existing datasets, synthetic data, or new data collection is an important technique for mitigating bias in data. However, note that augmenting with data enlarges the data pool, whereas balancing data by resampling operates on the existing data, and by collecting data from multiple datasets, you could increase representation of minority subgroups or failure cases. To add new examples for underrepresented minority subgroups, you can generate synthetic data. If budget and time are not pressing concerns, you can also add new examples by collecting new data. There are a few benefits to augmenting with more data. Additional data with improved fairness characteristics might be already available for widespread use. By increasing minority subgroup representation, you can balance the data without downsampling majority subgroups. Augmenting with synthetic data can be potentially less expensive than curating a new dataset to fill in these gaps. However, each of the techniques has its own limitations. For [inaudible] product uses with rare data attributes, usage of additional out-of-domain data may introduce technical complexity on the relevant MF tasks to overcome domain gaps. Training on synthetic data can be problematic, especially for simulator-generated data. For example, it's important to pay attention to whether a trained model is treating synthetic images differently from natural images in order to ensure the model doesn't cheat by simply changing scores for synthetic images, and oversampling might cause over-representation for the minority group, like upsampling. Now, of all the listed techniques, new data collection is the most expensive and time-consuming. Coming back to our previous example, we could also try a different approach to improve representation and reduce bias in our training model by augmenting our data with a new dataset with more male flight attendants. Lastly, relabel your data by removing harmful labels, updating labels to current standards, and adding labels that were missed during the initial annotation effort. This method can improve overall data quality. However, this may not change underlying sample bias. A real-life example of how relabeling data can be used to reduce bias in machine learning models is through sentiment analysis. Imagine you're training a machine learning model to classify movie reviews as positive or negative. Your training data might contain reviews with biased language, such as using stereotypical phrases or making generalizations about certain groups of people. To reduce bias, you could relabel some of the reviews by having human experts carefully review them, then assign more neutral labels. This results in helping the model learn to associate sentiment with the actual content of the reviews instead of biased language.

### Video - [Mitigate Bias - Threshold Calibration](https://www.cloudskillsboost.google/course_templates/985/video/515624)

- [YouTube: Mitigate Bias - Threshold Calibration](https://www.youtube.com/watch?v=CIpV9pMgPUY)

Now that we have learned how to mitigate bias from the data, let's see the techniques intervening machine learning models used to mitigate issues in different ways. First, even after training and fixing a machine learning model, there is still room to control classification behavior by calibrating thresholds. Threshold calibration defines the proportion of the prediction results by categorizing them into true positives, false positives, false negatives, and true negatives. After you determine these metrics on the entire data set, ensure that you go one step further by calculating the metrics across the different subgroups within your data. For example, you might find that a false negative rate at 0.1 is acceptable for the problem you're trying to solve with your machine learning system. Given that overall rate, how does that rate look across the subgroups as shown in these plots? After you compute your metrics, you can visualize the distribution of your evaluation metrics across each subgroup. This is depicted by the blue and green colors. Each color represents a different subgroup within the data. As you can see, predictions on each subgroup result in different proportions that could mean that the best and fair threshold can vary among different subgroups. By keeping this in mind, you are one step closer to identifying ways in which you can make your machine learning system more inclusive. To help you think through the thresholding from fairness purpose and to highlight what happens when you change variables. Let's look at an example where a machine learning model decides whether a bank approves or rejects loan applications. Imagine that we have two groups of people identified here by the colors blue and orange. Notice how the prediction score distributions are different between the two groups. Let's say we set the same threshold for the two groups at 55. With the same threshold. The blue and orange groups have different distributions and how they'll be able to pay back loans. However, the differences in group distribution and loan thresholds are a result of historical and systemic biases that have somehow made it into our data. Because of this, it might be unfair to ignore the differences between the two groups. Let's examine the distributions of these loan thresholds a bit more. The light gray color represents those who are denied loans as they would default anyhow. The dark gray color represents those who are denied loans but won't default. The light blue and orange colors represent those who are granted loans but default. The dark blue and orange colors represent those who are granted the loan and pay back. Examining these distributions between groups is important and varied, applicable in real life. For instance, in the United States, wealth inequality may affect the ability of different race and gender groups to repay a loan. This is a result of historical discrimination. Here's another example, women tend to live longer than men. Therefore, would it make sense to consider insurance rates and policies in the same way for both men and women? These two examples highlight the concept of group unaware or a simple threshold to set the same value across all groups. Looking back at the plots on the screen, the blue group is now being given more loans overall. But the orange group has been given fewer loans among people who would pay back the loan. The orange group is at a disadvantage. This is because the real differences that exist between the two groups are being ignored. Now let's consider another fairness definition, demographic parity. Let's have the bank issue the same percentage of loans to both blue and orange groups. What happens is that the ratio of loans given to each group becomes the same. But among the people who would pay back a loan, the blue group is now at a disadvantage. This is because the model only looks at loans given, not rates, at which loans are paid back. This demographic parity strategy is helpful in a use case where having equal final percentages of groups is important. Here's a third fairness definition, equality of opportunity. This means offering the same percentage of loans to those who can pay them back in both groups. In other words, set the same recall. Now you can see that among people who would pay back a loan, blue and orange proofs to be equally well. Throughout this loan example, we looked at different constraints to determine the level of fairness when issuing loans to two different groups. Each of these constraints has different mathematical properties and different outcomes. One is not necessarily better than the other. You also can't satisfy all properties and objectives at the same time. These are not the exhaustive list of the constraints. Which constraint you choose depends a lot on the context of the ML use case. The Aequitas Fairness Tree guideline offers you guidelines about which metrics you should prioritize on a specific use case. The decision tree includes conditions such as if positive predictions assist or punish people. If your system can intervene with most people with need, or only a small fraction.

### Video - [Mitigate Bias - Model Remediation](https://www.cloudskillsboost.google/course_templates/985/video/515625)

- [YouTube: Mitigate Bias - Model Remediation](https://www.youtube.com/watch?v=XdhF3yxlU34)

Threshold calibration is an easy way to adjust model output to a more desirable way without changing the model itself. What if we'd also want to intervene in the model training process to actually train a fair system. Remember that neural network models are trained to minimize loss values. If this is the case, does this mean we can change the training process itself by changing the loss function? The answer is yes. We can consider adding a penalty term that represents the magnitude of bias in some way, so that the training process can try to minimize not only the model performance but also biases. It is similar to the idea to reduce model complexity by adding regularization terms like L1 or L2 norms. There could be many ideas about this additional term for fairness. Here are two methods which are covered by the tensor flow model, remediation library MinDiff and counterfactual logit pairing. The two methods have different ideas and goals about fairness, but both add a supplemental term to original loss functions. Let's start with MinDiff method. MinDiff focuses on the distribution difference of model prediction with respect to different subsets of dataset. Let's say we are training a model with a dataset that has a sensitive feature such as gender group or racial group. After training, we split the dataset into two, subgroup A and subgroup B by using the sensitive feature and pass them to the model to get predictions. From this, we can then depict different prediction distributions on these two datasets. The idea of MinDiff is if the model is fair and robust enough, that output distribution of different subsets shouldn't be very different. Now in some situations, a sensitive feature reflects a social context related to the prediction objective and the model ends up capturing the pattern difference. This then becomes reflected in the distribution of the predictions. It does this by adding a penalty term that represents the discrepancy between two distributions by using mean maximum discrepancy. The term is added to the original primary loss function such as binary cross entropy loss and minimized in the train phase. The balance between the primary loss term and this MinDiff term can be controlled with Lambda value. With this term, we can expect the output distributions would be close compared to the original model without MinDiff. Let's move on to the next method, counterfactual logit pairing, or CLP. We already discussed the term counterfactual. In counterfactual analysis, we try to identify bias issues by modifying a sensitive feature, then checking how it affects model predictions. For example, let's say we are creating a machine learning model to validate the toxicity of text sentences, something Google's jigsaw team is actually working on and published as a perspective API. Using this as an example, if a machine learning model returns results that contain toxicity in text sentences, it will be regarded as unfair and harmful to some identities. Although the sentence structure is the same and the identities themselves are neutral, the risk scores are different when an identity token is changed. Again, because machine learning captures patterns from datasets, If harmful Beta patterns exist, it will be reflected in the result. In this case, some identity terms could be unfairly linked to negative scores. Counterfactual analysis provides us this information by setting up a proper what scenario. It helps us identify if the model's predictions are unfairly influenced by the presence of these identity terms, even when they are used in a neutral context. CLP also focuses on this perspective. The idea of this approach is if a model is fair and robust enough, the output result won't be changed much even if we change a sensitive feature value. Based on this idea, it adds an additional loss term that represents the gap of logits between original and counterfactual examples. Remember that logits is a classification models output before applying a normalization function, such as a sigmoid or softmax function. By minimizing this new loss term and the primary loss, we can expect the model would be more robust in terms of counterfactual fairness. This can be measured by some counterfactual metrics such as flip rate as shown in the graph. Counterfactual logit pairing can help models achieve lower flip rate. Although MinDiff and CLP have similar ideas to add a loss term that represents the magnitude of some bias, the target and idea are slightly different. MinDiff focuses on prediction distribution so that it leads to achieving fairness constraints such as equality of opportunity. Whereas CLP focuses on counterfactual fairness.

### Video - [Lab: Mitigate Bias with MinDiff in TensorFlow](https://www.cloudskillsboost.google/course_templates/985/video/515626)

- [YouTube: Lab: Mitigate Bias with MinDiff in TensorFlow](https://www.youtube.com/watch?v=g5rjLcBypkY)

Let's perform an exercise in a hands on lab. This lab helps you learn how to mitigate bias using MinDiff technique by leveraging TensorFlow Model Remediation library. In this lab, you will explore the toxicity text dataset, build and train a toxicity classification model, check the model bias by plotting the prediction results, apply MinDiff technique using TensorFlow Model Remediation library, and compare the result between the baseline and MinDiff models.

### Lab - [Mitigate Bias with MinDiff in TensorFlow](https://www.cloudskillsboost.google/course_templates/985/labs/515627)

This lab helps you learn how to mitigate bias using MinDiff technique by leveraging TensorFlow Model Remediation library.

- [ ] [Mitigate Bias with MinDiff in TensorFlow](../labs/Mitigate-Bias-with-MinDiff-in-TensorFlow.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/985/quizzes/515628)

#### Quiz 1.

> [!important]
> **As a data scientist, you are tasked with identifying bias in training data in an effective way. Which of the tools below can be used to identify bias in data?**
>
> - [ ] TensorFlow Data Validation, TensorBoard.
> - [ ] TensorFlow Data Validation, What-if Tool.
> - [ ] TensorFlow Data Validation, TensorFlow Serving.
> - [ ] TensorFlow Datasets, What-if Tool.

#### Quiz 2.

> [!important]
> **You have been asked to do a presentation in your organization on the importance of AI fairness and bias. As you are thinking through your key opening statement, what is the most appropriate statement you can make that explains why AI fairness is difficult?**
>
> - [ ] Fairness is difficult because bias is 100% harmful and it's critical to eliminate all bias in AI system development to achieve AI fairness.
> - [ ] Fairness is difficult because AI developers should not have direct access to their training and testing data, and can't find bias directly.
> - [ ] Fairness is difficult because there are pre-existing biases, a variety of scenarios, no standard definition of fairness, and incompatibility of fairness metrics.
> - [ ] Fairness is difficult because the decision-making process in AI systems can't be transparent and impossible to satisfy all parties.

#### Quiz 3.

> [!important]
> **A researcher did an anonymous survey with students in a mixed-gender middle school to learn the health and diet patterns of middle school students in the entire country. What type of bias could it introduce?**
>
> - [ ] Implicit bias.
> - [ ] Selection bias.
> - [ ] Automation bias.
> - [ ] Group attribution bias.

## Course Summary

This module provides a summary of the entire course by covering the most important concepts, tools, and technologies.

### Video - [Course Summary](https://www.cloudskillsboost.google/course_templates/985/video/515629)

- [YouTube: Course Summary](https://www.youtube.com/watch?v=aJiXrddFZko)

You've completed this course, Introduction to Responsible AI in AI Fairness and Bias. Let's recap what you have learned. In this course, we'd introduce responsible AI concepts and AI principles that lead to developing successful AI. You've learned about Google's seven AI principles. Be socially beneficial, avoid creating or reinforcing unfair bias, be built and tested for safety, be accountable to people, incorporate privacy design principles, uphold high standards of scientific excellence, and be made available for users that accord with these principles. You also learned why AI fairness and bias is important for machine learning and why it is challenging to achieve. We've identified a few tools that can help you identify fairness and bias in AI. For example, use the TensorFlow data validation to identify bias in data, and use what if tool and TensorFlow model analysis to identify bias in model. Additionally, you've learned about techniques that help mitigate bias. For example, techniques such as the refining data collection pipeline, balancing data, augmenting with other data, and relabeling data techniques all help mitigate bias in data and threshold calibration. MinDiff and counterfactual logic pairing help mitigate bias in models. As artificial intelligence continues its rapid ascent, the conversation around responsible AI becomes ever more vital. New technological developments constantly present fresh challenges and opportunities in this domain. It's even more important now to ensure that when you develop for AI, you are equipped with the latest insights and best practices for responsible AI implementation.

### Document - [Reading](https://www.cloudskillsboost.google/course_templates/985/documents/515630)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
