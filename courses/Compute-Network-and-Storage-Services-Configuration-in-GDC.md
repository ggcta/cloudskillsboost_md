---
id: 1198
name: 'Compute, Network, and Storage Services Configuration in GDC'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1198
date_published: 2025-01-24
topics:
  - Database
  - Databases
---

# [Compute, Network, and Storage Services Configuration in GDC](https://www.cloudskillsboost.google/course_templates/1198)

**Description:**

The course examines service resources or workload components that exist in projects. You'll learn about Kubernetes in GDC, Artifact Registry, GDC Object Storage, Database Service, Networking, and Key Management and Security.

**Objectives:**

* Configure services for the GDC platform and how the platform uses Kubernetes architecture and components.
* Recognize the benefit of working declaratively in deploying workloads and managing the Kubernetes environment.
* Create and enable access to storage buckets.
* Provision and work with a database cluster in GDC Database Service.
* Describe the networking features of GDC that let you isolate and interconnect workloads depending on your project needs.
* Recognize the requirements to maintain GDC platform security and the use and management of crypto keys.

## Course overview

This module introduces the audience, prerequisites, and agenda for the learning pathway.

### Video - [GDC Air-Gapped Practitioner Fundamentals overview](https://www.cloudskillsboost.google/course_templates/1198/video/522189)

* [YouTube: GDC Air-Gapped Practitioner Fundamentals overview](https://www.youtube.com/watch?v=Yy98wRTKghU)

SPEAKER: Welcome to the Google Distributed Cloud, or GDC air-gapped practitioner fundamentals series of courses. Within each of the three courses that make up this series, you'll focus on a specific aspect of GDC practitioner fundamentals. The first course provides an introduction to the GDC platform, which enables you to host, control, and manage infrastructure and services directly on your premises. GDC air-gapped is one component of Google Distributed Cloud offering, which aligns to Google's digital sovereignty vision and supports public sector customers and commercial entities that have strict data residency, security, or privacy requirements. The second course examines service resources or workload components that exist in projects. You'll learn about Kubernetes in GDC, Artifact Registry, GDC object storage, Database Service, networking, and key management and security. The third course explores advanced services such as machine learning and operational topics such as application deployment, monitoring and troubleshooting. In addition, we'll introduce GDC software upgrades, logging, billing, and cost monitoring. The intended target audience of this course series consists of various cloud operations professionals and system administrators, including DevOps engineers, application developers, cloud engineers, and database engineers. Additionally, this course is designed for solution architects planning to deploy applications and create application environments targeted for the GDC platform. To benefit fully from taking this course, you should have these prerequisite skills and knowledge-- an understanding of basic networking, familiarity with Linux, familiarity with identity and access management, familiarity with virtual machines, familiarity with Kubernetes, familiarity with databases, familiarity with Cloud APIs, and experience with application development. Along with the prerequisite skills and knowledge, it may help you to be familiar with the content from some Google Cloud courses, which you can find in the Course Resources. However, it is important to note that GDC differs in several significant ways from Google Cloud. Throughout this series of courses, you'll learn about GDC in context by exploring how Cymbal Federal, a fictional organization, accomplishes its goals of deploying three applications to the air-gapped platform. This case study attempts to mirror an enterprise level scenario, and aims to help you understand how the different components of the platform fit together and what their capabilities are.

## Kubernetes in GDC

This module delves into how GDC uses Kubernetes and how to manage user clusters in the platform. The module begins with some fundamentals of Kubernetes and containers before focusing on GDC clusters. You'll learn about cluster architecture, cluster configuration options, and how to create and manage user clusters.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1198/video/522190)

* [YouTube: Module overview](https://www.youtube.com/watch?v=4zxRAsIKF68)

SPEAKER: Welcome to Kubernetes in GDC. This module delves into how GDC uses Kubernetes and how to manage user clusters in the platform. You'll learn to-- identify key components of Kubernetes architecture, describe the ways that users interact with Kubernetes clusters in GDC, and describe how to create and view clusters. This module begins with some fundamentals of Kubernetes and containers before focusing on GDC clusters. You will learn about cluster architecture, cluster configuration options, and how to create and manage user clusters. Let's begin.

### Video - [Kubernetes and containers overview](https://www.cloudskillsboost.google/course_templates/1198/video/522191)

* [YouTube: Kubernetes and containers overview](https://www.youtube.com/watch?v=WmBQCgRJKyw)

SPEAKER 1: We'll start with an overview of Kubernetes and containers. If you're already an expert in Kubernetes, you can skip to the next video. In this overview, you'll encounter some important terminology related to Kubernetes. Listen for these terms as you review the basics of containerization and Kubernetes. Container. Container image. Docker file. Cluster. Node. Node pool. Pod. Control plane. Namespace and kubectl. Let's start with why, as an application developer, you might want to use containers to divide a monolithic application into services. Running a monolith that consists of multiple services within a single VM creates a problem. Services that share dependencies are not isolated from each other, and there is no way to independently scale the component services. One way to solve this problem is by moving to microservices on separate VMs. However, a problem with this approach is that two complete copies of the kernel run and consume resources, disk space, memory space, and CPU overhead. Scale this to hundreds or thousands of microservices and you see its limitations. A more efficient way to resolve the dependency problem for microservices is to implement abstraction at the level of the application and its dependencies. You don't have to virtualize the entire machine or even the entire operating system, but just the user space. The user space is all the code that resides above the kernel, and it includes applications and their dependencies. Creating containers means creating isolated user spaces for running service code. In fact, this diagram is not to scale. This diagram makes it seem like you could have maybe six containers in the same space as three VMs. However, the reality is something closer to having hundreds of containers that can all run in the same space. This is much more cost effective and efficient. Google runs just about everything in containers. If you can think of a Google service, it probably runs in a container in some form or fashion. You would normally think that containers run in VMs, but Google actually runs VMs in containers. But what exactly is a container? Containers are isolated user spaces for running service code. Containers are lightweight because they don't carry a full operating system. They can be scheduled or packed tightly onto the underlying system, which is very efficient. They can be created and shut down quickly because you're just starting and stopping operations, system processes and not booting an entire VM and initializing an operating system for each application. A service and its dependencies are called an image. A container is simply a running instance of an image. By building software into container images, developers can easily package and ship an application without worrying about the system it will be running on. A container image is structured in layers. The tool that you use to build the image reads instructions from a file called the container manifest. Docker is one of the most popular container runtimes. In the case of Docker-formatted container images, the container manifest is called a Docker file. Each instruction in the Docker file specifies a layer inside the container image. Each layer is read only. When a container runs from this image, it will also have a writable, ephemeral topmost layer. Although this open source technology can be used to create and run applications in containers, it doesn't offer a way to orchestrate those applications at scale like Kubernetes does. When you're running containers, you have to run them on some hardware, whether that's bare metal machinery or VMs. When you run containers at scale, this poses a problem. How do you determine where to run each container in a super speedy, automated fashion? Kubernetes shortened to k8s, with the 8 standing for the number of letters between the K and the S, tackles that problem for you by managing clusters of nodes and providing scheduling tooling to place your containers where they need to go, and fast. Kubernetes has built in commands to handle a lot of the heavy lifting that goes into application management, allowing you to automate day-to-day operations. It handles the compute, networking, and storage on behalf of your workloads. And Kubernetes continuously runs health checks against your services, restarting containers that fail or have stalled, and only making available services to users when it has confirmed they are running. Kubernetes works with clusters of computers. They can be physical machines or virtual machines. One or more computer is called the control plane, and others are called simply nodes. Node pools are composed of one or more nodes, all having the same machine type. Each node contains one or more pods. A pod has one or more containers. Pods are a group of one or more containers with shared storage and network resources, and a specification for how to run the containers. The job of the nodes is to run pods. The job of the control plane is to coordinate the entire cluster. In production environments, the control plane usually runs across multiple computers, and a cluster usually runs multiple nodes, providing fault tolerance and high availability. GDC projects are Kubernetes namespaces. Namespaces provide logical grouping of resources within a cluster which enables multi-tenancy. Resources are deployed per namespace and within a namespace, and names must be unique. Namespaces allow multiple teams to use a cluster because they are linked with resource quotas, so cluster resource consumption can be controlled. A resource quota defined by a resource quota object provides constraints that limit aggregate resource consumption per namespace. Kubernetes supports declarative configurations when you administer your infrastructure declaratively, you describe the desired state you want to achieve instead of issuing a series of commands to achieve that desired state. The job of Kubernetes is to make the deployed system conform to your desired state, and to keep it there in spite of failures. Declarative configuration saves you work because the system's desired state is always documented. It also reduces the risk of error. The kubectl command line tool lets you interact with the control plane for your Kubernetes cluster. The kubectl syntax is composed of several parts. The command specifies the action to perform. The type defines the Kubernetes object that the command acts on. The name lets you specify the object defined in type. When name isn't present, the command will act on all objects of the specified type, and optional flags let you add special requests like formatting the output in a certain way, such as a YAML file. Kubectl is a utility used by administrators to control Kubernetes clusters. You use it to communicate with the kube API server on your control plane. Kubectl transforms your command line entries into API calls that it sends to the kube API server within your selected Kubernetes cluster. Before it can do any work for you, kubectl must be configured with the location and credentials of a Kubernetes cluster. Kubectl stores its configuration in a file in your home directory in a hidden folder named . kube. The configuration file contains the list of clusters and the credentials that you'll use to attach to each of those clusters. To view the configuration, you can either open the config file or use the kube control command "config view." Just to be clear, kubectl config view tells you about the configuration of the kubectl command itself. Other kubectl commands tell you about the configurations of your cluster and workloads. Next, let's review GDC authentication in order to generate the kubeconfig file. To connect a GDC cluster with kubectl, you must retrieve your credentials for the specified cluster. To do this, use the get-credentials gdcloud command in any other environment where you've installed the gdcloud command line tool and kubectl. The gdcloud cluster's get-credentials command writes configuration information into a config file in the . kube directory in your home directory by default. If you rerun this command for a different cluster, it'll update the config file with the credentials for the new cluster. Before moving on to using Kubernetes clusters in GDC, let's briefly review the meaning of these terms and how they relate to one another. A container is a lightweight and portable executable image that contains software and all of its dependencies. A container image is an unchangeable static file that includes executable code, so it can run an isolated process on IT infrastructure. The image consists of system libraries, system tools, and other platform settings that a software program requires to run on a containerization platform such as Docker. A Docker file is a text document that contains all the commands a user could call on the command line to assemble an image. Clusters are sets of worker machines called nodes, that run containerized applications. Every cluster has at least one worker node. A group of nodes within a cluster that all have the same configuration, CPU, memory, networking are called a node pool. A pod represents a set of running containers on your cluster. It is the smallest and simplest Kubernetes object. The control plane provides a container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers. A namespace is an abstraction used by Kubernetes to support isolation of groups of resources within a single cluster. And finally, kubectl control is the command line tool for communicating with the Kubernetes clusters control plane using the Kubernetes API.

### Video - [GDC cluster architecture](https://www.cloudskillsboost.google/course_templates/1198/video/522192)

* [YouTube: GDC cluster architecture](https://www.youtube.com/watch?v=Wr2a6wAINWU)

SPEAKER: In this lesson, you'll learn about GDC cluster architecture. To help explain how clusters operate in GDC, let's return to the example of Cymbal Federal. Cymbal Federal wants to containerize its document archive solution in GDC. The application currently uses a container architecture that will run on the Kubernetes service in GDC. Current DevOps processes provide separation of duties for database management and application management. For this reason, the application and database tier will be deployed in different projects. The application and database have different life cycles. Transient or 1 to n application versions are deployed and tested against 1 to n databases. Following best practices, Cymbal Federal will have a cluster for each environment, dev and production. To do this, the platform administrator authenticates to the org admin cluster for Cymbal Federal's GDC instance and creates a user cluster. Once the platform administrator attaches the user cluster to the dev and production projects for the document archive solution, the application operators working in those projects can deploy containerized workloads to the cluster. Let's discuss how the GDC personas work together to provision and manage GDC clusters. Like Cymbal Federal, you can run orchestrated containerized workloads on GDC using the Kubernetes cluster service. GDC Kubernetes services lineage is based on technology from Google Kubernetes Engine, or GKE, and its rich add-on ecosystem. Developers seamlessly extend application development and delivery to GDC. Administrators manage cluster lifecycle and deployments across multiple tenants using existing DevOps automation tools and frameworks. Infrastructure operations uses the root admin cluster, a set of dedicated servers that hosts the system manager and core infrastructure services that need to interact with the system manager. Platform administrators create user clusters on the organization cluster. One or more projects can be attached to a user cluster. Application operators with the right permissions can deploy containerized workloads to a user cluster. Platform administrators have several options in GDC for how to organize clusters and projects. This is an important difference between Google Cloud and GDC. In Google Cloud, a GKE cluster is created within a specific project. In GDC, a user cluster is created by the platform administrators in the org admin cluster and can be attached and detached from projects. Letting platform administrators attach and detach clusters from projects provides flexibility for deployment models and teams. You can have a single project with multiple clusters, multiple projects in a single cluster, and multiple projects with multiple clusters.

### Video - [GDC configuration options](https://www.cloudskillsboost.google/course_templates/1198/video/522193)

* [YouTube: GDC configuration options](https://www.youtube.com/watch?v=5BvezDUT6us)

SPEAKER: In this next lesson, you'll learn about GDC configuration options by examining cluster functionality, GPU support, node pools, and multi-writer storage. Let's start with GDC features and functionality. GDC clusters can use different Kubernetes versions. Other important features of GDC clusters include GPU support with NVIDIA, resizable node pools, and support for node maintenance and multi-writer storage access. GDC clusters also have backup and restore features. You can create clusters using the GDC console and the Kubernetes API. GDC limits you to 16 user clusters per organization and 42 worker nodes per user cluster. Next, let's discuss some of these features in more detail. GDC Kubernetes clusters support GPU for machine learning, training, and inference. You can make GPUs available during cluster creation. Note that GPUs for pretrained Vertex AI ML APIs do not run on user clusters. Thus, for OCR and speech-to-text, no special configuration is needed to access GPU acceleration. GDC Kubernetes service supports node pools that can run multiple Kubernetes versions. They can be customized to enable different machine sizes and managed without cluster impact. Resizable node pools let you scale up and down nodes. The user cluster admin and the user cluster node viewer IAM predefine roles, [? encapsulate ?] cluster admin permissions. This YAML snippet configures a node pool. Workloads launched into production require some level of maintenance and support for compliance, troubleshooting, and new feature releases. GDC supports node maintenance and lets you manage your Kubernetes clusters after creation. How it works? Nodes are marked as unschedulable. Traffic is drained. Pods are terminated. VMs that are slated for maintenance will use a node selector to ensure they are restarted on the same node after maintenance is completed. Occasionally, containerized workloads require access to shared file block storage. For example, Cymbal Federal's document archive application processes audio files that will be transcribed into text. The files are received and loaded into shared persistent volume for batch processing by one or more pods running in the cluster. Access to this shared storage is enabled by the multi-writer storage feature configured in ReadWrite Mini mode. GDC provides multi-writer access to storage, including these features-- implementation using PersistentVolumeClaim, persistent storage dynamically provisioned and attached to the cluster, and storage classes for ReadWriteOnce, which allows single node access at a time, and ReadWriteMany, which allows multiple nodes access at a time. Next, let's review a YAML snippet of creating persistent storage. This example lets one or more nodes with pods access the storage. It assumes that you have already authenticated to the user cluster. This example depicts the deployment that mounts persistent storage. In this case, Cymbal Federal's Document Archive app will have three pods that will process audio files from the File path.

### Video - [Create clusters](https://www.cloudskillsboost.google/course_templates/1198/video/522194)

* [YouTube: Create clusters](https://www.youtube.com/watch?v=Y06AOR_7GTE)

SPEAKER: In this lesson, you'll examine the process of creating and backing up GDC clusters and discuss cluster and node pool management. Let's start with an overview of creating clusters. When creating a GDC Kubernetes cluster, you'll need to choose a deployment method, either the GDC console or the Kubernetes API. Then, configure the network and node pool settings. Clusters are created at the org level. When you create a user cluster in GDC, you create node pools that are responsible for running your container workloads in the cluster. You provision nodes based on your container workload requirements and can update them as your requirements evolve. Lastly, apply least privilege. The supporting predefined IAM role is user cluster admin, user-cluster-admin. Next, let's explore creating a cluster with a declarative YAML file. This snippet creates a cluster using the Kubernetes API through declarative configuration of the cluster resource. This approach lets you leverage software engineering practices to track changes and versions using code repositories. This file provides insight into how Kubernetes clusters are configured. You can also use the web console to create a cluster. Let's walk through the steps. Here, the platform administrator navigates to the compute section and clicks Clusters to arrive at the Clusters Management page. On the Clusters Management page, click Create Cluster. GDC provides a configuration wizard to guide the cluster creation. On the Cluster Basics page, you name the cluster and select the version. You must attach the cluster to at least one project. The networking page of the wizard setup offers IP and CIDR range information that cannot be changed after the cluster has been created. You can change the default network settings as necessary for your configuration. On the Node Pool page, select the machine type and the number of nodes. At least one node pool having at least one node is required. The administrator can create one or more node pools to accommodate different workload requirements, such as GPU, high memory, or general computing. GDC does not allow custom machine types for node pools. The number of node pools is limited by your address ranges.

### Video - [GDC BackupPlan](https://www.cloudskillsboost.google/course_templates/1198/video/522195)

* [YouTube: GDC BackupPlan](https://www.youtube.com/watch?v=Cjo-a5nHSa0)

SPEAKER: The GDC BackupPlan resource defines the backup process. If you are familiar with backups for applications or VMs, you would need to specify what files are being backed up and the time or frequency of backups. All these elements are configured within the BackupPlan resource. Google recommends providing every cluster with at least one backup plan. Instead of having one very large backup taken at a single time of the day, you want multiple smaller backups distributed throughout the day. A single cluster or VM can belong to more than one backup plan, and a single backup plan can capture multiple VMs if they exist in the same cluster. A backup plan cannot span more than one cluster. To configure backup plans, you need the user cluster backup admin IAM role. Let's go over backup operations. First we'll cover the kube control CLI. Then we'll walk through the process with the console. A backup repository is the storage location used by the cluster to write and read backups, restore plans and other GDC resources used within the backup, and restore process. The repository is an S3-compatible storage location, so naturally, as a practitioner, you should think about IAM and access and credentials necessary to accessible backup repositories. The creation of a backup repository is a two-step process. First, you need to configure access credentials for the storage. This manifest snippet uses standard Kubernetes resources types for proper handling of secrets. In the second step of creating a backup repository, you configure the BackupRepositoryManager. This example uses an external Google Cloud Storage location. The configuration of the backup repository is in read/write mode. Therefore, it can be used to schedule and create backups, backup plans, and restores. The BackupRepositoryManager allows access to the S3 credentials secret and creates a backup repository resource in the cluster. Next, you can use a manifest file to create a backup plan. This snippet creates a backup plan for mission-crm-cluster. The backup schedule uses a cronSchedule that performs the backup every day at midnight. The backup will be stored for 10 days within the backup repository. A manual backup must refer to an existing backup plan. A config backup includes a set of Kubernetes resource definitions. A volume backup backs up attached disks. This snippet creates a backup named mission-crm-backup using the backup plan we just created. You can also use the kube control CLI to view backups within a cluster. Controlling access to backups is often a security and compliance requirement. The User Cluster Backup Admin IAM role can see all backups in the organization. The Backup Creator role can see all backups in a given project. This example manifest file deletes backups within your cluster. This snippet removes a backup named backup-crm that exists within the mission-crm namespace. Now that you've learned how to create backups using kube control, let's walk through the process using the GDC console. First, select Backup for Clusters under the Backup & Restore menu within menu context of the organization. Recall that backups go to S3-compatible storage, and GDC uses custom resources to protect stateless and stateful workloads. From the Backup for Clusters console view, you can readily create, edit, and view backup management objects. This console view lets you access the GDC custom resources that participate in the backup process, including the backup repository, backup plan, and restore plan. This console screen lets you create a backup repository. You must have an S3 storage-compatible endpoint and bucket with configured access credentials. A working cluster must also be accessible. You can also use the console to create GDC backup plans. First, you define the plan details. This configures the cluster to back up, identifies its backup location, and provides the backup plan name. Then you define the scope or which resources to include in the backup and how to protect them. Finally, you configure the schedule and file retention time. Next, let's verify the components that must be installed on a cluster to enable backups. To determine whether the cluster can be backed up and workloads can be restored, you inspect the gpc-backup-system namespace to get a list of running pods. Reviewing the output, there should be two pods running, a control plane pod and an agent pod. To restore workloads to your cluster, you define restore plans. Restore plans are preconfigured scenarios that restore selected backups. Access to restore plan management permissions is governed by the user cluster backup admin predefined role that is associated with the platform administrator. A restore plan enables granular restores by selecting workload namespaces or resource attributes. Restore plans work in synchrony with backup plans. This manifest file creates a restore plan. A restore plan custom resource manages the restore process using an existing backup plan. A backup plan configures a backup, its location, and frequency of scheduling. In this example, the restore plan name is mission-crm-restoreplan, and the backup plan is mission-crm-backup-plan. Now let's discuss how to create manual restores within a cluster. When a backup is restored, the Kubernetes resources are recreated in the target cluster. From an operational perspective, you can also restore workloads and files from backups of other clusters. This manifest file specifies a manual restore request that is responsible for restoring workloads to clusters. A manual restore request resource performs restores from existing restore plans. During creation of a manual restore request, specify the backup that you want to restore, the name of the restore that you want to create, and the restore plan that matches the backup plan of the backup. After restoring GDC Kubernetes workloads, you can check the progress of a restore. You can use the kube control CLI or the console to inspect restores across the default namespace, all namespaces, and within the mission-crm namespace. Deleting a restore removes restore artifacts from object storage. Restores are deleted by removing the resource directly from the API server. In this snippet, you use the kube control CLI to delete a restore named mission-crm-restore from the mission-crm namespace.

### Video - [Cluster management and observability](https://www.cloudskillsboost.google/course_templates/1198/video/522196)

* [YouTube: Cluster management and observability](https://www.youtube.com/watch?v=KuAEw0gZwM0)

SPEAKER: Now let's discuss how to manage clusters using the GDC console. The Clusters section provides a list view of all clusters within the organization with an overview for each cluster. You can check the cluster status, version, and any notifications. To inspect cluster details, select the name of a cluster. In this example, the platform administrator inspects the details for cymbal-federal-cluster-1 and then navigates to the Node Pools section of the console. Multiple pools can be assigned to a cluster. Click on the name of the node pool to go into the pool details. In the node pool management section, you find more information about how a cluster was created. To monitor your cluster performance, GDC Kubernetes uses Grafana. Under the cluster details tab in the GDC console, you can view a collection of common monitoring metrics collected for each cluster and individual node. In this example, the cluster administrator navigates to the Kubernetes cluster dashboard by clicking View All in Grafana. Metrics that are aligned to site reliability engineering best practices are collected for each node of a cluster. Typical metrics include utilization, memory, and latency. Grafana can break down the details for individual items. For example, you can select the Cluster CPU Utilization for more detail.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1198/video/522197)

* [YouTube: Module review](https://www.youtube.com/watch?v=pfD3Y0v1Qzw)

SPEAKER: Let's review the key points from this module. First, we discussed the GDC VM service features, such as VM types and the management options for creating VMs and disks. This module also highlights the capabilities for data recovery that are available with the VM service backup and snapshot features. In addition, flexible network configurations for GDC VMs govern ingress using public and private IP addresses. GDC VMs have a life cycle that defines their running operational states. And the VM service includes inbuilt observability dashboards for monitoring VMs. Finally, the module presented video demos of VM management tasks, such as how to create VMs, manage VM disks, and access VMs using the secure shell protocol.

### Quiz - [Knowledge Check](https://www.cloudskillsboost.google/course_templates/1198/quizzes/522198)

#### Quiz 1.

> [!important]
> **Cymbal Federal wants to provide semantic search capabilities for its document metadata batch processing that currently runs on GDC Kubernetes. The new enhancements include a parsing and analysis library that uses GPUs. The team wants to run both solutions in parallel without interrupting the current solution. What should Cymbal Federal do?**
>
> * [ ] Deploy VMs with GPU support.
> * [ ] Use Vertex AI semantic search APIs.
> * [ ] Deploy a node pool to the existing cluster with nodes that use GPUs.
> * [ ] Rebuild a new cluster that uses GPUs.

#### Quiz 2.

> [!important]
> **The Cymbal Federal development team is testing a new software feature. The security team updates the underlying OS patches and periodically changes base libraries that the application needs. The development team wants to control the application dependencies. What should they do?**
>
> * [ ] Migrate the feature serverless functions.
> * [ ] Containerize the application.
> * [ ] Disable OS security updates.
> * [ ] Rewrite the application to use different libraries.

#### Quiz 3.

> [!important]
> **Cymbal Federal's Mission CRM application team plans to use Kubernetes service. Traffic for the application increases and decreases throughout the day. They want to design efficient infrastructure use for the application. Which Kubernetes feature supports that goal?**
>
> * [ ] Open source software
> * [ ] Flexible and dynamic storage orchestration
> * [ ] Self-healing clusters
> * [ ] Autoscaling of containerized apps

## Google Distributed Cloud (GDC) air-gapped object storage service

This module describes the GDC object storage service, which provides S3-compatible managed storage. The module begins by differentiating between storage types and how they are used in an example GDC deployment. You'll learn about configuring access to storage buckets and how to perform management operations in GDC object storage. 

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1198/video/522199)

* [YouTube: Module overview](https://www.youtube.com/watch?v=c_nvfKPutus)

SPEAKER: Welcome to Google Distributed Cloud, GDC, air-gapped Object Storage service. In this module, you will explore GDC storage options and their purpose. You'll also learn how to complete common bucket management tasks in GDC Object Storage and to configure access. This module begins by differentiating between storage types and how they are used by Cymbal Federal in an example GDC deployment. Then we'll go into more depth on the GDC Object Storage and its management.

### Video - [Overview of GDC object storage](https://www.cloudskillsboost.google/course_templates/1198/video/522200)

* [YouTube: Overview of GDC object storage](https://www.youtube.com/watch?v=pZHTzs7mWKY)

SPEAKER: Let's begin our overview of GDC Object Storage by revisiting Cymbal Federal. Cymbal Federal has recently acquired a new bulk data set of mixed media, requiring optical Character Recognition, OCR, speech-to-text, and machine translation services. Recall that Document Archive, the speech-to-text application suite, receives sensitive audio files and transcribes the files to text. It provides metadata indexing for cataloging and long-term archival storage in accordance with regulations required by past legislation. Cymbal Federal receives tarballs of audio files, and associated text, and metadata for each file. In accordance with the law, Cymbal Federal needs to archive audio files until its content is designated nonsensitive by the executive branch. Therefore, Cymbal Federal will need to design, configure, and deploy object storage buckets with a retention policy. GDC Object Storage provides this capability. GDC provides both block and object storage. File storage stores and organizes data into hierarchical folders, similar to the physical files you might store in a paper filing system in an office. Block storage improves on the performance of file storage, breaking files into separate blocks and storing them separately. A block storage system will assign a unique identifier to each chunk of raw data, which can then be used to reassemble them into the complete file when you need to access it. Block storage doesn't require a single path to data, so you can store it wherever is most convenient and still retrieve it quickly when needed. Object storage is a data storage architecture for storing unstructured data, which sections data into units or objects and stores them in a structurally flat data environment. Each object includes the data, metadata, and unique identifier that applications can use for easy access and retrieval. With object storage, the data blocks of a file are kept together as an object, together with its relevant metadata and a custom identifier, and placed in a flat data environment known as a storage pool. In this module, we'll focus on GDC Object Storage. GDC Object Storage has several benefits. Object storage size is essentially limitless, so data can scale to exabytes by simply adding new devices. Object storage has no folders or directories, removing much of the complexity that comes with hierarchical systems. The lack of complex trees or partitions makes retrieving files easier because you do not need to know the exact location. Object storage also provides searchability. Metadata is part of objects, making it easier to search through and navigate without the need of a separate application. You can tag objects with attributes and information, such as consumption, cost, and policies for automated deletion, retention, and tiering. Object storage supports cost efficiency by providing storage for large amounts of data, generally, at a lower cost than block and file storage. With a comprehensive set of features tailored to meet the demands of modern enterprises, GDC Object Storage excels in performance, security, and scalability. It provides platform teams the convenience of a fully managed storage solution that takes care of the complexities of data management, allowing you to focus on your core business operations. The GDC platform adheres to national security standards, such as NIST and FIPS, and includes hardware security features for encryption key-management and tamper-detection capabilities. Efficient data storage tiers let you maximize cost efficiency by utilizing different storage tiers tailored to your specific requirements. You can choose from standard and nearline classes, optimizing your data storage costs without compromising on accessibility. GDC Object Storage has additional features to support data security and availability. Practitioners can fine-tune data access permissions using Identity and Access Management, IAM controls. This ensures that only authorized personnel can access, modify, or delete your data, enhancing data security. You can seamlessly move data in and out of the platform with flexible data transfer options. GDC Object Storage also supports organizational multi-tenancy while maintaining isolation and control over each tenant's data and access rights. And it provides scalable and durable object storage capacity with high availability active-active storage controllers. Customers purchase data storage in increments of 10 tebibytes and can independently scale capacity per storage class. Which storage class you use depends on how often you will access the data and how you will use it. The standard class has 30-day minimum retention for billing purposes. It is best suited for hot data, such as email, videos, image files, web pages, and sensor data, online or actively accessed data. The nearline class has 60-day minimum retention for billing purposes. It is typically used for data archiving and backup. GDC Object Storage has robust security features that collectively contribute to a high level of data protection, ensuring the confidentiality and integrity of data, both at rest and in transit, as well as providing flexibility and control over data access and retention. GDC provides FIPS Level-3-compliant data encryption at rest with encryption on the organization level. Level 3 adds requirements for physical tamper resistance and identity-based authentication. There must also be physical or logical separation between the interfaces by which critical security parameters enter and leave the module. GDC supports object encryption using Service-Side Encryption, SSE, and SSE with Customers' provided keys, SSEC. You can use Customer-Managed Encryption Keys, CMEK. GDC Object Storage provides two levels of encryption at rest, with keys managed by the storage node and integrated with the Hardware Security Module, HSM. HSMs are physical computing devices that safeguard and manage user access credentials and perform encryption, decryption, and other cryptographic functions. GDC Object Storage supports data encryption in transit using TLS 1.3. It also supports WORM buckets-- Write Once, Read Many. Data may only be written once but can be read many times. You select a data retention policy and data is prevented from being overwritten. This can be used for ransomware prevention. Finally, GDC object storage also supports object encryption using service-side encryption, SSE, and SSE with customers' provided keys, SSEC. Client-side encryption can be used with SSE to ensure even more protection in multi-tenant environments. Now, let's discuss how to transfer data to GDC Object Storage. GDC has several options. First, the online transfer tool supports data transfers within the same GDC instance. You can also transfer bucket-to-bucket data across storage tiers to a different GDC instance. You can transfer from an external S3 compatible object storage to a GDC Storage Bucket. Or you can transfer from block storage to object storage. This is useful if you have an architecture change and want to limit custom scripting to move files to object storage. You can add data from a disk using the GD Cloud CLI or kubectl. Just like with other GDC resources, access to GDC Object Storage is controlled by GDC IAM, which is based on Kubernetes RBAC. You can use the web console, GD Cloud CLI tool, Kubernetes API, or the S3 API, which is XML-based, to perform bucket and object management activities.

### Video - [Configure access to GDC object](https://www.cloudskillsboost.google/course_templates/1198/video/522201)

* [YouTube: Configure access to GDC object](https://www.youtube.com/watch?v=OdqXO10qlts)

SPEAKER: Next, let's explore predefined IAM roles and how you grant access to GDC Object Storage buckets. GDC has predefined roles designed with least privilege in mind. Keep in mind that GDC Object Storage is a service resource and must exist within a project. IAM roles include Project Bucket Object Viewer, which provides read-only access, Project Bucket Object Admin, which provides read and write access, and Project Bucket Admin, which allows management of all buckets in the given namespace or project. Let's cover an example of granting access to a GDC Storage Bucket. This manifest file grants read and write access to all buckets in the namespace. Access is granted to both a service account and a user account. This action generates a set of two credentials for accessing storage buckets with the namespace, one for each storage class, standard and nearline. The format of access credentials includes the secret name, storage class values, standard or nearline, the subject type value, user or service account, and the subject hash. You need the Org Session Admin IAM role to query and obtain user storage keys. This command retrieves the bucket credentials secret names using kubectl. For a user, the secret is stored in the organization cluster within the object storage access key's namespace. There are two results, a secret name for each object class, standard and near-line. Next, let's retrieve the contents of the secrets. There are two steps to retrieve the contents of user access credentials, the secret. First, you retrieve the contents of the user access credentials. During step two, the result is decoded into the access key ID and secret and exported as variables. The process of retrieving service account bucket credentials is very similar to retrieving user access credentials. For service accounts, the secret is stored in the namespace or project as the workload. Next, we'll explain how to set up the gcloud CLI tool in order to manage GDC Object Storage. To configure the gcloud CLI for storage, you'll follow these steps. The access key and secret key are required to interact with the bucket. First, install the dependencies. Second, set the access key ID and secret access key. And, finally, set the endpoint.

### Video - [Manage GDC object storage](https://www.cloudskillsboost.google/course_templates/1198/video/522202)

* [YouTube: Manage GDC object storage](https://www.youtube.com/watch?v=OIdDqsYq6Cw)

SPEAKER: Next, let's explore how to manage GDC object storage. In this lesson, we'll explore creating, listing, and deleting buckets, as well as other management capabilities of GDC. GDC object storage can be orchestrated using the Kubernetes resource management API. To create a storage bucket in a project or namespace with standard storage class, name the bucket and the namespace or name of the project that the bucket exists in. This manifest file creates a storage bucket in a project or namespace with a standard storage class. This example manifest file creates a standard storage bucket with a retention policy. The retention day count specifies the minimum number of days that each version of every object will be retained. The default is none, and the maximum is 30 days. The retention period for a bucket can only be increased, not decreased. This is an example of the three-step manifest file to create a WORM storage bucket in a project or namespace, with an unspecified locking policy. To configure read-only and write-only access for service accounts, first, create a bucket. Then create role bindings for a service account that has write access. Finally, create role bindings for a different service account that has read-only access. You can generate a list of storage buckets using the Kubernetes API, GDC console, or GD Cloud. These kubectl commands list all storage buckets in a project or namespace. You can also list all objects and all object versions of objects in storage buckets using gdcloud CLI commands. The gdcloud CLI must be configured with gdcloud config to set the S3 access key ID, secret access key, and S3 endpoint. You can use gdcloud commands to upload a local object to a bucket, copy an object to another location, or move an object to another location. UTF-8 characters can be used in the bucket and file names. But refrain from using any personal identifiable information, PII. This command modifies an object's metadata, which is a list of key value pairs. In this example, you use the gdcloud CLI to set the metadata values for the approved and department keys. These gdcloud CLI commands let you download objects from storage buckets. For example, you can download a remote object and store it in a local file, download all objects from a bucket, or download all versions of a file. GDC storage buckets can be removed using gdcloud and kubectl CLI tools. However, you can't delete a GDC object storage bucket until all objects have been removed first. These command snippets remove a bucket. Deleting objects from storage buckets is a common management activity. Although you can remove objects in a bucket using the gdcloud CLI, you may commonly remove objects using the console. To delete objects and storage buckets, first, select Object Storage, then Storage in the navigation menu. Then select the name of the bucket containing the objects and wait to be redirected to the Bucket Details page. Click on the checkbox in the row for the bucket to be deleted, and confirm the delete action by entering Delete. Wait a few minutes and refresh to check that the object has been deleted. If you get the message, can't delete your bucket containing existing objects, delete the objects inside to delete the bucket. Then the bucket is not empty. Buckets must be empty before they can be deleted. To prevent objects from being permanently deleted, you can create a bucket with a locking policy and set the retention period for a specified duration of time. Locked objects cannot be deleted until the lock expires. This way, case objects under this policy cannot be deleted. Object locking prevents accidental deletion. The retention period for a bucket can be modified after creation. However, the change will only take effect for new objects and versions. Existing objects and versions will still use the previous value. If you do not set a retention period, there is no lock policy on the volume. Ensuring the intended deployment configuration of infrastructure is an important task in maintaining organizational policies, conventions, and security posture. These code snippets provide an example of a manifest file to view the configuration of a storage bucket in a project or namespace. The results can be inspected as part of automated auditing. Monitoring usage is another important piece of managing storage. GDC provides role-specific monitoring and alerts to track object storage usage. Grafana gives platform administrators and application operators access to the appropriate metrics for each persona. Currently, you can monitor the size of the objects, the number of objects being stored, and the storage quota for a GDC tenant. Grafana lets you set alarms to notify you when thresholds are reached. Grafana also enables additional observability using a query interface to granularly inspect the storage metrics based on the labels, such as the bucket name and SKU ID. This is a sample dashboard that presents metrics, in this case, the size of data stored for a storage bucket named cymbal-federal-logistics. Within the GDC platform, the dashboard solution is based on Prometheus and Grafana. Prometheus collects rich metrics and provides a powerful querying language. Grafana is a multi-platform, open-source analytics and interactive visualization web application that provides charts, graphs, and alerts for the web when connected to supported data sources. Note that for object storage, there are no pre-built dashboards for storage monitoring. You must build the dashboard to use the metrics that have been collected to display them in a way that is visually useful.

### Video - [Demo GDC GUI - Create, manage and delete storage buckets](https://www.cloudskillsboost.google/course_templates/1198/video/522203)

* [YouTube: Demo GDC GUI - Create, manage and delete storage buckets](https://www.youtube.com/watch?v=a9TeAlh04Ew)

SPEAKER: To add storage to your GDC project, the first thing you do is, make sure that you're in your project context. In this case, we're in backend. We go down to Object Storage from the hamburger menu. Select the Create Bucket icon. Give the bucket a name. Give it a description. Determine the storage class if you need it to be standard or nearline. We're going to leave it at standard for this example. Next, we specify a retention policy. The retention policy makes a determination as to how long a given version of a file will stay around. In this case, we're going to enable the retention policy, and we're going to specify two days. Currently, the maximum possible number of days for a retention policy is 30. Once you've selected this, we hit the Create button, and then we get the notification that the bucket has been created. It will take a few minutes for the status to go from Not Ready to Ready. To manage the contents of a GDC bucket, first, make sure that you're in the correct project that you want to manage. Use the hamburger icon. Drop down to Object Storage. Select the bucket that you want to manage, and from here, we can upload files, create folders. We can also delete things, and I'll show you that in a second. First, we're going to upload a file. I'm going to select this file. And we see that this file was successfully uploaded. If we wish to delete a file, we simply highlight that file. Hit the Delete button. Select Delete. To completely delete a bucket, make sure that bucket is empty. Hit the check box next to the bucket. Select Delete Bucket, and then we'll actually have to type in Delete. One thing of note, the bucket must be completely empty before GDC will allow you to delete it. And there, we see the deletion is in progress.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1198/video/522204)

* [YouTube: Module review](https://www.youtube.com/watch?v=ZFQiy09CQf0)

SPEAKER: Let's review the key points from this module. The GDC Object Storage service is an S3 protocol compatible, fully managed service that is scalable and generally more cost effective than block storage. It features enterprise-grade security, including encryption at REST and capabilities for customer-managed encryption keys. We explored common operations such as creating buckets and managing objects within storage buckets. Storage buckets and objects can be managed using the GD Cloud CLI and the Kubernetes API. Predefined IAM roles govern access and operations in GDC object storage. Access to buckets requires an IAM role and configured access keys and secrets, which are provisioned per storage class. In conclusion, its security, cost effectiveness, and S3 compatibility make the fully managed GDC Object Storage service an enterprise-grade solution for storing your workload data efficiently.

### Quiz - [Knowledge Check](https://www.cloudskillsboost.google/course_templates/1198/quizzes/522205)

#### Quiz 1.

> [!important]
> **Cymbal Federal's Document Archive application team generates metadata for audio files. Files with the most recent critical topics always need to be readily available for download. They want to optimize data access and storage costs in GDC. What should they do?
**
>
> * [ ] Configure an object storage bucket with the standard storage class.
> * [ ] Deploy a GDC persistent disk and store the files.
> * [ ] Deploy a GDC database cluster and store files in the database.
> * [ ] Configure an object storage bucket with the nearline storage class.

#### Quiz 2.

> [!important]
> **Cymbal Federal's Mission CRM application team is launching a new feature requiring partners to upload documents in PDF and image format. They want an efficient solution design in GDC that will make files highly available and searchable. What should they do?**
>
> * [ ] Design and build the new feature with an external S3 bucket.
> * [ ] Design and build the new feature with file storage in GDC.
> * [ ] Design and deploy a new Kubernetes cluster and mount external storage and access files using pods.
> * [ ] Design and build the new feature to utilize GDC Object Storage buckets with the standard storage class tier.

#### Quiz 3.

> [!important]
> **Cymbal Federal's Document Archive application team generates metadata for audio files. They plan for human-in-the loop random auditing of recording files and metadata. The engineers want to retrieve files from audit buckets for human review. What should they do?**
>
> * [ ] Procure and send a thumb drive to Infrastructure Operations.
> * [ ] Locate a backup and look for the files.
> * [ ] Write a job to send the files using email.
> * [ ] Install and configure the gdcloud CLI tool with access keys and endpoints and retrieve the files.

## Database service

In this module, you'll focus on the managed database service offering in GDC. The module begins by discussing the benefits of a managed database service. Then, you'll explore the features of GDC Database Service and the user experience for provisioning and working with the database cluster. 

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1198/video/522206)

* [YouTube: Module overview](https://www.youtube.com/watch?v=E0kOee_nJIk)

SPEAKER: Welcome to this module on Database Service. In this module, we'll focus on the managed Database Service offering in Google Distributed Cloud, GDC, air-gapped. You'll learn about the benefits of using a managed database service, and become familiar with the features and user experience provided by GDC Database service. This module begins by introducing GDC Database Service options and discussing the benefits of using a managed database. We'll talk about how a database can be used by Cymbal Federal in an example GDC deployment. Then you'll explore the user experience for provisioning and working with the database cluster.

### Video - [GDC Database Service](https://www.cloudskillsboost.google/course_templates/1198/video/522207)

* [YouTube: GDC Database Service](https://www.youtube.com/watch?v=5QVh89PXNoY)

SPEAKER: Let's begin with how Cymbal Federal uses GDC Database Service. Cymbal Federal's document archive solution receives sensitive audio files and transcribes the files to text. The solution stores metadata for the PDF documents and provides long-term archival storage. The metadata will be stored in a PostgreSQL relational database, so Cymbal Federal needs to design, configure, and deploy GDC Database Service. Why use a managed database service? To answer that question, let's discuss databases and what they offer. Applications that perform essential functions within an organization normally create, orchestrate, or operate on data. Data is born in your apps and in your operational databases. Databases are an essential aspect of every customer's cloud journey. Today's migration to the cloud centers around the pursuit of innovation, automation, and efficiency. It is heavily dependent on the modernization of applications. Managed databases let you focus on key products and services. They relieve you of operational processes, such as software installation, upgrades and patching, and storage and hardware management. In addition, they provide platform features to empower enterprise workloads, such as highly available compute and data storage. Managed databases can be designed to scale horizontally, adding more servers, or vertically, upgrading hardware with additional CPUs, memory, to handle increasing data volumes and user loads. Managed databases offer security features, such as access control, encryption, and auditing, to protect sensitive data from unauthorized access or breaches. Managed databases let you configure storage resources with minimal effort, which is essential for handling varying workloads. Managed database services provide mechanisms for regular backups and disaster-recovery planning. This ensures that data can be restored in case of hardware failures, data corruption, or other unexpected events. In summary, managed databases decrease product service launch times and management complexity by providing capabilities to store, scale, and secure your application data with lower operational effort. Database Service for GDC is a managed database as a service offering. It is tailor-made and fully integrated with GDC so you can provision, operate, and monitor all within the same platform with a common look and feel. Currently, it provides support for three database engines-- PostgreSQL, an open-source object relational database; Oracle, one of the most popular commercial databases; and AlloyDB Omni, an optimized PostgreSQL-compatible database. The database service is natively integrated in GDC, so it is governed by GDC IAM and integrates with the GDC dashboards powered by Grafana. Inbuilt GDC security features provide encrypted data storage at rest, SSL-enabled connections, and key management services to enable customer-managed encryption keys. Depending on your workload requirements, access and management boundaries can be configured at the project and organization level. Because the database service is governed by GDC IAM, users need the appropriate predefined roles to operate GDC clusters and view dashboards. These roles include project viewer, project DB viewer, project DB editor, project DB admin, project bucket-object viewer, and project Grafana viewer. Additional IAM roles grant the permissions to use and administer GDC database cluster maintenance policies. These include Project MP viewer, project MP editor, project MPB viewer, project MPB editor, and project MP admin. The key takeaway is the granularity and the encapsulation of permissions to enable separation of duties. For example, administration of maintenance policies and administration of policy bindings are separated across the roles project maintenance binding editor and project maintenance policy admin. Policy bindings become valid when they are attached to database clusters. Now, let's explore the database-platform features through the lens of three key areas-- security, manageability or operations, and backups and restore. For security, all data that is encrypted at rest and the network endpoints provided for your SQL connections are SSL enabled. Granular, predefined roles ensure separation of duties and control who can create, modify, view, or connect to database clusters in your project. This is controlled using roles that are integrated into the GDC IAM and RBAC systems. Logs are available to you. GDC takes special care to handle audit logs and make them immutable. In the area of operational manageability, database clusters have options for self-healing and High Availability, or HA. A standby and primary instance exists when they are configurable. Cluster-maintenance windows allow for automatic updates when it's convenient for your organization's workloads. Staying up-to-date with the latest database versions and security patches has never been easier. And you can change cluster resource settings, such as CPU, memory, or disk storage at any time through a simple API or the GDC console. GDC Database Service provides automatic backups that can be configured at any time, not just during database-cluster creation. Cluster-deletion protection prevents accidental deletion of databases. Point-in-time recovery lets you recover operations and database-cluster state at a specified timestamp. Database cloning allows efficient restorations for testing and tuning environments. Now that you've learned some of the features of the database service, we'll cover how to configure databases next.

### Video - [Configuring and managing databases](https://www.cloudskillsboost.google/course_templates/1198/video/522208)

* [YouTube: Configuring and managing databases](https://www.youtube.com/watch?v=6oH1QShhnJ0)

SPEAKER: In this lesson, you'll learn how to use some of GDC Database Service's enterprise features through common database tasks. You have several options to create a database cluster, including the gdcloud Command-Dash Line Interface, CLI, Kubernetes API, and Terraform. This code snippet shows how to create a database cluster using gdcloud CLI. You must manage database secrets with your application and deployment processes. GDC provides several connection options. You can connect within the same project as the database or connect from another project for cross-project access. You can also connect externally from outside of the GDC instance. All connections are SSL enabled. The Database Cluster Overview page shows the endpoint, hostname, port, and provides a link to download the CA certificates. Instances in your database cluster contain databases and provide capacities for the following. Computer-- example, one virtual central processing unit; memory-- example, one gigabyte; storage-- example, 100 gigabyte. When you want to connect to a PostgreSQL database cluster, the Database Cluster Overview page shows the endpoint, hostname, port, and provides a link to download the CA certificates. You can figure the path to the SSL Certificate Authority certificates. On the Connectivity page, you can select options such as the admin username, endpoint, port, whether to allow external connections, PostgreSQL, JDBC, and the certificate. Now let's create a user for the PostgreSQL database. After connecting to the database cluster, you get the same native database engine for administration. At this time, the GDC database does not support IAM integration for database accounts for accessing the database. You have to create the database users and passwords with your own automation. GDC provides scalability for database storage and performance, and you can increase storage sizing after its initial configuration. Understanding disk I/O performance helps you evaluate the effectiveness of database indexes. Within the Database Service, disk performance increases with disk size at a rate of 3 input/output operations per second per gigabyte. So if your database requires 3,000 input/output operations per second, you must provision 1 terabyte of space, even if your database is not that large. GDC Database Service provides database flags so that underlying database parameters can be tuned according to your workload's needs. For example, with PostgreSQL and database flags you can tune the number of allowed connections or enable logging of all database connections. You can set database flags using the GDC console or gdcloud CLI. GDC Database has features to enable redundancy and handle cluster failures. HA configuration reduces downtime when a database cluster instance becomes unavailable. This might happen when an instance runs out of memory. With HA, your data continues to be available to client applications. Within a GDC instance site, the configuration is made up of a primary instance and a standby replica. All writes made to the primary instance are replicated to the standby replica before a transaction is reported as committed. In the event of an instance failure, you can request that the standby replica become the new primary instance. Application traffic is then rerouted to the new primary instance. This process is called a failover. You can configure HA of a database cluster using the gdcloud CLI or the GDC console. Responding to incidents that impact the primary database cluster instance is important to ensuring application and workload availability. GDC provides the ability for teams to manually trigger a failover event to promote the standby instance to the primary. An important note is that the failover occurs within a GDC instance. This code snippet manually triggers a failover event to promote the standby instance to the primary instance using the gdcloud CLI. Operations and keeping your systems up to date are parts of the life cycle for production-grade enterprise platforms. GDC Database Service provides the capability to schedule automatic updates using maintenance windows. You create maintenance policies and bind them to a database cluster. These code snippets create and bind maintenance policies. Maintaining backups are an important part of operations, allowing for the recovery of data. Remember, GDC enables automatic backups that can be configured with the cluster's data protection configuration. You can also manually export a cluster's data. This code snippet performs a database export to S3 compatible storage using the gdcloud CLI. The service account for the database needs access to the bucket. You'll receive an error message in the GDC console if the permissions are incorrect. GDC Database Service lets you import data dump files. The database cluster must have access to the dump file, and there must be an active cluster to import the data into. This code snippet performs a database import of a data dump using the gdcloud CLI. You can also import data using the GDC console. Often during application development or database tuning, you must provision an exact copy of a database system to safely apply tuning or run load tests without impacting a production system. GDC Database Service provides database cloning that creates a new database cluster and allows for point-in-time recovery. Point-in-time recovery is used to restore the database state to a specific time. The code snippet clones a database cluster using the gdcloud CLI. Because GDC Database Service is fully integrated into the platform, it provides logs and dashboards for monitoring and process visualization. Therefore, you can expect permissions and IAM to determine what can be viewed and by whom. Some logs will be available to the application operator and/or platform administrator. There is a different set of logs available to infrastructure operations. All logs are available from Grafana, although the URLs will be different. Let's go over a few examples. This sample dashboard tracks the database uptime and this dashboard visualizes the database connections.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1198/video/522209)

* [YouTube: Module review](https://www.youtube.com/watch?v=ytRC5JU65ns)

SPEAKER: Let's review the key points from this module. In this module, we highlighted the security, scalability, storage management, integrated backups, and recovery features of the GDC Database service. These capabilities allow you to focus on your core product or service while reducing your infrastructure management effort. The GDC Database Service offers three options-- PostgreSQL, Oracle, and AlloyDB. We reviewed key security features such as encryption at rest and encrypted database connections. Fine grained predefined IAM roles govern all management operations, including backup configuration. GDC ensures database resilience by offering high availability with standby and primary instances. The GDC management interface provides multiple management options-- the GD Cloud CLI, Kubernetes API, and Terraform. Additionally, the service includes database observability with audit logging and monitoring dashboards. In conclusion, the GDC Database service delivers enterprise-grade security, manageability, and resiliency features, empowering your workloads with an air-gapped environment.

### Quiz - [Knowledge Check](https://www.cloudskillsboost.google/course_templates/1198/quizzes/522210)

#### Quiz 1.

> [!important]
> **Cymbal Federal's Mission CRM application team launches a new reporting feature that requires analyzing and summarizing tabular data. The team has few infrastructure engineers available and wants the most efficient solution to design and build this feature in GDC. What should they do?
**
>
> * [ ] Deploy a GDC PostgreSQL cluster.
> * [ ] Deploy MongoDB as a stateful set in GDC Kubernetes Service.
> * [ ] Deploy PostgreSQL as a stateful set in GDC Kubernetes Service.
> * [ ] Install PostgreSQL on a GDC VM.

#### Quiz 2.

> [!important]
> **Cymbal Federal's Document Archive app is launching a new database cluster for data processing. The team wants to plan for events in which the primary database fails. Which should they do?**
>
> * [ ] Configure a custom database flag to enable high availability (HA).
> * [ ] Deploy an additional database cluster as a hot standby.
> * [ ] Enable the HA configuration for the database cluster and deploy it.
> * [ ] Modify the application to cache data.

#### Quiz 3.

> [!important]
> **Your team at Cymbal Federal needs to move an application to GDC. The current application uses Oracle databases that have active multi-year licenses. You want the most cost-effective and efficient solution to move the app to GDC. What should you do?
**
>
> * [ ] Configure and deploy a GDC Oracle database cluster, and reuse the existing license.
> * [ ] Rewrite the application to use GDC PostgreSQL.
> * [ ] Deploy Oracle databases on GDC VMs, and reuse the existing license.
> * [ ] Deploy Oracle databases on GDC VMs, and buy new Oracle licenses.

## Networking

In this module, you'll learn about GDC networking features, which are based on Kubernetes. You'll delve into how to interconnect projects through network policies, or firewall rules. You'll also learn about DNS in GDC and how to implement load balancers to distribute workloadsIn this module, you'll learn about GDC networking features, which are based on Kubernetes. You'll delve into how to interconnect projects through network policies or firewall rules. You'll also learn about DNS in GDC and how to implement load balancers to distribute workloads

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1198/video/522211)

* [YouTube: Module overview](https://www.youtube.com/watch?v=PrXmUFcq8AA)

SPEAKER: Welcome to this module on networking. In this module, you'll learn about networking features of Google Distributed Cloud, GDC air-gapped, and how to set communication between projects and resources. This module begins with an introduction to GDC networking features, which are based on Kubernetes. You'll explore the options for interconnecting projects, load balancing, DNS, and firewall rules. Let's get started.

### Video - [GDC networking features](https://www.cloudskillsboost.google/course_templates/1198/video/522212)

* [YouTube: GDC networking features](https://www.youtube.com/watch?v=gRhKo3285tE)

SPEAKER: Let's start by exploring GDC networking features through the lens of Cymbal Federal. Cymbal Federal applications require the separation of duties, which is an industry and architecture best practice for workload operations. Cymbal Federal teams are composed of system administrators, database administrators, application developers, and DevOps engineers. Teams need to communicate across the app and database projects. Given the separation of duties among Cymbal Federal teams, the database and application for its document archive solution will be deployed in different projects. With GDC networking, Cymbal Federal can keep these projects separate but enable cross-project communication. Let's go over some key features of GDC networking. GDC networking features provide secure and efficient connectivity within the platform. GDC has the capability to use multi-tenant organizational architectures with hard isolation across organizations and logical isolation for project workloads. It provides private and external networking connectivity with in-built observability and audit logging. And you can configure DNS and load balancing for your workloads. The GDC network stack can be described as having two layers. Upper networking is the logical networking layer provided by Kubernetes and Anthos Networking. This delivers VM and pod connectivity, load balancers, and Network Address Translation, NAT. Lower networking refers to the underlying physical layer that provides the network connectivity protocol and packet forwarding. This is the Level 2, L2, and Level 3, L3, overlay. As a practitioner, you need to focus on the upper networking capabilities. Next, let's discuss a basic network layout. Within a GDC instance, there is logical isolation for the physical network. The logical network model used for organizations and projects is comparable to the shared virtual private cloud, VPC model, in Google Cloud. In the shared VPC model, the host project owns the networks in the VPC, which are then used by service projects. In GDC, the GDC organization itself acts as the host project, with the GDC projects acting like service projects in the shared VPC model. This network schematic takes a lower-level view of the physical network and how it relates to organizations within the same GDC instance. Notice that organizations have internal and external networks. The external network is reachable from outside the organization. Each organization in GDC has its own isolated virtual network, which is a flat IP address space. The internal network is only accessible from within the organization. Workloads are always assigned an IP address or addresses from the internal network of the organization. Inbound and outbound access is configured using a zero trust scenario. By default, no connectivity is granted, so connectivity must be explicitly configured. GDC isolates each organization at the network level from all other organizations. In order for workloads in one organization to communicate with another organization, all traffic must exit the organizational network boundary through firewall and Intrusion Protection System, or IPS, and do a hairpin turn back in again. Workloads in one organization don't have direct IP address connectivity to workloads in another organization. All workloads in the same organization do have direct IP address connectivity to one another. Using project network policies, you can control access between workloads in different projects in the organization. You now understand the basic architecture of a GDC network. Your network will be set up for you by infrastructure operations. This team will use the customer intake questionnaire your organization completed to design the architecture specific to your requirements. Next, let's go deeper into how to connect projects in your organization.

### Video - [Inconnecting projects](https://www.cloudskillsboost.google/course_templates/1198/video/522213)

* [YouTube: Inconnecting projects](https://www.youtube.com/watch?v=NU413EcMQiI)

SPEAKER: In this lesson, you'll learn about interconnecting projects. GDC Kubernetes clusters operate at the organization level. GDC projects function as formal Kubernetes namespaces, used to divide and organize cluster resources logically. GDC networks achieve logical isolation at the organization level through a standards-based overlay, Virtual eXtensible LAN, or VXLAN. Tenant traffic is tagged to a single Virtual Network Identifier, or VNI, which enables secure sharing of the physical infrastructure. Creating a Kubernetes namespace or project does not require explicit configuration of IP CIDR blocks or firewalls. Network isolation within Kubernetes relies on network policies. You configure connectivity between projects and within projects with the GDC custom resource type, a project network policy. In the GDC console, you configure network policies as firewall rules. They are the same. You can configure project communication and connectivity in GDC. By default, network cross-project connectivity is not allowed, so projects can't talk to each other. However, within a project namespace, all resources within that project can talk to each other. A hub-and-spoke pattern provides centralized connectivity to and from workload projects. A full mesh option gives all projects connectivity. Kubernetes network policies enable audit logging of network traffic, which is similar to VPC flow logs in Google Cloud. This addresses one of the often seen requirements and provides evidence that you are auditing traffic across workload or network boundaries. GDC has IP address base, CIDR notation, and port, TCP, UDP, and SCTP policy enforcement for org-external communications. When allowing connectivity from a project, configuration is not transitive or bidirectional, which means that you must configure two network policies for bidirectional connectivity between projects. This manifest file configures a project network policy that enables workloads in the PROJECT_1 project to permit connections from workloads in the PROJECT_2 project, as well as the return traffic for the same flows. The policy does not allow the GDC project named PROJECT_1 to connect to the GDC project named PROJECT_2. That connectivity requires an additional project network policy, which we'll review next. To finalize the bidirectional connectivity on the second project, you must set a policy to allow PROJECT_2 to receive ingress traffic from PROJECT_1, along with the return traffic for the same flows. Remember, this policy alone does not allow the GDC project named PROJECT_2 to connect to the GDC project named PROJECT_1. You must use both policies to permit bidirectional connectivity. An organization network policy defines the network access control for organization-level managed services, including the GDC console, gdcloud CLI, Key Management System, or KMS, GDC Object Storage, and Vertex AI services. You can define these access controls using the organization network policy resource from the networking API. You need both the org network policy admin and the org session admin IAM roles. The code snippet configures an organization network policy that restricts access to the GDC web console based on the network IP address range. You can expose VM-based workloads outside of the organization using the VM external access capability. You enable this capability for each VM. Each VM gets its own IP address from the external network IP address range of the organization. Enabling outbound traffic changes the IP address from workloads to an external IP address using Network Address Translation, or NAT, when connecting outside the organization. External access is also supported for containerized workloads.

### Video - [Load balancing](https://www.cloudskillsboost.google/course_templates/1198/video/522214)

* [YouTube: Load balancing](https://www.youtube.com/watch?v=egHcZYj4MM4)

SPEAKER: In this lesson, let's discuss load balancing. Configuring workloads in a load balancer is a common way to accommodate scalability and improved performance. Within GDC, you use standard Kubernetes resource types to implement load balancing. Load balancers allocate a stable virtual IP, or VIP, address that distributes traffic over a set of backend workloads. GDC load balancing supports pass through TCP/UDP load balancing, configuration for specific ports or all ports, and VMs or containers as backends. You can create an internal load balancer that exposes services to other clusters within the organization and an external load balancer that exposes services outside the GDC organization. Next, let's cover how to configure load balancers. This manifest file configures an internal load balancer, or ILB. An ILB service is never accessible from any endpoint outside of the organization, but ILB services are accessible within the same project from any cluster in the organization. An ILB is a standard Kubernetes resource, including the annotations that allow for vendor-specific integration configuration. Here, the load balancer integrates with Google and GKE. The changes are applied to the cluster after authentication using the kubectl CLI. This manifest file configures an external load balancer, or ELB. The configuration is similar to that of the ILB. You can omit the annotation to designate the service as external facing by default. This example retrieves the IP address of an external load balancer Kubernetes service. In this example, DNS records can be used to assign a host name to the IP address of the ELB for the mission CRM service. The output gives you the cluster IP address and external IP address.

### Video - [DNS](https://www.cloudskillsboost.google/course_templates/1198/video/522215)

* [YouTube: DNS](https://www.youtube.com/watch?v=kHLHzh9GtSw)

SPEAKER: In this lesson, let's discuss domain name system, DNS. DNS servers are primarily used for name resolution of GDC services, such as the console, and managed services such as AlloyDB Omni database service ODS. At the moment, end users who are platform administrators and application operators cannot create DNS registrations for name resolution. DNS deployments are added into the root and org admin cluster. There are three types of DNS servers deployed in each admin cluster. The external authoritative server contains the name to IP address mappings for service names, which can be accessed from both within and outside of an organization. The internal authoritative server contains the name to IP address mappings for internal-only services. The name resolution works only from within the org, and it will fail from outside of an org. The forwarder forwards DNS requests to an upstream resolver when the requests cannot be resolved within the org. Remember that GDC has an internal and external network to bolster its security posture. GDC uses split DNS services to resolve host names to IP addresses within its internal network and to host names that are externally accessible outside of the GDC instance.

### Video - [Adding firewall rules](https://www.cloudskillsboost.google/course_templates/1198/video/522216)

* [YouTube: Adding firewall rules](https://www.youtube.com/watch?v=kbNMSbxhCXo)

SPEAKER: Finally, let's discuss adding firewall rules in the console, which are the same as project network policies. To view all firewall rules across the project, the platform administrator clicks Firewall under the Networking section in the Main Navigation. You must have the org network-policy-admin role to view the firewall rules. There is one default user-created rule to allow intraproject communication. You can delete this rule if you do not want to allow it. Let's consider a scenario where an application operator wants to create a new firewall rule so that a service in one project can communicate with workloads in another project. This requires cross-project communication. From the Firewall page, click Create in the Action bar to begin creating a new firewall rule. The Firewall Rule Details page has the configuration of the firewall rule. Enter the name, direction of traffic, target, and other details. Firewall rules can control access to workloads, projects, and GDC-managed services, such as databases. Click Select Project to open the project selector modal. Then select one of the projects you can access to. Here, the project name support is selected to allow ingress traffic. After configuring the firewall, click Create to finalize and create the firewall rule. After you create a firewall rule, it is displayed in the table within the Firewall page. Here the firewall is configured to support Secure Shell, or SSH, TCP traffic from the customer-1-backend project. Now the application operator wants to create a new firewall rule so that a managed service in one project can communicate with workloads in another project. From the Firewall page, click Create in the Action bar to begin creating a new firewall rule for the service. The service dbs is preselected because it is currently the only available service. Select Service under the Target section of the firewall rule details. This indicates that the firewall rule will target a specific service within the scoped project. After you create the firewall rule, it will be displayed under User Created Rules on the Firewall page. This page gives the source, destination, and protocols to be governed for each firewall rule. Next, let's consider a scenario where an application operator wants to create a new firewall rule so that a service can communicate with workloads in another organization. This requires cross-organization communication. Click Create to begin the process to create a rule for an egress firewall. Select Egress under the Direction of Traffic section of the firewall rule to indicate that this firewall rule is controlling outbound traffic. Under Egress, specify the target and firewall configuration. After creating the egress firewall rule, you can verify it within the table on the Firewall page.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1198/video/522217)

* [YouTube: Module review](https://www.youtube.com/watch?v=1FXn39GebCs)

SPEAKER: Let's review the key points from this module. In this module, we discussed the GDC networking features that provide secure private and external connectivity for workloads. GDC networking employs standard-based logical networking isolation within the GDC physical network, ensuring secure multi-tenant networking across GDC organizations. You learned that connectivity within and across project workloads is managed by Kubernetes network policies, and load balancing within GDC is based on standard Kubernetes services. We also reviewed the GDC DNS architecture, which is designed to protect sensitive internal network information from being exposed externally. Finally, the module covered the management of firewall rules. These firewall rules are network policies with governance empowered by the open policy gatekeeper controller for Kubernetes.

### Quiz - [Knowledge Check](https://www.cloudskillsboost.google/course_templates/1198/quizzes/522218)

#### Quiz 1.

> [!important]
> **Cymbal Federal has special requirements for a new set of workloads that cannot share any hardware with any existing workloads. The team wants to meet compliance requirements and run these workloads in the GDC platform. What should they do?**
>
> * [ ] Deploy the new workloads in a new project in an existing organization.
> * [ ] Configure a new Kubernetes cluster, and add a new nodepool within the existing organization.
> * [ ] Create a new organization, and require separate hardware. Design project and resource hierarchy according to compliance regulations.
> * [ ] Deploy the workloads on separate VMs and Kubernetes clusters within the existing organization.

#### Quiz 2.

> [!important]
> **Cymbal Federal needs to move a data-driven application to the GDC. The application and its database have different deployment lifecycles and separate ownership that must be maintained. What should the team do?
**
>
> * [ ] Deploy both workloads in the same project and configure granular IAM roles for each team.
> * [ ] Deploy both workloads in different projects. Configure IAM and project network policies to allow connectivity from the application project to the database workload project.
> * [ ] Deploy both workloads in the same project on VMs and configure granular IAM roles for each team.
> * [ ] Deploy each workload in different projects and configure granular IAM roles for each team.

#### Quiz 3.

> [!important]
> **Cymbal Federal needs to adopt new security and compliance standards to control administrative access boundaries. The team wants to restrict access to the GDC console based on the network. What should they do?**
>
> * [ ] Configure firewalls to restrict access.
> * [ ] Configure an organization network policy to restrict access.
> * [ ] Raise a request to Infrastructure Operations to restrict access.
> * [ ] Configure a project network policy to restrict access.

## Key management and security

In this module, you'll explore the various layers that make up GDC platform security. The module begins with the requirements to maintain platform security, how to manage digital signatures, and the benefits of a key management system. Then, you'll learn about crypto key management options and the associated GDC personas. 

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1198/video/522219)

* [YouTube: Module overview](https://www.youtube.com/watch?v=yHvtHvLjmqw)

SPEAKER: Welcome to key management and security. In this module, you'll learn to recognize the various aspects of Google Distributed Cloud, GDC, air-gapped platform security. One of these is key management. You'll learn how to implement key management best practices. This module starts with the requirements to maintain GDC platform security, how to manage digital signatures, and the benefits of a key management system. We'll next cover crypto key management options and best practices and the associated GDC roles and personas. You'll also explore the use of crypto keys and how they are managed by Cymbal Federal in an example GDC deployment.

### Video - [Platform security overview](https://www.cloudskillsboost.google/course_templates/1198/video/522220)

* [YouTube: Platform security overview](https://www.youtube.com/watch?v=hXtw9aP_XNQ)

SPEAKER: Let's start by using Cymbal Federal's document archive solution to think about different aspects of GDC platform security. Cymbal Federal's document archive is a Speech-to-Text application suite that receives sensitive audio files, transcribes the files to text, and provides metadata indexing for cataloging and long-term archival storage. The Cymbal Federal team ingests the PDF documents. Some of the metadata being extracted from the PDFs will be encrypted to ensure the data handling meets regulatory control requirements, so they will implement the KMS service to manage encryption keys. To meet compliance requirements, the Cymbal Federal team needs to review all of the security features of the GDC platform, including those that impact disk and object storage. Let's go over the different areas of GDC security. The GDC platform is an enterprise grade compute platform designed to accommodate the most demanding security and compliance requirements for workloads. Its multilayered security approach is empowered by a collection of software and hardware capabilities, all designed to meet stringent security requirements. From the ground up, security is an intrinsic element within all GDC underlying components. You will explore the different security capabilities empowered by a well-architected GDC hierarchy, IAM, and the first class or native service offerings such as the KMS service, ranging from physical security to hardware security to logical isolation and security features. We'll also discuss the GDC native intrusion protection system, IPS, features. Let's begin with physical security. GDC instances are deployed in approved customer designated air-gapped data centers, so location specific physical security will vary based on individual customer requirements. If a GDC instance is deployed in Google-managed data centers, the data centers match the same stringent requirements for Google Public Cloud Data Centers. Security controls could include perimeter defense, cameras, access cards, and other security policies. Google places strong emphasis on security in its hardware, procurement and assembly processes. Google ensures security by carefully vetting and certifying partners to meet specific customer sovereignty requirements. This involves rigorous hardware testing to meet customer specific security needs. Google maintains close relationships with all vendors and teams using the hardware to ensure that top of the line security features are provided. Google sources all components and subcomponents within the hardware from trusted suppliers who have undergone vetting and are known for their reliability. Hardware is integrated into GDC racks at regional certified centers, and all personnel handling GDC hardware are background checked. Google provides chain of custody and country of origin reports if needed. Next, let's explore some of the logical security features. GDC personas are logical constructs that enable security through separation of duties. Infrastructure operations is responsible for the maintenance of the infrastructure. By default, they do not have access to users, organizations, and projects. The platform administrator manages an organization's configurations, policies, and projects. They typically have full access to all the projects within an organization. The application operator manages and monitors the applications and services running in a specific project. They typically have full access to a given project. Next, let's examine how the resource hierarchy plays a role within the GDC security landscape. GDC multi-tenancy involves the use of dedicated server hardware within an organization. There is no sharing of physical compute resources. This eliminates security concerns associated with data segregation, resource contention, management plane security, and compliance and regulatory concerns. Hence, you see the factors that improve or strengthen security due to the GDC resource hierarchy architecture. Next, let's discuss the security impact that GDC resource isolation provides. GDC employs hard isolation between organizations and provides physical hardware isolation to protect different organizations from each other. GDC provides a hardware security module for encryption key storage. Logical isolation at the project level, protects two teams at the same customer from each other. This gives customers the ability to enforce resource constraints. Using separate projects for separate teams is a recommended practice. This is a diagram of a multi-tenant GDC instance. Across organizations, hard multi-tenancy is enforced by dedicated hardware. Soft multi-tenancy, logical isolation, is enforced by projects. Notice that each organization has its own control plane. This diagram details hardware isolation segmentation across organizations. Within the GDC platform, organizations do not share compute hardware or servers for workloads. GDC networking security is implemented using a segmented control plane accessible by infrastructure operations and data plane networks accessible by platform administrators and application operators. Each organization has dedicated Virtual Extensible LAN, VXLAN, intrusion protection systems, and firewalls, and its own dedicated storage VMs. All external access is blocked by default. And all network traffic passes through firewalls and intrusion protection systems. GDC uses a secure network boot mechanism with hardened images that meets FIPS 140-2, DISA STIG evaluation, and IL5 and IL6 security requirements. Secure shell or SSH access for administration is managed with a custom certificate authority, CA in the root admin cluster, which provides unique host keys for each host and key rotation. All GDC data that is stored on persistent storage is encrypted at REST using multiple levels of encryption. All data is encrypted with tenant isolated keys that are protected by FIPS 140-2 level 3 hardware security modules, HSM. Drives are also self-encrypted with keys protected by the HSMs. Object storage also supports S3 SSE server side encryption, and SSE-C server side encryption with customer provided keys object encryption methods. All GDC traffic in transit is encrypted by default using FIPS 140-2 certified encryption and industry standard protocols such as TLS 1.2 plus, HTTPS, and IPsec tunnels. Based on the shared responsibility model, customers are also expected to use encrypted algorithms on the application layer to ensure that encryption requirements are met for all applications deployed on top of GDC. GDC provides a public key infrastructure, PKI, for convenient, tightly controlled trust distribution, and centralized X.509 CA management for infrastructure services. GDC can distribute trust stores centrally, which makes it easier to manage trust. GDC PKI has root certificates, which are stored offline, making them more secure compared to online root certificates. GDC allows cross signing of certificates, which allows for trust to be transferred between GDC and customer PKIs. GDC PKI supports name constrained cross signing, which allows for more granular control over which certificates are trusted between customer and GDC. GDC can also support installing customer-issued certificates at different end points, which allows for more flexibility in how certificates are used. The PKI infrastructure is provisioned by infrastructure operations while bootstrapping the GDC organization. Overall, GDC provides a powerful PKI solution that can be used to secure a variety of infrastructure services and allow convenient yet tightly controlled trust distribution. It is important to highlight the security capabilities introduced by Google's software supply chain with trusted, vetted software binaries, security operations powered by GDC services and best practices, and the platform's adherence to industry standards and international standards. This impacts security operations by leveraging IAM best practices, implementing role specific logging and monitoring capabilities, providing emergency remediation, supporting disaster recovery, and establishing security incident management response processes. Key Management Service, KMS, is a first party service based on Google Cloud platform cloud KMS, but built on Kubernetes. Customers can manage their own cryptographic keys using the Kubernetes API. KMS supports FIPS-approved algorithms and FIPS-validated modules. KMS provides multiple interfaces for administration, such as key management using the Kubernetes API and the data plane provided by the Kubernetes API and the gdcloud CLI. Key material stays within the KMS. Kubernetes Custom Resource Definitions, CRDs, hold encrypted material, and only the KMS has access to raw key material in memory. KMS management, permissions and operations are governed by GDC predefined roles. From a design perspective, there is one KMS per organization. Finally, because KMS is native service within the GDC platform, there is integrated auditing for key management activities.

### Video - [KMS keys](https://www.cloudskillsboost.google/course_templates/1198/video/522221)

* [YouTube: KMS keys](https://www.youtube.com/watch?v=aNcbnMh5ZCk)

SPEAKER: Now that you understand the broader security landscape in GDC, let's examine KMS keys in more detail. Symmetric encryption uses the same key for both encryption and decryption. This encryption method is suitable for bulk data encryption because it's simple, fast, and efficient. KMS supports storing keys for symmetric encryption. Typically, the application operator creates, uses, and destroys the keys in the KMS. This encryption key, AEADKey, is a Kubernetes CRD. And the associated encryption algorithm, AES256 is the default algorithm. This manifest file defines the encryption key named my-test-key. Notice that keys are defined within a namespace. Digital signatures use Public Key Infrastructure, PKI, which is considered the gold standard for digital identity authentication and encryption. Digital signatures are the digital equivalent of a handwritten signature or stamped seal, but offer far more security. The GDC KMS stores keys that support asymmetric encryption for digital signatures. This encryption key, SigningKey, is a CRD that defines the signing key named my-test-key. For customers having existing encryption keys and key management infrastructure, GDC has features to import keys. Existing GDC customers can import or export to move keys between KMS instances in a secure manner. Using import-export to securely move keys in and out of the KMS helps with cross-instance replication, key escrow and storage, and disaster recovery. GDC provides operational risk mitigation using separation of duties for data handlers managing the KMS. The platform administrator grants separate IAM-predefined roles, KMS key import admin and KMS key export admin, for the permissions to import and export keys. Broad grants of KMS administrative roles can introduce unnecessary risk and disrupt alignment to security and compliance requirements. Enforcing separation of duties using IAM is a best practice that enables alignment to regulations. It limits damage if credentials are compromised and reduces the risk of unauthorized access or misuse of cryptographic keys. Let's keep in mind the best practice of separating duties as we think about the relationship between GDC personas and KMS management activities. The platform administrator manages the Role Based Access Control, or RBAC, for KMS policies for the application operator and grants them the required IAM roles. The typical KMS user is the application operator. The platform administrator can perform root key rotation, which triggers all the keys in the org to be rewrapped by the new root key. Application operators can create and managed keys. They can choose to import or export keys. And with the KMS developer role, they can perform crypto operations with keys. Key rotation requires the KMS Rotation Job Admin role. Let's review the predefined KMS roles, starting with administrative roles. These GDC IAM predefined roles relate to KMS administration. The KMS Admin manages KMS keys in a project and imports and exports keys. The KMS Key Export Admin has access to export KMS keys as wrapped keys. The KMS Key Import Admin has access to import KMS keys as wrapped keys. These are less permissive predefined roles that developers and security reviewers would use. The KMS Creator role provides create and read access on keys. The KMS Developer has access to perform crypto operations using keys in projects. The KMS Viewer has read-only access and can view key import and export.

### Video - [Demo1 GDC CLI - KMS Symmetric Keys](https://www.cloudskillsboost.google/course_templates/1198/video/522222)

* [YouTube: Demo1 GDC CLI - KMS Symmetric Keys](https://www.youtube.com/watch?v=2jep263qOAI)

SPEAKER: This demonstration shows creating, managing, and using encryption keys with GDC. First, we need to authenticate with the GDC cluster. Logging in as my user. Once our user is logged in, we're going to establish a couple of environment variables, which will make it easier to go through this demonstration. First environment variable we're going to set is the cluster we're connecting to. We're connecting to the admin cluster for this organization. Next, we're specifying our kubeconfig file that we're going to be saving our credentials to. Then we're getting the credentials to the GDC cluster for the organization. Here we're listing out the projects that are in this organization. And then we are going to select the back end project. I'm also setting another environment variable for the username. This is just the first part of the username. It doesn't include the at domain portion. Before we can do anything else, need to check the roles that are associated with this user. This account has the kms-admin, kms-creator, and kms-developer roles that are required for this demonstration. The key is going to be called symmetrickey1. We're then going to use a kubectl command with some YAML. We're using the version 1 of the KMS API. We're creating an AEADKey, specifying our key name, specifying our project, and then specifying AES_256 as the algorithm. Here we see our key has been created. To view the key that is in this project, we can use the kubectl, kubeconfig, specify the AEADKey, and then the key name and the project namespace. Here we see that the key is working. Next, I'm going to create a file with text that we can encrypt. Just cadding some stuff out to a file. And then we can look at the contents of that file. And that's what our file looks like. That's what's going to be encrypted. We're going to set some additional environment variables. First, we're specifying our plaintext file, which is the file that we just created. Then we're going to specify a ciphertext file, which is where the output-encrypted information will go. To encrypt the file, we're going to use the gdcloud kms keys encrypt command. We're going to specify the namespace, including our project, the type of key that we're using, and the key name. Next, we specify the plaintext file and then what we want the output ciphertext file name to be. Once we hit Enter, it retrieves the key, retrieves the plaintext file, and then goes through the encryption process. Here we see we now have a plaintext file that is encrypted. Now that we have an encrypted ciphertext file, we can then decrypt it using our key. So we're going to specify a new file, named text.out, which will be our output plaintext. Another gdcloud command-- gdcloud kms keys decrypt. Specifying our namespace, including our project. Again, the key type, the name of the key, specifying the ciphertext file, and then what we want the output plaintext file to be named. Now, we see that that's been decrypted. We can then go through and list the contents of our text.out file. And we see that we have decrypted the file. To list out the keys in a project, we can use kubectl, specifying our Kubernetes configuration file, specifying the type of key that we want to retrieve, and specifying the namespace. Here we're listing out all of the names in the back end project. The key that we recently created is down here, the symmetrickey1. In order to delete that key, we'll use another kubectl command. This time, we're specifying delete, specifying the type of key, specifying our key name, and then, lastly, specifying the project that that key is associated with. Now that we see that that key has been deleted, we can rerun our previous command to list out the keys in the project. And we see that the key is now gone.

### Video - [Demo2 GDC CLI - KMS Asymmetric Keys](https://www.cloudskillsboost.google/course_templates/1198/video/522223)

* [YouTube: Demo2 GDC CLI - KMS Asymmetric Keys](https://www.youtube.com/watch?v=70S34OV_mxY)

SPEAKER: Our next demonstration shows using asymmetric cryptography with GDC. The first step is to log in to GDC itself. So here we're doing gdcloud auth login. This pops up the browser. We log in with our application operator user. Once the user is logged in, we're going to specify some environment variables. First one is the account that we're using. Next is the cluster that we connect to. Here we're connecting to the organization admin cluster, specifying a kubeconfig environment variable, and then getting the Kubernetes token. Now that we have the Kubernetes token, I'm going to list out the projects within our organization. The organization that we're going to be using is backend. So I'm setting a project variable to backend. Next is to view the roles that are assigned to my account. We're specifying the project that we're checking, the account name, and then we're doing a search specifically for the roles that have kms in them. The first three that we see here are the ones that we're interested in. We're interested in kms-admin, which allows us to manage and delete keys. Next is kms-creator, which allows us to create keys. And then is kms-developer. And developer allows us to use the keys. Next, we're setting an environment variable called GDC_PUBLIC_KEY. So that's going to be the name of the key that we create. And then here, we're actually using a Kubernetes API call. We're going to create a Signingkey. We're going to name it with our public key name. We're attaching it to the project that we specified. And then we're using a SHA384 algorithm. The next command looks for our Signingkey that we had just created within the GDC project and checks its status. Here we see that the key is ready for use. Another environment variable is being set here. This will be the public key that gets exported from GDC. This kubectl command gets our Signingkey with our public key name from our project, puts the output to JSON, does the JSON query, looking for status.publicKey, and then takes that output, Base64 decodes it, and saves it to this file name. So that is exporting our public key. Saving another environment variable to a plaintext file, and next, an environment variable which will create a signature file, or this is what the signature file will be named when it gets output. This command uses the gdcloud asymmetric signing call specifying our project, our Signingkey, our plaintext input file, and then the output signature file for the signed portion. Next, we're using the OpenSSL command, using the digest function to validate using our public key, SHA384 algorithm with the signature file that we had just created, and then our plaintext file. Should come back with verified. To see what would happen if the file had been tampered with, I'm going to make one little change, inserting a period into the plaintext file. Now, I'm going to rerun the validation with the OpenSSL. And we see that the verification failed because that file has been changed. It no longer matches the signature. To delete a key, use kubectl, specify your Kubernetes configuration file with the delete command, specifying the type of key, and then the name of our key and the name of the project. See the key was deleted. And then to validate that we can use the kubectl command, specifying to get Signingkeys in our GDC project namespace. And here we see there are no keys currently in that namespace.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1198/video/522224)

* [YouTube: Module review](https://www.youtube.com/watch?v=Uy71StPYguQ)

SPEAKER: Let's review the key points from this module. GDC employs a comprehensive, layered approach to ensure security at all of its components, hardware, networking, its services, and operational management. Hardware security modules manage encryption keys that secure and encrypt all data at rest. Secure boot loading maintains the integrity of compute systems by ensuring that only authorized code executes during stages of the boot process. Networking security is governed by standards-based logical network isolation empowered by virtual-network technology. This ensures secure multi-tenant network communications. Public key infrastructure included within the GDC platform allows it to serve as a private certificate authority that establishes and manages identities for use in all encrypted-data transmissions. GDC application delivery and packing of the platform is governed by industry-standard software supply-chain practices. GDC operational-management activities are governed by granular IAM roles that define the permission for administrative and management tasks. Finally, we explored the features of the key management service, which stores encryption keys. All key management activities are governed by granular, least-privileged, predefined GDC roles. This layered approach and inbuilt key management allow the GDC to meet stringent workload security requirements.

### Quiz - [Knowledge Check](https://www.cloudskillsboost.google/course_templates/1198/quizzes/522225)

#### Quiz 1.

> [!important]
> **Cymbal Federal's success to date is driving further GDC adoption within its organization. A new team wants to understand its security responsibility with Google as the Infrastructure Operations provider. Which describes the GDC security shared responsibility model?**
>
> * [ ] Customers are only responsible for application code. Google will address all other security responsibilities.
> * [ ] Google handles all application and infrastructure security.
> * [ ] Customers handle all application-related security and provide timely infrastructure updates while informing Google.
> * [ ] Customers are responsible for base images, application containers, dependencies, and application code. Google is responsible for firmware, hardware, networking, and physical security.

#### Quiz 2.

> [!important]
> **Cymbal Federal needs to move an application to GDC. The team is seeking to sunset its current on-premises secrets management system that stores encryption keys. The team wants to explore GDC features. What relevant features does the KMS service provide? (Choose two.)
**
>
> * [ ] Storing key-value pair metadata.
> * [ ] Encrypting GDC persistent disks.
> * [ ] Providing SSL encryption for communications.
> * [ ] Signing digital documents.
> * [ ] Encrypting and decrypting data using its cipher capabilities.

#### Quiz 3.

> [!important]
> **Cymbal Federal's Document Archive team is adding additional security features to its document metadata batch processes. The team wants to encrypt and decrypt database passwords within its processing, which is managed using shell scripts. What should the team do?**
>
> * [ ] Enable the KMS service to manage keys for the entire organization, and enhance the current batch processes.
> * [ ] Adopt KMS, and create a KMS instance for each database cluster. Enhance the current batch processes.
> * [ ] Create custom logic to store encryption keys in object storage, and enhance the current batch processes.
> * [ ] Adopt KMS, and create a KMS instance for each workload. Enhance the current batch processes.

## Review: Compute, network, and storage services configuration in GDC

Review course content


### Video - [Course review](https://www.cloudskillsboost.google/course_templates/1198/video/522226)

* [YouTube: Course review](https://www.youtube.com/watch?v=8uK2Plr2_bw)

SPEAKER: Thank you for taking the compute network and storage services configuration in GDC course. Hopefully, you now have a better understanding of how to configure services for the GDC platform and how it uses Kubernetes architecture and components. You should recognize the benefit of working declaratively in deploying workloads in managing the Kubernetes environment. You should be familiar with GDC Object Storage and how to create and enable access to storage buckets; the GDC Database Service, and how to provision and work with a database cluster in this managed Database Service; and the networking features of GDC that let you isolate and interconnect workloads depending on your project needs. You should also recognize the requirements to maintain GDC platform security and the use and management of crypto keys. Next, Google recommends enrolling in the AI Services and GDC Deployments and Operations course of the GDC practitioner fundamentals series. In that course, you learn how to use the GDC Vertex AI pre-trained APIs and explore how to manage operations in your GDC environment.

### Document - [Course resource links](https://www.cloudskillsboost.google/course_templates/1198/documents/522227)

### Document - [Additional resources](https://www.cloudskillsboost.google/course_templates/1198/documents/522228)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
