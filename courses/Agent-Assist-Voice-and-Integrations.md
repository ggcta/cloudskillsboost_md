---
id: 1161
name: 'Agent Assist Voice and Integrations'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1161
date_published: 2024-09-20
topics:
  - Agent Assist
---

# [Agent Assist Voice and Integrations](https://www.cloudskillsboost.google/course_templates/1161)

**Description:**

In this course you will learn how Contact Center AI Agent Assist can enhance the productivity of human agents while interacting with customers through the Voice channel, as well as the options available for integration with other platforms in the CCAI ecosystem.

**Objectives:**

* Learn the key capabilities of live transcription and how to configure it.
* Understand GKA and its configuration.
* Understand LLM baseline summarization and its implementation.
* Learn how to identify conversation profiles and data ingestion setups.
* Learn how to interpret solutions to integrate telephony for agent assist.
* Understand the UI integration module and its components.
* Learn how to evaluate agent assist and platform in CCAIP.
* Learn how to utilize Dialogflow to complete an agent setup.
* Understand how to Analyze Dialogflow runtime integrations.
* Learn how to identify the various phases of the delivery lifecycle.

## Live Transcription

After completing this module, you will be able to understand the key capabilities of live transcription for Agent Assist and its configuration process.

### Video - [Live transcription](https://www.cloudskillsboost.google/course_templates/1161/video/509148)

* [YouTube: Live transcription](https://www.youtube.com/watch?v=_T0WX77ML14)

Welcome “Agent Assist Voice and Integrations”. This course builds on top of the “Introduction to Agent Assist & its Gen AI capabilities” module of the CCAI Academy by covering all about Agent Assist Voice features, and the options you have available to integrate with other platforms in the CCAI ecosystem. Please note some sections in this training require you to have a basic understanding of the python programming language and Jupyter notebooks We'll start by covering various Agent Assist features related to voice like live transcription, Generative Knowledge Assist, or GKA, and Large Language Model, or LLM,baseline summarization. We will then go over telephony integrations, as well as integrations with the Agent Assist UI We'll also learn how to integrate Agent Assist with Contact Center AI Platform, a Dialogflow Virtual Agent, and Insights Finally, we'll discuss the best practices for the delivery lifecycle of Agent Assist implementations. Let's explore the cutting-edge features of Agent Assist that sets it apart. Agent Assist Live Transcription is a feature which enables conversation transcripts to be visible to the agent in real time. The objective of this section is to explain the key capabilities of live transcription and understand how to best configure it. The Live Transcription feature transcribes every word of the interaction between the customer and the agent in real-time It can automatically redact personally identifiable information for privacy compliance. The demo on the right shows the transcription of the ongoing call conversation between the agent and the customer. As you can see the agent is able to see the live transcriptions of the voice conversation in real time for better understanding of the context which in turn, heps them providing better support. Enabling Live transcription requires minimal configuration. A Telephony integration between the Customer's Telephony provider and Dialogflow CX is required in order to stream voice conversations to Dialogflow. Once the setup is complete; the conversations will flow into Dialogflow CX, transcribed automatically and displayed on the front end using the UI Integrations and individual web components. We are going to discuss the details in the UI integration section later in the course The Security Settings of the conversation profile governs how the data is redacted. The configuration for the same can be done either using the default options or by creating a custom "inspection and de-identification template" for Cloud DLP as shown in the screenshot here.

### Quiz - [Live Transcription Quiz](https://www.cloudskillsboost.google/course_templates/1161/quizzes/509149)

## Generative Knowledge Assist (GKA)

After completing this module, you will be able to understand the key capabilities of Generative Knowledge Assist (GKA) and its configuration process.

### Video - [Introduction to Generative Knowledge Assist](https://www.cloudskillsboost.google/course_templates/1161/video/509150)

* [YouTube: Introduction to Generative Knowledge Assist](https://www.youtube.com/watch?v=8LNNNPteqIc)

In this section, we’ll focus on the Generative Knowledge Assist. We’ll provide you with a high level understanding of Generative Knowledge Assist (also called as G-K-A) to then explain how to integrate it with a GenAI Agent (also known as data store) The prerequisites to best consume the content in this section include: The prior knowledge of data store agents (see a link to the CCAI Academy GenAI Agents training module in the additional resources). And the availability of data that the generative AI agent can use to answer questions. Both public and private data stores are acceptable as long as they contain the relevant information that can solve the customer’s request. The objectives of Generative Knowledge Assist is to analyze a live conversation between a human agent and an end-user and suggest relevant knowledge documents or FAQ answers to the agent to best answer the customer’s queries. For information on how to create a knowledge base in Agent Assist, please refer to the resource 34 in the additional resources document. Next, let's delve into the best practices that can help optimize this process. The quality of the content is the backbone of any knowledge document. However, not all content adds value, especially when it comes to quick, effective agent support. That’s why it's essential to keep our documents concise and relevant. The 'Article Suggestion' tool presents the initial sentences of a document as snippets. These snippets are designed to provide agents with a quick understanding of the document's content. This allows the agents to quickly use the opening lines as suggestions. Imagine being an agent and the snippet you receive is only about the last-modified date or a navigation bar. Not only is this unhelpful, but it can also mislead agents and hinder their ability to provide quick resolutions to end-users. So to summarise, always start your knowledge documents with the most vital information that can aid agents. Ensure that the beginning is free from distractions and is directly related to the document's core subject. When considering document format, it's also important to ensure compatibility with the support tools at your disposal and optimizing the Document Format. Firstly, remember that 'Article Suggestion' and 'FAQ Assist' focus on text. So, documents heavy on audio, video, or images aren't ideal. For lengthy documents, especially those surpassing 1000 words, consider breaking them down. This improves suggestion quality and eases the agent's task in locating the required information. A well-structured document ensures both efficient tool processing and ease of use for your agents. A crucial aspect of a reliable knowledge base is also the relevance and usefulness of its content. So tailor your knowledge base according to the objectives of your customer channels. For example, for tier-2 technical support, prioritize technical content often referred by the agents over sales content. Maintaining a knowledge base that’s up-to-date can be a lot of work. So try to be proactive and create a process for decluttering it from documents that are outdated, rarely accessed, or inactive. These don't add value and may even hinder the agent's process. Now that you have a good understanding of what a good knowledge base looks like, let’s explore the 'Generative Knowledge Assist' feature. Consider this example: A customer mentions they're a Home Internet user and inquires about discounts to save money on their account. The agent's original query is simply 'Discount'. Even with little context, the Generative Knowledge Assist steps in and transforms this query into a more specific one: 'What discounts are available for Home Internet customers?' This rewritten query is not just more precise; it's tailored to the customer’s specific context, greatly enhancing the relevance and accuracy of the response. This feature works hand-in-hand with a data store agent, which links agents' questions to the conversation context, providing generative answers and citations. It can even support a private Knowledge Base and utilizes conversation context as user metadata. In Generative Knowledge Assist understands and rephrases queries leading to more meaningful and targeted assistance. Knowledge documents play a vital role in the efficiency of our Agent Assist tools. Whether it's website links for suggesting content, or FAQ documents for assisting with common queries, having a well-organized knowledge base can greatly enhance the user and agent experience.

### Video - [Generative Knowledge Assist (GKA) configuration](https://www.cloudskillsboost.google/course_templates/1161/video/509151)

* [YouTube: Generative Knowledge Assist (GKA) configuration](https://www.youtube.com/watch?v=gc_Hd2BQrTI)

Now that we have mentioned the role that Generative Knowledge Assist plays in Agent Assist, let's walk through the process for it’s integration with Conversation Profile. First go to Agent Assist Console. Select the desired project and then go to“Conversation profiles”. Then click on “Create” to create a new conversation profile. Then, check "Generative knowledge assist" From the dropdown, select the configured GenAI agent and save the conversation profile. Creating the Knowledge Base in Vertex Conversation is out of the context of this value pack.

### Quiz - [Generative Knowledge Assist Quiz](https://www.cloudskillsboost.google/course_templates/1161/quizzes/509152)

## LLM Baseline Summarization

After completing this module, you will be able to understand the key capabilities of LLM baseline summarization for Agent Assist and its configuration process.

### Video - [Introduction to LLM Baseline Summarization](https://www.cloudskillsboost.google/course_templates/1161/video/509153)

* [YouTube: Introduction to LLM Baseline Summarization](https://www.youtube.com/watch?v=W30yQfdSKGw)

Next, we’ll focus on the Large Language Models, or LLM, Baseline Summarization. This features enables us to summarize the conversation to gather useful information using Google Cloud’s powerful Large Language Models. Let’s get started. In this section we’ll introduce LLM Baseline Summarization and review the requirements and key steps for its implementation. Let’s start with the introduction to LLM Baseline Summarization. LLM Baseline Summarization is an AI-powered Agent Assist feature that automatically summarizes customer conversations, both the ones occurred prior to the current agent escalation, with another live agent or a bot, as well as handled conversations after they ended by providing a summary of the interaction. There are two main ways to summarize conversations: using a pre-trained LLM baseline summarization model or a custom model trained on customer data. In this section, we will discuss the pre-trained LLM baseline summarization model because the custom one is still in preview as of February 2024. LLM Summarization is also referred to as the V2 Summarization baseline model. Regardless of naming, this feature model enables voice and chat agents to customize the content of the summary by selecting from predefined sections such as situation, action and outcome. While this feature is designed to support both Voice and Chat data. In this course, we will only be focusing on voice data only. The out-of-the-box model will generate summaries for the following predefined sections: First is Situation: The Situation section highlights the core of the customer's query or concern. It captures the essence of what they're reaching out for. The other is Action which details the steps or measures taken by the agent to assist the customer. This section provides insights into the operational responsiveness of the model. Resolution indicates the final result of the conversation. In other words, was the query resolved? The result can be Yes, No, Partially resolved or Not applicable. Next, Customer Satisfaction is a crucial metric for any customer centric business. Simply put, it's a gauge on how the customer felt by the end of the interaction. Were they satisfied or not? There’s also Reason for Cancellation: Should the discussion veer towards service cancellation, this metric is vital for businesses aiming to reduce churn and understand pain points in the customer journey. The Entities section extracts the crucial key-value pairs from the conversation These could range from product names and time frames, to specific issues. It offers a concise snapshot of the conversation's content. It’s imperative to maintain high-quality summaries, which is why the model has a safeguard in place. Any summary that doesn’t meet a quality benchmark is deleted, resulting in an empty return. This ensures that the summaries output of the model are always relevant and useful. At the heart of this feature is the benefit it offers to our customer service agents and effective support. Imagine handling countless customer requests daily. Having a concise, yet informative summary of prior interactions (whether with another agent or a bot) can drastically improve the quality of the service and resolution time. Also at the end of an interaction, the system doesn't just leave the agents to draft summaries from scratch. Instead, it proposes a draft. This isn't just a time-saver but also ensures that the summaries are consistent in format and quality. The end result? Agents can focus more on solving issues and less on administrative tasks, amplifying productivity. LLM Baseline Summarization isn't just for human-to-human interactions. It's equally proficient at summarizing bot-to-human dialogues. The results are transformative for contact center operations. On average we observed a reduction of average summarization time from 90 seconds to 9 seconds. The throughput for unedited summaries is ten times faster. Observed results from 80 to 100 seconds were reduced to 6 to 10 seconds Also, for edited summaries we observed that throughput was three times faster as well The solution not only helps improve productivity and effectiveness, but also regulates the operational costs of the contact center.

### Quiz - [LLM Baseline Summarization Quiz](https://www.cloudskillsboost.google/course_templates/1161/quizzes/509154)

## LLM Baseline Summarization Implementation

This module explores in detail the implementation steps required to enable LLM Summarization for Agent Assist, inclusive of the prerequisites for Data ingestion, the creation of a conversation profile and the process to analyze and evaluate LLM Summarization configurations.

### Video - [Prerequisites](https://www.cloudskillsboost.google/course_templates/1161/video/509155)

* [YouTube: Prerequisites](https://www.youtube.com/watch?v=FqsSzbRWdgs)

In this section will work through LLM Baseline Summarization implementation. A number of steps, including APIs setup, data ingestion, creation of a conversation profile and feature configuration etc are required. Further, we will close this section also with some guidelines on how to best evaluate the model’s performance. There’s much to cover so let’s get started! The first step is to enable APIs which are going to be used for LLM Summarization. We need to enable several APIs to allow LLM Summarization and functionality discussed to work. Let’s take a few moments to explore these APIs in more detail. First API required is Cloud Data Loss Prevention which is paramount ffor businesses today. It identifies, monitors, and protects sensitive information across your datasets, ensuring that confidentiality is always maintained. The Cloud Speech-to-Text API is also very useful as it converts audio to text by applying Google’s Speech-To- Text Models. Next is Cloud Storage API. Having a secure, scalable, and durable storage infrastructure is non-negotiable. This tool ensures your conversations remains accessible and protected, regardless of the volume. Cloud Dialogflow API relates to natural language processing and understanding in Dialogflow. This tool empowers developers to generate summaries from audio transcripts. For more information, refer to the resources in the Additional resources document.

### Video - [Data ingestion setup](https://www.cloudskillsboost.google/course_templates/1161/video/509156)

* [YouTube: Data ingestion setup](https://www.youtube.com/watch?v=a5sJ0EpNC0o)

after you enable the apis you can start the data ingestion process this process is applicable for bulk summarization and evaluation therefore only for the conversations not in agent assist for the conversations already available in agent assist this step is not required and can be avoided as the conversations are already there to do so first create the cloud storage bucket required to host your data if this is not already done navigate to the cloud storage console and click on the create icon in the bucket section provide a globally unique name to the bucket then select a region ensure that the same region or sub region is selected for the rest of the steps in the pipeline select standard storage class and then click on create when working with GCS organizing and uploading audio recordings is straightforward just create a folder in GCS bucket and upload call audio recordings also make sure to collect metadata related to the audio recordings such as sample rate number of channels Etc which will be used in the following sections for transcription required metadata is covered in detail in the following sections now let's see these configuration steps in action in a demo more specifically here we will cover the permissions and apis required for us to get started we will then focus on the steps to ingest the audio files to cloud and transcribing them to text format which then can be used as an input for some summarization we will also dive into details of the format of audio files the parameters that are required for the transcription and the procedural steps to verify the output of the transcription to access the notebook referenced in the demo please refer to Resource number 58 hosted in the additional resources document hi my name is Omar and I'm A Cloud AI engineer with Google today I'll be demoing agent assist llm Baseline summarization before we get started make sure that you have a billing project created and ensure to enable apis like cloud storage API Cloud speech to text API Cloud DLP API dialog flow API also make sure you have proper I am permissions to access CCI insights cloud storage bucket Cloud DLP and Cloud speech to text so what is llm Baseline summarization it is an AI powered agent assist feature that automatically summarizes customer conversations summarization of conversations can be done either using a pre-trained llm baseline summarization model or a custom model trained on customer data in this demo we will discuss pre-trained llm Baseline summarization model llm summarization is also known as V2 summarization Baseline model and supports voice and chat data the key benefits of summarization are that it helps agents understand customers request and previous conversation with call summary and suggest summary draft at the end of a conversation this in turn helps agents to write better formatted summary with much less effort as a next step we will start by loading data navigate to Google cloud storage and create a bucket if not already created make sure that the region of the bucket Remains the Same as other resources to avoid any cross region data transfer once the bucket is created load the audio files into the bucket to get more information on how to load the files into the bucket visit this link also to understand more about supported file formats visit this link while loading the files it is good to note the detail of the audio file like the channel the encoding of the file the file format and the signal frequency all of this information will be required later while invoking the summarization great now that we have our source conversations in the GCS bucket the next step is to convert the audio conversations into a format which can be given to summarization first create a folder inside our bucket to store the output files next navigate to The Script which will be used to convert our audio files to transcripts in this demo we will be converting them to transcripts in the JS n format this script will need us to have some configuration values handy which the script will use starting with the project name the input bucket URI name of the folder in the bucket which holds the audio files name of the folder which will store the transcribed output files we will also need configuration values with respect to audio files which we had noted earlier like number of channels in the file valid values for which are dependent on the encoding of the file the language code which specified which language has been used in the audio files this has to be provided in BCP 47 format the sample rate Herz which is the number of samples of audio carried per second valid values for which are between 8,000 to 48,000 the encoding of the audio file the minimum and maximum speaker count in the file currently the valid value supported for this configuration is between 1 and two for more information on the configuration visit this link once we have the configuration in place navigate to the utility function section of the script focus on redact pii using Google DLP section this code will be invoked during transcri describing audio files to redact the pii information to get more information on redaction of pii data using DLP please visit this link Now navigate to the transcribe audio file section of the script make sure to run all the boil plate sections of the file Above This section now run the code in this section this will start a long running job that will start transcribing the audio files once the script finishes running navigate to the configured output folder in GCS bucket verify that all the audio files have been successfully transcribed to Json text download one of the Json files and open it in the editor of your choice the content of the file should look like the one on your screen the transcript ID is generally the file name the position is the relative position in the conversation the actor specified who uttered the particular sentence the speaker specifies if the speaker ID is one or two and the utterance will give us the sentence that was uttered each object will represent one utterance by a speaker in the conversation and the entire Json will contain multiple such objects for each utterance in the conversation in a sequential manner to summarize we saw how to load audio files to the cloud and then transcribe audio files to conversations using Cloud speech to text next we will see how to generate summarization from the transcribed conversations

### Video - [Create a conversation profile](https://www.cloudskillsboost.google/course_templates/1161/video/509157)

* [YouTube: Create a conversation profile](https://www.youtube.com/watch?v=9-zGsrzjhyk)

After the data has been transcribed, you can create a conversation profile now or use an existing one. Enabling LLM 4 00:00:07,519 --> 00:00:09,760 Summarization, also known as V2 Summarization Baseline, means creating a model (Baseline model version 2.0) in the Agent Assist Console. To do so, click on the Summarization tab, write the name for the model and click start. Next, Select the LLM Summarization and proceed to next steps. You can tailor the model to your needs by choosing sections such as Situation, Action, and Customer Satisfaction, among others. Ensure you associate or integrate a new or an existing conversation profile when setting up your summarization model. A detailed guide is available. Please refer to the resource in the Additional resources document.

### Video - [LLM Baseline Summarization Configuration](https://www.cloudskillsboost.google/course_templates/1161/video/509158)

* [YouTube: LLM Baseline Summarization Configuration](https://www.youtube.com/watch?v=KCUkacjQNdA)

After you have your APIs setup and  you have ingested your data and created a conversation profile, you can configure your LLM Baseline Summarization feature in Agent Assist. Let’s see how we can do it. Please note that this section requires you to have a basic understanding of the python programming language and Jupyter notebooks The Jupyter notebook required to run end to end baseline summarization, can be found at resource #09 hosted in the Additional resources document. This section goes through the key configurable parameters in the notebook as shown in the screenshot. Configuration is a crucial first step to ensure that your tools function correctly in the GCP environment. Make sure each parameter aligns with your intended use and setup In your Jupyter notebook start by defining the PROJECT NAME, which iidentifies your G-C-P project. The GCS BUCKET URI represents the root path of your Google Cloud Storage bucket. Audio underscore FILE underscore INPUT underscore FOLDER underscore PREFIX is the folder within your G-C-S bucket where you'll upload audio files. This is different from SUMMARIZATION underscore OUTPUT underscore FOLDER underscore prefix which is the one designated for storing the summarized outputs from your conversations. Remember also to specify the audio type NUM underscore CHANNELS. A value of '1' denotes single-channel audio, while '2' indicates dual-channel. At present, we only support these two 39 00:01:31,720 --> 00:01:34,439 values. For specifics on the implications of each choice, please consult the links provided in the additional resources document. Next up, we have specific parameters that guide the transcription process based on the nature and source of your audio. The LANGUAGE underscore CODE defines the language present in your audio files. For our current setup, we primarily support en-US for English in the United States and fr-FR for French. Both these codes are compatible with our Google Speech To Text and Agent Assist Summarization APIs. The choice of MODEL directly impacts transcription accuracy. In next slide, we talk about different types of models that can be used while configuring this notebook. Let’s explore a brief rundown of the different MODELS available: latest_long: Best suited for extended content like interviews or lengthyy conversations. phone_call: Designed for audio coming from phone calls. Such recordings usually have an 8 kHz sampling rate. default: A versatile choice, ideal for varied audio types, especially those recorded at 16 kHz or a higher sampling rate. medical_conversation Specialized for interactions between medical professionals and patients. Each Aaudio type has its nuances, so it's essential to match your content with the most appropriate model. This ensures accurate and meaningful transcriptions. Correct configuration ensures our tool understands the nature of your data well and by specifying the right language and model, you're optimizing for the highest accuracy and relevance. In addition to the mentioned basic settings you also have access to advanced configuration settings to optimize the quality of our audio transcription SAMPLE underscore RATE underscore HZ indicates the frequency at which your audio files have been sampled. you can select values ranging from 8000 to 48000 Hertz using the provided slider. The sample rate affects the clarity and richness of the audio data, so ensure it's set appropriately for your files. The ENCODING parameter represents how your audio data is stored in the file. For best recognition results, it's advised to use lossless encoding methods such as FLAC or LINEAR16. Be cautious when using lossy codecs, especially in noisy environments. They might compromise recognition accuracy. Examples of these codecs are MP3, OGG underscore OPUS, and AMR, among others. Always select the encoding that matches your audio files from the dropdown menu. MIN underscore SPEAKER underscore COUNT allows the system to identify individual speakers in a conversation. If you're transcribing a dialogue between two participants, like an agent and a customer, set this value to 2. Presently, we support only single or dual speaker configurations. Correct configuration guarantees that the audio transcription system interprets your files accurately, providing reliable and meaningful output. Let's explore a couple more parameters that are crucial for optimal transcription and fine-tuning of audio configurations MAX underscore SPEAKER underscore COUNT helps in distinguishing the voices in multi-speaker audio. As of now, our system can distinguish between one or two speakers. For example, if you have an 122 00:04:58,759 --> 00:05:00,720 audio clip with both an agent and a customer speaking, set the MAX SPEAKER COUNT to 2 to ensure both voices are recognized separately. This is helpful in isolating audio streams in the recording file. The CONV_PROFILE_ID represents a unique identifier linked to a specific conversation profile in Agent Assist. It's an auto-generated value that takes the form of: projects/< project_name> /locations/  /conversationProfiles /< conversation_id>. This ID is essential as it's used to correlate your audio files with the correct profile, ensuring tailored transcription based on the specified settings from that profile. It's essential to provide accurate configurations for every audio transcription task. Misconfigurations can lead to incorrect transcriptions, so always double-check your parameters. As you go through your configurations, it is important to be aware of a unique but optional aspect: Fine-tuning the Cloud Data Loss Prevention, or DLP, Redaction. By default, our notebook utilizes a set of predefined INFO underscore TYPES. These are patterns or types that our Cloud DLP system uses to identify and redact potentially sensitive information. However, every business has unique needs, and you might want the Cloud DLP system to recognize certain patterns specific to your domain. To fine-tune INFO_TYPES, please refer to the referenced information in the Additional resources document at the end of the training. Once you are done with setting up the configuration, please run all the cells of the notebook sequentially and the summary of the audio files will appear in in the SUMMARIZATION_OUTPUT_ FOLDER_prefix folder in the Google Cloud bucket.

### Video - [LLM Baseline Summarization Evaluation](https://www.cloudskillsboost.google/course_templates/1161/video/509159)

* [YouTube: LLM Baseline Summarization Evaluation](https://www.youtube.com/watch?v=aDjtFFX0t1M)

now that we know how to configure the llm summarization feature and we have a summary of each conversation let's explore how to evaluate its performance one of the most critical aspects for the enablement of the llm summarization feature is evaluating its performance once the summaries of audio files are generated the following guidelines should be used for evaluating the performance of the pre-trained Baseline summarization model as we already know situation summarizes the reason for the call Action summarizes agent actions changes or brief discussions with the customer and resolution summarizes outcome of the customer service there's also customer satisfaction section representing level of satisfaction of the service when the conversation ends and reason for cancellation summarizing the reason if the customer requests a cancellation and there are entities these are key value payers of entity name and entity value in the conversation here's an example of an evaluation template for LL L Baseline summarization the first metric is situation accuracy it measures if the situation section of the summary is accurate and doesn't make up false information that isn't present in the conversation next is situation completeness this is used to measure if the situation section of the summary is complete and doesn't miss important information from the conversation also of importance is action accuracy this metric measures if the action section of the summary is accurate and doesn't make up false information information that isn't present in the conversation as showcased each of these metrics have an Associated scale which is used for evaluation and insuring quality there's also action completeness this measures if the action section of the summary is complete and doesn't miss important information from the conversation next is resolution it checks if customer requests are resolved by the end of the conversation the responses range from yes for full resolution to no or partial and even not applicable in cases where no request is made another crucial metric is customer satisfaction this measures the customer's sentiment at the conversation's end ranging from unsatisfied to satisfied again each of these metrics have an Associated scale as described on the screen we also examine if cancellation requests and their reasons are accurately captured with a simple yes or no response lastly we look at inaccurate entities and incomplete entities these metrics count the number of errors or emissions in the summarization regarding specific entities discussed in the conversation together these metrics provide a comprehensive view of our llm summarization model's Effectiveness ensuring we deliver precise and valuable summaries every time through a meticulous evaluation process we ensure that our llm summarization model is not just technically sound but also ethically responsible and user friendly it's about striking the right balance between technology and human values you can refer to the additional resources for an evaluation template as a reminder it is crucial to ensure that our system is not only producing relevant summaries but also adhering to compliance guidelines no matter how good a summary is if it violates compliance or security guidelines it's concerning summaries must strictly avoid capturing any sensitive or personal information or piis in addition to avoiding sensitive information some M should be objective we want to steer clear of capturing personal emotions opinions or accusations it's about stating facts not opinions evaluating our model's performance isn't just about the accuracy of summaries writing good summaries is a balance between accuracy relevance and compliance all these elements together ensure that our system is efficient and trustworthy Beyond factual accuracy our focus is also on ensuring our summaries are unbiased comprehensive and of high linguistic quality in today's diverse world it's imperative our summaries avoid any biased language this includes references to gender race or age our model aims to capture the essence of the conversation without resorting to these specific identifiers furthermore let's look at an example of a metric template to address these compliance and fairness considerations first we assess repetition this checks for any redundant content in the summary a simple yes or no answer determines if there are repeated phrases or sentences next is grammar and spelling it's essential that our summaries are free from errors so this metric evaluates the presence of any spelling or grammatical mistakes then we have safety this crucial metric ensures the summary is devoid of any toxic or abusive language maintaining professionalism and respect in all Communications fairness is another vital metric it checks for biases in the summary be it related to race age or any other Factor we aim for unbiased Equitable content lastly privacy examines if the summary inadvertently includes any personal identifiable information like credit card numbers or social security numbers it's imperative for us to safeguard privacy at all times together these metrics ensure that our llm summarization model not only performs well but also adheres to the highest standards of compliance and ethical practice in the previous demo we saw how to get transcribed files from audio conversations using Cloud speech to text now we will understand it with two Demos in the first we'll see how to create a conversation profile create an llm summarization model and fetch summary of the conversation using the conversation profile and verifying the output of the same hi my name is Omar and I'm A Cloud AI engineer with Google we are looking at agent assist llm Baseline summarization in the previous demo we saw how to get transcribed files from audio conversations using Cloud speech to text in this demo we will focus on how to get conversation summary using these transcribed files as input to llm Baseline summarization before we get started make sure you have dialog flow API enabled and I am permissions to access CCI insites next step is to create a conversation profile a conversation profile conf configures a set of parameters that control the suggestions made to an agent these parameters control the suggestions that are surfaced during runtime each profile configures either a dialogue flow virtual agent or a human agent for a conversation to create a conversation profile navigate to agent assist console and click on summarization on the left panel after that you will see two options use a pre-trained model or creating a custom model in this demo we will focus Fus on using the pre-trained model add a display name to the profile click Start next step is to select a baseline model agent assist now supports a new V2 summarization Baseline model for voice and chat data this model enables users to customize the content of the summary by selecting from a set of predefined sections as for this demo we will select the Baseline version as 2.0 next step is to select custom sections for your summary selected sections will show up as a part of the summary the situation section is what the customer needs help with or has questions about the action section is what the agent does to help the customer the resolution section is result of the customer service the customer satisfaction section is unsatisfied if the customer is unsatisfied at the end of the conversation and satisfied otherwise the reason for cancellation section has Val if the customer requests to cancel service any otherwise and the entity section is key value pairs of important entities extracted from the conversation let's go ahead and select all the sections for this demo for each feature you select you can select either a model for smart reply and summarization suggestions or a knowledge base for FAQ assist or article suggestion for now we will leave this blank next give a display name to the profile and select the conversation type as chat leave the rest of the settings to default and hit complete verify that you are able to see the conversation profile that you created next step is to get summarization for the transcribed conversations using summarization API navigate back to the notebook we were using in the previous demo verify the parameters required to call the summarization API the audio file output folder prefix should point to a location in the GCS bucket containing the transcribed files we created in the previous demo the summarization output folder prefix should point to a location in the GCS bucket where the script should store the summarization output the conversation profile ID variable should point to the conversation profile that we have created to get the value of the conversation profile variable navigate back to the agent assist console and click on conversation profiles look for the conversation profile we created and copy the name integration ID navigate back to the script update the project ID and the integration ID part of this variable leave the rest of the part of the variable as it is now navigate to the functions needed for calling summarization API section of the file and run it these are all helper functions which will help get the summarization using the API focus on the dialog flow conversations client and dialog flow participants client all the communication is handled via these clients which are a part of dialog flow client library next navigate to The Calling summarization API for transcript summarization section this section is responsible for getting all the transcribed files from the GCS location and return the summarization of the conversations which will be stored in the output location once the script is done navigate to the configured output location to verify that the summary file is generated now download and verify the summarization file the output file is a CSV file which for visualization purposes we have converted to a sheet each row in the sheet corresponds to a row in the CSV file this file will have three columns transcript ID the full conversation and the summary the transcript ID column will contain the name of the transcribed file the full conversation column contains the transcribed conversation from the audio file the summary column will contain the summary returned by the summary API the summary will contain all the sections that were selected while creating the conversation profile so in this demo we saw how to pass transcribed conversations to the summary API to get back the conversation summary based on the configured conversation profile next we'll see how to test agent assist llm Baseline summarization using the simulator in the agent assist console before we get started make sure you have the access to CCI insights in your desired project the key topics covered in this demo are how to use the agent assist simulator how to generate the summary from the conversation in the simulator and how to use the evaluation template to evaluate the quality of the conversation summary hi my name is Omar and I'm A Cloud AI engineer with Google we are looking at agent assist llm based line summarization in the previous demo we saw how to pass transcribed conversations to the summary API to get back the conversation summary based on the configured conversation profile in this demo we will see how to test agent assist llm Baseline summarization using the simulator in the agent assist console to access the agent assist simulator navigate to the agent assist console find the conversation profile we created in the previous demo click on the more actions and and from the drop- down select use simulator this will open up a simulator window for us in the simulator window as you can see we can simulate both the customer and agent side of conversation let's simulate a scenario where a customer's Wi-Fi is not working as you can see on the screen we have simulated a short conversation between a customer and an agent where the Wi-Fi of the customer is not working due to his bill being overdue now once we have the conversation in the simulator we can use the generate summary to get a summary of the conversation once you click on it you will see all the parameters we had selected while creating the conversation profile in this summary you can see that the situation was correctly identified as customer's Wi-Fi is not working in the action section we see what action the agent has taken throughout the conversation we see that the resolution is partial as the customer will have to pay the bill to get the Services back up and running since the customer was okay with the resolution provided the customer satisfaction section is satisfied as there was no cancellation reason for cancellation is na So based on the conversation we can see that the summary provided is accurate based on the given scenario you can use the simulator to test multiple scenarios and generate corresponding summaries for the conversations it is important to be able to evaluate the quality of the summarization to do this we will divide the sections of the summary into quantifiable metric the situation and the action sections of the summary will be further divided into accuracy and completeness the accuracy will measure the accuracy or correctness of the information provided while the completeness will measure if all the required information was covered in the summary we will assign a fivepoint metric for being able to quantify the quality of the information well one being the worse or inaccurate information while five being the most accurate or complete information next for resolution cancellation exists and customer satisfaction sections the values in the summary are to be verified with the conversation to check if what is provided in the summary is actually correct if the value of cancellation exists is yes we will also check the validity of the reason for cancellation being correct and Mark it to be yes or no the entities section for evaluation will be divided into two parts inaccurate entities and incomplete entities where we will note how many entities in the summary are inaccurately captured and how many entities in the conversation are missing in the summary respectively apart from the content metrics we will also focus on compliance metrics here we will determine if there are any repeated phrases or sentences in the summary spelling or grammatical error ER in the summary toxic or abusive language in the summary biases in the summary with respect to race age Etc and is there any personal identifiable information in the summary for all the compliance parameters we will note if they are present or absent in the summary by marking it as yes or no next let's go back to the example conversation we used in the simulator to get the summary and try to evaluate it using the metric we discussed on on your screen you see the conversation with the customer complaining about Wi-Fi not working in the summary section this is the summary we had seen via the simulator now to rate the summary let's focus on situation Since the situation of customers Wi-Fi is not working was correctly captured and completely encapsulates the situation we will rate the situation accuracy and situation completeness to be five as the action correctly captures all agents actions action accuracy and action completeness both will be rated as five the resolution will be rated as partial as the customer still needs to go and pay the bill to get the Wi-Fi working again hence complete resolution could not be provided by the agent in the conversation the customer did not need any further help and found the information to be useful hence the customer satisfaction section will be marked as satisfied as there was no cancellation from customers and the cancellation exists will be no and reason for cancellation correct will be na as we had no inaccurate entities or incomplete entities captured the count for both will be zero now let's move on to compliance metrics since we have no repetition grammar and spelling mistakes both are marked no there is no toxic or abusive language in the summary hence safety is marked no the summary does not show any bias towards specific entities hence fairness is marked no and as no pii information is revealed in the summary privacy is marked as no in this demo we saw how to test agent assist llm Baseline summarization using the simulator in the agent assist console and how to use the evaluation metric to quantify the quality of the summary

### Quiz - [LLM Baseline Summarization Implementation Quiz](https://www.cloudskillsboost.google/course_templates/1161/quizzes/509160)

## Telephony Integrations for Agent Assist

This module will enable you to understand the basics of telephony integrations for Agent Assist leveraging SIPREC and gRPC to unlock real time suggestions based on real time audio feed of live conversations

### Video - [Telephony Integrations for Agent Assist](https://www.cloudskillsboost.google/course_templates/1161/video/509161)

* [YouTube: Telephony Integrations for Agent Assist](https://www.youtube.com/watch?v=v61vrgSuH1o)

Telephony integrations are critical to unlock real time suggestions in Agent Assist because they enable you to receive a real time audio feed of live conversations. Let’s see how they work from an infrastructure perspective. In this section, we will cover: First, the flows involved in Agent Assist Integrations Second, Integrations using SIPREC Next, SIPREC for Agent Assist Deployment And lastly Integrations using Voice (gRPC) - Stream Audio Let’s get started. Agent Assist's effectiveness as an AI solution is notable on its own, but its potential is significantly enhanced through seamless integration with diverse platforms and partners. In this module, we'll explore two key integration approaches for telephony, OEM partners, and CCaaS providers: First is SIPREC, or Session Initiation Recording Protocol: This approach establishes a direct connection of phone calls to Google Telephony Gateway, or GTP, ensuring secure communication and enabling active call control via the CCAI API.The second is gRPC: With this method customers interact directly with public CCAI endpoints. CCAI internally manages the gRPC client, simplifying management and updates. These approaches enable the four types of Integration Flows using SIPREC and gRPC: In the first type of flow, Direct SIPREC: Customers and Agent Assist oversee both SIPREC and UI integrations, necessitating customer control over the Session Border Controller, or SBC. The second type of flow happens through CCaaS Platform. This happens when the Integration between Agent Assist and the customer's CCaaS provider (for eexample, Twilio, Five9) is facilitated by the CCaaS provider, streamlining the process. The third type is through Legacy Platform. For customers utilizing legacy telephony solutions, planned integration will likely involve a CCAI or OEM connector, ensuring compatibility and functionality. These connectors has the logic and code inbuilt which are likely product specific. And the last type of flow is through Native OEM Partner. This occurs when Customers benefit from streamlined gRPC  integration with native OEM partners, optimizing performance and user experience. These integration flows illustrate the versatility and adaptability of Agent Assist, ensuring seamless integration regardless of the telephony or CCaaS infrastructure in place. Let’s learn more about SIPREC and gRPC. SIPREC stands for Session Recording Protocol. SIPREC is an Internet Engineering Task Force, or IETF, defined standard. Its primary function is to enable call recording in telephony environments. It works as an extension to Session Initiation Protocol, or SIP. How does SIPREC  work? First it starts with Session Recording Client, or SRC, which occurs when device or system (like a PBX or Session Border Controller) initiates the recording process. This activates the Session Recording Server, or SRS: A specialized server responsible for receiving, storing, and managing call recordings. The SRC and SRS communicate using SIPREC messages to: Establish a recording session. Transmit metadata (for example, call details, participants, and so on). Send the actual audio stream to be recorded. In our Agent Assist Integration, Customer’s SBC or telephony acts as SRC and our Google Telephony Platform, or GTP, acts as RS. Let's take a closer look at how SIPREC calls work with GTP. GTP acts like a translator, helping your phone system talk to CCAI. It takes the incoming SIPREC messages and turns them into a format that CCAI can understand in gRPC format. In this setup, call signaling and audio (media) are handled separately. Let's break down how SIPREC works to enhance Agent Assist with audio conversation data: It starts with Signaling and Audio Separation: In general call management and audio handling are distinct processes. GTP's Telephony API, service manages call events like connections or drops, while its Iridessa service handles audio streams. Next is GTP's Role: Acting as a mediator, GTP translates SIPREC traffic from the customer's system into a format compatible with CCAI ensuring seamless communication. The outcome is a Simplified Call Flow where The client initiates a call to the designated phone number. GTP (Telephony Gateway) receives the SIP INVITE message, managing the signaling aspect. GTP then identifies the session ID from the SIP INVITE  for reference. And lastly, GTP sets up the CallContext, incorporating the session ID, and notifies about the incoming call to CCAI. Let's deep dive further on telephony integration about how the mapping works from GTP to CCAI. or incoming calls to the telephony gateway, we have several ways to link this call to the corresponding session First is Gateway Connection with Dedicated Phone number: in this instance the end user dials a dedicated CCAI phone number. This call is routed then through the matching conversation profile in a CCAI project Second is Proxy Phone number Assignment : In this case CCAI will assign a temporary phone number to each conversation. This temporary phone number acts as an identifier to track the associated conversation. Third is SIP Integration: The customer may use the CallMatcher API to retrieve SIP header information and further link the conversation to relevant data on their end. This shows when the SIPREC call comes IN, how do we match the call with CCAI conversation. Next we will discuss about what the end to end flow looks like using this logic Agent Assist supports the use of audio conversation data in addition to chat conversation data. This is facilitated through a SIPREC endpoint, a protocol that you can use to establish recording sessions and report metadata. Let's say a customer calls a company's support line number. Here's how Agent Assist can use SIPREC to participate in the conversation and offer support: First is Audio Forking: The company's Session Border Controller, or SBC, sends a copy of the call's audio stream to a SIPREC endpoint hosted by Google Telephony Platform, or GTP. Second is Streaming Audio to CCAI: GTP forwards the audio stream to CCAI and Agent Assist. Third steps sees Agent Assist as Listener Agent Assist joins the conversation as a passive listener in order to process audio from all participants. Fourth is Transcription & Analysis: Underneath, Agent Assist leverages Speech-to-Text, or STT, to transcribe the conversation in real-time. It then analyzes the content to understand the context and the customer's needs. Lastly is Agent Suggestions: Agent Assist provides helpful suggestions to the human agent in two ways: REST APIs: Suggestions are sent directly to the agent's desktop application. And Pub/Sub Messages: Suggestions are broadcast to desktop clients subscribed to the relevant Pub/ Sub topic. Let’s finally discuss what the end to end architecture of an Agent Assist Integration may look like. This is a sample end to end architecture for Agent Assist integration using SIPREC, and how it sends the live transcriptions back to Agent desktop UI application. The audio gets processed and sent back as transcript through a six step process: First is Audio Forking: The Session Border Controller, or SBC, duplicates the audio stream from a call sending a copy to a SIPREC endpoint at Google. Second is SIPREC to CCAI: The SIPREC endpoint, hosted by Google Telephony Platform, or GTP, forwards the audio stream to CCAI and Agent Assist. Third is Agent Assist as Listener: Agent Assist joins the conversation as a passive listener, processing audio from all participants. Fourth is Transcription: Agent Assist leverages Speech-to-Text, or STT, to transcribe the conversation in real-time. Fifth is Optional Redaction: For privacy, transcriptions can be routed through Data Loss Prevention, or DLP, tools for redaction before further processing. Sixth is Agent Desktop UI Integration: Redacted (or unredacted) data is sent to the agent's desktop UI, linked to the conversation ID. Once the transcripts are available: the original SIPREC audio can be stored in Google Cloud Storage, or GCS, alongside the transcript. And CCAI Insights can integrate with Virtual Agent (Dialogflow CX) and Agent Assist to provide automated analysis of the conversations. CCAI Transcription allows you to convert your streaming audio data to transcripted text in real time. As previously mentioned, Agent Assist makes suggestions based on text, so audio data must be converted before it can be used. You can also use transcripted streaming audio with CCAI Insights to gather real-time data about agent conversations. Now let’s discuss about another way of integration using gRPC. This high level infrastructure blueprint focuses the process of transcribing streaming audio data using gRPC calls. This setup enables suggestions and transcripts from Dialogflow to flow back to the agent's desktop during a live call between a customer and agent Note that some Dialogflow APIs are private, so ensure you have the right access. This process requires below steps to be configured. First a Conversation Profile Creation: Use the API's 'create' method on the ConversationProfile resource. S econd make Conversation Setup: Use the API to initiate a conversation, defining participants and their roles. Third create Participants: Use the 'create' method on the participant resource to establish participant types. Choose from HUMAN underscore AGENT, AUTOMATED underscore AGENT or END underscore USER. Note: Only END underscore USER or HUMAN underscore AGENT roles can use Streaming analyze content for transcription lastly is audio transmission and transcription Utilize the dialogflow Streaming AnalyzeContent method to send audio from the relevant participant. This will simultaneously provide real-time transcriptions We hope you now have a good overview of the two main ways of integrating customer telephony infrastructure to Google’s Agent Assist.

### Quiz - [Telephony integrations for Agent Assist Quiz](https://www.cloudskillsboost.google/course_templates/1161/quizzes/509162)

## UI integration

This module will explore how to integrate Agent Assist features into the User Interface of customers agent dashboards using pre-built modules and components. 

### Video - [Introduction to UI integration](https://www.cloudskillsboost.google/course_templates/1161/video/509163)

* [YouTube: Introduction to UI integration](https://www.youtube.com/watch?v=pMdXJiFSjhI)

Now that we have a high level knowledge about the Agent Assist infrastructure, in this section we will cover how to integrate the previously mentioned Agent Assist features such as LLM Summarization, Generative Knowledge Assist etc. into the User Interface of customers agent dashboards using pre-built modules and components. These integrations are common for chat and voice use-cases and require similar configurations and setup. We will start with an introduction for the rationale of UI integrations, followed by a walkthrough of their main building blocks, an architecture review, its UI Components and lastly, a deep dive of the Backend 18 00:00:40,200 --> 00:00:43,000 Integration through Pub/Sub. Let’s get started! . First off, why we need to integrate? Agents in the contact centers need to access the right information and tools to service the customers. Finding the right information is generally difficult, time consuming and induces friction in the process for the customers to get the desired quality of support. So what options do we have for integration? Providing the right context on the agent desktop is enabled by pre-built components and solution integrations which leverages Google’s state-of-the-art Agent Assist features integrations and platform. And lastly, how can we integrate into any existing customer dashboard to help deliver value agent desktops allow users to integrate third-party widgets through an iFrame The user or the agent desktop application just needs to provide a URL of a deployed application. Agent Assist provides two UI Integration options: UI Components ( for simpler Web Components based integration) and Custom Pub/Sub Based Integration Solution for deploying and enabling access to the real-time agent assist capabilities. Both enable the customer's existing Agent desktop systems to interact with Agent Assist side-by-side a conversation where the Agents get the appropriate help they need. These integrations handle important UI tasks, including API communication, agent authentication, rendering suggestions, and agent feedback. Now let’s dive deeper with an Architecture review of the ecosystem as whole There are two scenarios for a conversation to occur : In the first scenario, a call gets transferred seamlessly from a virtual agent to human agent with all the context. In another scenario, Agent Assist will silently listen into the conversation and provide valuable suggestions to the human agent. When the handoff is initiated, the customer call is forked and a third connection is established with Agent Assist, which continuously streams audio to Dialogflow. The audio is transcribed in real time using speech-to-text, or STT. Transcribed text is sent to the Agent Assist NLU module which can match the configuration of Dialog intents and knowledge base articles. Knowledge services in Dialogflow can provide two types of documents: FAQ and article. Agent Assist then tries to match either the FAQ or article and provide suggestions through an API to the agent desktop. Next, the human agent gains access to the full call transcript for immediate context as well as historical analysis, which can be used for further improvements. Lastly, the human agent receives suggestions from Agent Assist and provides feedback on whether those suggestions were accurate or useful in order to continuously improve the model All this happens on the agent desktop and requires no context switching using the UI Integrations. Now that we have reviewed the architecture explaining where does UI modules fit into entire picture, let’s explore the two different approaches available to integrate UI modules into your application - the managed container approach, or as individually imported components. In Managed container Approach, a single component that renders the desired Agent Assist features in a unified panel is provided. This panel will also handle all shared module concerns, including the loading of connectors and any error messaging. This is the recommended approach if integrating the modules into a third-party agent desktop such as LivePerson or Genesys Cloud, or when minimal to no customization is required. In Individual Components Approach, you need to import each component and connector independently. This approach is recommended if a custom application is used where the modules may need to be rendered in different sections of the page or if significant customization is required. In this case, the module for each Agent Assist feature used will need to be independently imported. In addition, the base connector service will also need to be imported, as well as any custom connectors that can integrate with non-supported agent desktops. Next, let's review the details of the various pre built UI components at our disposal to integrate with the Agent Assist features. Among the two options, namely Individual components and Managed Containers for UI Integration we just discussed, let’s consider the Managed Containers scenario such as, a single component that contains all the Agent Assist features. This is based on Web Components which can be added to HTML Views just like a front-end framework component. The integration process involves setting up the requirements by including a script dependency in header of the HTML Document, including the managed container component such as the agent-assist-ui-modules-container and providing all the configuration parameters as depicted in the features parameter of the component tag These components enable the interaction with Agent Assist APIs in the backend. Here’s an example snippet for the integration with a conversation profile with voice enabled features. It is accessible in the Additional resources document. And this example snippet for is for the integration with a conversation profile with chat enabled features. It is also accessible in the Additional resources document.

### Video - [Backend integration - Pub/Sub](https://www.cloudskillsboost.google/course_templates/1161/video/509164)

* [YouTube: Backend integration - Pub/Sub](https://www.youtube.com/watch?v=EER0pUYE8-U)

Now let’s dive into a more complicated form of Backend Integration through Pub/Sub to impart real time capabilities to Agent Desktop via Agent Assist. This sample architecture will help us understand the various components involved in the system: First is Cloud Pub/Sub,this is used as a messaging queue for the Agent Assist Notification channels relaying the messages to the next downstream service called Cloud Pub /Sub Interceptor. This service is a containerized and deployed on Cloud Run it intercepts the event messages pushed by the Cloud Pub/Sub and writes the event information to Memory Store Redis. MemoryStore Redis is a cache system used for synchronization and event storage. These stored messages in various designated channels are subscribed by UI Connector Service. This is another containerized service that serves three functions : first acts as a backend of the Agent Assist UI Service, second serves websocket requests of the frontend and third acts as proxy between Dialogflow and the frontend ui relaying the feedback signals and triggers. Agent Assist UI Service serves html pages that can be reference in agent desktops using iFrames. Overall security is managed by security keys stored in Secret Managers. Authentication can be implemented using external identity managers using simple HTTP APIs. Scalability is managed via Cloud Run and Load Balancers before the Agent Assist UI Service. Let’s discuss UI Connector, Cloud Pub/Sub Interceptor and Memorystore Redis in detail since they are core to the implementation and serve critical functions that can be complicated to understand. Cloud Pub/Sub Interceptor Service enables the system to serve as a webhook processor for Pub/Sub events related to agent assist and ensures they are delivered to the correct channels in the memory store redis. The format used is open curly brace connector underscore eye dee close curly brace colon open curly brace conversation underscore name close curly brace. Memory Store for Redis ensures that the asynchronous communication between servers occurs. It forwards the event and data from interceptor service and delivers to UI Connector using the publisher or subscriber mechanism. UI Connector Service deployed on Cloud Run supports the following tasks : It supports a customized authentication method for agent desktops. It generates temporary JWT after authenticating agent desktops customized tokens. When they send requests for Dialogflow API or WebSockets connection, UI Connector validates the attached JWT instead of checking the original agent token. It establishes a WebSockets connection with the authenticated agent desktop based on a given conversation name. It subscribes event messages to Redis Pub/Sub channels for conversations it handles. And lastly it pushes Agent Assist Events to the desktop UI as they are received. The event data is retrieved or received from Redis Subscribers from the channel corresponding to the ongoing conversation. A channel name is identified by the connector id and the conversation name of the ongoing conversation. The UI Connector server, linked to the Agent desktop via websockets, then handles different conversations and subscribes to distinct Redis Pub/Sub channels corresponding to the ongoing conversations. The websocket connections are stateful and the agent desktop will stay connected  to the to the same container throughout the lifespan of the connection. All the events and data to be displayed in a front-end app that can be embedded in the iFrame of the Agent Desktop. The Agent Assist Front-end Application app has all the view layers and front-end logic to interact with the UI Connector. The app uses the front-end UI Components mentioned earlier to display the views for the features and utilizes the UI Connector and Pub/Sub Interceptor for imparting the realtime abilities. The system is designed to support the following three workflows : Workflow 0: JWT registration for retrieving an access token for subsequent authentication. Workflow 1: Agent Assist event handling Workflow 2: Dialogflow request processing Let’s double click into each of these next. Let’s consider Workflow Zero for Authentication and Authorization. The following API Sequence Diagram depicts the life cycle of the / register endpoint. This process requires an API call to the register endpoint and returns a JWT token that needs to be used for further flows and api calls to the UI Connector Service. The register endpoint implements an auth method that is customised on the basis of the customer requirements and takes care of authentication and authorization. It requires a Customized Authorization Token to be passed in the header and validated with any external authentication or authorization provider based on requirements. Ultimately this workflow provides a Javascript Web Token for authenticating further API calls and downstream flows. Next let’s consider workflow one for Agent Assist Event Handling with this sample example API lifecycle. Let’s start from the right hand side of the sequence diagram. When the conversation is updated, the agent assist events are triggered and pushed to pub/sub which are then relayed to the Cloud Pub/Sub Interceptor service with event payload using the configured endpoints such as human-agent-assistant-event in the agent assist conversation profile console. . The pub sub events are then written to Memorystore Redis instance for the consumption by UI connector. The UI Connector acts as subscriber to the chat channel and Memorystore Redis acts as a publisher to impart the realtime functionalities. Finally the UI connector subscribes to these events from the Redis and forwards them onto the Agent Desktop over a websocket connection, which can be then surfaced. Lettly, let’s double click on the next workflow when the agent interacts with the UI connector from the frontend and requires Dialogflow request processing: First the UI connector acts as a proxy for the dialog flow requests. Then the JWT token acquired in the previous workflows like workflow-0, is used for authentication in the interaction with ui connector After, the Agent Assist Front End Applications sends a request to the ui connector with the jwt and desired payload of the proxy request. The request is then authenticated and forwarded to the dialog flow servers. And finally the response from dialog flow is returned in the reverse flow as depicted through the UI connector here. As you can see here, there are various endpoints and methods that can be used by the proxy. Keep in mind that any additional endpoints that need to be proxied need to be added in the main.py file of the service.

### Video - [Integration steps](https://www.cloudskillsboost.google/course_templates/1161/video/509165)

* [YouTube: Integration steps](https://www.youtube.com/watch?v=StBBmz6w7-E)

Assuming that the desired Agent Assist Models and Feature such as Summarization, Generative Knowledge Assist, and Live Transcription are ready for use in production and integration, let's see how to integrate the Agent Desktops with the Custom Pub/Sub Based Integration Solution. For each conversation profile configured with Cloud Pub/Sub topics, Agent Assist can publish three kinds of event messages to different topics:suggestions, new messages and conversation lifecycle-related events. Suggestions are in the form of HumanAgentAssistantEvent and both new messages and conversation lifecycle events are packed as ConversationEvent. For step-by-step instructions to create a Cloud Pub/Sub topic, visit the Additional resources document. The content of the Cloud Pub/Sub message depends on the event that triggers that Cloud Pub/Sub notification. Please take a moment to familiarize yourself with the response data by looking at the relevant resources referenced in the Additional resources document. The ConversationProfile can be configured using APIs as well as from the Agent Assist Console. The Agent Assist Console is the recommended way. You can also configure Cloud Pub/Sub for handing event triggers from Agent Assists. Generally there are 3 types of events that the Agent Assist emits. These are: Suggestion underscore event - for suggestions generated by Agent Assist Life underscore cycle underscore event - for the conversation life cycle events New underscore message underscore event - for new messages and utterances appearing in the conversation For more information on items under Configuration in Conversation Profile, please refer to resources in the Additional resources document. Lastly, you need to configure the topic sinks in the Agent Assist Console as shown in this image. You need to paste the topic id’s for the previously created topics in the designated sections

### Video - [Deploying the solution](https://www.cloudskillsboost.google/course_templates/1161/video/509166)

* [YouTube: Deploying the solution](https://www.youtube.com/watch?v=35TEN1WJLBY)

Once the Pub/Sub configuration is done, we can move forward to deploy the prebuilt repository to Google Cloud Platform. First we need to make sure that Prerequisites are met first ,install Google Cloud CLI if you haven't yet Then configure Dialogflow resources: First by Enabling the Dialogflow API. Then Creating a conversation profile with desired Agent Assist features. Using Agent Assist Console for creating conversation profile is recommended. And astly, Enable Cloud Pub/Sub notifications for the conversation profile. Now make sure the environmental variables are set and run the bash script as mentioned in the documentation referenced resources #21 and #22 in the additional resources document. The bash script does the following tasks : Set up Google Cloud CLI Configurations Google Cloud Services Enablement Google Cloud resource creation and permission assignment Google Cloud Run service deployment At this point, the frontend app should be available. Please refer to the additional resources referenced here for additional customization options.

### Quiz - [UI integration Quiz](https://www.cloudskillsboost.google/course_templates/1161/quizzes/509167)

## Integrate Agent Assist with Contact Center AI Platform

This module will enable you to understand the basics for the configuration of the Agent Assist integration in CCAIP.

### Video - [Integrate Agent Assist with Contact Center as a Service (CCaaS)](https://www.cloudskillsboost.google/course_templates/1161/video/509168)

* [YouTube: Integrate Agent Assist with Contact Center as a Service (CCaaS)](https://www.youtube.com/watch?v=L48GGAT6cUo)

In this section we will cover the details of how to integrate Agent Assist with Contact Center AI Platform, an End to End Contact Center as a Service solution from Google Cloud. The objectives of this section are to cover the foundational principles for the: Configuration for Agent Assist Integration in CCAIP How to Enable Agent Assist Platform in CCAIP How to Enable Agent Assist Features for Chat and Calls in CCAIP And lastly, how to enable Agent Assist for a Queue in CCAIP Please note that these integration instructions are very similar for both th chat and the voice channel. The integration of Agent Assist in CCAIP requires configuration in two places: in the Agent Assist Console or Project where it is required to and in the CCAIP Platform itself Configuration in the in Agent Assist Console or Project, requires to: Setup an Agent Assist Conversation Profile with the desired features enabled Create a Service Account Key with permission to use that Conversation Profile Configuration in the CCAI platform, requires to: Enable Agent Assist Platform in CCAIP Developer Settings Enable Agent Assist Feature for Chat and Calls in CCAIP Enable Agent Assist for a Queue in CCAIP The steps for integration are as follows: In the Developer Settings of CCAIP Platform: Add an Agent Assist Platform Provide the platform name Add and validate the Service account key Enable the platform Next, In the CCAIP Chat/Call Settings Enable Agent Assist and select all the features applicable and required for the use case you are trying to develop And lastly, for a Given Channel and Queue: Enable and Configure Agent Assist by selecting the added Agent Assist Platform applicable conversation Profile and Enable Wrap Up settings for the Summarization feature, if applicable, for queues. This successfully establishes a linkage between the Agent Assist and Contact Center AI Platform.

### Quiz - [Integrate Agent Assist with Contact Center AI Platform Quiz](https://www.cloudskillsboost.google/course_templates/1161/quizzes/509169)

## Integrate Agent Assist with Dialogflow Virtual Agent

This module will enable you to understand the basics for the configuration of the Agent Assist integration with Dialogflow CX.

### Video - [Integrate Agent Assist with Conversational Agent](https://www.cloudskillsboost.google/course_templates/1161/video/509170)

* [YouTube: Integrate Agent Assist with Conversational Agent](https://www.youtube.com/watch?v=Ewu7anqBpaU)

In this section we will cover the methodology for integrating Agent Assist with Dialogflow Virtual Agents. As we will see, this integration is really powerful to augment Agent Assist. First we will go through the Virtual Agent setup in Dialogflow Console and then we will go through how to link a Virtual Agent to an Agent Assist Conversation Profile Let’s begin! How do we setup a Virtual Agent in Dialogflow Console? We need to create a new virtual agent or select an existing virtual agent to be used and iintegrated with Agent Assist. Virtual agents are useful in the context of Agent Assist because they attempt to resolve customer queries on their own before escalating to a human agent Alternatively, end-users are sent straight to human agents without first interacting with virtual agents as an intermediate step. Inside the Agent Assist console, create or edit an existing conversation profile. In the conversation profile details page, toggle the “Enable virtual agent” and link the Virtual Agent to the Agent Assist conversation profile. This way, Agent Assist features will work after the hand off to a Human Agent from Virtual Agent. In this example “Steering underscore Bot” is the 32 00:01:13,920 --> 00:01:15,960 virtual agent linked with the Agent assist conversation profile.

### Quiz - [Integrate Agent Assist with Dialogflow Virtual Agent Quiz](https://www.cloudskillsboost.google/course_templates/1161/quizzes/509171)

## Dialogflow Runtime Integrations

This module will explore how to best configure Dialogflow integrations and utilize Contact Center Insights to view conversations originated in Agent Assist or Dialogflow CX

### Video - [Conversational Agents runtime integrations](https://www.cloudskillsboost.google/course_templates/1161/video/509172)

* [YouTube: Conversational Agents runtime integrations](https://www.youtube.com/watch?v=w2y1m1rn52U)

In this section, we will walk through various Dialogflow runtime integration alternatives. We will start by understanding the prerequisites to configure the Dialogflow integrations, followed by various available options, namely: 1) how to create a conversation in Agent assist and view in Contact Center Insights 2) how to Create a conversation in virtual agents and view in Contact Center Insights 3) and lastly, how to Create a conversation in DialogFlow CX console and view in Contact Center Insights The Dialogflow runtime integration allows you to integrate data from another Dialogflow-powered Contact Center AI service (like Agent Assist and virtual agents) into Contact Center AI Insights. These services also include Agent Assist and Virtual Agents. After you enable the integration, you can view conversations created in Agent Assist and virtual agent in Contact Center AI Insights. CCAI Insights operates by running analyses, using the CCAI Insights API, on conversations stored in a Google Cloud Storage bucket. As prerequisites to setting up this integration, the following API’s need to be enabled: 1) Storage  2) Speech-to-Text 3) Dialogflow 4) Insights Additionally, you need to create a service account and download the private JSON key file. You will need this file to authorize the integration. The service account associated with your project should be granted the Dialogflow API Admin role in order to authorize the integration You also need to authorize the integration by turning on the Contact Center AI Insights export feature. While configuring Dialogflow Integration, you can create a conversation in Agent Assist and view it in Contact Center AI Insights by following 4 steps: First, create a conversation profile using the Agent Assist console by following the tutorial instructions referenced in the additional resources document at the end of the training. Do not enable the virtual agent option. Second, as an optional step, test the performance of your conversation profile using the Agent Assist simulator. Follow the instructions to create a Smart Reply conversation. Handling conversations at runtime requires direct API calls. These actions cannot be carried out using the Agent Assist console. Make sure that you complete the conversation before moving on. Only completed conversations are visible in Contact Center AI Insights. Next navigate to the Contact Center AI Insights console. And lastly, enter the project ID that you used to create the Agent Assist conversation above and view this conversation in Contact Center AI Insights. The Contact Center AI I nsights conversation name matches the conversation name in Agent Assist. Once you configured the Dialogflow integration, you can create a conversation in virtual agents console and view it in Contact Center AI Insights. To do so: Create a virtual agent and optionally import sample data to your agent. And then follow the same steps as in the previous slide, with the exception that you must enable the virtual agent option when you create a conversation profile in order for the profile to use your newly created agent. While configuring Dialogflow Integration, you can also create a conversation in Dialogflow CX console and view it in Contact Center AI Insights. To do so: Create a virtual agent. Go to agent settings, enable interaction logging and attach the same security setting used in the prerequisites. Test the agent to create a conversation. And finally, navigate to the Contact Center AI Insights console. After a few moments, you should see your conversation with the test agent on your conversation history.

### Quiz - [Dialogflow runtime integrations Quiz](https://www.cloudskillsboost.google/course_templates/1161/quizzes/509173)

## Delivery Lifecycle

This module explores the typical phases of the delivery lifecycle for Agent Assist, both relevant to a Proof Of Concept or full implementation

### Video - [Discovery phase](https://www.cloudskillsboost.google/course_templates/1161/video/509174)

* [YouTube: Discovery phase](https://www.youtube.com/watch?v=_o7T4mjz6Jg)

Once a Statement of Work is in place, what does a typical delivery lifecycle look like for a Proof Of Concept or full Agent Assist implementation? It is usually divided into four phases: The Discovery Phase, the Design Phase, Implementation and Testing And deploy and post-launch  Let’s start with Discovery. The Discovery Phase begins with Identifying the business requirements, the features in scope and assessing the channel that the customer wants Agent Assist capabilities for. Also, it is important in this phase to determine whether the business requires any level of model customization, so make sure to investigate this early to best tailor the implementation to the needs of the target use case Understanding the current ecosystem for the implementation is also critical part in the discovery phase. We need to identify which Telephony provider is currently in use. Once Identified, we need to assess the feasibility of integration of the Telephony provider with Dialogflow C X Identification of customer’s Agent Desktop or CRM is also crucial. Integration of Agent Desktop or CRMs with Agent Assist can vary from vendor to vendor. Some may provide integration out of the box while for some a custom implementation may be required. Keep this in mind in the directional setup of your POC or implementation. The second step of the Discovery Phase requires you to understand how the historical data can be made accessible. You must also understand the format it is currently hosted in so that it can be effectively processed. For example, raw audio needs to be run through S-T-T or be converted to JSON-formatted files that match the response format from Cloud Speech-to Text recognition. This information is key for the improvement or fine-tuning of the underlying models for Agent Assist features, if applicable. In the third step of the Discovery Phase, you need to understand the current GCP setup. The right GCP environment needs to be in place with access to all required GCP resources such as Dialogflow, and STT Cloud Storage. For example, the CCAI suite of products. like Dialogflow CX, Agent Assist and Insights, may live in one project, while the conversations and documents live in another project. Account for this in this phase. Additionally for UI UX integration, make sure the supported business has the right access to deploy the backend modules and setup pub-subs for the Agent Assist event lifecycle. In the last step of the Discovery process, you need to make sure that the implementation team has the expertise to implement the Agent Assist UI modules in their Agent Desktop / Frontend. This will also involve the integration of Customer’s Telephony provider with the Dialogflow CX. If they have the expertise, you are set. If not, then the service provider (Googler or a Google Partner) can help the customer with Telephony and Salesforce integration by demoing the end-to-end Agent Assist functionality. You also need to make sure that the implementation team can set-up Agent Assist Backend modules in their environment If they can, that’s great. If not, then as their service provider (Google or a Google Partner)we can help the customer set up the default Backend module in their GCP environment for demo ing the end-to-end Agent Assist functionality, without any customization.

### Video - [Design phase](https://www.cloudskillsboost.google/course_templates/1161/video/509175)

* [YouTube: Design phase](https://www.youtube.com/watch?v=vgkPz1B1plc)

Now, let’s move on to the Design phase. Let’s assume the end user is using their device to call an Agent, while the Agent uses the Agent Desktop Client to answer the calls. Both of them can be connected to each other through a Telephony provider. The voice conversion is also streamed to Agent Assist via Dialogflow Telephony Gateway. All the Agent Assist events are pushed to Pub/ Sub topics The events on Pub/Sub topics are processed by the Backend Modules. Backend Module can sent these events to Agent Desktop through a Websocket Connects. Backend Module also works as Dialogflow proxy for Agent Desktop. We can also make use of export conversation from Agent Assist in order to download the conversation in to a GCS bucket. This is a complete sample blueprint of a typical Agent Assist Architecture coming out of the Design Phase.

### Video - [Implementation and testing](https://www.cloudskillsboost.google/course_templates/1161/video/509176)

* [YouTube: Implementation and testing](https://www.youtube.com/watch?v=impJw_n0SHM)

Once the implementation team has a vision for the end state architecture, they can move to the Implementation and test phase. During implementation, each Agent Assist feature should be under close scrutiny to ensure proper configuration. Thankfully testing is made easy in Agent Assist by leveraging its simulator. Let's take the example of transcription evaluation. In this case you should simulate a voice conversations and assess the corresponding transcribed text from the simulator. Another example can be made for Generative Knowledge Assist. For GKA evaluation, you can use a predefined set of queries on GKA and rate the GKA generated answers. You should also evaluate for associated documents and links that GKA suggests answers from. In doing so, you need to make sure the overall evaluation meets the standard as defined by the customers. For additional information on how to assess data store generated responses, please refer to the Virtual FAQ, Generative AI Agents, training module of the CCAI Academy. Summarization can also be evaluated similar to GKA. You should run the summarization for around 30 to 50 historical conversations and ask the customer to rate them. Make sure that different critical metrics like accuracy, completeness and correctness of the summaries are included in the rating system. Finally, to ensure that the entire Telephony and UI-UX integration with the Agents Desktop and the Agent Assist Backend is working as expected, you should perform end-to-end testing to ensure each functionality is performing according to expectations. This can be done by simulating a few rounds of conversations.

### Video - [Deploy and post-launch](https://www.cloudskillsboost.google/course_templates/1161/video/509177)

* [YouTube: Deploy and post-launch](https://www.youtube.com/watch?v=KkGKDAhDFTU)

Once all of the Agent Assist features are implemented and tested, it's time to move them into the production environment. When working with custom modeling, there are several issues to consider regarding development and production environments. If the custom model (or models) was developed in a test or development instance, then be sure to migrate them to the production environment. Also, make sure to validate that the model is working as expected on the initial set of conversations. This approach also needs to be replicated for the conversation profile. Make sure all the configurations are set properly in the production environment conversation, especially the pub-sub configuration [If applicable] Also monitor an end-to-end integration for both Telephony and UI/UX integration. You can do this by starting with 1 percent of the traffic and subsequently scaling up to 10 percent, then 20 percent and so on. Congrats on completing this training. Enjoy building in Agent Assist!

### Quiz - [Delivery lifecycle Quiz](https://www.cloudskillsboost.google/course_templates/1161/quizzes/509178)

#### Quiz 1.

> [!important]
> **What are all the GenAI powered features in the Agent Assist ?**
>
> * [ ] Agent Coaching
> * [ ] LLM Summarization
> * [ ] These are all GenAI powered features in the Agent Assist
> * [ ] Generative Knowledge Assist

#### Quiz 2.

> [!important]
> **True or False: A conversation profile in Agent Assist configures a set of parameters that control the suggestions made to an agent. These parameters control the suggestions that are surfaced during runtime**
>
> * [ ] False
> * [ ] True

#### Quiz 3.

> [!important]
> **What activities are performed in the discovery phase of the Delivery Life cycle?**
>
> * [ ] Telephony integration identification
> * [ ] UI/UX Integration identification
> * [ ] Agent Assist feature and Channel identification
> * [ ] All of these activities are performed in the discovery phase of the Delivery Life cycle.

## Additional Resources

This module includes the list of additional resources that complement the course learning.

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/1161/documents/509179)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
