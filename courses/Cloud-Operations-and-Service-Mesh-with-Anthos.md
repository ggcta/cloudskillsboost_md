---
id: 435
name: 'Cloud Operations and Service Mesh with Anthos'
type: Course
url: https://www.cloudskillsboost.google/course_templates/435
date_published: 2023-11-30
topics:
  - Service Management
  - Anthos
  - Observability
---

# [Cloud Operations and Service Mesh with Anthos](https://www.cloudskillsboost.google/course_templates/435)

**Description:**

Course two of the Architecting Hybrid Cloud with Anthos series prepares students to operate and observe Anthos environments. Through presentations and hands-on labs, participants explore adjusting existing clusters, setting up advanced traffic routing policies, securing communication across workloads, and observing clusters in Anthos. This course is a continuation of course one, Multi-Cluster, Multi-Cloud with Anthos, and assumes direct experience with the topics covered in that course.

**Objectives:**

* Identify multiple ways to observe and monitor services running on Anthos Clusters in multiple environments.
* List the steps to install Anthos Service Mesh on Anthos Clusters in multiple environments.
* Understand how to do traffic routing, improve network security and enhance observability with Anthos Service Mesh.
* Learn how to create multi-cluster networking architectures with Anthos Service Mesh.

## Introduction

Welcome to Cloud Operations and Service Mesh with Anthos.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/435/video/427965)

* [YouTube: Course Introduction](https://www.youtube.com/watch?v=ppUSVLg2ZRk)

Welcome to Cloud Operations and Service Mesh with Anthos, the second course in the Architecting Hybrid Cloud Infrastructure with Anthos course series. This course includes lectures that cover Anthos concepts and hands-on lab exercises to reinforce those concepts. Module 0 - Introduction - describes what will be covered in this course. Module 1 - Introducing Anthos Service Mesh - describes the benefits of Anthos Service Mesh and how to install it, configure it, and how to observe workload telemetry. Module 2 - Anthos Service Mesh Routing - describes encrypting microservice traffic, authenticating and authorizing services and requests, and limiting service access in the network. Module 3 - Multi-cluster Concepts on Anthos - introduces the Anthos fleet, which is a group of clusters and other resources. It also covers fleet networking, multi-cluster services, and multi-cluster gateways. Module 4 - Multi-cluster Networking with Anthos Service Mesh - describes multi-cluster networking, installing Anthos Service Mesh on different Anthos GKE clusters, configuring the network, and installing and using multi-cluster Gateways and multi-cluster Services (or MCS). Let’s get started!

## Introducing Anthos Service Mesh

In this module, you are introduced to the Anthos Service Mesh. You will learn how to use Anthos Service Mesh to facilitate service communication across clusters.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/435/video/427966)

* [YouTube: Module overview](https://www.youtube.com/watch?v=umxfo4eQcIg)

Welcome to Introducing Anthos Service Mesh. In this module, we: Understand the benefits of Anthos Service mesh, including running distributed services across clusters and enhancing service observability, traffic management, and security. Install Anthos Service Mesh on different Anthos clusters and choose the right capabilities for you depending on the level of management and automation you want. Collect workload telemetry including metrics, traces and logs, and learn to visualize your services on the Anthos Service Mesh dashboards. Operate an Anthos Service Mesh understanding the capabilities, limitations, and costs of running it on different Anthos clusters. This is our agenda for the module, shown on the slide. Let’s get started.

### Video - [Lab intro: Anthos Service Mesh Walkthrough](https://www.cloudskillsboost.google/course_templates/435/video/427967)

* [YouTube: Lab intro: Anthos Service Mesh Walkthrough](https://www.youtube.com/watch?v=W7sC8k5FqHs)

We’ll start with an Introduction to Anthos Service Mesh, by doing a lab exercise. Instead of talking through all the features that Anthos Service Mesh offers, let’s go directly to the first lab, which provides a walkthrough of its capabilities. After the lab, you can can continue with the module, which covers these features and capabilities in more detail. In this lab, you perform the following tasks: Explore the app and its workloads deployed in Anthos clusters Force cross-cluster traffic routing Observe distributed services Verify service mesh security

### Lab - [AHYBRID032 Anthos Service Mesh Walkthrough](https://www.cloudskillsboost.google/course_templates/435/labs/427968)

Qwikstart - Anthos Anthos Service Mesh Walkthrough

* [ ] [AHYBRID032 Anthos Service Mesh Walkthrough](../labs/AHYBRID032-Anthos-Service-Mesh-Walkthrough.md)

### Video - [Lab Review](https://www.cloudskillsboost.google/course_templates/435/video/427969)

* [YouTube: Lab Review](https://www.youtube.com/watch?v=luCbXDsxxx4)

In this lab, we saw a multi-cluster GKE environment running Anthos Service Mesh. Services were deployed into multiple clusters as distributed services. Distributed services provide multi-regional availability and remain up even if one or more GKE clusters are down, as long as the healthy clusters are able to serve the load. All service telemetry was collected and aggregated in the Anthos Service Mesh dashboards, and made ready for consumption, so that you can analyze data over time and make sure your services meet your Service Level Objectives, or SLOs. All traffic between services was encrypted over mTLS and you had the possibility to implement and enforce policies to and from every service.

### Video - [The need for a service Mesh](https://www.cloudskillsboost.google/course_templates/435/video/427970)

* [YouTube: The need for a service Mesh](https://www.youtube.com/watch?v=SNZRXrFYJ-4)

In monolith applications, functions call each other directly. When moving to microservices, the functions may be encapsulated in separate services, and those services might be containerized. So, no direct calls can be made from function a to function b. The services are no longer necessarily inside the same computer, yet they still must communicate with each other. This requires networking. We want to expose the two services via a contract. We abstract away how we handle the communication and standardize the way that the services communicate. You don’t want to assume that just because the two containers are on the same network, that Container B should trust Container A. Rather, you want Container A to go through authentication and authorization whenever it calls the API exposed by Container B. Therefore, each container needs to be provided an identity. And there needs to be some security scaffolding that can request/accept the identity of a calling service and handle authorization. In addition, developers and architects must think about fault tolerance, latency, circuit breakers, etc. This requires additional scaffolding. The application may also need to enforce quotas and rate limiting and other API access policies. Thus, more supporting functionality needs to be in place. Lastly, if you want observability data like logs, metrics, tracing, and topology, you need yet more supporting functionality. So, there is a lot of technical functionality that’s required for successful operations of your application. This functionality is replicated for every instance of your service, but it really has nothing to do with the business logic of the services themselves. This is really networking functionality, and perhaps can be handled at the network level rather than the application level.

### Video - [Separating Application from Network Functionality](https://www.cloudskillsboost.google/course_templates/435/video/427971)

* [YouTube: Separating Application from Network Functionality](https://www.youtube.com/watch?v=eaJfniwbwEs)

A service mesh separates applications from network functionality. One approach is to take a replica of a service. Extract the business logic into one container. And then extract all the network functionality into a separate container. In Kubernetes, you would place the two containers together into a pod, which represents one instance of your service. This pattern of marrying a business logic container with a utility container that handles additional technical functionality is called the sidecar pattern. This allows developers to focus on implementation of business logic, without having to invest in all the surrounding technology. Meanwhile, DevOps/SRE teams can focus on building all the technical scaffolding that adds valuable functionality around the business logic. You might be wondering… Where does the sidecar container with network functionality come from? How are the sidecar containers added to the pods? How are the sidecar containers configured? How are the metrics and logs from sidecar containers collected and forwarded? The service mesh provides, manages, and works with the sidecar containers to make everything work across all your services. First, a DevOps operator configures the service mesh with security and routing policies. Second, on pod creation, the service mesh injects the sidecar container and configures it with the network configuration. Third, communication is handled by the injected container and metrics, logs, and traces get exported from the sidecars to enhance observability. While service mesh started with containers and microservices, its benefits can be applied to traditional applications. Service mesh addresses workloads that stretch across clusters and environments with isolation and security. It extends beyond Kubernetes container clusters to bare metal servers and virtual machines and links them together in an elegant, logical, and secure manner. That way, you can modernize existing brownfield applications in place, without the need to go through the expensive process of rewriting them. In the previous video, you learned what a service mesh can do. Let’s now discuss the specifics of Anthos Service Mesh. Anthos Service Mesh is a managed service based on Istio, the leading open-source service mesh. Istio draws on a lot of inspiration from Google-internal systems, but is built by multiple parties, including Google, IBM, and Lyft, the ride-sharing company. Google takes new Istio versions, tests them, adds in some Google Cloud-specific components that improve management, and makes them available for installation on all your Anthos clusters. Google Cloud provides support and SLAs. When installed, Anthos Service Mesh provides all the standard Istio goodness with additional features such as the user interface. Anthos Service Mesh can be purchased as a standalone product or as part of the Anthos subscription.

### Video - [Architecture](https://www.cloudskillsboost.google/course_templates/435/video/427972)

* [YouTube: Architecture](https://www.youtube.com/watch?v=muOgrQtw0PE)

Let’s talk about the architecture of Anthos Service Mesh. Anthos Service Mesh is composed of the data plane and the control plane. The data plane encompasses all network communication between microservices. It does so by leveraging Envoy, an open-source, high-performance C++, distributed network proxy, that is deployed as a sidecar proxy to each Kubernetes pod in the mesh. Envoy takes care of the communication protocol, security, and observability in the mesh. You can use: Protocols such as HTTP 1 and 2, gRPC or TCP. Encryption over TLS, mutual TLS, or none at all, having the communication go over plain text. And telemetry reporting on the mesh, capturing information such as requests latencies and package sizes. The control plane manages and configures the Envoy proxies to route traffic and acts as a certificate authority to authenticate and establish trust between microservices. To configure the Envoy proxies, administrators use Kubernetes CustomResourceDefinitions implemented by the Istio project. Once they are applied, the control plane then synchronizes changes with the Envoy proxies over the Envoy xDS APIs. You have two options for hosting your service mesh control plane. You can use the in cluster installation with Istiod. In this case: Anthos Service Mesh runs as a Kubernetes Deployment inside your cluster. Part of your cluster capacity is used for hosting the control plane. You have to manage control plane upgrades. It’s compatible with all Anthos installations including attached clusters. Or you can use the Google Managed control plane. In this case: Anthos Service Mesh runs as a managed service outside of your cluster. Full cluster capacity utilization is dedicated to running your workloads. Google Cloud handles reliability, upgrades, scaling, and security of the control plane. Optionally, you can enable a Google-managed data plane, so that proxies and gateways are also automatically upgraded with the control plane. If you do so, you can subscribe your mesh to the Rapid, Regular, or Stable channel, depending on the cadence of updates that you want. Rapid uses the latest version of Anthos Service Mesh with the newest features. Regular offers a balance of feature availability and release stability, and is what we recommend for most users. And Stable prioritizes stability over new functionality, so you get a version that has been tested and validated on the Rapid and Regular channels. Anthos Service Mesh with a managed control plane is only available for GKE clusters. You also have two options for hosting your service mesh certificate authority, or CA. Remember that the certificate authority generates certificates to allow secure mTLS communication in the data plane. You can use the in cluster installation with Istio CA. In this case: Certificate authority management takes place inside the istiod Kubernetes Deployment in your cluster. Cross-cluster service authentication requires hosting your own root CA. And Istio CA is compatible with all Anthos installations, including attached clusters. Alternatively, you can use the Google managed Mesh CA. In this case: The certificate authority runs as a managed service outside of your cluster. Google Cloud handles reliability, upgrades, scaling, and security of the certificate authority. When running services across multiple clusters that are part of the same Anthos Fleet, you can use a single Mesh CA to simplify secure service-to-service communication. In addition, you can use Certificate Authority Service as a root CA. Mesh CA is only available for GKE clusters and Anthos clusters on-premises. In addition to Mesh CA, you can configure Anthos Service Mesh to use Certificate Authority Service, or CA Service. CA Service is suitable for the following use cases: If you need different certificate authorities to sign workload certificates on different clusters. If you want to use Istiod custom CA plugin certificates. If you need to back your signing keys in a Google-managed HSM (or Hardware Security Module.) If you are in a highly regulated industry and are subject to compliance. If you want to chain up your Anthos Service Mesh CA to a custom enterprise root certificate to sign workload certificates. CA Service provides an SLA and is charged separately. While Mesh CA does not provide an SLA, the service is included in the Anthos Service Mesh base price.

### Video - [Installation](https://www.cloudskillsboost.google/course_templates/435/video/427973)

* [YouTube: Installation](https://www.youtube.com/watch?v=c8gk7OZSKlQ)

Now that we have seen the main Anthos Service Mesh architectural components, let’s see how to install them on an Anthos cluster. Before installing Anthos Service Mesh, you must fulfill certain requirements. You must have created a GKE or an Anthos cluster with at least 16 vCPUs and 15 GB of memory. Also, your clusters must have GKE Workload Identity enabled and be registered to an Anthos Fleet. When deploying Anthos clusters on VMware, bare metal, or AWS, the enablement of GKE workload Identity and the registration in an Anthos Fleet takes place automatically.

### Video - [Life of a request in the mesh](https://www.cloudskillsboost.google/course_templates/435/video/427974)

* [YouTube: Life of a request in the mesh](https://www.youtube.com/watch?v=qlGCW9m44e4)

Let’s take a look a the life of a request inside a service mesh. When a pod is created, your workload container comes up, which in this case is called “Service A”. Then the Envoy sidecar container is injected in the pod and IP Tables are configured so that all requests are redirected to Envoy. Envoy fetches service information, routing, and configuration policy from the control plane so that it knows where to send requests. Finally, the service mesh certificate authority, in this case Mesh CA, securely distributes TLS certificates to the Envoy proxies. When Service A places a call to service B, the client-side Envoy proxy intercepts the call. Envoy consults the local configuration to know how and where to route call to service B. For instance, Envoy might change the protocol to gRPC and establish a secure mTLS connection. Then Envoy forwards the request to the appropriate instance of service B. There, the Envoy proxy deployed with the service intercepts the call. The receiving Envoy validates certificates and establishes the mTLS connection. The receiving Envoy checks the request using the Envoy Filters to validate that call should be allowed. For instance, Envoy can perform: Access control list, or ACL, checks to verify service A can communicate with service B. Or quota checks to verify that service A does not surpass the entitled number of requests per second. Once those validations take place, the server-side Envoy forwards request to service B, which processes the request and returns a response. Envoy forwards response to the original caller, where the response is then intercepted by Envoy on the caller side. The server-side Envoy is done with the request and it reports telemetry to the wasm extensions. Wasm extensions are in-proxy extensions that allow developers to add custom networking functionality. Wasm extensions in turn notify appropriate plugins. Client-side Envoy forwards response to original caller. Once the client-side Envoy has processed the request, it reports telemetry to Envoy extensions, including client-perceived latency. The Envoy extensions in turn notify the appropriate plugins.

### Video - [Mesh telemetry and instrumentation](https://www.cloudskillsboost.google/course_templates/435/video/427975)

* [YouTube: Mesh telemetry and instrumentation](https://www.youtube.com/watch?v=AHHpgy_rZpo)

Let’s take a closer look at mesh telemetry and instrumentation. Traditionally, programmers have implemented instrumentation in the form of code instructions that monitor specific components in a system in order to monitor or measure the level of a product's performance, to diagnose errors, and to write trace information. As containers are becoming the compute unit of choice, environments becomes more polyglot as more and more diverse programming languages are introduced, making this effort very hard to maintain and track; it hinders the philosophy of separation of duties between operators and developers. To obtain telemetry data, Anthos Service Mesh relies on the Envoy sidecar proxies that you inject in the workload pods. The proxies intercept all inbound and outbound HTTP traffic to the workloads and report the data to Anthos Service Mesh. They report data by calling the Cloud Monitoring, Cloud Logging, and Cloud Tracing APIs, as well as the Prometheus service in the cluster. With Anthos Service Mesh, service developers don't have to instrument their code to collect telemetry data. Logs are exported to the node. A fluentd daemonset that comes with the Anthos cluster forwards the logs to Cloud Logging. Standard and custom metrics collected in the Anthos Service Mesh by the Envoy sidecar proxies include proxy, service, and control plane metrics. You can configure custom metrics with Envoy Filters, an Istio CRD that gets applied directly to the Envoy Proxy. However, Envoy Filters are not supported in ASM 1.11 and, therefore, won’t be covered in this course. You might be wondering, how does Envoy know how to communicate with Google Cloud’s operations suite APIs? Anthos Service Mesh installs a WebAssembly plugin in the Envoy proxy that contains the functionality to call the Cloud Monitoring and Cloud Tracing APIs. WebAssembly (or wasm for short) is a portable bytecode format for executing code written in multiple languages at near-native speed built into Google’s high performance V8 engine. After receiving a W3C recommendation in Dec 2019 to run natively in all major browsers, it is the fourth standard language (following HTML, CSS, and JavaScript). Wasm enables you to deploy third-party telemetry filters that can redirect requests to your observability and policy backends. Benefits of Wasm include: Multi-language plugin support, such as C++, Rust, and AssemblyScript with more to come. Also, over 30 programming languages can be compiled to WebAssembly, allowing developers from all backgrounds to write Envoy extensions in their language of choice. Plugin development agility. Extensions can be delivered and reloaded at runtime using the Istio control plane. This enables a fast develop → test → release cycle for extensions without requiring Envoy rollouts. Reliability and isolation. Extensions are deployed inside a sandbox with resource constraints, which means they can now crash, or leak memory, without bringing the whole Envoy process down. CPU and memory usage can also be constrained. Security. The sandbox has a clearly defined API for communicating with Envoy, so extensions only have access to, and can modify, a limited number of properties of a connection or request. Furthermore, because Envoy mediates this interaction, it can hide or sanitize sensitive information from the extension (for example, “Authorization” and “Cookie” HTTP headers, or the client’s IP address). Easily create and deploy modules to your service mesh, and publish and share them to WebAssembly Hub as a repository. (This is similar to how Helm charts are deployed to the stable.) Like Docker, the WebAssembly Hub stores and distributes Wasm extensions as OCI images. This makes pushing, pulling, and running extensions as easy as Docker containers. Wasm extension images are versioned and cryptographically secure, making it safe to run extensions locally the same way you would in production. This allows you to build and push, as well as trust the source when they pull down and deploy images. While logging and monitoring are enabled by default, you have to enable tracing yourself. To enable tracing, configure an overlay file for In-cluster ASM or apply a Kubernetes ConfigMap with tracing set to stackdriver as a backend for Managed ASM. When using a Service Mesh, some individual traces are collected automatically, so that you can see the service topography or understand the latency per service. However, if you want to understand an end-to-end trace that takes places across multiple microservices, you need to instrument your application. In the instrumentation process, you use libraries in your code to add unique headers to your requests, so that you can provide a trace of the request. There are some frameworks, protocols, and databases that provide automatic instrumentation. However, most of the time, you will need to provide additional instrumentation. To instrument your applications, there are a variety of options. Tracing implementation is evolving rapidly, so the details may change. Across all languages, Google is transitioning to make use of OpenTelemetry libraries. However, at the moment, for library support by language, use: OpenTelemetry for Node.js and Go. OpenCensus for Python, Java, Go, and PHP. And Cloud Trace for Ruby, Node.js, and C# (both ASP.NET core and regular ASP.NET). Once traces are collected by your library of choice, you can visualize requests in the Google Cloud Console using the Cloud Trace dashboards.

### Video - [Anthos Service Mesh dashboards](https://www.cloudskillsboost.google/course_templates/435/video/427976)

* [YouTube: Anthos Service Mesh dashboards](https://www.youtube.com/watch?v=Gz8jNQIuS5k)

Once telemetry has been collected, let’s see the visualizations that the Anthos Service Mesh dashboards offer. Anthos Service Mesh delivers service dashboards so that you can: Get an overview of all services in your mesh, providing you critical, service-level metrics on three of the four golden signals of monitoring: latency, traffic, error, and saturation. Define, review, and set alerts against service level objectives, or SLOs, which summarize your service’s user-visible performance. View metric charts for individual services and deeply analyze them with filtering and breakdowns, including by response code, protocol, destination pod, traffic source, and more. Get detailed information about the endpoints for each service, and see how traffic is flowing between services - as well as what performance looks like for each communication edge. Decide on the key metrics that you want to report on, for instance latency and availability of your service. These metrics are your Service Level Indicators, or SLIs. Set up thresholds for performance and compliance on your SLIs and define an error budget for them. These targets are called Service Level Objectives, or SLOs. Anthos Service Mesh allows you to monitor your SLIs and set up SLOs, so that service operators have a unified location to understand the health of their services and can set up alerts to notify them when an SLO runs out of budget. Anthos Service Mesh dashboards also visualize your mesh topology. A service chart is automatically generated to represent relationships and traffic flow between services. You can also drill down to see the workloads and pods behind services and see the queries per second, or QPS, rates between services.

### Video - [Anthos Service Mesh pricing and support](https://www.cloudskillsboost.google/course_templates/435/video/427977)

* [YouTube: Anthos Service Mesh pricing and support](https://www.youtube.com/watch?v=xrP0tNoXle4)

Finally, let’s review Anthos Service Mesh pricing and support model. Anthos Service Mesh (or ASM) is available as part of Anthos or as a standalone offering on Google Cloud. Google APIs enabled on the project determine how you are billed. To use Anthos Service Mesh as a standalone service, don't enable the Anthos API on your project. If you want to use Anthos Service Mesh on-premises or on other clouds, you must subscribe to Anthos. If you are subscribed to Anthos, there are no additional costs for ASM. Pricing for Anthos Service Mesh as a standalone service is based on the number of clusters and the number of Anthos Service Mesh clients. There is a GKE flat fee of 7 cents per hour, which is around 50 dollars per month. The flat fee includes 100 ASM clients, which is equivalent to 72,000 client hours per cluster in a month. Every additional client costs around 50 cents per client per month. Anthos Service Mesh standalone pricing includes: Compute Engine VMs and GKE pods. Telemetry dashboards. Anthos Service Mesh standard metrics are included in the fees noted above. Custom metrics are charged based on Cloud Monitoring pricing. Anthos Service Mesh managed control plane. Mesh CA, a managed certificate authority service, with no per-certificate charge. Not all Anthos Service Mesh features are available in all installations of Anthos, so take a moment to review the supported configurations. GKE clusters are the first to adopt new ASM features. Therefore, GKE offers the most comprehensive range of options available, including ASM dashboards or the support for CA Service, the SLA-backed root Certificate Authority. Installations of Anthos clusters on-premises such as VMware and bare metal are the second in line for ASM feature support. All types of metrics, including in-proxy HTTP metrics and a Google’s managed Mesh CA, are supported here. Traditionally, operators of clusters on-premises have used open-source dashboarding options such as Grafana, Prometheus, Zipkin, and Kiali to visualize mesh. These can be easily installed to bypass current limitations with regards to ASM dashboards. Finally, Anthos Service Mesh offers basic support for Anthos clusters running on other clouds, such as AWS or Azure, where most of the managed features can be replaced in the short term with open-source solutions or cloud-specific services. Additionally, there are a couple of limitations on Anthos Service Mesh with respect to Istio as of version 1.11. Those include: Service mesh integration with custom CAs. With Istio, you can set up your own custom root CA which might manage identities both inside and outside of your cluster. Usage of Envoy Filters to extend the service mesh for additional telemetry and policy. With Istio, you can extend the default functionality to provide additional checks at the networking layer. Usage of arbitrary telemetry and logging backends. Anthos Service Mesh provides an out-of-the-box integration with Google Operation Suite, but it’s not possible to configure additional backends. Multi-network support and IPv6 support for Kubernetes is also only available in Istio. Finally, Consistency Hash and locality-weighted lb policies are settings that you can configure on sidecar proxies running open-source istio but are not available on Anthos Service Mesh. Gateways will be covered in depth later in the course, but we wanted to share a few notes because you encounter them in the next lab. Gateways allow traffic to ingress and egress the service mesh. You can think of them as a load balancer and a NAT service. The same way as other pods in the mesh, Gateways leverage Envoy proxies to configure traffic routing, security, and observability. Similar to a Kubernetes Ingress, a Gateway creates and configures Google Cloud Load Balancing, and you can set up ports, protocols, and certificates.

### Video - [Lab intro: Observing Anthos Services](https://www.cloudskillsboost.google/course_templates/435/video/427978)

* [YouTube: Lab intro: Observing Anthos Services](https://www.youtube.com/watch?v=Y1z7GTXEVW8)

In this lab, you learn to install Anthos Service Mesh (ASM) on Google Kubernetes Engine. You will perform the following tasks: Install Anthos Service Mesh, with tracing enabled and configured to use Cloud Trace as the backend. Deploy Bookinfo, an Istio-enabled multi-service application. Enable external access using an Istio Ingress Gateway. Use the Bookinfo application. Evaluate service performance using Cloud Trace features within Google Cloud. Create and monitor service-level objectives (SLOs). Leverage the Anthos Service Mesh Dashboard to understand service performance.

### Lab - [AHYBRID041 Observing Anthos Services](https://www.cloudskillsboost.google/course_templates/435/labs/427979)

Do a custom install of Cloud Service Mesh, evaluate performance with monitoring, logging and tracing, and define and measure SLOs. Optionally, perform similar observability tasks using common OSS tools.

* [ ] [AHYBRID041 Observing Anthos Services](../labs/AHYBRID041-Observing-Anthos-Services.md)

### Quiz - [Test your knowledge](https://www.cloudskillsboost.google/course_templates/435/quizzes/427980)

#### Quiz 1.

> [!important]
> **Wasm plugins can be used to send telemetry to external backend such as Cloud Monitoring.**
>
> * [ ] False
> * [ ] True

#### Quiz 2.

> [!important]
> **Which CLI tool is used to install and manage Anthos Service Mesh?**
>
> * [ ] kubectl
> * [ ] meshctl
> * [ ] gcloud
> * [ ] asmcli

#### Quiz 3.

> [!important]
> **The Anthos Service Mesh control plane interacts with the Envoy proxies to**
>
> * [ ] Configure them on pod creation.
> * [ ] Route requests as they flow through the mesh.
> * [ ] Check quotas and ACLs as requests flow through the mesh.
> * [ ] Secure requests using mTLS as they flow through the mesh.

## Managing Traffic Flow with Anthos Service Mesh

In this module, you will learn how to use Anthos Service Mesh for advanced routing and traffic management.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/435/video/427981)

* [YouTube: Module overview](https://www.youtube.com/watch?v=JqGFGYeBJtw)

Welcome to the introduction of Anthos Service Mesh. In this module, you: Understand how Anthos Service Mesh learns the network from Kubernetes and builds on top to provide advanced routing capabilities. Deploy mesh API resources such as the VirtualService, DestinationRule, Gateway, Service Entry, and the Sidecar to configure the mesh. Harden the mesh network by introducing new functionality such as request retries, request timeouts, and circuit breakers. Test the mesh network by creating failures and delays on specific services, so that you can improve overall resilience. This is our agenda for the module, shown here on the slide. Let’s get started.

### Video - [Networking and service discovery](https://www.cloudskillsboost.google/course_templates/435/video/427982)

* [YouTube: Networking and service discovery](https://www.youtube.com/watch?v=fV8L7BGUbZg)

We start by discussing the need of a service mesh. Let’s recap how Kubernetes networking works. If you want to route traffic to your workloads, you first need to create a Kubernetes Service. This Service is issued a static Virtual IP address. When a request hits that Service VIP, the request is forwarded to pods under management using a round-robin strategy. Services use selectors, where they describe which labels pods should have in order to receive traffic from that service. Once a service is configured, an Endpoint is created for each one of the pods under management. Consider the Kubernetes networking example shown here. Notice how the selector in the Service is pointing to the same labels defined in the Deployments. On the slide, these are all highlighted. That way requests are routed from the Service VIP into the pods. All Kubernetes resources including deployments, pods, services, and endpoints are stored in the Kubernetes control plane’s etcd store. In order to direct traffic within your mesh, Anthos Service Mesh (or ASM) needs to know where all your endpoints are, and which services they belong to. To populate its own service registry, ASM connects to the service discovery system of the underlying platform such as Kubernetes, Consul, or plain DNS. For example, if you’ve installed ASM on a Kubernetes cluster, then ASM automatically detects the services and endpoints in that cluster. The service mesh control plane adapters read the Kubernetes topology under it and register it in a service registry. All that information is propagated to the Envoy proxies, where load balancing is done against a service’s endpoints. Once the proxy chose a specific endpoint, the packet travels directly to it, subject to policy authorization from the Envoy Filters. Anthos Service Mesh offers advanced traffic management features. It lets you easily control the flow of traffic and API calls between services with much greater granularity than standard Kubernetes. It also simplifies configuration of service-level properties like circuit breakers, timeouts, and retries, and makes it easy to set up important tasks like A/B testing, canary rollouts, and staged rollouts with percentage-based traffic splits. Beyond that, it provides out-of-box failure recovery features that help make your application more robust against failures of dependent services or the network.

### Video - [Anthos Service Mesh API resources](https://www.cloudskillsboost.google/course_templates/435/video/427983)

* [YouTube: Anthos Service Mesh API resources](https://www.youtube.com/watch?v=CRpMYqFHrXk)

Let’s learn how we can use the Anthos Service Mesh API resources to configure routing. The same way as other Kubernetes extensions, Anthos Service Mesh is configured by specifying Kubernetes custom resource definitions, or CRDs, which you can configure using YAML. The mesh control plane component is responsible for reading those configurations from Kubernetes and sharing them with the Envoy proxies and does so via the Envoy xDS API. Let’s take a look at the service mesh resources. You can think of the VirtualService as the ‘Where’ and the Destination Rule as the ‘How’. A Virtual Service: Defines how requests for a service are routed within an Istio service mesh. Can use routing rules to determine the destination to which a request should be sent. Can use rules to route traffic based on things like request headers, target host name, url path, etc. Can use target destinations as a service in the mesh registry using proxies, a subset of the service (specific group of pods), or a non-proxied service registered via a ServiceEntry. The Destination Rule: Defines how traffic aimed at a particular destination gets handled. Can break a single service into multiple subsets - in other words, subcollections of pods using labels to select. The VirtualService can then route to the subset. Can specify load balancing behavior within the destination - random, round robin, weighted, etc. And it can specify TLS security mode, circuit breaker settings, etc. Thus, a request coming from a client will select the pod to connect to by using both the VirtualService settings (to pick a destination) and DestinationRule settings (to select how to connect to that destination). Gateways Gateways are used to manage inbound and outbound traffic for the mesh. The configuration settings are applied to a deployment on Envoy poxy pods running on the edge of the mesh, not as sidecars to particular workloads. A gateway can, for instance, allow incoming HTTP traffic for a specific host. You then bind a VirtualService to the Gateway, and the Gateway uses the VirtualService information to route the incoming request to the correct destination. The Service Entry is commonly used to enable requests to services outside of an Istio service mesh. Sidecar configurations are used to fine-tune Envoy proxy settings. By default, sidecar proxies are configured to accept traffic on all ports used by workload, and can reach every workload in the mesh. If you want to limit the protocols, ports, or reachable services in the mesh, you do this with sidecar configurations. By default, the Envoy proxies distribute traffic across each service’s load balancing pool using a round-robin model, where requests are sent to each pool member in turn, returning to the top of the pool once each service instance has received a request. You can improve this behavior with what you know about the workloads. For example, some might represent a different version. This can be useful in A/B testing, where you might want to configure traffic routes based on percentages across different service versions, or to direct traffic from your internal users to a particular set of instances. You configure traffic behavior with the VirtualService. The virtual service hostname can be an IP address, a DNS name, or, depending on the platform, a short name (such as a Kubernetes service short name) that resolves, implicitly or explicitly, to a fully qualified domain name, or FQDN. Using short names like this only works if the destination hosts and the virtual service are actually in the same Kubernetes namespace. Because using the Kubernetes short name can result in misconfigurations, we recommend that you specify fully qualified host names in production environments. You can also use wildcard prefixes, for example, *. myapp.com, letting you create a single set of routing rules for all matching services. Virtual service hosts don’t actually have to be part of the Istio service registry, they are simply virtual destinations. This lets you model traffic for virtual hosts that don’t have routable entries inside the mesh. In this example, requests that contain the user Jason are sent to v2, while all traffic that doesn’t match this condition is sent to v3, as rules are evaluated in sequential order from top to bottom. VirtualService offers different routing options. In this example, we use the conditional syntax with a regex expression in order to inspect the user-agent HTTP header and direct iPhone users to a specific service that handles their workload. You can also route traffic based on HTTP header, URIs, Ports, and sourceLabels. VirtualServices can also modify the traffic by injecting faults to test how your application performs under an unstable environment, and duplicate the traffic with traffic mirroring to test shadow deployments or analyze traffic to detect network intrusions. A DestinationRule is usually coupled with a VirtualService. Once the routing has been determined by the VirtualService, the destination rule determines how to route the traffic. In particular, you use destination rules to specify named service subsets, such as grouping all a given service’s instances by version. You can then use these service subsets in the routing rules of virtual services to control the traffic to different instances of your services. Each subset is defined based on one or more labels, which in Kubernetes are key/value pairs that are attached to objects such as pods. These labels are applied in the Kubernetes Service’s Deployment as metadata to identify different versions. DestinationRules offer different routing strategies. For example, it configures load balancing, with round-robin, random, least request, and pass-through, and allows you to configure session affinity, so that the clients get consistently routed to the same service. DestinationRules also allow you to configure circuit-breaking capabilities. Circuit breaking allows you to write applications that limit the impact of failures, latency spikes, and other undesirable effects of network peculiarities. For example, you can set the connections pool size from the sidecar to limit 100 connection to a Redis service, or configure outlier detection settings to detect and evict unhealthy hosts from the load balancing pool. Having multiple destinations in the route attribute provides traffic splitting functionality to your Virtual Service. A subset defines any subset of a service, either a canary, A/B or blue-green subsets. Notice that the labels in the destination rule must match your pod labels for this configuration to take effect. Also, the subset field in the VirtualService must match the subsets in the DestinationRule. Instead of depending on the number compute instances in the infrastructure to split traffic, we can rely on the traffic splitting functionality and set a specific percentage of traffic regardless of the number of compute instances in each subset, resulting in traffic control that is decoupled from infrastructure scaling. You use a service entry to add an entry to the service registry that the service mesh maintains internally. After you add the service entry, the Envoy proxies can send traffic to the service as if it was a service in your mesh. Configuring service entries allows you to manage traffic for services running outside of the mesh, including the following tasks: Redirect and forward traffic for external destinations, such as APIs consumed from the web, or traffic to services in legacy infrastructure. Define retry, timeout, and fault injection policies for external destinations. Run a mesh service in a Virtual Machine, or VM, by adding VMs to your mesh. Logically add services from a different cluster to the mesh to configure a multicluster Istio mesh on Kubernetes. You don’t need to add a service entry for every external service that you want your mesh services to use. By default, Anthos Service Mesh configures the Envoy proxies to passthrough requests to unknown services. However, you can’t use Anthos Service Mesh features to control the traffic to destinations that aren’t registered in the mesh. Along with support for Kubernetes Ingress, Anthos Service Mesh offers Gateways to manage inbound and outbound traffic to the mesh. A Gateway provides more extensive customization and flexibility than Ingress, and allows service mesh features such as monitoring and route rules to be applied to traffic entering the cluster. You use a gateway to manage inbound and outbound traffic for your mesh, letting you specify which traffic you want to enter or leave the mesh. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads. Gateways let you configure layer 4-6 load balancing properties such as ports to expose, TLS settings, and so on. Then, instead of adding application-layer traffic routing (L7) to the same API resource, you bind a VirtualService to the gateway. This lets you basically manage gateway traffic like any other data plane traffic in an Anthos Service Mesh. There are three main types of gateways: Ingress gateways: describe a load balancer operating at the edge of the mesh receiving incoming HTTP/TCP connections. Egress gateways: represent a dedicated exit node for the traffic leaving the mesh, letting you limit which services can or should access external networks, or to enable secure control of egress traffic to add security to your mesh, for example. You can also use a gateway to configure a purely internal proxy. East-west gateways are proxies for east-west traffic to allow service workloads to communicate across cluster boundaries in a multi-primary mesh on different networks. Bind your VirtualService to a Gateway to route inbound traffic from outside the mesh. That way, the Gateway can provide routing configuration for layers 4-6 while the VirtualService can be configured a the application layer or L7. In this example, the Gateway enables HTTPS connection and links to an external certificate. The link takes place in the VirtualService, specifically in the field called gateways, where it links to the bookinfo-gateway. Also, notice that hosts is set to asterisk, which means traffic to any destination. When the incoming traffic enters from the Gateway, the VirtualService evaluates the conditions set under the match attribute and routes the traffic based on the route attribute. The host attribute, on spec.http.match.route.destination.host is actually the Kubernetes service. The istio-ingressgateway is a Kubernetes deployment and Kubernetes Service of type LoadBalancer. Therefore, when the ingress gateway is created, Google creates a Google Cloud Load Balancer. The Load Balancer forwards the traffic to a NodePort on a worker node, and the worker node forwards the traffic to the ingress gateway pod which handles ingress functionality. The ingress gateway deployment is comprised of a single Envoy container per Pod. That Envoy is configured with VirtualServices and DestinationRules, the same way as any other Envoys in the mesh. Once the Ingress Gateway’s Envoy processes the request, it forwards it to the right destination. You can also configure Egress Gateways for controlling the traffic leaving your mesh. Consider an organization that has a strict security requirement that all traffic leaving the service mesh must flow through a set of dedicated nodes. These nodes run on dedicated machines, separated from the rest of the nodes running applications in the cluster. These special nodes serve for policy enforcement on the egress traffic and are monitored more thoroughly than other nodes. Another use case is a cluster where the application nodes don’t have public IPs, so the in-mesh services that run on them cannot access the internet. Defining an egress gateway, directing all the egress traffic through it, and allocating public IPs to the egress gateway nodes allows the application nodes to access external services in a controlled way. There are four steps required to configure an Egress Gateway: Add a ServiceEntry to the external location. For example, imagine you want to access the Google Sheets API, you would point the host field to sheets.google.com. Create a Gateway resource, using the same host as the ServiceEntry. Optionally, specify routing configuration or load balancing strategies using VirtualServices and DestinationRules. You can also manage Sidecar configurations. By default, Anthos Service Mesh configures every Envoy proxy to accept traffic on all the ports of its associated workload, and to reach every workload in the mesh when forwarding traffic. You can use a sidecar configuration to do the following: Fine-tune the set of ports and protocols that an Envoy proxy accepts. Limit the set of services that the Envoy proxy can reach. You might want to limit sidecar reachability like this in larger applications, where having every proxy configured to reach every other service in the mesh can potentially affect mesh performance due to high memory usage. You can specify that you want a sidecar configuration to apply to all workloads in a particular namespace, or choose specific workloads using a workloadSelector. For example, the following sidecar configuration configures all services in the bookinfo namespace to only reach services running in the same namespace and the mesh control plane.

### Video - [Network resilience and testing](https://www.cloudskillsboost.google/course_templates/435/video/427984)

* [YouTube: Network resilience and testing](https://www.youtube.com/watch?v=8vWf4OxtkI4)

As well as helping you direct traffic around your mesh, Anthos Service Mesh provides opt-in failure recovery and fault injection features that you can configure dynamically at runtime. Using these features helps your applications operate reliably, ensuring that the service mesh can tolerate failing nodes and preventing localized failures from cascading to other nodes. A timeout is the amount of time that an Envoy proxy should wait for replies from a given service, ensuring that services don’t hang around waiting for replies indefinitely and that calls succeed or fail within a predictable timeframe. A timeout that is too long could result in excessive latency from waiting for replies from failing services, while a timeout that is too short could result in calls failing unnecessarily while waiting for an operation involving multiple services to return. To find and use your optimal timeout settings, VirtualServices lets you easily adjust timeouts dynamically on a per-service basis without having to edit your service code. Here’s an example that specifies a 10-second timeout for calls to the service_b service. Notice that Envoy timeouts for HTTP requests are disabled by default. A retry setting specifies the maximum number of times an Envoy proxy attempts to connect to a service if the initial call fails. Retries can enhance service availability and application performance by making sure that calls don’t fail permanently because of transient problems such as a temporarily overloaded service or network. The interval between retries (25 milliseconds or higher) is variable and determined automatically by Anthos Service Mesh, preventing the called service from being overwhelmed with requests. The default retry behavior for HTTP requests is to retry twice before returning the error. Like timeouts, Anthos Service Mesh’s default retry behavior might not suit your application needs in terms of latency or availability - for example, too many retries to a failed service can slow things down. Also like timeouts, you can adjust your retry settings on a per-service basis in virtual services without having to touch your service code. You can also further refine your retry behavior by adding per-retry timeouts, specifying the amount of time you want to wait for each retry attempt to successfully connect to the service. This example configures a maximum of three retries to connect to this service subset after an initial call failure, each with a 2-second timeout. Circuit breakers are another useful mechanism for creating resilient microservice-based applications. In a circuit breaker, you set limits for calls to individual hosts within a service, such as the number of concurrent connections or how many times calls to this host have failed. Once that limit has been reached, the circuit breaker “trips” and stops further connections to that host. Using a circuit breaker pattern enables fast failure rather than clients trying to connect to an overloaded or failing host. As circuit breaking applies to “real” mesh destinations in a load balancing pool, you configure circuit breaker thresholds in destination rules, with the settings applying to each individual host in the service. This example limits the number of concurrent connections for the reviews service workloads of the v1 subset to 100. Also, you can configure outlier detection settings to detect and evict unhealthy hosts from the load balancing pool. In this example, we can see that if the reviews service returns three consecutive 500 errors in a 1-second interval, the service will be ejected for 2 minutes. Up to 50% of all services can be evicted in this example. Inject faults to test the resiliency of your application without instrumentation. Delay requests are applied before forwarding the request from the calling Envoy, emulating various failures such as network issues, overloaded upstream service, etc. This example introduces a 5-second delay in one out of every 1000 requests. The fixedDelay field is used to indicate the amount of delay in seconds. The optional percentage field can be used to only delay a certain percentage of requests. If left unspecified, all request will be delayed. Abort http request attempts and return error codes back to downstream service, giving the impression that the upstream service is faulty. This example introduces a 400 HTTP status code to return to the caller. The optional percentage field can be used to only abort a certain percentage of requests. If not specified, all requests are aborted. Delay and abort faults are independent of one another, even if both are specified simultaneously. Shadow deployments is a powerful concept that allows feature teams to bring changes to production with as little risk as possible. Mirroring sends a copy of live traffic to a mirrored service. The mirrored traffic happens out of band of the critical request path for the primary service. Traffic analysis is another interesting use case. Send a copy of your live traffic to an analytics service to detect intrusions in the network, discover unauthorized traffic paths or find unsecure plain text traffic. In this example, we are mirroring 100% of the traffic going to v1 to v2.

### Video - [Lab intro: Managing Traffic Flow with Anthos Service Mesh](https://www.cloudskillsboost.google/course_templates/435/video/427985)

* [YouTube: Lab intro: Managing Traffic Flow with Anthos Service Mesh](https://www.youtube.com/watch?v=BRpk9TIpyys)

In this lab, you learn how to manage service discovery, traffic routing, and load balancing for your services without having to update code in your services. You will perform the following tasks: Configure and use Istio Gateways Apply default destination rules, for all available versions Apply virtual services to route by default to only one version Route to a specific version of a service based on user identity Shift traffic gradually from one version of a microservice to another Use the Anthos Service Mesh dashboard to view routing to multiple versions Setup networking best practices such as retries, circuit breakers and timeouts

### Lab - [AHYBRID051 Managing Traffic Flow with Cloud Service Mesh](https://www.cloudskillsboost.google/course_templates/435/labs/427986)

Architecting Hybrid Infrastructure with CSM: Configure fine-grained traffic control for routing to services.

* [ ] [AHYBRID051 Managing Traffic Flow with Cloud Service Mesh](../labs/AHYBRID051-Managing-Traffic-Flow-with-Cloud-Service-Mesh.md)

### Quiz - [Test your knowledge](https://www.cloudskillsboost.google/course_templates/435/quizzes/427987)

#### Quiz 1.

> [!important]
> **An Ingress Gateway runs as**
>
> * [ ] A Kubernetes deployment.
> * [ ] A Kubernetes DaemonSet.
> * [ ] The Google Router.
> * [ ] The DNS service in your cluster.

#### Quiz 2.

> [!important]
> **How does service discovery work in an Anthos Service Mesh?**
>
> * [ ] Mesh learns the services in the network using machine learning.
> * [ ] You must configure a ServiceEntry for each service you create.
> * [ ] Mesh control plane reads the Kubernetes endpoints and services.
> * [ ] You have to tell the mesh which services are available.

#### Quiz 3.

> [!important]
> **You can reduce errors in your applications with the help of Anthos Service Mesh by (select all that apply)**
>
> * [ ] Mirroring traffic to new versions.
> * [ ] Doing canary deployments using weights in your VirtualServices.
> * [ ] Setting connection pools on mesh services.
> * [ ] Introducing retries in the Envoy proxies.

## Securing Network Traffic with Anthos Service Mesh

In this module, you will learn how to use Anthos Service Mesh to secure comunications with services - both from users and from other services.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/435/video/427988)

* [YouTube: Module overview](https://www.youtube.com/watch?v=XxXjet_YsaM)

Welcome to the introduction of Anthos Service Mesh. In this module, you learn to: Encrypt traffic between microservices to avoid anyone in the network gaining access to private information. Authenticate services and requests, verifying trust among services in the mesh as well as end users. Authorize services and requests, making sure services only access the information that is allowed access from other services. Limit service access in the network, so that granular controls over the communication can be established. This is our agenda for the module, shown on the slide. Let’s get started.

### Video - [Security across services](https://www.cloudskillsboost.google/course_templates/435/video/427989)

* [YouTube: Security across services](https://www.youtube.com/watch?v=I_LUpDNFGjk)

We start by describing why a service mesh is useful. Breaking down a monolithic application into atomic services offers various benefits, including better agility, better scalability, and better ability to reuse services. However, microservices also introduce security concerns: Unintended services might read traffic flowing in the network. Services might impersonate other services and perform a person-in-the-middle attack. Services might access, create, or modify private or confidential information from other services. Services might disrupt other services by performing denial of service attacks. To solve those issues, we should have some basic network security goals: Traffic between microservices should be encrypted, that way avoiding anyone in the network gaining access to information. Services should be authenticated, being able to verify the service you are communicating with to establish trust. Services should be authorized, so that services only access the information that is allowed access from other services. Once our services are authenticated and authorized, we might choose to add other access limitations such as quotas to control the load.

### Video - [Authentication and encryption](https://www.cloudskillsboost.google/course_templates/435/video/427990)

* [YouTube: Authentication and encryption](https://www.youtube.com/watch?v=JTi3PAiiN84)

We saw the importance of having security goals in a network. Let’s see how authentication and encryption work. Let’s conceptualize how services communicate. Service A sends data over the network to Service B. However, this communication does not happen directly, but rather, there might be a web of intermediaries such as routers, switches, or other servers. All these intermediaries were able to read the data and even modify it, and Service A cannot be sure that the original request actually arrived at Service B To ensure that data is not read by intermediaries, we can use the Transport Layer Security protocol, or TLS, to encrypt it. To perform this process, we use two keys. A public key is used to encrypt data, and a private key is used to decrypt data. Server A will use the public key from Server B to encrypt the message. Then Server B will use its own private key to decrypt the received message. This process is called asymmetric encryption since we are only encrypting the sent message. However, in a microservice architecture, we are going to have services communicating both ways, so traffic must also be encrypted from Service B to Service A. Therefore, when Service A establishes the connection with Service B, it will share a symmetric key, which both services can use to encrypt and decrypt messages. Since services A and B are the only who have access to the symmetric key, nobody else can decrypt the information. There’s another problem though. What if an intermediate computer intercepts the traffic and claims to be Service B? We need a way to ensure that Service B, which is providing the public key, is really Service B. Part of the TLS standard is also designed to solve this problem. Specifically, when Service A first tries to establish an encrypted connection, it not only shares a symmetric key but also asks Service B for a certificate of its identity, in the form of an X.509 certificate. Service A then asks a trusted adviser called a Certificate Authority (or CA) that proves that the certificate is valid and proceeds based on what the CA says. This is how websites running over HTTPs work. Since we need authentication and encryption both ways, when Service A wants to establish the connection with Service B, both certificates are exchanged and checked against the Trusted CA and the symmetric key is used to encrypt the messages. The process of authenticating services and encrypting messages both ways is called mutual TLS or mTLs.

### Video - [Service authentication in the mesh](https://www.cloudskillsboost.google/course_templates/435/video/427991)

* [YouTube: Service authentication in the mesh](https://www.youtube.com/watch?v=jxCvfF_MR_U)

Now that we have seen how authentication and encryption work in general, let’s see specifically how it’s implemented in Anthos Service Mesh. Anthos Service Mesh offers a managed CA and tools to control the communication protocol. When pods are created, Mesh CA shares the certificates and keys that are later used to verify identity in the network. Admins control the communication protocol using destination rules and authentication policies. Those policies are applied to the sidecar proxies, which work as Policy Enforcement Endpoints, or PEPs, and secure the communication between client and server. That way, the traffic can be secured with mTLS between services in the mesh. Mesh CA is the Certificate Authority in a service mesh and it is responsible for distributing, rotating, and managing certificates to provide identity. The provisioning flow looks as follows: The pod gets created and the sidecar proxy is injected. The sidecar contains the Envoy proxy and the istio agent, which is responsible for communicating with the control plane over gRPC. The istio agent issues a certificate signing request, or CSR, to the control plane, which communicates with Mesh CA. 2. Mesh CA sends the X.509 certificates to the istio agent. 3. The Istio agent sends the certificates received from Mesh CA and the private key to Envoy via the Envoy SDS API. 4. The istio agent monitors the expiration of the certificate and the process repeats periodically for certificate and key rotation. Secure naming is a mesh feature that enables envoys to check the server’s identity against the certificate information to see if it is an authorized runner of the workload. Suppose that a malicious user successfully hijacked the traffic sent to Service B and redirected it to the forged server. In this example, we see a hijacked DNS service, but an attacker could have also hijacked the BGP protocol or spoofed the ARP protocol. When the services try to establish the connection, the proxy secure naming feature will not be able to verify the identity of the certificate provided by pod 3 and the connection will not be performed. Administrators specify the protocol used in the mesh by setting up destination rules and peer authentication policies. Destination rules are applied to specific services and dictate protocol to be used, that is TLS, mTLS, or plain text. Peer authentication policies are applied mesh-wide or to a specific namespace or workload and enforce a protocol to be used. Destination rules dictate the protocol to be used for a specific workload and you can specify one of the following options: ISTIO_MUTUAL: offers a secure connections to the upstream using mutual TLS by presenting the default client certificates for authentication. MUTUAL: is the same as ISTIO_MUTUAL but with custom certificates. SIMPLE: originates a TLS connection to the upstream endpoint. DISABLE: does not set up a TLS connection to the upstream endpoint. Peer authentication policies dictate the level of TLS performed in the mesh: Workload-specific policies are specified with a namespace and selector. Namespace-wide have a namespace, but no selector. And mesh-wide don’t have a namespace, and can have a selector. If there are conflicts across policies, the workload-specific will take priority, next the namespace-specific, and finally the mesh-wide. If there are multiple policies in one of the levels, let’s say there are two mesh-wide policies, the oldest one will take effect. You can also specify the level of TLS enforcement: STRICT enforces mTLS and, thus, client authentication. If a workload cannot perform mTLS, the communication won’t be performed. PERMISSIVE: defaults to the maximum amount of TLS by all clients, allowing to have plaintext connections. This is used when you are first rolling out a mesh, to allow teams to enable TLS on their workloads. DISABLE: TLS is disabled. This option is not recommended.

### Video - [End-user authentication in the mesh](https://www.cloudskillsboost.google/course_templates/435/video/427992)

* [YouTube: End-user authentication in the mesh](https://www.youtube.com/watch?v=BD2iXlpGAGc)

In addition to service authentication, we also have request authentication, which is used to authenticate end users. Request authentication works using JSON Web Tokens, or JWT. The end user performs authentication using a custom authentication provider or any OpenID Connect providers, for example, Auth0, ORY Hydra, Firebase Auth, or Keycloak. This provider returns the JWT token which is included in subsequent requests to services in the mesh. The gateway’s sidecar proxy checks and validates the token before authenticating the request. Request authentication policies verify the caller’s identity using JSON Web Token, or JWT. The values in these policies include: The location of the token in the request. The issuer or the request. The public JSON Web Key Set, or JWKS. Requests are rejected if the authentication information is invalid. Requests without identity are accepted but won’t be authenticated. Request authentication policies can accept multiple JWTs from different providers, but requests must only have one valid token, otherwise, the output principle of the request is undefined.

### Video - [Authorization in the mesh](https://www.cloudskillsboost.google/course_templates/435/video/427993)

* [YouTube: Authorization in the mesh](https://www.youtube.com/watch?v=BzbKCVC--TU)

Once the request has been authenticated, we can use this information to authorize the request. Sidecar and perimeter proxies work as Policy Enforcement Points, or PEPs, to authorize requests. When a request comes to the proxy, the authorization engine evaluates the request context against the current authorization policies, and returns the authorization result, either ALLOW or DENY. Operators specify the mesh authorization policies using . yaml files. Authorization policies define access control for mesh workloads and are applicable both workload-to-workload and end-user-to-workload. They are defined using a single AuthorizationPolicy CRD, which provides flexible semantics with custom conditions and CUSTOM, DENY, and ALLOW actions. Those conditions are enforced natively on Envoy, achieving high performance. Authorization policies support a range of protocols such as gRPC, HTTP, HTTPS, and HTTP/2 natively, and any TCP protocol. Let’s look at the structure of this CRD. An Authorization Policy includes a selector, an action, and a list of rules: The selector field specifies the workloads affected by the policy. The action field specifies whether to allow or deny requests. The rules specify when to trigger an action. The from field in the rules specifies sources of requests. The to field in the rules specifies the methods and paths allowed/denied. Notice that we apply this rules to layer 7 networking, while in Kubernetes, we can only apply network policies at layer 4. The when field specifies the conditions that would cause a request to match the rule.

### Video - [Bonus: Employee authentication and authorization in the mesh](https://www.cloudskillsboost.google/course_templates/435/video/427994)

* [YouTube: Bonus: Employee authentication and authorization in the mesh](https://www.youtube.com/watch?v=ixH_zPLAWhM)

Employee authentication is a specialized use case of end-user authentication. In Google Cloud, you can use Identity Aware Proxy, or IAP, as the authentication and authorization provider for employees of your organization. That way, you allow access to internal applications without relying on network-level firewalls or the need of a VPN. With IAP, you can establish a central authorization layer for applications accessed by HTTPS with group-based controls, so that departments are granted individual access to different applications. Requests come through Cloud Load Balancing (HTTPS), or internal HTTP load balancing. The serving infrastructure code for these products checks if IAP is enabled for the app or backend service. If IAP is enabled, information about the protected resource is sent to the IAP authentication server. This includes information like the Google Cloud project number, the request URL, and any IAP credentials in the request headers or cookies. Next, IAP checks the user's browser credentials. If none exist, the user is redirected to an OAuth 2.0 Google Account sign-in flow that stores a token in a browser cookie for future sign-ins. If you need to create Google Accounts for your existing users, you can use Google Cloud Directory Sync to synchronize with your Active Directory or LDAP server. If the request credentials are valid, the authentication server uses those credentials to get the user's identity (email address and user ID). After authentication, IAP applies the relevant IAM policy to check if the user is authorized to access the requested resource. If the user has the IAP-secured Web App User role on the Cloud Console project where the resource exists, they're authorized to access the application. To manage the IAP-secured Web App User role list, use the IAP panel on the Cloud Console. When you turn on IAP for a resource, it automatically creates an OAuth 2.0 client ID and secret. If you delete the automatically generated OAuth 2.0 credentials, IAP won't function correctly. You can view and manage OAuth 2.0 credentials in the Cloud Console APIs & services. Create an ingress resource to configure an HTTPS Load Balancer. Google-managed SSL certificates are provisioned, renewed, and managed for your domain. The Ingress resource then forwards the request to the mesh ingress gateway, which then forwards the request to your app via the VirtualService configuration. RequestAuthentication configures IAP as the request authentication method. By default, IAP generates a JSON Web Token, or JWT, that is scoped to the OAuth client. For Anthos Service Mesh, you can configure IAP to generate a RequestContextToken, or RCToken, which is a JWT but with a configurable audience. RCToken lets you configure the audience of the JWT to an arbitrary string, which can be used in the Anthos Service Mesh policies for fine-grained authorization.

### Video - [Lab intro: Securing Traffic with Anthos Service Mesh](https://www.cloudskillsboost.google/course_templates/435/video/427995)

* [YouTube: Lab intro: Securing Traffic with Anthos Service Mesh](https://www.youtube.com/watch?v=YTAxW4GgChU)

Next, you will do a lab exercise to get some practice with the Anthos Service Mesh. In the lab, you perform the following tasks: Enforce STRICT mTLS mode across the service mesh Enforce STRICT mTLS mode on a single namespace Explore the security configurations in the Anthos Service Mesh Dashboard Add authorization policies to enforce access based on a JSON Web Token (JWT) Add authorization policies for HTTP traffic in an Istio mesh

### Lab - [AHYBRID061 Securing Traffic with Anthos Service Mesh](https://www.cloudskillsboost.google/course_templates/435/labs/427996)

Secure mesh traffic using Authenication and Authorization policies

* [ ] [AHYBRID061 Securing Traffic with Anthos Service Mesh](../labs/AHYBRID061-Securing-Traffic-with-Anthos-Service-Mesh.md)

### Quiz - [Test your knowledge](https://www.cloudskillsboost.google/course_templates/435/quizzes/427997)

#### Quiz 1.

> [!important]
> **Authorization policies define access control for mesh workloads using the following mechanisms: (select three)**
>
> * [ ] Check that service accounts are allowed.
> * [ ] Check that specific API endpoints are allowed.
> * [ ] Check that specific times of the days requests are allowed.
> * [ ] Check that specific networks are allowed.

#### Quiz 2.

> [!important]
> **Peer authentication policies should be set to permissive to**
>
> * [ ] Slowly adopt mTLS across the mesh without breaking the current experience.
> * [ ] Enforce TLS communication.
> * [ ] Never; strict is recommended.
> * [ ] Allow external communication to the mesh.

#### Quiz 3.

> [!important]
> **Each request is checked against the Mesh CA to establish trust between the client and the server.**
>
> * [ ] False
> * [ ] True.

## Multi-Cluster Networking with Anthos Service Mesh

In this module, you learn how to implement multi-cluster networking, including in multi-cloud and hybrid environments.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/435/video/427998)

* [YouTube: Module overview](https://www.youtube.com/watch?v=qHoPkE4qkPY)

Welcome to Multi-cluster Networking with Anthos Service Mesh. In this module, you: Understand how to do multi-cluster networking, both north-south as well as east-west routing with different network configurations. Install Anthos Service Mesh on different Anthos GKE clusters and choose the right network configuration depending on where you want to run your cluster. Learn how to configure east-west networking on different Anthos clusters running on multi-cloud and hybrid locations with Anthos Service Mesh. Combine Anthos Service Mesh with Multi-Cluster Gateways and Multi-Cluster Services to seamlessly run distributed services. This is our agenda for the module, shown here on the slide. Let’s get started.

### Video - [Fleet networking](https://www.cloudskillsboost.google/course_templates/435/video/427999)

* [YouTube: Fleet networking](https://www.youtube.com/watch?v=o2kuDPlkLMg)

Let’s start reviewing concepts on fleet networking. There are two ways of communicating between Anthos clusters, and in most cases, you will be using both approaches. North-South routing handles the communication from a load balancer into our clusters. That load balancer can be receiving traffic from the internet or from some internal applications. East-West routing handles the communication between clusters. This is important for use cases, such as having dependencies in other clusters, implementing fall-back strategies, or performing blue-green deployments. In this module, we focus primarily on East-West routing routing. North-South is covered in depth in the Multi-Cloud, Multi-Cluster with Anthos course.

### Video - [Single network east-west routing](https://www.cloudskillsboost.google/course_templates/435/video/428000)

* [YouTube: Single network east-west routing](https://www.youtube.com/watch?v=zwk2WAzyaOQ)

Let’s start by discussing East-West routing In multi-cluster mesh networking there are different ways to configure Anthos Service Mesh. The main differences are dependent on the network and the type of cluster. First, we discuss a single network with clusters running on Google Cloud with Google Kubernetes Engine. Let’s set up a multi-cluster mesh on GKE using a single subnet in a single VPC network. First, create and register your GKE clusters to Anthos Connect Hub. Install Anthos Service Mesh with Mesh CA on both clusters. In order for each cluster to route requests to services in the other cluster, the clusters need to have a list of endpoints from the other cluster. To discover the endpoints, clusters must be registered to the same fleet, and must be able to access the other cluster's API server for endpoint enumeration. The “asmcli create mesh” command registers your clusters and creates a secret with the other cluster’s API server. If you want to access your services from outside the mesh, you might want to create an ingress gateway. Ingress routing into the cluster is called north-south routing. In this diagram, the ingress gateway only appears in one cluster, but you could also have one ingress gateway per cluster. Then, you could use Multi-cluster Gateways and Multi-cluster Services to balance requests from Google’s load balancer into one of the gateways. We discuss this option later in the module. That’s it - label namespaces for automatic sidecar injection and deploy your application across clusters. Services in one cluster will automatically be able to communicate with services deployed on the other cluster. The second variant uses multiple projects to host your infrastructure. This is a common approach for large teams that want to have better controls on permissions and cost. To keep the communication secure, you should set up peering between both networks, which allows communication using private IPs. This configuration is the same as the first variant, since peered networks act as different subnets of the same network. Therefore, you would need to create firewall rules on both projects allowing communication between all CIDR blocks in both clusters. Let’s look at some variants. In the first variant, we move from clusters in the same region to clusters in multiple regions, which improves availability and reduces latencies for end customers. When clusters were in the same region, they were also in the same subnet and traffic could flow between the nodes because GKE automatically adds firewall rules on creation. However, when you put GKE clusters in different regions, and therefore different subnets, you must explicitly set up the firewall rules to allow cross-subnet traffic. The firewall rules should include all CIDR ranges in the cluster, including nodes, services, and pods. The third variant expands on the controls that we introduced in variant 2. In addition to separating the infrastructure into different projects, you centralize all network configuration on a Host project, and create a Shared VPC so that service teams in other projects can launch their infrastructure using that common network. In this configuration, a centralized networking or security team must create firewall rules on the host project that enable communication between all CIDR ranges in both clusters. Also, make sure that all clusters are registered to the same Anthos Fleet in the host project. Variant 4 introduces private clusters, a security best practice that limits the Kubernetes API server accessibility to services on the same VPC network. However, this brings some challenges because clusters are no longer able to communicate with each other. To make that communication possible, we need to perform three actions. First, configure endpoint discovery. Previously, we used the “asmcli create mesh” command, which created a secret in each other’s cluster with the public Kubernetes API server IP. Now, we must get the private Kubernetes API server IP from each cluster and create a secret pointing to that IP in the other cluster. That way, clusters are able to read the pod endpoints registered in the remote cluster. Second, the Anthos Service Mesh control plane in each cluster needs to call the GKE control plane of the remote clusters. To allow traffic, you need to add the pod address range in the calling cluster to the authorized networks of the remote clusters. Third, enable control plane global access to allow the Anthos Service Mesh control plane in each cluster to call the GKE control plane of the remote clusters.

### Video - [Multiple network east-west routing](https://www.cloudskillsboost.google/course_templates/435/video/428001)

* [YouTube: Multiple network east-west routing](https://www.youtube.com/watch?v=z-1ihdfTvHs)

We covered how to configure multiple GKE clusters in the same network. Next, let’s cover the additional steps needed to make the east-west communication work on different networks without a VPN connection, which might take place when we deploy clusters in multi-cloud or hybrid environments. First, let’s take a look at this diagram. We have two clusters in two different projects, and therefore, two VPC networks which are not peered. That means, if you want to communicate between clusters, you will have to create public IPs. Do you see any new component in the diagram that we did not have before? The East-West Gateway is the new component in the diagram. This gateway proxies all communication to services on another cluster and makes the communication transparent, so that you don’t need to worry where your services are located. To make this possible, you must expose all services on the East-West Gateway. While this gateway is public on the internet, services behind it can only be accessed by services with a trusted mTLS certificate and workload ID, just as if they were on the same network. However, production systems may require additional access restrictions, for example, via firewall rules, to prevent external attacks. The first variant of the multi-cluster mesh on separate networks shows the way to deploy a mesh across clusters located both inside and outside of Google Cloud. In this example, we see an Anthos GKE cluster on Google Cloud and an Anthos cluster on VMware. The installation commands vary but the steps are the same. Refer to the documentation for the specific commands. The major difference lies in the control plane, as we can no longer use the Google managed control plane and must default to the in-cluster control plane with istiod. Also, Mesh CA can be used but it does not work with CA Service as the root CA. The second variant shows a diagram of a mesh spanning across an Anthos GKE cluster and an Attached Anthos cluster running on a customer’s location. Remember that attached clusters are Anthos compatible clusters that are managed by you and have been registered to Anthos Connect Hub. Examples of these clusters include Amazon EKS, Microsoft AKS, Red Hat Openshift, both OKE and OCP, KIND, K3s, and K3d. The support for these clusters varies depending on the attached cluster you are connecting and you might need to downgrade the version of Anthos Service Mesh. The biggest change with respect to the first variant is the usage of an in-cluster Istio CA instead of Mesh CA. Since you will have multiple Certificate Authorities, you need to create a Root CA that will establish trust between the two clusters.

### Video - [North-south routing](https://www.cloudskillsboost.google/course_templates/435/video/428002)

* [YouTube: North-south routing](https://www.youtube.com/watch?v=t_UNerzhT9E)

Finally, let’s cover north-south routing. Remember this means distributing the traffic from the load balancer into the clusters. We only cover the basics here, if you want a deep dive, refer to the Fleet Networking module in the Multi-Cluster, Multi-Cloud with Anthos course. How can we distribute traffic from the load balancer into multiple clusters? You can use Multi-Cluster Gateway and Multi-Cluster Service (MCS) for multi-cluster load balancing. Let’s see how it works. First, create a new GKE cluster and register it into Anthos Hub. This cluster is used to host all cross-cluster configurations. 2. Create a multi-cluster Service in the configuration cluster and link the clusters you want to distribute the load to. A MultiClusterService, or MCS, is a custom resource that is a logical representation of a Service across multiple clusters. An MCS is similar to, but substantially different from, the core Service type. An MCS exists only in the config cluster and generates derived Services in the target clusters. An MCS does not route anything like a ClusterIP, LoadBalancer, or NodePort Service does. Like a Service, an MCS is a selector for pods but it is also capable of selecting labels and clusters. The pool of clusters that it selects across are called member clusters, and these are all the clusters registered to the fleet. This MCS deploys a derived Service in all member clusters with the selector app: foo. If app: foo pods exist in that cluster, then those pod IPs will be added as backends for the MCI. The following service is a derived Service that the MCS generated in one of the target clusters. This Service creates a Network Endpoint Group, or NEG, which tracks pod endpoints for all pods that match the specified label selector in this cluster. A derived Service and NEG will exist in every target cluster for every MCS (unless using cluster selectors). If no matching pods exist in a target cluster, then the Service and NEG will be empty. The derived Services are managed fully by the MCS and are not managed by users directly. 3. Create a multi-cluster Gateway to route traffic from the load balancer to your distributed services To configure Google Cloud load balancer, first you need to use the Gateway CRD. In the metadata, specify the name of the resource and the namespace where you want to launch it. In the spec, use the gatewayClassName to specify which controller you want to use. To deploy an internal load balancer, use RILB, while to deploy an external, use GXLB instead. Routes can be created inside the Gateway or in a separate resource. In this case, we specify the HTTPRoute inside the Gateway and then link it with the Gateway using a selector. Notice that we are specifying the file for the credentials that will be used to establish TLS encryption on the requests.

### Video - [Lab intro: Configuring a multi-cluster mesh](https://www.cloudskillsboost.google/course_templates/435/video/428003)

* [YouTube: Lab intro: Configuring a multi-cluster mesh](https://www.youtube.com/watch?v=6v0YS1iNVrI)

In this lab exercise, you build a service mesh encompassing two clusters, west and east. You deploy an application comprised of services, some running on west and some on east. You test the application to make sure that services can communicate across clusters without problem. In this lab, you will perform the following tasks: Prepare to install Anthos Service Mesh, verifying that the clusters have been created and registered, setting up some environment variables, installing the amscli command line utility, etc. Install Anthos Service Mesh on both clusters. Configure the mesh to span both clusters. Review Service Mesh control planes. Deploy the Online Boutique application across multiple clusters. Evaluate your multi-cluster application. Distribute one service across both clusters.

### Lab - [AHYBRID081 Configuring a multi-cluster mesh with Anthos Service Mesh](https://www.cloudskillsboost.google/course_templates/435/labs/428004)

Configure a multi-service application across multiple clusters within a mesh using Anthos Service.

* [ ] [AHYBRID081 Configuring a multi-cluster mesh with Anthos Service Mesh](../labs/AHYBRID081-Configuring-a-multi-cluster-mesh-with-Anthos-Service-Mesh.md)

### Quiz - [Test your knowledge](https://www.cloudskillsboost.google/course_templates/435/quizzes/428005)

#### Quiz 1.

> [!important]
> **Multi-cluster gateways**
>
> * [ ] Replicate Kubernetes services across clusters.
> * [ ] Are automatically created by Anthos Service Mesh on multi-cluster mode.
> * [ ] Balance load across clusters.
> * [ ] Expose internal Kubernetes services for east-west load balancing.

#### Quiz 2.

> [!important]
> **East-west routing handles communication between clusters.**
>
> * [ ] False
> * [ ] True

#### Quiz 3.

> [!important]
> **When are east-west Gateways needed? (select 2)**
>
> * [ ] Multi-cluster with multiple networks in AWS.
> * [ ] Multi-cluster with multiple networks in Google Cloud.
> * [ ] Multi-cluster with a single network in Google Cloud.
> * [ ] Multi-cluster with a single network in AWS.

## Resources

### Document - [Course Introduction](https://www.cloudskillsboost.google/course_templates/435/documents/428006)

### Document - [Introducing Anthos Service Mesh](https://www.cloudskillsboost.google/course_templates/435/documents/428007)

### Document - [Anthos Service Mesh Routing](https://www.cloudskillsboost.google/course_templates/435/documents/428008)

### Document - [Anthos Service Mesh Security](https://www.cloudskillsboost.google/course_templates/435/documents/428009)

### Document - [Multi Cluster Networking with Anthos Service Mesh](https://www.cloudskillsboost.google/course_templates/435/documents/428010)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
