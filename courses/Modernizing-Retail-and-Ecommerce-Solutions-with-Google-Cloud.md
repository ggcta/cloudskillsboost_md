---
id: 506
name: 'Modernizing Retail and Ecommerce Solutions with Google Cloud'
datePublished: 2023-04-06
topics:
- Data Extraction
- Machine Learning
- Cloud Technology
type: Course
url: https://www.cloudskillsboost.google/course_templates/506
---

# [Modernizing Retail and Ecommerce Solutions with Google Cloud](https://www.cloudskillsboost.google/course_templates/506)

**Description:**

In this course, you will learn about the various services Google Cloud offers for modernizing retail applications and infrastructure. Through a series of lecture content and hands-on labs, you will gain practical experience deploying cutting-edge retail and ecommerce solutions on Google Cloud.

**Objectives:**

- Recognize how Google Cloud can solve retail industry business challenges.
- Deploy a retail application on Google Cloud.
- Scale and automate a retail application’s infrastructure with Google Cloud.
- Manage and analyze ecommerce data with Google Cloud.
- Extract, transform, and load ecommerce data with Google Cloud.
- Leverage machine learning to analyze ecommerce data with Google Cloud.

## Google Cloud in the Retail and Ecommerce Industry

In this module, you will recognize how Google Cloud can solve retail industry business challenges through a series of videos and hands-on labs.

### Video - [Module 1 Overview](https://www.cloudskillsboost.google/course_templates/506/video/375087)

- [YouTube: Module 1 Overview](https://www.youtube.com/watch?v=pv5BXHZVAuk)

hello and welcome to the module on Google cloud in the retail and e-commerce industry in this section you learn how Google Cloud can solve retail industry business challenges and hear from a few customers on how they're using Google Cloud for retail use cases in this module you learn to describe Google Cloud's mission in the retail industry describe how Google cloud is transformative and realize the value of Google cloud in the retail industry describe the larger Retail modernization Landscape and Define the specific areas and scope that this course covers recognize Google cloud services used in retail and e-commerce applications and explain how Google Cloud can benefit Cloud practitioners in the retail and e-commerce industry let's get started

### Video - [Why Google Cloud for retail](https://www.cloudskillsboost.google/course_templates/506/video/375088)

- [YouTube: Why Google Cloud for retail](https://www.youtube.com/watch?v=eQ03eUBpJAU)

foreign [Music] hi there Welcome to our ygcp for retail session I'm Carrie Tharp and I lead up retail industry Solutions let's Jump Right In from a cloud perspective the retail industry is definitely well progressed into the transformation era many years ago a lot of retailers started shifting to the cloud both for cost production but also to meet the ever increasing demands of their growing e-commerce businesses now these retailers have shifted into using Ai and ml to really transform and reshape the actual value chain steps in retail in a way that they never could before retail is a industry that it is very important to map to Consumer Dynamics and Trends to meet them where they are where they're going and what they want you to do before they've even explained what that is so if we talk a little bit about the things that the c-suite is focused on in retail I call out a few key areas first of all it all starts with Omni you'll hear us talk about omnichannel retailing Channel lists and frictionless commerce but really about removing these boundaries between stores and online and creating the really seamless experience for the customer because they don't think about channels from a technology standpoint there's a lot of work to be done there these systems and the channels were set up very differently historically with e-commerce often being a bolt-on business with a completely different Tech stack so lots of transformation and evolution to enable these experiences going forward you'll also hear a lot about modern store what is the future of physical retail as a consumer you probably experienced during covet the changing of store Footprints people setting up new ways to interact with buy online pickup in store how to do curbside changing of store Footprints and even retailers and Food Service rethinking the design of their stores holistically going forward the importance of first party data is another item that our customers talk about pretty significantly having a relationship with a customer so that you can best personalize to their needs is Paramount as retailers are all buying per share and trying to get those eyeballs of consumers to convert into transaction as efficiently and effectively as possible another dynamic in retail that is always going on is this great margin squeeze so you may not be familiar but for most retailers every shift from in-store to online is a driver of margin squeeze so the profitability of their online Channel often being below the store channel from a shifting ship shipping and returns perspective so Omni retailers have anything differently about how they're making Investments going forward and enabling new experiences to compete with all the digital natives out there other things going on around really new trends and Paradigm shifts that had happened in the last couple of years of course you can't go anywhere without hearing about supply chain Dynamics impacting retail so supply chain visibility really becoming a top priority for the retail c-suite sustainability has now been increasing in the consumers minds and Brands having to be more proactive in how they're thinking about that as well as cyber security being more important than ever as people are doing more of their shopping digitally we do expect uncertainty in the industry to continue so there's unpredictable shifts in demand how people are interacting with different channels you know really the boundary of retail itself has expanded into kind of media and gaming platforms as an example really people being able to transact and get service interactions with Brands kind of anywhere they go and so what do we need to be doing to enable that there's also a lot of Dynamics going on in the workforce so you may have heard about uh kind of challenges and getting the right level of Staffing and so a big retail focus on enabling and empowering Associates for the best experiences possible when customers come to Google it's often to talk about really everything that Google and alphabet can bring to bear so when you a retailer executive team is thinking about what they need to be doing this year it always includes kind of these four key buckets of how do they continue to increase traffic and customer engagement with their brand improving conversion rates once they get traffic to their sites or into the store increasing that basket size and always decreasing costs and driving operational efficiencies to fund back into Innovation and across Google cloud and alphabet we have products and services that can do each of these things that takes kind of our value proposition well beyond an infrastructure provider dynamic

### Video - [Google Cloud retail solutions and services](https://www.cloudskillsboost.google/course_templates/506/video/375089)

- [YouTube: Google Cloud retail solutions and services](https://www.youtube.com/watch?v=sIQJgZcn8yI)

when you double click on that and think specifically about our solution catalog and where we're focused today we're focused in four key areas first is helping retailers become customer-centric data-driven retailers this is really everything about pulling all of these separate data sources and the retail value chain together and being able to use them in real time making sure the right data platform Foundation has set up and then being able to not just see those insights but then Trigger action second is helping a retailer capture digital and omnichannel revenue growth think of this as our pillar that is everything about e-commerce and digital experience including digital experiences in stores so how do you create seamless and frictionless interaction stores to online how do you have the the reliability and scalability you need in e-commerce and have a rapid Innovation platform so retail is all about test and learn how do you set up that e-commerce stack as well as the other systems interacting in a way that you can do rapid tests and learn and bring new Solutions and offerings into the customer experience third is creating the modern store so what is the future of physical store this is both from an infrastructure standpoint so a lot of these stores kind of have servers in the back that are responsible for running all the core elements of the store that are kind of aged and need refresh and really will take a big upgrade moving into a retailers bringing Ai and ml into the store for Edge use cases so we're really just at the beginning of that journey in retail today last is driving sustainable and efficient operations so this covers everything that you can kind of Drive costs out of the system for retailers to put back into that customer experience so that includes everything from a kind of call center chat Bots uh various kind of Fleet routing and different capabilities that are going to remove costs whether it's from online or store operations I'll talk about each of these a little bit more but if you step back from our retail business holistically and you're thinking about okay where do we really make our money in retail what you should keep in mind across these business pillars that we have is the two key zones for us to land and expand in is e-commerce and Enterprise data platforms we often start in one of these areas they're Central to digital transformation in retail and then expand from there so if we talk a little bit more about customer-centric data-driven retailing as I mentioned this is about leveraging data to gain real-time insights and activations and Retail right now it's all about taking data and actually using it in the customer Journey as it's happening so how do you kind of interact with that customer during the shop journey to get to conversion before they effectively bounce and move on to the next thing from a Solutions perspective two things I'll call out here versus our retail customer date data platforms they sit at the heart of creating the foundation for all of these one-to-one interactions in retail a lot of the original Focus was from an e-commerce perspective so how do you have personalization while you're shopping online via SMS and email very kind of connected to the current experience but really has expanded to Omni capabilities over time and so pulling all those different data sources together from a product and supply chain standpoint directly linked to the customer to enable new use cases that drive everything from marketing optimization to individual personalization use cases I'll also mention here this ties into the solution we call retail common services in this data space what you should think about in retail they're dealing with these Legacy Tech stacks uh both from an e-commerce and an Erp perspective and they want to get after these Ai and ml use cases that are at that intersection of customer supply chain and product data and often they need help kind of abstracting above that Tech stack and being able to interact with that data in a way that they can get after the use case faster than kind of re-architecting or kind of trying to reshape some of those Legacy systems and so retail common Services is kind of a combination of bigquery and spanner that turns a lot of data into an event Ledger based setup where then you can actually interact and do a lot of real-time use cases whether there are associate performance in store forecasting or different marketing use cases and so this is something that really resonates in our industry that does not have the time or the capital to go back and reshape the elements of their Tech stack that aren't going to meet the needs going forward

### Video - [Google Cloud AI and ML capabilities](https://www.cloudskillsboost.google/course_templates/506/video/375090)

- [YouTube: Google Cloud AI and ML capabilities](https://www.youtube.com/watch?v=RIw9TKauOpU)

next I'll talk about product Discovery so when we think about the e-commerce platform I was mentioning we're really building our solution set to kind of expand upon the core infrastructure you need for e-commerce and create differentiation and our product Discovery capabilities are geared at driving conversion in those digital channels and interacting with the customer in new ways so if we jump into what's in that product Discovery solution Suite it is really all about kind of reshaping the historical way that consumers interact with websites today so if you think about navigation on a website or a search bar it pretty much looks like what it looked like 20 years ago and so we're bringing things to Market that can improve that quality of results so first is Vision product search this is a capability that helps you visually search for different products find like items recommendations AI is a solution and that is really shaping recommendations as the customers on their Journey so it moves away from historical ways that retailers were just saying well everybody likes this item so you must too as well and really understanding what's that customer's historical interaction with your brand with your products and what are they doing right now and evolving those recommendations as they go and then retail search of course this is bringing all the power and magic of Google search onto our retailers website very exciting historically a lot of a search is driven by kind of manual interaction between merchants and kind of setting up business rules and connections between data and products it is very manual and leads to a bad experience so tapping into our product knowledge graph and our natural language understanding driving a superior search interaction on the Retailer's website next I'll talk a little bit about modern store so as I mentioned here everything about what does store the future look like is really undefined and what we're seeing here is there's no one answer for all of the different sub-segments of retail so whether you're a grocery or luxury apparel things may look very different for you this is our next big workload unlock when we talk about retail so as I mentioned kind of the store infrastructure needs in overhaul and retailers thinking really proactively about bringing Ai and ml into the store so call out some of our Solutions here we're working on things like helping with pickpack and ship in store leveraging Google Glass so you know how do you kind of put those orders together in the most efficient way as labor is tight you can use Google Glass for hands-free and then wrap a lot of intelligence around that so we have customers that have put operational chat Bots around these capabilities that's really directing kind of associate activity in the store and driving a lot of improved customer experience in the store as well as associate experience we're also developing our vision platform for story use cases so things like shelf checking understanding what's really in stock what needs to be replenished this has big implications back into the entire kind of supply chain inventory replenishment process within retail and can drive a lot of cost reduction but also reduce out of stock which is kind of the Holy Grail within retail so really when you think about the store think about us digitizing the store so everything a retailer can do online and has great visibility and data exhaust coming off of that digital channel really bringing that into the store as well

### Video - [Google Cloud integrations](https://www.cloudskillsboost.google/course_templates/506/video/375091)

- [YouTube: Google Cloud integrations](https://www.youtube.com/watch?v=cyqme4wdvmA)

overall when you think about all these different areas we also do things from an operational efficiency standpoint as I mentioned but just to kind of wrap up we're also working with retailers across one Google so in many cases our other product areas are creating Innovation that our retailers can stream together and kind of create new experiences for themselves in a way that's faster and allows them to lead Prague the competition so there's Innovations within gpay or Maps team is bringing kind of chat and conversational Commerce into places pages so we're often leaning in and saying how can we pull the best of these different products together both for off-site experiences and on-site for retailers what this looks like from a customer portfolio standpoint we're very excited to say we have deep engagement across all of the retail sub verticals I'll call out a few here so first we're working with a lot of digital native so very Advanced uh kind of players that are already using Ai and ml in their businesses today to drive new experiences so this includes players like Etsy and Wayfair uh very large consumer platforms that people are used to to interacting with that are scalable e-commerce platforms that we support also from a scalability and reliability standpoint when you shift into traditional and omnichannel retailing we are as I mentioned those themes of e-commerce and Enterprise data platforms we are running e-commerce for Brands like Target Shopify which is a Commerce platform as well as Macy's when you shift over to data platforms we are the data cloud of Walmart so just kind of pause there and have all the Innovation they're driving as a team you know big success for our sales organization to kind of land a small data estate and keep expanding their other brands where we're working with them from a data platform perspective include Best Buy Woolworths Carrefour who are doing a lot of really interesting Innovation from you know localized inventory and assortment in store as an example Carrefour works on with our teams and then when we kind of double click and think about industry Solutions I'll call out a few examples here so if you want to experience our retail search capabilities live on a Retailer's website today be sure to go check out Macy's and lowes.com they go through peak season of this year are fully leveraging our search capabilities Sephora and Ikea as two examples who use our recommendations AI capabilities fast retailing which includes Uniqlo using our forecasting capabilities so a lot of these industry Solutions really already kind of brought to light and consumers experiencing them live and our retailers having uh kind of publicly reported improvements in conversion basket size Revenue per visit we've published a bunch of blogs on these that you can check out but that's the type of thing we're very excited about so retail is often uh kind of an industry that watches each other and so having those staff uh wildly helpful as you think about your industry a lot of these themes and Trends we're talking about exist in the the consumer experience in other Industries like Telco media Healthcare uh and so these these reference examples become relevant uh for all of us as well as you're thinking about customer data platforms recommendations and and things like that so uh to me this team is always happy to engage in kind of how you bring some of these different solutions to light to to address the trends and challenges you have inside your own industry so with that I will wrap up I hope that you have a little bit better insight to the things we have going on in retail and reach out for more detail on all of these customers uh and assets we have available if interested thanks bye

### Video - [Using Google Cloud for new priorities in retail](https://www.cloudskillsboost.google/course_templates/506/video/375092)

- [YouTube: Using Google Cloud for new priorities in retail](https://www.youtube.com/watch?v=F_wjd27ZEb8)

let's talk a little bit more about solutions to help you accelerate your go forward plans Google Cloud can help you solve for what's next in retail in three important ways first our Solutions can help you improve operations in ways that drive Capital efficiency as well as create cost production and operational Improvement second we can help unlock value in your p l through data-driven decisions at the intersection of customer product and supply chain data finally we can help you capture that digital and omnational growth with modern and responsive systems and new experiences these times of Crisis are creating lasting brand Impressions with your customers no one wants to lose customers to operationally more agile competitors and cost reduction will be key to fund Innovation going forward we know it isn't easy to operate in a world where both consumer behavior and Supply chains are disrupted and entire teams have had to go remote Google cloud has solutions that can connect your Workforce to drive real-time collaboration to use AI to provide customer service or create operational chat Bots to streamline store operations and solutions that can help optimize inventory and Logistics we can also help streamline your store infrastructure and get you ready for compute at the edge with anthos and 5G offerings we can also help do all of these things in weeks instead of months with the cost savings security and Agility that come with our Cloud offerings an example of a retailer who used our solutions to improve customer service during the pandemic is Albertsons Albertsons operates in over 2 000 locations in the U.S and they saw call volumes to their stores increased five times versus the norm during covet 19. questions like do you have toilet paper and can you deliver took on a whole new urgency to help customers get faster responses Albertsons enlisted our rapid response virtual agent that's part of our contact center AI offering to manage surging inbound calls and to address customers basic questions with contact center AI your virtual agent can converse naturally with customers 24 7 and assist human agents on complex cases integrating with your existing infrastructure while decreasing costs to serve and improving metrics like net promoter score next is becoming data driven you've all heard it retail is in the detail and the details are in the data retail has been on a journey to make customer-centric data-driven decisions for years but the transition from reporting and insights into real-time action can sometimes be a challenge with Legacy Tech stacks and Silo data Google Cloud can jumpstart or accelerate your agenda whether it's creating one-to-one engagement with AI or using more signals to create a better demand forecast we can also migrate and modernize your data platforms for all of your marketing merchandising and supply chain needs Solutions such as retail common Services can help you activate AI in real time and move from reactive to proactive decision making leveraging data from all parts of your business we will work with you to fully capture the value of your data and take the hype out of AI to deliver Real Results you can feel in the p l Ulta beauty is an example of a customer who uses data and personalization to create a differentiated digital experience while their stores were temporarily closed they leaned into their online channels to stay connected with shoppers their virtual beauty advisor tool built on Google Cloud for particularly useful providing consumers with tailored recommendations powered by customer and product insights as well as a high quality virtual try on experience while the store-based beauty experience was not an option [Music] and lastly e-commerce and channelless Retail are here to stay but now it's all about how fast you can move Google Cloud can help you accelerate your shift to digital by untangling Legacy eCommerce systems introducing a containerized architecture that provides for greater agility to deploy new services and capabilities and provide Superior reliability and scalability we can help you build a forward-looking architecture that creates a flexible platform that grows to meet your expanding transformation needs and tools like appsheet are no code development platform for web and mobile app experiences can take your rapid test and learn agenda to the next level enabling better discovery of your products is also of utmost importance Google Cloud provides product Discovery Solutions such as Google Cloud retail search which is powered by Google's unique and leading query understanding and contextual search capabilities lastly and very relevant in these recent months we've also have a white glove e-commerce service to help you get ready to manage Peak capacity needs working shoulder to shoulder with your War Room teams to ensure a Flawless Peak these capacity Management Services have helped many retailers during the spikes caused by covet 19. core solution areas were also focused on bringing together the best of Google for you through Google Cloud platform the broader Google ecosystem can play a significant role in helping retailers deliver a true channelist experience with eight products that have over a billion users Google can augment the shopping Journey helping consumers discover products and Brands while improving conversion and reducing friction Google Cloud can be your trusted technology partner creatively leveraging all of our consumer Innovation and broader offerings to help you deliver on a new and unique Journey for your brand

### Video - [Using Google Cloud to solve for the future of retail](https://www.cloudskillsboost.google/course_templates/506/video/375093)

- [YouTube: Using Google Cloud to solve for the future of retail](https://www.youtube.com/watch?v=X7yU5gASUek)

hello everyone and welcome to apac's Google Cloud on air where today's topic is seize the opportunity and sharing with you how some of our top retailers are solving for the future I'm Diana Britt I lead our retail industry and solutions here in Asia Pacific and I'll be your host for today's session um we are very glad and excited to have many folks on this digital platform today joining us from Australia New Zealand India southeast Asia North Asia and we have with us today three fantastic customers we like to call them customer Heroes um who share their experiences in this new economy and how they're solving for the future and we also have Accenture joining us today as well to also share their perspective and thoughts as they're working with their customers in this new economy so just some quick housekeeping tips as you take a look by clicking on the Home tab on the navigation bar located at the top of the page you'll be able to view today's agenda and all the sessions that you've seen today have been pre-recorded just to ensure top quality at the same time this isn't an interactive platform so please post question there's a blue button at the bottom that says ask a question click there and we can engage with you and then to engage with us further we have started polls which we will be doing during the event and the polling we will ask you asking and posing questions and we would welcome your engagement and the results will be posted after the panel session ends so feel free also from a social perspective to follow us on our social media channels and ask any questions you might have at hashtag Google Cloud retail so I'd like to kick off with you retail is a very big bet for Google Cloud uh very much an industry focus and I'd like to share with you an overview of what we're seeing here in the Asia Pacific Market and an overview of our Solutions before we jump into our customer hero segment so let's jump in what history has taught us about navigating a crisis is that a decision made today can make a significant impact and just as little as six months time so it's the decision today and what is it going to take to get you to the Future but this crisis is just a little bit different for most of us the path forward does present some significant uncertainty especially for Enterprise organization not only are we facing the epidemic but it's also what are the consumer buying habits and what's happening with the economic and policies in the different region or different countries that we operate in when a shelter in place going to be lifted how is it going to get phased by region as we look at behaviors it takes 66 days they say to form a new habit so what if those habits are going to become permanent what are be what will be impermanent and then what is the shape of the economic um recovery going to look like for the region here today and also different countries and then the impact around privacy and regulation moving forward so outside of these macro Trends we're also seeing key implications for retailers so what's that right balance on the store versus online for your organization how will consumers balance their privacy versus personalization um what buying patterns might evolve what changes might we see in seasonality so these are all things to consider from a macro perspective and from a retail perspective to dive a little bit deeper we're looking at this as Market recovery stages and you're going to hear today from our customer Heroes and Accenture around some of these key areas around not only how they've reacted but now how are they responding rebuilding and reframing and everyone's a little bit different in their Journey from a response standpoint is we see demand we see demand fall how are we taking action and what are we setting up to be ready for Success when that demand returns from a rebuild perspective it's how are we monitoring the right signals monitoring that demand and capturing the most of that demand and then from a reframe it's how are we actually now going to be operating our business across the region or within country or across the globe and how are we going to restructure it so it's a right balance for us to be most profitable so Google cloud from a Google Cloud retail perspective Thomas currian our CEO of Google cloud has made a big bet in industry and Retail is one of our top Focus Industries and we're here we've got three different key pillars that we're offering our customers it's how do we help them Drive operational Improvement how do we help them capture that digital and omni-channel growth and how are we helping them make better data-driven decisions in real time so these three key pillars it's been a foundation seven out of 10 retailers are running Google Cloud and to give you some ideas on some of the areas to highlight in these different pillars so from an operations standpoint it's how can we reduce your operating costs and improve efficiencies whether that's Workforce transformation using G Suite as most of your employees might be working from home and improving that collaboration capability with Google meet whether it's inventory optimization whether it's improving your customer service and how are you taking advantage of the edge capability with our anthos and hybrid multi-cloud platforms so these are just some of the ways that we're working with our customers to drive cost reduction and optimization becoming country customer Centric and data driven so this is really all about your data and how are you Gathering that data and you'll hear from some of our customers today around this but how are you taking that data to actually take action and get meaningful insights out of that data so whether that comes to targeted streaming analytics whether that comes to demand forecasting and we have a new platform called retail common Services which is an aggregation of your data platforms and that data structure to take advantage of additional mlai capabilities and capturing digital and omnichannel revenue growth and this is a big one for us as well it's how are we helping our our customers modernize their e-commerce platforms ensure they're fully stable ensure that they're resilient and then secure and fully available at high demand and then from a product standpoint what things can we do to Delight the customer whether that's giving them the opportunity to have a recommendation given to them via the recommendations AI or Vision product search so just several examples here but as you think about those three pillars there's no one way to start everyone's journey is going to be a little bit different and unique focusing on what's most important to your business today are you solving to enhance and streamline your support are you solving to uncover insights for innovation are you really trying to solve how do we better personalize our experience for our customers so as you think about this journey think about where you would might want to start and you'll hear again from three of our customers on their Journeys and to wrap it up just sharing with you some of our customers across the United States uh Latin America emea Japan and Asia Pacific we've got top retailers that are leveraging Google Cloud who have significant amount of Legacy infrastructure whether that be food and grocery whether that be specialty whether the big big box or whether that be Pure Play so you see some of these logos here who all have fantastic stories on their Journey with Google Cloud

### Video - [Customer story - Lowes](https://www.cloudskillsboost.google/course_templates/506/video/375094)

- [YouTube: Customer story - Lowes](https://www.youtube.com/watch?v=XjcUkUDqxfE)

so now let's talk to a customer that has been on a path of Rapid digital transformation with Google cloud and what they've been able to achieve in just the last few years Lowe's Grew From a single small town hardware store in North Carolina to become one of the largest Home Improvement retailers in the world today they serve 18 million customers a week and ended 2019 with 72 billion dollars in sales to talk about their experience I recently spoke to nalima Sharma senior vice president of technology for e-commerce marketing and Merchandising at Lowe's hi nalima it's great to see you thank you so much for joining us today hey carrots good to see you again great you I saw that you guys had a great quarter last quarter at Lowe's and digital was a big part of that success can you tell us a little bit about your experience and the response to the covid-19 pandemic our primary focus right now is on the health and safety of our customers and Associates and providing them a good experience however customers choose to shop with us in this environment you know the fundamental improvements that we've made over the last 18 months or so on our technology foundations have allowed us to be able to create new capabilities and meet our customer needs both online and also in stores customers increasingly turned to us in March for essential items to keep their homes running there was a sharp uptick in sales on lowes.com many spikes exceeded Black Friday demand pivoting to serve that demand requires agility and stability of our people in our systems investments in our online infrastructure and our progress with our Google Cloud migration greatly improved our site stability and allowed us to be effect to effectively handle that increased traffic lowes.com sales were up 80 in the last the first quarter of 2020 and we saw Triple digit growth on lowes.com in April and into May wow that's really impressive I'm glad that you guys were able to meet all the demands through this pandemic period I know you guys at Lowe's have made a concerted effort over the past couple years to invest in digital can you tell us a little bit about how you went about this journey and how to prioritize where you invested Lowe's had historically under invested in technology and when Marvin Ellison became CEO he was candid about some fundamental things that needed to be fixed he noted that all of the successful retail companies have a successful technology footprint and that immediately set the ambition for Lowe's to become a world-class technology leader we view technology as a backbone for delivering a best-in-class personalized integrated experience Home Improvement projects take time and a customer may start researching their Project online and complete their transaction in store or they may start in store and then move online or go back and forth so our focus is on making sure their journey is seamless no matter where they are on at any point in time great that's great to hear you're really thinking about that customer Journey at the center that's so critical to meet all the new demands of customer expectations we're headed in to yet another holiday season where our Omni channel is expected to be more important than ever before can you talk a little bit about what you're thinking of for Readiness in this new retail reality yeah you know actually um you know over the last quarter our team has been through a sustained peak of demand that amounted to an extended holiday season it was like having a Black Friday for a whole quarter and they've responded tremendously to this unprecedented circumstances changing in customer demand all in the while of a multi-year technology transformation as the ongoing impacts of covet point to the increasing importance of omni-channel capabilities to Home Improvement customers that the work completed over the past few months has given us that confidence that we are on the right path whether it's Black Friday whether it's Cyber Monday or or just our busy spring season our goal is for the holidays to be a non-event Omni channel is a tremendous growth opportunity and we're excited about our technology vision and the progress we've made and we feel that we're well on our way to build a channelless retail experience for our customers thank you so much for sharing your journey with us in the Lima I'm excited for the results you guys have seen and what lies ahead both as a customer and a partner I know there's lots more to come thank you to your company your Associates and your team for helping support everyone through the pandemic thank you so much Kerry and really appreciate our partnership and looking forward to more work and amazing things together so now that you've heard a little bit about our Solutions and directly from a customer I'd like to mention our partners to help deliver our Solutions at scale we are growing our ecosystem of Partners who have deep expertise in retail and can help you get faster results with integrated or out-of-the-box SAS Solutions ultimately our goal at Google cloud is to bring the best of Google to our customers and to be a trusted partner that helps you fulfill your business goals for more information about our Solutions and to hear success stories from your peers attend our retail breakout sessions throughout next on air also will be conducting Regional digital events that go deeper into your concerns and Roundtable and webinar formats lastly we understand each Enterprise's journey is different so reach out to your Google Cloud sales representative for a conversation on how we can help you solve for what's next in retail thank you for joining [Music]

### Link - [Case study - Carrefour](https://www.cloudskillsboost.google/course_templates/506/documents/375095)

- [Case study - Carrefour](https://cloud.google.com/customers/carrefour)

### Quiz - [Module 1 Quiz](https://www.cloudskillsboost.google/course_templates/506/quizzes/375096)

#### Quiz 1.

> [!important]
> **What statement does NOT describe the larger retail modernization landscape?**
>
> - [ ] Ecommerce is now at the center of the customer journey.
> - [ ] The pandemic forced consumers to break their routines and necessity has led people to try new products and services.
> - [ ] Taking data and using it in the customer journey as it's happening is critical to attaining conversion.
> - [ ] Ecommerce was a trend during the pandemic, but is no longer a relevant avenue for consumer shopping experiences.

#### Quiz 2.

> [!important]
> **What answer does NOT describe the value of Google Cloud in the retail industry?**
>
> - [ ] Google Cloud provides technology to improve associate processes and provide better customer experiences.
> - [ ] Google Cloud can create a better customer journey starting from a variety of Google entry points, and optimizing marketing from journey insights.
> - [ ] Google Cloud provides many disconnected services that retailers can use individually to accelerate their digital transformation.
> - [ ] Google Cloud gives shoppers a personalized ecommerce experience with AI-driven search, recommendations, and conversational interfaces.

#### Quiz 3.

> [!important]
> **What statement does NOT describe Google Cloud's benefits to cloud practitioners in the retail and ecommerce industry?**
>
> - [ ] Google Cloud gives cloud practitioners the tools to leverage embedded ML and AI capabilities to analyze and predict consumer behavior.
> - [ ] Google Cloud gives cloud practitioners a small set of managed services for modernizing real and ecommerce applications.
> - [ ] Google Cloud gives cloud practitioners the tools to analyze data and share findings across their organization.
> - [ ] Google Cloud gives cloud practitioners the tools to leverage data to gain real time insights.

#### Quiz 4.

> [!important]
> **Which types of managed services are used to modernize retail and ecommerce applications?**
>
> - [ ] Big data/anlaytics
> - [ ] Machine learning/AI
> - [ ] All of the above
> - [ ] Compute/Storage

#### Quiz 5.

> [!important]
> **What does NOT describe Google Cloud's mission in the retail industry?**
>
> - [ ] Google Cloud helps retailers capture digital and omnichannel revenue growth.
> - [ ] Google Cloud helps retailers create the modern store.
> - [ ] Google Cloud helps retailers deliver their ecommerce goods.
> - [ ] Google Cloud helps retailers become customer-centric and data driven.

## Deploying a Retail Application on Google Cloud

In this module, you will learn how to deploy a retail application on Google Compute Engine through a series of videos and hands-on labs.

### Video - [Module 2 Overview](https://www.cloudskillsboost.google/course_templates/506/video/375097)

- [YouTube: Module 2 Overview](https://www.youtube.com/watch?v=2Y3UZ2oSu-4)

welcome to this section on deploying a retail application on Google cloud with Google Cloud you can modernize your retail applications and power your eCommerce operations with reliability scalability and flexibility deploy retail applications to deliver frictionless customer experiences in this section you learn to architect a strategy to deploy a retail application on Google compute engine create instance templates and managed instance groups from Source instances deploy a retail application on managed instance groups create https load balancers and health checks for a retail application and use a Content delivery Network to catch a retail application's assets ready to get started let's begin

### Video - [Architecting an ecommerce application on Google Cloud](https://www.cloudskillsboost.google/course_templates/506/video/375098)

- [YouTube: Architecting an ecommerce application on Google Cloud](https://www.youtube.com/watch?v=aOaR4GAcKYU)



### Video - [Compute Engine overview](https://www.cloudskillsboost.google/course_templates/506/video/375099)

- [YouTube: Compute Engine overview](https://www.youtube.com/watch?v=FStKrX7C4qs)

As mentioned in the introduction to the course, there is a spectrum of different options in Google Cloud for compute and processing. We will focus on the traditional virtual machine instances. Now the difference is, Compute Engine gives you the utmost in flexibility: run whatever language you want—it's your virtual machine. This is purely an infrastructure as a service or IaaS model. You have a VM and an operating system, and you can choose how to manage it and how to handle aspects, such as autoscaling, where you’ll configure the rules about adding more virtual machines in specific situations. Autoscaling will be covered later in the course. The primary work case of Compute Engine is any general workload, especially an enterprise application that was designed to run on a server infrastructure. This makes Compute Engine very portable and easy to run in the cloud. Other services, like Google Kubernetes Engine, which consists of containerized workloads, may not be as easily transferable as what you’re used to from on-premises. So what is Compute Engine? At its heart, it's physical servers that you're used to, running inside the Google Cloud environment, with a number of different configurations. Both predefined and custom machine types allow you to choose how much memory and how much CPU you want. You choose the type of disk you want, whether you want to use persistent disks backed up by standard hard drives or solid-state drives, local SSDs, Cloud Storage, or a mix. You can even configure the networking interfaces and run a combination of Linux and Windows machines. We will discuss these options in more detail later in the module. Several different features will be covered throughout this module, such as machine rightsizing, startup scripts, metadata, availability policies, OS patch management, and pricing and usage discounts. Let’s start by looking at the compute options. Compute Engine provides several different machine types that we’ll discuss later in this module. If those machines don’t meet your needs, you can also customize your own machine. Your choice of CPU will affect your network throughput. Specifically, your network will scale at 2 gigabits per second for each CPU core, except for instances with 2 or 4 CPUs which receive up to 10 gigabits per second of bandwidth. As of this recording, there is a theoretical maximum throughput of 100 gigabits per second for an instance with 224 vCPU, when you choose an N2D machine series. When you're migrating from an on-premises setup, you're used to physical cores, which have hyperthreading. On Compute Engine, each virtual CPU (or vCPU) is implemented as a single hardware hyper-thread on one of the available CPU Platforms. For an up-to-date list of all the available CPU platforms, refer to the links section of this video [https://cloud.google.com/compute/docs/cpu-platforms] After you pick your compute options, you want to choose your disk. You have three options: Standard, SSD, or local SSD. So basically, do you want the standard spinning hard disk drives (HDDs), or flash memory solid-state drives (SSDs)? Both of these options provide the same amount of capacity in terms of disk size when choosing a persistent disk. Therefore, the question really is about performance versus cost, because there's a different pricing structure. Basically, SSDs are designed to give you a higher number of IOPS per dollar versus standard disks, which will give you a higher amount of capacity for your dollar. Local SSDs have even higher throughput and lower latency than SSD persistent disks, because they are attached to the physical hardware. However, the data that you store on local SSDs persists only until you stop or delete the instance. Typically, a local SSD is used as a swap disk, just like you would do if you want to create a ramdisk, but if you need more capacity, you can store those on a local SSD. You can create instances with up to eight separate 375-GB local SSD partitions for a total of 3 TB of local SSD space for each instance. Standard and non-local SSD disks can be sized up to 257 TB for each instance. The performance of these disks scales with each GB of space allocated. As for networking, we have already seen networking features applied to Compute Engine in the previous module’s lab. We looked at the different types of networks and created firewall rules using IP addresses and network tags. You’ll also notice that you can do regional HTTPS load balancing and network load balancing. This doesn’t require any pre-warming because a load balancer isn't a hardware device that needs to analyze your traffic. A load balancer is essentially a set of traffic engineering rules that are coming into the Google network, and VPC is applying your rules destined for your IP address subnet range. We’ll learn more about load balancers in a later course of the Architecting with Google Compute Engine series.

### Video - [Compute options](https://www.cloudskillsboost.google/course_templates/506/video/375100)

- [YouTube: Compute options](https://www.youtube.com/watch?v=sJsKmbxM_Vc)

Now that you have completed the lab, let’s dive deeper into the compute options that are available to you in GCP by focusing on CPU and memory. You have three options for creating and configuring a VM. You can use the GCP Console as you did in the previous lab, the Cloud Shell command line, or the RESTful API. If you’d like to automate and process very complex configurations, you might want to programmatically configure these through the RESTful API by defining all the different options for your environment. If you plan on using the command line or RESTful API, I recommend that you first configure the instance through the GCP console and then ask Compute Engine for the equivalent REST request or command line, as shown in the demo earlier. This way you avoid any typos and get dropdown lists of all the available CPU and memory options. Speaking of CPU and memory options, let’s look at the different machine types that are currently available. When you create a VM, you select a machine type from a machine family that determines the resources available to that VM. There are several machine families you can choose from and each machine family is further organized into machine series and predefined machine types within each series. A machine family is a curated set of processor and hardware configurations optimized for specific workloads. When you create a VM instance, you choose a predefined or custom machine type from your preferred machine family. Alternatively, you can create custom machine types. These let you specify the number of vCPUs and the amount of memory for your instance. There are four Compute Engine machine families. General-purpose, Compute-optimized, Memory-optimized, and Accelerator-optimized. Let's look at each in more detail. The general-purpose machine family has the best price-performance with the most flexible vCPU to memory ratios, and provides features that target most standard and cloud-native workloads. The E2 machine series is suited for day-to-day computing at a lower cost, especially where there are also no application dependencies on a specific CPU architecture. E2 VMs provide a variety of compute resources for the lowest price on Compute Engine, especially when paired with committed-use discounts. You simply pick the amount of vCPU and memory that you want, and Google provisions it for you. Standard E2 VMs have between 2 to 32 vCPUs with a ratio of 0.5 GB to 8 GB of memory per vCPU. They are a great choice for web servers, small to medium databases, development and test environments, and many applications that don't have strict performance requirements. They offer a compatible performance baseline with the current N1 VMs for those of you who have been using them. The E2 machine series also contains shared-core machine types that use context-switching to share a physical core between vCPUs for multitasking. Different shared-core machine types sustain different amounts of time on a physical core. In general, shared-core machine types can be more cost-effective for running small, non-resource intensive applications than standard, high-memory, or high-CPU machine types. Shared-core E2 machine types have 0.25 to 1 vCPUs with 0.5 GB to 8 GB of memory. N2 and N2D are the next generation following N1 VMs, offering a significant performance jump. N2 and N2D are the most flexible VM types and provide a balance between price and performance across a wide range of VM shapes, including enterprise applications, medium-to-large databases, and many web and app-serving workloads. Committed use and sustained use discounts are supported. N2 VMs support the latest second generation scalable processor from Intel with up to 128 vCPUs and 0.5 to 8 GB of memory per vCPU. At the time of recording, Cascade Lake is the default processor for machine types up to 80 vCPUs. In beta, for larger machine types Ice Lake is the default processor for specific regions and zones. for specific regions and zones. N2D are AMD-based general purpose VMs. They leverage the latest EPYC Milan and EPYC Rome processors, and provide up to 224 vCPUs per node. Tau T2D VMs are optimized for cost-effective performance of demanding scale-out workloads. T2D VMs are built on the latest 3rd Gen AMD EPYCTM processors and offer full x86 compatibility. They are suited for scale-out workloads including web servers, containerized microservices, media transcoding, and large-scale Java applications. T2D VMs come in predefined VM shapes, with up to 60 vCPUs per VM and 4 GB of memory per vCPU. If you have containerized workloads, Tau VMs are supported by Google Kubernetes Engine to help optimize price-performance. You can add T2D nodes to your GKE clusters by specifying the T2D machine type in your GKE node pools. The compute-optimized machine family has the highest performance per core on Compute Engine and is optimized for compute-intensive workloads. C2 VMs are the best fit VM type for compute-intensive workloads, including AAA gaming, electronic design automation, and high-performance computing across simulations, genomic analysis, or media transcoding. They might also be applications that have very expensive per core licensing and thus would benefit from higher per core performance. Powered by high-frequency Intel-scalable processors, Cascade Lake, C2 machine types offer up to 3.8 Ghz sustained all-core turbo and provide full transparency into the architecture of the underlying server platforms, enabling advanced performance tuning. The C2 series comes in different machine types ranging from 4 to 60 vCPUs, and offers up to 240 GB of memory. You can also attach up to 3 TB of local storage to these VMs for applications that require higher storage performance. The C2D machine type series provides the largest VM sizes and are best-suited for high-performance computing (HPC). The C2D series also has the largest available last-level cache (LLC) cache per core. The C2D machine series come in different machine types ranging from 2 to 112 vCPUs, and offers 4 GB of memory per vCPU core. You can also attach up to 3TB of local storage to these machine types for applications that require higher storage performance. 2CD VMs are available on the third generation AMD EPYC Milan platform. The memory-optimized machine family provides the most compute and memory resources of any Compute Engine machine family offering. They are ideal for workloads that require higher memory-to-vCPU ratios than the high-memory machine types in the general-purpose machine family. The M1 machine series has up to 4 TB of memory, while the M2 machine series has up to 12 TB of memory. These machine series are well-suited for large in-memory databases such as SAP HANA, as well as in-memory data analytics workloads. Both the M1 and M2 machine series offer the lowest cost per GB of memory on Compute Engine, making them a great choice for workloads that utilize higher memory configurations with low compute resource requirements. Additionally, they offer up to 30% sustained use discounts and are also eligible for committed use discounts, bringing additional savings of greater than 60% for three-year commitments. The accelerator-optimized machine family is ideal for massively parallelized Compute Unified Device Architecture (CUDA) compute workloads, such as machine learning (ML) and high-performance computing (HPC). This family is the optimal choice for workloads that require GPUs. The A2 series has 12 to 96 vCPUs, and up to 1360 GB of memory. Each A2 machine type has a fixed number (up to 16) of NVIDIA’s Ampere A100 GPUs. An A100 GPU provides 40 GB of GPU memory—ideal for large language models, databases, and HPC. Additional information, including the latest specs for currently available VM machine types, can be found in the machine types documentation. If none of the predefined machine types match your needs, you can independently specify the number of vCPUs and the amount of memory for your instance. Custom machine types are ideal for the following scenarios: When you have workloads that are not a good fit for the predefined machine types that are available to you. Or when you have workloads that require more processing power or more memory, but don't need all of the upgrades that are provided by the next larger predefined machine. It costs slightly more to use a custom machine type than an equivalent predefined machine type, and there are still some limitations of memory and vCPUs you can select: only machine types with 1 vCPU or an even number of vCPUs can be created. Memory must be between 0.9 GB and 6.5 GB per vCPU (by default). The total memory of the instance must be a multiple of 256 MB. By default, a custom machine can have up to 6.5 GB of memory per vCPU. However, this might not be enough memory for your workload. At an additional cost, you can get more memory per vCPU beyond the 6.5 GB limit. This is referred to as extended memory, and you can learn more about this in the link provided in the module PDF located in the Course Resources. The first thing you want to consider when choosing a region and zone is the geographical location in which you want to run your resources. This map shows the current and planned Google Cloud regions and number of zones. For up-to-date information on the available regions and zones, see the documentation linked for this video. Each zone supports a combination of Ivy Bridge, Sandy Bridge, Haswell, Broadwell, and Skylake platforms. When you create an instance in the zone, your instance will use the default processor supported in that zone. For example, if you create an instance in the us-central1-a zone, your instance will use a Sandy Bridge processor.

### Video - [Common Compute Engine actions](https://www.cloudskillsboost.google/course_templates/506/video/375101)

- [YouTube: Common Compute Engine actions](https://www.youtube.com/watch?v=5zMLo5we2Ug)

Now that we have covered all the different compute image and disk options, let's look at some common actions that you can perform with Compute Engine. Every VM instance stores its metadata on a metadata server. The metadata server is particularly useful in combination with startup and shutdown scripts because you can use the metadata server to programmatically get unique information about an instance without additional authorization. For example, you can write a startup script that gets the metadata key value pair for an instance's external IP address and use that IP address in your script to setup a database. Because the default metadata keys are the same on every instance, you can reuse your script without having to update it for each instance, this helps you create less brittle code for your applications. Storing and retrieving instance metadata is a very common Compute Engine action. I recommend storing these startup and shutdown scripts in Cloud Storage as you will explore in the upcoming lab of this module. Another common action is to move an instance to a new zone. For example, you might do so for geographical reasons or because a zone is being deprecated. You can move a VM even if one of the following scenarios apply: The VM instance is in a TERMINATED state, or The VM instance is a Shielded VM that uses UEFI firmware. If you move your instance within the same region, you can automate the move by using the gcloud compute instances move command. To move your VM, you must shut down the VM, move it to the destination zone or region, and then restart it. After you move your VM, update any references that you have to the original resource, such as any target VMs or target pools that point to the earlier VM During the move, some server-generated properties of your VM and disks change. If we move your instance to a different region, you need to manually do so by following the process outlined here. This involves making a snapshot of all persistent disks and creating new disks in the destination zone from that snapshot. Next, you create a new VM in the destination zone and attach the new persistent disks, assign a static IP, and update any references to the VM. Finally, you delete the original VM, its disks and the snapshot. Speaking of snapshots, let's take a closer look at these. Snapshots have many use cases. For example, they can be used to backup critical data into a durable storage solution to meet application, availability, and recovery requirements. These snapshots are stored in Cloud Storage, which is covered later. Snapshots can also be used to micro data between zones. I just discussed this when going over the manual process of moving an instance between two regions, but this can also be used to simply transfer data from one zone to another. For example, you might want to minimize latency by migrating data to a drive that can be locally attached in the zone where it is used. Which brings me to another snapshot use case of transferring data to a different disk type. For example, if you want to improve disk performance, you could use a snapshot to transfer data from a standard ECD persistent disk to a SSD persistent disk. Now that I've covered some of these snapshot use cases, let's explore the concept of a disk snapshot. First of all, this slide is titled persistent disk snapshots because snapshots are available only to persistent disks and not to local SSDs. Snapshots are different from public images and custom images which are used primarily to create instances or configure instance templates, in that snapshots are useful for periodic backup of the data on your persistent disks. Snapshots are incremental and automatically compressed, so you can create regular snapshots on a persistent disk faster and at a much lower cost than if you regularly created a full image of the disk. As we saw with the previous examples, snapshots can be restored to a new persistent disk, allowing for a move to a new zone. To create a persistent disk snapshot, see the link section of this video. Another common Compute Engine action is to resize your persistent disk. The added benefit of increasing storage capacity is to improve I/O performance. This can be achieved while the disk is attached to a running VM without having to create a snapshot. Now, while you can grow disk and size, you can never shrink them. So keep this in mind.

### Video - [Managed instance groups](https://www.cloudskillsboost.google/course_templates/506/video/375102)

- [YouTube: Managed instance groups](https://www.youtube.com/watch?v=VHO7LQLswbE)

Person: A managed instance group is a collection of identical VM instances that you control as a single entity using an instance template. You can easily update all the instances in a group by specifying a new template in a rolling update. Also when your applications require additional compute resources, managed instance groups can scale automatically to the number of instances in the group. Managed instance groups can work with load balancing services to distributor network traffic to all of the instances in the group. If an instance in the group stops, crashes or is deleted by an action other than the instance group commands, the managed instance group automatically recreates the instance so it can resume its processing tasks. The recreated instance uses the same name and the same instance template as the previous instance. Managed instance groups can automatically identify and recreate unhealthy instances in a group to ensure that all instances are running optimally. Regional managed instance groups are generally recommended over zonal managed instance groups because they allow you to spread the application load across multiple zones instead of confining your application to a single zone or having you manage multiple instance groups across different zones. This replication protects against zonal failures and unforeseen scenarios where an entire group of instances in a single zone malfunctions. If that happens, your application can continue serving traffic from instances running in another zone in the same region. In order to create a managed instance group, you first need to create a instance template. Next, you're going to create a managed instance group of N specified instances. The instance group manager then automatically populates the instance group based on the instance template. You can easily create instance templates using the cloud console. The instance template dialogue looks and works exactly like creating an instance, except that the choices are recorded so they can be repeated. When you create an instance group, you define the specific rules for that instance group. First, you decide what type of managed instance group you want to create. You can use managed instance groups for stateless serving or batch workloads. such as website front end or image processing from a queue, or for stateful applications. such as databases or legacy applications. Second, provide a name for the instance group. Third, decide whether the instance group is going to be single or multizoned and where those locations will be. You can optionally provide port name mapping details. Fourth, select the instance template that you want to use. Fifth, decide whether you want to autoscale and under what circumstances. Finally, consider creating a health check to determine which instances are healthy and should receive traffic. Essentially, you're creating virtual machines, but you're applying more rules to that instance group.

### Video - [Autoscaling and health checks](https://www.cloudskillsboost.google/course_templates/506/video/375103)

- [YouTube: Autoscaling and health checks](https://www.youtube.com/watch?v=oKetJ5xzYmM)

Person: Let me provide more details on the autoscaling and health checks of the managed instance group. As I mentioned earlier, managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increase or decrease in load. Autoscaling helps your applications gracefully handle increase in traffic and reduces cost when the need for resources is lower. You just define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load. Applicable autoscaling policies include scaling based on CPU utilization, load-balancing capacity or monitoring metrics or via queue-based workload like Cloud [Indistinct]. For example, let's assume you have two instances that are at 100 percent and 85 percent CPU utilization as shown on the slide. If your target CPU utilization is 75 percent, the autoscaler will add another instance to spread out the CPU load and stay below the 75 percent target CPU utilization. Similarly, if the overall load is much lower than the target, the autoscaler will remove instances as long as that keeps the overall utilization below the target. Now you might ask yourself, "How do I monitor the utilization of my instance group?" When you click on an instance group or even an individual virtual machine, a graph is presented. By default, you'll see the CPU utilization over the past hour, but you can change the time frame and visualize other metrics like disk and network usage. These graphs are very useful for monitoring your instance's utilization and for determining how best to configure your autoscaling policy to meet changing demands. If you monitor the utilization of your virtual-machine instances and Stackdriver monitoring, you can even set up alerts through several notification channels. For more information on autoscaling, see the links section of this video. Another important configuration for a managed instance group and load balancer is a health check. A health check is very similar to an uptime check in Stackdriver. You just define a protocol, port and health criteria as shown in this screenshot. Based on this configuration, GCP computes a health state for each instance. The health criteria defines how often to check whether an instance is healthy. That's the check interval. How long to wait for a response, that's the time-out. How many successful attempts are decisive, that's the healthy threshold. And how many failed attempts are decisive, that's the unhealthy threshold. In the example on this slide, the health check would have to fail twice over a total of 15 seconds before an instance is considered unhealthy.

### Video - [Overview of HTTP(S) load balancing](https://www.cloudskillsboost.google/course_templates/506/video/375104)

- [YouTube: Overview of HTTP(S) load balancing](https://www.youtube.com/watch?v=DRtpmnsimDo)

Person: Now let's talk about HTTPS load balancing which acts at layer seven of the OSI model. This is the application layer which deals with the actual content of each message allowing for routing decisions based on the URL. GCPs HTTPS load balancing provides global load balancing for HTTPS requests destined for your instances. This means that your applications are available to your customers at a single Anycast IP address which simplifies your DNS setup. HTTPS load balancing balances HTTP and HTTPS traffic across multiple back-end instances and across multiple regions. HTTP requests are load balanced on port 80 or 8080 and HTTPS requests are load balanced on port 443. This load balancer supports both IPv4 and IPv6 clients, is scalable, requires no prewarming and enables content-based and cross regional load balancing. You can configure URL maps that route some URLs to one set of instances and route other URLs to other instances. Requests are generally routed to the instance group that is closest to the user. If the closest instance group does not have sufficient capacity, the request is sent to the next closest instance group that does have the capacity. You will get to explore most of these benefits in the first lab of the module. Let me walk through the complete architecture of an HTTPS load balancer by using this diagram. A global forwarding rule directs incoming requests from the Internet to a target HTTP proxy. The target HTTP proxy checks each request against a URL map to determine the appropriate back-end service for the request. For example, you can send requests for www.example.com/audio to one back-end service which contains instances configured to deliver audio files, and the requests for www.example.com/video to another back-end service which contains instances configured to deliver video files. The back-end service directs each request to an appropriate back-end based on serving capacity, zone and instance held of its attached backends. The back-end services contain a health check, session affinity, a time out setting and one or more backends. A health check pulls instances attached to the back-end service at configured intervals. Instances that pass the health check are allowed to receive new requests. Unhealthy instances are not sent requests until they are healthy again. Normally HTTPS load balancing uses a round Robin [Indistinct] to distribute requests among available instances. This can be overridden with session affinity. Session affinity attempts to send all requests from the same client to same virtual machine instance. Back-end services also have a time out setting which is set to 30 seconds by default. This is the amount of time the back-end service will wait on the backend before considering the request a failure. This is a fixed time out, not an idle time out. If you request longer lived connections set this value appropriately. The backends themselves contain an instance group, a balancing mode and a capacity scaler. An instance group contains virtual machine instances. The instance group may be a managed instance group with or without auto scaling or an unmanaged instance group. A balancing mode tells the load balancing system how to determine when the backend is at full usage. If all the backends for the back-end service in a region are at the full usage, new requests are automatically routed to the nearest region that can still handle requests. The balancing mode can be based on CPU utilization or requests per second. A capacity setting is an additional control that interacts with the balancing mode setting. For example, if you normally want your instances to operate at a maximum of 80 percent CPU utilization you would set your balancing mode to 80 percent CPU utilization and your capacity to 100 percent. If you want to cut instance utilization in half, you could leave the balancing mode at 80 percent CPU utilization and set capacity to 50 percent. Now any changes to your back-end services are not instantaneous, so don't be surprised if it takes several minutes for your changes to propagate throughout the network.

### Video - [HTTP(S) load balancing](https://www.cloudskillsboost.google/course_templates/506/video/375105)

- [YouTube: HTTP(S) load balancing](https://www.youtube.com/watch?v=DGUtmnAniOI)

Person: A HTTPS load balancer has the same basic structure as a HTTP load balancer, but it differs in the following ways. A HTTPS load balancer uses a target HTTPS proxy instead of a target HTTP proxy. A HTTPS load balancer requires at least one signed SSL certificate installed on the target HTTPS proxy for the load balancer. The client SSL sessions terminate at the load balancer. HTTPS load balancers support the quick transport layer protocol. QUIC is a transport layer protocol that allows for faster client connection initiation, eliminates head-of-line blocking in multiplex streams, and supports connection migration when a client's IP address changes. For more information on the QUIC protocol, see the link in the course resources. To use HTTPS, you must create at least one SSL certificate that can be used by the target proxy for the load balancer. You can configure the target proxy with up to 15 SSL certificates. For each SSL certificate, you need to create an SSL certificate resource which contains the SSL certificate information. SSL certificate resources are only used with load balancing proxies such as a target HTTPS proxy or target SSL proxy, which we'll discuss later in this module. Back-end buckets allow you to use Google Cloud Storage buckets with HTTPS load balancing. An external HTTPS load balancer uses a URL map to direct traffic from specified URLs to either a back-end service or a back-end bucket. One common use case is send requests for dynamic content, such as data, to a back-end service, and send requests for static content, such as images, to a back-end bucket. In this diagram, the load balancer sends traffic with a path of /love-to-fetch to a Cloud Storage bucket in the Europe North region. All the other requests go to a Cloud Storage bucket in the U.S. East region. After you configure a load balancer with the back-end buckets, requests to URL paths that begin with /love-to-fetch are sent to the Europe North Cloud Storage bucket, and all other requests are sent to the U.S. East Cloud Storage bucket. A network endpoint group, or NEG, is a configuration object that specifies a group of back-end endpoints or services. A common use case for this configuration is deploying services in containers. You can also distribute traffic in a granular fashion to applications running on your back-end instances. You can use NEGs as back ends for some load balancers and with Traffic Director. Zonal and Internet NEGs define how endpoints should be reached, whether they are reachable and where they are located. Unlike these NEG types, serverless NEGs don't contain endpoints. A zonal NEG contains one or more endpoints that can be Compute Engine VMs or services running on the VMs. Each endpoint is specified by either an IP address or an IP [Indistinct] port combination. An Internet NEG contains a single endpoint that is hosted outside of Google Cloud. This endpoint is specified by host name FQDN:port or IP:port. A hybrid connectivity NEG points to Traffic Director services running outside of Google Cloud. A serverless NEG points to Cloud Run, App Engine, Cloud Functions services residing in the same region as the NEG. For more information on using NEGs, please see the link in course resources.

### Lab - [Hosting a Web App on Google Cloud Using Compute Engine](https://www.cloudskillsboost.google/course_templates/506/labs/375106)

Deploy and scale a Web App on Google Compute Engine.
 

- [ ] [Hosting a Web App on Google Cloud Using Compute Engine](../labs/Hosting-a-Web-App-on-Google-Cloud-Using-Compute-Engine.md)

## Scaling and Automating Retail Application Infrastructure

In this module, you will learn how to scale and automate a retail application's infrastructure with Google Kubernetes Engine through a series of videos and hands-on labs.

### Video - [Module 3 Overview](https://www.cloudskillsboost.google/course_templates/506/video/375107)

- [YouTube: Module 3 Overview](https://www.youtube.com/watch?v=nXQf7J73nak)

in this section you've learned about scaling and automating your retail applications infrastructure digital Commerce has become a rapidly growing channel for sales customer expectations have risen dramatically in response to Rapid growth retailers need to automate and scale their applications in this section you learn the skills you need to overcome scalability and automation challenges you learn to architect the strategy to deploy a retail application on Google kubernetes engine containerize a retail application with Docker expose a retail application front end with a kubernetes service scale a retail application with multiple kubernetes replicas and modify and roll out a new version of a retail application with zero downtime let's get started

### Video - [Scaling and automating retail infrastructure](https://www.cloudskillsboost.google/course_templates/506/video/375108)

- [YouTube: Scaling and automating retail infrastructure](https://www.youtube.com/watch?v=ONl6FmFfrqA)

in this lesson you learn more about scaling and automating your retail applications infrastructure after surveying nearly 200 U.S retail Executives to understand their expectations and metrics for Success some interesting findings were revealed as digital sales become a bigger part of overall sales retailers will increasingly use traffic growth and website performance to measure success for example 55 percent of respondents said they are going to use traffic growth as the key metric to measure Black Friday and Cyber Monday outcomes just below the traditional sales volume metric 60 and just above customer satisfaction 54 digital Commerce has become a rapidly growing channel for sales customer expectations have risen dramatically in response bounce rates increase by 90 when a page load time goes from one to five seconds over half of mobile site visitors leave a page if it takes longer than three seconds to load around three-fourths of customers expect Brands to personalize content towards them one of the most common scenarios where customer expectations are crucial is during Black Friday and Cyber Monday imagine a customer visiting a site and turning away because of low performance lag slow loading speeds or crashes this incident results in poor customer experience lower Grand reputation lost sales and lower customer lifetime value as stated previously even though retailers want to achieve traffic growth they still face many challenges for example based on a recent survey one in 10 retail Executives reported that their company's website went down during Black Friday or Cyber Monday last year and 4 and 10 said they experienced an outage within the past three years that's a lot of lost Revenue these technical challenges illustrate the impact that a slow or unavailable website can have on a brand and Customer Loyalty 91 percent of respondents said they have left a website because it was too slow and thirty percent of Shoppers said they would think twice before using that retailer again but are retailers addressing this somewhat surprisingly nearly a quarter of respondents 24 percent do not have a plan in place should their website go down during this time so retailers are not fully prepared and not entirely confident some of these pain points can be attributed to design patterns that are common in retail applications shown here is a typical deployment model that retails use this platform follows the three-tier model web business logic and database for the web tier it serves HTTP content back to the user and provides some load balancing functionality the application tier provides Corey Commerce functionality the database tier uses relational databases to support e-commerce transactions typically an application is a single Deployable unit providing core e-commerce functionality additionally front-end and back-ends are tightly coupled both shipped to the same package and deployed together as part of the e-commerce application all application data is held by a single relational database all requests are handled by a single common application code base on the platform even though the e-commerce monolith has been the typical deployment model over the years it presents many challenges not only affecting retailers that also has large implications for the customer experience more specifically it contains hard and slow to scale Hardware that makes it a challenge to provision for Peak loads it has low agility for development as the code base is a single product with many interdependencies this leads to increased difficulty to release new customer facing features due to the single application base for all functionality low agility also leads to the slow rollout of security patches bug fixes and feature releases e-commerce monoliths also produce slow to little Innovation package e-commerce features are underutilized and lagged behind the latest Technology Innovation and lastly most monolithic architectures do not support multiple channels and to address this most retailers have had to build separate systems for each Channel each with their own back end so how do we handle these challenges moving to a containerized architecture gives core capabilities that allow you to overcome the challenges of the traditional monolithic e-commerce architecture more specifically it provides scalability you can scale up or down as needed when traffic spikes or drops e-commerce companies can scale their storefronts best on Google Cloud flexibility you can build and test e-commerce applications easily without disrupting current website content you can break down all e-commerce functionality into decoupled microservices which increases your agility agility and Innovation Google Cloud can host e-commerce pages on its content delivery Network or CVN resulting in much better performance and improving conversion Google Cloud provides e-commerce companies the best uptime every year since 2017 and has specific Black Friday programs to plan and manage critical windows together more developer agility leads to more innovation in the organization for example development or sandbox environments can be provisioned on demand to experiment with new features for innovation retail Commerce workloads require a number of cloud native features in order to meet demand from an ever-growing number of consumer devices and platforms typically these deployments must be multi-region to serve a global customer base they must support some degree of Auto scaling or scheduled scaling scaling up to meet Peak demand during busy seasons and scaling down to reduce infrastructure costs when demand is lower retail Commerce deployments must be able to deliver features and functionality to customers quickly and efficiently to meet changing Market demands retail Commerce deployments should also take advantage of managed infrastructure to allow developers to focus on customer facing functionality finally these deployments must be centrally secured and managed Google kubernetes engine is a good fit for all of these requirements individual microservices can be deployed and scaled independently of one another which lets you rapidly deliver new features and functionality Services can be small modular Loosely coupled and organized around your specific business capabilities and needs depicted here is a sample reference architecture of a modernized e-commerce platform on Google Cloud here are some of its core attributes microservices-based architecture with Services running on kubernetes clusters solution deployed in two Google Cloud regions for high availability and redundancy leverages load balancing and unmanaged services to deliver features and functionality to customers quickly and efficiently to summarize there are a number of advantages of just running a platform on Google Cloud highlighted on the left instead of focusing on running infrastructure retailers can let Google do that it's part of our core set of competencies this allows retailers to focus on rapid innovation in their Core Business to meet the demands on their end consumer the full benefit of modernization is summarized on the right agility is key breaking up the Legacy monolithic platform into microservices allows the retailer to iterate quickly at lower risk over new and improved features to drive sales and what better place to run microservices than kubernetes on gke using a customized platform allows the retailer to innovate without and deliver differentiated experiences integration with Advanced data analytics and machine learning Technologies on our Cloud boosts their sales revenue which we'll cover in a later lesson now you'll learn more about the ins and outs of gke and afterwards you'll receive Hands-On practice deploying scaling and updating an e-commerce site on Google kubernetes engine in a lab

### Video - [Introduction to Containers](https://www.cloudskillsboost.google/course_templates/506/video/375109)

- [YouTube: Introduction to Containers](https://www.youtube.com/watch?v=-nsqdFpk0oI)

Let's start by introducing containers. In this video, you'll learn about the key features of containers and the advantages of using containers for application deployment compared to alternatives such as deploying apps directly to virtual machines. You'll learn about Google's Cloud Build, and then you can see how you can use it to build and manage your application images. It's now not very long ago, the default way to deploy an application was on its own physical computer. To set one up, you'd find some physical space, power, cooling, network connectivity for it, and then install an operating system, any software dependencies, and then finally the application itself. If you need more processing power, redundancy, security, or scalability, what'd you do? Well you'd have to simply add more computers. It was very common for each computer to have a single-purpose. For example, a database, web server, or content delivery. This practice as you might imagine, wasted resources and it took a lot of time to deploy and maintain and scale. It also wasn't very portable at all. Applications were built for a specific operating system and sometimes even for specific hardware as well. In comes the dawn of virtualization. Virtualization helped by making it possible to run multiple virtual servers and operating systems on the same physical computer. A hypervisor is the software layer that breaks the dependencies of an operating system with its underlying hardware, and allow several virtual machines to share that same hardware. KVM is one well-known hypervisor. Today you can use virtualization to deploy new servers fairly quickly. Now adopting virtualization means that it takes us less time to deploy new solutions, we waste less of the resources on those physical computers that we're using, and we get some improved portability because virtual machines can be imaged and then moved around. However, the application, all of its dependencies and operating system are still bundled together and it's not very easy to move from a VM from one hypervisor product to another. Every time you start up a VM, it's operating system still takes time to boot up. Running multiple applications within a single VM also creates another tricky problem, applications that share dependencies are not isolated from each other, the resource requirements from one application, can starve out other applications of the resources that they need. Also, a dependency upgrade for one application might cause another to simply stop working. You can try to solve this problem with rigorous software engineering policies. For example, you could lock down the dependencies that no application is allowed to make changes, but this leads to new problems because dependencies do need to be upgraded occasionally. You can add integration tests to ensure that applications work. Integration tests are great, but dependency problems can cause new failure modes that are harder to troubleshoot, and it really slows down development if you have to rely on integration tests to simply just perform basic integrity checks of your application environment. Now, the VM-centric way to solve this problem is to run a dedicated virtual machine for each application. Each application maintains its own dependencies, and the kernel is isolated. So one application won't affect the performance of another. One you can get as you can see here, is two complete copies of the kernel that are running. But here too we can run into issues as you're probably thinking. Scale this approach to hundreds of thousands of applications, and you can quickly see the limitation. Just imagine trying to do a simple kernel update. So for large systems, dedicated VMs are redundant and wasteful. VMs are also relatively slow to start up because the entire operating system has to boot. A more efficient way to resolve the dependency problem is to implement abstraction at the level of the application and its dependencies. You don't have to virtualize the entire machine or even the entire operating system, but just the user space. Again, the user space is all the code that resides above the kernel, and includes the applications and their dependencies. This is what it means to create containers. Containers are isolated user spaces for running application code. Containers are lightweight because they don't carry a full operating system, they can be scheduled or packed tightly onto the underlying system, which is very efficient. They can be created and shut down very quickly because you're just starting and stopping the processes that make up the application and not booting up an entire VM and initializing an operating system for each application. Developers appreciate this level of abstraction because they don't want to worry about the rest of the system. Containerization is the next step in the evolution of managing code. You now understand containers as delivery vehicles for application code, they're lightweight, stand-alone, resource efficient, portable execution packages. You develop application code in the usual way, on desktops, laptops, and servers. The container allows you to execute your final code on VMs without worrying about software dependencies like application run times, system tools, system libraries, and other settings. You package your code with all the dependencies it needs, and the engine that executes your container, is responsible for making them available at runtime. Containers appeal to developers because they're an application-centric way to deliver high performance and scalable applications. Containers also allow developers to safely make assumptions about the underlying hardware and software. With a Linux kernel underneath, you no longer have code that works in your laptop but doesn't work in production, the container's the same and runs the same anywhere. You make incremental changes to a container based on a production image, you can deploy it very quickly with a single file copy, this speeds up your development process. Finally, containers make it easier to build applications that use the microservices design pattern. That is, loosely coupled, fine-grained components. This modular design pattern allows the operating system to scale and also upgrade components of an application without affecting the application as a whole.

### Video - [Containers and Container Images](https://www.cloudskillsboost.google/course_templates/506/video/375110)

- [YouTube: Containers and Container Images](https://www.youtube.com/watch?v=HMpDYTEiL9g)

An application and its dependencies are called an image. A container is simply a running instance of an image. By building software into container images, developers can easily package and ship an application without worrying about the system it will be running on. You need software to build container images and to run them. Docker is one tool that does both. Docker is an open-source technology that allows you to create and run applications in containers, but it doesn’t offer a way to orchestrate those applications at scale as Kubernetes does. In this course, we will use Google’s Cloud Build to create Docker-formatted container images. Containers are not an intrinsic, primitive feature of Linux. Instead, their power to isolate workloads is derived from the composition of several technologies. One foundation is the Linux process. Each Linux process has its own virtual memory address space, separate from all others, and Linux processes are rapidly created and destroyed. Containers use Linux namespaces to control what an application can see: process ID numbers, directory trees, IP addresses, and more. By the way, Linux namespaces are not the same thing as Kubernetes namespaces, which you will learn about later in this course. Containers use Linux cgroups to control what an application can use: its maximum consumption of CPU time, memory, I/O bandwidth, and other resources. Finally, containers use union file systems to efficiently encapsulate applications and their dependencies into a set of clean, minimal layers. Now let’s see how this works. A container image is structured in layers. The tool you use to build the image reads instructions from a file called the “container manifest.” In the case of Docker-formatted container images, that’s called a Dockerfile. Each instruction in the Dockerfile specifies a layer inside the container image. Each layer is read-only. (When a container runs from this image, it will also have a writable, ephemeral topmost layer.) Let’s look at a simple Dockerfile. This Dockerfile will contain four commands, each of which creates a layer. (At the end of this discussion, I’ll explain why this Dockerfile is a little oversimplified for modern use.) The FROM statement starts out by creating a base layer, pulled from a public repository. This one happens to be the Ubuntu Linux runtime environment of a specific version. The COPY command adds a new layer, containing some files copied in from your build tool’s current directory. The RUN command builds your application using the “make” command and puts the results of the build into a third layer. And finally, the last layer specifies what command to run within the container when it is launched. Each layer is only a set of differences from the layer before it. When you write a Dockerfile, you should organize from the layers likely to change, through to the layers most likely to change. By the way, I promised that I would explain how this Dockerfile example is oversimplified. These days, the best practice is not to build your application in the very same container that you ship and run. After all, your build tools are at best just clutter in a deployed container, and at worst they are an additional attack surface. Today, application packaging relies on a multi-stage build process, in which one container builds the final executable image, and a separate container receives only what is needed to run the application. Fortunately for us, the tools we use support this practice. When you launch a new container from an image, the container runtime adds a new writable layer on top of the underlying layers. This layer is often called the container layer. All changes made to the running container, such as writing new files, modifying existing files, and deleting files, are written to this thin writable container layer. And they’re ephemeral: When the container is deleted, the contents of this writable layer are lost forever. The underlying container image remains unchanged. This fact about containers has an implication for your application design: whenever you want to store data permanently, you must do so somewhere other than a running container image. You will learn about several choices in this specialization. Because each container has its own writable container layer, and all changes are stored in this layer, multiple containers can share access to the same underlying image and yet have their own data state. The diagram shows multiple containers sharing the same Ubuntu image. Because each layer is only a set of differences from the layer before it, you get smaller images. For example, your base application image may be 200 MB, but the difference to the next point release might only be 200 KB. When you build a container, instead of copying the whole image, it creates a layer with just the difference. When you run a container, the container runtime pulls down the layers it needs. When you update, you only need to copy the difference. This is much faster than running a new virtual machine. It’s very common to use publicly available open-source container images as the base for your own images, or for unmodified use. For example, you’ve already seen the “ubuntu” container image, which provides a Ubuntu Linux environment inside a container. “Alpine” is a popular Linux environment in a container, noted for being very small. The nginx web server is frequently used in its container packaging. Google maintains a container registry, gcr.io. This registry contains many public, open-source images, and Google Cloud customers also use it to store their private images in a way that integrates with Cloud IAM. Google Container Registry is integrated with Cloud IAM, so, for example, you can use it to store images that aren’t public -- instead, they are private to your project. You can also find container images in other public repositories: Docker Hub Registry, GitLab, and others. The open-source docker command is a popular way to build your own container images. It’s widely known and widely available. One downside of building containers with the docker command is that you must trust the computer that you do your builds on. Google provides a managed service for building containers that’s integrated with Cloud IAM. This service is called Cloud Build, and we’ll use it in this course. Cloud Build can retrieve the source code for your builds from many code repositories, including Cloud Source Repositories, or git-compatible repositories like GitHub and Bitbucket. To generate a build with Cloud Build, you define a series of steps. For example, you can configure build steps to fetch dependencies, compile source code, run integration tests, or use tools such as Docker, Gradle, and Maven. Each build step in Cloud Build runs in a Docker container. Then Cloud Build can deliver your newly built images to various execution environments: not only GKE, but also App Engine and Cloud Functions.

### Video - [Introduction to Kubernetes](https://www.cloudskillsboost.google/course_templates/506/video/375111)

- [YouTube: Introduction to Kubernetes](https://www.youtube.com/watch?v=BAkE9-h-DEw)

Now, let's introduce a popular container management and orchestration solution called Kubernetes. Let's say your organization has really embraced the idea of containers. Because containers are so lean, your coworkers are creating them in numbers far exceeding the counts of virtual machines you used to have, and the applications running in them need to communicate over the network. But you don't have a network fabric that lets containers find each other. You need help. How can you manage your container infrastructure better? Kubernetes is an open source platform that helps you orchestrate and manage your container infrastructure on-premises or in the Cloud. So what is Kubernetes? It's a container-centric management environment. Google originated it and then donated it to the open source community. Now it's a project of the vendor-neutral Cloud Native Computing Foundation. It automates the deployment, scaling, load balancing, logging, monitoring, and other management features of containerized applications. These are the features that are characteristic of a typical platform as service solutions. Kubernetes also facilitates the features of an infrastructure as a service, such as allowing a wide range of user preferences and configuration flexibility. Kubernetes supports declarative configurations. When you administer your infrastructure declaratively, you describe the desired state you want to achieve instead of issuing a series of commands to achieve that desired state. Kubernetes' job is to make the deployed system conform to your desired state and then keep it there in spite of failures. Declarative configuration saves you work. Because the system's desired state is always documented, it also reduces the risk of error. Kubernetes also allows imperative configuration in which you issue commands to change the system state. But administering Kubernetes as scale imperatively, will be a big missed opportunity. One of the primary strengths of Kubernetes is its ability to automatically keep a system in a state that you declare. Experienced Kubernetes administrators use imperative configuration only for quick temporary fixes and as a tool in building a declarative configuration. Now that you know what Kubernetes is, let's talk about some of its features. Kubernetes supports different workload types. It supports stateless applications such as an Nginx or Apache web server, and stateful applications, where user and session data can be stored persistently. It also supports batched jobs and daemon tasks. Kubernetes can automatically scale in and out containerized applications based on resource utilization. You can specify resource request levels and resource limits for your workloads and Kubernetes will obey them. These resource controls let Kubernetes improve overall workload performance within the cluster. Developers extend Kubernetes through a rich ecosystem of plugins and add-ons. For example, there's a lot of creativity going on currently with Kubernetes custom resource definitions, which bring the Kubernetes declarative management model to amazing variety of other things that need to be managed. The primary focus of this specialization though, is architecting with Kubernetes because it's provided as a service by Google Cloud. So extending Kubernetes is not within our scope. Because it's open source, Kubernetes also supports workload portability across on-premises or multiple Cloud service providers such as GCP and others. This allows Kubernetes to be deployed anywhere. You can move Kubernetes workloads freely without a vendor lock-in.

### Video - [Introduction to Google Kubernetes Engine](https://www.cloudskillsboost.google/course_templates/506/video/375112)

- [YouTube: Introduction to Google Kubernetes Engine](https://www.youtube.com/watch?v=LLcbIIQS-wM)

Google cloud's managed service offering for Kubernetes is called Google Kubernetes Engine, or GKE. So why do people choose it? What if you've begun using Kubernetes in your environment, but the infrastructure has become too much of a burden for you to maintain? Is there anything within Google Cloud Platform that can help you? Absolutely, totally, that's going to be Google Kubernetes Engine, or GKE. GKE, let's talk about what it can do. It will help you deploy, manage, and scale Kubernetes environments for your containerized applications on GCP. More specifically, GKE is a component of the GCP compute offerings, it makes it easy to bring your Kubernetes workloads into the cloud. GKE is fully-managed, which means that you don't have to provision the underlying resources. GKE uses a container- optimized operating system. These operating systems are maintained by Google. They are optimized to scale quickly and with a minimal resource footprint. The container-optimized OS, it will be discussed later on in this course. When you use GKE, you start by directing the service to instantiate a Kubernetes system for you. This system is called a cluster. GKE's auto-upgrade feature can be enabled to ensure that your clusters are automatically upgraded with the latest and greatest version of Kubernetes. The virtual machines that host your containers inside of a GKE cluster are called nodes. If you enable GKE's auto repair feature, the service will automatically repair unhealthy nodes for you. It'll make periodic health checks on each node in the cluster. If a node is determined to be unhealthy and requires repair, GKE will drain the node. In other words, it will cause it's workloads to gracefully exit and then recreate that node. Just as Kubernetes supports scaling workloads, GKE supports scaling the cluster itself. GKE seamlessly integrates with Google Cloud's Build and Container Registry. This allows you to automate deployment using private container images that you've securely stored in Container Registry. GKE also integrates with Google's Identity and Access Management, which allows you to control access through the use of accounts and role permissions. Stackdriver, this Google Cloud system for monitoring and management for services, containers, applications, and infrastructure. GKE integrates with Stackdriver Monitoring to help you understand your application's performance. GKE is integrated with Google virtual private clouds or VPC's, it makes use of GCP's networking features. And finally, the GCP Console provides insights into GKE clusters and the resources, and allows you to view, inspect, and delete resources in those clusters. You might be aware that open source Kubernetes contains a dashboard, but it takes a lot of work to set it up securely. But the GCP Console is a dashboard for your GKE clusters and workloads they don't have to manage. It's more powerful than the Kubernetes dashboard.

### Video - [Services and scaling](https://www.cloudskillsboost.google/course_templates/506/video/375113)

- [YouTube: Services and scaling](https://www.youtube.com/watch?v=35fmD7piXHk)

You now understand a Deployment will maintain the desired number of replicas for an application. However, at some point you probably need to scale the Deployment. Maybe you need more web frontend instances, for example. You can scale the Deployment manually using a kubectl command, or in the Cloud Console by defining the total number of replicas. Also, manually changing the manifest will scale the Deployment. You can also auto scale the Deployment by specifying the minimum and maximum number of desired Pods along with a CPU utilization threshold. Again, you can perform autoscaling using the kubectl autoscale command, or from the Cloud Console directly. This leads to the creation of a Kubernetes object called the HorizontalPodAutoscaler. This object performs the actual scaling to match the target CPU utilization. Keep in mind, we're not scaling the cluster as a whole, just the particular Deployment within that cluster. While Horizontal Pod autoscaling is indicated for sudden increases in resource usage. Vertical Pod autoscaling provides recommendations for resource usage over time. Multidimensional autoscaling frees you from choosing a single way to scale your clusters. With multidimensional Pod autoscaling, you can use horizontal scaling based on CPU and vertical scaling based on memory at the same time.

### Document - [A note about Services](https://www.cloudskillsboost.google/course_templates/506/documents/375114)

### Video - [Deployments](https://www.cloudskillsboost.google/course_templates/506/video/375115)

- [YouTube: Deployments](https://www.youtube.com/watch?v=JM6IZnh2z20)

Deployments describe a desired state of Pods. For example, a desired state could be that you want to make sure that you have five nginx Pods running at all times. It's declarative stance means that Kubernetes will continuously make sure this configuration is running across your cluster. Kubernetes also supports various update mechanisms for Deployments, which I'll tell you about later in this module. Deployments declare the state of Pods. Every time you update the specification of the Pods, for example, updating them to a newer container image, a new ReplicaSet is created that matches the altered version of the Deployment. This is how deployments roll out updated Pods in a controlled manner: old Pods are removed from the old ReplicaSet and replaced with newer Pods in a new ReplicaSet. If the updated Pods are not stable, the administrator can roll back the Pods to a previous deployment revision. You can scale Pods manually by modifying the Deployment configuration. You can also configure the Deployment to manage the workload automatically. Deployments are designed for stateless applications. Stateless applications don't store data or application state to a cluster or to persistent storage. A typical example of a stateless application is a web front end. Some backend owns the problem of making sure that data gets stored durably, and you'll use Kubernetes objects other than Deployments to manage these backends. The desired state is described in a Deployment yaml file containing the characteristics of the Pods, coupled with how to operationally run these Pods and their lifecycle events. After you submit this file to the Kubernetes control plane, it creates a deployment controller, which is responsible for converting the desired state into reality and keeping that desired state over time. Remember what a controller is: it's a loop process created by Kubernetes that takes care of the routine tasks to ensure the desired state of an object, or a set of objects, running on the cluster matches the observed state. During this process, a ReplicaSet is created. A ReplicaSet is a controller that ensures that a certain number of Pod replicas are running at any given time. The Deployment is a high-level controller for a Pod that declares it's state. The Deployment configures a ReplicaSet controller to instantiate and maintain a specific version of the Pods specified in the Deployment. Here's a simple example of a Deployment object file in yaml format. The Deployment named 'my-app' is created with three replicated Pods. In the 'spec.template' section, a Pod template defines the metadata and specification for each of the Pods in this ReplicaSet. In the Pod specification, an image is pulled from the Google Container Registry, and port 8080 is exposed to send and accept traffic for the container. Any Deployment has three different lifecycle states. The Deployment's 'Progressing' state indicates that a task has been performed. What tasks? Creating a new ReplicaSet, or scaling up, or scaling down a ReplicaSet. The Deployment's 'Complete' state indicates that all new replicas have been updated to the latest version, and are available, and no old replicas are running. Finally, the 'Failed' state occurs when the creation of a new ReplicaSet could not be completed. Why might this happen? Maybe Kubernetes couldn't pull the images for the new Pods. Or maybe there wasn't enough for some resource quota to complete the operation. Or maybe the user who launched the operation lacks permissions. When you apply many small fixes across many rollouts, that translates to a large number of revisions, and to management complexity. You have to remember which small fix was applied to which rollout, which can make it challenging to figure out which revision to roll back to when issues arise. Remember, earlier in the specialization, we recommended that you keep your yaml files in a source code repository? That can help manage some of this complexity.

### Video - [Ways to Create Deployments](https://www.cloudskillsboost.google/course_templates/506/video/375116)

- [YouTube: Ways to Create Deployments](https://www.youtube.com/watch?v=1NeUoatBXGs)

You can create a deployment in three different ways. First, you create the deployment decoratively using a manifest file such as the YAML file you've just seen and kubectl apply command. The second method creates a deployment imperatively using a kubectl run command that specifies the parameters inline. Here the image and tag specifies which image and image version to run in the container. This deployment will launch three replicas and expose port 8080. Labels are defined using key and value dash hash generator specifies the API version to be used and dash hash saved dash config saves the configuration for future use. Your third option is to use the GKE workloads menu in the GCP console. Here you can specify the container image in version or even select it directly from the container registry. You can specify environment variables and initialization commands. You can also add an application name and namespace along with labels. You can use the view YAML button on the last page of the deployment wizard to view that deployment specification in YAML format. The replica set created by the deployment ensures that the desired number of pods are running and always available at any given time. If a pod fails or is evicted, the replica set automatically launches a new pod. You can use the kubectl get and describe commands to inspect the state and details of the deployment. As shown here, you can get the desired current, up-to-date, and available status of all the replicas within the deployment along with their agents using the kubectl get deployment command. Desired shows the desired number of replicas in the deployment specification. Current is the number of replicas currently running. Up-to-date shows the number of replicas that are fully up-to-date as per the current deployment specification. Available displays the number of replicas available to the users. You can also output the deployment configuration in a YAML format. This is a useful trick, maybe you originally created a deployment with kubectl run, then you decided you'd like to make it permanent managed part of your infrastructure. Edit that YAML file to remove the unique details of the deployment you created it from and then you can add it to your repository of YAML files for future deployments. For more detailed information about the deployment, use the kubectl describe command. You'll learn more about this command in the lab. Another way to inspect a deployment is the use of GCP console. Here you can see detailed information about the deployment revision history, the pods, events, and also view the live configuration in YAML format.

### Video - [Managing Deployments](https://www.cloudskillsboost.google/course_templates/506/video/375117)

- [YouTube: Managing Deployments](https://www.youtube.com/watch?v=IJtMaveGbMI)

When you edit a deployment your action normally triggers and automatic rollout, but if you have an environment where small fixes are released frequently, we'll have a large number of rollouts. In a situation like that, you'll find it more difficult to link issues with specific rollouts. To help, you can temporarily pause these roll-outs by using the kubectl rollout pause command. The initial state of the deployment prior to pausing will continue its function, but new updates suited deployment will not have any effect while the rollout is paused. The changes will only be implemented once a rollout is resumed. When you resume the rollout all these new changes will be rolled out with a single revision. You can also monitor the rollout series by using the kubectl rollout status command. What if you're done with a deployment? You can delete it easily using the kubectl delete command and you can also delete it from the GCP cancel. Either way, kubernetes will delete all resources managed by the deployment especially running pods

### Video - [Updating Deployments](https://www.cloudskillsboost.google/course_templates/506/video/375118)

- [YouTube: Updating Deployments](https://www.youtube.com/watch?v=pv2A2LkDeAk)

When you make a change to a Deployment's Pod specification, such as changing the image version, an automatic update rollout happens. Again, note that these automatic updates are only applicable to the changes in Pod specifications. You can update a Deployment in different ways. One way is to use the kubectl apply command with an updated Deployment specification yaml file. This method allows you to update other specifications of a Deployment, such as the number of replicas outside the Pod template. Another way is to use a kubectl set command. This allows you to change the Pod template specifications for the Deployment, such as the image, resources, or selector values. Another way is to use the kubectl edit command. This opens the specification files using the vim editor that allows you to make changes directly. Once you exit and save the file, kubectl automatically applies the updated file. The last option for you to update a Deployment is through the GCP Console. You can edit the Deployment manifest from the GCP Console and perform a rolling update along with its additional options. Rolling updates are discussed next. Let's look at how a Deployment is updated. When a Deployment is updated, it launches a new ReplicaSet and creates a new set of Pods in a controlled fashion. First, new Pods are launched in a new ReplicaSet. Next, all Pods are deleted from the old ReplicaSet. This is an example of a rolling update strategy also known as a ramped strategy. Its advantage is that updates are slowly released, which ensures the availability of the application. However, this process can take time, and there's no control over how the traffic is directed to the old and new Pods.

### Video - [Blue-Green Deployments](https://www.cloudskillsboost.google/course_templates/506/video/375119)

- [YouTube: Blue-Green Deployments](https://www.youtube.com/watch?v=GubUG49tXeI)

We haven't yet discussed how to locate and connect to the applications running in these Pods, especially as new Pods are created or updated by your Deployments. While you can connect to individual Pods directly, Pods themselves are transient. The Kubernetes Service is a static IP address that represents a Service, or a function, in your infrastructure. It's a network abstraction for a set of Pods that deliver that Service, and it hides the ephemeral nature of the individual Pods. Services will be covered in detail in the Networking module. A blue-green deployment strategy is useful when you want to deploy a new version of an application and also ensure that application services remain available while the Deployment is updated. With a blue-green update strategy, a completely new Deployment is created with a newer version of the application. In this case, it's 'my-app-v2'. When the Pods in the new Deployment are ready, the traffic can be switched from the old blue version to the new green version. But how can you do this? This is where a Kubernetes Service is used. Services allow you to manage the network traffic flows to a selection of Pods. This set of Pods is selected using a label selector. Let's take a closer look at how this is implemented. Here, in the Service definition, Pods are selected based on the label selector, where Pods in this example belong to 'my-app' and to version 'v1'. When a new Deployment, labeled 'v2' in this case, is created and is ready, the version label in the Service is changed to the newer version, labeled 'v2' in this example. Now, the traffic will be directed to the newest set of Pods, the green deployment with the 'v2' version label, instead of the old blue deployment Pods that have the 'v1' version label. The blue deployment with the older version can then be deleted. The advantage of this update strategy is that rollouts can be instantaneous and the newer versions can be tested internally before releasing them to the entire user base, for example, by using a separate service definition for test user access. The disadvantage is that the resource usage is doubled during the Deployment process.

### Video - [Canary Deployments](https://www.cloudskillsboost.google/course_templates/506/video/375120)

- [YouTube: Canary Deployments](https://www.youtube.com/watch?v=-EkEBtBCNuU)

Hi, I'm Eoin Carrol, and I'll be teaching a couple of lectures in this course. In this video we're going to be covering canary deployments. The canary method is another update strategy based on the blue/green method. The traffic has gradually shifted to the new version. The main advantages of using canary deployments are that you can minimize the excess resource usage during the updates. Because the rollout is gradual, issues can be identified before they affect all instances of the application. In this example, 100% of the application traffic is directed initially to my-app-v1. When the canary deployment starts, a subset of the traffic, 10% in this case, is redirected to the new version my-app-v2. When the stability of the new version is confirmed, 100% of the traffic can be routed to this new version. But how is this done? In the blue/green update strategy previously covered, both app and version labels were selected by the Service, so traffic would only be sent to the Pods that are running the version defined in the Service. In a Canary update strategy, the Service selector is based on the application label and does not specify the version. The selector in this example covers all Pods with the app: my-app label. This means that with this Canary update strategy version of the Service, traffic is sent to all Pods regardless of the version label. This setting allows the Service to select and direct traffic to Pods from both deployments. Initially, the new version of the deployment will start with zero replicas running. Over time, as the new version is scaled up, the old version of the deployment can be scaled down and eventually deleted. With the Canary update strategy, a subset of users will be directed to the new version. This allows you to monitor for errors and performance issues as these users use the new version and you can quickly roll back, minimizing the impact to your overall user base if any issues arise. However, the complete rollout of a deployment using the Canary strategy can be a slow process and may require tools such as Istio to accurately shift the traffic. There are other deployment strategies such as A/B testing and shadow testing, but these strategies are outside the scope of this course. A Service configuration does not normally ensure that all requests from a single client will always connect to the same Pod. Each request is treated separately and can connect to any Pod deployment. This potentially can cause issues if there are significant changes in the functionality between Pods, as may happen with a Canary deployment. To prevent this, you can set the sessionAffinity field to ClientIP in the specification of the Service if you need a client's first request to determine which Pod will be used for all subsequent connections. With A/B testing, you test a hypothesis by using variant implementations. A/B testing is used to make business decisions (not only predictions) based on the results derived from data. When you perform an A/B test, you route a subset of users to new functionality based on routing rules, as shown in the example here. Routing rules often include factors such as browser version, user agent, geolocation and operating system. After you measure and compare the versions, you update the production environment with the version that yielded better results. A/B testing is best used to measure the effectiveness of functionality in an application. Use cases for the deployment patterns discussed earlier focus on releasing new software safely and rolling back predictably. In A/B testing, you control your target audience for the new features and monitor any statistically significant differences in the user behavior. Sequential experiment techniques like canary testing can potentially expose customers to an inferior application version during the early stages of the test. You can manage this risk by using offline techniques like simulation. However, offline techniques do not validate the applications improvements because there is no user interaction with the new versions. With shadow testing, you deploy and run a new version alongside the current version, but in such a way that the new version is hidden from users, as the diagram shown illustrates. An incoming request is mirrored and replayed in the test environment. This process can happen either in real time or asynchronously after a copy of the previously captured production traffic is replayed against the newly deployed service. You need to ensure that the shadow tests do not trigger side effects that can alter the existing production environment or the user state. Shadow testing has many key benefits. Because the traffic is duplicated, any bugs and services that are processing the shadow data have no impact on production. When used with tools such as Diffy, traffic shadowing lets you measure the behavior of your Service against live production traffic. This ability lets you test for errors, exceptions, performance and result parity between application versions. Finally, there is a reduced deployment risk. Traffic shadowing is typically combined with other approaches like canary testing. After testing a new feature by using traffic shadowing, you then test the user experience by gradually releasing the feature to an increasing number of users over time. No full rollout occurs until the application meets stability and performance requirements. You can deploy and release your application in several ways. Each approach has its advantages and disadvantages. The best choice comes down to the needs and constraints of your business. You should consider the following when choosing the right approach. What are your most critical considerations? For example, is downtime acceptable? Do costs constrain you? Does your team have the right skills to undertake a complex rollout and rollback setup? Do you have tight testing controls in place, or do you want to test new releases against production traffic to ensure the stability of the release and limit any negative impact? Do you want to test features among a pool of users, to cross verify certain business hypotheses? Can you control whether targeted users accept the update? For example, updates on mobile devices require explicit user action and might require extra permissions. Are microservices in your environment fully autonomous? Or do you have a hybrid of micro-service style applications working alongside traditional, difficult to change applications? Does the new release involve any schema changes. If yes, are the schema changes too complex to decouple from the code changes? The table shown here summarizes the salient characteristics of the deployment and testing patterns. When you weigh the advantages and disadvantages of various deployment and testing approaches, consider your business needs and technological resources, and then select the option that benefits you the most. So that's 'rollout'. Next, let's discuss how to roll back updates, especially in rolling update and recreate strategies. You roll back using the kubectl 'rollout undo' command. A simple 'rollout undo' command will revert to deployment to its previous revision. You can rollback to a specific version by specifying the revision number. If you're not sure the changes, you can inspect the rollout history using kubectl 'rollout history' command. The Cloud Console does not have a direct rollback feature; however, you can start Cloud Shell from your Console and use these commands. The Cloud Console also shows you the revision list with summaries and creation dates. By default, the details of the 10 previous ReplicaSets are retained, so that you can roll back to them. You can change this default by specifying a revision history limit under the Deployment specification.

### Lab - [Deploy, Scale, and Update Your Website on Google Kubernetes Engine](https://www.cloudskillsboost.google/course_templates/506/labs/375121)

Deploy a website to Google Kubernetes Engine, scale it out to more instances, and deploy a new version using Rolling Updates.
 

- [ ] [Deploy, Scale, and Update Your Website on Google Kubernetes Engine](../labs/Deploy-Scale-and-Update-Your-Website-on-Google-Kubernetes-Engine.md)

## Managing and Analyzing Ecommerce Data

In this module, you will learn how to manage and analyze ecommerce data through a series of videos and hands-on labs.

### Video - [Module 4 Overview](https://www.cloudskillsboost.google/course_templates/506/video/375122)

- [YouTube: Module 4 Overview](https://www.youtube.com/watch?v=KiCH2QQZu8M)

welcome to this section on managing and analyzing e-commerce data with Google cloud data is critical for all Industries and Retail is no exception managing and making sense of your e-commerce data can help you understand your customers better and accelerate your ability to digitally transform and reimagine your business in this section you learn to ingest eCommerce transactional data into bigquery explore the data you ingested using SQL and bigquery ingest eCommerce transactional data into cloud storage and Export and analyze e-commerce transactional data with Google Sheets let's begin

### Video - [Introduction to the Google Analytics ecommerce dataset](https://www.cloudskillsboost.google/course_templates/506/video/375123)

- [YouTube: Introduction to the Google Analytics ecommerce dataset](https://www.youtube.com/watch?v=mAqol8cfjlk)

One of the hardest parts about making a course like this is finding a practice data set that any analyst can relate to and want to query for insights. Now, with the latest edition of this course, we're excited to bring you a real products and e-commerce dataset with over a million site hits in a year of transaction records from Google's own online store, where you can buy Google branded merchandise. These transactions have been recorded through Google Analytics and made available publicly through BigQuery. With the knowledge of BigQuery that you'll practice throughout this course, you'll be able to answer real world e-commerce questions like, which customers have added items to their carts but then abandoned them? How do I create a review of those customers that can nudge them to complete their transactions? How do I set up cohorts or a segment of my customers based on their behavior so I can target each one of them in a more personalized way? Or even what keywords and referring partner sites have led our visitors to our e-commerce site, and what pages did they visit while they were there? My personal favorite for analysis, how close were they to transacting? The great part about working with this dataset, is all the queries and the insights that you derive here are going to be directly applicable to your own Google Analytics datasets. Now, if you're not a fan of e-commerce, the analytical tips and techniques that we'll cover here are sure to be useful in your own queries.

### Video - [Common data exploration techniques](https://www.cloudskillsboost.google/course_templates/506/video/375124)

- [YouTube: Common data exploration techniques](https://www.youtube.com/watch?v=v2PmcNhpdsI)

This is one of my favorite modules. What we're going to do here is compare common data exploration techniques, and focus on writing some good SQL in BigQuery on our own course dataset. Now that you're a little bit more familiar with the dataset that we're going to be exploring, which is that IRS charity dataset, let's talk about your different options for how you can actually explore that data. So as a data analyst, you're not necessarily limited to just using SQL instead of the BigQuery WEB UI. There's also some other pretty cool data preparation tools like Cloud Dataprep that we're going to introduce you to in later on. And last but not least, you can actually explore your data visually using a visualization tool, like Google Data Studio, or Tableau, or Looker, or another one of those tools. So it's not necessarily a linear process going from writing SQL in Bigquery, processesing it, and then visualizing it. You can use each of these different tools at any point that you want, and a lot of it is a matter of preference, depending upon who you talk to. So the first thing that we're going to explore first is using the SQL approach, and using the BigQuery Web UI. Why? Because SQL is a very good skill to have, almost an imperative skill to have as a data analyst. And it's one of the fastest ways you can interact with data behind the scenes inside of BigQuery. And it's fun. So what do you actually have to do before you write this query on your dataset? The first is often the hardest part, is thinking up of a question, or some kind of unknown, or anything that just kind of gives you interest about the dataset. So, here you could look for something as simple as, well, just give me the top revenue or the lowest revenue for these organizations in this dataset. But coming up with these really complex questions can often be the the most challenging or difficult part right? It's like a blank canvas when you first look at the dataset, and then you have to determine, well, okay, well, where do we start? And as you see a little bit later on, when we get into Cloud Dataprep, having some basic statistics of frequency of values, and how many meet the data type constraints, and whether or not you have missing values, can be a great first start. But for now, we're going to start with a blank canvas, which is just the dataset schema, and then throw some SQL on it. Second, of course, is accessing that dataset. This presumes that you already have loaded your data into BigQuery. And again, here we're using the public dataset. And last but not least, as you're going get very familiar with in this talk, is writing the SQL that is going to query the fields and rows that you actually want returned as part of your question. And converting it from your question all the way in the left, to interpreting it as part of like SQL is a skill that you're going to master as you get more and more familiar with basic, intermediate, and advanced SQL. A 2 second background on SQL. So I call it Sequel or SQL. It is that Structured Query Language. It's been around since the eighties. There is a national standard language library for SQL, called ANSI 2011 SQL, and BigQuery Standard SQL follows that, those standards and writing the query in Standard SQL mode gives you a lot of those performance advantages that we'll talk about in later courses. It is pseudo English, so you're using things like SELECT, which basically means give me these columns FROM this dataset, give me the names of these charities, give me the revenue that they have FROM this particular dataset table and then do some kind of manipulation on it. So if we hear ORDER BY, it's synonymous with sorting, sorted alphabetically, sorted highest to lowest. So you get very familiar with these dark blue, all capitalized words called SQL Clauses, as well as the order in which you place them in your queries. And that's what we're going to review for a large part.

### Video - [Query basics](https://www.cloudskillsboost.google/course_templates/506/video/375125)

- [YouTube: Query basics](https://www.youtube.com/watch?v=RC8sGxRJGb4)

Okay now the really fun part. Let's think about how we're going to convert those amazing questions that we just came up with into some best practice SQL. Which is a little bit more rigid than just asking these questions kind of willy nilly. All right. So the first thing, as we mentioned previously, you want to enable that Standard SQL as part of your code, again, it's faster, it's more standards compliant. And if you need a reference in case you've inherited old code that was written in Legacy SQL, there's that reference available to you there, which also includes a migration for which functions now have these new names in Standard SQL. And an additional key point that we want to highlight is, when you're running multiple queries, or say you have a long script because you're just exploring and you want to keep everything all within one document, you can highlight a portion of your code and then execute, Selected or run Selected in just that piece by clicking on that down arrow and then running that selected. Another key difference, and this is what you need to keep in mind when you're pulling data from one of these tables is, you need to escape the table name. And what that means is you need to actually enclose not just the table name, but all the prefixes where it comes from, right? So your table is stored within a dataset like the IRS 990 dataset, and then stored within a project, in this particular case, the public BigQuery project. So we need to actually have project.dataset.table, so a common error here is say, just having just the table. So if you had just a table, BigQuery wouldn't know which table is part of which dataset. Maybe 20 people that are taking this class already just created a table with that name. Whose project am I querying from, because you can actually query data across multiple people's projects. So of course naturally you want to actually get to some table that can actually be resolved to a specific BigQuery table in the backend. All right, so let's take a look at this query. So this query says, give me, SELECT means give me the columns that follow the SELECT statement, In this particular case top revenue or total revenue. Give me the total revenue from this particular dataset. All right, so keep in mind what we're actually going to be returning here, so total revenue, that's not going to be the total revenue for all the filings in 2015. Total revenues is a field, so in actuality, what you're going to get back is, you're going to get back just the total revenue for every single one of those records, you know, a million of the filings for 2015. Okay, so maybe we can limit the results to ten so we're not just returning a bunch of data that we have to paginate our way through. But is this useful, especially if you have just the column for total revenue? Not really, I mean you can see, you can get a general feel that there is a lot of potentially non-zero revenue there. But we don't know who it's from, and I mean in this particular case, it's not even ordered either, so maybe we could potentially add an ORDER BY clause. All right, now we're getting somewhere, right? So now we have revenue, total revenue for 2015 filings, for those exemption filings. Now we're going to order it, and we're going to order it from highest to lowest. And we do that by adding that ORDER BY clause on, near the end, second to last in the query. And we're sorting it from highest to lowest, and how you actually denote that in SQL is descending or DESC is one of the parameters for that value there. It defaults the ORDER BY clause, defaults to ascending or ASC, which would be the opposite, and it seems it works the same way with alphabetically, A to Z or Z to A. Okay, so a key point here is whenever you're ordering something, especially if you have more than 50 rows, make sure you limit it, because there's no reason to return back all that data in those paginated results. You're just wasting your bytes processed, your amount of data that you're consuming.

### Video - [Filters, aggregates, and duplicates](https://www.cloudskillsboost.google/course_templates/506/video/375126)

- [YouTube: Filters, aggregates, and duplicates](https://www.youtube.com/watch?v=yiAp_fzq-yY)

Okay, so back with a little bit more details about the WHERE clause. This is more of a recap of what we went through in great detail before, but why can't we use is_school is equal to yes for the filter here? And if you remember, that's because the alias is not known at that filtering time. So that's where we're actually having to specify the entire field. Operatessschools170 code or cd is equal to yes. And be very careful that your data is in capital Y format and not lower y or Y-E-S, or something like that as well. That's a very, very common trip-up as well. Great. So now we have just schools in those top ten revenue results. Okay, moving on from kind of simple formatting functions as you saw before. Let's talk about some aggregation functions that can do some cool mathematical operations like total revenue across all of 2015. So aggregations can perform calculations over entire sets of values. That is to say, if you had 1.5 million charities in your tables and you just wanted to collapse or sum together or count or get a max in all of those rows, you can do that through these aggregation functions. So for here, we're invoking five of them. We're saying, sum the total revenue, but then average it and count the number of employers and then count the distinct number of employers, much like we did in that exploration lab, and then give us the maximum amount of employees that one of these 2015 filers has and we have that information here. So the total revenue for all US nonprofits for 2015 is that number featured there and the average revenue is there. And the number of nonprofits and the number of distinct nonprofits and the one who had the most employees, almost 800,000. Now, if that average had too many points after the decimal, what you could do again is round and one of the interesting point here is you can actually nest functions inside of other functions. So SQL is going to go ahead and produce the average of that total revenue figure. And then after it's done that, it's going to look outside and say, okay, well, we're not done with our work yet. The user also wants us to round this to two decimal places. So ROUND is another function that'll basically take two inputs here. And you can basically have your first input be total revenue and then the amount of decimal places you want to appear after it. So again, this is kind of merges between stylistic, as we were talking about with the pitfalls for the format, and also potentially necessary if you just wanted to reduce it to those two. But again, possible tradeoff with precision. If you are storing this and doing downstream calculations on that. So again, be careful with round as well. As we mentioned previously in the lab in that we worked with, you can investigate that uniqueness with COUNT and then DISTINCT on that field as well. All right. So an absolutely key concept when we talk about aggregations is, when you mix together fields that aren't aggregated, say like the identifying number for the charity, EIN. And then the count of all the charities for 2015, you get a little bit of a mismatch. So what you need to do is aggregate together, or group together anything that's not under a formal aggregation function like COUNT, SUM, MIN, MAX, and then put those field values in what's called a GROUP BY clause. And GROUP BY will immediately and always follow the FROM statements. So here we have, give us the identifying number for the charities and the COUNT of them from the table there and GROUP BY the EIN number. And again here, the point of this entire query is actually to see for each of the identifying numbers, count the occurrences or the number of filings that are present in that 2015 table. So if you're thinking real hard, you could be wondering like, I would normally just expect to see one filing per each unique organization's number. So this is almost like a data quality check. So you might say like, I don't normally want to group by something like EIN. But for here, I'm going to check the data quality. But before we get into that, again, the key pitfall is don't forget to include that GROUP BY clause. When you start invoking things like sums and mins and maxes, immediately add that GROUP BY, or the validator will let you know when you try to run that query. Interesting, so after we ran that query, here are the results. You see that there are EINs, or organizations, that have seven different records or seven rows that are present in that final year 2015. All right, let's see if we can't get a little bit more insight into why that is. Why on earth would you have seven filings for one particular year for 2015? Okay. Well, let's see how often that does occur. So excluding what we would consider the normal use case, which is having just one filing, how do we actually write that out in SQL? So if you wanted to exclude an aggregation, after filtering has already been done, so we basically want to say, all right, we've done this aggregation for the counts and we basically want to now filter the aggregation. Filtering an aggregation is done by using a special SQL clause called the HAVING clause. So whereas the WHERE clause filters out rows pre aggregation, as we mentioned before, this is immediately going to the dataset. And the query engine says, all right, cool, what can I lose from here immediately? That's done in that WHERE clause. After you've done the aggregations, after that work is done, that's where you can have that HAVING clause. And why am I making such a big deal of this? It's because now we can use an alias in the HAVING, because some of the work has actually been done and that field is then recognized as the alias as well. So HAVING is very, very, very useful when you're filtering aggregations. So you want to say, all right, well, you showed me that in the previous slide. There are all of these different individual charities with these seven filings. How often is that the case? And here, after we execute that, and just looking at the paginated results there at the bottom, almost 18,000 instances or 18,000 different charities. Another way to get that instead of looking at the paginated results, is tracking the actual queries input and output, and that's in the BigQuery results panel. You can actually click over and click on Explanation. And in the third course, Achieving Advanced Insights, you're going to get a very deep dive into what this query plan actually does, because this will help you performance optimize your queries. But if you're looking for just the amount of rows that came in, the amount of rows that came out, they're in red. That's exactly what happens. The processing and shuffling that occurs between each of these different stages is a very, very exciting topic in the architecture lecture in the third course. So a lot of you are still wondering, all right, you still haven't told me why there are seven records. Please tell me why there are seven records. So we're going to zoom in on one particular EIN here. And if we wanted to zoom in on one, how you actually do that is by filtering every single record that's returned just for that particular condition that you want to meet. And then here, you want to make sure that you're using the correct data type. So if EIN is stored as an integer or if it's stored as a string, strings you need to have in single quotes. Integers, you do not want to put in quotes here. So keep that in mind. Okay, so here's the result. So we have seven paper filings for one EIN for 2015, which is the tax period of 2014 since you file your taxes a year after the actual tax period. And we pull up another interesting field called the tax period that's associated with the filing. So in the 2015 calendar year, we have this EIN 2008 to 2014 filings as well. So thinking about why that is, to cut to the chase, the real kind of answer that's staring everybody in the face is human error or dirty data or an organization is submitting more than one tax period filing. Because maybe they knew they could get caught up to speed or maybe there's some kind of refiling. If there's another flag in here that says, this was a corrected filing or something like that. There could be many, many cases. But the bottom line is unless you can tease that out with other columns in the data, you need to go to the subject matter expert who understands how this data was set up in the first place. But you want to be aware of anomalies like this. So when you start totaling things up like, give me the sum of all revenue, or the average of all revenue for across all of your different employers. If you have something that occurs seven times, that's going to skew your results. So just keep in mind for situations like this. Now if you wanted to handle that inside of SQL, say, if you just wanted to pull the latest 2015 filing for that tax period. So you just wanted to pull that record number five there. How could you extract just the tax periods for 2014? And again, that's for calendar year 2015. So if you just wanted 2014, you can invoke a date filtering function. So you basically want to say, all right, well, I want to treat this field as a date since it's not currently stored in very friendly year, year, year, year, month, month, day, day format. We need to invoke or use a parsing function that basically says, hey, treat this tax period in the middle as a string instead of an integer, which it's currently stored as. As that's where we're using that cast function and then you're nesting that inside of a date parsing function, basically saying, all right, now we have this string that I want you to interpret it as. Year, year, year, year, year. Month, month. And that's the percentage sign while a percentage sign lowercase m there as well. And then once you can actually parse that as a date, treat that as tax period. And again we cannot use that tax period alias for filtering in the WHERE clause unless you had separated this into two separate queries or as part of a subquery that you're going to learn a little bit later in later courses, we're going to repeat that exact same function in your filter, except here, We're going to basically say, all right, well, once you've returned that result of what that tax period is, what will the only ones that we want are the ones that are for 2014, for this particular calendar year table of 2015. And how you use that is yet a third function, which is the EXTRACT function for dates so you can extract from that date field up a year. Now why are we bothering to do all this date parsing? Why don't you use something just like a left for and then just grab the first four characters and, and the benefit is kind of that if we can properly treat this field as a date, then we can do normal operations like extracting the year or doing date difference. It just allows it to treat it as it as it should be, which is as a date field. So this kind of preprocessing, again, you would want to do when you're ingesting the data or you want to do it before you store a final reporting table as well. So in future modules, I'm going to teach you how to clean up and transform this data, both using SQL, as you see here, but also a little bit easier if you don't have these functions immediately recalled to memory through a UI, a user interface called Cloud Dataprep. And again for these functions, PARSE, DATE, and EXTRACT, I don't even have a lot of these committed to memory. You can just search online for Google Standard SQL BigQuery, and that'll take you to that one page guide.

### Video - [Data types, date functions, and NULLs](https://www.cloudskillsboost.google/course_templates/506/video/375127)

- [YouTube: Data types, date functions, and NULLs](https://www.youtube.com/watch?v=--bpwFrhPsA)

Okay, yeah, if you looked at that and said, hey, I was following along perfectly fine until we got to that slide, that's completely fine. Parsing and converting dates is just nuts. So you'll refer to that one-page guide, as I mentioned. And there's a ton of different options for converting between data types and parsing those dates. There's a link down below to a lot of those functions as well, but let's take a look at a few. Before we do that, just understand the fact that there are different types of data, data types, that are stored within your tables. You could have numeric data, like your integers, so somebody's salary could be stored as numeric. Or you just have strings, like their first and last name is string data. As you saw, the nuances there, you need to have those stored. And you need to operate, if you're trying to do equivalency operators like name equals something, they need to be in single quotes. And then dates, again, are in that year, year, year, month, month, day, day format for BigQuery. Other values, of course, you can have your Booleans, your yes/no fields. And we'll introduce these last two in great detail in our architecture lecture as part of course three, which is your array, which is kind of a series of values, and your structs, which are flexible containers. Don't worry about those two yet. Okay, so say you were given a dataset that wasn't particularly clean and you had to convert or treat data types as another before storing it into an alternate reporting table. This is super useful for kind of transforming your data. How would you use that? So if you're familiar with a little bit of SQL, you also probably have ran into the CAST or convert function. So in BigQuery and Standard SQL, It's the CAST function. So you have "12345", which is a string of numbers. You want that to be treated as an integer, boom, The result is that an integer, 12345. Or if you have something that's stored as 2017-08-01, treat that as an actual date and allow me to use these special functions that expect data in a certain type. Or if you want to convert it to string. Or the bottom example is an interesting one. If you're unsure of the data that's coming in, so say you don't know that it's apple. You ask somebody to put in their I don't know, their age or something like that. You're expecting it as an integer and you're not doing any kind of, you know, prefiltering or validation. You can use something that like a SAFE_CAST, so instead of erroring out, you can basically say, well, I'm going to convert it, and if it doesn't match what I think it is, I'm going to actually have it be an empty value, or a NULL. Now, NULLs are super interesting, right? So they are valid values, but a NULL is the absence of a value. It's not "", it's not an empty string, and it's not, you know, like if you hit the spacebar once, it's not like just a single space. It is the absence of data. And then you'll see a lot more of that when we get into a lecture on JOINS where you're actually merging two different datasets together. And when they don't have matches, then you actually have valid missing values or NULL values. So if you work a lot with data, you become very, very familiar with NULL values. And actually how to deal with them when you're trying to match and just find those NULLs. So let's get into a little bit of that now. If you want to match on those NULLs, like for example, you want to find all of the values where the state is missing for these charities because you're saying, hey, well, you know, if it's a US charity, it should have a state value. Now, you would normally think that if you're going to use an equivalency operator, like state equals California or state equals New York, that you would use an equal sign. So NULLs, again, super tricky beasts. They cannot be equivalent to anything, not even themselves. So you actually have a separate set of operators for NULLs and that's the IS, so state IS NULL or state IS NOT NULL. So just be familiar when you're using NULLs, you'll be using the IS. And there's special functions that handle NULLs as well, so if you wanted to parse in data and if it was NULL, then set it to zero or something like that. There's those NULL functions as well that you can wrap around your fields. So for the state example, take a look at these. These are U.S. charities, or at least the ones they're they're filing with the U.S. And they're missing the state value here. So we have some cities, Canada for some of these cities as well. But they're missing those state values, which is really interesting. So not all countries could necessarily be using a state value. For example, maybe Canada doesn't use a state value, or Italy doesn't use a state value much like the U.S. would, but they're operating or at least doing business in the U.S.. As we mentioned briefly before, year, year, year, year or YYYY-MM-DD is the expected format for dates. And that unlocks a ton of different date functions that you can then use on that date itself. So extracting the year, extracting the week, extracting the day, and that's the second row there. The EXTRACT function is very powerful if you want to add or subtract or take the difference between two intervals of time. Truncate, so you only want to look at that month or you only want to look at the year. Or format into a different format, you can do that as well. So once you get it into that proper DATE or DATETIME data type, you unlock a lot of really, really, really powerful things that you can do. So dates, it's a necessary evil inside of SQL, but you'll get familiar with them. You can parse these string values. So for example, before we were just casting between values for data types. Now we can actually do some fun things like string manipulation functions. So you're probably already familiar with concatenation. So if you had two string values, much like "12345" and "678", you could merge those two together. Or say you have two fields, one's a first name, another person's a last name. You could string together their first name and last name to get first and last name. Or if it's just, a lot of these are kind of self-explanatory. If it ends with the letter e, then return true or false. A very popular one is convert everything to lowercase or convert everything to uppercase. So in the example we had way earlier on whether or not it IS school or IS NOT school for our charities. You can think of, well, I don't know whether or not in my dataset it's going to be capital y or lowercase y. I'm just going to put everything to lowercase and then match on a lowercase y. So you would do LOWER parentheses - whatever the field name was for that school I think it was like OperatesAsSchool - and then you could set that equal to lowercase y. And that will allow you to not worry about guessing. And then of course, you could use regular expressions in here as well.

### Lab - [Ingesting New Datasets into BigQuery](https://www.cloudskillsboost.google/course_templates/506/labs/375128)

This lab focuses on how to ingest new datasets into tables inside of BigQuery.

- [ ] [Ingesting New Datasets into BigQuery](../labs/Ingesting-New-Datasets-into-BigQuery.md)

## Extracting, Transforming, and Loading Ecommerce Data

In this module, you will learn how extract, transform, and load ecommerce data with Google Cloud through a series of videos and hands-on labs.

### Video - [Module 5 Overview](https://www.cloudskillsboost.google/course_templates/506/video/375129)

- [YouTube: Module 5 Overview](https://www.youtube.com/watch?v=Asv4N80SxYw)

before you can generate insights from your eCommerce data you need to transform it into a state in which you can analyze it in this section extract transform and load or ETL your data with Google cloud in this module you learn to connect Ecommerce data hosted in bigquery to cloud data prep explore e-commerce data fields with the cloud data prep UI clean eCommerce data by deleting unused columns eliminating duplicates creating calculated fields and filtering out unwanted rows in Rich e-commerce data using action types and leverage data flow to run continuous ETL jobs on e-commerce data ready to start learning let's begin

### Video - [Managing and extracting value from your retail and ecommerce data](https://www.cloudskillsboost.google/course_templates/506/video/375130)

- [YouTube: Managing and extracting value from your retail and ecommerce data](https://www.youtube.com/watch?v=BoNAUvp4xpo)

one of Google Cloud's key missions is to accelerate every organization's ability to digitally transform and reimagine their business through data-powered Innovation the rationale behind this Mission couldn't be clear reports state that there will be 181 zettabytes of data by 2025 and that in this same year more than a quarter of data created in the global data sphere will be real time in nature providing Enterprises with the right tools to manage this data is of utmost importance while the volume of data is ever expanding data analytics is still hard for Enterprises according to a recent report less than 50 percent of structured data is used to make decisions and less than one percent of unstructured data is analyzed or used at all and the retail industry is no exception before diving into the data challenges facing the retail industry let's first take a look at some of the larger Trends happening in this space first there is decreasing Market spend as consumers experience a loss of income their spending intent has has also continued to Trend negative second consumers are also changing their behaviors faced with a prolonged period under pandemic conditions consumers are trying new things and forming new habits such as curbside pickup and increased online shopping there is also an increasing focus on safety personal safety has become top of mind for Shoppers and consumers will expect higher hygiene standards from retailers going forward consumers are also focusing on Essentials consumers are continuing to prioritize everyday Basics such as grocery food deliveries and home space while pausing luxury and aspirational shopping and last but certainly not least customers expect seamless experiences increasingly reliant on digital Technologies to stay connected consumers are shopping online for purchases that would normally be made in the store they're looking for a seamless experience throughout the journey all the way to delivery here's an overview of some of the key challenges the retail industry faces with data first and foremost is data-led decision making as retailers aim to reduce costs at the same time as facing reduced footfall there is a significant need to drive increased efficiency and ensure the business is oriented towards actions that drive the highest levels of profitability introducing data Into the Heart of business decision making is critical to ensure internal resources are focused on high value activities evidenced with clear data visualization and removing the Reliance on historical performance indicators managing customer interaction data is also of utmost importance changes in consumer behavior and expectations that were already disrupting retail have accelerated during the pandemic adoption of digital shopping has been accelerated by three to five years in several scenarios as consumers have embraced new online experiences and retailers have been quick to introduce new innovation in this space as physical stores began to reopen expectations have shifted from online to Omni Channel with customers expecting seamless cross-channel experiences powered by technology the ability to clean and manage complex omni-channel data is a requirement for today's retailers the good news Google collab is here to help as the industry transitions into a new normal it's important to have the right partner retail is one of Google Cloud's top priority verticals that we're rallying our teams around Google Cloud CEO Thomas curion said this this is a very defining moment for all of us around the world to have the hope and the optimism to reimagine your business as you recover from the pandemic we at Google take responsibility to support you in that mission according to Market Research firm Canalis this philosophy has paid off in the retail sector as Google Cloud was named the number one cloud services provider for the retail sector its success comes from its vertically aligned strategy with retail being one of its core segments Google cloud has also been establishing worldwide systems integrator Partnerships to accelerate retail penetration it recently expanded its partnership with Best Buy which is named Google Cloud as the provider of its data Enterprise platform in a multi-year agreement Google Cloud's client portfolio has many of the world's leading retailers including Costco germany-based Metro and Target by using its strength in advertising and search Google Cloud can offer retail customers much more than compute and storage with all of the world-class services available it's not hard to see why Google Cloud was named the number one cloud provider for the retail sector from collection to processing to storage to analytics all the way to predicting and activating Google cloud has you covered in every step of your data powered Innovation plans for retail one of these Services cloud data prep is incredibly valuable for managing and extracting value from your retail and e-commerce data cloud data prep is an intelligent data service for visually exploring cleaning and preparing structured and unstructured data for analysis reporting and machine learning because cloud data prep is serverless and works at any scale there is no infrastructure to deploy or manage your next ideal data transformation is suggested and predicted with each UI input so you don't have to write code with automatic schema data type Hospital joins and anomaly detection you can skip time consuming data profiling and focus on data analysis cloud data prep is an integrated partner service operated by trifacta and based on their industry-leading data preparation solution trifacta Wrangler Google works closely with trifacta to provide a seamless user experience that removes the need for upfront software installation separate licensing costs or ongoing operational overhead cloud data prep is fully managed in scales on demand to meet your growing data preparation needs so you can stay focused on analysis here's an example of a cloud data prep architecture as you can see Cloud dataprep can be leveraged to prepare raw data from bigquery cloud storage or a file upload before ingesting it onto a transformation pipeline like Cloud dataflow the refined data can then be exported to bigquery or cloud storage for analysis and machine learning now that you've learned about the common challenges retailers face with data you'll learn more about cloud dataprep and receive Hands-On practice building an e-commerce data transformation pipeline in a lab

### Video - [Dataprep](https://www.cloudskillsboost.google/course_templates/506/video/375131)

- [YouTube: Dataprep](https://www.youtube.com/watch?v=4YuQnvuBgJc)

Person: Let's learn a little bit about Cloud Dataprep. Cloud Dataprep is an intelligent data service for visually exploring, cleaning and preparing structured and unstructured data for analysis reporting and machine learning. Because Cloud Dataprep is serverless and works at any scale, there's no infrastructure to deploy or manage. Your next ideal data transformation is suggested and predicted with each UI input, so you don't have to write code. With automatic schema, data types, possible joins and anomaly detection, you can skip the time-consuming data profiling and focus on data analysis. Cloud Dataprep is an integrated partner service operated by Trifacta and based on their industry-leading data-preparation solution, Trifacta Wrangler. Google works closely with Trifacta to provide a seamless user experience that removes the need for up-front software installation, separate license and cost or ongoing operational overhead. Cloud Dataprep is fully managed and scaled on-demand to meet your growing data-preparation needs, so you can stay focused on analysis. Here's an example of a Cloud Dataprep architecture. As you can see, Cloud Dataprep can be leveraged to prepare raw data from BigQuery, Cloud Storage or a file upload before ingesting it into a transformational pipeline like Cloud Dataflow. The refined data can then be exported to BigQuery or Cloud Storage for analysis and machine learning.

### Video - [Dataprep](https://www.cloudskillsboost.google/course_templates/506/video/375132)

- [YouTube: Dataprep](https://www.youtube.com/watch?v=_x0UbPwCsfc)

>> In this lesson, we will explore how Dataprep can also be considered a data preprocessing option. We'll explore data transformation and see how data can be cleaned, structured, enriched, and validated with Dataprep by Trifacta. This graphic shows the data life cycle process, all of which can be done in Dataprep. Business data is brought in from various sources. And discover is about finding anomalies and correlations among the data. Cleanse is the process of detecting and correcting or removing corrupt or inaccurate data and refers to identifying incomplete, incorrect, inaccurate, or irrelevant parts of the data and then replacing, modifying, or deleting it. Structure is about changing the format of data to conform to the correct or desired state. For example, changing a string to an actual date, or pivoting rows into columns, or extracting data from JSON into a relational model. Enrich is about combining sources and deriving new values that augment the data with further information for analysis. Validate is about ensuring the data conforms to the required dataset for analysis. Let's talk about where Dataprep sits in the data journey. Here, you can see that Dataprep allows you to make that smooth and speedy transition from data ingestion, to data preparation, to data analysis. In ingestion, Dataprep can access any data available in Google Cloud. Note that BigQuery can be a source for Dataprep, as well as an output for cleaned data. Prepare is where you will prepare your transformations. Under the hood, Dataprep will produce Dataflow jobs. You can automate or schedule the Dataprep jobs because of Dataflow. Clean data comes out at the other end, ready for use, analyzing, visualizing, creating ML models, et cetera. Dataprep automatically detects schemas, data types, possible joints and anomalies such as, missing values, outliers, and duplicates, so you get to skip the time-consuming work of profiling your data and go right to exploration and analysis. Visual representations help interpret large volumes of data. And the innovative profiling techniques of Dataprep, visualizes key statistical information in a dynamic, easy-to-consume format. Dataprep automatically detects over 17 different data types and can transform structured or unstructured datasets stored in CSV, JSON or relational table formats of any size, megabytes to petabytes, with equal ease and simplicity. When you import data into Dataprep, you are creating a reference to a source of data. When the data is required for use, Dataprep reads a sample of the source data into the application for your use through an object known as a connection. There are three connection types for Dataprep: Upload/Download, where you can upload data directly from your local desktop. Cloud storage, where you can read from and write to files in cloud storage. And BigQuery, where you can store relational content in BigQuery from which Dataprep can read. Dataprep uses intelligent data cleansing with predictive transformations, this allows you to quickly identify data quality issues and get automatic data transformation suggestions. You may see suggestions to exclude or include particular values or perhaps to change to a data type that Dataprep considers better suited to the data it samples. Essentially, Dataprep enables powerful data processing capabilities. It can handle structured and unstructured data. It can prepare datasets of any size, petabyte or megabyte, with equal ease. It is built on top of the powerful Dataflow service. And it's auto-scalable and can easily handle the processing of massive datasets. In Dataprep, the flows are implemented as a sequence of recipes. The recipes are data processing steps built from a library of so-called Wranglers. Dataprep has Wranglers from many common data processing tasks shown on the left. Dataprep can take your flow and its recipes and convert them to a Dataflow pipeline. Using the same Dataprep interface, you can then take the flow, run it as a job on Dataflow, and monitor the progress of the job. Recipes are a repeatable set of transformation steps, built by chaining data Wranglers together. You can include end-to-end steps from ingestion, transformation, aggregation, and save to BigQuery. When you are done writing your data transformation recipes, you will then run your Dataprep job which processes your recipes against your entire dataset.

### Video - [Dataprep in the real world](https://www.cloudskillsboost.google/course_templates/506/video/375133)

- [YouTube: Dataprep in the real world](https://www.youtube.com/watch?v=sSNUtzXStQs)

in this lesson i'll provide you with an overview of transformation and cloud data prep you'll recall this slide from the previous module now the data has been ingested in collect it's time to explore the transform concept cloud data prep gives us an intelligent cloud data service to visually explore clean and prepared data for analysis and machine learning i think we can all empathize with what's happening here your customers face overwhelming and disorganized streams of data dealing with dirty data is one of the most constant challenges we face in transformation someone has overloaded a field with values that shouldn't be there you're expecting numerics but the field has text in it no matter how much we try to ensure data quality even in the front-end initial capture data will often get dirty along the way cloud data prep which is a hosted trifacta instance brings data transformation and cleansing to everyone not just it or those with coding or query language skills today's computers are faster than ever yet businesses often find it hard to get insights from their data at the right time enterprises across the globe have two tasks that often slow down the journey from data to meaningful insights firstly the time and cost to set up specialized infrastructure for big data analysis and secondly the time to prepare the data for analysis time to prepare is what we'll be discussing and what we'll look at how you can improve this who gets excited about data preparation well apparently not many of us most people that do it don't like it for example often data scientists spend more time preparing and cleaning data than actually working on and improving their models this graphic shows the data lifecycle process all of which can be done in cloud data prep business data is brought in from various sources discover is about finding anomalies and correlations among the data example one column having the same value for all records cleanse is the process of detecting and correcting or removing corrupt or inaccurate data and refers to identifying incomplete incorrect inaccurate or irrelevant parts of the data and then replacing modifying or deleting it structure is about changing the format of data to conform to the correct or desired state example changing a string to an actual date pivoting a row into columns or extracting data from json into a relational model enrich is about combining sources and deriving new values that augment the data with further information for analysis and validate is about ensuring that data conforms to the required data set for analysis once validated you can automate your recipe and schedule it as an analyst you don't need to use multiple data preparation tools or write complex code a ui is available cloud data prep helps you accelerate the critical data preparation process using a simple and easy to use interface discover clean structure enrich and validate data with clicks and easily get the data ready for further analysis cloud data prep is google's self-service data wrangling tool data wrangling is the process of taking data in its native format and making it usable for analysis cloud dataprep gives data-driven organizations the ability to visually and intuitively prepare their data for downstream analytics this kind of activity typically has been locked in the confines of it what if we can remove that barrier to entry for data analysis cloud data prep enables us to do exactly that it removes the barrier to entry for data analysis let's talk about where cloud data prep sits in the data journey here you can see that data prep allows us to make that smooth and speedy transition from data ingestion to data preparation to data analysis ingestion data prep can access any data available in the gcp platform note that bigquery can be a source for data prep as well as an output for clean data prepare this is where you prepare your transformations under the hood data prep will produce data flow jobs you can automate or schedule the data prep jobs clean data comes out at the other end ready for use in analyzing visualizing creating machine learning models and so on cloud data prep enables you to visually explore and interact with data and instantly understand data composition and patterns with visual data distributions cloud data prep automatically detects schemas data types possible joins and anomalies such as missing values outliers and duplicates so you get to skip the time-consuming work of profiling your data and go right to exploration and analysis for example marketing could quickly see the distribution of campaign history against campaign type like sales retention and service to get a quick understanding of how many of each campaign has been delivered the distribution of campaign delivery timestamps could quickly uncover large sends in a short period of time highlighting a specific event or potential anomaly see and explore your data through interactive visual distributions of your data to assist in discovery cleansing and transformation visual representations help interpret large volumes of data and the innovative profiling techniques of cloud data prep visualizes key statistical information in a dynamic easy to consume format cloud data prep automatically detects over 17 different data types and can transform structured or unstructured data sets stored in csv json or relational table formats of any size megabytes to petabytes with equal ease and simplicity when you import data into cloud data prep you're creating a reference to a source of data the source itself is immutable and doesn't change when a data prep flow runs only a new source of cleansed data is created when data is required for use cloud data prep reads a sample of the source data into the application for use through an object known as a connection there are three connection types for data prep upload download you can upload data directly from your local desktop you can also save it locally on export google cloud storage read from and write to files in cloud storage and bigquery you can store relational content in bigquery from which cloud data prep can read cloud data prep uses a proprietary inference algorithm to interpret the data transformation's intent of a user's data selection cloud data prep automatically generates a ranked set of suggestions and patterns for the selections to match you may see suggestions to exclude or include particular values or perhaps to change a data type that cloud data prep considers may be better suited to the data it samples cloud dataprep uses an ml driven approach to suggest what the user might want to do with the dataset this can make the entire transformation work much quicker cloud data prep enables powerful data processing capabilities it can handle structured and unstructured data it can prepare data sets of any size petabytes or megabytes with equal ease it is built on top of the powerful cloud data flow service and it's auto scalable and can easily handle the processing of massive data sets and as i said earlier cloud data prep is seamlessly integrated with the flows processes and components of google cloud platform you can easily process data stored in cloud storage bigquery or from your desktop you can export clean data directly into bigquery for further analysis and seamlessly manage user access and data security with google cloud identify and access management

### Video - [Cleaning retail and marketing data with Cloud Dataprep](https://www.cloudskillsboost.google/course_templates/506/video/375134)

- [YouTube: Cleaning retail and marketing data with Cloud Dataprep](https://www.youtube.com/watch?v=WM8GnMPmxy0)

when it comes to data it's important to show and don't tell in this lesson and the lab that we'll follow you will see how you can use wranglers predefined or built by you to demonstrate how you can cleanse and transform data cloud data prep is a graphical user interface for creating and previewing data transformation steps you can pick from pre-defined wranglers to process your data data wrangling is the process of cleaning structuring and enriching raw data into a desired format for better decision making in less time you can chain together multiple wranglers into a repeatable recipe and perform common tasks like record deduplication and derived fields this self-service model with pre-defined wranglers allows analysts to tackle more complex data more quickly produce more accurate results and make better decisions refer to the additional resources document for a link to the cloud data prep guides a link is also provided there to cloud data prep on the gcp console recipes are a repeatable set of transformation steps built by chaining data wranglers together you can include end to end steps from ingestion transformation aggregation and save to bigquery when you are done writing your data transformation recipes you will then run your cloud data prep job which processes your recipes against your entire data set not just the 10 megabyte sample brought into the transformer in the above simple example the analyst is breaking up a customer data set based on a new line delimiter splitting columns based on forward slash converting rows to headers changing data types and concatenating fields all able to be automated in a single recipe

### Video - [Introduction to lab 5](https://www.cloudskillsboost.google/course_templates/506/video/375135)

- [YouTube: Introduction to lab 5](https://www.youtube.com/watch?v=ndG4kShS2Kw)

in this lesson you'll create a data transformation pipeline with cloud data prep by completing a hands-on lab in this lab you'll explore the cloud data prep ui to build a data transformation pipeline that runs at a scheduled interval and outputs results into bigquery during the lab you'll connect bigquery data sets to cloud data prep explore data set quality with cloud data prep create a data transformation pipeline with cloud data prep and schedule transformation jobs that output to bigquery

### Lab - [Exploring and Creating an Ecommerce Analytics Pipeline with Cloud Dataprep v1.5](https://www.cloudskillsboost.google/course_templates/506/labs/375136)

In this lab we will explore the Cloud Dataprep UI to build an ecommerce transformation pipeline that will run at a scheduled interval and output results back into BigQuery.

- [ ] [Exploring and Creating an Ecommerce Analytics Pipeline with Cloud Dataprep v1.5](../labs/Exploring-and-Creating-an-Ecommerce-Analytics-Pipeline-with-Cloud-Dataprep-v1.5.md)

## Predicting Customer Behavior with Machine Learning on Google Cloud

In this module, you will learn how to predict customer behavior with Big Query's machine learning capatbilties through a series of videos and hands-on labs.

### Video - [Module 6 Overview](https://www.cloudskillsboost.google/course_templates/506/video/375137)

- [YouTube: Module 6 Overview](https://www.youtube.com/watch?v=G4ra_QpsGyM)

a i and ml can be intimidating but once harnessed they can drag tremendous value traditional search engines are rule-based providing one consistent experience for all cohorts by using Ai and NL with Google Cloud you can build personalized search experiences for your customers in this section you learn how to leverage machine learning to analyze e-commerce data with Google cloud in this section you learn to query and explore an e-commerce data set create a training and evaluation data set to be used for batch prediction create a classification model in bqnl evaluate the performance of a machine learning model and predict and rank the probability that customers will make purchases let's jump in

### Video - [The importance of machine learning and AI for retail](https://www.cloudskillsboost.google/course_templates/506/video/375138)

- [YouTube: The importance of machine learning and AI for retail](https://www.youtube.com/watch?v=CnCzn-YKzjE)



### Video - [Introduction to BigQuery ML](https://www.cloudskillsboost.google/course_templates/506/video/375139)

- [YouTube: Introduction to BigQuery ML](https://www.youtube.com/watch?v=IINW5wfCYa8)

person: Welcome to training machine learning models with BigQuery ML. After completing this module, you'll be able to describe BigQuery ML, understand how BigQuery ML supports machine learning models, describe BigQuery ML hyperparameter tuning, and explain how to build a recommendation system with BigQuery ML.

### Video - [Training an ML model using BigQuery ML](https://www.cloudskillsboost.google/course_templates/506/video/375140)

- [YouTube: Training an ML model using BigQuery ML](https://www.youtube.com/watch?v=mapkJAj-dlw)

Person: Let's revisit our team at XYZ company. They have just defined a business use case and established success criteria, and they want to deliver an ML model to production. It will be their second project. Recall that the team consists of a software developer who knows Java, a data analyst who knows SQL but has no ML knowledge and a data scientist who has domain knowledge but has limited experience putting a machine learning model into production. The team had a very successful deployment of their first model into production using Vertex AI AutoML. Now the team needs the flexibility of a custom model. Which framework should they use? Should they continue with Vertex AI AutoML and ramp up on their coding skills, use a custom-based Python framework or should they consider BigQuery ML? One consideration for the team is their data requirements. They are again using structured data, but this time the data set will be larger than 100 gigabytes, and it may contain fewer than 1,000 rows. The data requirements for Vertex AI AutoML specify that the maximum data set size is 100 gigabytes and the minimum row requirement is 1,000. Also, the target column must be categorical or numerical. Clearly the data requirement limits the teams' ability to use Vertex AI AutoML tables. The team could use a custom-based Python framework, or they could use BigQuery Machine Learning. BigQuery ML is an easy to use way to invoke machine learning models on structured data using just SQL. Google's BigQuery is more than just a data warehouse. It can provide decision-making guidance through predictive analytics by using its machine-learning tool, BigQuery ML. More excitingly, you can create and train a model without ever exporting data out of BigQuery. Simply put, Google BigQuery ML is a set of SQL extensions to support machine learning. If the team decides to do custom modeling without BigQuery ML, there is an increased complexity and multiple tools are required. Custom modeling also reduces speed, moving and formatting large amounts of data for Python based ML frameworks takes longer than model training in BigQuery. BigQuery ML speeds up the time to production, makes development work much easier, and automates a number of the steps in the ML workflow. For example, BigQuery ML will import and preprocess data, split data, build the model and deploy the model. The team sees that with BigQuery ML all they really have to do is have the data in BigQuery, identify a use case, write a short piece of SQL code, and they're ready to deliver their model. BigQuery ML is a middle ground between using pretrained models and building your own TensorFlow model with Vertex AI platform. A challenging thing about machine learning is that it can take months to get something off the ground. Exploratory data analysis or EDA in a Jupyter Notebook, prototyping and scaling out to a managed service is a time-consuming process. From a logistics viewpoint, it is also challenging to manage security and permissions when using many different tools in the machine learning process. Working with BigQuery ML the team can achieve their goals in four major steps. First, write a SQL query to extract their training data from BigQuery. Second, create a model specifying model type. Third, evaluate the model and verify that it meets requirements, and fourth, predict using the model on data extracted from BigQuery. Since the team already has data that lives in BigQuery, they are able to execute their initiatives without needing to move any data, and since one team member is very familiar with SQL, development will be faster. And finally, BigQuery ML will automate common machine learning tasks and perform hyperparameter tuning. Here is an example use case. Let's examine a public data set of 1.1 billion taxi rides from New York City that is already loaded into BigQuery. You will use this data set to build a model in place in BigQuery. Our goal will be to train a model to predict the taxi fare for a ride. First, let's select the data we want to use for training. We will select the fare amount, our label for what we are trying to predict, the pickup location, latitude and longitude, drop-off location and the passenger count. To create a model, we only need a few more lines of code. We use the create model statement to create the model in one of our BigQuery data sets. Next, the input label cols option specifies the column we wish to predict when we specify the model type and any of the options associated with that model type. We can evaluate our models by using the ML.EVALUATE function. We pass in our model name and the data set we wish to evaluate on if it's different from what was used at training time. To serve predictions it's the same idea, but now with ML.PREDICT, and that's it. Here's an example of the training job metrics.

### Video - [BigQuery ML supported models](https://www.cloudskillsboost.google/course_templates/506/video/375141)

- [YouTube: BigQuery ML supported models](https://www.youtube.com/watch?v=P2okxpxlfi8)

>> BigQuery supports many different model types for classification and regression. For example, you can use logistic reg for either BigQuery ML binary logistic, or multiclass classification. The use for binary classification is when the label is true, false, one, zero, or when there are only two categories. For example, if you need to determine whether or not a flight will be late. The use for multiclass classification is when the label is in a fixed set of strings. For example, when an email is classified in the primary, social, promotions, updates, or forums tabs. To solve regression problems, you use linear reg when the label is a number. For example, if you want to forecast product sales on a certain day. You create TensorFlow- based DNN to solve regression and classification problems. Use DNN regressor for regression problems. Use DNN classifier for binary and multiclass classification problems. Boosted decision trees have better performance than decision trees on extensive datasets, so you can use Boosted Tree Models for classification and regression problems. Use Boosted Tree Regressor for regression. Use Boosted Tree Classifier for binary and multiclass classification problems. For creating a recommendation system, use Matrix factorization. For example, if you want to recommend the next product for a customer to buy based on their past purchases, historical behavior, and product ratings. K-means clustering is used when labels are unavailable; for example, to perform customer segmentation. BigQuery ML for time series is popular for estimating future demands such as, retail sales, or manufacturing production forecast. It also automatically detects and corrects for anomalies, seasonality and holiday effects. You can use AutoML Tables for any regression classification and time series forecasting problems, it will automatically search through various models and find the best one for you. Use TensorFlow model importing if you have previously trained TensorFlow models and want to import them to BigQuery to perform predictions on them.

### Video - [BigQuery ML hyperparameter tuning](https://www.cloudskillsboost.google/course_templates/506/video/375142)

- [YouTube: BigQuery ML hyperparameter tuning](https://www.youtube.com/watch?v=Kt2f0nlinv4)

Person: In machine learning, hyperparameter tuning identifies a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. By contrast, the values of other parameters, such as coefficients of a linear model, are learned. Hyperparameter tuning allows you to spend less time manually iterating hyperparameters and more time focusing on exploring insights from data. This hyperparameter tuning feature is made possible in BigQuery ML by using vertex Bezier. In this lesson, we examine BigQuery ML hyperparameter tuning options. BigQuery ML supports hyperparameter tuning when training ML models using create model statements. Hyperparameter tuning is commonly used to improve model performance by searching for the optimal hyperparameters. Hyperparameter tuning supports the following model types: linear regression, logistic regression, K means, matrix factorization, boosted tree classifier, boosted tree regressor, DNN classifier and DNN regressor. In this example of hyperparameter tuning, a simple deep neural network, or DNN, has three standard hyperparameter options: data split method set to random, early stop set to true, hidden units set to 30, 50. In the second example, the same DNN goes beyond the default hyperparameters. You now have the following additions: num trials equals 10, max parallel trials equals two, dropout HParam range zero, 0.2, optimizer equals HParam candidates, Adam, Adagrad, learn rate HParam candidates 001, .01. Let's explore a quick example of how adding and changing hyperparameters improves model performance. In this deep neural network example, a DNN model without hyperparameter tuning gives us an ROC of .53. What is the AUC ROC curve? Recall that ROC AUC is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against the FPR at various threshold values and essentially separates the signal from the noise. The area under the curve, or AUC, is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes. In this deep neural network example, a DNN model with hyperparameter tuning gives us an ROC of 0.79. When the two model options are compared, ROC AUC has improved from 0.53 to 0.79.

### Lab - [Predict Visitor Purchases with a Classification Model in BigQuery ML](https://www.cloudskillsboost.google/course_templates/506/labs/375143)

In this lab, you use an available ecommerce dataset to create a classification (logistic regression) model in BigQuery ML that predicts customers' purchasing habits.

- [ ] [Predict Visitor Purchases with a Classification Model in BigQuery ML](../labs/Predict-Visitor-Purchases-with-a-Classification-Model-in-BigQuery-ML.md)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
