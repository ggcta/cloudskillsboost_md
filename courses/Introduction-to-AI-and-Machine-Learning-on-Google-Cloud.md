---
id: 593
name: 'Introduction to AI and Machine Learning on Google Cloud'
datePublished: 2024-09-13
topics:
- Data
- Data Analysis
- Generative AI
type: Course
url: https://www.cloudskillsboost.google/course_templates/593
---

# [Introduction to AI and Machine Learning on Google Cloud](https://www.cloudskillsboost.google/course_templates/593)

**Description:**

This course introduces the AI and machine learning (ML) offerings on Google Cloud that build both predictive and generative AI projects. It explores the technologies, products, and tools available throughout the data-to-AI life cycle, encompassing AI foundations, development, and solutions. It aims to help data scientists, AI developers, and ML engineers enhance their skills and knowledge through engaging learning experiences and practical hands-on exercises.

**Objectives:**

- Recognize the data-to-AI technologies and tools provided by Google Cloud.
- Build generative AI projects by using Gemini multimodal, efficient prompts, and model tuning.
- Explore various options for developing an AI project on Google Cloud.
- Create an ML model from end-to-end by using Vertex AI.

## Introduction

This module covers the course objective of helping learners navigate the AI development tools on Google Cloud. It also provides an overview of the course structure, which is based on a three-layer AI framework including AI foundations, development, and solutions.

### Video - [Course introduction](https://www.cloudskillsboost.google/course_templates/593/video/508301)

- [YouTube: Course introduction](https://www.youtube.com/watch?v=6OHoeSCo5cQ)

Artificial Intelligence (AI) and machine learning (ML) are rapidly evolving, especially in the field of generative AI. Generative AI allows machines to produce content based on human input, which opens a wide range of possibilities that were not available even a few months ago. If you’re a developer, you might want to incorporate new AI capabilities in your applications or use them to improve productivity. If you’re a data scientist, you might want to train an ML model with your own data to solve a business problem. If you’re an ML engineer, you might want to build an ML pipeline and deploy it to production. Even if you’re not an AI professional, you might be curious about cutting-edge advancements, such as generative AI, and how they can spark new business ideas. If you are any of these people, we have a course for you: Introduction to AI and Machine Learning on Google Cloud. I’m Dr. Yoanna Long, an AI and machine learning educator at Google Cloud. I was a college professor for fifteen years, and I’m passionate about making complex concepts simple so everyone can understand and use AI. I’m here to help you embark on the learning journey and succeed in this course! So, what is this course about? This course presents a toolbox, which is full of AI technologies and tools offered by Google. These technologies and tools are organized into layers to make navigation easier. This course presents a toolbox, which is full of AI technologies and tools offered by Google. These technologies and tools are organized into layers to make navigation easier. You begin with the AI foundation layer, where you learn about cloud essentials like compute, storage, and network, and data tools such as data pipelines and data analytics. These tools help you start your journey from data to AI. You then move on to the AI development layer, where you explore different options to build an ML project, including out-of-the-box solutions, low-code or no-code, and DIY (do-it-yourself). You also walk through the workflow to train and serve an ML model using Vertex AI, the AI development platform provided by Google Cloud. Finally, you are introduced to generative AI, and you learn how generative AI empowers the AI development and AI solutions layers. After completing this course, you will be able to: Recognize the data-to-AI technologies and tools offered by Google Cloud. Leverage generative AI capabilities in applications. Choose between different options to develop an AI project on Google Cloud. And build ML models end-to-end using Vertex AI. How can you succeed in this course? Here are some suggestions. Write down three keywords after each lesson, lab, and module. These keywords can be points you learned, use cases to apply, or new business ideas. Look at this list and see how the terms relate to each other. [Animation - fade on each note one by one] You can then draw lines to connect the terms, or group them together in a way that makes sense to you. This practice will strengthen your understanding of the concepts. Apply what you learned to your own work. This is the best way to develop your skills as an AI practitioner. The evolution and capabilities of AI are fascinating! As you learn more about this technology, you are better prepared to meet the challenges of today and tomorrow. Let's get started!

## AI Foundations

This module begins with a use case demonstrating the AI capabilities. It then focuses on the AI foundations including cloud infrastructure like compute and storage. It also explains the primary data and AI development products on Google Cloud. Finally, it demonstrates how to use BigQuery ML to build an ML model, which helps transition from data to AI.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/593/video/508302)

- [YouTube: Introduction](https://www.youtube.com/watch?v=ckRlO75TkTo)

Welcome to the first module of this course, AI foundations. The first question you may have is why AI. Specifically, how AI helps you innovate business processes and improve business efficiency. Lesson one answers this question by showing you a use case powered by recent AI technologies like generative AI. You may also wonder why Google and how to start an AI project on Google Cloud. Lesson two illustrates an AI/ML framework and helps you navigate through the whole course. Next, you explore Google Cloud's infrastructure, focusing on compute and storage. You then examine the products that support your journey from data to AI on Google Cloud. After that, you advance to more AI and ML content, starting with ML model categories, which provides context to understand ML model building. You then explore BigQuery, and specifically BigQuery ML, and walk through the steps to build an ML model with SQL commands. Finally, you complete a hands-on lab to build your first ML model on Google Cloud with BigQuery ML. Let’s start.

### Video - [Why AI](https://www.cloudskillsboost.google/course_templates/593/video/508303)

- [YouTube: Why AI](https://www.youtube.com/watch?v=QSskSv_4YCg)

Why AI and why Google? These might be your first questions. Let's explore an example to understand how AI can enhance business efficiency and transform operations. Coffee on Wheels, an international company that sells coffee on trucks in cities like London, New York, San Francisco, and Tokyo, provides a compelling case study. Coffee on Wheels is facing three main challenges: Coffee on Wheels is facing three main challenges: Location selection and route optimization Predicting popular locations for truck placement, and optimizing routes based on weather and traffic conditions. Sales forecast and real-time monitoring Forecasting sales and monitoring performance in real-time. Marketing campaign automation Automating marketing campaigns to increase efficiency and effectiveness. Recognizing the potential of AI, ANIMATION ANIMATION Coffee on Wheels sought assistance from Data Beans, a digital native company, to leverage data and AI technologies to resolve their business challenges. Let's take a tour of the demo. Choose one of the four current locations, such as London. The dashboard displays overall statistics across cities, including revenue, operating margin, and the number of trucks. This information is generated by data tools like BigQuery and Looker, as well as AI tools and models like Gemini and Vertex AI. On the right, you can view the final data for London with a summary. It shows how London's revenue compares to the average, and provides insights into revenue per truck and customer loyalty. In the top left corner, the dashboard displays the weather and generates route suggestions based on weather conditions. For example, if lower temperatures are forecasted, it might suggest a new itinerary that focuses on covered areas. You can click "show updated route" and "publish route" to implement these changes. By clicking on a specific time on the timeline, you can see route suggestions based on city events. For example, if there's a football game happening, it might suggest rerouting trucks to avoid congestion. Clicking on the truck sign provides a detailed dashboard with information such as street view and revenue forecast. To monitor the performance of the business in real-time, you can access a dashboard by clicking "show menu." If an item is underperforming, you can click the "generate" button to get suggestions for a new item. Additionally, you have the option to generate marketing campaigns by selecting "yes" to "save the suggestion." This feature enables you to automatically create campaigns that include both text and images. You can further streamline your marketing efforts by sending campaign emails to targeted customers with just a click of the "post" button. Finally, you can generate an operational report and export the insights to any format, such as Google Slides. To customize the application using code, click "how it's made." This reveals the tools and technologies, including BigQuery, Gemini, and Vertex AI, that were used to create the app. You can also click "open in notebook" to access the sample code in the code development environment. Isn’t it remarkable? The process is actually straightforward: Multimodal input: this involves incorporating various forms of data, such as text (customer reviews), images (coffee and dessert pictures), and videos (real-time street view). 2. Prediction and generation: this is powered by data analytics like customer segment analysis, predictive AI like sales forecasting, and generative AI like marketing campaign automation. 3. Visual output: the insights and reports are then presented visually, empowering businesses to make real-time data-driven decisions and optimize their operations. Behind the scenes, many Google products collaborate to make this application possible. For example, Gemini multimodal enables data acquisition, BigQuery provides data analytics, Vertex AI handles ML development, and Looker and Google APIs contribute to data visualization and app creation. The course will explore these tools in more depth later, giving you the chance to learn about them in detail. This application's development encompasses the entire data to AI lifecycle. It includes data ingestion, data analytics, data engineering, model training, testing, and deployment. These processes are supported by Google's unified development platforms. Additionally, Google's AI development platform enables the utilization of various types of AI, including predictive AI for tasks like sales forecasting, generative AI for tasks like automating marketing campaigns, and hybrid approaches that combine both. By leveraging the application, Coffee on Wheels gained the following benefits: Streamlined business processes in key areas such as marketing, digital commerce, and back-office operations. Modernized customer service through features such as automated comment replies and actionable consumer insights and predictions. Enhanced employee productivity through the utilization of GenAI for code assistance and marketing content generation.

### Video - [AI/ML architecture on Google Cloud](https://www.cloudskillsboost.google/course_templates/593/video/508304)

- [YouTube: AI/ML architecture on Google Cloud](https://www.youtube.com/watch?v=xaH3cMWO1ws)

You might be excited about the capabilities and potentials of AI, but how to start your AI projects with Google? You explore the AI/ML toolbox in this lesson. So, why should you trust Google for AI? Firstly, Google has been an AI-first company and used AI to power its products since the beginning. Second, Google is a leader in AI and ML innovations. Thirdly, Google is a believer in responsible AI. Let's take a brief look at how Google began innovating with data and AI in its products. Data is the basis of AI. Historically speaking, Google experienced challenges related to data processing quite early. As a search engine, Google needed to constantly invent new data processing methods to index the World Wide Web and keep up with the rapidly growing internet. The innovation started in the early 2000s from Google File System, or GFS, which is the foundation for Cloud Storage, and MapReduce, which aims to manage large-scale data processing. To address the challenges of different types of data, whether structured or unstructured, and different processing requirements, either streaming or batch, Google has continued to invent multiple products and technologies in data analytics and engineering. For example, BigQuery, the data warehouse widely used on Google Cloud, can analyze large amounts of data and build ML models at the same time with SQL, Structured Query Language. In 2015, Pub/Sub was invented to help build data pipelines for streaming analytics. In the field of AI, Google has also contributed with many key technologies. For example, The widely-used ML library in Python for general purposes scikit-learn was a Google summer coding project back in 2007. TensorFlow, an open-source ML platform for training deep learning neural networks was developed by Google in 2015. And it has evolved in multiple versions. The transformer, the basis of all generative AI (or gen AI) applications seen today, was invented by Google in 2017. From this, large language models evolved rapidly, including Bidirectional Encoder Representations from Transformers (BERT) in 2018, Language Model for Dialogue Applications (LaMDA) in 2021, Pathways Language Model (PaLM) in 2022, Gemini, the most recent cutting-edge genAI model to process multimodal data released at the end of 2023, and Gemma, an open model released at the beginning of 2024. In terms of AI products, Google built an end-to-end AI development platform that evolves from AutoML in 2018, to Al Platform in 2019, and Vertex AI in 2021. In 2023, Google also announced a series of generative AI products on Vertex AI such as Vertex AI Studio, Model Garden, and Search and Conversation. You’ll explore all of these AI technologies and products in depth later in this course. Google is a leader in AI and ML innovations. Here are four major reasons: The ML development platform is empowered by Google's state-of-the-art ML models. So you build on excellence. Google provides an end-to-end development platform to convert an ML model from experiment to production so you can be more efficient. Google provides a unified data-to-AI platform so you can develop data and AI projects with ease. Like Google Cloud itself, AI services are based on an efficient and scalable infrastructure so you can get more for less. AI comes with its own set of unique challenges. Google integrates responsible AI into its AI principles. Responsible AI refers to the development and use of artificial intelligence systems in a way that prioritizes ethical considerations, fairness, accountability, safety, and transparency. In June 2018, Google announced seven AI principles to guide its work. These are concrete standards that actively govern Google’s research and product development, and directly impacts Google’s business decisions. Here’re seven principles for responsible AI: 1. AI should be socially beneficial. 2. AI should avoid creating or reinforcing unfair bias. 3. AI should be built and tested for safety. 4. AI should be accountable to people. 5. AI should incorporate privacy design principles. 6. AI should uphold high standards of scientific excellence. 7. AI should be made available for uses that accord with these principles. Establishing principles were a starting point rather than an end. They are a foundation that establishes what Google stands for, what to build, and why to build it, and they are core to the success of Google’s enterprise AI offerings. To learn more about responsible AI and how Google implements it in research and product design, please check the Google doc in the reading list. In sum, Google’s AI principles revolve around being an AI-first company with a rich history in AI development, a leader in AI innovations, and a practitioner of responsible AI ethics. Now you know why you can trust Google for artificial intelligence and machine learning. The next question you may have is how do I start? Assuming you are a data scientist and you want to develop an ML model from beginning to end, what options do you have? Say you are an ML engineer, and you want to build an ML pipeline to automatically monitor the model performance. What tools do you have? Or perhaps you are an AI developer who would like to leverage generative AI capabilities in an application. What products can you use? You’ll find all the answers in this course. This course presents a toolbox that is based on the AI/ML framework on Google Cloud. The toolbox is organized into three layers to make navigation easier. You begin with the AI foundations layer in this module, module one. Here, you learn about cloud essentials like compute and storage. Also about data and AI products that support your journey from data to AI. You then advance to the AI development layer. In module 2, you focus on the options to develop an ML model from beginning to end on Google Cloud. There, you explore out-of-the-box solutions like pre-built APIs, low or no-code solutions like AutoML, and a do-it-yourself approach like custom training. In module 3, you walk through the workflow to build an ML model using Vertex AI, the end-to-end AI development platform, from data preparation, to model training, and finally model serving. Additionally, you learn how to automate this ML workflow using the Vertex AI Pipelines SDK. Finally, in module 4, you are introduced to generative AI, how it works, the tools to develop generative AI projects, and how generative AI empowers AI solutions. Regardless your role, module one provides you with an overview of the AI/ML framework on Google Cloud, the cloud infrastructure, and data and AI products. If you are an AI developer, a data scientist, or an ML engineer who would like to build your own ML models, module two to four will walk you through all the details you need to know. If you are a business user and would like to leverage AI/ML in your applications, AI solutions in module 4 help you navigate through a use case, tools, and technologies.

### Video - [Google Cloud infrastructure](https://www.cloudskillsboost.google/course_templates/593/video/508305)

- [YouTube: Google Cloud infrastructure](https://www.youtube.com/watch?v=vinTUrGmoM4)

Let’s explore Google Cloud infrastructure. Google has been working with data and artificial intelligence since its early days as a company in 1998. Ten years later, in 2008, Google Cloud was launched to provide secure and flexible cloud computing and storage services. You can think of the Google Cloud infrastructure in terms of three layers. At the base layer is networking and security, which lays the foundation to support all of Google’s infrastructure and applications. On the next layer sit compute and storage. Google Cloud separates, or decouples, as it’s technically called, compute and storage so they can scale independently based on need. The top layer includes data and AI/machine learning products, which enable you to perform tasks to ingest, store, process, and deliver business insights, data pipelines, and ML models. Thanks to Google Cloud, these tasks can be accomplished without a need to manage and scale the underlying infrastructure. Let’s begin with compute. Organizations with growing data needs often require lots of compute power to run data and AI jobs. And as organizations design for the future, the need for compute power only grows. Google offers a range of computing services. The first is Compute Engine. Compute Engine is an Iaas, or infrastructure as a service offering, which provides compute, storage, and network resources virtually that are similar to a physical machine. You use the virtual compute and storage resources the same as you manage them locally. Compute Engine provides maximum flexibility for those who prefer to manage server instances themselves. The second is Google Kubernetes Engine, or GKE. GKE runs containerized applications in a cloud environment, as opposed to on an individual virtual machine like Compute Engine. A container represents code packaged up with all its dependencies. The third computing service offered by Google is App Engine, a fully managed PaaS, or platform as a service, offering. PaaS offerings bind code to libraries that provide access to the infrastructure application needs. This allows more resources to be focused on application logic. Then there is Cloud Run, a fully managed compute platform that enables you to run requests or event-driven stateless workloads without having to worry about servers. It abstracts away all infrastructure management so you can focus on writing code, and it automatically scales up and down from zero, so you never have to worry about scale configuration. Cloud Run charges you only for the resources you use, so you never pay for over-provisioned resources. And finally, there is Cloud Run functions, which executes code in response to events, like when a new file is uploaded to Cloud Storage. It’s a completely serverless execution environment, which means you don’t need to install any software locally to run the code and you are free from provisioning and managing servers. Cloud Run functions is often referred to as Functions as a Service. Where does the processing power come from? It’s from the hardware: from computer chips. However, traditional computer chips, like a CPU, or central processing unit, and even the more recent GPU, or graphics processing unit, may no longer scale to adequately reach the rapid demand for ML. To help overcome this challenge, in 2016, Google introduced the Tensor Processing Unit, or TPU. TPUs are Google’s custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads. TPUs act as domain-specific hardware, as opposed to general-purpose hardware like CPUs and GPUs. This allows for higher efficiency by tailoring the architecture to meet the computation needs in a domain, such as the matrix multiplication in machine learning. TPUs are generally faster than current GPUs and CPUs for AI and ML applications. They are also significantly more energy-efficient. Cloud TPUs have been integrated across Google products, making this state-of-the-art hardware and supercomputing technology available to Google Cloud customers. Let’s now examine storage. For proper scaling capabilities, compute and storage are decoupled. That is one major difference between cloud and desktop computing. With cloud computing, compute and storage can scale separately. Most applications require a database and storage solution of some kind. Google Cloud offers fully managed database and storage services. These include: Cloud Storage Cloud Bigtable Cloud SQL Cloud Spanner Firestore And BigQuery How do you choose from these products and services? Well, it depends on the data type and business needs. Let’s look at the data type, which includes unstructured versus structured data. Unstructured data is information stored in a non-tabular form such as documents, images, and audio files. Unstructured data is usually suited to Cloud Storage. Cloud Storage has four primary storage classes. The first is standard storage. Standard storage is considered best for frequently accessed, or “hot,” data. It’s also great for data that is stored for only brief periods of time. The second storage class is nearline storage. This is best for storing infrequently accessed data, like reading or modifying data once per month or less, on average. Examples include data backups, long-tail multimedia content, or data archiving. The third storage class is coldline storage. This is also a low-cost option for storing infrequently accessed data. However, as compared to nearline storage, coldline storage is meant for reading or modifying data at most once every 90 days. The fourth storage class is archive storage. This is the lowest-cost option, used ideally for data archiving, online backup, and disaster recovery. It’s the best choice for data that you plan to access less than once a year, because it has higher costs for data access and operations and a 365-day minimum storage duration. Alternatively, there is structured data, which represents information stored in tables, rows, and columns. Structured data comes in two types: transactional workloads and analytical workloads. Transactional workloads stem from online transaction processing systems, which are used when fast data inserts and updates are required to build row-based records. This is usually to maintain a system snapshot. They require relatively standardized queries that impact only a few records. Then there are analytical workloads, which stem from online analytical processing systems, which are used when entire datasets need to be read. They often require complex queries, for example, aggregations. Once you’ve determined if the workloads are transactional or analytical, you then need to identify whether the data will be accessed using SQL or not. So, if your data is transactional and you need to access it using SQL, then two options are Cloud SQL and Spanner. Cloud SQL works best for local to regional scalability, while Spanner works best to scale a database globally. If the transactional data will be accessed without SQL, Firestore might be the best option. Firestore is a transactional, NoSQL, document-oriented database. If you have analytical workloads that require SQL commands, BigQuery is likely the best option. BigQuery, Google’s data warehouse solution, lets you analyze petabyte-scale datasets. Alternatively, Bigtable provides a scalable NoSQL solution for analytical workloads. It’s best for real-time, high-throughput applications that require only millisecond latency.

### Video - [Data and AI products](https://www.cloudskillsboost.google/course_templates/593/video/508306)

- [YouTube: Data and AI products](https://www.youtube.com/watch?v=lXXoSXUhjJU)

In this lesson, let’s investigate the primary data and AI products on Google Cloud. In the last lesson, you examined Google Cloud infrastructure, specifically compute and storage. The final layer of the Google Cloud infrastructure that is left to explore is data and AI products. As you explored in the earlier lesson, Google offers a range of data and AI tools. So, how do you know which one is best for your business needs? Let’s look closer at the list of products, which can be divided into four general categories along the data-to-AI workflow: You gather data from multiple sources through ingestion and process. The data is then saved in different types of storage based on data type and business needs. You analyze the data and visualize the results. And you can further train an ML model with historical data to either predict future trends or generate new content. These tools are seamlessly connected on Google Cloud, making it easy for data scientists and AI developers to transition from data to AI. For example, BigQuery provides embedded AI features that allow you to directly call SQL commands to train an ML model. Additionally, on Vertex AI, the AI development platform, you can use SQL commands in a notebook to import data from BigQuery and further train ML models. Let’s cover the products available in each stage of the data-to-AI workflow. The first category is ingestion and process, with products that are used to digest both real-time and batch data. The list includes Pub/Sub, Dataflow, Dataproc, and Cloud Data Fusion. Please check cloud.google.com/training for more training on data ingestion and process. The second product category is data storage, and you’ll recall from earlier that there are six storage products: Cloud Storage, which saves unstructured data such as text, image, audio and video. BigQuery Cloud SQL Spanner Bigtable, and Firestore. BigQuery, Cloud SQL, and Spanner focus on SQL databases, while Bigtable and Firestore are NoSQL databases. The third product category is analytics. The major analytics tool is BigQuery, which can be used to analyze data through SQL commands. BigQuery is a fully managed data warehouse that provides two services in one, storage plus analytics. In addition to BigQuery, you have business intelligence (BI) tools to analyze data and visualize results. For example: The Looker family, which includes comprehensive BI tools to visualize, analyze, model, and govern business data. And the final product category is AI and machine learning, which includes both AI development tools and AI solutions. These products are either integrated with generative AI or embedded, with generative AI capabilities: The major product to support AI development is Vertex AI, which is a unified platform and includes multiple tools such as AutoML for predictive AI, Workbench and Colab Enterprise for coding, Vertex AI Studio and Model Garden for generative AI. AI solutions are built on the ML development platform and include state-of-the-art technologies to meet both horizontal and vertical market needs. These include: Document AI Contact Center AI Vertex AI Search for Retail, and Healthcare Data Engine These products unlock insights that only large amounts of data can provide. Many of them have been recently embedded with generative AI capabilities. For example, Contact Center AI is equipped with chatbots that are powered with large language models to understand natural language and engage in conversation like a human agent. You’ll explore the machine learning options and workflow together with these products in greater detail later.

### Video - [ML model categories](https://www.cloudskillsboost.google/course_templates/593/video/508307)

- [YouTube: ML model categories](https://www.youtube.com/watch?v=ivELDGdeXWc)

Before you start building an ML model, let’s revisit foundational knowledge and explore the ML model categories. First, let’s pause to clarify two terms: artificial intelligence and machine learning. You may note that people often use the terms interchangeably, but they do have some differences. Artificial intelligence, or AI, is an umbrella term that includes anything related to computers mimicking human intelligence. Some examples of AI applications include robots and self-driving cars. Machine learning is a subset of artificial intelligence (AI) that allows computers to learn without being explicitly programmed. This is in contrast to traditional programming, where the computer is told explicitly what to do. Machine learning mainly includes supervised and unsupervised learning. You might also hear the terms deep learning, or deep neural networks. This is a subset of machine learning that adds layers in between input data and output results to make a machine learn at much depth. You’ll learn more about neural networks and deep learning later in the course. Lastly is generative AI, which produces content and performs tasks based on requests. Generative AI relies on training extensive models like large language models. These models are a type of deep learning model. So, what’s the difference between supervised and unsupervised learning? Imagine two types of problems: In problem one, you are asked to classify dogs and cats from a very large set of pictures. You already know the difference between dogs and cats. So you label each picture and pass the labeled pictures to a machine. By learning from the data, in this case, pictures with the answers or labels, supervised learning is being enacted, allowing the machine to tell if a new picture represents a dog or cat in the future. In problem two, you are asked to classify breeds of dogs. Unfortunately, this time you don’t know many of them and are not able to label the pictures. So you send these unlabeled pictures to a machine. In this case, the machine learns from the data without the answers and finds underlying patterns to group the animals. This is an example of unsupervised learning. Put simply, supervised learning deals with labeled data, is task-driven, and identifies a goal. Unsupervised learning, however, deals with unlabeled data, is data-driven, and identifies a pattern. An easy way to distinguish between the two is that supervised learning provides each data point with a label, or an answer, while unsupervised does not. There are two major types of supervised learning: The first is classification, which predicts a categorical variable, such as determining whether a picture shows a cat or a dog. In ML, you use models like a logistic regression model to solve classification problems. The second type of supervised learning is regression, which predicts a numeric variable, like forecasting sales for a product based on its past sales. You use ML models like a linear regression model to solve regression problems. There are three major types of unsupervised learning: The first is clustering, which groups together data points with similar characteristics and assigns them to "clusters," like using customer demographics to determine customer segmentation. You use ML models like k-means clustering to solve clustering problems. The second type is association, which identifies underlying relationships, like a correlation between two products to place them closer together in a grocery store for a promotion. You use association rule learning techniques and algorithms like Apriori to solve association problems. And the third type of unsupervised learning is dimensionality reduction, which reduces the number of dimensions, or features, in a dataset to improve the efficiency of a model. For example, combining customer characteristics like age, driving violation history, or car type, to create a simplified rule for calculating an insurance quote. You use ML techniques like principal component analysis to solve these problems. All right, time to test your learning. You are asked to predict customer spending based on purchase history. Is this supervised or unsupervised learning? Yes, that’s supervised learning because you have the labeled data, the amount the customers have spent, and you want to predict their future purchases. Is this a classification or a regression problem? Yes, it’s a regression problem because it predicts a continuous number: future spending. Which ML model should you use? A logistic regression or a linear regression? Yes, a linear regression. A logistic regression model is for classification problems, while a linear regression model is for regression problems. Let’s look at another scenario. Imagine you are using the same dataset. However, this time you are asked to identify customer segmentation. You don’t want to base your judgement on stereotypes such as age or gender, so you use a computer for help. Is this supervised or unsupervised learning? Yes, it’s unsupervised learning because you don’t have each customer labeled as belonging to a certain segment. Instead, you want the computer to discover the underlying pattern. Is it a clustering, association, or dimensionality deduction problem? Yes, identifying customer segmentation is a clustering problem. Which ML model should you use: logistic regression, linear regression, or k-means clustering analysis? Right, it’s a clustering analysis scenario. You will find these models within BigQueryML, AutoML, and custom training later on in this course.

### Video - [BigQuery ML](https://www.cloudskillsboost.google/course_templates/593/video/508308)

- [YouTube: BigQuery ML](https://www.youtube.com/watch?v=3fZQyT4S7sk)

With the different types of ML models in your mind, let’s apply concept to practice. In this lesson, you explore BigQuery ML and walk through the steps to build an ML model with SQL commands. You learned about BigQuery from the previous lesson. BigQuery provides two services in one. It’s a fully managed storage facility to load and store datasets, and a fast SQL-based analytical engine. The two services are connected by Google's high-speed internal network. It’s this super-fast network that allows BigQuery to scale both storage and compute independently, based on demand. Although BigQuery started out solely as a data warehouse, over time it has evolved to provide features that support the data-to-AI lifecycle, meaning you can perform both data analytics and build pre-defined ML models within BigQuery. In this lesson, you explore BigQuery’s capabilities to build ML models and walk through the steps and key SQL commands to do so. If you’ve worked with ML models before, you know that building and training them can be very time-intensive. You must first import and prepare the data. Then, experiment with different ML models and tune the parameters. To improve model performance, you also need to go back and forth to train the model with new data and features. And finally you need to deploy the model to make predictions. This is an iterative process that requires a lot of time and resources. Now, with BigQuery ML, you can manage tabular data and execute ML models in one place with just a few steps. BigQuery ML tunes the parameters for you and helps you manage the ML workflow. Let’s walk through the phases of a machine learning project and the key SQL commands. In phase 1, you extract, transform, and load data into BigQuery if it isn’t there already. If you’re already using other Google products, like YouTube for example, look out for easy connectors to get that data into BigQuery before you build your own pipeline. You can enrich your existing data warehouse with other data sources by using SQL joins. In phase 2, you select and preprocess features. You can use SQL to create the training dataset for the model to learn from. BigQuery ML does some of the preprocessing for you, like one-hot encoding of your categorical variables. One-hot encoding converts your categorical data into numeric data that is required by a training model. In phase 3, you create the model inside BigQuery. This is done by using the 'CREATE MODEL' command. In this example, you want to create an ML model to predict customer purchasing behavior, specifically if they will buy this product in the future. You give the model a name: ecommerce.classification. You then specify the model type. Remember the previous lesson about ML model types. If you want to predict whether a customer will buy or not, which ML model should you use? That’s right. A logistic regression model is the answer because you are solving a classification problem. Other than the logistic regression model to solve the classification problem, BigQuery ML also supports other popular ML models. They include regression models such as linear regression and other models, such as k-means clustering and time series forecasting models. In addition to providing different types of machine learning models, BigQuery ML supports MLOps, machine learning operations. MLOps turns your ML experiment to production and helps deploy, monitor, and manage the ML models. You’ll learn more about MLOps later in this course. You are recommended to start with simple options such as logistic regression and linear regression, and use the results as a benchmark to compare against more complex models such as DNN (deep neural networks), which take more time and computing resources to train and deploy. After specifying the model type, you also need to define the label column. Why? Remember the two major categories of ML models, supervised and unsupervised? The former deals with labeled data and predicts a goal, whereas the latter handles unlabeled data and identifies a hidden pattern. Is this a supervised or unsupervised model? Of course, it’s a supervised classification problem, thus a labeled column. From there, you can run the query. In phase 4, after your model is trained, you can execute an 'ML.EVALUATE' query to evaluate the performance of the trained model on your evaluation dataset. It’s here that you specify which evaluation metrics the model will access, such as accuracy, precision, and recall. You’ll explore these metrics later in the next module. FInally, in phase 5, when you’re happy with your model performance, you can then use it to make predictions. To do so, invoke the 'ML.PREDICT' command on your newly trained model to return with predictions and the model’s confidence in those predictions. With the results, your label field will have “predicted” added to the field name. This is your model’s prediction for that label. You’ll practice all these steps and the SQL commands using BigQuery ML in the hands-on lab.

### Video - [Lab introduction](https://www.cloudskillsboost.google/course_templates/593/video/508309)

- [YouTube: Lab introduction](https://www.youtube.com/watch?v=kJmRuF2vIT4)

Now it's time to get some hands-on practice building a machine learning model in BigQuery. In the lab that follows this video, you'll use e-commerce data from the Google Merchandise Store, shop.merch.google. The site's visitor and order data has been loaded into BigQuery, and you'll build a machine learning model to predict whether a visitor will return for more purchases later. You'll get practice creating a dataset in BigQuery, training an ML model, evaluating the model, and using the model for prediction. Let's get started.

### Lab - [Predict Visitor Purchases with BigQuery ML](https://www.cloudskillsboost.google/course_templates/593/labs/508310)

In this lab, you learn how to create and evaluate a machine learning model with BigQuery ML and use the model to predict purchase behavior.

- [ ] [Predict Visitor Purchases with BigQuery ML](../labs/Predict-Visitor-Purchases-with-BigQuery-ML.md)

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/593/video/508311)

- [YouTube: Summary](https://www.youtube.com/watch?v=CuSmaHAV9O8)

In the first module of this course, you learned about AI foundations. Let’s have a quick recap. You started with the story of Coffee-on-Wheels, which demonstrates how AI enables business processes transformation and efficiency enhancement. You were then introduced to the toolbox of AI/ML on Google Cloud, which consists of three layers: AI foundations, AI development, and AI solutions. This course focuses on the second layer, which delves into both predictive AI and generative AI. Next, you explored the Google Cloud infrastructure. Specifically, compute and storage. Google Cloud decouples compute and storage so they can scale independently based on need. Additionally, you examined data and AI products, which enable you to perform tasks to support the data-to-AI journey, from data ingestion, storage and analytics, to AI and machine learning. After that, you advanced to the fundamental ML concepts including the categories of ML models. Specifically, you learned about supervised versus unsupervised learning. With these concepts in mind, you can choose from different models and follow the steps to build an ML model to fit your needs with BigQuery ML. Finally, you had a hands-on lab where you applied those steps to build your own ML model using SQL commands. This concludes the overview of the AI Foundations module. In the next module, you’ll advance to AI development, and explore the different options to build an AI/ML project. See you soon.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/593/quizzes/508312)

#### Quiz 1.

> [!important]
> **You want to use machine learning to discover the underlying pattern and group a collection of unlabeled photos into different sets. Which should you use?**
>
> - [ ] Unsupervised learning, cluster analysis
> - [ ] Supervised learning, logistic regression
> - [ ] Supervised learning, linear regression
> - [ ] Unsupervised learning, dimensionality reduction

#### Quiz 2.

> [!important]
> **Which SQL command would you use to create an ML model in BigQuery ML?**
>
> - [ ] ML.EVALUATE
> - [ ] ML.PREDICT
> - [ ] CREATE MODEL
> - [ ] CREATE CLASSIFICATION

#### Quiz 3.

> [!important]
> **Vertex AI, AutoML, and Generative AI Studio align to which stage of the data-to-AI workflow?**
>
> - [ ] Storage
> - [ ] Machine learning
> - [ ] Analytics
> - [ ] Ingestion and process

#### Quiz 4.

> [!important]
> **On Cloud Storage, which data storage class is best for storing data that needs to be accessed less than once a year?**
>
> - [ ] Nearline storage
> - [ ] Archive storage
> - [ ] Coldline storage
> - [ ] Standard storage

#### Quiz 5.

> [!important]
> **Which Google hardware innovation tailors architecture to meet the computation needs on a domain, such as the matrix multiplication in machine learning?**
>
> - [ ] CPUs (central processing units)
> - [ ] GPUs (graphic processing units)
> - [ ] TPUs (tensor processing units)
> - [ ] DPUs (data processing units)

#### Quiz 6.

> [!important]
> **Which of the following is one of Google's seven principles for responsible AI?**
>
> - [ ] AI should be used to solve any problem regardless of the ethical principles.
> - [ ] AI should avoid creating or reinforcing unfair bias.
> - [ ] Privacy design should not be a concern of AI.
> - [ ] Financial benefit should be the only consideration of AI.

#### Quiz 7.

> [!important]
> **What are the three layers of the AI/ML framework on Google Cloud?**
>
> - [ ] AI foundations, AI development, and AI solutions
> - [ ] Foundation models, large language models, and application models
> - [ ] AI, ML, and deep learning
> - [ ] ML development, ML applications, and ML use cases

#### Quiz 8.

> [!important]
> **If you have unstructured data, like images, text, and/or audio, which storage option on Google Cloud would you choose?**
>
> - [ ] Cloud Bigtable
> - [ ] Cloud Spanner
> - [ ] Cloud SQL
> - [ ] Cloud Storage

### Document - [Reading](https://www.cloudskillsboost.google/course_templates/593/documents/508313)

## AI Development Options

This module explores the various options for developing an ML project on Google Cloud, from ready-made solutions like pre-trained APIs, to no-code and low-code solutions like AutoML, and code-based solutions like custom training. It compares the advantages and disadvantages of each option to help decide the right development tools.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/593/video/508314)

- [YouTube: Introduction](https://www.youtube.com/watch?v=XsYYgCc-HG4)

In the first section of this course, you learned about AI foundations, specifically on cloud essentials like compute, storage, and networking. You also explored data tools like BigQuery, which helps you start the journey from data to AI. Now you’ll explore how to develop an AI project on Google Cloud, specifically the options available to build a machine learning, or ML, model. You begin by comparing AI development options on Google Cloud, from pre-made, to low-code and no-code, and finally a do-it-yourself approach. You then delve into the first option, pre-trained APIs, which use pre-trained ML models that don’t require any training data. Next, you are introduced to Vertex AI, Google’s unified platform to build an ML model end-to-end and support both no-code and code development. After that, you explore AutoML on Vertex AI, a low- or no-code option to automate the ML development, from data preparation to model training and model serving with your own training data. Finally, you explore the last option, custom training, which allows you manually code ML projects with tools like Python, Keras, and TensorFlow. Consolidating the background knowledge gained throughout this module, you conclude with a hands-on practice using the Natural Language API to identify subjects and analyze sentiment in text. Let’s get started.

### Video - [AI developement options](https://www.cloudskillsboost.google/course_templates/593/video/508315)

- [YouTube: AI developement options](https://www.youtube.com/watch?v=FcHT6Ff3B5c)

Let’s begin with Google Cloud’s AI development options, which include out-of-the-box, low-code and no-code, and do-it-yourself. You’ll explore the pros and cons of each and examine a decision tree to help you consider which option might be best for your business problem. Imagine that you’re helping your organization use AI to transform your business model and operation. Where and how should you start? Let’s say you are a business user or an application developer, and you don’t have training data and lack experience developing an ML model. However, you really want to use AI to automatically label and classify customer feedback. What can you do? Or perhaps you’re a data analyst, and you have some training data and experience using SQL. How can you build a custom ML model with your existing skills? Maybe you’re a data scientist, you have a large amount of data and want to train a custom ML model; however, you don’t want to spend hours tuning ML parameters from the beginning. What choices do you have? Or what if you’re an ML engineer or scientist and enjoy using do-it-yourself models and code to operationalize the ML pipeline. What tools can you use? Well, you can find tools on Google Cloud to help achieve your goals, from preconfigured solutions such as pre-trained APIs, to low or no-code solutions such as BigQuery ML and AutoML, to a completely DIY approach with a code-based solution by using custom training. Let’s look at each of them. Google Cloud offers four options for building machine learning models. The first option is to use pre-trained APIs. API stands for application programming interface. This option lets you use pre-trained machine learning models, so you don’t need to build your own if you don’t have training data or machine learning expertise in-house. The second option is BigQuery ML, which you learned about in the previous module. This option uses SQL queries to create and execute machine learning models in BigQuery. If you already have your data in BigQuery and your problems fit the pre-defined ML models offered by BigQuery ML, this could be your choice. The third option is AutoML, which is a no-code solution that helps you build your own machine learning models on Vertex AI through a point-and-click interface. And finally, there is custom training, through which you can code your very own machine learning environment, training, and deployment. This option allows you flexibility and control over the ML pipeline. Let’s compare the four options to help you decide which one to use for building your ML model. Note that the technologies change constantly and this is only a brief guideline. BigQuery ML only supports tabular data, whereas the other three support tabular, image, text, and video. Pre-trained APIs also process audio. In terms of training data size, pre-trained APIs do not require any training data, Whereas BigQuery ML and custom training require a large amount of data. Pre-trained APIs and AutoML are user friendly with low requirements for machine learning and coding expertise, whereas custom training has the highest requirement, and BigQuery ML requires you to understand SQL. At the moment, you can’t tune the hyperparameters with pre-trained APIs or AutoML; however, you can experiment with hyperparameters by using BigQuery ML and custom training. Pre-trained APIs require no time to train a model because they directly use pre-trained models from Google. The time to train a model for the other three options depends on the specific project. Normally, custom training takes the longest time, because it builds the ML model from the beginning, unlike AutoML and BigQuery ML. The best option depends on your business needs and ML expertise. Budget is also an important consideration. Visit Google Cloud’s website for detailed pricing information. If you have little ML experience and no intention to train your own ML models, using pre-trained APIs might be the best choice. Pre-trained APIs address common perceptual tasks such as vision, video, and natural language. They are ready to use without any model development effort. If your data engineers, scientists, or analysts are familiar with SQL and already have data in BigQuery, BigQuery ML lets you use SQL queries to build predefined ML models. If you wish to build custom models with your own training data while you spend minimal time coding, then AutoML on Vertex AI is your choice. AutoML allows you focus on business problems instead of the underlying model architecture and provisioning. If your ML engineers and data scientists want full control of ML workflow, Vertex AI custom training lets you train and serve custom models with code on Vertex AI Workbench or Google Colab. Let’s walk through pre-trained APIs, AutoML, and custom training one by one later in this module.

### Video - [Pre-trained APIs](https://www.cloudskillsboost.google/course_templates/593/video/508316)

- [YouTube: Pre-trained APIs](https://www.youtube.com/watch?v=fWTBMRrChso)

Now that you’ve been introduced to the different AI development options available with Google Cloud, let’s now focus on the first: pre-trained APIs. Good machine learning models require lots of high-quality training data. You should aim for hundreds of thousands of records to train a custom model. But what if you don't have that kind of data? How can you use AI to serve your purposes? Pre-trained APIs are a great place to start. API stands for application programming interface, and they define how software components communicate with each other. Imagine APIs are electric sockets. Different regions have different standards, for example, the US has type A and B whereas Europe has type F. As a traveller, you only need to know which adapter to use without worrying about what’s behind the wall and how to build the electric network. The same principle applies to APIs. As a user, you only need to know which API to fit your code without worrying about the implementation of the APIs. Or in other words, how to train and deploy ML models. After you plug the API to your code, you can directly use the functions. Pre-trained APIs are offered as services. In many cases, they can act as building blocks to create the application you want without the expense or complexity of creating your own models. They save the time and effort of building, curating, and training a new dataset so you can just directly deal with predictions. So, what are the pre-trained APIs provided by Google Cloud? Let’s explore a short list. Speech, text, and language APIs For example, the Natural Language API derives insights from text using pre-trained large language models. It recognizes the entities and sentiment of a sentence. Image and video APIs For example, the Vision API recognizes content in static images, and the Video Intelligence API recognizes motion and action in video. Document and data APIs For example, the Document API processes documents like text extraction and form parser. It can be used in specialized use cases like lending, contracts, procurement, and identity documents. Conversational AI APIs For example, the Dialogflow API builds conversational interfaces. Let’s try out the Natural Language API in a browser. Scroll down to try the API by uploading a paragraph of text, which can be in over ten different languages including Chinese, English, and Spanish. Feel free to view the full list by clicking See supported languages. For example, if you use the sample paragraph about Google, and click ANALYZE, You find four types of analysis: entity, sentiment, syntax, and category. Entity analysis identifies the subjects in the text including: A proper noun, such as the name of a particular person, place, organization, or thing. In this case, the Natural Language API automatically detects Google as an organization, Mountain View as a location, and Sundar Pichai as a person. Common nouns such as goods are also addressed. In this case, It identifies Android and phone as consumer goods. How can entity analysis be applied to solve your business problems? How about automatic tagging? Say you have tons of legal documents and you want to auto-tag the main words of each document. How about document classification? Say you want to classify your documents to different categories based on key information in the text. Or what about information extraction, so you can generate summaries based on the key entities in the doc? There are many other usages of entity analysis. Take a minute to think about how to apply it to your use cases. Sentiment analysis is used to identify the emotions indicated in the text such as positive, negative, and neutral. Score ranges between -1.0 (negative) and 1.0 (positive) and magnitude indicates the overall strength of emotion within the given text, between between zero and positive infinity. The Natural Language API can analyze the sentiment for the entire document and at entity level. How can sentiment analysis be used solve your business problems? Well, it can be used to analyze the emotion of customer feedback, social network comments, and conversations. You can then use this data as feedback to adjust your offerings. In addition to entity and sentiment analysis, the Natural Language API can analyze syntax and extract linguistic information for further language model training in a specific field. It can also do category analysis for the entire text. For example, this text is about an internet and telecommunications company. The UI, or user interface, demonstrates the major features of the Natural Language API. When you’re ready to build a production model, you can integrate the APIs into your code. We’ll show you how in the hands-on lab later in this module. In summary, you can build AI projects and applications without training your own ML models or providing training data. Instead, you can use pre-trained AI models through APIs provided by Google. In addition to the above APIs, generative AI APIs have rapidly evolved recently. These APIs allow you to use different foundation models to generate various types of content. Some examples include Gemini multimodal, processing data in multiple modalities like text, image, and video. Embeddings for both text and multimodal, converting multimodal data into numerical vectors that can be processed by ML models, especially gen AI foundation models. Gemini for text and chat, performing language tasks and conducting natural conversations. Imagen for Image, generating images and captions. Chirp for speech, building voice-enabled applications. And Codey for code, generating, completing, and chatting about code. As gen AI advances every day, you’ll find new APIs and models emerging in the near future. In the latter part of this course, you get a chance to delve deeper into the multifaceted realm of generative AI.

### Video - [Vertex AI](https://www.cloudskillsboost.google/course_templates/593/video/508317)

- [YouTube: Vertex AI](https://www.youtube.com/watch?v=iTLbZhlZktM)

In this lesson, you’ll explore Vertex AI, which is the unified platform that supports various technologies and tools on Google Cloud to help you build an ML project from end to end. For years now, Google has invested time and resources into developing data and AI. Google had developed crucial technologies and products, from scikit-learn as a Google summer coding project back in 2007, to Vertex AI and generative AI today. As an AI-first company, Google has applied AI technologies to many of its products and services, like Gmail, Google Maps, Google Photos, and Google Translate, just to name a few. But developing these technologies doesn’t come without challenges. Some traditional challenges include handling large quantities of data, determining the right machine learning model to train the data, and harnessing the required amount of computing power. Then, there are challenges around getting ML models into production. Some of the challenges can be scalability, monitoring, and continuous integration, delivery, and training. In fact, according to Gartner, only half of enterprise ML projects get past the pilot phase. There are also ease-of-use challenges. Many tools on the market require advanced coding skills, which can take a data scientist’s focus away from model configuration. Without a unified workflow, data scientists often have difficulties finding tools. Google’s solution to many of the production and ease-of-use challenges is Vertex AI, a unified platform that brings all the components of the machine learning ecosystem and workflow together. So, what exactly does a unified platform mean? There are two primary aspects: Firstly, it means that Vertex AI provides an end-to-end ML pipeline to prepare data, and create, deploy, and manage models over time, and at scale. For instance, During the data readiness stage, users can upload data from wherever it’s stored: Cloud Storage, BigQuery, or a local machine. Then, during the feature readiness stage, users can create features, which are the processed data that will be put into the model, and then share them with others by using the feature store. After that, it’s time for training and hyperparameter tuning. This means that when the data is ready, users can experiment with different models and adjust hyperparameters. And finally, during deployment and model monitoring, users can set up the pipeline to transform the model into production by automatically monitoring and performing continuous improvements. You’ll learn how to do this later in the course, when you explore MLOps. Second, Vertex AI is a unified platform that encompasses both predictive AI and generative AI. Predictive AI allows for sales forecasting and classification, while generative AI enables the creation of multimodal content. Vertex AI allows users to build ML models with either AutoML, a no-code solution, or custom training, a code-based solution. AutoML provides an easy to navigate UI. It lets data scientists focus on what business problems to solve instead of how to code and deploy an ML solution. Custom training gives data scientists and ML engineers more control over the development environment and process. They can use tools like Vertex AI Workbench and Colab to DIY their ML projects. One convenient feature is that data scientists can now write SQL with Workbench on Vertex AI to seamlessly connect BigQuery and Vertex AI. Being able to perform such a wide range of tasks in one unified platform has many benefits. This can be summarized with four Ss: It’s seamless. Vertex AI provides a smooth user experience from uploading and preparing data all the way to model training and production. It’s scalable. The machine learning operations (MLOps) provided by Vertex AI help to monitor and manage the ML production and therefore scale the storage and computing power automatically. It’s sustainable. All of the artifacts and features created using Vertex AI can be reused and shared. And it’s speedy. Vertex AI produces models that have 80% fewer lines of code than competitors. In addition to AutoML and custom training, Vertex AI also provides tools for generative AI. You can use these tools to generate content and embed generative AI into your applications. We will discuss generative AI technologies and tools later in this course.

### Video - [AutoML](https://www.cloudskillsboost.google/course_templates/593/video/508318)

- [YouTube: AutoML](https://www.youtube.com/watch?v=sPMwoVMqng8)

In the last lesson, you learned about Vertex AI, a unified platform that supports both AutoML, a no-code solution, and custom training, a code-based solution. In this lesson, you’ll explore AutoML in depth, including the technologies used to power automated ML development. AutoML, which stands for automated machine learning, aims to automate the process to develop and deploy an ML model. If you've worked with ML models before, you know that building them can be extremely time consuming, because you need to repeatedly add new data and features, try different models, and tune parameters to achieve the best result. When AutoML was first announced in January of 2018, the goal was to save the manual work from data scientists and automate machine learning pipelines from pre-processing data to model training and deployment. Since 2021, AutoML features are embedded in Vertex AI and have become part of the platform. But how could this be done? How can you trust AutoML to generate the best results without bias and do so in a speedy way? Let’s look deeper to explore how AutoML works and the main technologies behind it. AutoML is powered by the latest research from Google. It’s an ongoing endeavor. There are four distinct phases. Phase one is data processing. After you upload a dataset, AutoML provides functions to automate part of the data preparation process. For example, it can covert numbers, datetime, text, categories, arrays of categories, and nested fields into a certain format of data so that it can be fed into an ML model. Phase two includes searching the best models and tuning the parameters. Two critical technologies support this auto search. The first is called neural architect search, which helps search the best models and tune the parameters automatically. And the second is called transfer learning, which helps speed the searching by using the pre-trained models. Let’s look at transfer learning first. Machine learning is similar to human learning. It learns new things based on existing knowledge. AutoML has already trained many different models with large amounts of data. These trained models can be used as a foundation model to solve new problems with new data. A typical example are large language models (LLM), which are general purpose, and can be pre-trained and fine-tuned for specific purposes. LLMs are trained for general purposes to solve common language problems such as text classification, question answering, document summarization, and text generation across industries. The models can then be tailored to solve specific problems in different fields such as retail, finance, and entertainment, using a relatively small size of field datasets. Transfer learning is a powerful technique that lets people with smaller datasets or less computational power achieve great results by using pre-trained models trained on similar, larger datasets. Because the model learns through transfer learning, it doesn’t have to learn from the beginning, so it can generally reach higher accuracy with much less data and computation time than models that don’t use transfer learning. Now let’s look at neural architecture search. The goal of neural architecture search is to find optimal models among many options. Specifically, AutoML. Tries different architectures and models, and compares against the performance between models to find the best ones. For instance, AutoML can search through multiple advanced ML models and automatically tune the parameters to find the best fit for your data. In phase three, the best models are assembled from phase 2 and prepared for prediction in phase 4. Note that AutoML does not rely on one single model, but on the top number of models. The number of models depends on the training budget, but is typically around 10. The assembly can be as simple as averaging the predictions of the top number of models. Relying on multiple top models instead of one greatly improves the accuracy of prediction. By applying these advanced ML technologies, AutoML automates the pipeline from feature engineering, to architecture search, to hyperparameter tuning, and to model ensemble. It might seem that AutoML can do a better job than a human to find the optimal models that fit your data. Perhaps, the best feature of AutoML is that it provides a no-code solution. You can point and click through a UI to build an ML model with your own data. You’ll walk through the details from preparing training data, to train you model, and finally get prediction in the next module.

### Video - [Custom training](https://www.cloudskillsboost.google/course_templates/593/video/508319)

- [YouTube: Custom training](https://www.youtube.com/watch?v=npmS3j91xiA)

Now let’s look at custom training, a do-it-yourself solution to build an ML project. You explored the options that Google Cloud provides to build machine learning models by using pre-trained APIs, where you directly call Google’s trained models to solve your problems, BigQuery ML, where you write a few lines of SQL code to train your own model, and AutoML, where you build your ML model with your own data through a UI. But if you want to create your own machine learning environment to experiment with and build your own pipeline, you need custom training. Before any coding begins, you must determine what environment you want your ML training code to use. There are two options: a pre-built container or a custom container. A pre-built container is like a furnished kitchen with cabinets, appliances, and cookware. So, if your ML training needs a platform like Python, TensorFlow, and PyTorch, and you’re not particular about the underlying infrastructure to run on, or to use our kitchen analogy, which oven or knife you use, a pre-built container is probably your best choice. A custom container, alternatively, is like an empty room. You define the exact appliances and tools that you prefer to cook with. That means you must determine the details like the environment, machine type, and disks when creating the custom container. In terms of the tools to code your ML model, you can use Vertex AI Workbench. You can think of Vertex AI Workbench as a Jupyter notebook deployed in a single development environment that supports the entire data science workflow, from exploring to training and then deploying a machine learning model. You can also use Colab Enterprise, which was integrated into Vertex AI Platform in 2023 so data scientists can code in a familiar environment. After you decide the working environment, the next step is to start writing code. These days, you don’t have to code from scratch. Instead, you can leverage ML libraries. An ML library is a collection of pre-written code that can be used to perform machine learning tasks. These libraries can save developers time and effort by providing them with the tools they need to build machine learning models without having to write everything from the beginning. As a data scientist, you might already be familiar with popular ML libraries like TensorFlow, scikit-learn, and PyTorch. They are open source and widely used by a large community of users and developers. Let’s explore TensorFlow, an end-to-end open platform for machine learning supported by Google. Tensorflow contains multiple abstraction layers. You use TensorFlow APIs to develop and train ML models. The TensorFlow APIs are arranged hierarchically, with the high-level APIs built on the low-level APIs. The lowest layer is hardware. TensorFlow can run on different hardware platforms including CPU, GPU, and TPU. The next layer is the low-level TensorFlow APIs, where you can write your own operations in C++ and call the core, basic, and numeric processing functions written in Python. The third layer is the TensorFlow model libraries, which provide the building blocks such as neural network layers and evaluation metrics to create a custom ML model. The high-level TensorFlow APIs like Keras sit on top of this hierarchy. They hide the ML building details and automatically deploy the training. They can be your most used APIs. Note that Vertex AI fully hosts TensorFlow from low-level to high-level APIs. Regardless of which abstraction level you are writing your TensorFlow code at, Vertex AI gives you a managed service. Now let’s look at an example of using tf.keras, a high-level TensorFlow library commonly used, to build a simple regression model. Typically, it takes three fundamental steps: In step one, you create a model, where you piece together the layers of a neural network. In step two you compile the model, where you specify hyperparameters such as performance evaluation and model optimization. Finally, you train your model to find the best fit. Assume you already imported necessary packages like TensorFlow and uploaded the data. The first step is to create a model by using tf.keras. Sequential. To demonstrate, you can define your model as a three-layer neural network. You’ll explore more details about neural networks such as activation functions in the next module. The next step is to compile the model by specifying how you want to train it by using the method compile. For instance, you can decide how to measure the performance by specifying a loss function. You can also optimize the training by pointing to an optimizer. The last step is to train the model by using the method fit. For instance, you can define the input, the training data, and the output, the predicted results. You can also decide how many iterations you want to train the model by specifying the numbers of epochs. After you train the model and are satisfied with the performance, you can then deploy the model and make predictions. Apart from Tensorflow, Google is consistently introducing new frameworks. One of the most promising frameworks is JAX. JAX is a high-performance numerical computation library that is highly flexible and easy to use. It offers new possibilities for both research and production environments.

### Video - [Lab introduction](https://www.cloudskillsboost.google/course_templates/593/video/508320)

- [YouTube: Lab introduction](https://www.youtube.com/watch?v=S0NiyWPRzAU)

Now it’s time for some hands-on practice. In the following lab, you’ll use the Natural Language API to analyze text. Specifically, you’ll identify entities and analyze sentiment with code. Before you begin, let’s briefly review the main features of the NaturalLanguage API you learned in the previous lesson: You can identify entities, which are subjects in the inputted text, such as Google as a company name and Mountain View as a location. You can identify the sentiment, which indicates emotion at both the overall document and individual subject level. You can analyze syntax and extract linguistic information such as the relationship between words. And you can also classify the text to categories based on topics or keywords, similar to assigning a tag to a piece of text. You perform all of this analysis through a UI, which is a quick and efficient way to demonstrate and test these features. However, if you want to incorporate these features in production, you must embed the APIs into code. Using APIs in your code is similar to ordering a sandwich at a deli: you order from the menu and get your food without worrying about how it was made in the kitchen. The same concept applies to using APIs; you only need to know three things: the features (the menu), the input (the order), and the output (the sandwich). Like a menu, features are the types of requests that you can make to the Natural Language API. Like a food order, the input is how how you construct the requests. Then, the sandwich you receive after you place the order is the response (or output). With this, you can determine next steps. So, what are different types of requests that you can make? The Natural Language API provides several methods for performing analysis and annotation on your text. You’ll practice with most of them in the lab. For entity analysis, you can use the analyzeEntities method. The sentiment analysis is performed through the analyzeSentiment method at the entire text level, and analyzeEntitySentiment at the individual entity and subject level. The syntactic analysis is performed with the analyzeSyntax method. And the content classification is performed by using the classifyText method. Now, how do you construct those requests? The Natural Language API is a REST API and consists of JSON requests and responses. A simple JSON request for entity analysis looks like the code shown here, where you define: The type of the document, for example, plain text The language, like EN, which stands for English The content, which can be the text itself or the file location in Cloud Storage, and finally, The encoding type like UTF8 After you construct the request, you need to call the API, just like after you decide what you want at a deli, you then need to place the order with the counter person. Here is an example to call the API with cURL. cURL stands for Client URL and is a command line tool to transfer data between client and server. You can also use other programming languages such as Python and Java SDKs to call the APIs. Typically, the vendors of the product and services you are using define the APIs and provide the SDKs in different languages for you to choose. In this example, you call the Natural Language API feature, analyzeEntites, pass the request.json file that you just constructed, and save the response to result.json file. Finally, how should you handle the responses? You can review the result by using a command like cat result.json or parse it for further usage. Equipped with the technical details, in this lab you’ll use the Natural language API to: Extract entities. Analyze sentiment. And analyze syntax. By completing the lab, you’ll get practice: Creating a Natural Language API request and calling the API with cURL. Extracting entities and running sentiment analysis on text. Performing linguistic analysis on text. And creating a Natural Language API request in a different language. Let’s start!

### Lab - [Entity and Sentiment Analysis with the Natural Language API](https://www.cloudskillsboost.google/course_templates/593/labs/508321)

The Cloud Natural Language API lets you extract entities, and perform sentiment and syntactic analysis on a block of text.  In this hands-on lab you’ll learn how to extract entities and sentiment from text using the Cloud Natural Language API.

- [ ] [Entity and Sentiment Analysis with the Natural Language API](../labs/Entity-and-Sentiment-Analysis-with-the-Natural-Language-API.md)

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/593/video/508322)

- [YouTube: Summary](https://www.youtube.com/watch?v=FGw-Wpvgmj4)

This concludes the module on AI development options. Let’s do a quick recap. You compared AI development options, from a ready-made approach, to low-code and no-code, to do-it-yourself. You started with pre-trained APIs, which are ready-made solutions using pre-trained machine learning models without the need for any training data. You were then introduced to Vertex AI, Google’s unified platform to build a machine learning project from end-to-end. You learned about AutoML, which is a low or no-code tool on Vertex AI that lets you automate ML development from data preparation to model training and model serving with your own data. And then you learned about custom training, which lets you manually code ML projects by using tools like Python, Tensorflow, and Vertex AI Workbench. Finally, you got hands on practice with a lab on the Natural Language API, which identifies subjects and analyzes sentiment in text. In the next module, you’ll explore the AI development workflow, where you walk through the ML pipeline and build an ML model end-to-end. See you soon!

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/593/quizzes/508323)

#### Quiz 1.

> [!important]
> **Your company has a massive amount of data, and you want to train your own machine learning model to see what insights ML can provide. Due to resource constraints, you require a codeless solution. Which option is best?**
>
> - [ ] AutoML
> - [ ] BigQuery ML
> - [ ] Pre-trained APIs
> - [ ] Custom training

#### Quiz 2.

> [!important]
> **Which of the following can you do with the Natural Language API?**
>
> - [ ] Analyze sentiment and identify subjects of text.
> - [ ] Classify pictures.
> - [ ] Generate a caption for a YouTube video.
> - [ ] Complete new areas of an existing image.

#### Quiz 3.

> [!important]
> **You work for a global hotel chain that has recently loaded some guest data into BigQuery. You have experience writing SQL and want to leverage machine learning to help predict guest trends for the next few months. Which option is best?**
>
> - [ ] Custom training
> - [ ] BigQuery ML
> - [ ] Pre-trained APIs
> - [ ] AutoML

#### Quiz 4.

> [!important]
> **Which code-based solution offered with Vertex AI gives data scientists full control over the development environment and process?**
>
> - [ ] AI Platform
> - [ ] AutoML
> - [ ] Custom training
> - [ ] AI Solutions

#### Quiz 5.

> [!important]
> **tf.keras is a high-level TensorFlow library that has been commonly used to build ML models. Which of the following lets you create a neural network with multiple layers?**
>
> - [ ] model.fit
> - [ ] model.compile
> - [ ] tf.keras.Run
> - [ ] tf.keras.Sequential

#### Quiz 6.

> [!important]
> **A video production company wants to use machine learning to categorize event footage but does not want to train its own ML model. Which option can help you get started?**
>
> - [ ] AutoML
> - [ ] BigQuery ML
> - [ ] Pre-trained APIs
> - [ ] Custom training

### Document - [Reading](https://www.cloudskillsboost.google/course_templates/593/documents/508324)

## AI Development Workflow

This module walks through the ML workflow from data preparation, to model development, and to model serving on Vertex AI. It also illustrates how to convert the workflow into an automated pipeline using Vertex AI Pipelines.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/593/video/508325)

- [YouTube: Introduction](https://www.youtube.com/watch?v=WdgZmloMmcg)

In the previous section of this course, you learned about different options to develop an AI project on Google Cloud, from a ready-to-use approach like pre-trained APIs, to low or no-code solutions like AutoML, and to DIY solutions, such as custom training. In this section, you further investigate how to develop an AI project on Google Cloud. Specifically, you walk through the ML workflow and explore how to create an automated pipeline. You begin with an overview of the ML workflow. You then focus on the first stage of the workflow, data preparation, which includes data uploading and feature engineering. After that, you advance to the second stage, model development. This includes model training and evaluation. You then proceed to the third stage, model serving. This includes model deployment and monitoring. Next, you learn about machine learning operations, or MLOps, which takes ML models from development to production. You’ll also be shown an example of how to build a pipeline to automate the training, evaluation, and deployment of an ML model by using Vertex AI Pipelines. You conclude this module with a hands-on lab, where you walk through the three stages to build an ML model with AutoML on Vertex AI. Gaining a solid grasp of ML terminology requires a clear understanding of how a neural network learns. This module offers an optional lesson that delves into the neural network's learning process, along with the key terminologies. If you're already familiar with the ML theories, feel free to skip this lesson. Let’s get started.

### Video - [ML workflow](https://www.cloudskillsboost.google/course_templates/593/video/508326)

- [YouTube: ML workflow](https://www.youtube.com/watch?v=2dLOv8pPhRk)

Let’s look at the ML workflow and walk through the main stages. Building an ML model is actually not too different from serving food in a restaurant. You start by preparing raw ingredients and finish by serving the dishes on the table. There are three main stages to the ML workflow with Vertex AI. The first stage is data preparation, which includes two steps: data uploading and feature engineering. A model needs a large amount of data to learn from. The quality and quantity of the data decide how much and how well the machine learns. The data used in machine learning can be real-time streaming data or batch data. The data can also be structured or unstructured. Structured data is data that can be easily stored in tables, such as numbers and text. Unstructured data is data that cannot be easily stored in tables, such as images and videos. The second stage of the ML workflow is model development. A model needs a tremendous amount of iterative training. This is when training and evaluation form a cycle to train a model, then evaluate the model, and then train the model some more. The third and final stage is model serving. A model needs to actually be used in order to predict results. This is when the machine learning model is deployed and monitored. If you don’t move an ML model into production, it has no use and remains only a theoretical model. Compare this process to serving food in a restaurant, data preparation is when you prepare the raw ingredients, model development is when you experiment with different recipes, and model serving is when you finalize the menu to serve the meal to customers. Now, it’s important to note that an ML workflow isn’t linear, it’s iterative. For example, during model training, you might need to return to the raw data and generate more useful features to feed the model. When monitoring the model during model serving, you might find data drifting, or the accuracy of your prediction might suddenly drop. You might need to check the data sources and adjust the model parameters. Fortunately, these steps can be automated with MLOps. You’ll learn more about this later in this module. Although the main stages remain the same, you have two options to set up the workflow with Vertex AI: The first choice is to use AutoML, a no-code solution that lets you build an ML model through UI. It's user friendly and doesn't require a lot of ML expertise. Also, no coding skills are needed. 2. Alternatively, you can code the workflow with Vertex AI Workbench or Colab using Vertex AI Pipelines. Vertex AI Pipelines is essentially a tool kit that includes pre-built SDKs and software development kits, which are the building blocks of a pipeline. Coding the pipeline is a good option if you're an experienced ML engineer or data scientist and want to automate the workflow programmatically. Let’s focus on AutoML next and then explore the code-based approach later in this module.

### Video - [Data preparation](https://www.cloudskillsboost.google/course_templates/593/video/508327)

- [YouTube: Data preparation](https://www.youtube.com/watch?v=E84btG5KXfg)

Let’s start with the first stage of ML workflow, data preparation. [pause] During this stage, you must upload data and then prepare it for model training with feature engineering. The data can come from Cloud Storage, BigQuery, or even your local machine. AutoML supports four types of data: image, tabular, text, and video. For each data type, AutoML solves different types of problems, called objectives. For image data, you can train the model to classify images into either single-label or multi-label. Single-label means you can only assign one tag to an image like dog or cat, whereas multi-label means you can assign multiple tags to one image like dog, large, and brown. You can also train the model to detect objects and discover image segmentation. For tabular data, you can train the model to solve regression, classification, or forecasting problems. Forecasting is vital to many industries like retail. To learn more about how to build a forecasting model, please check the course titled Introduction to Vertex Forecasting and time series in practice in the reading list. For text data, you can train the model to: Classify text. Extract entities. And conduct sentiment analysis. Do these tasks sound familiar? [Pause for one second] Yes, you explored these tasks in the previous module when you learned about natural language APIs. Finally, for video data, you can train the model to Recognize video actions. Classify videos. And track objects. Right, you learned about entity extraction and sentiment analysis when you explored natural language APIs in the previous module. Although the tasks can be similar, pre-trained APIs rely on Google’s pre-trained ML models with no customer data, whereas AutoML trains a custom model using your own data. In reality, you might not be restricted to just one data type and one objective. Instead, you might need to combine multiple data types and different objectives to solve a business problem. AutoML is a powerful tool that can help you do this. After the data is uploaded to AutoML, the next step is preparing it for model training with feature engineering. Imagine you’re preparing a meal. Your data is like your ingredients, such as carrots, onions, and tomatoes. Before you start cooking, you'll need to peel the carrots, chop the onions, and rinse the tomatoes. This is what feature engineering is like: the data must be processed before the model starts training. A feature refers to a factor that contributes to the prediction. It’s like an independent variable in statistics or a column in a table. Preparing features can be both challenging and tedious. To help, Vertex AI provides a service called Vertex AI Feature Store, which is a centralized repository to manage, serve, and share features. It aggregates the features from different sources in BigQuery and makes them available for both real-time (often called online) and batch (often called offline) serving. Vertex AI automates the feature aggregation to scale the process with low latency. Additionally, Vertex AI Feature Store is ready for the challenge of generative AI. It can manage and serve embeddings, which is the crucial data type in gen AI. It also supports retrieving similar items in real time, ensuring low latency. The workflow to set up and start real time (online) serving using Vertex AI Feature Store can be summarized as follows: Prepare the data source in BigQuery. Optional: Register the data sources by creating feature groups and features. . . Set up online store and feature view resources to connect the feature data sources with online serving clusters. Serve the latest feature values online from a feature view. So what are the benefits of Vertex AI Feature Store? First, features are shareable for training and serving. They are managed and served from a central repository, maintaining consistency across your organization. Second, features are reusable. This helps to save time and reduces duplicated efforts. Third, features are scalable. They automatically scale to provide low-latency serving, so you can focus on developing the logic to create them without worrying about deployment. And fourth, features are easy to use. Vertex AI Feature Store is built on an easy-to-navigate user interface.

### Video - [Model development](https://www.cloudskillsboost.google/course_templates/593/video/508328)

- [YouTube: Model development](https://www.youtube.com/watch?v=jJnjjEGkcas)

Let’s advance to the second stage, model development, where you train the model and evaluate the result. Now that our data is ready—which, if you return to the cooking analogy, is the ingredients— it’s time to train the model. This is like experimenting with recipes. This stage involves two steps: model training, which is like cooking the recipe, and model evaluation, which is like testing how good the meal is. This process might be iterative. To set up an ML model, you need to specify a few things. First of all is the training method, where you tell Vertex AI the dataset you just uploaded from the preparation stage. Depending on the data type, whether it is tabular, image, text, or video, you specify the training objective. This is the goal of the model training and the task you want to solve. Then you choose the training method: AutoML, without code, or custom training, using code. The next step is to determine the training details. For example, if you are training the model to solve a supervised learning problem such as regression and classification, you must choose the target column from your dataset. In training options, you can choose certain features to participate in the training and transform the data type if needed. Finally you specify the budget and pricing, and then click Start training. AutoML will train the model for you and choose the best performed models among thousands of others. Do you recall the powerful technologies behind AutoML? Right, the credit there goes to neural architecture search and transfer learning. While you are experimenting with a recipe, you need to keep tasting it to ensure that it meets expectations. This is the evaluation portion of the model development stage. Vertex AI provides extensive evaluation metrics to help determine a model’s performance. Let's focus on the metrics of recall and precision when evaluating the performance of classification models. To do this, you’ll use a confusion matrix. A confusion matrix is a specific performance measurement for machine learning classification problems. It’s a table with combinations of predicted and actual values. To keep things simple, we assume the output includes only two classes. Let’s explore an example. The first is true positive, which can be interpreted as, “The model predicted positive, and that’s true.” The model predicted that this is an image of a cat, and it actually is. The opposite of that is true negative, which can be interpreted as, “The model predicted negative, and that’s true.” The model predicted that the image is not a cat, and it actually isn’t. Then there is false positive, otherwise known as a Type 1 Error, which can be interpreted as, “The model predicted positive, and that’s false.” The model predicted that the image is a cat but it actually isn’t. Finally, there is false negative, otherwise known as a Type 2 Error, which can be interpreted as, “The model predicted negative, and that’s false.” The model predicted that the image is not a cat, but it actually is. A confusion matrix is the foundation for many other metrics used to evaluate the performance of a machine learning model. Let’s look at the two popular metrics, recall and precision, that you will encounter in the lab. Recall refers to all the positive cases, and looks at how many were predicted correctly. This means that recall is equal to the true positives, divided by the sum of the true positive and false negatives. Precision refers to all the cases predicted as positive, and how many are actually positive. This means that precision is equal to the true positives, divided by the sum of the true positives and false positives. Imagine you’re fishing with a net. Using a wide net, you caught both fish and rocks: 80 fish out of 100 total fish in the lake, plus 80 rocks. The recall in this case is 80%, which is calculated by the number of fish caught, 80, divided by the total number of fish in the lake, 100. The precision is 50%, which is calculated by taking the number of fish caught, 80, and dividing it by the number of fish and rocks collected, 160. Let’s say you wanted to improve the precision, so you switched to a smaller net. This time you caught 20 fish and 0 rocks. The recall becomes 20% (20 out of 100 fish collected), and the precision becomes 100% (20 out of 20 total fish and rocks collected). Precision and recall are often a trade-off. Depending on your use case, you might need to optimize for one or the other. Consider a classification model where Gmail separates emails into two categories: spam and not-spam. If the goal is to catch as many potential spam emails as possible, Gmail might want to prioritize recall. In contrast, if the goal is to only catch the messages that are definitely spam without blocking other emails, Gmail might want to prioritize precision. Vertex AI visualizes the precision-recall curve so it can be adjusted based on the problem that needs to be solved. You’ll get the opportunity to practice adjusting precision and recall in the AutoML lab. In addition to the confusion matrix and the metrics generated to measure recall and precision, the other useful measurement is feature importance. In Vertex AI, feature importance is displayed through a bar chart to illustrate how each feature contributes to a prediction. The longer the bar, or the larger the numerical value associated with a feature, the more important it is. This information helps decide which features are included in a machine learning model to predict the goal. You will observe the feature importance chart in the lab as well. Feature importance is just one example of Vertex AI’s comprehensive machine learning functionality called Explainable AI. Explainable AI is a set of tools and frameworks to help understand and interpret predictions made by machine learning models. Please check the reading list if you want to know about Explainable AI on Google Cloud.

### Video - [Model serving](https://www.cloudskillsboost.google/course_templates/593/video/508329)

- [YouTube: Model serving](https://www.youtube.com/watch?v=pVuWRHXK-Jk)

Let’s focus on the third stage of the ML workflow, model serving. The recipes are ready, and now it’s time to serve the meal. This represents the final stage of the machine learning workflow, model serving. Model serving consists of two steps: First, model deployment, which you can compare to serving a meal to a hungry customer, and second, model monitoring, which is like checking with the waitstaff to ensure that the restaurant is operating efficiently. It’s important to note that model management exists throughout this whole workflow to manage the underlying machine learning infrastructure. This lets data scientists focus on what to do, and not on how to do it. Let’s start with model deployment, which is the exciting time when the model is implemented and ready to serve. You have two primary options: Deploy the model to an endpoint for real-time predictions (or often called online predictions). This option is best when immediate results with low latency are needed, such as making instant recommendations based on a user’s browsing habits whenever they’re online. A model must be deployed to an endpoint before it can be used to serve real-time predictions. Request the prediction job directly from the model resource for batch prediction. This option is best when no immediate response is required. For example, sending out marketing campaigns every other week based on the user’s recent purchasing behavior and what’s currently popular on the market. Batch prediction does not require deploying the model to an endpoint. You can deploy a model either using the UI on Vertex AI or using code by calling APIs. You’ll practice building an endpoint later in the lab. Beyond making predictions in the cloud, deploying the model off-cloud is also possible. This approach is generally adopted when the model needs to be deployed in a specific environment to mitigate latency, ensure privacy, or enable offline functionality. For instance, consider an IoT (Internet of Things) application like object detection that utilizes a camera feed in a manufacturing plant. In such use case, the added latency of relying on the cloud can be impractical. Once the model is deployed and begins making predictions or generating contents, it is important to monitor its performance. The backbone of automating ML workflow on Vertex AI is a tool kit called Vertex AI Pipelines. It automates, monitors, and governs machine learning systems by orchestrating the workflow in a serverless manner. Imagine you’re in a production control room, and Vertex AI Pipelines is displaying the production data on-screen. If something goes wrong, it automatically triggers and displays a warning based on a predefined threshold. With Vertex AI Workbench, which is a notebook tool, you can define your own pipeline using SDKs. You can do this with prebuilt pipeline components, which means that you primarily need to specify how the pipeline is put together using components as building blocks. You’ll explore more details about Vertex AI Pipelines in the next lesson. And it’s with these final two steps, model deployment and model monitoring, that you complete the exploration of the machine learning workflow. The restaurant is open and operating smoothly. Bon appetit!

### Video - [MLOps and workflow automation](https://www.cloudskillsboost.google/course_templates/593/video/508330)

- [YouTube: MLOps and workflow automation](https://www.youtube.com/watch?v=HJDtn_mZQBo)

In this lesson, you explore an advanced topic: MLOps and workflow automation. You learned from the previous lessons to build an ML model through three main stages: data preparation, model development, and model serving. You have two approaches to build an end-to-end workflow. One is codeless through Google Cloud console, like AutoML on Vertex AI. But what if you want to automate this workflow to achieve continuous integration, training, and delivery? Here comes the other option: to code a pipeline that automates the ML workflow. Machine learning operations, or MLOps, play a big role. MLOps combines machine learning development with operations and applies similar principles from DevOps (or development operations) to machine learning models. MLOps aims to solve production challenges related to machine learning. In this case, this refers to building an integrated machine learning system and operating it in production. These are considered to be some of the biggest pain points by the ML practitioners’ community, because both data and code are constantly evolving in machine learning. Practicing MLOps means automating and monitoring each step of the ML system construction to enable continuous integration, training, and delivery. The backbone of MLOps on Vertex AI is a tool kit called Vertex AI Pipelines, which supports both Kubeflow Pipelines, or KFP, and TensorFlow Extended, or TFX. If you already use TensorFlow to build ML models that process terabytes of structured data, it makes sense to use TFX and turn that code into an ML pipeline. Otherwise, KFP can be a good alternative. Learn more about how to choose between the Kubeflow Pipelines SDK and TFX from the reading list. An ML pipeline contains a series of processes and runs in two different environments. First is the experimentation, development, and test environment, And second is the staging, pre-production, and production environment. In the development environment, you start from data preparation which includes data extraction, analysis, and preparation, to model development like training, evaluation, and validation. The result is a trained model that can be entered in model registry. Once the model is trained, the pipeline moves to the staging and production environment, where you serve the model, which includes prediction and monitoring. Each of these processes can be a pipeline component, which is a self-contained set of code that performs one task of a workflow. You can think of a component as a function, which is a building block of a pipeline. You can either build a custom component on your own or leverage the pre-built components provided by Google. If you want to accomplish a specific task to tailor your ML workflow, such as determining a special threshold for model deployment, you may need to code a custom component. Before doing so, check the pre-built components offered by Google Cloud. You may find a pipeline component to reuse or customize to suit your needs. Learn more about using Google Cloud pipeline components in your pipeline in the reading list. All these components are like pieces of an ML pipeline. You need to assemble them together to automate the entire ML workflow. Organizations often implement ML automation in three phases. Phase zero is the starting point, where you have not configured any MLOps. You typically use the graphical user interface, or GUI, based workflow such as AutoML for training, deployment, and serving. Phase zero is critical because it helps you build an end-to-end workflow manually before you automate it. In phase one, you start automating your ML workflow by building components using the Vertex AI Pipelines SDKs. An example of a component would be the training pipeline. It is in this phase that you develop the building blocks for future use. In phase two, you integrate the separate components to form an entire workflow and to achieve CI, CT, and CD. Let’s look at an example. Assume you want to build a pipeline to train, evaluate, and deploy an AutoML model that classifies beans into 1 of 7 types based on their characteristics. You have two main steps: build a pipeline and then run it. To build a pipeline, you first plan it as a series of components, which can be a combination of custom and pre-built. To promote reusability, each component should have a single responsibility. Second, you build any custom components that are needed. For example, you create a component called classification_model_eval_metrics. You use this component to compare the evaluation metrics to a threshold after the model is trained, and determine whether the model should be deployed. If the model performs well, you deploy it. Otherwise, you retrain the model. 3. Third, you assemble the pipeline by adding pre-built components, for example: TabularDatasetCreateOp creates a tabular dataset in Vertex AI given a dataset source either in Cloud Storage or BigQuery. AutoMLTabularTrainingJobRunOp kicks off an AutoML training job for a tabular dataset. EndpointCreateOp creates an endpoint in Vertex AI. And ModelDeployOp deploys a given model to an endpoint in Vertex AI. You also include a custom component from the previous step, classification_model_eval_metrics, to compare the performance of the trained model to a threshold. After the pipeline is built, you must compile and run it. First, you compile it using the compiler.Compiler(). compile() or compile() commands. And then, you define and run the pipeline job. The good news is you don’t have to create a pipeline from the beginning. Vertex AI provides a few templates, like the one for classification/regression of tabular data with AutoML, to help you start your journey. Now you have an automated pipeline to train, evaluate, and deploy an ML model. This pipeline will check the performance of the model constantly and decide whether it should be deployed or retained without your intervention. The nice thing is that Google Cloud also visualizes the pipeline based on the coding, with which you can easily check the components and the corresponding artifacts. This example demonstrated the overall process to build a pipeline. To know more about pipeline details, please practice with the coding example in the demo, Introduction to Vertex AI Pipelines, in the reading list for this course.

### Video - [Lab introduction](https://www.cloudskillsboost.google/course_templates/593/video/508331)

- [YouTube: Lab introduction](https://www.youtube.com/watch?v=9uufA-xzAI4)

Let’s put what you’ve just learned into practice with a hands-on lab. In this lab, you’ll use AutoML, a no-code tool, to build a machine learning model to predict loan risk. The dataset used in the lab relates to loans from a financial institution and has 2,050 data points. AutoML requires at least 1,000 data points in a dataset. The goal is to practice working through the three phases of the machine learning workflow: data preparation, model development, and model serving. Before you start the lab, let’s explain the details about model evaluation so that you can interpret the training results. Let’s start with the confusion matrix. You’ll get a similar result like this in the lab. Pause for a second and try to interpret this matrix yourself. What does it tell you? 100% true positive rate: this means the model perfectly identifies everyone who will repay their loan. It never misses a "good" borrower. That's fantastic. Note that the true positive rate equals true positives divided by the sum of true positives and false negatives. If the terms sound unfamiliar, please refer to the previous example, where you learned about the confusion matrix. 87% true negative rate: the model also correctly identifies 87% of people who won't repay (defaulters). Note that the true negative rate equals true negatives divided by the sum of false positives and true negatives. . 13% false positive rate: this means the model mistakenly identifies 13% of good borrowers (those who actually repay) as defaulters. This can lead to the bank rejecting potentially good customers, which is not ideal. Note that the false positive rate equals false positives divided by the sum of false positives and true negatives. Finally, 0% false negative rate: this means the model never incorrectly identifies a borrower who will repay their loan as a potential defaulter. Note that the false negative rate equals false negatives divided by the sum of true positives and false negatives. Let's look at the precision-recall curve you will encounter in the upcoming AutoML Lab. The confidence threshold determines how a machine learning model counts the positive cases. A higher threshold increases the precision but decreases recall. A lower threshold decreases the precision, but increases recall. Moving the confidence threshold to zero produces the highest recall of 100% and the lowest precision of 50%. So, what does that mean? That means the model predicts that 100% of loan applicants will be able to repay a loan they take out. However, actually only 50% of people were able to repay the loan. Using this threshold to identify the default cases in this example can be risky, because it means that you can only get half of the loan investment back. Now let’s view the other extreme by moving the threshold to 1. This will produce the highest precision of 100% with the lowest recall of 1%. What does this mean? It means that of all the people who were predicted to repay the loan, 100% of them actually did. However, you rejected 99% of loan applicants by only offering loans to 1% of them. That’s a pretty big business loss for your company. These are both extreme examples, but it’s important that you always try to set an appropriate threshold for your model. Now that we’ve reviewed, let’s start the lab.

### Video - [How a machine learns](https://www.cloudskillsboost.google/course_templates/593/video/508332)

- [YouTube: How a machine learns](https://www.youtube.com/watch?v=Mcan5bjlLos)

To understand machine learning, you must first understand how neural networks learn. This includes exploring this learning process and the terms associated with it. If you are already familiar with the ML theories and terminologies, feel free to skip this lesson. How do machines learn? And how do they assess their learning? Before you dive into building an ML model, let's take a look at how a neural network learns. You may already know about various neural networks, such as deep neural networks (or DNN), convolutional neural networks (or CNN), recurrent neural networks (or RNN), and more recently large language models (LLMs). These networks are used to solve different problems. All of these models stem from the most basic: artificial neural network (or ANN). ANNs are also referred to as neural networks or shallow neural networks. Let’s focus on ANN to see how a neural network learns. An ANN has three layers: an input layer, a hidden layer, and an output layer. Each node represents a neuron. The lines between neurons stimulate synopses, which is how information is transmitted in a human brain. For instance, if you input article titles from multiple resources, the neural network can tell you which media outlet or platform the article belongs to, such as GitHub, New York Times, and TechCrunch. How does an ANN learn from examples and then make predictions? Let’s examine how it works in depth. [Animation: Zoom/fade into the center of the “Hidden Layer” above, transitioning to next slide] Let’s assume you have two input neurons or nodes, one hidden neuron, and one output neuron. Above the link between neurons are weights. The weights retain information that a neural network learned through the training process and are the mysteries that a neural network aims to discover. The first step is to calculate the weighted sum. This is done by multiplying each input value by its corresponding weight, and then summing the products. It normally includes a bias component bi . However, to focus on the core idea, ignore it for now. The second step is to apply an activation function to the weighted sum. What is an activation function, and why do you need it? Let’s pause your curiosity for just a moment and get back to that soon. In the third step, the weighted sum is calculated for the output layer, assuming multiple neurons in the hidden layers. The fourth step is to apply an activation function to the weighted sum. This activation function can be different from the one applied to the hidden layers. The result is the predicted y, which consists of the output layer. You use y hat to represent the predicted result and y as the actual result. Now let’s get back to activation functions. What does an activation function do? Well, an activation function is used to prevent linearity or add non-linearity. What does that mean? Think about a neural network. Without activation functions, the predicted result y hat will always be a linear function of the input x, regardless of the number of layers between input and output. Let’s walk through this for clarity. Without the activation function, the value of the hidden layer h equals a total of w1 times x1 and w2 times x2. Please note that to make this illustration easy, we ignored the bias component b, which you often see in other ML materials. The output y hat therefore equals to w3 times h, and eventually equals to a total of constant number a times x1 and a constant number b times x2 In other words, the output Y is a linear combination of the input X. If y is a linear function of x, you don’t need all the hidden layers, but only one input and one output. You might already know that linear models do not perform well when handling comprehensive problems. That’s why you must use activation functions to convert a linear network to a non-linear one. What are the widely used activation functions? You can use the rectified linear unit (or ReLU) function, which turns an input value to zero if it’s negative, or keeps the original value if it’s positive. You can use the sigmoid function, which turns the input to a value between 0 and 1. And hyperbolic tangent (Tanh) function, which shifts the sigmoid curve and generates a value between -1 and +1. Another interesting and important activation function is called softmax. Think about sigmoid: it generates a value from zero to one and is used for binary classification in logistic regression models. An example for this would be deciding whether an email is spam. What if you have multiple categories, such as GitHub, NYTimes, and TechCrunch? Here you must use softmax, which is the activation function for multi-class classification. It maps each output to a [0,1] range in a way that the total adds up to 1. Therefore, the output of softmax is a probability distribution. Skipping the math, you can conclude that softmax is used for multi-class classification, whereas sigmoid is used for binary-class classification in logistic regression models. Also note that you don’t need to have the same activation function across different layers. For instance, you can have ReLU for hidden layers and softmax for the output layer. Now that you understand the activation function and get a predicted y, how do you know if the result is correct? You use an assessment called loss function or cost function to measure the difference between the predicted y and the actual y. Loss function is used to calculate errors for a single training instance, whereas cost function is used to calculate errors from the entire training set. Therefore, in step five, you calculate the cost function to minimize the difference. If the difference is significant, the neural network knows that it did a bad job in predicting and must go back to learn more and adjust parameters. Many different cost functions are used in practice. For regression problems, mean squared error, or MSE, is a common one used in linear regression models. MSE equals the average of the sum of squared differences between y hat and y. For classification problems, cross-entropy is typically used to measure the difference between the predicted and actual probability distributions in logistic regression models. If the difference between the predicted and actual results is significant, you must go back to adjust weights and biases to minimize the cost function. This potential sixth step is called backpropagation. The challenge now is how to adjust the weights. The solution is slightly complex, but indeed the most interesting part of a neural network. The idea is to take cost functions and turn them into a search strategy. That’s where gradient descent comes in. Gradient descent refers to the process of walking down the surface formed by the cost function and finding the bottom. It turns out that the problem of finding the bottom can be divided into two different and important questions: The first is: which direction should you take? The answer involves the derivative. Let’s say you start from the top left. You calculate the derivative of the cost function and find it’s negative. This means the angle of the slope is negative and you are at the left side of the curve. To get to the bottom, you must go down and right. Then, at one point, you are on the right side of the curve, and you calculate the derivative again. This time the value is positive, and you must slide again to the left. You calculate the derivative of the cost function every time to decide which direction to take. Repeat this process, according to gradient descent, and you will eventually reach the regional bottom. The second question in finding the bottom is what size should the steps be? The step size depends on the learning rate, which determines the learning speed of how fast you bounce around to reach the bottom. Step size or “learning rate” is a hyperparameter that is set before training. If the step size is too small, your training might take too long. If the step size is too large, you might bounce from wall to wall or even bounce out of the curve entirely, without converging. When step size is just right, you’re set. The seventh, and last step, is iteration. One complete pass of the training process from step 1 to step 6 is called an epoch. You can set the number of epochs as a hyperparameter in training. Weights or parameters are adjusted until the cost function reaches its optimum. You can tell that the cost function has reached its optimum when the value stops decreasing, even after many iterations. This is how a neural network learns. It iterates the learning by continuously adjusting weights to improve behavior until it reaches the best result. This is similar to a human learning lessons from the past. We have illustrated a simple example with two input neurons (nodes), one hidden neuron, and one output neuron. In practice, you might have many neurons in each layer. Regardless of the number of neurons in the input, hidden, and output layer, the fundamental process of how a neural network learns remains the same. Learning about neural networks can be exciting, but also overwhelming with the large number of new terms. Let’s take a moment to review them. In a neural network, weights and biases are parameters learned by the machine during training. You have no control of the parameters except to set the initial values. The number of layers and neurons, activation functions, learning rate, and epochs are hyperparameters, which are decided by a human before training. The hyperparameters determine how a machine learns. For example, the learning rate decides how fast a machine learns and the number of epochs defines how many times the learning interates. Normally, data scientists choose the hyperparameters and experiment with them to find the optimum combination. However, if you use a tool like AutoML, it automatically selects the hyperparameters for you and saves you plenty of experiment time. You also learned about cost or loss functions, which are used to measure the difference between the predicted and actual value. They are used to minimize error and improve performance. You use backpropagation to modify the weights and bias if the difference is significant, and gradient descent to decide how to tune the weights and bias and when to stop. These terms are your best friends when building an ML model. You’ll revisit them in upcoming lessons and labs.

### Lab - [Vertex AI: Predicting Loan Risk with AutoML](https://www.cloudskillsboost.google/course_templates/593/labs/508333)

In this lab, you will use AutoML on Vertex AI to train and serve a model with tabular data. Vertex AI is the newest AI product offering on Google Cloud, and is currently in preview.

- [ ] [Vertex AI: Predicting Loan Risk with AutoML](../labs/Vertex-AI-Predicting-Loan-Risk-with-AutoML.md)

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/593/video/508334)

- [YouTube: Summary](https://www.youtube.com/watch?v=WbR5ykqGRtI)

This concludes the module on AI development workflow. Let’s do a quick recap. In this module, you learned about the three main stages of the machine learning workflow with the help of the restaurant analogy. In stage one, data preparation, you uploaded data and applied feature engineering. This translated to gathering our ingredients and then chopping and prepping them in the kitchen. In stage two, model development, the model was trained and evaluated. This is where you experimented with the recipes and tasted the meal to ensure that it turned out as expected. And in the final stage, model serving, the model was deployed and monitored. This translates to serving the meal to customers and adjusting the menu as more people tried and reviewed the dish. There are two ways to build a machine learning model from end to end. One is through a user interface, like you practiced in the AutoML lab. The other is with code, which you were shown using prebuilt SDKs with Vertex AI Pipelines. The latter helps you automate the ML pipeline to achieve continuous integration, training, and delivery. We hope you enjoyed this module! Next, you’ll advance to Generative AI, which is the most recent AI development that offers lots of exciting opportunities! See you soon!

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/593/quizzes/508335)

#### Quiz 1.

> [!important]
> **Select the correct machine learning workflow.**
>
> - [ ] Model serving, data preparation, model development
> - [ ] Data preparation, model development, model serving
> - [ ] Data preparation, model evaluation, model training
> - [ ] Model training, data preparation, model serving

#### Quiz 2.

> [!important]
> **Which stage of the machine learning workflow includes data upload and feature engineering?**
>
> - [ ] Model training
> - [ ] Model serving
> - [ ] Data preparation

#### Quiz 3.

> [!important]
> **Which of the following provides a toolkit to automate, monitor, and govern machine learning systems by orchestrating the workflow in a serverless manner?**
>
> - [ ] Vertex AI Pipelines
> - [ ] Responsible AI
> - [ ] Explainable AI
> - [ ] Vertex AI Feature Store

#### Quiz 4.

> [!important]
> **When you build an ML pipeline on Vertex AI to automate the ML workflow, what are the components you can use?**
>
> - [ ] You can include both prebuilt components (by Google) and custom components into the pipeline.
> - [ ] You can only use prebuilt components.
> - [ ] You can only use the prebuilt pipeline template without the flexibility to customize it.
> - [ ] You can only rely on custom components.

#### Quiz 5.

> [!important]
> **A hospital uses the machine learning technology of Google to help pre-diagnose cancer by feeding historical patient medical data to the model. The goal is to identify as many potential cases as possible. Which metric should the model focus on?**
>
> - [ ] Feature importance
> - [ ] Recall
> - [ ] Precision
> - [ ] Confusion matrix

#### Quiz 6.

> [!important]
> **Which stage of the machine learning workflow includes model training and evaluation?**
>
> - [ ] Model serving
> - [ ] Model development
> - [ ] Data preparation

#### Quiz 7.

> [!important]
> **A farm uses the machine learning technology of Google to detect defective apples in their crop, like those with irregular sizes or scratches. The goal is to identify only the apples that are actually bad so that no good apples are wasted. Which metric should the model focus on?**
>
> - [ ] Feature importance
> - [ ] Confusion matrix
> - [ ] Recall
> - [ ] Precision

### Document - [Reading](https://www.cloudskillsboost.google/course_templates/593/documents/508336)

## Generative AI

This module introduces generative AI (gen AI), the newest advancement in AI, and the essential toolkits for developing gen AI projects. It starts by examining the gen AI workflow on Google Cloud. It then investigates how to use Gen AI Studio and Model Garden to access Gemini multimodal, design prompt, and tune models. Finally, it explores the built-in gen AI capabilities of AI solutions.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/593/video/508337)

- [YouTube: Introduction](https://www.youtube.com/watch?v=PKOqYusED8U)

In the previous module, you learned how to develop an ML model from beginning to end. More specifically, you walked through the ML workflow and explored how to convert it into an automated pipeline. In this module, you learn about generative AI, the most recent AI innovation that offers a wide range of exciting opportunities. You explore the generative AI tools that assist AI development and the integration of its capabilities into AI solutions. To begin, it is important to understand what generative AI (or gen AI) entails, how it functions, and how to create a gen AI project on Google Cloud. You then explore Gemini multimodal, one of the latest foundation models trained by Google, and how to use it with Vertex AI Studio, a primary tool for a developer to access and tune gen AI models. After that, you delve into the art and science of prompt design, and a more advanced topic: model tuning. In addition to Vertex AI Studio, you can rely on Model Garden, a model library to access different gen AI models. Finally, you explore how gen AI is embedded in the AI solutions like CCAI, Contact Center AI. With the gen AI tools and knowledge in hand, you conclude your learning journey with a hands-on lab using Vertex AI Studio to create prompts and conversations. Let’s get started.

### Video - [Generative AI and workflow](https://www.cloudskillsboost.google/course_templates/593/video/508338)

- [YouTube: Generative AI and workflow](https://www.youtube.com/watch?v=6KPfJik4WnY)

To begin, it is important to understand what generative AI (gen AI) entails, how it functions, and how to create a gen AI project on Google Cloud. Imagine you are a marketing manager spending hours creating compelling content and distributing it through multiple channels, both traditional and new media. Or you're a data scientist, writing SQL commands to run complex queries on various data sources. Or perhaps you're an app developer, creating a chatbot in a specific field like healthcare. Well, generative AI can be your new go-to friend for all these tasks. Generative AI is transforming how we interact with technology. So, what is generative AI? It’s a type of artificial intelligence that generates content for you. What kind of content? Well, the generated content can be multi-modal, including text, code, images, speech, video, and even 3D. When given a prompt or a request, generative AI can help you with a variety of tasks, such as launching marketing campaigns, generating code, creating chatbots, extracting information, summarizing documents, and providing virtual assistance. And these are just some examples. Then, how does AI generate new content? It learns from a massive amount of existing content such as text, image, and video. The process of learning from existing content is called training, which results in the creation of a “foundation model.” A foundation model is usually a large model in the sense of a significant number of parameters, massive size of training data, and high requirements of computational power. Google has a long history innovating in the area of foundation models and gen AI technologies. Starting from Transformer in 2017, which lays the foundation for all modern gen AI applications seen today, to Gemini in 2023, which extends the word general in artificial general intelligence (AGI) in the sense of handling multimodal processing. At present, the primary foundation models trained by Google include: Gemini for multimodal processing, Gemma, a light-weight, open model, for language generation, Codey for code generation, and Imagen for image processing. Please note that this list is subject to change as foundation models evolve. Additionally, Gemini has the potential to replace some of these models, as it can process data across multiple modalities. A foundation model can then be used directly to generate content and solve general problems, such as content extraction and document summarization. It can also be trained further with new datasets in your field to solve specific problems, such as financial model generation and healthcare consulting. This results in the creation of a new model that is tailored to your specific needs. This leads to the next point, pre-trained and fine-tuned, meaning to pre-train a foundation model for a general purpose with a large dataset and then fine-tune it for specific aims with a much smaller dataset. Imagine training a dog. Often, you train your dog basic commands such as sit, come, down, and stay. These commands are normally sufficient for general purposes in everyday life, and help your dog become a good canine citizen. However, if you need a special-service dog such as a police dog, a guide dog, or a hunting dog, you add special trainings. This similar idea applies to pre-trained versus fine-tuned models. For example, a large language model, which is a typical type of generative AI foundation model, is trained for general purposes to solve common language problems such as text classification, question answering, document summarization, and text generation across industries. The models can then be tailored to solve specific problems in different fields, such as: retail, finance, and entertainment, by using relatively small datasets from these fields. Powered by the foundation models like Large Language Models, or LLMs, generative AI is driving new opportunities to enhance productivity, save operational costs, and create new values. You might have seen these opportunities from the use case about Coffee on Wheels in Module 1, where you use gen AI capabilities to automate the marketing campaign, generate customer feedback, and optimize truck route. This sounds exciting. How can you access gen AI models and create a gen AI project on Google Cloud? Recall your best friend Vertex AI, which is the end-to-end AI development platform supporting both predictive AI and generative AI? Aside from the capabilities you learned in previous modules for powering the experimentation, training, and implementation of predictive ML models, Vertex AI offers a range of tools for tuning your gen AI models and developing gen AI projects, like Vertex AI Studio and Model Garden. Before you dive into these tools, let’s walk through the gen AI workflow on Google Cloud: Before you dive into these tools, let’s walk through the gen AI workflow on Google Cloud: 1. Input prompt: Via the Vertex AI Studio UI, input a prompt—a natural language request to gen AI models. 2. Responsible AI and safety measures: The prompt undergoes responsible AI and safety checks, configurable through the UI or code. 3. Foundation models: The screened prompt proceeds to foundation models like Gemini multimodal or other gen AI models like Imagen and Codey based on your choices. 4. Model customization: Optionally, customize gen AI models to fit your data and use cases by further tuning them. 5. Results grounding: Gen AI models return results that undergo grounding (optional) and citation checks to prevent hallucinations. 6. Final response: The final response appears on the Vertex AI Studio UI after a final check through responsible AI and safety measures. Let’s focus on Vertex AI Studio, the interface to assess gen AI models, and Model Garden, the repository of gen AI models in the following lessons.

### Video - [Gemini multimodal](https://www.cloudskillsboost.google/course_templates/593/video/508339)

- [YouTube: Gemini multimodal](https://www.youtube.com/watch?v=wx0joK32CE8)

Let’s explore Gemini multimodal, one of the latest foundation models trained by Google. In the previous lesson, you walked through the gen AI workflow, where Vertex AI Studio provides an intuitive interface between developers and the foundational models. It enables you to build gen AI applications in a low-code or even no-code environment, where you can: Rapidly test and prototype models. Tune and customize models using your own data. Augment them with real-world, up-to-date information. Deploy models efficiently in production environments with auto-generated code. Let’s uncover the capabilities of Gemini multimodal and delve into practical ways to access it with Vertex AI Studio. So, what is a multimodal model? It’s a large foundation model that is capable of processing information from multiple modalities, including text, image, and video. The generated content can also be in multiple modalities. For example, you can send the model a photo of a plate of cookies and ask it to give you a recipe for those cookies. Gemini is such a multimodal model trained by Google. You can access or even tune it with Vertex AI Studio. How can Gemini help you with your business use cases? Gemini excels at a diverse range of multimodal use cases. Here are some notable examples: Description and captioning: Gemini can identify objects in images and videos, providing detailed or concise descriptions as needed. Information extraction: it can read text from images and videos, extracting important information for further processing. Information analysis: it can analyze the information it extracts from images and videos based on specific prompts. For instance, it can classify expenses on a receipt. Information seeking: Gemini can answer questions or generate Q&A based on the information it extracts from text, images and videos. Content creation: it can create stories or advertisements using images and videos as inspiration. Gemini excels at a diverse range of multimodal use cases. Here are some notable examples: Description and captioning: Gemini can identify objects in images and videos, providing detailed or concise descriptions as needed. Information extraction: it can read text from images and videos, extracting important information for further processing. Information analysis: it can analyze the information it extracts from images and videos based on specific prompts. For instance, it can classify expenses on a receipt. Information seeking: Gemini can answer questions or generate Q&A based on the information it extracts from text, images and videos. Content creation: it can create stories or advertisements using images and videos as inspiration. Data conversion: it can convert text responses to various formats, such as HTML and JSON. Can you think of a real-world use case to apply Gemini multimodal? In light of these exciting advancements, how can developers engage with Gemini and create applications that leverage multimodal capabilities? There are three primary approaches, each essentially achieving the same objective: Using a user interface (UI) with the Google Cloud console. This no-code solution is ideal for exploring and testing prompts. Using predefined SDKs in different languages like Python and Java with notebooks like Colab and Workbench, which are seamlessly integrated within the Vertex AI platform. Utilizing Gemini APIs in conjunction with command-line tools like Curl. Regardless which method to access the Gemini multimodal, you start with a prompt. So, what is a prompt? In the world of generative AI, a prompt is a natural language request submitted to a model in order to receive a response. You feed the desired input text like questions and instructions to the model. The model will then provide a response based on how you structured your prompt. Therefore, the answers you get depend on the questions you ask. Let’s look at the anatomy of a prompt, which includes one or more of the following components: input, context, and examples. Input (required) An input represents your request for a response from the model. It can take various forms: A question that the model can answer, which is a question input. A task that the model can perform, which is a task input. An entity that the model can operate on, which is an entity input. Partial input that the model can complete or continue, which is a completion input. Context (optional) A context can serve multiple purposes: Specify instructions to guide the model's behavior. Provide information for the model to use or reference in generating a response. When you need to supply information or limit responses within the scope of your prompt, include contextual information. For instance, you could assume the role of an IT help desk and consistently advise users to restart their computers, regardless of the nature of their inquiries. Examples (optional) Examples are pairs of inputs and outputs that demonstrate the desired response format to the model. Incorporating examples in the prompt is an effective technique for tailoring the response format. For example, you can establish the context and assume the role of an IT help desk. And then, you provide a few examples. This is called few-shot prompting. Couldn’t log in, suggest a password reset. Lost internet connection, suggest checking devices like a modem or router. Screen went black, suggest restarting the computer. Subsequently, when the question becomes, "What should I do when my computer freezes?", the model may suggest restarting the computer. Context and examples are widely used when training or tuning a gen AI model to behave as you desire. The process of figuring out and designing the input text to get the desired response back from the model is called prompt design, which often involves a lot of experimentation. You’ll learn more about prompt and prompt design later in this course. Let's explore the Gemini multimodal feature within Vertex AI Studio. Navigate to the Vertex AI Studio Overview page and click on "Multimodal powered by Gemini: Try it now." You'll see three sections: a prompt field at the top, a response field at the bottom, and a configuration panel on the right. Click "Insert media" and upload an image. For example, let's use a departure board image from an airport. Enter your first prompt: "Read the text from the image." Before clicking submit, let's look at the configuration on the right. You can choose from a list of models. The default model is usually the most recent cutting-edge model, which is currently Gemini Pro Vision. The temperature setting controls the degree of randomness in the response, with 0 being the most expected answer and 1 being the most creative. The safety settings allow you to adjust the likelihood of receiving a response that could contain harmful content. Content is blocked based on the probability of harmfulness. For example, for hate speech, you can choose from "block few," "block some," and "block most." Adjust these settings based on your use case and click "Save." In the next lesson, you'll learn more about advanced settings such as top-k and top-p. Once you've completed the configuration, it's time to get the response. Click "Submit" and wait a moment. Here's the result. If the result is not easy to read, you can further adjust your prompt to: "Read the text from the image and put it into two columns: time and destination." Does the result look better? You can also be more adventurous and do some analysis. For example, you can change the prompt to: "Calculate the percentage of the flights to different continents and put them into two columns: percentage and continent." Here is the result. If you desire to further develop the application and make the process productionalized, you can click the code located on the top right corner. There, you'll find the code describing the prompt and the settings in the user interface. Alternatively, you can retrieve Curl, which serves as the API to call in a command-line interface. Additionally, you have the option to open a notebook with the SDKs code of your preferred programming languages, such as Python. These automated generated coding and the integrated development environment significantly simplify the production process. Hopefully, this provides a straightforward guide on how to utilize Gemini multimodal with Vertex AI Studio. At the end of this course, you practice with prompts and settings in a hands-on practice. Enjoy learning.

### Video - [Prompt design](https://www.cloudskillsboost.google/course_templates/593/video/508340)

- [YouTube: Prompt design](https://www.youtube.com/watch?v=ITq7q0SFEU0)

Let’s delve into the art and science of prompt design in this lesson, a fundamental skill for utilizing generative AI effectively. To get started experimenting with the gen AI models, click on New prompt. Let’s start with a free-form prompt. One way to design a prompt is to simply tell the model what you want. In other words, provide an instruction. For example, “Generate a list of items I need for a camping trip to Joshua Tree National Park.” You send this text to the model and you can see that the model outputs a useful list of items you don’t want to camp without. This approach of writing a single command so that the model can adopt a certain behavior is called zero-shot prompting. Generally, there are 3 methods that you can use to shape the model's response in a way that you desire. Zero-shot prompting is a method where the model is given a prompt that describes the task without additional examples. For example, if you want the LLM to answer a question, you just prompt "What is prompt design?" One-shot prompting is a method where the LLM is given a single example of the task that it is being asked to perform. For example, if you want the LLM to write a poem, you might provide a single example poem. And few-shot prompting is a method where the model is given a small number of examples of the task that it is being asked to perform. Recall the examples of an IT help desk in the previous lesson. You can use the structured mode to design the few-shot prompting by providing a context and additional examples for the model to learn from. The structured prompt contains a few more components. First, you have the context, which instructs how the model should respond. The context applies each time you send a request to the model. Let’s say you want to use the model to answer questions about the changes in rainforest vegetation in the Amazon. You can paste this background text as the context. Then, you add some examples of questions, such as “What does LGM stand for?” or “What did the analysis from the sediment deposits indicate?” You also need to include the corresponding answers to demonstrate how you want the model to respond. Then, you can test the prompt you’ve designed by sending a new question as input. And there you go, you’ve prototyped a Q&A system based on few-shotth prompting in just a few minutes. Please note a few best practices around prompt design. The prompt needs to Be concise. Be specific and write clearly-defined prompts. be specific and clearly defined. Ask one task at a time. Additionally, you can Improve response quality by including examples. Adding instructions and a few examples tends to yield good results. However, there’s currently no one best way to write a prompt. You may need to experiment with different structures, formats, and examples to see what works best for your use case. For more information about prompt design, please check prompt design in the reading list. So, if you designed a prompt that you think is working pretty well, you can save it and return to it later. Your saved prompt will be visible in the prompt gallery, which is a curated collection of sample prompts that show how generative AI models can work for a variety of use cases. Finally, in addition to testing different prompts and prompt structures, there are a few model parameters you can experiment with to try to improve the quality of the responses. First, there are different models you can choose from. Each model is tuned to perform well on specific tasks. You can also specify the temperature, top P, and top K. These parameters all adjust the randomness of responses by controlling how the output tokens are selected. When you send a prompt to a model, it produces an array of probabilities over the words that could come next. And from this array, you need some strategy to decide what it should return. A simple strategy might be to select the most likely word at every timestep. But this method can result in uninteresting and sometimes repetitive answers. On the contrary, if you randomly sample over the distribution returned by the model, you might get some unlikely responses. By controlling the degree of randomness, you can get more unexpected and, some might say, creative responses. Back to the model parameters, temperature is a number used to tune the degree of randomness. Low temperature: Means to narrow the range of possible words to those that have high possibilities and are more typical. In this case, those are flowers and the other words that are located at the beginning of the list. This setting is generally better for tasks like questions and answers, and summarization, where you expect a more "typical" answer with less variability. High temperature: It means to extend the range of possible words to include those that have low possibility and are more unusual. In this case, those are bugs and other words that are located at the end of the list. This setting is good if you want to generate more “creative” or unexpected content like an advertisement slogan. In addition to adjusting the temperature, top K lets the model randomly return a word from the top K number of words in terms of possibility. For example, top 2 means you get a random word from the top 2 possible words including flowers and trees. This approach allows the other high-scoring word a chance of being selected. However, if the probability distribution of the words is highly skewed and you have one word that is very likely and everything else is very unlikely, this approach can result in some strange responses. The difficulty of selecting the best top K value leads to another popular approach that dynamically sets the size of the shortlist of words. Top P allows the model to return a random word from the smallest subset with the sum of the likelihoods that exceeds or equals to P. For instance, P of 0.75 means you sample from a set of words that have a cumulative probability greater than 0.75. In this case, it includes three words: flowers, trees, and herbs. This way, the size of the set of words can dynamically increase and decrease according to the probability distribution of the next word on the list. In sum, Vertex AI Studio provides a few model parameters for you to play with such as the model type, temperature, top K, and top P. Note that you are not required to adjust them constantly, especially top K and top P.

### Video - [Model tuning](https://www.cloudskillsboost.google/course_templates/593/video/508341)

- [YouTube: Model tuning](https://www.youtube.com/watch?v=4MTXlToSz5Q)

Having explored prompt design, a fundamental aspect of interacting with gen AI models, let's progress to a more advanced topic: model tuning. You explore different types of tuning and walk through a demo tuning a model with Vertex AI Studio. If you’ve been prototyping with the gen AI models like an LLM, you might wonder if there are ways to enhance the quality of responses and tailor them to specific domains, beyond just prompt design. Here comes model tuning. Let’s explore the different tuning methods, and how to use Vertex AI Studio to launch a tuning job. You have different choices to customize and tune a gen AI model, from less technical like prompt design, to more technical, like distilling. Let’s explore them one by one. You are already familiar with prompt design, which lets you tune a gen AI model using natural language without any ML background. The prompt is a request to a model for a desired outcome. To enhance the model's performance, you can provide context and examples to guide its responses. Prompt design does not alter the parameters of the pre-trained model. Instead, it improves the model's ability to respond appropriately by teaching it how to react. One benefit of prompt design is that it enables rapid experimentation and customization. Another benefit is that it doesn't require specialized machine learning knowledge or coding skills, making it accessible to a wider range of users. However, producing prompts can be tricky. Small changes in wording or word order… …can affect the model results in ways that aren’t totally predictable. And you can’t really fit all that many examples into a prompt. Even when you do discover a good prompt for your use case, you might notice the quality of model responses isn’t totally consistent. One way to address the issues is to tune the models using your own data. However, fine-tuning the entire model can be impractical due to the high computational resources, cost, and time required. As the name "large" suggests, LLMs have a vast number of parameters, making it computationally demanding to update every weight. Instead, parameter-efficient tuning can be employed. This involves making smaller changes to the model, such as training a subset of parameters or adding additional layers and embeddings. This approach has gained significant attention in the research community, as it aims to reduce the challenges of fine-tuning large language models by only training a subset of parameters. For example, you can have: Adapter tuning, which is supervised tuning. It lets you use as few as one hundred examples to improve model performance. Reinforcement tuning, which is unsupervised reinforcement learning with human feedback. Distillation, a more technical tuning technique, enables training smaller task-specific models with less training data, and lower serving costs and latency than the original model. This technique is exclusive to Google Cloud. Through Vertex AI, you can access the newest techniques from Google Research available with distilling step-by-step. This technology transfers knowledge from a larger model to a smaller model to optimize performance, latency, and cost. The rationale is to use a larger teacher model that trains smaller student models to perform specific tasks better and with improved reasoning capabilities. The training and distilling process uses labeled examples and rationales generated by the teacher model to fine-tune the student model. Rationales are like asking the model to explain why examples are labeled the way they are. Similar to how you learn better by understanding the reason behind an answer, this type of "teaching" makes the student model more accurate and robust. Now, let's move to Vertex AI Studio and see how to start a tuning job. Please note the UI may change as the product progresses. From the Language section of Vertex AI Studio Tune and Distill, select Create tuned model. For model details, you can choose from either Supervised tuning (which is the adapter tuning as mentioned earlier) or Unsupervised tuning (which is reinforcement tuning). Give the tuned model a name, choose the base model, the region, and the output directory. Parameter-efficient tuning is ideally suited for scenarios where you have "modest" amounts of training data. The number of examples can be as low as ten. However, it’s recommended to have one hundred training examples for better performance. Tuning dataset is where you specify the location of your training dataset. Please note it needs to be on Cloud Storage. Your training data should be structured as a supervised training dataset in a JSONL file. Each record or row contains a pair of text data: the input text, which is the prompt, and the output text, which is the expected response from the model. This structure allows the model to learn and adapt to your desired behavior. You can then start the tuning job and monitor the status on the Google Cloud console. When the tuning job completes, you see the tuned model in the Vertex AI Model Registry, and you can deploy it to an endpoint for serving or further test it in Vertex AI Studio.

### Video - [Model Garden](https://www.cloudskillsboost.google/course_templates/593/video/508342)

- [YouTube: Model Garden](https://www.youtube.com/watch?v=H-wOioGIxR8)

In addition to Vertex AI Studio, Model Garden with Vertex AI offers access to a wide range of gen AI models, not limited to those developed by Google. Model Garden is like a model library, where you can search, discover, and interact with Google’s, third-parties’, and open-source gen AI models. When you want to build, train, and fine-tune gen AI models in a comprehensive environment supported by both UI and coding, Vertex AI Studio can be a good start point. However, if you want to quickly find and apply the right model to solve a problem, Model Garden is a good choice. Model Garden offers a model card for each model, encapsulating critical details such as overview, use cases, and relevant documentation. Furthermore, it seamlessly integrates with Vertex AI Studio, allowing you to initiate project development through a user-friendly interface. Additionally, it provides access to sample code and facilitates code development via notebooks. You can find three major categories of models: foundation models, task-specific solutions, and fine-tunable or open-source models. Foundation models are pretrained, multitask, large models that can be tuned or customized for specific tasks by using Vertex AI Studio, Vertex AI APIs and SDKs. Some of these models are Gemini for multimodal processing, Embeddings for creating text and multimodal embeddings, Imagen for image, Chirp for speech, and Codey for code generation. Recall that you learned the APIs for these models in the previous lesson when you explored pre-trained APIs. Task-specific solutions are pre-trained models which are optimized to solve a specific problem. These include some tasks you practiced in the previous section by using Natural Language APIs, like entity analysis, sentiment analysis, and syntax analysis. More interesting tasks like object detector and text translation are also task-specific solutions. And finally, there are fine-tunable models. These are mostly open-source models that you can fine-tune by using a custom notebook or pipeline. To find the model that best suits your requirements, you can also use three filters in your search. They are: Modalities, such as language, vision, and speech. Tasks like generation, classification, and detection. And features such as pipeline, notebook, and one-click deployment support. With all these models to choose from, how can you start your workflow? Model Garden lets you: Use Google's foundation models directly through the Google Cloud console with Vertex AI Studio or through code with pre-built APIs. Tune models in Vertex AI Studio. Deploy models. Customize and use open-source models. Let's walk through a use case of using Model Garden. Assume you are interested in computer vision. From the Model Garden page, let’s filter for vision-related models. Next, in the Tasks section, select Detection. Great. The search result shows there's an Owl-Vision Transformer model, an open-source model. It says it's a zero-shot, text-conditioned object detection model that can query an image with one or multiple text queries. Let's dive into this. Here, you see the model card, where you can see more details. As a data scientist, you might want to try using this model, so you click on Open Notebook. This opens up a Colab notebook for the Owl-ViT model. This notebook in particular shows how you can deploy the model to an endpoint on Vertex AI and then send an image to the endpoint to get a prediction, which is a text caption describing what's in the image. As this example illustrates, the flow from finding a model to deploying and using it becomes super easy with Model Garden.

### Video - [AI solutions](https://www.cloudskillsboost.google/course_templates/593/video/508343)

- [YouTube: AI solutions](https://www.youtube.com/watch?v=MjTq5QLjo48)

You learned about Vertex AI Studio and Model Garden, the tools provided by Vertex AI to access gen AI models and develop gen AI projects. How is gen AI embedded in AI solutions? You’ll explore more in this lesson. AI solutions include both vertical and horizontal solutions. Vertical or industry solutions refer to solutions that solve specific problems within certain industries. Examples include: Healthcare Data Engine, which generates healthcare insights and provides services across patients, doctors, and hospitals. And Vertex AI Search for Retail, which enables retailers to provide Google-quality search and recommendations on their own digital property, which can help increase conversion rate and reduce search abandonment. Horizontal solutions, in contrast, usually refer to solutions that solve similar problems across different industries. For instance, Contact Center AI, or CCAI, aims to improve customer service in contact centers through the use of artificial intelligence. It can help automate simple interactions, assist human agents, and unlock caller insights. Document AI uses computer vision, optical character recognition, and natural language processing to create pretrained models that can extract information from documents. This increases the speed and accuracy of document processing, which can help organizations make better decisions faster and reduce costs. Let’s take CCAI as an example, exploring its main features and learning how gen AI supercharges those features. Contact Center Artificial Intelligence (CCAI) is Google’s solution to apply AI in contact centers. The goal is to use AI to increase customer satisfaction and operational efficiency, while requiring minimum AI expertise. At the center of Contact Center AI is the conversational core, which empowers three major components: virtual agent, Agent Assist, and Insights. Let’s briefly look at how Contact Center AI works. When a customer contacts CCAI, the system automatically determines whether the request is routine or complex. Routine requests are passed to a virtual agent, a large-language-model-powered bot that can converse naturally with customers. The virtual agent may then process the request and pass it to backend fulfillment. Complex requests are sent to Agent Assist, an AI assistant that helps human agents summarize the problem, gather information, provide solutions, and generate insights. Relevant information and insights can also be saved to the knowledge base throughout the process. Now let’s look at how generative AI is used to transform the CCAI platform. Today, the Contact Center AI platform is an end-to-end, AI-first call center as a service (CCaaS) solution that is integrated with a set of prebuilt tools and features including Virtual Agent, Agent Assist, and Insights. The platform will soon be upgraded with natural language understanding call and chat, as well as ML-driven routing. This means that the platform will be able to understand what customers are saying and direct them to the appropriate person or department. It will also be able to learn from the interactions and improve its routing over time. Today, a CCAI virtual agent provides 24/7 customer self-service. It provides a conversational voice and chat bot that a customer can get help from at any time using natural language. It also integrates the function to hand off calls to live agents. Through generative AI, the bots will be faster and easier to use. They will also be able to handle a wider range of customer interactions, answer customer questions in a more comprehensive and accurate way, and engage in natural conversations with customers. Today, Agent Assist empowers live agents with step-by-step help during calls and chats. It also provides agents an automated summary of every interaction afterwards. In the near future, Agent Assist will be enhanced with a generative AI feature that can coach live agents on demand. CCAI insights currently use natural language processing to analyze conversations such as sentiment analysis, entity identification, and topic detection. These insights can be used to improve the performance of virtual and live agents. They can also be used to create training materials for agents and identify areas where customer service can be improved. Soon, Insights will generate FAQs automatically to improve customer experience with generative AI capabilities. You can learn more about Google Cloud’s growing list of AI solutions and their embedded generative AI capabilities at cloud.google.com/solutions/ai. With the rapid development of AI and machine learning, you might hear exciting news every day. What do you think about the future of AI and machine learning? Here are a few of our expectations: The transition from data to AI is inevitable. Data is the fuel that powers AI, and AI extracts insights and possibilities from data. As AI becomes increasingly powerful and capable, it will learn and adapt in ways that data alone cannot. As a result, AI is becoming more and more essential for businesses that want to stay ahead of the competition. Generative AI becomes more important. It will be used to produce content and improve productivity, which will create new opportunities and offer great potential. And these opportunities will be more accessible to all as AI and ML development tools become easier to use and allow people without a technical background to benefit from their capabilities. What do you think, dear learners?

### Video - [Lab introduction](https://www.cloudskillsboost.google/course_templates/593/video/508344)

- [YouTube: Lab introduction](https://www.youtube.com/watch?v=NQ_XUsmjLYo)

With a grasp of the gen AI concepts, it’s time to play with Vertex AI Studio in a hands-on lab, where you: Analyze images with Gemini multimodal. Explore multimodal capabilities. Design prompts with free-form and structured mode. Generate conversations. By the end of this lab, you will be able to use the capabilities of Vertex AI Studio discussed in this course. Have fun exploring.

### Lab - [Get Started with Vertex AI Studio](https://www.cloudskillsboost.google/course_templates/593/labs/508345)

In this lab, you will learn how to use Vertex AI Studio to create prompts and conversations with Gemini's multimodal capabilities.

- [ ] [Get Started with Vertex AI Studio](../labs/Get-Started-with-Vertex-AI-Studio.md)

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/593/video/508346)

- [YouTube: Summary](https://www.youtube.com/watch?v=aFXvwupVi5o)

This concludes the last module on generative AI. Let’s do a quick recap. You began by understanding how gen AI works. Through extensive training on vast amounts of data, foundation models like Large Language Models (LLMs) are built. These models are capable of tackling general problems or can be fine-tuned to address specific issues. You then walked through the gen AI workflow on Google Cloud. After that, you focused on two generative AI development tools on Vertex AI, Vertex AI Studio, which is the interface between developers and backend gen AI models. It helps you access foundational models like Gemini multimodal, design prompts to achieve tasks, and tune gen AI models to solve your business problems. and Model Garden, which is a model library, helps you search, discover, and interact with Google and open-source gen AI models. You then explored vertical and horizontal AI solutions and their embedded generative AI capabilities. You concluded with a hands-on lab, where you used Vertex AI Studio to create prompts and conversations. We hope you enjoyed this module and feel motivated to use generative AI in your projects.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/593/quizzes/508347)

#### Quiz 1.

> [!important]
> **You run a call center that handles customer questions from multiple channels, such as email, phone calls, and chat. You want to improve customer satisfaction and agent efficiency by using AI to automate routine requests, help agents with complex tasks and discover insights. Which AI solution on Google Cloud should you choose?**
>
> - [ ] Healthcare AI
> - [ ] Document AI
> - [ ] Contact Center AI
> - [ ] Discovery AI

#### Quiz 2.

> [!important]
> **How does generative AI generate new content?**
>
> - [ ] It's programmed based on predetermined algorithms that cannot be altered.
> - [ ] It's a random process.
> - [ ] It learns from a massive amount of existing content and can then be used to solve general problems or be further tuned to solve specific problems.
> - [ ] The training leads to a foundation model that cannot be further tuned with a new dataset.

#### Quiz 3.

> [!important]
> **Which of the following is a type of prompt that allows a large language model to perform a task with only a small number of  examples?**
>
> - [ ] Few-shot prompt
> - [ ] Zero-shot prompt
> - [ ] Unsupervised prompt
> - [ ] One-shot prompt

#### Quiz 4.

> [!important]
> **What are the two categories of AI solutions provided by Google Cloud?**
>
> - [ ] Prebuilt solutions and custom solutions
> - [ ] Vertex AI and generative AI
> - [ ] Contact Center AI and Document AI
> - [ ] Vertical solutions, which focus on specific industries, and horizontal solutions, which solve problems across industries

#### Quiz 5.

> [!important]
> **Which of the following is the best way to generate more creative or unexpected content by adjusting the model parameters in Generative AI Studio?**
>
> - [ ] Set the top K to 1.
> - [ ] Set the temperature to a low value.
> - [ ] Set the top P to 25%.
> - [ ] Set the temperature to a high value.

#### Quiz 6.

> [!important]
> **What is Vertex AI Studio?**
>
> - [ ] A machine learning model that is trained on text only.
> - [ ] A tool that lets you quickly test and customize generative AI models so you can leverage their capabilities in your applications.
> - [ ] A technology that lets you code programming languages without learning them.
> - [ ] A type of artificial intelligence that writes emails for you.

#### Quiz 7.

> [!important]
> **What is a prompt?**
>
> - [ ] A prompt is used to evaluate a generative AI model.
> - [ ] A prompt is used to fine tune a large language model.
> - [ ] A prompt is used to explain how a large language model generates text.
> - [ ] A prompt is the natural language request or instruction to guide a model to generate a desired output.

### Document - [Reading](https://www.cloudskillsboost.google/course_templates/593/documents/508348)

## Summary

This module provides a summary of the entire course by covering the most important concepts, tools, technologies, and products.

### Video - [Course summary](https://www.cloudskillsboost.google/course_templates/593/video/508349)

- [YouTube: Course summary](https://www.youtube.com/watch?v=WsXPMFYUCgk)

Congratulations on completing the course, Introduction to AI and Machine Learning on Google Cloud! Regardless of your role–an AI developer, data scientist, ML engineer, or simply a passionate AI and machine learning enthusiast–we trust that this course has provided you with valuable insights to enhance your career. Before the final review, let's take a moment to reflect on what you've learned so far. Can you come up with three to five keywords and group them together in a way that makes sense to you? Throughout this course, we aim to help you navigate a comprehensive toolbox provided by Google Cloud, so you can effectively utilize these tools and resources to build an AI/ML project. Let’s walk through the main concepts. You started from AI foundations, where you focused on Cloud essentials and data tools. You began with Google Cloud infrastructure. Of the three layers, you explored the middle and top layers. On the middle layer sit compute and storage. Google Cloud decouples compute and storage so they can scale independently based on need. And on the top layer sit data and AI products, which enable you to perform tasks to support the data-to-AI journey, from data ingestion, storage and analytics, to AI and machine learning. After that, you advanced to the fundamental ML concepts including the categories of ML models. Specifically, you learned about supervised versus unsupervised learning. By grasping these concepts, you can make an informed decision when selecting an ML model. You then learned the steps to build an ML model by using BigQuery, Google’s widely-used data warehouse. Finally, you had a hands-on lab where you applied these steps to build your own ML model with SQL commands. In the second module, you advanced to AI development. Specifically, you learned about the options available to build an ML model. You compared these options, from a ready-made approach, to low- and no-code, to do-it-yourself. You started with pre-trained APIs, which are ready-made solutions using pre-trained machine learning models without the need for any training data. You were then introduced to Vertex AI, Google’s unified platform to build a machine learning project from end to end. You learned about AutoML, which is a low- or no-code tool on Vertex AI and lets you build the ML model with your own data through the Google Cloud console. You also explored custom training, which is a code-based solution on Vertex AI. It allows you to control the working environment and automate the ML workflow by using programming tools that you’re familiar with, such as Python, Tensorflow, and notebooks. Finally, you practiced with a hands-on lab using the Natural Language API to identify subjects and analyze sentiment in text. With a grasp of the various options for developing an ML project, you are now ready to build your own ML model. In the third module, you walked through the ML workflow and explored how to create an automated pipeline. You began with the three main stages of the machine learning workflow with the help of the restaurant analogy. In stage one, data preparation, you uploaded data and applied feature engineering. This translates to gathering your ingredients and then prepping them in the kitchen. In stage two, model development, the model was trained and evaluated. This is where you experiment with the recipes and taste the meal to ensure that it turned out as expected. And in the final stage, model serving, the model was deployed and monitored. This translates to serving the meal to customers and adjusting the menu as more people tried and reviewed the dish. There are two ways to build a machine learning model from end to end with Vertex AI. One is through a user interface, like you practiced in the AutoML lab. The other is with code, which you were shown using prebuilt SDKs with Vertex AI Pipelines. The latter helps you automate the ML pipeline to achieve continuous integration, training, and delivery. In the end, you completed a lab using AutoML on Vertex AI, where you built an ML model to predict loan risk through the Google Cloud console. In the final module, you were introduced to the most recent development in AI: generative AI. You explored the gen AI development tools and the integration of the gen AI capabilities into AI solutions. You began by understanding how gen AI works. Through extensive training on vast amounts of data, foundation models like Large Language Models (LLMs) are built. These models are capable of tackling general problems or can be fine-tuned to address specific issues. You then walked through the gen AI workflow on Google Cloud. After that, you focused on two generative AI development tools on Vertex AI, Vertex AI Studio, which is the interface between developers and backend gen AI models. It helps you access foundational models like Gemini multimodal, design prompts to achieve tasks, and tune gen AI models to solve your business problems. Model Garden, which is a model library, helps you search, discover, and interact with Google and open-source gen AI models. You then explored vertical and horizontal AI solutions and their embedded gen AI capabilities. You concluded with a hands-on lab, where you used Vertex AI Studio to create prompts and conversations. We hope that this course is the start of your journey into AI and machine learning. Remember what we suggested at the beginning of the course: apply what you’ve learned to your own work. This is the best way to develop your skills as an AI practitioner. For more training with machine learning and AI, please explore the options available at cloud.google.com/training/machinelearning-ai. If you are interested in validating your expertise and showcasing your ability to transform businesses with Google Cloud technology, you may want to consider pursuing a Google Cloud certification. You can learn more about Google Cloud’s certification offerings at: cloud.google.com/certifications. Thanks for completing this course. We’ll see you next time!

### Document - [Reading](https://www.cloudskillsboost.google/course_templates/593/documents/508350)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
