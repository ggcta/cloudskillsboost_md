---
id: 1192
name: 'GDC Platform Introduction'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1192
date_published: 2025-01-24
topics:
  - Google Cloud Services
  - IAM
  - API
---

# [GDC Platform Introduction](https://www.cloudskillsboost.google/course_templates/1192)

**Description:**

This course provides an introduction to the GDC platform—which enables you to host, control, and manage infrastructure and services directly on your premises.
GDC air-gapped is one component of Google Distributed Cloud offering which aligns to Google's digital sovereignty vision. It supports public-sector customers and commercial entities that have strict data residency, security or privacy requirements.


**Objectives:**

* Use GDC console, gdcloud CLI, or APIs to perform GDC practitioner persona responsibilities and tasks.
* Administer GDC IAM to control access to resources.
* Create and manage VMs.

## Course overview

This module introduces the audience, prerequisites, and agenda for the learning pathway.

### Video - [GDC Air-Gapped Practitioner Fundamentals overview](https://www.cloudskillsboost.google/course_templates/1192/video/521931)

* [YouTube: GDC Air-Gapped Practitioner Fundamentals overview](https://www.youtube.com/watch?v=Yy98wRTKghU)

SPEAKER: Welcome to the Google Distributed Cloud, or GDC air-gapped practitioner fundamentals series of courses. Within each of the three courses that make up this series, you'll focus on a specific aspect of GDC practitioner fundamentals. The first course provides an introduction to the GDC platform, which enables you to host, control, and manage infrastructure and services directly on your premises. GDC air-gapped is one component of Google Distributed Cloud offering, which aligns to Google's digital sovereignty vision and supports public sector customers and commercial entities that have strict data residency, security, or privacy requirements. The second course examines service resources or workload components that exist in projects. You'll learn about Kubernetes in GDC, Artifact Registry, GDC object storage, Database Service, networking, and key management and security. The third course explores advanced services such as machine learning and operational topics such as application deployment, monitoring and troubleshooting. In addition, we'll introduce GDC software upgrades, logging, billing, and cost monitoring. The intended target audience of this course series consists of various cloud operations professionals and system administrators, including DevOps engineers, application developers, cloud engineers, and database engineers. Additionally, this course is designed for solution architects planning to deploy applications and create application environments targeted for the GDC platform. To benefit fully from taking this course, you should have these prerequisite skills and knowledge-- an understanding of basic networking, familiarity with Linux, familiarity with identity and access management, familiarity with virtual machines, familiarity with Kubernetes, familiarity with databases, familiarity with Cloud APIs, and experience with application development. Along with the prerequisite skills and knowledge, it may help you to be familiar with the content from some Google Cloud courses, which you can find in the Course Resources. However, it is important to note that GDC differs in several significant ways from Google Cloud. Throughout this series of courses, you'll learn about GDC in context by exploring how Cymbal Federal, a fictional organization, accomplishes its goals of deploying three applications to the air-gapped platform. This case study attempts to mirror an enterprise level scenario, and aims to help you understand how the different components of the platform fit together and what their capabilities are.

## Google Distributed Cloud (GDC) air-gapped platform overview

Throughout the five lessons of this module, you'll learn about how GDC can be deployed and scaled to meet customer needs and discover Cymbal Federal, the use case for this course.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1192/video/521932)

* [YouTube: Module overview](https://www.youtube.com/watch?v=5UUihWiwFRI)

SPEAKER: Welcome to our Google Distributed Cloud GDC air-gapped platform overview. In this module, you'll learn to identify GDC deployment models and the services available with GDC, and locate GDC documentation that supports practitioners. GDC air-gapped is a component of the Google Distributed Cloud offering, which aligns to Google's digital sovereignty vision and supports public sector customers and commercial entities that have strict data residency, security, or privacy requirements. Throughout the five lessons of this module, you'll learn about how GDC can be deployed and scaled to meet customer needs and be introduced to Cymbal Federal, the use case for this course.

### Video - [What is GDC](https://www.cloudskillsboost.google/course_templates/1192/video/521933)

* [YouTube: What is GDC](https://www.youtube.com/watch?v=WQPZM9HoBuM)

SPEAKER: Let's get started with the Essentials of GDC. So what is GDC? It is a fully air-gapped solution that is designed for highly compliance-sensitive workloads. It is scalable and is equipped with fully-managed services. GDC provides API-driven, Cloud-native services and is based on Google Cloud technologies. And it can be fully managed by Google or a partner. GDC provides you with a safe and secure way to modernize an on-premises deployment, regardless of whether you do it yourself or choose to host through a designated trusted partner. GDC lets you apply data and security controls while still adopting best in class open source software solutions. It doesn't require connectivity to Google Cloud at any time to manage infrastructure, services, APIs, or tooling, and uses a local control plane provided by Anthos for operations. Let's take a moment to go over how GDC differs from Google Cloud. Google Cloud is a suite of cloud computing services that is offered by Google. It externalizes the technology and services that power all of Google's products around the world. To use Google Cloud Services, you need connectivity to Google infrastructure. GDC is a customer use, case-aligned version of Google's commercial cloud service. In other words, GDC is customized to a customer's use requirements. It's not just Google Cloud that's unplugged from the internet. Google did not take Google Cloud and modify it. GDC is a totally different code base. GDC is a platform built from the ground up to provide the necessary sovereignty requirements. If a feature is not turned on and built into the GDC platform, that feature will not be available. How does GDC support the highest level of digital sovereignty? The public cloud supports commercial users and data through core controls on access, encryption, and residency. For controlled and unclassified information, or CUI, data sovereignty controls need to be added. For highly sensitive data, operational sovereignty controls are also needed. With these types of data, external connections to the public internet may still be supported and desired. GDC lets you completely disconnect in order to support classified, secret, and top secret data. With GDC, organizations can implement workload by workload level policies and controls to meet specific requirements. This is because the meaning of sovereignty may differ between countries, between industries, and sometimes even between organizations within a given industry in a given country. As part of GDC management operations, the only incoming data are endpoint detection and response, or EDR, definitions and platform code updates. EDR is a security solution that continuously monitors end user devices to detect and respond to cyber threats like ransomware and malware. So there are only five pathways for updates into GDC. Let's learn more about each one. First, vulnerability detection signatures detect attempts to exploit system flaws or gain unauthorized access to systems. Second, EDR definition updates are allowed to support the detection of threats by the EDR tooling. Third, firewall indicators of compromise, or IOC, and signature updates are digital clues or forensics that determine whether an endpoint or network has been breached. Fourth, threat information that has been aggregated, transformed, analyzed, interpreted, or enriched to provide the necessary context for decision making processes is also updated in GDC. Finally, code updates can be made to GDC core platform binaries. Another characteristic of GDC is that the platform requires low management. It is autonomous. In other words, GDC does not require day-to-day human intervention to provide standard operations and maintenance in support of customer workloads. GDC is self-contained and configured to customer requirements. GDC provides all infrastructure, services, APIs, and tooling through a locally managed control plane that uses the Kubernetes controller pattern to enforce the state of the system. All auxiliary services and ancillary support are connected to the operations suite infrastructure environment, including monitoring, billing, SecOps, and ticketing. Infrastructure operations provides the ticketing system, or ServiceNow, security and incident management, framework for backups and restores and installation of updates and patches to GDC. Now that you can answer the question, What is GDC? you'll learn about deployment models next.

### Video - [GDC deployment models](https://www.cloudskillsboost.google/course_templates/1192/video/521934)

* [YouTube: GDC deployment models](https://www.youtube.com/watch?v=IqquAyy7d3U)

SPEAKER: Let's explore how GDC is deployed in a customer environment. Deployment models are determined by who is managing the infrastructure, Google, a partner, or the customer. When Google provides infrastructure operations services, Google operates the SecOps and management team that runs the platform. When someone else, likely a vendor or integrator, provides the infrastructure operations services, that partner operates the SecOps and platform management team. The customer may also hire a managed service provider or manage the platform solely themselves. In this case, the customer, or MSP, will patch and maintain the GDC platform. In all cases, the customer is responsible for workloads themselves and the data on those workloads. But keep in mind that GDC deployments will vary by customer. And so too will the customer responsibilities. Regardless of the deployment model, GDC is deployed at customer-chosen, accredited, secure facilities, and the customer has physical control and sovereignty over the GDC platform. Lastly, GDC includes an integrated management platform. This means you can deploy GDC across multiple geographically dispersed locations. The integrated management platform is referred to as the Operations Suite Infrastructure, OI. The GDC's independent management component is not dependent on any other Google resources. The integrated management platform is used by infrastructure operations.

### Video - [GDC scalability](https://www.cloudskillsboost.google/course_templates/1192/video/521935)

* [YouTube: GDC scalability](https://www.youtube.com/watch?v=AD7MuuQfVCQ)

SPEAKER: Scalability for Cloud Infrastructure imparts flexibility and agility for cloud workloads, providing the ability to handle varying request loads. Let's explore how GDC offers scalability for customers. GDC is a multi-tenant system. GDC currently supports organizations as tenancy units. A GDC instance can host multiple organizations, each consisting of one or more projects. The concepts of organizations and projects in the GDC resource hierarchy derive from Google Cloud and carry similar meanings. However, there are no folders in GDC. Instead, you can use tags or K8's taints. An organization typically represents an administrative unit or a business function that shares the same resource pool and billing. GDC provides physical isolation between organizations using hardware level multi-tenancy features. Each organization consists of one or more projects within which services can be enabled and used. Similar to Google Cloud, GDC provides a variety of first party and third party services within each project, such as VM, container, database, and artificial intelligence and machine learning or AI and ML services. GDC hardware version 3.0 starts at four racks, three racks for GDC and one rack for infrastructure operations, and is scalable up to 30 racks. An expansion rack has five slots. You can mix CPU and GPU in a rack, but block and object storage require dedicated racks. The GDC platform supports workload mobility. Compute workloads such as a VM database or AI and ML can potentially be transferred from a GDC instance in a data center and manually transferred into a portable appliance and then fully deployed to an appliance without connectivity. The appliance can be reconnected to its GDC organization at a later time. Data can then be ingested and synchronized with GDC. Maritime or airborne platforms are possible use cases.

### Video - [GDC platform services](https://www.cloudskillsboost.google/course_templates/1192/video/521936)

* [YouTube: GDC platform services](https://www.youtube.com/watch?v=8PKQvZ1kPZs)

SPEAKER: GDC offers many different services to deploy your applications. As GDC practitioners, this is where you'll spend most of your time working with the platform. Let's go over the available services and features, such as virtual machine service, Kubernetes clusters, storage, Vertex AI, and third-party service integration options. GDC includes both first-party services based on Google Cloud offerings and third-party service integration. Let's start with the first-party services that are integrated within the platform. Some of them are foundational services. Others are built on top of the service platform. They include Vertex AI, which provides pretrained models for Optical Character Recognition, or OCR, translation, and speech to text; along with AI Workbench networking, including private network, firewalls, and NAT; options for block and object storage; Kubernetes cluster service with lifecycle management for running Kubernetes clusters and an artifact registry; Database Service with PostgreSQL and Oracle Database BYOL, or Bring Your Own License; security services, including key management, IAM, and audit logging; operations providing logging and monitoring tools; a marketplace with third-party Independent Software Vendors, or ISVs; and a virtual machine service that provides various machine types, images, and GPUs. In addition to the first-party services, GDC offers third-party integration centered around identity access management, observability, Domain Name System, or DNS, and Public Key Infrastructure, or PKI. These services operate under a shared responsibility model where Google infrastructure operations, the third-party provider, and the GDC platform user all have specific responsibilities for security and compliance. GDC federates identity to a customer-provided IDentity Provider, or IDP. In other words, GDC delegates access management to you, the customer, where you can integrate your own IDP or access management into the GDC platform. Authentication is token-based. Practitioners handle access management in GDC using Role-Based Access Control, or RBAC controls. These controls let you assign fine-grained access based on the principle of least privilege. Workload telemetry data is generated inside the GDC platform, and detailed data is available to customers. You can ingest the data display in your own visualization stack. DNS integration and PKI are pending features at the moment. GDC documentation is publicly available to support customers as you adopt the platform. GDC provides extensive documentation on operating, developing, administering, and troubleshooting accessible within the console. To access the documentation, navigate to the Home screen and click Documentation. Also, customers will receive a tarball of the documentation upon receipt. Documentation is also available on the web at this address.

### Video - [Cymbal Federal’s deployment](https://www.cloudskillsboost.google/course_templates/1192/video/521937)

* [YouTube: Cymbal Federal’s deployment](https://www.youtube.com/watch?v=usjCxw3Q834)

SPEAKER: Now, let's put what you've learned about GDC in context by examining the case study for Cymbal Federal, an organization that has chosen to deploy applications to the GDC platform. Cymbal Federal is a government entity aligned to support the broad mission objectives of an executive branch of government. Cymbal Federal teams focus on application development and DevOps. DevOps is an effort that leverages software development practices to automate IT operations in order to release software applications more often and efficiently. The teams will interface with a third party infrastructure operations team using a ticketing system. This ticketing system ensures that requests are tracked according to compliance requirements. Cymbal Federal's teams have recently scaled up on Google Cloud fundamentals and Kubernetes. They are familiar with Cloud capabilities. During recent capabilities reviews, Cymbal Federal's operations and applications have been designated as mission critical and their supported mission capabilities are growing. In addition, Cymbal Federal's teams are charged with architecting scalability of their applications during high and low usage periods. An organization-wide, multiyear digital transformation program is underway at Cymbal Federal. To build resiliency, Cymbal Federal wants to modernize a suite of applications so that they can ensure timely recovery in the event of an unplanned service interruption. As another outcome of the modernization effort, Cymbal Federal has awarded its existing third party data center management partner the operations portion of the program. This partner has a strong past performance, running confidential workloads and the demonstrated capability of managing modernized workloads in private and public clouds in support of other executive branches of government. Let's explore how Cymbal Federal navigates a modernization journey while adopting the GDC platform. Cymbal Federal has three major applications to deploy to the GDC platform-- Document Archive, Mission Logistics, and Mission Customer Relationship Management, or CRM. Document Archive is Cymbal Federal's most critical application. It provides audio transcription to text. It includes archival storage that aligns with government regulations. Document Archive uses a container architecture and requires high availability and disaster recovery planning. Mission Logistics provides inventory, ordering, and delivery scheduling of supplies to partner locations. This application currently runs on large footprint, CPU RAM, VMs, and is targeted for a replatform to VMs on the GDC platform. The Mission CRM application maintains vetted contractors providing services for Cymbal Federal. Cymbal Federal plans to re-engineer the application for Kubernetes service. Let's take a look at how each application could leverage the features and services of the GDC platform. Cymbal Federal's document archive is currently implemented as VMs at the app tier and uses PostgreSQL at the database tier. The application suite receives sensitive audio files and transcribes the files to text, then provides metadata indexing for cataloging and long-term storage in alignment with legislation. The current application team has tested and verified that the application can be containerized. The long-term archival storage of the audio files can be provided by the GDC object storage service. Current DevOps processes provide separation of duties for database management and application management. For this reason, the application and database tier will be deployed in different projects in GDC. The application and database have a different life cycle. Transient or 1-to-n application versions are deployed and tested against 1-to-n databases. The Mission Logistics application provides inventory, ordering, and delivery scheduling of supplies to partner locations. Mission Logistics runs computationally with memory intensive scheduling processes. In replatforming, the application Cymbal Federal needs to consider the sizing of the compute resources accordingly to ensure the smooth running of the capability with the GDC platform. GDC, by design, offers configurable VM sizing, backup, and recovery capabilities for managing enterprise VM workloads. Current DevOps processes provide separation of duties for database management and application management, so the application and database tier will be deployed in different projects. The CRM application maintains vetted contractors for Cymbal Federal. The application is three-tier built on Linux, VMs, Java and Spring Boot and has been identified and tested as a fit for containerization. Within the database, there are business sensitive data fields that are encrypted with a customer managed encryption key. Cymbal Federal will maintain this key in GDC key management service.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1192/video/521938)

* [YouTube: Module review](https://www.youtube.com/watch?v=VND0zvBepgY)

SPEAKER: Let's review the key points from this module. First, we discussed that the GDC platform is designed for entities with strict security and compliance needs. We introduced the deployment model concept, which is really focused on who is managing the infrastructure-- Google, a Google partner, or the end user. GDC has top industry hardware and is designed from the ground up with scalability features. As your workload compute needs change, the GDC platform can accommodate these changes. In addition, we reviewed in-built enterprise grade services such as VM, Kubernetes, and database services that are available for your workloads to use. Finally, through the lens of Cymbal Federal, an example government entity, we discussed the journey and objectives of deploying existing VM and container-based workloads on the GDC platform. We'll continue to discuss Cymbal Federal's applications as you learn more about GDC throughout this course.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1192/quizzes/521939)

#### Quiz 1.

> [!important]
> **Which is a core feature of the GDC air-gapped platform?**
>
> * [ ] It runs at Google Cloud regions.
> * [ ] It requires internet access.
> * [ ] It is deployed on customer-chosen premises.
> * [ ] It includes all Google Cloud services.

#### Quiz 2.

> [!important]
> **Which two core services are integrated within the GDC air-gapped platform?**
>
> * [ ] Cloud Run (frontend/backend services, batch jobs, queue processing workloads)
> * [ ] Database Service (PostgreSQL, Oracle Database (BYOL))
> * [ ] BigQuery (serverless, cost-effective enterprise data warehouse that works across clouds and scales with your data)
> * [ ] Vertex AI (Translate, Speech-to-Text, Workbench)
> * [ ] Cloud Bigtable (scale to billions of rows and thousands of columns, store terabytes or even petabytes of data)

#### Quiz 3.

> [!important]
> **Which two choices are regulatory challenges that the GDC air-gapped platform specifically addresses?**
>
> * [ ] Health Insurance Portability and Accountability Act (HIPAA) compliance regulations
> * [ ] Data residency regulations
> * [ ] Personal identifiable information (PII) compliance regulations
> * [ ] Data sovereignty regulations
> * [ ] Payment card industry (PCI) compliance regulations

## GDC air-gapped user personas

The responsibilities of the Infrastructure Operations team, the Platform Administrator, and the Application Operator are different and require a separation of duties. Based on their goals, they work with different teams and use tools specific to their tasks.
In this module's four lessons, you'll learn about each of these personas in more detail.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1192/video/521940)

* [YouTube: Module overview](https://www.youtube.com/watch?v=6920Esp_Yag)

SPEAKER: Welcome to this module on Google Distributed Cloud GDC airgapped User Personas. In this module, you'll explore the personas that the GDC platform provides, the tasks the personas perform, and the interactions across those personas. The responsibilities of the infrastructure operations team, the platform administrator, and the application operator are different and require a separation of duties. Based on their goals, they work with different teams and use tools specific to their tasks. In this module's four lessons, you'll learn about each of these personas in more detail.

### Video - [Overview of user roles personas](https://www.cloudskillsboost.google/course_templates/1192/video/521941)

* [YouTube: Overview of user roles personas](https://www.youtube.com/watch?v=6xVdchRjTEo)

SPEAKER: Let's begin with an overview of user roles, or personas, and how they interact with one another through the lens of our use case, Cymbal Federal. Cymbal Federal applications require the separation of duties, which is an industry and architecture best practice for workload operations. Remember that Cymbal Federal teams are composed of system administrators, database administrators, application developers, and DevOps engineers. These teams interact with third-party infrastructure operations, using a web-based ticketing system, to ensure proper request and platform change tracking. This separation of duties will be continued and implemented within the GDC platform. Let's explore this idea. From a separation of duties perspective, there are three separate personas for GDC, on the Operations side, the infrastructure operations team, and on the GDC Customer side, the platform administrators and application operators. Infrastructure operations manages the day-to-day operations of the system infrastructure but has no access to customer data. They create organizations, allocate resources, and manage the hardware. This role is the highest level administrator of the system and could be Google, a contracted third party, or the customer, depending on the nature of sovereignty restrictions. For example, for US military contracts, the US government requires that all personnel who work on such systems be both US citizens and have particular security clearances. Infrastructure Operations works with platform administrators. The platform administrator is a customer or practitioner role that manages resources and permissions for their organization. They interact with infrastructure operations, manage organization-level customer resources, create projects, and manage user clusters and resource quotas. This is the highest level of administrative privilege granted to the customer and is the only administrative level that can grant access to customer data. The Platform Administrator works with both infrastructure operations and the application operator. The Application Operator manages projects, user clusters and workloads, and VMs on GDC. They develop applications to deploy and execute and configure granular resources and permissions on the GDC platform. As such, this user holds the most accountability for ensuring that security, compliance, controls, and requirements for customer workloads are met. The application operator works with the platform administrator. Let's examine the functional areas of responsibility, the tools used, and the collaborative interaction across personas. The goals for infrastructure operations are to provide a reliable platform, resolve customer tenant problems quickly, and provision resources for tenant organizations. On the Platform Operator Team, the platform developer and platform admin want to ensure compliance and provision tools and clusters for application teams. On the Application Team, application developers and application operators build and manage applications and services to support their organization. They work with business and operations teams. Infrastructure Operations uses tools for hardware administration, logging and monitoring, and Kubernetes. The platform operator team uses Kubernetes, ID providers, config management, and logging and monitoring tools. The application team uses VMs and Vertex AI. Users follow different journeys depending on the tasks they need to perform in the system. For example, infrastructure operations creates an organization and then assigns the platform administrator to that organization. The platform administrator manages the organization, including setting up identity providers or IDPs, creating projects, setting permissions, and giving access to the projects. The application operator creates project-level resources and enables, runs, and deploys managed services.

### Video - [Infrastructure Operator](https://www.cloudskillsboost.google/course_templates/1192/video/521942)

* [YouTube: Infrastructure Operator](https://www.youtube.com/watch?v=U7XCX-6Bw84)

SPEAKER: In this lesson, you'll examine the responsibilities of infrastructure operations through the lens of Cymbal Federal. As Cymbal Federal adopts GDC, infrastructure operations works with the platform administrator and the security operations team at Cymbal Federal to gather the infrastructure and networking requirements. Infrastructure operations sizes the hardware and designs the network to ensure that Cymbal Federal can meet its organizational goals for application reliability. They install and configure the GDC instance in an approved data center in a location that ensures that Cymbal Federal can exercise compliant sovereignty controls over its data and applications. Infrastructure operations generates notifications and monitoring for the GDC platform to ensure it meets the Service Level Objectives, or SLOs, for availability and network latency. Additionally, the infrastructure operations' partner organization devises an appropriate platform update process to ensure robust security posture of the GDC platform application environment. In the case of Cymbal Federal, the partner adheres to the key elements of the infrastructure operations role. Infrastructure operations manages the hardware and software infrastructure for GDC tenant customers, such as Cymbal Federal. Infrastructure operations has the necessary access to administer the GDC hardware but not the data or customer applications. They are responsible for managing and maintaining the infrastructure, hardware, and security of the operational system. From an operations perspective, the infrastructure operation goals focus on the hardware and software of the infrastructure. Infrastructure operations ensures that hardware and software infrastructure performs as expected. They maintain an infrastructure that is in compliance with regulatory requirements. Infrastructure operations is not aware of the workloads that are run by the GDC application teams. They do not know the functional requirements of the workloads or the GDC services used by GDC application teams. Infrastructure operations tasks are centered around three categories. Beginning with infrastructure setup, infrastructure operations tasks involve the installation and configuration of GDC tenants and resource quota configuration. Infrastructure operations installs and configures the software to manage the cloud service, creates organizations, and installs admin clusters to provide customers tenancy access to the platform and sets or enforces resources for tenants. Monitoring and support involves proactive monitoring of the platform and the configuration of notifications and service-level objectives. Infrastructure operations proactively monitors the performance of platform hardware and software, sets up notifications and SLOs to act on poor performance, and resolves infrastructure issues or outages. They also import security updates and help their end users to troubleshoot problems. Infrastructure operations tasks for ongoing improvements involve platform updates and resource provisioning. Infrastructure operations makes platform updates available to tenants, provisions new resources for tenants, and tests and rolls out new versions of platform infrastructure.

### Video - [Platform Administrator](https://www.cloudskillsboost.google/course_templates/1192/video/521943)

* [YouTube: Platform Administrator](https://www.youtube.com/watch?v=b93h611tifU)

SPEAKER 1: In this lesson, you'll examine the responsibilities, goals, and requirements of the platform administrator. Platform administrators manage resources and policies at the organization level. They provision resources for application teams and interact with infrastructure operations to secure additional bulk resources, get support, plan for upgrades, and request specific configuration changes. Platform administrators manage the organization's central infrastructure and can create and delete clusters on demand. The Cymbal Federal Platform Administrator is responsible for ensuring appropriate IAM set-up and role assignments, and ensuring the resource hierarchy is designed to fit requirements. They implement least privilege and workload segmentation best practices. The platform administrator deploys application infrastructure resources. At Cymbal Federal, the current DevOps and cloud engineering teams perform duties within the platform administrator persona. Recall that Cymbal Federal needs to set up the resource hierarchy with different projects to accommodate the different application environments, including dev, staging, test, and production. They also need to use different IAM groups for database and application management to separate duties. For the application tier, the overall solution stack uses VMs, Kubernetes service, Database Service, GDC object storage, and KMS service. The platform administrator will need to create Kubernetes clusters for the document archive and mission CRM applications and set up VM service for the mission logistics application. In the database tier, Cymbal Federal will use the Managed Database Service. The Cymbal Federal Platform Administrator coordinates with the application operator to ensure the necessary services in Kubernetes clusters have been created. SPEAKER 2: Platform administrators need to ensure that applications and services comply with governance requirements and security policies by coordinating with infrastructure operations. They automate processes for developers, operators, and platform teams to improve productivity. Platform administrators ensure the infrastructure is performing as expected so that users have uninterrupted access to tools they need. They integrate the tools used across the org so that deploying, managing, scaling, and monitoring applications is a cohesive experience.

### Video - [Application Operator](https://www.cloudskillsboost.google/course_templates/1192/video/521944)

* [YouTube: Application Operator](https://www.youtube.com/watch?v=kXAoeu2wJnk)

SPEAKER: Finally, in this lesson, you'll examine the Application Operator Persona. GDC application operators are responsible for operating live deployments; monitoring the health of deployments; and rolling out updates; and managing workloads, applications, and projects. In many organizations, the application operator may be a member of the development team. The infrastructure for Cymbal Federal's applications is being deployed within the GDC platform. Cymbal Federal's application operators are deploying the document archive, mission CRM, and Mission Logistics applications on the GDC platform. They will focus on documenting the entire application ecosystem to understand all of the dependencies, deploying VMs for the Mission Logistics application, and designing the CI/CD build process for all applications. The application operator has multiple goals. They need to ensure application performance, enable safe, efficient updates, and assist with the application-developer experience to speed up the delivery of application changes. Application operators also need to understand the application-ecosystem dependencies. The application operators tasks are mainly centered around two areas-- deployment and monitoring. The application operator deploys apps and services to production. This involves granting permissions, rolling out new service instances, versions, bug fixes, and/or security patches, planning capacity and infrastructure needs, and creating alerting policies for the applications. The application operator also monitors the health of deployed applications and services, for example, SLO compliance and performance metrics. They respond to alerts and pages when SLOs or applications fail and perform root cause analysis during problem management. In addition, depending on your organization's application development practices, application operators may perform tasks, such as publishing APIs, and documentation for their app or service, and planning capacity and infrastructure. They may also participate in tasks that make apps easier to operate, such as defining SLOs and instrumenting the application.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1192/video/521945)

* [YouTube: Module review](https://www.youtube.com/watch?v=xbe5B--4kMc)

SPEAKER: In this module, we review the key management areas for the GDC platform, infrastructure, platform administration, and application workloads, and the associated personas, infrastructure operations, platform administrator, and application operator. Infrastructure operations manages and secures the hardware infrastructure. The platform administrator manages access controls and provisions global resources for application teams. The application operator deploys and manages application workloads within projects. The management of the strictly logical GDC components, entities created by the GDC APIs, its solution components, and user workloads, is performed by the platform administrator and the application operator personas. Additionally, to effectively manage access controls and upgrades within GDC, we review the coordination of efforts across personas. Infrastructure operations and the platform administrator coordinate to ensure patching and platform updates are scheduled and performed. The platform administrator and application operator work together to ensure roles, permissions, and resources are provisioned to the appropriate teams. In conclusion, GDC lays the foundation for zero-trust architecture by designating specific areas of focus for each persona group, thereby supporting the principle of least privilege.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1192/quizzes/521946)

#### Quiz 1.

> [!important]
> **What persona manages deployed workloads and live application deployments within the GDC air-gapped platform?**
>
> * [ ] Application operator
> * [ ] Change manager
> * [ ] Project manager
> * [ ] Infrastructure Operations

#### Quiz 2.

> [!important]
> **What persona maintains and configures the GDC air-gapped hardware?**
>
> * [ ] Infrastructure Operations
> * [ ] ITL CMDB owner
> * [ ] Application operator
> * [ ] Platform administrator

#### Quiz 3.

> [!important]
> **What are two tasks performed by the platform administrator persona within the GDC air-gapped platform?**
>
> * [ ] Developing application source code.
> * [ ] Enforcing GDC resource or usage quotas and role-based access control policies.
> * [ ] Creating GDC resources for application teams.
> * [ ] Procuring additional GDC hardware components.
> * [ ] Managing the organization's central hardware infrastructure.

#### Quiz 4.

> [!important]
> **What persona is the highest level granted to the customer and can grant access to customer data?**
>
> * [ ] ITL CMDB owner
> * [ ] Platform administrator
> * [ ] Application operator
> * [ ] Infrastructure Operations

## Interacting with Google Distributed Cloud (GDC) air-gapped

GDC provides a similar management experience to Google Cloud. You can interact with the GDC platform in three ways: through the GDC console, a web-based used interface; the command line; or programmatically. You'll explore the different ways to interact with GDC throughout the lessons of this module.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1192/video/521947)

* [YouTube: Module overview](https://www.youtube.com/watch?v=eiFpsUyy7Co)

SPEAKER: Welcome to interacting with Google Distributed Cloud, GDC, air-gapped. In this module, you'll learn through the perspective of different GDC personas to navigate the web console and Command Line Interface, CLI, to administer GDC resources. You'll also be able to identify and troubleshoot gdcloud CLI commands and learn about the GDC API interface. GDC provides a similar management experience to Google Cloud. You can interact with the GDC platform in three ways, through the web-based, the command line, or programmatically. The GDC console is a web-based user interface that provides a centralized view of your GDC resources. You can use the console to create and manage clusters, deploy and manage workloads, and monitor your GDC environment. The gdcloud CLI is a command line tool that provides access to the GDC APIs. You can use the gdcloud CLI to perform all of the tasks that you can do with the GDC console, as well as some additional tasks, such as managing Kubernetes' resources directly. The GDC APIs provide direct access to GDC functionality. You can use the GDC APIs to build and integrate your own work tools and applications with GDC. You'll explore the different ways to interact with GDC throughout the lessons of this module.

### Video - [GDC console overview](https://www.cloudskillsboost.google/course_templates/1192/video/521948)

* [YouTube: GDC console overview](https://www.youtube.com/watch?v=uWjD5PF26JI)

SPEAKER: This lesson introduces the GDC console. The GDC console has the same look, feel, and navigation of the Google Cloud console. It lets you manage the configuration and monitor services and workloads. You can access your workloads through the GDC console or the GD Cloud CLI. To sign in to the GDC console or cluster, open the URL in a new browser tab using the domain name that infrastructure operations provides. When opening any URL for the first time, GDC redirects you to your IDP Login page if infrastructure operations configured the page. The UI session inactivity timeout is 15 minutes. The GDC console menu functions similarly to the one in Google Cloud. And you can find GDC services categorized under contextual menus. This lets you quickly navigate the menu for specific operational needs. The Identity and Access context menus are found under Identity and Access in the Navigation menu. These screens let you manage roles and permissions in GDC. The GDC console provides a view of organization IAM roles. The Access page displays a summary of configured principals, users and groups, their respective assigned roles, and the identity provider used. You can also find project IAM and the roles assigned on the Access page, providing an understanding of the project-level scoping. Projects have a name and a project ID. You can use the project ID to browse projects within the GDC console. The Service Identities page lists project's scope service identities or service accounts. You can manage the identities within the console and by using the GDC API or GD Cloud CLI. On the Monitoring page, you use centralized navigation to access monitoring, logging, audit logging, and alerting. On the Clusters page, you can fully manage, create, inspect, and update clusters. On the Instances page, under VM Instances, you can fully manage, create, update, and inspect the lifecycle status of virtual machines. On the Object Storage page, you can fully manage, create, update, and inspect objects and storage buckets on the Database Service page, under Database Clusters, you can fully manage, create, update, and inspect the lifecycle status of databases. Vertex AI provides a pretrained APIs view, which contains a list of pretrained APIs statuses including endpoints used for inference. This list indicates services that are enabled. The Vertex AI Workbench page provides navigation to the project workbenches and workbench logging and monitoring dashboards powered by Grafana. The Virtual Network page shows internal and external IP address ranges. You will explore the internal and external features later in the networking module. Note that networking is not configured by practitioners. This is read only. The GDC Capacity Management dashboard lists links to storage and compute usage dashboards. The Firewall page displays project user-created firewall rules and system-defined rules. And finally, the Marketplace page lets you browse and enable services, such as Dataproc Container for Spark, and Elastic Cloud on Kubernetes.

### Video - [gdcloud CLI setup and configuration](https://www.cloudskillsboost.google/course_templates/1192/video/521949)

* [YouTube: gdcloud CLI setup and configuration](https://www.youtube.com/watch?v=_PIAtQrvT9U)

SPEAKER: In this lesson, you'll learn about the gdcloud CLI setup and configuration. The gdcloud CLI has the same feel as the Google Cloud command line tool in its design flow and styling, navigation structure, and its help, accessibility, and output. It uses the same base configurations such as projects. So when should you use the gdcloud CLI instead of the console? The gdcloud CLI is a good choice when you need to perform repetitive tasks, automate platform management tasks, or test and integrate GDC resource configuration. The requirements for gdcloud CLI setup include a Linux-based system or Windows with Windows Subsystem for Linux, a GDC account, access to the GDC Welcome page, and a GDC organization login URL. To configure the gdcloud CLI setup, do the following. First, on Linux, open a browser. Next, browse to the GDC organization Welcome page. Then, sign in with your credentials. Finally, at the bottom of the page, in the Connect with CLI section, click Download CLI Bundle. You can extract the CLI package to a folder in the directory. Note that it's recommended to add the gdcloud CLI directory to the path environment variable. Otherwise, you'll have to specify the entire directory structure with each command. After adding the CLI installation directory to the path, you can invoke the command without entering its path location. Gdcloud commands work without the full path being entered. Use these commands to add the bin directory to your path and validate the path changes. To initialize gdcloud for your organization, follow these steps. First, configure your organization project type, gdcloud init. Next, you must provide two properties to complete the initialization, the URL to your current GDC organization's console and the name of your current GDC project. You can view these configuration properties using the gdcloud config list command. Third, find the configuration stored in your user config directory. Note that the config directory must be write-enabled. Let's review a configuration example. First, enter gdcloud init to get to the welcome. Next, enter the organization console URL, and enter the name of the GDC project. If all was entered correctly, you'll receive the message that your gdcloud is configured and ready to use. Remember, configurations are stored in your user config directory. The config directory must be write-enabled. The gdcloud CLI accommodates one or more configurations to assist in managing multiple GDC organizations or multiple GDC project workloads. A configuration is a set of key value pairs that define how the CLI connects to a GDC instance's control plane. Note that there is a default configuration if you don't specify one. The sample GDC configuration identifies the active status, other properties such as accessibility, and the project that is being managed. This command creates a configuration named cymbal-federal, and this sets the GDC console URL. You can use some common tasks to manage gdcloud configurations, for example, listing GDC configurations, activating a GDC configuration, viewing a configuration's properties. You can inspect your configuration using gdcloud config list, which is very similar to that of gdcloud's CLI. You can also upgrade components of gdcloud. To install updates, enter the following, gdcloud components update. This ensures that the latest version of all installed components is installed on the local workstation. The command lists all components it is about to update and asks for confirmation before proceeding. By default, this command will update all components to their latest version.

### Video - [gdcloud authentication](https://www.cloudskillsboost.google/course_templates/1192/video/521950)

* [YouTube: gdcloud authentication](https://www.youtube.com/watch?v=q-NRTg6eGcA)

SPEAKER: Let's discuss the gdcloud authentication command. Authentication is the first thing you do after configuration. After all, cloud is an authenticated and authorized orchestration of compute and storage resources. You can authenticate with and without a browser. To log in using a browser window, enter gdcloud auth login. A browser window will pop up asking for credentials. Logging in with no browser requires a secondary device. Enter the command gdcloud auth login --no-browser to be given another gdcloud command that must be executed on the secondary device later. Then run gdcloud auth login to log into the secondary device using a browser. After authenticating with the web page, enter the gdcloud command given on the first machine. You can also enable Kubernetes admin features. gdcloud lets you retrieve Kubernetes cluster credentials. After retrieval, you can connect to your specified cluster. Now, let's learn how to end your authentication session. gdcloud lets you end authenticated sessions and log off the GDC platform. To log off gdcloud and revoke the session, enter gdcloud auth revoke. This causes GDC to mark the session as ended, requiring a login to access resources again. This means you are required to re-authenticate to access the platform again.

### Video - [gdcloud CLI commands and examples](https://www.cloudskillsboost.google/course_templates/1192/video/521951)

* [YouTube: gdcloud CLI commands and examples](https://www.youtube.com/watch?v=OV1MtBSxYco)

SPEAKER: In this lesson, you'll learn about gdcloud CLI commands by working through some examples. There are many available commands to use in gdcloud. These commands are similar to that of regular Google Cloud command line tooling. Some common groups, commands, and flags might not be available because they may not be approved for use in your deployment. You can access them when they are approved. Gdcloud provides a help menu. You can access the help menu options by typing gdcloud --help. Gdcloud also provides context-aware help and hints for specified command when you type gdcloud, the command you want to learn about, --help. Depending on your need, use the basic CRUD, Create, Read/describe, Update, Delete, features to manage GDC projects. To get started, type gdcloud projects and your command. The create command lets you create a project. Describe displays information regarding a particular project. List identifies if the project is accessible by the current user. Update will update a particular project's attributes. And the delete command lets you delete a project. You can also manage gdcloud object storage with gdcloud. All the capabilities are very similar to that of S3, Google Cloud Storage, or gsutil tools. To manage GDC object storage configurations, type gdcloud storage and your command. Cat outputs the contents of one or more URLs to standard output, stdout. cp uploads, downloads, and copies objects. ls lists buckets and objects. mv moves or renames objects. Objects is the command group for GDC object storage objects. And finally, rm deletes objects and buckets.

### Video - [Troubleshooting common issues](https://www.cloudskillsboost.google/course_templates/1192/video/521952)

* [YouTube: Troubleshooting common issues](https://www.youtube.com/watch?v=EmXlT-uEdtI)

SPEAKER: Common troubleshooting tasks are centered around validating the configuration of an authentication setting for the gdcloud CLI. In this lesson, you'll learn about basic troubleshooting of the command line tool. gdcloud requires a proper certificate trust be established on the workstation. If the proper certificates are not installed on your workstation, the gdcloud command will not connect due to untrusted certificate use. The error shown here indicates the SSL certificate for the console.org-1 URL was issued by an untrusted certificate authority, one that is unknown to the HTTP client within the gdcloud tooling. For certificate authority to be trusted, its root certificate must be included in the list of trusted root certificates maintained by a web browser vendors and operating system providers. A trusted certificate authority verifies the identity of a website and helps users trust that they are communicating with the legitimate website. To check whether your gdcloud is configured with the correct settings to connect to the organization and project, use gdcloud init to configure, and use gdcloud config list to view the existing configuration. You can have multiple configurations for gdcloud. GDC uses the organization page for authentication and to generate an access token. To verify that the page is reachable, browse to the organization page in a web browser on your Linux-based workstation and try authenticating with the same credentials you use for gdcloud. Note that this assumes you have already been authenticated. To ensure you are using the correct version of gdcloud, download the current version from the web page or check your version using gdcloud version. Follow the steps provided by gdcloud components update to download the latest version.

### Video - [API interface](https://www.cloudskillsboost.google/course_templates/1192/video/521953)

* [YouTube: API interface](https://www.youtube.com/watch?v=ss5e3Lf-VJY)

SPEAKER: In addition to the web console and the gdcloud CLI, you can use APIs to interact with GDC. In this lesson, we'll focus on these APIs. GDC Application Programming Interfaces, or APIs are programmatic interfaces to the GDC platform services. There are two types of GDC APIs. those that are Kubernetes based and those that are not. Many GDC APIs are extensions to the open source Kubernetes API. Google builds the control plane APIs on top of Kubernetes, using the Kubernetes Resource Model, KRM. The control plane performs resource management for services such as creation, deletion, and updates. These APIs, like the Kubernetes API, are HTTP-based, RESTful APIs, accepting and returning JSON as the default or in Protobuf. The API endpoint is the relevant Kubernetes server. Other non-Kubernetes-based GDC APIs, such as the Vertex pre-trained AI APIs, have their own endpoints. In addition to supporting HTTP, some of these APIs may also be accessible by the gRPC or Open Source Remote Procedure Call Framework. The data plane is the relative API that is used on the resource, such as Vertex AI's Speech-to-Text. Depending on the origin of the data plane, the APIs can be REST or gRPC, if they are Google first party services. Specific services have these APIs and their own data plane APIs, which are XML, JSON, or gRPC-based. Let's consider when to use APIs. Both the gdcloud CLI tools and the GDC console leverage the GDC APIs. Google recommends that you use those to explore GDC, or to do one off operations without leveraging the direct GDC APIs. However, if you use automated or programmatic access to GDC, Google recommends that you use the GDC APIs directly. Most GDC APIs provide a JSON HTTP interface that you can call directly. The Kubernetes-based APIs use the Kubernetes client libraries. Some non-Kubernetes GDC APIs have a gRPC interface, which provides improved performance and usability. Google also provides client libraries for GDC APIs that aren't based on Kubernetes. This slide identifies APIs for various services in GDC. First party services such as Vertex AI follow Google Cloud standards. Platform APIs such as observability and monitoring Grafana use open source standards such as Loki. Third party APIs are unique based on the specific provider. Third party APIs are in the service roadmap for the future. GDC APIs accept requests using TLS encryption. GDC recommends that you use existing clients, especially those provided by Google to interact with GDC APIs. If you use one of the Kubernetes or GDC client libraries, the library handles in-transit encryption for you. If you've used your own HTTP or gRPC client, you must authenticate with GDC, which requires TLS. Creating your own API client is an arduous software engineering task that, if done incorrectly, might lead to unforeseen behaviors in your systems that are difficult to diagnose. The GDC management plane is built using Kubernetes, and because many of the GDC APIs are extensions to the Kubernetes API and rely on the KRM, an understanding of these concepts can help you use GDC APIs effectively. The Kubernetes API is fully declarative, and everything in the Kubernetes API is a resource that follows the KRM. API clients, both human and machine, act on those resources, often with Create, Read, Update, and Delete, CRUD operations. The Kubernetes database stores the resource and represents the state of the system. Kubernetes continuously watches those resources and reconciles the real state of the system with the desired state. For example, if you update a deployment resource to indicate that you want five replicas of your container instead of four, Kubernetes detects the change and the desired number of replicas and creates an additional container. For the core Kubernetes API, Kubernetes performs the reconciliation between desired and real states itself. The Kubernetes API extensions are custom resources that are not part of the core Kubernetes API. The custom software continuously watches and interacts with the Kubernetes API and performs the reconciliation. The kubctl Kubernetes CLI is the primary way to work directly with the Kubernetes API in any Kubernetes-based APIs. You can examine your current kubectl configuration to see the clusters you can access with the command kubctl config view. To access a cluster, you need the cluster location information and the credentials to access it. You must first authenticate using gdcloud and retrieve k8s credentials. You can directly access the REST API with an HTTP client like curl, wget, or browser in two ways. Rely on kubectl to handle the authentication by using it in proxy mode, or handle the authentication yourself, passing a token directly to the API server. The kubectl control proxy command runs kubectl in a mode where it acts as a reverse proxy. This command authenticates using the cluster credentials obtained using gdcloud. Running kubectl in proxy mode, uses the stored API server location and verifies the identity of the API server using a certificate.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1192/video/521954)

* [YouTube: Module review](https://www.youtube.com/watch?v=91-rx6sgZ-w)

SPEAKER: Let's review the key points from this module. In this module, we discussed the management interfaces for the GDC platform. Practitioners can interact with the GDC platform through the web-based UI, the console, the CLI, or programmatically using the GDC API. You explored how the web console enables comprehensive management of the GDC platform and provides context-aware menus for ease of navigation. You also learned how to configure the gdcloud command line interface tool, reviewed frequently used commands, and discussed basic troubleshooting techniques for the command line interface. For teams interested in automation, the GDC platform offers a Kubernetes-based custom resource model that can be programmatically orchestrated. Although the platform is designed for specific workload requirements, GDC provides robust tools and interfaces to effectively manage the platform and your workloads.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1192/quizzes/521955)

#### Quiz 1.

> [!important]
> **What command snippet ends authenticated sessions to the GDC air-gapped platform?**
>
> * [ ] gdcloud auth revoke
> * [ ] gdcloud init
> * [ ] gdcloud version
> * [ ] gdcloud components update

#### Quiz 2.

> [!important]
> **What is one way to access GDC APIs?**
>
> * [ ] GDC integrated development environment
> * [ ] Google Cloud software development kit (SDK)
> * [ ] gdcloud CLI tools
> * [ ] GDC software development kit (SDK)

#### Quiz 3.

> [!important]
> **What command snippet initializes the configuration to connect to the GDC air-gapped platform?**
>
> * [ ] gdcloud version
> * [ ] gdcloud init
> * [ ] gdcloud auth
> * [ ] gdcloud components update

#### Quiz 4.

> [!important]
> **You receive the following error "x509: certificate signed by unknown authority." What additional task should you do during gdcloud configuration to remedy the error?**
>
> * [ ] Enforce GDC resource or usage quotas and role-based access control policies.
> * [ ] Reboot the workstation.
> * [ ] Reinstall the gdcloud CLI tool.
> * [ ] Install the Certificate Authority (CA) certificate in the gdcloud workstation's trusted certificates store.

#### Quiz 5.

> [!important]
> **What command snippet starts an authenticated session to the GDC air-gapped platform?**
>
> * [ ] gdcloud components update
> * [ ] gdcloud init
> * [ ] gdcloud auth
> * [ ] gdcloud version

## IAM and organization policies

This module explains how Identity and Access Management (IAM) is implemented in GDC along with how organization policies can be used to control access to resources and deployment of resources.

You'll begin with an overview of IAM in the first lesson, then dive deeper into authentication and authorization. In the last lesson, you'll focus on organization policies.


### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1192/video/521956)

* [YouTube: Module overview](https://www.youtube.com/watch?v=UfpZ08H2rNY)

SPEAKER: Welcome to IAM and Organization Policies. IAM and organization policies are crucial components in managing security and access control. They work in tandem to ensure only authorized users and workloads have proper access to specific resources in the cloud. In this module, you'll learn about Google Distributed Cloud, GDC, air-gapped features, such as the resource hierarchy and policy inheritance that support identity management and fine-grained, role-attributed access control. You'll learn how GDC uses federated identities through Anthos Identity Service, AIS, to authenticate human users and explore best practices for GDC IAM. You'll also learn about using service identities and implementing RBAC and ABAC in GDC. Finally, you'll examine organization policies. This module explains how Identity and Access Management, IAM, is implemented in GDC, along with how organization policies can be used to control access to resources and deployment of resources. You'll begin with an overview of IAM in the first lesson, then dive deeper into authentication and authorization. In the last lesson, you'll focus on organization policies.

### Video - [IAM overview](https://www.cloudskillsboost.google/course_templates/1192/video/521957)

* [YouTube: IAM overview](https://www.youtube.com/watch?v=HWvLWfZatMw)

SPEAKER: Let's start this lesson with an example that uses Cymbal Federal and its GDC deployment to give context to various scenarios where you need to control access to resources in GDC. Cymbal Federal set up a resource hierarchy in GDC that lets them use IAM for role-based access control to specific organization and project-level resources. Unlike in Google Cloud resource hierarchies, GDC does not implement the concept of folders for aggregation of projects to apply RBAC. All resources in GDC are owned by the organization, not by a particular team. In their resource hierarchy design, Cymbal Federal has adopted best practices. These practices will be highlighted later in this module. Cymbal Federal has one organization in its GDC instance. You can attach organization policies here, for example, to govern allowable roles in resource usage. Cymbal Federal has separate GDC clusters, with one for each environment, production or prod, and development or dev. This is a recommended best practice. The Kubernetes clusters are global and live outside of projects but must be attached to one project or more. Cymbal Federal has also followed recommended practice by separating the project resources with one for each environment. Each application has a separate project for production and development. Within the projects, the service resources for each workload are deployed, including databases, storage, and Vertex AI Speech-to-Text. The GDC Platform Administrator persona has the ability to manage and configure this hierarchy and all resources within it. Now let's examine one component of Cymbal Federal's resource hierarchy which corresponds to the document archive application. Cymbal Federal set up functional groups that will be working in different areas of the application, and a platform administrator assigned the appropriate permissions to these groups. The functional groups exist within Cymbal Federal's identity store. In practice, this can be an LDAP server, Active Directory, or an external identity provider. For example, the DevOps group of application operators focuses on deployment of applications and cloud resources, so the platform administrator assigns a predefined role in GDC, Project Bucket Admin, with the necessary permissions to the group. Both Google Cloud and GDC IAM help you define who can do what on which resources. Before you learn about GDC in more detail, let's cover some of the key differences between GDC IAM and Google Cloud IAM at a high level. For highly compliant workloads, GDC provides the appropriate set of access privileges and capabilities. Both platforms have resource ownership. Google Cloud resources are owned by the creator, and GDC resources belong to the organization. Only Google Cloud has basic and custom roles. Google Cloud has basic roles with broad permissions. It also lets you assign custom roles with permissions that you can define on a case-by-case basis. GDC has no default, basic, or custom roles. All permissions must be granted using predefined roles. Google Cloud services require connectivity through either the internet or private connections. GDC does not require the internet. As a fully air-gapped and secure system, GDC does not connect through a public network. Google Cloud IAM is embedded within Google Cloud. GDC has no Google Cloud dependencies. Both platforms allow personified accounts, known as service accounts or identities, for workloads to interact with the respective platform services. While there are some key differences between Google Cloud IAM and GDC IAM, GDC provides the appropriate set of access privileges and capabilities for highly compliant workloads.

### Video - [Demo1 GDC CLI - Create and manage service Accounts](https://www.cloudskillsboost.google/course_templates/1192/video/521958)

* [YouTube: Demo1 GDC CLI - Create and manage service Accounts](https://www.youtube.com/watch?v=0E5QT5Gvqeo)

SPEAKER: Next are two demonstrations, the first one showing creating service accounts using the command line interface. After that, a demonstration showing creating a service account using the graphical interface. When creating a service account from the command line, the first thing we want to do is we want to log into the system. So use our gdcloud auth login. This will pop up the graphical interface. We're going to log in as our application operator user. Once we're authenticated, we then want to make sure that we go through and grab our Kubernetes credentials. So we're doing gdcloud clusters get-credentials, and then we're authenticating to the admin cluster in our organization. Now that we've got our Kubernetes authentication token, we're going to list the projects in the organization. In this organization, I am currently working with the backend project, so I'm going to create an environment variable called GDC_PROJECT. I'm going to set that equal to my backend. This isn't necessarily needed for this, but it will simplify things as we go through this example. I'm also going to specify a service account environment variable. And in this case, we're going to call it service-db1. So this is going to be a database service account that we're creating. Now that we have our environment variables set, what we're going to do is we're going to use gdcloud iam service-accounts. We're going to create our service account in our project. We've created our service account. Now I'm going to go list all of the service accounts in this project. There's actually two commands that will let me do this. The gdcloud iam service-accounts command will allow us to do this. There's also a kubectl command that will allow us to list service accounts in that project as well. So you can use either one. Now that we've got our service account created, we're going to need to assign roles to that service account in order for it to be able to do its activity. So here we're doing an iam-policy-binding. We're specifying our project, and we're specifying the role of project-db-admin and then giving the name of our service account. This will associate the DB admin role with the service account. I can then go through and list the role bindings for my service account in that project. And here we see we now have our service account associated with one role that's in the project. Now I have my service account, and I've got a role assigned to it. The next thing that I need to do is create a key. So gdcloud iam service-accounts keys create. We're specifying an output file of key.json. This will be saved to the local computer. And we're specifying our project. We're specifying our service account name. And the certificate path information is not needed if you properly have the certificate set up on your end-user workstation. Now I'm going to actually go and list the contents of that key to show you what the file looks like. And here we actually have the contents of the key, Base64 encoded, in that output file. That's what it looks like. Next what we can do is we can go and we can list out the keys that are associated. When we list out those keys, it is also going to give us an index, and this index right here is what is going to be needed to delete the key later. Let's say that this account has ended its useful life, and we now want to remove the account from the system. There's actually a couple of steps that we need to do for that. First thing that we're going to do is we're going to actually delete the key. We're going to specify this identifier, and then we're going to specify the project and the service account that it's associated with. So it's asking me if I'm sure that I want to delete that key. I say yes, and now that key is deleted from the system. So I can go through, and I want to look at my role bindings. So I want to make sure that I remove all my role bindings from that service account. Then I'll use a kubectl command to actually go through, delete the role binding, specify that we're doing a service account, specifying the project, specifying the name of the service account itself, and then the role that's associated with it. So we've now deleted that role from the service account. And then lastly, we are going to delete the service account itself, gdcloud iam service-accounts delete specifying the name of the service account and specifying the project. Specify yes, and we have it saying that it has deleted it. We're going to go through, and we're going to use the kubectl command to list service accounts for that project one more time. And notice that it still appears out there. Now, in testing, I've noticed that this will actually sit out there sometimes for a couple of minutes before it disappears. Rest assured, though, that after a short amount of time, it will then be gone. Now the account is gone.

### Video - [Demo2 GDC GUI - Create and remove service accounts](https://www.cloudskillsboost.google/course_templates/1192/video/521959)

* [YouTube: Demo2 GDC GUI - Create and remove service accounts](https://www.youtube.com/watch?v=CanPbN0Dgqs)

SPEAKER: Service identities within a project are managed by selecting the hamburger menu, dropping down to Identity and Access, and selecting Service Identities. From here, we can click on the Create Service Identity button. Name our service identity. And click Create. Once that service identity is created, we can go into it, and we can create a new key. That key will be downloaded as a file, and that file will contain the Base64-encoded key that is used for that service identity. Once we've created this service identity, we then need to give it roles. So we go down to Access, select Add Member. Use the Service Identity radio button. Select the dropdown. And then select the service that we created. From here, we can then select which roles we want to allocate to that service identity. Here, we're going to give it Project Bucket Object Admin so it can interact with the storage in this project. Now that we have this service account created and its service key, we can then use it. When the service identity is going to be retired, from the Service Identities menu, we select the checkbox next to the service identity and click the Delete button. We need to type in the word remove. That deletes the service identity and the access to the key. However, we still need to delete the role. Click on Access. Find your service identity and select it. And then, click on the Remove All Access button. Again, we need to type in remove and click the Remove button.

### Video - [Authorization](https://www.cloudskillsboost.google/course_templates/1192/video/521960)

* [YouTube: Authorization](https://www.youtube.com/watch?v=5IGr--WqIOw)

SPEAKER: In this lesson, you'll learn how authenticated users and workloads are authorized in GDC. As the Cymbal Federal example illustrated, GDC IAM lets you control who can do what, on which resources, based on the organization's resource hierarchy. GDC IAM specifically answers the following question. Do the right principals have the proper access to the correct resources? For identity, the principal can be an operator, a customer, or a workload personified as a service identity. Access aligns to predefined roles that provide a collection of permissions, such as an organization IAM admin role. The question of management means, can these principals access the correct resources? GDC authorization is built on top of or powered by Kubernetes Role-Based Access Control, RBAC, using Kubernetes-native entities. Roles are collections of permissions. A ClusterRole is similar but can be used anywhere in the cluster instead of just a namespace. RoleBinding maps a role to a user or set of users and grants that role's permissions to those users for any resources in that namespace. A ClusterRoleBinding grants a cluster role for authorization across the entire cluster, instead of being restricted to just a namespace. In GDC, roles at the cluster, project, or organization level contain the sets of permissions for the respective scopes. RoleBindings grant the role's permissions to users or sets of users or service accounts. Permissions are cumulative. There is no deny rule. For those familiar with Kubernetes RBAC, the area of concern is centered upon controlling access to the cluster, cluster resources, and the Kubernetes API server. The figure explains how the underlying Kubernetes RBAC resources relate to GDC personas and associated IAM predefined roles. Remember that the purpose of the GDC resource hierarchy is both to create a hierarchy of ownership or resources owned by the parent and to provide attach points and inheritance for access control and organization policies. An organization is the top of the GDC resource hierarchy and defines the security boundary. All resources that belong to an organization are grouped under the organization resource. Projects are organization-scoped, logical groupings of service resources. Service resources live within projects. And in GDC, Kubernetes clusters are organization-scoped sets of compute nodes. Recall that, in GDC, platform administrators manage organization-level resources and project lifecycle management for the resources in the hierarchy. And the application operators are members of the development team within the platform administrator's organization. Application operators interact with the project-level resources. GDC projects organize workload resources, such as logical groupings. This enables segmentation of resources within an organization because projects provide a lifecycle and policy boundary for managing resources. Service resources inside a project can never outlive the project itself or move between projects, ensuring that control is guaranteed for the life of a resource. A project is considered a proper Kubernetes namespace that spans across multiple clusters in an organization. The project name is unique across the entire organization and all Kubernetes clusters. Service providers create project scope services by creating control plane and data plane components in the namespace. Projects can be associated with one or more Kubernetes clusters. The namespace for a project hosts project-scoped service APIs and project-level policy configurations, such as Role and RoleBindings. You can associate a project with only a subset of clusters in an organization. Users can deploy containerized workloads on these clusters within a project namespace. The namespace sameness concept applies to the project namespace on these clusters. Namespace-scoped policies, such as role-based access, RBAC policies, apply to all those namespaces. GDC Kubernetes clusters are organization-scoped sets of compute nodes. They subdivide infrastructure resources into isolated pools to be consumed by projects within an organization. Clusters are also logically separated from each other to provide different failure domains and isolation guarantees. Clusters are shareable across GDC projects. The enforcement of policies per organization ensures clusters can be shared across teams and users, while also maintaining performance and resource guarantees. Additionally, this lets VM workloads run alongside container workloads without introducing operational complexity. User clusters are beneficial for instances where you deploy containerized workloads. However, with the option to deploy VM-based workloads, you're not required to have user clusters. Also, clusters can scale after initial deployment. GDC Kubernetes clusters are organization-scoped sets of compute nodes. They subdivide infrastructure resources into isolated pools to be consumed by projects within an organization. Clusters are also logically separated from each other to provide different failure domains and isolation guarantees. Clusters are shareable across GDC projects. The enforcement of policies per organization ensures clusters can be shared across teams and users, while also maintaining performance and resource guarantees. Additionally, this lets VM workloads run alongside container workloads without introducing operational complexity. User clusters are beneficial for instances where you deploy containerized workloads. However, with the option to deploy VM-based workloads, you're not required to have user clusters. Also, clusters can scale after initial deployment. Both projects and Kubernetes clusters are organization-scoped, and they can be associated with one another to organize service resources. Clusters must be associated with one or more projects. However, projects and clusters function independently from one another. This flexibility provides many different options for how to organize services and workloads. For example, you can have a cluster dedicated to a single project, or a cluster can span across multiple projects. One or more clusters can exist in an organization. GDC service resources are organized by projects. They must belong to a project or, optionally, a cluster for cluster deployments, and they cannot be shared across projects. Service resources inside a project can never outlive the project itself, ensuring that control is guaranteed for the life of the resource. They are enabled by default and can be disabled using an organization policy. Service resources include the following entities-- VMs, databases, storage buckets, containerized workloads, and backups. As mentioned, the persona's platform administrator and applications operator are not single roles but an abstraction of a collection of roles with different capabilities. GDC predefined roles use least privilege and authorization by task. Best practices for users' access follow the principle where only the least set of permissions are given for completing an intended operation. Let's explore some examples of how GDC personas relate to IAM predefined roles. Think about IAM requirements for an effective landing zone design. You need to define the functional roles, assign those functional roles to identity groups, and associate those groups with the necessary permissions. In practice, your organization's IAM groups exist in its identity provider. This group would be assigned GDC predefined roles as needed. For example, in this deployment, the Application Operator persona represents the development team within the organization. They need permissions to manage and interact with project resources. The GDC IAM role Project IAM Admin gives this permission. Creating an IAM group called App-Engineer lets you assign that role to the group. Remember, GDC predefined roles are those created and maintained by Google, and they align to the GDC personas. The platform administrator is responsible for administering the organization and creating projects. They would typically have the following roles-- Organization IAM Admin, AI Platform Administrator, Backup Repository Admin, Billing Viewer, Bucket Admin, and Bucket Object Admin. The Application Operator is responsible for managing project-scoped resources and would typically have the following roles-- Project IAM Admin, Vertex AI Optical Character Recognition, OCR Developer, Vertex AI Speech-to-text Developer, Vertex AI Translation Developer, Artifact Management Admin, Artifact Management Editor, and Backup Creator. In addition, there are common roles or roles available to all authenticated users. These include AI Platform Viewer, DB Options Viewer, DB UI Viewer, DNS Suffix Viewer, Marketplace Service Viewer, and Marketplace Viewer. Attribute-Based Access Control, ABAC, is provided by Open Policy Agent GateKeeper, OPAGK, and enables fully programmable policies in the clusters. ABAC prevents any changes to the configuration of the Kubernetes API from violating security, operational, or compliance controls. ABAC works in conjunction with Kubernetes RBAC. Both Kubernetes RBAC and ABAC must permit a request. IAM admins can use user attributes from IDP and draft finer-grained policies. Note that these attributes will need to be configured for inclusion with the IDP. Attribute-based access control, ABAC, lets IAM administrators implement access policies based on a user's attributes, as configured in an IDentity Store, or IDP. ABAC can be applied to both subject or request objects. With subject object, only users with specific citizenship attributes can be allowed to access certain storage buckets. With request objects, project creation can be constrained to certain namespaces. ABAC doesn't replace k8 RBAC, but you can use ABAC as a complement to define finer-grained policies. Now, let's explore the user flow of requesting IAM permissions. First, you log in to GDC. You're redirected to your IDP's login page to provide credentials for AIS to authenticate. Next, you perform operations with appropriate access. Organization admins and project IAM admins manage the role assignment to users and groups so users can perform their day-to-day jobs. This should be invisible to other users. Finally, you can request access to specific roles as needed. If you do not have the appropriate permissions, request that your IAM administrator grant you specific roles.

### Video - [IAM best practices](https://www.cloudskillsboost.google/course_templates/1192/video/521961)

* [YouTube: IAM best practices](https://www.youtube.com/watch?v=V-Pipf8EitM)

SPEAKER: Implementing IAM best practices is essential for securing your cloud resources and protecting your data. By following these best practices, you can minimize the risk of unauthorized access, data breaches, and other security incidents. The best practices involve resource hierarchy design, workload segmentation, least privilege for roles, and ability to provision users' roles permissions leveraging automation. Google recommended practices center around designing access boundaries between resources and segregation for workloads and GDC using organizations, projects, and Kubernetes clusters. This guidance balances efficient resource usage, isolation of workloads, and ease of operations. One of the areas of concern is organization isolation. You must decide how to implement GDC's physical separation across capabilities in your organization. We'll talk more about this in a moment. A trust boundary is a logical boundary where access is configured for teams to let them manage the application. Project isolation is about determining the management scopes, logical boundaries, and teams that administer applications and workloads. Both can be useful for satisfying compliance requirements. Google recommends separating your deployment environments into different projects. For example, you might have projects for development, staging, and production. You also need to think about how to implement project resource level IAM provisioning and the logical isolation of Kubernetes operations. You must consider the design of namespaces and address cross-project network communication requirements. Let's have a deeper discussion of some of these concerns, starting with organization isolation. When you design organization resources, consider how GDC provides physical and logical isolation. Organizations have physical isolation for compute infrastructure and logical isolation for networking, storage, and other services, such as vertex and databases. Interorganizational resource access and cross-organizational network connectivity is not allowed by default. Also, consider billing and budget visibility when you group workloads in an organization. In general, it is recommended that you group multiple workloads into a single organization based on these considerations. Workloads can share dependencies. For example, this might be a shared data source, connectivity between workloads, or a shared monitoring tool. Workloads can share an administrative root of trust. The same platform administrator can be trusted with privileged access over all workloads in the organization. Workloads are allowed to share underlying physical infrastructure with other workloads in the same organization, as long as sufficient logical segregation is in place. The same budget holder has accountability for workload budgets in aggregate. For details on viewing aggregate costs for the organization or granular analysis per workload, see the Billing page. Workload availability requirements can be met in a single-site GDC instance. When you require high availability that spans geographic distance, you must configure organizations across multiple GDC instances and external network connectivity. Within an organization, it is recommended that you provision multiple projects to create a logical separation between resources. Projects in the same organization might share the underlying physical infrastructure, but projects are used to separate workloads with a logical boundary based on identity and access management, IAM policies, and network policies. When designing project boundaries, think of the largest set of functionality that can be shared by resources such as IAM allow policies, network policies, or observability requirements. Group the resources that can share this functionality into a project, and move resources that cannot share this functionality to another project. In Kubernetes terms, a project is a Kubernetes namespace, so project names must be unique within an organization. Though a namespace is reserved across multiple clusters, that does not mean a pod is automatically scheduled across all clusters. A pod scheduled to a particular cluster remains scheduled to that particular cluster. Access to resources is governed by the IAM roles. If you are wondering why you can't access resources in one project but can in another, it is important to note that the default action is to deny resource access between projects. An additional GDC resource hierarchy best practice is creating projects per environment, such as development, staging, or production. Creating separate projects in each environment for each workload provides multiple advantages. It enables fine-grained definition of IAM allow policies and network policies, and prevents changes made in a development project from impacting the production environment. Some level of automation helps delivering granular policies consistently. Depending on your team structure and requirements, you might allow application operators to modify any VM within the project they manage. Or you might require more granular access control. Within a project, you grant granular IAM allow policies to allow individual developers access to some but not all resources in the project. For example, a team might have a database administrator who must manage the database but not modify the VM running an application, while the software developers on the team must not have permission to modify the database. You can have IAM groups from your identity store aligned to the functional focus of your team, such as separate groups for developers and database engineers. IAM policy assignment can be for each group, limiting or focusing their access to resources that the groups would operate or access. Additionally, think about your Kubernetes cluster design when implementing IAM in GDC. A Kubernetes cluster is not a hard tenant boundary because IAM allow policies and network policies apply to GDC projects not Kubernetes clusters. Kubernetes clusters and projects have a many-to-many relationship. You might have multiple Kubernetes clusters attached to a single project, or a single Kubernetes cluster that spans multiple projects. Design your clusters for logical isolation of Kubernetes operations by using separate projects per deployment environment and using separate Kubernetes clusters per deployment environment. This provides isolation of resource consumption, maintenance events, and cluster-level configuration changes across production and nonproduction workloads. For example, you can design a Kubernetes cluster for multiple workloads that span projects, clusters, deployment environments, and machine classes. This same architecture assumes that workloads within a deployment environment are allowed to share clusters. Each deployment environment has a separate set of Kubernetes clusters. You then assign projects to the Kubernetes cluster of the appropriate deployment environment. A Kubernetes cluster might be further subdivided into multiple node pools for different machine class requirements. A design without sharing of clusters across applications can also be useful. For example, you can use multiple Kubernetes clusters for container operations like these scenarios. You have some workloads pinned to a specific Kubernetes version, so you maintain different clusters at different versions. You have some workloads that require different GDC Kubernetes cluster configurations, such as the backup policy, so you create multiple clusters with different configurations. You run copies of a cluster in parallel to facilitate disruptive version upgrades or blue-green deployment strategy. You build an experimental workload that risks throttling the API server or other single point of failures within a cluster, so you isolate it from existing workloads.

### Video - [Organization policies](https://www.cloudskillsboost.google/course_templates/1192/video/521962)

* [YouTube: Organization policies](https://www.youtube.com/watch?v=L4YnOGKT3C4)

SPEAKER: Finally, in this lesson, let's discuss organization policies and review related terminology. Organization policies are policies that the organization administrator can configure for their whole organization or for specific projects. These policies have the following functions-- centralized control to configure restrictions on how an organization's resources can be used and define and establish guardrails for development teams to stay within compliance boundaries. Organization policies are essential for ensuring that only authorized users have access to specific resources in the cloud, thereby maintaining the security and integrity of the cloud environment. Organization policies apply to the entire organization, providing control over all resources in the hierarchy. Policy inheritance means that policies set at a higher level are inherited by all underlying resources. For enforcement and compliance, organization policies enable administrators to ensure that policies are enforced in GDC and that resources are compliant with these policies. And GDC organization policies provide the flexibility to update and scale policies as your organization's needs evolve. Let's examine how organization policies relate to GDC IAM. Organization policies let you define how a resource can be used. For example, load balancers can never be created with a public IP address. Organization policies also let you set default security rules that can apply to a whole organization and supersede any IAM policy. For example, there can't be any IAM policy allowing users outside of domain example dot organization. And organization policies provide exceptions to rules. For example, you might set a policy, the rule about load balancers public IP addresses doesn't apply in the project public ingress. On the other hand, GDC IAM lets you define who can do what action on what resource. For example, group DevA can create VMs in ABC project. One difference is in the frequency of change. IAM policies are changed frequently, for example, with new projects and new team members. Organization policies are static. Exceptions occur on a case-by-case basis, but overall modification of a policy indicates a major change in the security posture of the organization. GDC organization policies are built with the open source project, Open Policy Agent Gatekeeper. They use policy as code and are enforced at runtime. The validation steps are as follows. First, a DevOps engineer creates a Kubernetes deployment and attempts to deploy it. OPA Gatekeeper intercepts the request and validates requested cluster management activities against the policy. If the intended management activity does not violate the policy, the action is allowed to be successful. If the intended management activity is a violation of the policy, the action is not allowed. Organization policies are Kubernetes custom resources. You use kubectl or the Kubernetes API to manage them. For example, for the GDCHRestrictedServices policy, as an organization admin, you first assign yourself the gdchrestrictedservice policy manager IAM role. Then use kubectl apply -f on the policy file. Nobody will be able to update marketplace services until this policy is removed.

### Video - [Demo GDC - GUI Create a user and create a project](https://www.cloudskillsboost.google/course_templates/1192/video/521963)

* [YouTube: Demo GDC - GUI Create a user and create a project](https://www.youtube.com/watch?v=rs8v9UyrSqg)

SPEAKER: To create a user and give them the rights to create a project, we need to make sure that we're in the organization context. Select the hamburger menu, Identity and Access, Access. The user that we're using needs to have the organization IAM admin rights in order to have the permissions to create a member. Select the Add Member button, select your identity provider, and then select your username. From the Roles, we need to drop down and select Project Creator. Click the Add button, and we see that a new member has been added. Now we will log out and log in as our user. Once we're logged in as our user, we can click on the Create a Project button. Type in our project name. Select our Task Order and our contract line item. We can then make a determination as if we're attaching clusters, if we want to enable data exfiltration protection or not, and then we can review our selections. Lastly, hit Create, and that creates our project. Please note that once you've created this project, you are not actually able to manage it. The right only gives you the ability to create the project.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1192/video/521964)

* [YouTube: Module review](https://www.youtube.com/watch?v=g9himxdr3lQ)

SPEAKER: Let's review the key points from this module. In this module, we review the resource hierarchy for the GDC and its contributing influence to platform security. The resource hierarchy provides logical boundaries that can be governed to ensure data protection and least privilege access. The GDC resource hierarchy is composed of an organization resource in global resources such as Kubernetes clusters and projects, which are formal Kubernetes namespaces. Service resources such as VMs, database instances, and object storage must belong to projects. Within the GDC platform, all resources are owned by the organization. Lastly, GDC has multi-tenant capabilities that allow segmentation of physical hardware and logical networks. We reviewed the features of Anthos Identity Service, AIS, which acts as a bridge between your existing identity systems and the GDC platform, making it easier to manage authentication and authorization while maintaining security and reducing operational overhead. In summary, AIS provides a consistent way to manage user access and permissions across the GDC platform components, the console, Kubernetes clusters, and the gcloud CLI. We reviewed how to use RBAC to provision access based on a practitioner's role. Attribute-based access control, ABAC, considers the context, such as location of the user, the environment, or the type of resource to be accessed or the task to be performed. ABAC offers more granular control than RBAC. We Explored the best practices for implementing GDC IAM. It's recommended that you assign predefined IAM roles to user groups and that you identify and establish management boundaries to segment workloads. Finally, we reviewed how to bolster the security posture of the GDC by using organization policies which control access to resources and determined how resources can be deployed. Organization policies provide consistent governance irrespective of the types and number of workloads. By leveraging organization policies in the GDC resource hierarchy, configuring predefined roles and establishing projects as management boundaries to meet compliance needs, administrators and application teams can securely run workloads within the GDC platform.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1192/quizzes/521965)

#### Quiz 1.

> [!important]
> **What enforces organization policies within the GDC air-gapped platform?**
>
> * [ ] Kubernetes scheduler
> * [ ] Open Policy Agent Gatekeeper
> * [ ] Kubernetes API server
> * [ ] SSL encryption

#### Quiz 2.

> [!important]
> **What account type is used by workloads or services to programmatically consume resources and access microservices securely?**
>
> * [ ] Cloud Identity accounts
> * [ ] Active Directory accounts
> * [ ] Service identities
> * [ ] Super user accounts

#### Quiz 3.

> [!important]
> **Your organization has DevOps engineers located in four different states or regions. Regional regulations require that data access for object storage buckets be governed by location. The identity provider used for GDC defines a location for all users. What is the best option to govern access for DevOps engineers?**
>
> * [ ] Implement a custom Kubernetes controller to govern access.
> * [ ] Implement an attribute-based access control restriction policy for the object storage buckets.
> * [ ] Require team members to move to a different geographic location.
> * [ ] Implement custom application logic to authenticate team members.

#### Quiz 4.

> [!important]
> **Which two identity provider authentication protocols does GDC air-gapped support?**
>
> * [ ] Smart card and biometric authentication
> * [ ] Active Directory Federation Services (ADFS)
> * [ ] OpenID Connect (OIDC)
> * [ ] Google Cloud social identity provider
> * [ ] Kerberos

#### Quiz 5.

> [!important]
> **Which two GDC resource hierarchy entities are organization-wide in scope?**
>
> * [ ] Kubernetes clusters
> * [ ] Projects
> * [ ] Storage buckets
> * [ ] Databases
> * [ ] Folders

#### Quiz 6.

> [!important]
> **Which would a platform administrator use to restrict how a GDC air-gapped resource is used or configured?**
>
> * [ ] Service identity
> * [ ] Organization policy
> * [ ] Firewall rule
> * [ ] IAM

## Virtual machines

This module contains three lessons. First, you walk through the process to create and configure VMs. Then, you learn about the VM disks, snapshots, and backups available with GDC air-gapped, and how to manage VMs.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1192/video/521966)

* [YouTube: Module overview](https://www.youtube.com/watch?v=OPftFAKI4TM)

SPEAKER: Welcome to our module on virtual machines. In this module, you'll learn to identify virtual machine VM types, and you'll examine network, image, disk, and snapshot options for VM instances. You'll also learn about the Google Distributed Cloud GDC air-gapped VM lifecycle, and how to create and configure a VM instance in GDC. This module contains three lessons. First, we'll walk through the process to create and configure VMs. Then you'll learn about the VM disks, snapshots, and backups available with GDC air-gapped and how to manage VMs.

### Video - [Create and configure VMs](https://www.cloudskillsboost.google/course_templates/1192/video/521967)

* [YouTube: Create and configure VMs](https://www.youtube.com/watch?v=jtHHdurgmyk)

SPEAKER: In this lesson, we'll discuss how to create and configure VMs in GDC air-gapped. Let's start by reviewing how our example entity, Cymbal Federal, intends to use VMs in its GDC deployment. Recall that Cymbal Federal's logistics application provides inventory, ordering, and delivery scheduling of supplies to partner locations. Mission logistics runs computationally and memory-intensive scheduling processes. GDC, by design, offers configurable VM sizing, backup, and recovery capabilities for managing enterprise VM workloads. Current DevOps processes provide separation of duties for database management and application management. So the application and database tier will be deployed in different projects. Cymbal Federal's mission logistics application is just one example of how you can use VMs in GDC. GDC offers robust features for VM workloads. GDC VM features include customizable instance sizes, network security using public and private IP addresses, and VM and image management tools with package management for Linux distributions. You can interact with VMs using the GDC console, the gcloud CLI, or APIs. Cymbal Federal's mission logistics application runs computationally and memory-intensive scheduling processes, so they need a machine type optimized for this workload. GDC offers an array of machine types to accommodate various workload types-- general purpose, memory-optimized, CPU-optimized, and GPU-optimized. Notice the naming convention for the VMs, which reflects the configuration. You select the machine type when you create a VM instance. You name the VM only once, but can label it, change labels, and have multiple labels. You can customize the machine configuration or select from a preset instance from the Machines tab. The description box lets you describe the goal and use of your instances. GDC VM instance networking has configurable network options for ingress and egress that can bolster application security. Ingress rules govern access from external networks to GDC-hosted VMs, which will always use private IP addresses, RFC 1918. Connectivity is achieved by Network Address Translation, NAT. Egress rules control access to external networks from GDC-hosted VMs. VM egress can be cross-project, cross-organization, or external to the GDC environment. If you enable egress for the VM, you see an egress IP address generated. The VM connects to the external services with this IP address. GDC offers Linux distribution VM image options, including Ubuntu APT packages and Rocky Linux RPM packages. Also, the GDC platform provides APT and RPM packages for keeping systems up to date. GDC offers modern image-management features to let you upload custom-created images from existing disks and golden images imported from files. GDC provides multiple interfaces to create VMs-- the GDC console, the gcloud CLI, kubectl, and Terraform automation. Terraform is used for declarative infrastructure-as-code deployments. Choose the tool that works for your team. To create VMs and VM disks in a project, you need to request the appropriate permissions from the project IAM admin. Several predefined roles let you manage VMs. Project VirtualMachine Admin manages VMs in the project namespace. Project VirtualMachine Image Admin manages VM images in the project namespace. And VM Admin manages VMs and disks in user clusters. There are also common predefined roles for VMs. VM Type Viewer has read access to predefined VM types, and Public Image Viewer has read access to images that GDC provides. You can create VMs in GDC in various ways-- from an image, from a backup or snapshot, or by using a custom machine type. Let's cover some examples of creating VMs. First, this example creates a VM from an image using the gcloud CLI. For this VM, Cymbal Federal selects a machine type with two CPUs and 8 gigabytes of memory for its mission logistics application. This example uses kubectl to create a VM from an image, also selecting a VM with two CPUs and 8 gigabytes of memory. Next, this example creates a VM image from an existing disk using kubectl and the Kubernetes API. Here, the custom resource definition VirtualMachineImageImport is declaratively configured. This example creates a virtual machine from an image using Terraform. Terraform implements the Kubernetes API to provision GDC resources. Finally, this example imports a golden image, or a base image, using the Kubernetes API. Typically, these are highly configured and governed images with curated and approved tools and libraries installed. Note that you can only import bootable disks, and you must upload your virtual disk to an object storage bucket before starting the import process with the API. See the references for more information.

### Video - [VM disks, snapshots, and backups](https://www.cloudskillsboost.google/course_templates/1192/video/521968)

* [YouTube: VM disks, snapshots, and backups](https://www.youtube.com/watch?v=sg6wVSZV1e8)

SPEAKER: Now that you've learned about creating VMs and the available machine types in this lesson, we'll discuss VM disks, snapshots, and backups. The GDC VM platform offers a comprehensive suite of standard Linux disk features, suitable for a broad range of storage requirements and operational scenarios. Disks are encrypted at rest, and they can be encrypted using customer-managed encryption keys, CMEK. Disks can be mounted, formatted, and unmounted using standard Linux commands. And when you back up VMs, all disks attached to the VM are also backed up. Snapshots enable archiving data stored on persistent disks. GDC VM snapshots let you perform the periodic backup of GDC-persistent disks. Note that all backups require that a backup repository and backup plan exist. GDC management capabilities support creating, viewing, and deleting snapshots. Based on your management preferences and the task, management operations may be performed using the console, all operations, Kubectl CLI interface, view/delete, and Kubernetes API interface, create. The snapshots section shows all existing snapshots and actions to be taken. There is also a restore section to manage backups or versions. GDC VM backups provide for full VM backup for all attached disks. Again, it's important to note that all backups require that a backup repository and backup plan exist. GDC supports creating, viewing, and deleting backups and restoring VMs from backups. Based on your management preferences, you can perform management operations using the console and Kubectl CLI that invokes the Kubernetes API. Next, let's review GDC image, disk, and VM operations. Least privilege and IAM are key components that define who can produce backups and the associated configurations that enable backups within GDC. This is an important consideration because data protection is of highest concern to ensure data integrity and availability during recovery. Project IAM admins can grant predefined IAM roles that let users create GDC VM backups and disk snapshots. The required predefined roles include the project virtual machine admin role to manage VMs in the project namespace, the backup creator role to create manual backups and restores, and the project viewer role for read-only access to project resources. In order to create a backup, a few GDC configurations have to be in place. First, you need a backup repository. This indicates where to store backups and lets you restrict IAM permissions to organization admins. Second, you need a backup plan. This provides the configuration and location for backups. You can have one or more backup plans for each VM. Finally, you need to assign the appropriate supporting IAM roles. The backup repository admin creates backup repositories, and the user cluster backup admin creates backup plans. Note that organization admins grant the necessary permissions to let users create backup plans with the user cluster backup admin role. The first example is a manifest file for creating a GDC backup template. It includes a backup repository named mission_logistics_vm_backup that was created earlier. Next, this example backs up a VM from an image using Kubernetes API. Keep in mind that a backup repository and backup plan must exist. Custom resource definitions, VirtualMachineBackupRequest is configured here. This example manifest file backs up a VM. This example uses a restore to create a VM. The results of the restore creates a new VM with the same configuration and disk state as the backup. Finally, this example creates a new VM using a snapshot.

### Video - [VM management in GDC](https://www.cloudskillsboost.google/course_templates/1192/video/521969)

* [YouTube: VM management in GDC](https://www.youtube.com/watch?v=RrR8VDiv7lM)

SPEAKER: Now let's discuss how to manage VMs in GDC. As we mentioned earlier, there are several predefined roles for VM management. These include Project VirtualMachine Admin, which lets you manage VMs in a project, Project VirtualMachine Image Admin, which lets you manage VM images in a project, and VM Admin, which lets you manage VMs and disks in user clusters. In addition, predefined common roles may be used for read-only access to VM and image resources. These include VM Type Viewer, which gives read access to predefined VM types, and Public Image Viewer, which has read access to images that GDC provides. Checking VM status is an important part of operations, both when you need to troubleshoot using incident management and to confirm VM status after restarting. You can inspect the list of VMs and their status using gdcloud. GDC VMs can have one of the following operational states at any one time, starting, running, stopped, paused, or terminating. GDC VM Manager presents a management dashboard, listing the status, name, ingress and egress IP addresses, and protocols allowed for remote administrative access. VMs can be fully managed from this page. The status of each VM indicates its current lifecycle state. The VM-level metrics dashboard in Grafana provides you with graphs for viewing VM-specific metrics for your projects. You can visualize metrics, such as CPU utilization, memory statistics, network statistics, and disk performance. Let's examine each one in turn. The CPU utilization metrics give you the amount of time that each virtual CPU, or vCPU, spends in different states. The graph displays information for each of the vCPUs specified by your machine type and plots metrics for each vCPU. For example, the graph in this example indicates the VM has two vCPUs. You can also find memory statistics on the VM dashboard. You can examine the total memory, the amount of memory available, and the amount of memory being used. From the network page, you can view network information, such as the number of packets received from the VM and sent to the network, the quantity of bytes received from the VM and sent to the network, the number of receive and transmit packets that have been dropped, and the network transmit error packet count. Finally, the disk dashboard provides disk metrics. You can use the disk dashboard page to view read/write operations and disk performance.

### Video - [Demo GDC GUI - VMS Create, enable SSH access, login, manage, and delete](https://www.cloudskillsboost.google/course_templates/1192/video/521970)

* [YouTube: Demo GDC GUI - VMS Create, enable SSH access, login, manage, and delete](https://www.youtube.com/watch?v=z9AnVldbyiI)

SPEAKER: To create and manage virtual machines within a project, first, we need to make sure that the project context is selected. Here, we're using the backend project. Use the hamburger menu, drop down to Compute, Virtual Machines. From this page, we can click on the Create Instance and name our virtual machine. If we wish, we can add labels. I'm going to label this as key group and give it a value of research. For machine configuration, we have two options. One is to select a general-purpose machine that is already created. Notice, these have different numbers of CPUs, different amount of memory, and if you wish, you can create one with a graphical processing unit. Here, we're going to select a two-CPU, eight-gigabyte virtual machine. I select Next. We can make a determination as to if we want to allow external communication to the project. Here, we're going to allow it. Select Next. Here, we can do any customization to it. Right now, we're going to use a 64-gig disk using Rocky Linux. If I want to change the operating system, I can hit this Change button. Select a different golden image. So here, we're going to change it to Ubuntu. We're going to leave secure boot turned on. And if the virtual machine is deleted, the disk will be deleted as well. Hit Save. Select the Create button. And now, the virtual machine is in the creation process. This will take a few minutes. While this virtual machine is being created, we can enable SSH access into this project. To do that, I need to give myself network policy admin rights for my user. Currently, I'm the admin AO, so I go out to Edit Roles, add a new role, specify the project network policy admin. Now that we have that role, we can use the hamburger menu, drop down to Firewall, select Create, give a name, specify ingress, all workloads. And we're going to select a specific port, TCP port 22. Once you've created the virtual machine and the firewall rules, we can then go in and access the virtual machine itself. So from the hamburger menu, drop down to Virtual Machines, Instances. For the virtual machine you want to control, you see to the right, you have an action of SSH. You can click on that. It will create a pop-up window saying that there's an SSH browser. And it will take a minute or two in order for it to load the terminal session. Once the terminal session is loaded, you can then run commands as if you were on the machine. Right now, I'm running whoami. And this shows that I am my admin AO user for the site. I also have the ability of transferring files using the FTP button. From here, I can move files around. I can also upload them and download them. So here, I'm selecting a file, selecting Open, and then hitting the Upload button. And now, we see the file is there. I can then also Download, Rename, or Delete the file. And the file is gone. Other management activities that we can perform on virtual machines include selecting them. We can edit the description. We can change or edit labels. We can change the ingress port. The boot disk can be changed. We can also add a new disk. Some of these maintenance tasks require the virtual machine to be off in order for that activity to be performed. Here, we have a virtual machine that is stopped. We can go into the virtual machine. We can add a new disk, give it a name, specify the size that we want the disk to be in gigabytes, and then we can set a deletion rule as to if we want to keep the disk or delete the disk when the virtual machine is retired. Here, we see we have our 10-gig disk. We can then go in and we can start the virtual machine again. To delete a virtual machine, select it, click on Delete, and select Delete.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1192/video/521971)

* [YouTube: Module review](https://www.youtube.com/watch?v=pfD3Y0v1Qzw)

SPEAKER: Let's review the key points from this module. First, we discussed the GDC VM service features, such as VM types and the management options for creating VMs and disks. This module also highlights the capabilities for data recovery that are available with the VM service backup and snapshot features. In addition, flexible network configurations for GDC VMs govern ingress using public and private IP addresses. GDC VMs have a life cycle that defines their running operational states. And the VM service includes inbuilt observability dashboards for monitoring VMs. Finally, the module presented video demos of VM management tasks, such as how to create VMs, manage VM disks, and access VMs using the secure shell protocol.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1192/quizzes/521972)

#### Quiz 1.

> [!important]
> **Cymbal Federal's Mission Logistics application team uses VMs for the application tier. The application team aims to shut down development environments nightly. What VM state indicates that the virtual machines have been shut down?**
>
> * [ ] Pending
> * [ ] Stopped
> * [ ] Terminating
> * [ ] Provisioning

#### Quiz 2.

> [!important]
> **Cymbal Federal's Mission Logistics application team uses VMs for the application tier. The application team wants to deploy the VM using infrastructure as code. Which deployment method should they choose?**
>
> * [ ] GDC API interface
> * [ ] Terraform
> * [ ] kubectl command-line interface (CLI)
> * [ ] gdcloud command-line interface (CLI)

#### Quiz 3.

> [!important]
> **Cymbal Federal's Mission Logistics application uses a high memory VM. Which virtual machine type should they select?
**
>
> * [ ] n2-standard-4-gdc
> * [ ] n2-highcpu-8-gdc
> * [ ] a2-highgpu-1g-gdc
> * [ ] n2-highmem-8-gdc

## Review: GDC platform introduction

Review course content


### Video - [Course 1 review](https://www.cloudskillsboost.google/course_templates/1192/video/521973)

* [YouTube: Course 1 review](https://www.youtube.com/watch?v=lCV8_Qx33Pg)

SPEAKER: Thank you for taking the Google Distributed Cloud, or GDC, Airgapped Platform Introduction Course. Hopefully, you now have a better understanding of the GDC platform and how it delivers a managed cloud experience for highly secure and compliant workloads. You should also recognize how your responsibilities on the platform align to one of the GDC practitioner personas and the types of tasks you will engage in as a platform administrator or an application operator as you interact with the system through the GDC Console, the GDCloud CLI, or the APIs. You should be able to administer GDC IAM to control access to resources and create and manage VMs. Next, Google recommends enrolling in the Compute, Network, and Storage Services Configuration in GDC Course of the GDC Practitioner Fundamentals series. In that course, you learn how to use the different services in GDC to design and manage your instance.

### Document - [Course slides](https://www.cloudskillsboost.google/course_templates/1192/documents/521974)

### Document - [Additional resources](https://www.cloudskillsboost.google/course_templates/1192/documents/521975)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
