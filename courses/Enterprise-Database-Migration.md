---
id: 145
name: 'Enterprise Database Migration'
datePublished: 2024-11-26
topics:
- Oracle
- Data Management
- Databases
type: Course
url: https://www.cloudskillsboost.google/course_templates/145
---

# [Enterprise Database Migration](https://www.cloudskillsboost.google/course_templates/145)

**Description:**

This course is intended to give architects, engineers, and developers the skills required to help enterprise customers architect, plan, execute, and test database migration projects. Through a combination of presentations, demos, and hands-on labs participants move databases to Google Cloud while taking advantage of various services.

This course covers how to move on-premises, enterprise databases like SQL Server to Google Cloud (Compute Engine and Cloud SQL) and Oracle to Google Cloud bare metal.

**Objectives:**

- Plan, execute, test, and monitor simple and complex enterprise database migrations to Google Cloud.
- Choose an appropriate Google Cloud database, migrate SQL Server databases and run Oracle databases on Google Cloud bare metal.
- Recognize and overcome the challenges of moving data to prevent data loss, preserve data integrity, and minimize downtime.
- Evaluate on-premises database architectures and plan migrations to make the business case for moving databases to Google Cloud.

## Introduction

In this module, you learn the about the structure and flow of the course. The overall goal of this course is to teach you how to migrate enterprise databases from on-premises to Google Cloud. 

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/145/video/515080)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=Q8knssKIdGw)

Hi, everyone, and welcome. My name is Damon Runion. I am a Technical Curriculum Developer at Google. I'm delighted to welcome you to this course on Enterprise Database Migration. Let's talk about what you're going to learn in this course. The overall goal of this course is to teach you how to migrate enterprise databases from on-premises to Google Cloud. To do that, you first need to evaluate the on-premises database, its architecture, its dependencies, and its dependents. This helps you plan a migration that minimizes disruptions, is less prone to errors, and has minimal downtime. Google Cloud has many ways of running databases. You might simply create a virtual machine and install your favorite database software on it. There are other, more automated approaches, though. For example, you could install a pre-configured machine from the marketplace with the database software already in place. You could also use a managed database solution, like Cloud SQL or Spanner. Or if you are using Kubernetes, you can run your database in your GKE cluster. You will learn the pros and cons of each of these various choices and get hands-on experience with them. This will help you choose the right service when moving a real-world database. Google has developed a recommended methodology for moving applications to the cloud. This methodology consists of four steps. Assess, plan, deploy, and optimize. In this course, you learn about those steps and the activities you should be performing in each one. Security is obviously important when you are migrating databases to the cloud. And a big part of that is designing and creating a secure network. You will learn how to create a secure network for your database and also how to automate network creation so it is more maintainable and reproducible. SQL Server and Oracle are very common enterprise databases. Many large organizations have both. This course covers how you can run both of those databases in Google Cloud. Testing and monitoring are certainly important in all IT projects. You'll learn how to effectively test your database migration projects and how to use Google Cloud's built-in monitoring tools to ensure a successful project and to keep your databases running smoothly over time. Lastly, you'll learn how to leverage tools for database migration. You'll learn about some tools provided by Google Cloud and some third-party migration tools that will make your migration projects go more smoothly and quickly. This course is intended for engineers planning a data migration or working on a database migration project. Technical managers, IT decision makers, and others who want to understand how to migrate databases to Google Cloud may also benefit from this course. This course is not meant to be an introduction to Google Cloud. Google Cloud experience and prior training on the fundamentals is assumed. This course is also not a database administration class. It is assumed that you already know how to administer the databases you are moving to the cloud, have experience with SQL, and have some programming background. Qwiklabs provisions for you Google account credentials so you can access the Google Cloud console for each lab at no cost. Specifically, for each lab, Qwiklabs offers a free set of resources for a fixed amount of time and a clean environment with permissions. Good luck, and I hope you enjoy the class.

## Migrating Enterprise Databases to the Cloud

In this module, you learn some theories and motivations behind the course.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/145/video/515081)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=riud5TysE-E)

Hi, Damon again. Welcome back to Enterprise Database Migration. In every course, it's a good idea to start with some theory about what the course is about and why it is important. So let's talk briefly about why customers want to move to the cloud and discuss some real-world problems you may encounter. In this module, you get a high-level overview of why customers want to move their database workloads to Google Cloud, what it entails, and what our competitors offer in this space. You learn how to assess different database architectures and how they affect your projects. You learn how to optimize databases for the cloud. Lastly, you learn to architect database solutions for high availability, scalability, and durability.

### Video - [Solution Overview](https://www.cloudskillsboost.google/course_templates/145/video/515082)

- [YouTube: Solution Overview](https://www.youtube.com/watch?v=ghuTxddtJPM)

Let's get started with an overview of database migration problems and some solutions provided not only by Google Cloud, but other providers as well. Take a minute to consider some real-world challenges you may encounter when you migrate databases to the cloud. Databases can be the hardest part of an application to move. Often, many dependent applications rely on the database for their data. And those applications may be constantly adding new data to the database. Moving the database can break those connections. There are also security concerns. When hackers are attacking your systems, they're usually after your data. You need to design and architect your system in a way that protects the database, but allows dependent applications to have access. For some applications, defining a maintenance window and bringing the database down for that window of time is acceptable. For mission-critical applications, though, downtime needs to be minimized and sometimes avoided completely. This complicates your migration project significantly. Automation, database replication, and extensive testing will be required to ensure that when you flip the switch to move from the old database to the new one, everything will continue to work seamlessly. Because the database is so important to the operation of its dependent applications, moving it to the cloud is often a critical first step when migrating applications. After the database is moved and running, moving other applications is comparatively easy. Customers often want to take advantage of the many services Google provides as part of a digital transformation. When you're running in the cloud, you can more easily take advantage of Google's advanced machine learning and big data processing services, for example. Moving the database can also be a big cost-saving opportunity for customers. Between hardware, maintenance, licensing, and administration, the database tier is likely the most expensive part of an application. Moving to Google Cloud can help reduce all of those costs. When their database is in the cloud, customers can work to optimize their databases and move to even cheaper, completely managed data services like BigQuery, Firestore, and Spanner. There are many great reasons customers want to move their databases and applications to Google Cloud. Google has data centers in regions all over the world, and each region is divided into multiple zones. You can deploy your database to multiple zones or even multiple regions for greater scalability and fault tolerance. Google will manage all the hardware for you, which simplifies the management of your databases. If you use a managed database service like Cloud SQL or Spanner, Google does practically all the maintenance for you. Sometimes people think moving to the cloud is less secure. On the contrary, Google's security is unmatched. If you know what you're doing, moving to Google Cloud can, in fact, enhance the security of your applications and data. And of course, decreasing the total cost of ownership of running your applications is often a primary driver for moving to Google Cloud. Google Cloud offers advanced capabilities that only the largest organizations could replicate in their own data centers. There is practically unlimited compute power and storage provided by Google's many hyperscale data centers located all around the globe. The data centers are connected by Google's fast and reliable global network. All resources in Google Cloud can be automated for easier management and cost savings. Google also embraces open source technologies and is a big contributor to the open source community. Google uses a custom Linux distribution for its own servers. Some very popular open source technologies like TensorFlow and Kubernetes originated at Google. Google is a leader in advanced technologies, which is often why customers choose Google over other cloud providers. Some customers want to add machine learning capabilities to their applications using TensorFlow or one of Google's artificial intelligence APIs. Many customers want to simplify their data center management using Kubernetes. Enhanced security is also an important factor when customers move to the cloud. Because Google Cloud is already certified by many government and industry compliance standards, running on Google Cloud can make compliance easier for many customers. Other cloud providers competing with Google also offer compelling platforms and services. Amazon Web Services is one of the largest cloud providers in the world. They offer a managed database solution called RDS. RDS supports SQL Server, Oracle, MySQL, and other databases. Amazon also has a customized version of MySQL called Aurora that is optimized to run on their cloud. RDS has many advanced features like automated backups and automatic replication for high availability. For customers who rely heavily on Windows, Microsoft Azure, as you would expect, provides very strong integration with Microsoft products and tools. Azure SQL Database provides SQL Server as a service. There is also a cloud-based Azure Active Directory service. Azure also supports open source technologies. You can run Linux or Windows virtual machines and many other databases like MySQL and MongoDB. Azure also supports Kubernetes through Azure Kubernetes Service. Oracle also provides their own cloud for those customers who rely on Oracle databases. Oracle Cloud automates the provisioning of Oracle databases. It differs from the managed service provided by AWS RDS in that it supports all Oracle features, which RDS does not, and supports all Oracle versions. In summary, customers on a path to digital transformation want to leverage Google Cloud's hyperscale data centers, global reach, and advanced services. That path starts with moving your data and databases and then moving your applications. After you're in the cloud, you can start optimizing for the cloud and ultimately transform your business with machine learning and artificial intelligence.

### Video - [Traditional Database Architectures](https://www.cloudskillsboost.google/course_templates/145/video/515083)

- [YouTube: Traditional Database Architectures](https://www.youtube.com/watch?v=3RW6CHnP30Y)

In this section, you learn about the different database architectures you'll encounter. In one sense, this is a history lesson on how database architectures have evolved. However, in large organizations, you might find a mix of database architectures, depending on who created the databases and what they were designed for. Client-server databases have been a standard for many years. Client-server databases tend to be highly normalized. Also, business logic tends to be provided by the database server itself, rather than by the client. Business rules are implemented using constraints on fields and relationships between tables, as well as stored procedures and triggers. Clients connect directly to the database. Clients in a client-server architecture were intended to be as thin as possible. The work of managing transactions and enforcing rules was the domain of the database server, where this logic was centrally located and shared by the clients. Client-server databases are fast and secure and are the preferred architecture for many DBAs. Programmers, especially object-oriented programmers, argued that business logic and transaction management should be the domain of the application, not the database. With a three-tier or n-tier architecture, much of the business logic is pulled from the database and put into the application code. The database becomes more of a storage tier. The middle tier handles the complexity. In a three-tier architecture, there are fewer, if any, stored procedures. While this makes programmers happy, database performance overall can suffer, especially when accessing multiple tables within a transaction. In this architecture, clients don't connect directly to the database. Instead, they connect through an intermediary application server. As the internet has grown and more and more different types of clients have appeared, there has been a trend toward a service-oriented architecture. Databases sit behind a firewall, and their functionality is encapsulated or hidden behind a service that is made available over the network. Clients don't need to know anything about the details of the database. Applications only connect to the service via HTTP and pass data back and forth using a text-based format like XML or JSON. Only the service makes the connection to the database. Of course, in the real world, you won't find a company that uses one architecture or the other. You will find a mix of all these architectures, depending on who designed and programmed the systems and when. In older applications, you might find a mix of architectures with a single database, depending on when new features and data were added over time. This can lead to confusion when you are analyzing applications and databases you want to migrate. A key aspect to migrating the databases is finding dependents and dependencies. A dependent is something that relies on the database or application, and a dependency is something that the application or client must have to function properly. For example, in a three-tier architecture, the application server is a dependency for the client. At the same time, the application server is a dependent of the database. Understanding the architecture of the database you are trying to move is important when planning a migration project. Generally, client-server databases are harder to move because there is more logic provided by the stored procedures that needs to be verified and tested. Any error would probably break the clients, and more dependent applications tend to be connected directly to the database. Three-tier or n-tier applications have most of the business logic in the application code, so there is less testing required in the database tier. Also, there tend to be fewer dependents because the clients connect to the application server, which in turn connects to the database. Service-oriented applications can be the easiest to move to the cloud because the database and all its details are hidden behind a service layer. The service can be used to synchronize the source and target databases during the migration. Eventually, the new database takes over and the old one can be turned off. There is no need to worry about the clients because they are simply passing data between themselves and the service.

### Video - [Optimizing Databases for the Cloud](https://www.cloudskillsboost.google/course_templates/145/video/515084)

- [YouTube: Optimizing Databases for the Cloud](https://www.youtube.com/watch?v=Ppy1RoTnr84)

After you move your databases and applications, there is an opportunity for optimizing them to run in the Cloud. Let's briefly talk about how you could do that. Microservice architectures have become popular as more and more applications have been migrated to the Cloud. With microservices, large applications are divided into smaller, simpler, independent services. Each microservice should be responsible for its own data. This is a big change. You want to design your services to minimize the dependencies between them. Microservices should be loosely coupled. That means one service can use another without understanding its details. Data passed between services is simply text in JSON or XML format. Also, different microservices should be able to be deployed and versioned independently. Recently, a customer complained that they were having trouble synchronizing the deployment of three services because they all had to be deployed at the same time for the application to work. This is definitely an anti-pattern. In this case, the services are too entangled. They need to be either refactored or combined. Because each microservice should be responsible for its own data, when applications are optimized for the Cloud, databases also have to be split into multiple smaller pieces. Some developers make the mistake of having one main database service provide the data to all the microservices in an application. This complicates your deployment without providing all the benefits of a microservice architecture. There are many benefits of microservices. One benefit is ability to choose which type of database is most appropriate for each individual service. For some services, a relational database is required. But for others, using a NoSQL database or a data warehouse can save money and make programming easier. With monolithic apps, the database you choose has to be one that works for the entire application. That means you probably need a relational database. Relational databases are great and they work for nearly any application. However, relational databases tend to be more expensive and require more administration than other simpler types of databases. With microservices, you can keep the relational database when the service requires it. But you can use NoSQL databases or data warehousing solutions when they are a better fit for the microservice use case. Use relational databases for online transaction processing use cases when strong schemas and strong consistency are required. NoSQL databases can be simpler and less expensive than relational systems. Use a NoSQL database when you prefer or can tolerate a schema-less database or when a higher degree of flexibility is needed. If a microservice is for a big data analytics or object storage, consider a data browsing solution like BigQuery or Cloud Storage. These solutions are extremely inexpensive and support unlimited amounts of data. Google Cloud provides a complete collection of database and storage solutions. There are two managed database services, Cloud SQL and Spanner. Cloud SQL allows you to run MySQL, PostgreSQL, and SQL Server databases. Spanner is a Google-created relational database service that is massively scalable and can easily be deployed across multiple regions for extremely high availability and low latency around the world. Google provides a completely managed, highly scalable, and inexpensive NoSQL database solution called Firestore. Memorystore provides a managed Redis database solution. Bigtable is a managed wide column NoSQL database similar to Cassandra. For data warehousing and analytics using SQL, use BigQuery. Use Cloud Storage for cheap object storage.

### Video - [Architecting Scalable and Highly Available Database](https://www.cloudskillsboost.google/course_templates/145/video/515085)

- [YouTube: Architecting Scalable and Highly Available Database](https://www.youtube.com/watch?v=lCP0RRl1Kxs)

A system that is scalable is one that continues to work even as the number of users and the amount of data grow. Highly available systems are fault-tolerant and continue to work even when there is a failure of some of the nodes. Let's talk about how you can architect your databases to achieve scalability and high availability. High availability is achieved by deploying infrastructure across more than one zone in a region. In Google Cloud, each region is divided into three zones. Think of a zone as a fault boundary. No single thing that can fail would cause resources deployed in different zones to be unavailable. If you're deploying an application like a web app or service, create instances in multiple zones and then create a load balancer that routes traffic to those back-end instances. The load balancers will monitor the health of the instances and only send traffic to healthy ones. The load balancers are services provided by Google and are also fault-tolerant. Load balancers can be either regional or global. In either case, they will survive a zonal outage. For your applications to be highly available, your databases must be too. Failover replicas are used to ensure database reliability. Deploy two databases in different zones. One is the main and one is the failover. Requests go to the main. All data is synchronized with the failover. If the main ever goes down, the failover takes requests until the main is back up and running. Scalable databases continue to work as the number of users and amount of data grow very large. To handle a large volume of writes, you split the database into pieces called shards, then use multiple nodes, or servers, to process different shards. You can process high volumes of reads by creating multiple copies of the database called replicas. Read replicas handle analytics and reporting use cases, while the main is left to handle the writes. The main ensures that the data is synchronized with the read replicas when changes are made. Customers with users all over the world can create read replicas in multiple regions. Requests from users are routed to the region geographically closest to them. This can be automated using Google's global load balancers. A main will still handle the writes and synchronize those changes to the replicas. When synchronizing across regions, you just have to be aware of the increased latency around the world. If you are using asynchronous replication across regions, the replicas can have stale data for a short period of time. This is known as eventual consistency, as opposed to immediate consistency. Distributed databases use clusters of nodes and multiple shards to process high-volume writes. This diagram is overly simplified, but it gets the idea across. If you split the data into smaller pieces, you can use multiple servers to distribute the workload. As the workload increases, you can keep adding more nodes and more shards, thus providing a system that seems to be infinitely scalable. This is known as horizontal scaling or scaling out, as opposed to vertical scaling or scaling up, where you make the database server bigger to handle greater workloads. Spanner is a relational database that scales horizontally by adding nodes. Cloud SQL databases scale vertically increasing memory, vCPUs, and disk space.

### Quiz - [Module 1 Quiz](https://www.cloudskillsboost.google/course_templates/145/quizzes/515086)

#### Quiz 1.

> [!important]
> **You're deploying an application for high availability and have created web servers in multiple zones in a region. What should you do for the database to also be highly available?**
>
> - [ ] Create a failover replica in another zone.
> - [ ] Create a failover replica in another region.
> - [ ] Create a read replica in another region.
> - [ ] Replicate the database on each of the web servers.

#### Quiz 2.

> [!important]
> **What are some things you should do when migrating an enterprise database to the cloud?**
>
> - [ ] Use unit and integration testing for all stored procedures and data migration code.
> - [ ] Make sure you have a dependable backup and restore process in place.
> - [ ] Take an inventory of client applications.
> - [ ] All of the above.

#### Quiz 3.

> [!important]
> **You have a microservice responsible for managing customer accounts and orders. Strong consistency, ACID transactions, and strong schemas are all important. What type of database would you use?**
>
> - [ ] Relational
> - [ ] Object
> - [ ] Document
> - [ ] NoSQL

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/145/video/515087)

- [YouTube: Module Review](https://www.youtube.com/watch?v=XXVN1hVIErc)

In this module, you got a high level overview of cloud database migration, use cases, customers, and competitors. You learned about traditional database architectures and how to optimize databases for the cloud. You also learned to architect cloud databases for high availability, scalability, and durability.

## Google Cloud Data Migration Solutions

In this module, you learn about the various solutions Google provides for running your databases.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/145/video/515088)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=moO0oJfdwAM)

Hi, I'm Rashmi Turgalmuth, and I'm a Customer Engineer at Google. Welcome back to Enterprise Database Migration. There are many ways to run databases in the cloud, each with its own advantage. In this module, you'll learn about various solutions Google provides for running your databases. You'll get some hands-on experience using these database solutions and learn what you need to evaluate each one for your own use cases. You'll learn to use Compute Engine to run your databases on virtual machines using Google's infrastructure. You'll learn to configure databases to run in a Kubernetes cluster using GKE. You'll automate the creation and administration of databases using Cloud SQL. You'll learn how you can run Oracle databases using Google's bare-metal solution. And you'll use the Google Cloud Pricing Calculator to estimate the cost of running a database using these various solutions. Google provides many ways to run your relational database workloads. The one you choose depends on how much flexibility you need for your specific use cases. On one end, managed databases are more automated and require less administration from you. Next, Compute Engine and Kubernetes Engine gives you greater control over your deployment, but you have to do more work. And on the other end, with the bare-metal solution, you provision physical machines that you have complete control over.

### Video - [Leveraging Google Cloud Infrastructure as a Service](https://www.cloudskillsboost.google/course_templates/145/video/515089)

- [YouTube: Leveraging Google Cloud Infrastructure as a Service](https://www.youtube.com/watch?v=b4VbNGZziRI)

Let's start by running databases on Google's infrastructure using Compute Engine. Compute Engine is the Google Cloud infrastructure as a service product. You can run Linux or Windows virtual machines. Several different Linux distributions and Windows versions are supported. Compute Engine supports very large virtual machines with up to 96 cores, hundreds of gigabytes of RAM, and hundreds of terabytes of disk space. For SQL Server users, Google maintains pre-configured virtual machine images with SQL Server already installed. It's also easy to automate the creation of virtual machines using scripts or Terraform, which you will see in this class. Configure your virtual machines as required to run your databases. Give the machine a name that describes its purpose. Specify the zone where you want the machine to live. Pick a boot disk. The boot disk is simply a persistent disk image that is configured with an operating system already installed. Finally, specify how much disk space is required for your machine. If you want to run SQL Server, you can create a virtual machine and install SQL Server on it. What makes it easier than that, though? There are pre-built SQL Server images you can choose from. You will find multiple versions from SQL Server 2014 and up to the latest versions. There are images configured to run the different SQL Server editions, Express, Web, Standard, and Enterprise. By default, SQL Server and Windows licenses are included in the hourly rate. However, you can bring your own license through the Microsoft License Mobility for Google Cloud program. SQL Server images are also built using Google's shielded VN security features. This ensures that the images have not been tampered with, and if they ever are, they will not boot. For automating the installation of components, you can include a startup script when configuring the machine. The script can be added to the machine configuration directly, or you can refer to an external script stored in Cloud Storage. For Linux machines, use Bash shell scripts. For Windows, you can use Batch, Command, or PowerShell scripts, whichever you prefer. Using the Google Cloud SDK, you can easily script the creation of your Compute Engine virtual machines and all other infrastructure resources like networks, firewall rules, instance templates, and instance groups. To make scripting easy, you can use Google's web console to configure the resources and click the command line link at the bottom of the window. The SDK command for the resource as configured will be provided for you. Another way to deploy virtual machines to compute is through the Google Cloud Marketplace. Cloud Marketplace lets you quickly deploy functional software packages that run on Google Cloud. Even if you're unfamiliar with services like Compute Engine or Cloud Storage, you can start up a familiar software package without having to manually configure the software, virtual machine instances, storage, or network settings. Search for a product and select one that meets your business needs. When you launch the deployment, you can use the default configuration for that product or customize the configuration based on the available options.

### Video - [Lab intro: Creating Databases on Compute Engine](https://www.cloudskillsboost.google/course_templates/145/video/515090)

- [YouTube: Lab intro: Creating Databases on Compute Engine](https://www.youtube.com/watch?v=ztSV6TteAGc)

In this lab, you learn to create databases on Compute Engine. You create a MySQL database in Linux and a SQL Server database on Windows. You will use the Google Cloud SDK to automate the creation of your servers.

### Lab - [Creating Databases in Compute Engine](https://www.cloudskillsboost.google/course_templates/145/labs/515091)

In this lab, you will create database servers running on Google Cloud Compute Engine. You will create both Linux and Windows servers and will use the CLI to automate the creation of a server.

- [ ] [Creating Databases in Compute Engine](../labs/Creating-Databases-in-Compute-Engine.md)

### Video - [Lab review: Creating Databases on Compute Engine](https://www.cloudskillsboost.google/course_templates/145/video/515092)

- [YouTube: Lab review: Creating Databases on Compute Engine](https://www.youtube.com/watch?v=-Ral0KPFJuE)

In this lab, you created a MySQL database on Linux, created a SQL Server database on Windows, and automated server creation using the Google Cloud SDK. The key takeaway is that there are almost no limitations when running databases using Compute Engine. You can configure your database just as you would if you're running them in your own data center. The exception is Oracle. Oracle does not certify its database to run on virtual machines in Google Cloud. Later you will learn how to use the Bare Metal Solution to run Oracle databases on Google Cloud. In the next section, you will learn to run databases in a Kubernetes cluster.

### Video - [Running Databases in a Kubernetes Cluster](https://www.cloudskillsboost.google/course_templates/145/video/515093)

- [YouTube: Running Databases in a Kubernetes Cluster](https://www.youtube.com/watch?v=Z21KoieswAw)

Kubernetes is an open-source, cross-platform framework for running applications inside a cluster of shared resources. In this section, you learn to create a Kubernetes cluster on Google Cloud and configure a database to run inside that cluster. Kubernetes automates the deployment of applications running in containers. To use Kubernetes, you must first package your application inside a Docker image. In the case of a database, you can probably find a Docker image with the database software already configured. These images will be provided by the vendor or another third party and will be available for download over the Internet. After you have the Docker image, you need a configuration file in a YAML format that describes how you want the image to be deployed in the cluster. This is perhaps the hardest part of learning Kubernetes. Luckily, configuration is standard and you will find completed examples that you can adapt for your specific workloads. After you have the configuration code, you can use a few simple CLI commands to deploy, manage, and delete your deployments. You use the CLI to upload your configuration to the cluster. The configuration describes what you want and Kubernetes ensures that this is what you get. If your deployments become out of sync with the configuration, Kubernetes fixes it. If the configuration is changed, Kubernetes changes the deployment to match the new configuration. After you deploy your applications, Kubernetes ensures that they remain healthy. If they become unhealthy, Kubernetes fixes it. People new to Kubernetes and even people who use Kubernetes might ask, why would you run a database in a Kubernetes cluster? There are many advantages to Kubernetes. Automation may be the most important reason to use Kubernetes. It is important for deployments to be reproducible and testable. After the initial configuration is complete, removing, updating, and deleting deployments can be done with simple scripts. This allows you to move your deployments to different environments like development, testing, and production quickly and easily. Kubernetes is also open source and cross-platform. Kubernetes is an extremely popular open source project managed by the Cloud Native Foundation with many contributors. It is supported by Linux and Windows and can be used in many different public and private clouds. Kubernetes is supported by all the major cloud providers. AWS has a service called EKS, Elastic Kubernetes Service. Microsoft has Azure Kubernetes Service, AKS, and Google has GKE. Google Kubernetes Engine. Kubernetes is also supported by private clouds like Red Hat's OpenShift and is supported by automation tools like Pivotal Cloud Foundry. Kubernetes clusters can also be run on-prem. You can install Kubernetes yourself on a cluster of machines running in your own data center. Kubernetes configurations and scripts are also the same regardless of where the cluster is running. This allows you to have one tool and one set of scripts and deploy your software to any environment. Many customers want this type of hybrid deployment scenarios provided by Kubernetes. You can configure the software once and deploy it to any cloud or even on-prem with little or no change to the configuration. To use Kubernetes, you first need a cluster. A cluster requires one or more control planes and some number of worker nodes. Commands and configuration are sent to the control plane. The control plane stores the configuration and deploys the applications to the worker nodes. The control plane then monitors the applications to ensure that they are running as expected. If something fails, the control plane fixes it. When you use Google Kubernetes Engine, Google provides the control plane as a service. Google ensures that the control plane is highly available and scalable. Use Google Kubernetes Engine to create clusters on Google Cloud. You specify the location, number of nodes and the size of each node. The nodes themselves are really just pre-configured virtual machines running in Compute Engine and managed by the GKE service. The control plane node is automatically provided by Google. You just configure the workers. There are many additional options you can set. The details are really beyond the scope of this course though. Like all other Google Cloud resources, you can easily script the creation of the cluster using the Google Cloud SDK and the CLI. The commands for creating a cluster are simple. gcloud container clusters create, followed by the name of the cluster and parameters. A simple example is shown on this slide. Note, there are defaults for almost all parameters. So in the command line here, the zone and Google Cloud project are specified. For everything else, like machine size, number of nodes, security settings, etc., the defaults are being used. After your cluster is created, you have to connect to it in order to send commands to it. Notice that the command here is the same as the previous command, but instead of create, we are calling get credentials. You could have multiple clusters, so you have to connect to the right one before you start sending commands to it. After you're connected to your cluster, you use the kubectl CLI to send commands to it. The CLI is included with the Google Cloud SDK and already installed in Cloud Shell. You could also download the CLI from Cloud Native Foundation, which is responsible for maintaining it. YAML is used to configure an application to run in a Kubernetes cluster. For a database, you need to configure a disk space. This is done with persistent volume claim. Then you configure the deployment. The deployment, among other things, specifies what Docker image you want to run. Lastly, you configure a service. The service provides access to the database from the client applications that need to connect to it. In the example shown here, a persistent volume claim is being configured. This results disk space on the cluster that will be used by your database. Note the name, MySQL Data Disk. The access mode makes it a read-write disk used by one deployment. In this case, just one GB of disk space is being allocated. Shown here is a portion of the deployment configuration. Notice the image property near the bottom. In this case, the MySQL version 5.7 Docker image will be downloaded and run in the cluster. Also, notice the container port property. This is the port that MySQL runs on by default, port 3306. Shown here is more of the deployment configuration. Notice that in the volume section, the persistent volume claim, MySQL Data Disk, that you saw a minute ago, is being attached as a volume called MySQL Data. Then above that, that volume is being mounted. The mount path slash var slash lib slash MySQL is where your MySQL database will write its data. Here again is more of the deployment configuration. Notice that an environment variable is being created for the password for the MySQL root user. The value of the password is stored separately as a Kubernetes secret. Here, the secret is being set for the root user password. It is a best practice to keep the sensitive data out of the configuration files. That's why the password is set separately. At this point, you've seen the configuration for the deployment and the volume. The last thing to configure is a service. Services provide access to your programs that are running in the cluster. Clients don't connect directly to the application in the cluster. They go through the service. Each service is assigned an IP address. Requests are made to the service, which proxies the request to the deployed application. In this case, that application will be the MySQL database configured previously. You can store the configuration code in one or more files. When you're ready to deploy the application, use the command kubectl apply and specify the configuration file as shown here. The same line with delete instead of apply would remove everything that was configured from the cluster. If you're new to Kubernetes, you might be overwhelmed by all the configurations. I know I was when I was first learning it. Fortunately, there is an easier and better way to deploy a database into a cluster. Helm is an open source package manager for Kubernetes, similar to apt-get in Linux or Chocolaty on Windows. You can configure Helm to run on your cluster and then automate deployments using Helm charts. Helm charts provide pre-configured deployments for hundreds of applications, including many databases like MySQL, SQL Server, and PostgreSQL. A huge community provides the charts. You could also create your own charts. You first run Helm init to initialize Helm on a Kubernetes cluster. Helm is already installed in a Google Cloud Shell. So if you're using the Cloud Shell, you don't even have to install Helm itself. After Helm is initialized, you simply use the Helm install command while specifying the name of the chart and any parameters that are required. In the example above, MySQL is being deployed. This eliminates the need for you to write your own configuration files. You can search for the charts on the Helm Hub website.

### Video - [Lab intro: Running Databases in GKE](https://www.cloudskillsboost.google/course_templates/145/video/515094)

- [YouTube: Lab intro: Running Databases in GKE](https://www.youtube.com/watch?v=oZfA5CTihho)

In this lab, you will run databases in a Google Kubernetes Engine cluster. First, you will need to create the cluster. Then, you will configure and deploy MySQL into the cluster. After you do the configuration manually, you see how to automate the deployment using Helm.

### Lab - [Running Databases in GKE](https://www.cloudskillsboost.google/course_templates/145/labs/515095)

In this lab, you will create a Kubernetes cluster and then deploy databases into it. You will see two ways to deploy the databases. First using your own configuration code and then using a Kubernetes package manager called Helm.

- [ ] [Running Databases in GKE](../labs/Running-Databases-in-GKE.md)

### Video - [Lab review: Running Databases in GKE](https://www.cloudskillsboost.google/course_templates/145/video/515096)

- [YouTube: Lab review: Running Databases in GKE](https://www.youtube.com/watch?v=W3yU_QZ1zMI)

In this lab, you created a GKE cluster, deployed MySQL onto the cluster, and used Helm to deploy MySQL on the cluster. There are many advantages of using Kubernetes. These include automated resource creation, application deployment, and health monitoring. Kubernetes is also ideal if you want a cross-cloud or a hybrid cloud environment. In this lab, you saw how to easily run databases in a Kubernetes cluster. Next, you will learn to use Google-managed database services.

### Video - [Managed Relational Databases](https://www.cloudskillsboost.google/course_templates/145/video/515097)

- [YouTube: Managed Relational Databases](https://www.youtube.com/watch?v=zfvN5HvOfJQ)

Hubel Cloud provides a couple of managed database as-a-service products that can simplify and automate the creation and administration of your relational databases. Cloud SQL is the first managed relational database service. Cloud SQL databases are easy to set up, either in the console or with a command-line command. Cloud SQL databases are scalable up to 96 cores and about 30 terabytes of disk space. Setup is simple and can be done from the command-line or console. You can configure Cloud SQL to automatically create a failover database in another zone. Backups and maintenance tasks, like installing patches, are automatically taken care of for you. Cloud SQL databases are also secure by default. A firewall allows access to the database only from the project where the database is created. You can add clients from other networks as required for your specific use case. Cloud SQL supports MySQL, PostgresQL, and SQL Server. You choose your preferred database when creating a Cloud SQL instance. This list for various features varies slightly depending on the database you choose. If you choose SQL Server, you will also be charged for the license. See the product documentation for more details. When creating the database, you can use the Google Cloud Console or a simple CLI command as shown here. See the documentation for a complete list of parameters for your preferred database. When configuring a database, you can enable automatic backups and specify when you want them to run. Backups are done by creating snapshots of the persistent disk being used by the database server. Be aware that these backups can only be restored to another Cloud SQL instance. If you wanted to restore a backup on a server other than one running in Cloud SQL, you would run a traditional SQL backup command. Backups are done daily. You can enable point-in-time recovery. If you enable this feature, in the event of a disaster, you can restore the database back to any point in time before the disaster. If your database was corrupted and you had to recover from a backup, you would be down for the amount of time required to run the restore. It shouldn't take very long for the restore to run, but you would first have to discover the problem and determine that you needed to run the restore. That might take considerably longer. If you can't tolerate much downtime, you can enable high availability when deploying your database. This creates a failover database in another zone. If the main database becomes unavailable Cloud SQL automatically switches to the failover until the main comes back online. It takes only about a minute for the system to perform the failover operation, so downtime is minimized. When configuring a database, start small and scale up as needed by adding machine resources. Cloud SQL supports up to 128 vCPUs and 864 GB of RAM. Disk size ranges from 10 GB to 64 TB. As you increase the number of CPUs, the amount of network throughput automatically increases. When configuring disk storage, you can choose standard or SSD drives. SAP drives are much faster, but also significantly more expensive. If you enable automatic storage increases, the service will allocate more disk space for you as needed. For users who need to support high volumes of reads, you can automate the creation of read replicas. Read replicas can even be in multiple regions around the world. The replicas are automatically synchronized with the main. Each read replica will be allocated a different IP address. When configuring your applications, send all the writes to the main and send the reads to the replica closest to the user. As we said earlier, a firewall blocks access to the database. By default, there is no access to the database from outside the project. You can authorize networks as needed using CIDR addressing. Google also provides the Database Migration Service, which can help you migrate an on-premises database or a database running in another cloud, or from one Cloud SQL database to another one in a different project. Spanner is a global, massively scalable relational database developed by Google for Google. It is the second choice we discuss for managed relational database solutions in Google Cloud. Spanner is completely managed. You don't have to worry about patches, backups, or any other administrative chores. Setup is simple. You just answer a couple of questions, and the database is created in a few minutes. Spanner provides options for cross-region deployments. Spanner is massively scalable. You scale out horizontally by adding nodes. Each node can accommodate approximately 2 terabytes of data, and you could have thousands of nodes, assuming, of course, that you can afford it. To create a Spanner database, you only have to specify a name, a location, and the number of nodes. Like all other resources, you can use the Google Cloud Console or the command-line interface. When specifying location, you can choose a regional or multi-regional deployment. Regional deployments have all nodes and data in a single region and offer a 99.99% availability SLA. Multi-regional deployments replicate nodes and data in more than one region. These offer a 99.999% availability SLA. You can choose multiple regions in Europe or the U.S. This will provide five nines of availability on a single continent. As an example, a large bank that operates only in the United States or only in Europe might choose this type of deployment in order to have extremely high availability for their customers. If a company had users all over the world, they could choose a deployment replicated across the U.S., Europe, and Asia. This will provide low latency and high performance to users everywhere. When you create an instance, you specify its compute capacity as a number of processing units or as a number of nodes, with 1,000 processing units being equal to one node. For instances smaller than one node, 1,000 processing units, Spanner allocates 409.6 gigabytes of data for every 100 processing units in the database. For instances of one node and larger, Spanner allocates 4 terabytes of data for each node. Increased storage capacity, 10 terabytes per node, is available in select regional and multi-region Spanner instance configurations.

### Video - [Lab intro: Creating a Cloud SQL Database](https://www.cloudskillsboost.google/course_templates/145/video/515098)

- [YouTube: Lab intro: Creating a Cloud SQL Database](https://www.youtube.com/watch?v=VS1tjYEaMGE)

In this lab, you create a PostgreSQL database using Cloud SQL. You also see how to connect to it from a client machine using the Google Cloud SDK.

### Lab - [Creating Cloud SQL Databases](https://www.cloudskillsboost.google/course_templates/145/labs/515099)

In this lab, you will use Google Cloud SQL to create databases that are managed by Google. You will create both PostgreSQL and MySQL databases and you will connect to those databases using the Google Cloud CLI and from a virtual machine.

- [ ] [Creating Cloud SQL Databases](../labs/Creating-Cloud-SQL-Databases.md)

### Video - [Lab review: Creating a Cloud SQL Database](https://www.cloudskillsboost.google/course_templates/145/video/515100)

- [YouTube: Lab review: Creating a Cloud SQL Database](https://www.youtube.com/watch?v=XqtDBKF0xhY)

In this lab, you created a Cloud SQL PostgreSQL database and connected to the database using the Google Cloud SDK. Cloud SQL provides a more managed database solution compared to the one with Kubernetes Engine or Compute Engine. While there are some limitations, it automates backups and other administrative chores. It also allows you to easily create failovers and read replicas. Cloud SQL supports MySQL, PostgreSQL, and SQL Server. Next, you will learn how to use bare metal solution to run Oracle.

### Video - [Bare Metal Solution](https://www.cloudskillsboost.google/course_templates/145/video/515101)

- [YouTube: Bare Metal Solution](https://www.youtube.com/watch?v=wJ03fWC1ikY)

In theory, an Oracle database would run on a Compute Engine virtual machine. However, Oracle has not approved Google Cloud as an official platform, so that is not a viable option. It would not be consistent with Oracle's license agreement, and it would not be supported. There is a workaround though, for customers who want to take advantage of Google Cloud for their applications and still keep their Oracle databases. It is the Google Cloud Bare Metal solution. As the name implies, Bare Metal solution provides physical servers. There are no limitations. You connect to these machines as if they were in your own data center, and you can configure them anyway you like. For Oracle databases, you can use any version you like, even older versions that are not supported by managed services like Amazon's RDS. All Oracle features are also supported. Most importantly, real application clusters RAC, which is not supported by managed Cloud database services running Oracle. The hardware that is provisioned for you is certified and optimized to run Oracle, so you won't have any license and support issues. Also, for managed Cloud services, Oracle charges for licenses by the virtual CPU. There are two vCPUs per CPU core. When running on physical hardware, you pay by the physical core. This is a significant savings on the license cost compared to managed Cloud based solutions. The Bare Metal servers actually run in a partner's data center that is geographically close to a Google region. Google guarantees less than two millisecond latency between the partner data center and Google. At the time of this writing, Bare Metal solution is available in nine regions. The data centers where the Bare Metal solution servers run our enterprise grade in terms of security and reliability and are backed with Google support. When you use bare metal solution, the servers take some time to be provisioned, but migration times traditionally can be completed in only eight to ten weeks. The partner data centers where the Bare Metal solution servers run are called regional extensions. They are connected to Google via a high speed interconnect and are geographically close for a very fast connection. The partner provisions the servers. A network is created by Google that provides access to your machines. You connect to your servers via a bastion host and configure them anyway you like. Bare Metal solution servers use the latest Intel processors. Depending on your preference, either Windows or Linux is installed on the servers for you. Storage is also provision for you along with a Netapp SAN if desired. A high speed link is set up. You can choose the amount of bandwidth required for your workloads. The customer manages the machines after the initial setup. You can configure those machines anyway you like. Finally, Google handles all the billing and support. Customers are responsible for the software applications and data in the Bare Metal solution environment. This includes licensing, security, and patch management. Customers can integrate Google Cloud Observability services for monitoring and logging. Customers are also responsible for administering the database, including backups and encryption. This chart shows the current server configurations available when setting up Bare Metal solution. Choose the server configuration and number of machines that best fits your needs. Visit the Google Cloud website for latest information on configurations as options are subject to change. When you place an order, you can request the operating system you want installed on your servers. For Linux, you can choose Oracle Enterprise Linux, Red Hat Enterprise Linux, or SUSE Linux. If you prefer Windows, you can choose Windows Server 2016 or 2019 enterprise edition. You can alternatively have a hypervisor installed for virtualization. You can choose either Oracle VM or VMware ESXi. You also have control over the networking of your Bare Metal solution. You use VPC peering to connect your Google Cloud VPCs to your servers. When placing the order, you can provide the internal IP address ranges you want in your bare metal solution environment. By default, Bare Metal solution has no Internet access. However, you can set up an Internet gateway. Bare Metal solution machines have no external IP addresses. All access to the machines would be through a bastion host or a chump server.

### Video - [Estimating Costs](https://www.cloudskillsboost.google/course_templates/145/video/515102)

- [YouTube: Estimating Costs](https://www.youtube.com/watch?v=5dwhnPtL4qI)

In this module, you have seen multiple ways to run your databases in Google Cloud. These include Compute Engine, Kubernetes Engine, Cloud SQL and Bare Metal Solution. Understanding the costs of various solution is helpful for determining which one is appropriate to use. Then using Compute Engine, your primary charges are four vCPUs, RAM, and disk space. You also need to pay for the licenses if you're using licensed software. With GKE, you pay for the virtual machines and disk space used by the cluster. When using Cloud SQL, you configure a machine type. Larger machines cost more and of course you also pay for the disk space used and the license, if you use SQL server. There are additional charges if you configure the database for high availability or if you create real replicas. Backups are stored in Cloud Storage, so you also pay a little for those. With Spanner, you pay for the nodes and the amount of storage space used. The charge increases if you deploy your database to multiple regions. Finally, with Bare Metal Solution, you choose the hardware configuration that best matches your needs. You can also configure the amount of storage required and the amount of bandwidth required between your Google project and the regional extension. You can use the Google Cloud pricing calculator to estimate the cost for various solutions. Use it to compare between the various services. Of course, this is just the price Google charges. Make sure you estimate the total cost of ownership for your choices. Balance cost, administrative overhead, and required features when evaluating which service to use. A completely managed solution like Spanner may initially look expensive, but when you add in the administrative overhead of other solution, it may be more cost effective than you expected. You'll find the pricing calculator on the Google Cloud website. Next is an activity to apply what you just learned. For this activity consider a fictitious customer and their existing systems. The customer is particularly interested in their database deployments with specifications provided on the screen and in the case study. Use the pricing calculator to estimate the cost of running the database using various services. When doing the estimate, some values are hard to predict, and almost a guess. But hey, do your best. This activity will give you a good idea of how you are being charged for each service. Doing a comparison like this for any real world projects you work on is highly recommended.

### Document - [Enterprise Database Migration Case Study](https://www.cloudskillsboost.google/course_templates/145/documents/515103)

### Quiz - [Module 2 Quiz](https://www.cloudskillsboost.google/course_templates/145/quizzes/515104)

#### Quiz 1.

> [!important]
> **You have a SQL Server database running on a Windows server on-premises. You want to move it to the cloud as quickly as possible with minimal changes, and the database must continue to run on a Windows server. You should run the database using which service below?**
>
> - [ ] Spanner
> - [ ] Cloud SQL.
> - [ ] Compute Engine.
> - [ ] Kubernetes.

#### Quiz 2.

> [!important]
> **You are creating an application to sell to customers. Customers will run it on the public cloud of their choice: either Google Cloud, Azure, or AWS. Some customers may even want to run the application on-premises. What would be the best deployment choice for the database?
**
>
> - [ ] Compute Engine.
> - [ ] Spanner
> - [ ] Kubernetes.
> - [ ] Cloud SQL.

#### Quiz 3.

> [!important]
> **You've been hired to help a customer with their migration to the cloud. They have a heavy investment in Oracle and want to move their existing Oracle databases to Google Cloud. What is your advice to them?**
>
> - [ ] Use Cloud SQL for Oracle as a managed database solution.
> - [ ] Run Oracle in virtual machines on Compute Engine.
> - [ ] Run Oracle using Bare Metal Solution.
> - [ ] Use the Oracle on Spanner migration toolkit.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/145/video/515105)

- [YouTube: Module Review](https://www.youtube.com/watch?v=4rqekVvW9pM)

In this module, you evaluated the database solutions available on Google Cloud, run databases on Google Cloud infrastructure using Compute Engine, leveraged Kubernetes and GKE for deploying databases, use Cloud SQL for managed database solutions, learned about provisioning Bare Metal solution for Oracle databases. Lastly, you estimated the cost of database solutions.

## Google Implementation Methodology

In this module, you learn about Google's recommended approach for migrating enterprise applications and databases to the cloud. 

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/145/video/515106)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=qQxnNa63KDY)

Hi everyone, Damon here. Migrating to the Cloud can go smoothly, or it can be filled with delays, bugs, downtime, and cost overruns. This is especially true when you are moving databases. Over time, it's easy to lose track of database dependencies, and when you switch the old database off, unexpected things will fail. Careful planning can help mitigate this risk. Google has helped many enterprises move to the Cloud and has developed an implementation methodology based on this experience. In this module, you learn to use the Google Cloud implementation methodology to migrate to the Cloud. You learn what are the key database migration activities. Lastly, you learn how to choose the appropriate database migration approach.

### Video - [Google Migration Methodology](https://www.cloudskillsboost.google/course_templates/145/video/515107)

- [YouTube: Google Migration Methodology](https://www.youtube.com/watch?v=saiSkwJdBiw)

Let's start by defining the Google Cloud migration methodology. Google's methodology for migrating to the cloud has four steps, assess, plan, deploy, and optimize. The assessment phase is where you determine the requirements and dependencies to migrate your apps to Google Cloud. The assessment phase is crucial for the success of your migration. You need to gain deep knowledge about the apps you want to migrate, their requirements, their dependencies, and your current environment. You need to understand your starting point to successfully plan and execute a Google Cloud migration. In this phase, you perform the following steps. Assessment steps include the following. First, building a comprehensive inventory of your apps. Second, cataloging your apps according to their properties and dependencies. Third is training and educating your teams on Google Cloud. Fourth, building an experiment and proof of concept on Google Cloud. Fifth, calculating the total cost of ownership of the target environment. And finally, sixth, choosing the workloads that you want to migrate first. To scope your migration, you must first understand how many items, such as applications and hardware appliances, exist in your current environment, along with their dependencies. Building the inventory is a non-trivial task that requires a significant effort, especially when you don't have any automatic cataloging system in place. To have a comprehensive inventory, you need to use the expertise of the teams that are responsible for the design, deployment, and operation of each workload in your current environment, as well as the environment itself. The inventory shouldn't be limited to only applications, but should at least contain the following. Dependencies of each app. Services supporting your app infrastructure. Servers, either virtual or physical. Physical appliances, such as network devices, firewalls, and other dedicated hardware. Shown here is an example inventory. In your projects, the fields may vary, but you get the idea. Show the things that are important for the success of your migration project. This table is an example of the dependencies of a database listed in the inventory. These dependencies are necessary for the database to correctly function. When you move the database, changes to the dependencies will also need to be made. This analysis is largely about identifying dependencies and dependents. It is important to catalog the applications that are dependent on the database. Care must be taken not to break those apps when the database is moved. It is also important to identify the things that the database depends on so the database continues to function correctly when it is moved. Here is an example of an app catalog. Once you understand your current environment, you can start planning your migration. To show the value and efficacy of Google Cloud, consider designing and developing one or more proofs of concept for each category of app in your app catalog. Experimentation and testing lets you validate assumptions and demonstrate the value of Cloud to business leaders. At a minimum, your POC should include the following. A comprehensive list of the use cases that your apps support, including uncommon ones and corner cases. All the requirements for each use case, such as performance and scalability requirements, expected consistency guarantees, bailover mechanisms, and network requirements. A potential list of technologies and products that you want to investigate and test. You should design POCs and experiments to validate all the use cases on your list. Each experiment should have a precise validity context, scope, expected outputs, and measurable business impact. A POC will help you calculate the total cost of ownership of a Cloud solution. When you have a clear view of the resources you need in the new environment, you can build a total cost of ownership model that lets you compare your cost on Google Cloud with the cost of your current environment. When building this cost model, you should consider both the cost for hardware and software, and also all the operational cost of running your own data center, such as power, cooling, maintenance, and other support services. Consider that it's also typically easier to reduce costs, thanks to the elastic scalability of Google Cloud resources, compared to a more rigid, on-premises data center. Now that you have an exhaustive view of your current environment, you need to complete your migration plan by choosing the initial order in which you want to migrate your apps. You can refine this order during the migration when you gain experience with Google Cloud and understand your environment and apps. The apps you migrate first are the ones that let your teams build their knowledge and experience on Google Cloud. Greater cloud exposure and experience from your team can lower the risk of complications during the deployment phase of your migration and make subsequent migrations easier and quicker. For this reason, choosing the right first movers is crucial for successful migration. You can pick one app or put many apps from across your apps matrix in your first mover list. The task of identifying the first movers is complex, but here are some criteria to guide you. Consider the business value of the app. Choose an app that provides value, but don't choose a mission-critical app where the risk is too great. Consider the teams responsible for development, deployment, and operations of the app. Choose teams with Google Cloud knowledge and choose teams motivated to use Google Cloud. You may also want to consider using a Google Cloud expert partner to help. Make sure you understand the number, type, and scope of dependencies of the app. More dependencies can create a more difficult migration. Be careful to understand the compliance and licensing requirements of the app. Make sure your licenses allow you to run in the cloud and don't violate any compliance rules you are subject to. When planning your migration to Google Cloud, you need to understand an array of topics and concepts related to cloud architecture. A poorly planned foundation can cause you to face delays, confusion, and downtime, and can put the success of your cloud migration at risk. Shown here is a checklist of organizational functions that need to be in place before you can start moving your applications and databases. If you put this foundation in place before your migration project, all you need to worry about is the migration. If you don't, you'll be tasked to add these foundational items as you go, making the migration harder and causing delays along the way. Cloud Identity allows you to centrally manage users and groups who can access cloud resources. You manage users and groups from the Google Admin website. And you can synchronize users and groups from your on-premises Active Directory or LDAP servers. You can delegate the responsibility for identifying users to Google. Or you can easily set up a third-party single sign-on identity provider. The third party can be another cloud service, like Microsoft Active Directory Online, or it can be a service managed by you in your data center. In any case, to use Google Cloud, your organization has to be set up under Cloud Identity. After that, you will automatically have a Google Cloud organization set up. The Google Cloud organization hierarchy can greatly simplify the management of resources, users, and groups. At the top of the hierarchy is the organization. Remember, this is created automatically when you set up your Cloud Identity account. An organization contains zero or more folders. Folders can have subfolders, which can have subfolders, and so on. Projects can be created within any folder or subfolder or directly below the organization. Resources are created within projects. Permissions can be granted at any level of the hierarchy, and those permissions flow downward. You also need to identify the people who will manage the organizational hierarchy. There are roles like organizational admin, folder creator, and project creator that have to be assigned to the right people who will manage this on behalf of the development teams. Identity and access management is a service that is used to grant permission to team members. IAM controls who can do what to which resources. Your users are added as Google Cloud Identity accounts and authenticate to Google Cloud with their login. Add members to groups to simplify management of permissions. Groups are managed by Google or by using your third-party SSO provider. Users or groups are assigned one or more roles. A role is simply a list of API functions. If the function is in the list, it can be called by members of that group. Again, the assignment of roles to members can be done at the organization, folder, project, or resource level, whichever is most appropriate. Once you have a plan, it's time to start deploying resources to the cloud. A key aspect of making your deployment successful is automation. If you are only going to create a Google Cloud resource once, the fastest and easiest way would be to manually create it with the web console. In real projects, things aren't that simple, though. You make mistakes along the way, change your mind as to how you are architecting your solution, new versions of things come along, and you want to move your resources from one project to another project. If you're creating your resources manually, your projects will take much longer, maintenance will be harder, change will be more difficult, and your projects will be more prone to human error. Almost every Google Cloud resource can be created manually using the web console. The console is a great way to see how things work and to do experiments. However, every resource can also be created using code, and there are different ways to write that code. One way to automate resource creation is through scripting. You saw earlier in the course how you could use the Google Cloud SDK and some simple CLI commands to create virtual machines and Cloud SQL databases. All resources can be created with similar commands. You could easily stream together multiple commands inside a code file and run them as a shell script. You could also download a Google Cloud client library for your favorite programming language and write a program using it to create resources. Many languages are supported, including Java, Python, . NET, and Node.js. You can also use infrastructure as code tools to automate resource creation. With IAC tools, you define templates which describe what resources you want to create in a project. Then, with a simple command, you can create, delete, or update your resources based on the templates. This is similar to programming a shell script, but uses a more declarative approach to describe what you want. The advantage of Terraform is that it works with any cloud provider and also works in private clouds. Often, companies already have experience using Terraform and want to leverage that prior knowledge when migrating to Google Cloud. Terraform is considered a first-class citizen on Google Cloud. It is even installed by default in Google Cloud Shell. You also saw earlier in the course how Kubernetes works. Kubernetes is a great way to automate the deployment of your applications, databases, and resources. In a way, it is similar to Terraform. You create a template that describes your applications and use simple commands to deploy them. There is even a Terraform provider that can be used in place of the Kubernetes templates and kubectl CLI. Whichever tool you use, scripting, IAC, or Kubernetes, the important thing is that you're not doing things manually. Instead, you're automating as much as possible. In many cases, you will end up with a combination of these tools. After you've done your initial deployment, the optimization phase begins. Optimization is an ongoing process of continuous improvement. You optimize your environment as it evolves to avoid uncontrolled and duplicative efforts. You can set measurable optimization goals and stop when you meet those goals. After that, you can always set new and more ambitious goals. But consider that optimization has a cost in terms of resources, time, effort, and skills. This diagram shows the optimization loop. Assess the current environment. Set your optimization goals. Optimize your environment. If you meet your goals, reassess the environment. If not, try again.

### Video - [Database Migration Activities](https://www.cloudskillsboost.google/course_templates/145/video/515108)

- [YouTube: Database Migration Activities](https://www.youtube.com/watch?v=ZzbTeKaHmm0)

Several activities have to be performed to migrate a database to Google Cloud. Let's talk about those now. There are five steps required to migrate a database. First, transfer the data from the old site to Google Cloud. Second, resolve any data integration issues you might have. Third, validate the migration. Fourth, promote the new database. And fifth, retire the old database. Transferring the data can be easy or hard, depending on the amount of data you have. If you only have a few gigabytes, you don't have a problem. But if you have terabytes of data, you had better have a fast network connection to get that initial data transferred quickly. The table here shows the theoretical data transfer time required for given amounts of data based on your bandwidth. If you have many terabytes of data, you can consider setting up a high-speed cloud interconnect connection between your data center and Google. Interconnect connections are sold in 10 gigabyte increments, and you can have a maximum of 200 gigabytes per second bandwidth. Even with a connection that fast, if you have petabytes of data, it's going to take too long. If your data transfer job will take too long over the internet, you can order a transfer appliance. When migrating large amounts of data to Google Cloud, consider using Cloud Storage as a staging area. After the data is in storage, you can now move it around at Google speed. Cloud Storage is also relatively inexpensive compared to other storage locations. The simplest way to copy data into storage is using the gcloud storage command line interface. You can make this part of your automation scripts. If you are familiar with basic Linux file system commands, you will easily pick up gcloud storage commands. There are also a few more automated transfer services. Storage transfer service can be used to transfer data over the internet from an AWS S3 bucket, Azure storage, or another Cloud Storage bucket, or from an on-premises file server. The transfer jobs are simple to create and can be run on a recurring schedule if needed. A transfer appliance is a physical storage device that you copy your data to. Google sends you the appliance and you mount it on your network. You copy the data, encrypting it with an encryption key you generate. Send it back to Google and then you can copy the data into a Cloud Storage bucket and use your key to decrypt it. You keep the encryption key, so if your appliance is lost or stolen and transferred, no one will have access to the unencrypted data. Often, customers want to alter their database structure or move to a different database when moving to the cloud. If the data needs to be altered prior to loading to the new database, you can automate that with an ETL pipeline. Google Cloud provides multiple tools for doing that. These are Dataflow, Data Fusion, and Cloud Composer. There are also third-party tools like Stream or vendor-specific tools from Microsoft or Oracle. Later in the course, these tools are discussed in more detail. The third activity is to validate the data migration. This can be done with thorough automated testing. Unit tests are used to verify each individual function, property, stored procedure, trigger, etc. Integration tests ensure that the different components work together. Clients can use services, services can use their databases. Regression tests ensure that the new platform is consistent with the older one. New functionality does not break other, older code. For lower-level tests, use an automated test framework that works with your favorite programming language. There are many automated, third-party testing tools that you can use for system, security, and black-box testing. The last two activities in the database migration process are to promote the new database and retire the old one. You can use site reliability engineering techniques to ensure that the new server works before the switchover. Blue-green deployments can mitigate the risks of that switchover. With a blue-green deployment, you run both environments at the same time. You test the new one before migrating workloads on it. When you're ready, switch the traffic. Thus, the blue becomes the green, and vice versa. If something fails, you can quickly switch them back. If not, you can eventually turn off the old environment. With a canary deployment, you can run both the old and new environments simultaneously. Gradually migrate production requests to the new environment, and monitor for errors. If there are no errors, increase traffic to the new environment until it eventually takes over completely. At some point, you're safe to retire the old environment.

### Video - [Database Migration Approaches](https://www.cloudskillsboost.google/course_templates/145/video/515109)

- [YouTube: Database Migration Approaches](https://www.youtube.com/watch?v=eDnwpEqBbyc)

There are some different approaches you can take to database migration. Let's talk about those now. Broadly speaking, there are four approaches you can take when migrating a database. They are scheduled maintenance, continuous replication, split reading and writing, and data access microservice. The one you choose largely depends on the amount of downtime you can tolerate and the complexity of your current environment. Here are four different use cases. For each one on the left, choose the amount of downtime you think you could tolerate on the right. Pause the video and take a couple of minutes to do this. With so little information, maybe you can't be sure of the answers. But in the first case, the application is internal to the company without a large number of users. It would probably be okay if the application was taken down for a weekend. The second case describes a customer-facing critical application. You might be able to tell users that the application is unavailable for an hour or so. The business might tell you that there can't be any downtime though. The third case describes a data warehouse used for reporting and analytics. I'll assume users are internal and we could function with a few hours of downtime, maybe even a weekend. The last case describes a database that is critical to sales in our online store. If I'm running the business, I don't want to lose orders. I want as close to no downtime as possible. Use scheduled maintenance if you can tolerate some downtime. Define a time window when the database and applications will be unavailable. Migrate the data to the new database, then migrate client connections. Lastly, turn everything back on. This approach is simple. The length of the window will be dependent on the amount of data you need to transfer. When the database is very large, you can do an initial migration of historical data. Then when the time comes to do the migration, you only need to worry about the new data added since the initial transfer. Continuous replication uses your database's built-in replication tools to synchronize the old database to the new database. This is relatively simple to set up and can be done by your database administrator. There are also third-party tools like Stream that will automate this process. Eventually, you will move the client connections from the old database to the new one. Then you can turn off the replication and retire the old site. With split reading and writing, the clients read and write to both the old and new databases for some amount of time. Eventually, you can retire the old database. Obviously, this requires code changes on the client. You would only do this when you're migrating to different types of databases. For example, if you're migrating from Oracle to Spanner. If you are migrating from Oracle on-premises to Oracle on Google Cloud Bare Metal Solution, continuous replication would make more sense. If you have a large number of clients, split reading and writing is impractical. It would require too many code changes. In that case, you can create a data access microservice. All data access is encapsulated or hidden behind the service. First, migrate all client connections to the service. The service then handles migration from the old to the new database. Essentially, this makes split reading and writing seamless to the clients. This is a complex approach, but it is useful when you have many clients to migrate and can tolerate minimal or no downtime. In this activity, you analyze a case study and perform some of the planning tasks we covered in this module. The Enterprise Database Migration case study is available for download from the course outline. The case study represents a fictitious enterprise customer who wants to move to the cloud. Like all large organizations, it has a mix of many different applications and databases of varying importance. Review the case study and then follow the instructions for Activity 1.

### Document - [Enterprise Database Migration Case Study](https://www.cloudskillsboost.google/course_templates/145/documents/515110)

### Video - [Activity review: Planning a database migration project](https://www.cloudskillsboost.google/course_templates/145/video/515111)

- [YouTube: Activity review: Planning a database migration project](https://www.youtube.com/watch?v=uaCeUDgJJCk)

In this activity, you analyze the case study and perform some of the planning tasks we covered in this module. The case study is a fictitious enterprise customer who wants to move to the Cloud. Like all large organizations, it has a mix of many different applications and databases of varying importance. In Activity 1, you were asked to create an application inventory, outline database dependencies, and build an app catalog. Here is an example application inventory that was done by a prior student. You may pause the video to look at it in detail. Here are the database dependencies. You may pause the video to look at it in detail. Lastly, here is the completed app catalog. You may pause the video to look at it in detail.

### Quiz - [Module 3 Quiz](https://www.cloudskillsboost.google/course_templates/145/quizzes/515112)

#### Quiz 1.

> [!important]
> **What are the four steps in Google's cloud migration methodology?**
>
> - [ ] Assess, Plan, Deploy, Optimize.
> - [ ] Manage, Engineer, Develop, Test.
> - [ ] Requirements Gathering, Analysis, Program, Test.
> - [ ] Program, Deploy, Fix, Repeat

#### Quiz 2.

> [!important]
> **You need to deploy an application to Google Cloud. The database is relatively small, and only one application uses it. It is not mission-critical, so some downtime can be tolerated. Which migration approach should you use?**
>
> - [ ] Data access microservice.
> - [ ] Split reading and writing.
> - [ ] Continuous replication.
> - [ ] Scheduled maintenance.

#### Quiz 3.

> [!important]
> **What is a good reason to automate a cloud migration?**
>
> - [ ] All of the above.
> - [ ] Repeatable.
> - [ ] Less prone to human error.
> - [ ] Easier to make changes over time.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/145/video/515113)

- [YouTube: Module Review](https://www.youtube.com/watch?v=5W4RgIMIxu8)

In this module, you learned how to migrate to the cloud using Google's implementation methodology. Google's methodology consists of four steps. Assess, plan, deploy, and optimize. You also learned what the key database migration activities are. They are transfer data from old site to new, resolve data integration issues, validate data migration, promote the new site, and finally, retire the old site. In the final section of the module, you learned how to choose the appropriate database migration approach, scheduled maintenance, continuous replication, split reading and writing, or data access microservice.

## Migration Strategies

In this module, you learn about different strategies you can take when moving your applications and databases to the cloud.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/145/video/515114)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=0Tb0hlBJ7fc)

Hi, this is Rashmi again. Welcome back to Enterprise Database Migration. There are a number of different strategies you can take when moving your applications and databases to the cloud. Let's talk about those now. A lift and shift strategy involves picking up and moving virtual machines from one location into Google Cloud. Backup and restore is a traditional and simple way to move a database. To migrate databases to the cloud with no downtime, you can use database replication. Once moved, you should look for ways to optimize databases for the cloud.

### Video - [Lift and Shift](https://www.cloudskillsboost.google/course_templates/145/video/515115)

- [YouTube: Lift and Shift](https://www.youtube.com/watch?v=23BYKTyVHWg)

Lift and Shift means you are moving an application or database as it is into your new Cloud environment. This is often an easy and effective way of migrating databases and other applications as well. You may already be running your databases in virtual machines on premises or in another Cloud. Sometimes the best way to move it is simply importing the entire VM into the Cloud and turning it on there. Monolithic apps like a Wordpress site, for example, might be good candidates for a lift and shift approach. Also letacy apps like an asp.net app written years ago in an older version that can't be containerized and is not practical to rewrite. Small departmental databases, apps that are already virtualized on premises or in another Cloud, and many other applications can also be moved this way. In theory, Lift and Shift is simple. First, create an image of the virtual machine in the current environment. Then export the image from the current environment and copy it to a Cloud storage bucket. Last, create a compute engine image from the exported image. Once you have a compute engine image, use it to create your virtual machine. Google Cloud images can be created from external virtual disks. Both VMware VMVK and Microsoft VHD file formats are supported. This can be done from the Console. A compute engine virtual machine is created automatically to perform the conversion. If you're running a virtual machine in another Cloud, first create an image of that VM. Then you can export that image from the native Cloud providers format to a VMware or Microsoft image. In the example above, an AWS EC2 image is exported to VMware format and saved in an S3 bucket. Once that is done, you can copy the image to Cloud Storage. There are some caveats to this and it doesn't always work. Make sure you understand the license agreements for the software running on the VMs. You could unknowingly violate a license by moving it from one platform to another. Make sure you can log into the VMs after they are moved. You might have to move SSH keys along with the image so you can log into it when you run it in Google Cloud. Sometimes it's not possible to export a VM. For example, images created from AWS marketplace cannot be exported. Google Cloud migrate to Virtual Machines software will help you automate the migration of many VMs. This works with VMware, AWS and Azure Virtual Machines. Machines are moved very quickly, and then their data is streamed into Google Cloud before going into production. Because this is an automated system, you can have test runs, and rolling back to the previous environment is easy. This is a complex service, the details of which are beyond the scope of this course. There is a separate three day VM migration training course available though. With migrate to virtual machines, VM migrations happen in waves. Identify a set of VMs to migrate first, prepare the VMs, migrate them, then test to verify that they are migrated correctly. As you learn from your experience, look for ways to improve the process. Once you've move your VMs into Google Cloud, you can begin optimizing your applications and databases for the Cloud. Migrate to Virtual Machines supports both Windows and Linux Virtual Machines. This slide lists the Windows versions and Linux distributions that are supported. Large organizations are encouraged to invest in third party migration tools and work with experienced partners. Two tools that are popular are migVisor and Striim. MigVisor is an assessment tool that helps identify dependencies and dependence and will recommend target services and databases based on the analysis. Striim is an online database migration tool. It supports many different database types as both target and source databases. It supports both homogeneous migrations, for example, Oracle to Oracle in bare metal solution and heterogeneous migrations, for example, Oracle to spanner.

### Video - [Backup and Restore](https://www.cloudskillsboost.google/course_templates/145/video/515116)

- [YouTube: Backup and Restore](https://www.youtube.com/watch?v=R13nMobAuIY)

There are different ways to migrate a database from an old server to a new one. The one you choose depends on different factors, including the size of the database, how much downtime you can tolerate, and are you migrating to a new version of the database or even to a totally different database? Backup and restore is the simplest way to move a database. Perform a SQL backup on the source database, then copy the backup files into Google Cloud. Then, run a restore on the new target database server. For large databases, use differential backups to minimize downtime. Start with a full backup and restore. This will take the longest. Then, to differential backups and restores, until it can be done quickly. At some point, schedule a maintenance event when the database is unavailable. Do the final backup and restore, and then, migrate connections to the new database. Assuming all goes well, you can retire the old database.

### Video - [Live Database Migration](https://www.cloudskillsboost.google/course_templates/145/video/515117)

- [YouTube: Live Database Migration](https://www.youtube.com/watch?v=xlq6xyLWk8o)

A live database migration means we switch databases with no downtime. Using database replication can minimize downtime compared to backup and restore. First, configure the existing database as main. Second, create the new database and configure it as the replica. Third, the main synchronizes the data with the replica. And fourth, after some time has passed, migrate the clients to the replica and promote it to the main. All modern databases support replication. This is mostly a database administration chore. Having a very large number of clients complicates the problem. Migrate large numbers of clients with no downtime by using a data access service. First, create a service that encapsulates all data access. Second, migrate clients to use the service rather than connecting to the database. Third, once all clients are updated, the service is the only direct database client. Fourth, replicate the database and then migrate the service connection. Use blue-green deployments to migrate data access services from on-premises to the cloud. Initially, a data access service exists on-premises. Duplicate the service in the cloud, test the cloud service, and use a reverse proxy or DNS to migrate client connections when ready. You can switch connections back if migration fails. Blue-green deployments reduce the risk of a migration by allowing you to quickly revert back to the older service if a mistake is made.

### Video - [Optimize Databases for the Cloud](https://www.cloudskillsboost.google/course_templates/145/video/515118)

- [YouTube: Optimize Databases for the Cloud](https://www.youtube.com/watch?v=BJF07rgx67E)

Most customers moving to Google Cloud are doing this so that they can take advantage of the advanced features. Once you have your existing applications and databases migrated, you have an opportunity to optimize to take greater advantage of the features and the cost savings that the public cloud offers. Redesigning applications using a microservice architecture is a good way to get started for optimizing your infrastructure. Large monolithic applications are broken down into some number of smaller independent services. While many microservices might run on a single Kubernetes cluster, each should be programmed, deployed, and versioned separately. Also, each microservice should be responsible for its own data. This requires large databases to be broken into smaller pieces, one for each service. To experienced DBAs, this might seem like a bad idea, but it allows different services to use different types of databases that might be more appropriate and less expensive. In the example pictured above, the monolithic application might start with an Oracle database for all data. However, when broken apart, maybe the products and review services could be migrated to a NoSQL database like Firestore. This would be significantly cheaper, easier to manage, and easier to program than Oracle. Maybe you could continue to use Oracle for the order database, which would require a strong schema and asset transactions. When designing microservices, you're looking for ways to break up an application that minimizes the dependencies between the various microservices. Analyzing the monolithic database would help you determine where the microservice boundaries are. In a large database, you'll certainly find cases where groups of data are only loosely related, if at all. Finding these islands of data will help you determine how you can split off some functionality without negatively impacting the application as a whole. If you're storing binary data inside your relational database, you might move that to Google Cloud Storage, where it is significantly cheaper to store. You would often find data specific to web applications and web sessions inside a relational database. This type of data might be better suited for Firestore. Again, it would be cheaper, but also it would be easier to program for web developers. Online analytical processing, OLAP data, can be moved from a relational system to Google BigQuery. BigQuery is massively scalable, easy to use, and storage is plentiful and inexpensive. After the database is simplified, if you're using Oracle on Bare Metal Solution or SQL Server on Compute Engine, you might consider refactoring the application to use a managed database service like Cloud SQL or Spanner. This would likely make the total cost of ownership go down significantly by saving on administrative and licensing costs. Don't try to refactor applications all at once. Rather, do it a piece at a time. Little by little, look for functionality that can be moved out of a larger application and into a separate service. And where possible, use services already provided by Google. For example, if your monolithic application is responsible for authentication and authorization, and your database is storing user information and password, refactor that to use a cloud service like Google Identity Platform. This will simplify your applications, be more secure, and likely save you money. Martin Fowler named this idea of refactoring one component at a time as this triangular pattern.

### Quiz - [Module 4 Quiz](https://www.cloudskillsboost.google/course_templates/145/quizzes/515119)

#### Quiz 1.

> [!important]
> **You need to move your WordPress site to Google Cloud. It uses a LAMP stack on a single machine, and some downtime is OK. What migration strategy would you recommend?
**
>
> - [ ] Use a data access service and replication.
> - [ ] Backup and restore.
> - [ ] Replication.
> - [ ] Lift and shift.

#### Quiz 2.

> [!important]
> **You've completed the migration of a large monolithic application and its database to Google Cloud. The database included web session management data, which you've decided to move to a separate microservice. Which managed database service would be easiest and cheapest to use to store the session data?**
>
> - [ ] Spanner.
> - [ ] BigQuery.
> - [ ] Bigtable.
> - [ ] Firestore.

#### Quiz 3.

> [!important]
> **Your company needs to move a large Oracle database to Google Cloud. There are many clients, some of which are customer-facing, core business services. Minimizing downtime is extremely important. What migration strategy would you recommend?**
>
> - [ ] Replication.
> - [ ] Lift and shift.
> - [ ] Backup and restore.
> - [ ] Use a data access service and replication.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/145/video/515120)

- [YouTube: Module Review](https://www.youtube.com/watch?v=B_SvimcIDcA)

In this module, you learn the different migration strategies that you can use to move your applications and databases to the cloud. They are lift and shift when you want to simply migrate VMs directly to Google Cloud, backup and restore from on-prem databases to Google Cloud, and live migration when you need to move databases to the cloud with zero downtime. You also learn how you can optimize applications once they are moved to the cloud using the strangler pattern.

## Networking for Secure Database Connectivity

In this module, you learn how to build a secure network for database connectivity.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/145/video/515121)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=YnqZi2HA_Xw)

Hi, my name is Julianne Cuneo, and I'm a data analytics specialist at Google. And welcome back to Enterprise Database Migration. Security should always be a top concern. So let's have a look at how to build a secure network for database connectivity. There are various levels of network security we need to focus on. First is building a secure network for the database servers and the client applications that will connect to them. Next, in cases where you need to communicate across networks, you can use VPNs and VPC peering. As always, firewall rules are a useful way to control access to the databases. And throughout this module, you will automate building the network infrastructure using Terraform.

### Video - [Building Secure Networks](https://www.cloudskillsboost.google/course_templates/145/video/515122)

- [YouTube: Building Secure Networks](https://www.youtube.com/watch?v=xedFGkq0YU0)

Building a secure network to host the database server is the first place to start. In order to build a secure network configuration, all of the resources need to be in a project. Projects contain one or more VPCs which provide virtual networking In Google Cloud VPCs are a global resource. Each VPC contains one or more subnets which are regional resources. You need to create a subnet for each region you want VMs located in. By default, machines in the same VPC can communicate with one another via their internal IP address, regardless of the region that they're in. However, machines in different VPCs can communicate through external IP addresses instead. That means that a VM with no public IP address is only reachable by other resources within the same VPC. You can use multiple networks to control which machines are reachable from other machines and to isolate machines from the outside world. Firewall rules can control which machines can communicate with one another through designated ports. By default, all ports are closed to ingress, but are open to egress. Firewall rules consist of targets which specify which machines in the VPC the rules apply to, and sources which specify which machines outside of the VPC the rules apply to. Because ingress is closed by default, you use allow rules to permit specific ingress. And because egress is open by default, you use deny rules to prevent specific egress. When you enable the Compute Engine service in a project, the system automatically creates a default network. This default network contains a subnet for each region in the world. Also, default firewall rules are created for SSH, RDP, HTTP, HTTPS and ICMP, and all internal traffic is allowed. The default network makes it easy to get started, but it should be just that, a starting point. Most production environments will need a modified set of these rules. For example, you probably don't want every network to be used and certainly don't want the overly permissive rules allowing SSH and RDP from all sources. The best option is to create your own custom VPC network. You start by giving it a name and then add a subnet for each region you want to use. Subnets requiring internal IP address range. Make sure that address ranges in different subnets don't overlap. Here's an example of using the console to fill in the relevant details for creating a firewall rule. The parameters for configuring a firewall rule include name, network, priority, ingress or egress, allow or deny, target sources, protocols and ports. Most of the configuration is straightforward. Each firewall rule has a unique name. And you should use a consistent naming convention. Each rule is scoped to a network. If multiple rules conflict, the priority determines which rule wins. Lower numbers have higher priority. Direction determines whether this is an ingress or an egress rule. Recall the ingress is blocked by default, and egress is allowed by default. Therefore, the action is either allow or deny. If you're creating an ingress rule, it's probably an allow and if it's an egress rule, it's probably a deny. Targets determine which machines in your network the rules apply to. This could be set to all of the machines or you can use tags or service accounts to specify only certain machines. Sources are used to determine which machines outside of the network the rules apply to. Sources are usually determined using IP address ranges. Finally, you specify protocols and ports. Protocols or either TCP, UDP, or ICMP, which is pink. And you can specify ports or ranges. It's not so different from defining firewall rules on your own router. The interesting thing to know is priority, which handles conflicts when multiple rules overlap. In that case, the rule of the lower priority number wins, and it's used over the higher number. You have three ways to specify both targets and sources. For targets, you can specify all machines on the network, or you can specify VMs with a particular network tag. A network tag is simply a string that you can assign to VMs. And lastly, you can specify the service account that VMs are assigned when they're created. Sources are usually specified using IP addresses or ranges using CIDR notation. With CIDR notation an IP addresses specified, followed by a slash and a number. The number determines the range. It could be from 0 to 32. Lower numbers indicate larger ranges, so a slash 32 means that that's the only address. A slash 24 means any IP addresses beginning with the first three numbers. A slash 16 means all addresses that begin with the first two numbers and so on. The range 0.0.0.0/0 means that all machines in the world are considered sources for this rule. As with targets, you can use the tags or service accounts to determine the sources. Automating makes it easier to reproduce and test a solution. Terraform is a tool from HashiCorp that allows you to automate the creation of resources. Each cloud provider has their own version of a similar tool. AWS has cloud formation and Microsoft Azure has resource manager. But Terraform is commonly used to automate cloud resources because it's supported on all of the major cloud platforms. Thus, many organizations who want to use multiple or hybrid cloud environments prefer it. So they only need to learn one tool. Terraform is included in the Cloud Shell by default. And it requires no installation or separate Terraform server in order to run. Using it will make most tasks easier and automated. Terraform uses files with instructions. And you put all the files for specific deployment into one folder. You create templates that describe what you want to create. And there are templates available for creating all types of Google Cloud resources. You add one or more of these . tf files to the folder and combine them for deployment. If there are variables, you can set them in the . tfvars file. And the . tfstate file keeps track of existing resources and it's used when resources are updated or destroyed. There are Terraform providers for every major cloud provider. This code snippet shows how to use the Google Cloud Terrarform provider. Google collaborates with HashiCorp to keep this provider up to date. In the example, the Google Cloud Project ID and region are being set. Note that the values are being set to variables that are created in another . tf file. Variables are often a better choice than hard coding values because they promote reuse of the configuration file more easily. In this code block three variables are being declared. "project_id," "gcp_region_1," and "subnet_cidr_public." This is done in the file variables . tf. Note that the values are not being set here, the variables are simply being declared. You could however, set the default value for each variable here if you wanted to. You can set the values of your variable in the . tfvars file, or, alternatively, you can set them at runtime. If you do not set the value of a variable in this file, you will be prompted for it when running the template. You could also pass the variable values as parameters when running the template. Here's an example template for creating a VPC with one subnet. And the first code block a network is created called “public-vpc.” It's a resource of the type Google compute network. The network is then referenced in the second code block when creating the subnet. Notice the network property of the subnet. It refers to the network created above it using the resource type and name. A network also has a property called "name", which is being used to set the value of the subnets network property. Using Terraform involves using the templates to make your own config scripts, then running several Terraform commands. The command Terraform init should be run from the folder where the Terraform files are. It sets the working directory, and you only need to do this once whenever you change directories. Terraform plan generates and displays an execution plan. Terraform decides on the order in which to create resources independently of how you order them in your configuration. This is part of the Terraform logic and dependencies are known by the provider you're using. The apply command builds or modifies infrastructure according to the plan that was developed from your templates. If you don't want to be prompted to approve the plan, you can include the -auto-approve option when you apply. Finally, destroy would get rid of the resources defined in a template. This is all tracked in the . tfstate file. There's also a -auto-approve option here. A good practice in securing your environment is to only allow known clients to have access to your database server through firewall rules. Here is an example template for creating a virtual machine based on the Ubuntu image. The network tag,"allow-ssh", is used as the target tag for the SSH firewall rule. Note the various properties that are being set for the machines. These are the same properties you would set in the web console. You're just doing it with some code. Here's an example of how to create a firewall rule using Terraform. In this case, all sources can connect via tcp through port 22. But only to targets with the tag, "allow-ssh." Perhaps that last example is a bit too permissive. In general, you should avoid allowing all sources. So watch out for using 0000/0, unless you have to. For example, allowing HTTPS access to Web servers from all sources is probably what you want. But use more restrictive rules for dangerous protocols like SSH, RDP or database access. Instead, use tags or service accounts. Also avoid using external IP addresses on VMs if you don't need them. In addition to weakening security, they cost money, and they also increase egress cost. When configuring subnets, try to use CIDIR ranges that are small to reduce the chance of IP overlap conflicts. Always use SSL when communicating between Google Cloud and the outside world. Within Google Cloud, all traffic is encrypted by default. But outside of the Google Network, you need to encrypt it. And when you need to connect on premises networks to Google resources, the best option is to use VPN or Cloud Interconnect, not external IPs.

### Video - [Lab intro: Using Terraform to Create Networks and Firewalls](https://www.cloudskillsboost.google/course_templates/145/video/515123)

- [YouTube: Lab intro: Using Terraform to Create Networks and Firewalls](https://www.youtube.com/watch?v=k7gu3luPHhI)

In this lab, you use Terraform to automate the creation of network infrastructure. You will create two VPCs, one for public service and one for the database server. You'll also create firewall rules for each network.

### Lab - [Using Terraform to Create Networks and Firewalls](https://www.cloudskillsboost.google/course_templates/145/labs/515124)

In this lab, you will create a secure network infrastructure for you database migration projects. You will create both public and private VPCs with appropriate firewall rules in each. You will add virtual machines to each network and test the commnication between them. You will do all of this using Terraform to demonstrate a more real-world workflow that you can use in your migration projects.

- [ ] [Using Terraform to Create Networks and Firewalls](../labs/Using-Terraform-to-Create-Networks-and-Firewalls.md)

### Video - [Lab review: Using Terraform to Create Networks and Firewalls](https://www.cloudskillsboost.google/course_templates/145/video/515125)

- [YouTube: Lab review: Using Terraform to Create Networks and Firewalls](https://www.youtube.com/watch?v=ENnd92X3Faw)

In this lab you used Terraform to automate infrastructure creation. You also created two VPCs, one for public servers and one for databases. You also created firewalls for each network. The two key takeaways should be one, how can use multiple networks and firewall rules to control access to machines and databases and also two, how can you use Terraform to automate to automate resource creation in Google Cloud.

### Video - [Connecting Networks](https://www.cloudskillsboost.google/course_templates/145/video/515126)

- [YouTube: Connecting Networks](https://www.youtube.com/watch?v=QLkC-OMxNaE)

After you have networks, you want to connect resources with them. The whole point of networks is to allow resources to connect to one another. But you want to make sure that you set this up right, because different types of resources need to talk to one another through different network protocols to make sure that the system is secure and to minimize costs. End users will use the public internet to connect to a web server that hosts your app. But that web server will need to talk to a database, which should only allow connections from the same project through VPC. You don't want the general public to be able to reach your database server. However, you will have some on-premises resources, such as your admin or data analysts, who will need to connect to the database. For them, a VPN or Interconnect is the more appropriate way to connect versus a public IP. Cloud VPN is a way to connect your on-premises network to your Google Cloud VPC network. It's highly available and secure. Cloud VPN can support connections up to about three gigabits per second, and it can be configured two ways. The Classic configuration consists of a single VPN tunnel and offers a 99.9% availability SLA. A high-availability configuration uses a second tunnel to achieve 99.99% availability. Cloud VPN can also be configured for both static and dynamic routing using Cloud Router. Google Cloud provides a VPN gateway as a service that you use on-premises to connect your local network to the VPC network. You set up a VPN gateway in your on-premises network and Cloud VPN in your Google Cloud VPC, and then connect the two. This provides an encrypted connection for secure communication between the two networks. Cloud Router is not a physical router, but is instead a service that works over Cloud VPN or Cloud Interconnect connections to provide dynamic routing by using the Border Gateway Protocol. Dynamic routing just means that if the network on either side changes, the connection between the two is automatically updated. Dedicated Interconnect provides direct physical connections between your on-premises network and Google's network. This enables you to transfer large amounts of data between the two networks, which can be more cost effective than purchasing additional bandwidth over the public internet. In order to use Dedicated Interconnect, you need to provision a cross connect between the Google network and your own router in a common colocation facility, as shown in this diagram. To exchange routes between the networks, you configure a BGP session over the interconnect between the Cloud Router and the on-premises router. This will allow user traffic from the on-premises network to reach Google Cloud resources on the VPC network, and also vice versa. Dedicated Interconnect can be configured to offer 99.9% or 99.99% uptime SLA. You can refer to the Dedicated Interconnect documentation for details on how to achieve these SLAs. Creating a Dedicated Interconnect connection is as simple as these four steps. First, you order your Dedicated Interconnect. Second, you send your LOA-CFAs to your vendor. Third, test the Interconnect. And then finally, fourth, create VLAN attachments and establish BGP sessions. Partner Interconnect provides connectivity between your on-premises network and your VPC network through supported service provider. This is useful if your data center is in a physical location that can't reach a Dedicated Interconnect colocation facility, or if your data needs don't warrant a Dedicated Interconnect. In order to use Partner Interconnect, you work with a supported service provider to connect your VPC and on-premises networks. You can refer to the documentation for a full list of our providers. These service providers have existing physical connections to Google's network that they make available for customers to use. After you establish connectivity with the service provider, you can request a Partner Interconnect connection from your service provider. Then, you can also establish a BGP session between your Cloud Router and the on-premises router to start passing traffic between your networks via the service provider’s network. Partner Interconnect can be configured to offer 99.9%, or 99.99% uptime SLA between Google and the service provider. See the Partner Interconnect documentation for details on how to achieve these SLAs. Now let's compare the interconnect options that we just discussed. The IPsec VPN tunnels that Cloud VPN offers have a capacity of 1.5 to 3 gigabit per second per tunnel, and require a VPN device on your own on-premises network. The 1.5 gigabit per second capacity applies to traffic that traverses the public internet, and the three gigabit per second capacity applies to traffic that is traversing a direct peering link. You can configure multiple tunnels if you want to scale this capacity. Dedicated Interconnect has a capacity of ten gigabit per second, or 100 gigabits per second per link, and requires you to have a connection in a Google-supported colocation facility. You can have up to eight links to achieve multiples of tens of gigabits per second, or two links to achieve multiples of hundreds of gigabits per second, but ten gigabit per second is the minimum capacity. Partner Interconnect has a capacity of 50 megabit per second to ten gigabit per second per connection, and requirements depend on the service provider. All of these options provide internal IP address access between your resources and your on-premises network, and in your VPC network. The main differences are the connection capacity and the requirements for using a service. My recommendation is to start with VPN tunnels. When you need enterprise-grade connections to Google Cloud, then you can switch to Dedicated Interconnect or Partner Interconnect, depending on your proximity to a colocation facility and your capacity requirements. VPC Network Peering allows private RFC 1918 connectivity across two VPC networks, regardless of whether they belong to the same project or even the same organization. Now remember that each VPC network will have firewall rules that define what traffic is allowed or denied between the networks. For example, in this diagram, there are two organizations that represent a consumer and a producer. Each organization has its own organization node, VPC Network, VM instances, Network Admin, and Instance Admin. In order for VPC Network Peering to be established successfully, the Producer Network Admin needs to peer the Producer Network with the Consumer Network and the Consumer Network Admin needs to peer the Consumer Network with the Producer Network. When both peering connections are created, the VPC Network Peering session becomes Active and routes are exchanged. This allows the VM instances to communicate privately using their internal IP addresses. VPC Network Peering is a decentralized or distributed approach to multi-project networking, because each VPC network may remain under the control of separate administrator groups and maintains its own global firewall and routing tables. Historically, such projects would consider external IP addresses or VPNs to facilitate private communication between VPC networks. However, VPC Network Peering does not incur the network latency, security, and cost drawbacks that are present when using external IP addresses or even VPNs. VPC Network Peering works with Compute Engine, Kubernetes Engine, and App Engine flexible environment. Remember a few things when using VPC Network Peering. Peered networks can be in the same project or in different projects. Peered networks can be owned by different organizations. Each side of a peering association is set up independently. Peering will be active only when the configuration from both sides matches. This allows either side to delete the peering association at any time. A subnet CIDR prefix in one peered VPC network cannot overlap with the subnet CIDR prefix in another peered network. This means that two auto mode VPC networks that only have the default subnets cannot peer. And only directly peered networks can communicate, which means that transitive peering is not supported. In other words, if VPC network N1 is peered with N2 and N3, but N2 and N3 are not directly connected, VPC network N2 cannot communicate VPC network N3 over the peering. This is critical if N1 is a SaaS organization offering services to N2 and N3. As pointed out on the last slide, each side of the peering association needs to be set up separately. So note here how the public and private networks each reciprocally connect to the other. If you only do one, it's not going to automatically allow for the other. As always, make sure that you don't have overlapping IP address ranges between the peered networks. Creating the peering requests automatically generates the necessary routes to allow the traffic to flow between the peered networks. Terraform templates make automating this easier. In the code shown here, note that there is a public-private request and then the reciprocal private-public request. It's simple. You just make two peering requests that are the inverse of each other.

### Video - [Lab intro: Use Terraform to Create a Network Peering](https://www.cloudskillsboost.google/course_templates/145/video/515127)

- [YouTube: Lab intro: Use Terraform to Create a Network Peering](https://www.youtube.com/watch?v=BdaV01T3tiQ)

Multiple networks allow you to easily isolate machines and control which machines have access to other ones. In the course example, you want databases isolated in their own network with no external IP addresses. Those servers by default will only be accessible for machines in the same network. You can then put database clients like web servers in another network, then peer the two networks allowing communication via only internal addresses. In this lab, you peer two networks using Terraform.

### Lab - [Use Terraform to Create a Network Peering](https://www.cloudskillsboost.google/course_templates/145/labs/515128)

In this lab, you will peer two networks. This will allow communication between machines in those networks using internal IP addresses. This means databases can be deployed on a private network without external IP addresses, and those database will only be reachable from from the peered network.

- [ ] [Use Terraform to Create a Network Peering](../labs/Use-Terraform-to-Create-a-Network-Peering.md)

### Video - [Lab review: Use Terraform to Create a Network Peering](https://www.cloudskillsboost.google/course_templates/145/video/515129)

- [YouTube: Lab review: Use Terraform to Create a Network Peering](https://www.youtube.com/watch?v=3EVb01r2niQ)

In this lab, you learned how to use Terraform to set up network peering. The peering allows machines in two different networks to communicate using internal IP addresses. This allows you to protect your databases by controlling exactly which machines have access to them and from which networks.

### Video - [Enabling Communication Across Networks](https://www.cloudskillsboost.google/course_templates/145/video/515130)

- [YouTube: Enabling Communication Across Networks](https://www.youtube.com/watch?v=0NFroH0ab_c)

Now that the networks are in place and connected to each other, you need to set up the appropriate connections between them. First, allow clients from the peer network to talk to the database server. For SQL Server, if you're using a Windows server, you would use the default port of 1433 and RDP. For MySQL, assuming that you're running a Linux server, you would open port 3306 and SSH. Here's an example Terraform script to allow the clients to connect to the SQL Server instance through port 1433. Note that this code is using the variable that sets the internal IP address range of the client network to set the source ranges property of the firewall rule. Thus, the SQL Server instance is in a private network that is only accessible from the client network. Users can connect to the web servers in the public network, but only the servers in the public network can connect to the SQL Server instance. Also, note the target tags variable. This rule only applies to the servers tagged with the string allow SQL. When you have a machine with no public IP address, which is usually the preferred configuration, in order for that machine to reach the Internet, you would need a NAT proxy. You can create a NAT proxy using either a custom compute engine VM or by using the Cloud NAT service. Here's a Terraform example using the Cloud NAT gateway service. Again, the NAT gateway will allow machines with no external IP address to access the Internet. Note the region and router parameters. These parameters refer to a Cloud Router, which also must be configured. Here, the Cloud Router configuration is shown. Note, Cloud Router is a regional service. By default, Cloud Router advertises subnets in its region for regional dynamic routing, or all subnets in a VPC network for global dynamic routing. New subnets are automatically advertised by Cloud Router. Also note, the network that the router serves must be specified. If a machine with no public IP address needs Internet access, you now know how to fix that using Cloud NAT. But what about letting that VM access Google Cloud services like Cloud Storage or BigQuery? In order to permit that, you need to turn on private Google Access. This is done when configuring subnets. Private Google Access permits access to Cloud and developer APIs and most Google Cloud services with a few exceptions. A real-world database use case might be allowing the database server access to Cloud Storage for storing backups.

### Video - [Lab intro: Using Terraform to Create Clients and Servers](https://www.cloudskillsboost.google/course_templates/145/video/515131)

- [YouTube: Lab intro: Using Terraform to Create Clients and Servers](https://www.youtube.com/watch?v=TltO5SXOUHo)

In this lab, you use Terraform to create client and server VMs in both public and private networks, and set up the communications between them to allow them to communicate with each other.

### Lab - [Using Terraform to Create Clients and Servers](https://www.cloudskillsboost.google/course_templates/145/labs/515132)

In this lab, you will provision a Linux virtual machine in a private network to act as a database server. You will also create a virtual machine that you can use as a bastion host to connect to the database server as an administrator. You will configure the database server for remote connections and add a user account. You will then install the MySQL client software to connect to the database server from a client machine.

- [ ] [Using Terraform to Create Clients and Servers](../labs/Using-Terraform-to-Create-Clients-and-Servers.md)

### Video - [Lab review: Using Terraform to Create Clients and Servers](https://www.cloudskillsboost.google/course_templates/145/video/515133)

- [YouTube: Lab review: Using Terraform to Create Clients and Servers](https://www.youtube.com/watch?v=RJOO5Mip-ik)

In this lab, you used Terraform to create client and server VMs in both public and private networks, and you set up the communications between them to allow them to communicate with each other. At this point, you have learned how to automate the setup of secure networks. This is very important when migrating databases to the cloud.

### Video - [Network Considerations for Managed Databases](https://www.cloudskillsboost.google/course_templates/145/video/515134)

- [YouTube: Network Considerations for Managed Databases](https://www.youtube.com/watch?v=STJAgGndnrA)

We've looked at network considerations of VM-based database servers, so now let's look at network issues when using Google Cloud managed database services. Cloud SQL allows you to choose either private or public IP addresses, or both. Also make note that once a private IP address is enabled, it cannot be disabled. When you enable the private IP, Google automatically creates a network peering between your network (which you specify) and their network where your Cloud SQL database is running. This is just like what you set up earlier in the course. The difference is that Google manages the Cloud SQL network. After the private IP is enabled, machines in the peered network can communicate without needing a public IP address. You can clear the public IP option and the server will not be given one. A Cloud SQL instance will be in Google's own managed network, and you may need to allow resources in your project to connect to that Google Cloud SQL instance. In that case, you would choose a private IP to do that. This will associate your chosen network with Google's network and then peer them. If you choose a public IP, your database server is protected by firewall rules instead. By default, only apps in the current project can access the database, but you can authorize additional networks to have access by using IP addresses or ranges just like you do when specifying source IPs in a firewall rule. You can also create a VPN or Cloud Interconnect to connect networks outside Google Cloud. This would be the preferred way to connect your on-premises network to the managed database instance.

### Quiz - [Module 5 Quiz](https://www.cloudskillsboost.google/course_templates/145/quizzes/515135)

#### Quiz 1.

> [!important]
> **Why would you use Terraform instead of the Google Cloud Console to create network infrastructure?**
>
> - [ ] Allows infrastructure to be created and destroyed automatically.
> - [ ] Easier to make changes over time.
> - [ ] Less prone to human error.
> - [ ] All of the above.

#### Quiz 2.

> [!important]
> **You've created two Google Cloud VPCs: a public one for web servers and database clients, and a private one for the databases. You want to connect them to allow communication via private IP addresses. Which should you use?**
>
> - [ ] Cloud VPN.
> - [ ] Cloud Router.
> - [ ] Cloud Interconnect.
> - [ ] VPC Peering.

#### Quiz 3.

> [!important]
> **Your DBA wants to securely connect to databases in a private Google Cloud VPC. You want to allow them to directly connect from their workstation in their office using their Windows workstation. What should you do?
**
>
> - [ ] Give the database a public IP address, and create a firewall rule that allows them to connect to the server.
> - [ ] Set up a Cloud Interconnect direct connection from their network.
> - [ ] Set up a VPN between their network and the VPC.
> - [ ] All of the above would work.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/145/video/515136)

- [YouTube: Module Review](https://www.youtube.com/watch?v=LxFIkebyV8I)

In this module, you built secure networks to host databases and database client applications. You also allowed secure communication across networks using VPC Peering, VPNs, and Interconnect. You also controlled access to databases using firewall rules, and automated network infrastructure using Terraform.

## Migrating SQL Server Databases to Google Cloud

In this module, you learn about the options available for migrating a SQL Server database to Google Cloud.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/145/video/515137)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=FUL8npao0qM)

Hello, everyone. Julianne here. There are several options available for migrating a SQL Server database to Google Cloud. Let's explore those in some detail. A lift and shift strategy involves picking up and moving virtual machines into Google Cloud. You can employ this strategy with your SQL Server databases by using Compute Engine. Alternatively, you could use a managed version of SQL Server provided by Cloud SQL. In any case, you also need to architect your SQL Server databases for security, high availability, as well as disaster recovery. You can also run SQL Server in a Kubernetes environment using Google Kubernetes Engine. Choose the right Google Cloud service to run SQL Server workloads based on your use cases. Each of the three services offer advantages over the others. Compute Engine offers the most straightforward and familiar option with minimal changes from your existing SQL Server installation. You basically build a custom SQL Server with the exact features you want, just as you would on an on-premises machine. But it also means DBAs have to do all the same work they would on a local server. Cloud SQL offers all the advantages of a managed service and removes some of the administrative burden from the DBAs. However, not all SQL Server features are supported, primarily because it's running on Linux. It's critical to ensure that none of the unsupported features are important to you before you choose this option. GKE offers flexibility in deploying the database into the same cluster as your applications, as well as the ability to create hybrid deployments and portability to other Cloud environments.

### Video - [SQL Server on Compute Engine](https://www.cloudskillsboost.google/course_templates/145/video/515138)

- [YouTube: SQL Server on Compute Engine](https://www.youtube.com/watch?v=d-x6-Wpv6Sk)

SQL Server on Compute Engine is basically just relocating the server from a local machine to a Compute Engine VM. This is often the easiest way to do the migration itself, but may not be the best long-term option. When you create the Compute Engine VM, you could start with a bare Windows image and install your own SQL Server database onto it. But Google offers pre-built versions of SQL Server in many different versions, which simplifies the whole process of creating the VM. Using the Google-provided images also offers benefits beyond convenience during the build process. They provide a higher level of security against attempts to hack your server by providing mechanisms to detect and prevent unauthorized changes to the software and to protect keys and certificates. Secure Boot ensures that only verified software runs on your VM. It uses digital signatures to verify software components and stops the boot process if something has been tampered with. Shielded VMs use a custom chip called a Virtual Trusted Platform Module, or VTPM, to protect digital signatures. And Integrity Monitoring compares boot information to a stored baseline hash that is stored in the VTPM. The Shielded VM features are available when you configure the VM, and you can enable just the ones you want. Another advantage of using the pre-built Google images is that the license cost of using Windows and SQL Server is built into the running costs of the machine. The pricing varies depending on which edition of SQL Server you choose and how many cores you configure the VM for. Still, it is often a lot easier to manage than manually paying for your own SQL Server license. However, if you already pay for licenses, there is a Microsoft-licensed Mobility for Google Cloud program. You can see the documentation for more information. When configuring the VM, you also need to allocate storage space. This is done by adding data disks. As usual, they can be SSDs for faster performance or standard disks for cost savings. You can configure the size to meet your needs, but remember that you pay for what you allocate, not what you use. Disks can be very large, up to 64 terabytes. And you can also schedule automatic disk snapshots to make disk backups, but remember that this is a different kind of backup from SQL Server backups. Snapshot backups can be configured to be stored in one or multiple regions and on any schedule that suits you. It's best to schedule the snapshots at off-peak hours. Also, if you're running a Windows instance, you can choose the volume shadow copy service to allow snapshots without shutting down the SQL Server service. Ultimately, you will have a SQL Server VM that clients will need to connect to. So in order to reach the server, you need to configure the networking. Typically, you don't want an external IP address, but instead want to make a private network for the clients to access the server. This provides more security. To administer a Windows server, you will use RDP. In order to RDP into the server, you will need to set a Windows username and password. This can be done from the console or the CLI. First, you specify any username you like, and then a strong password will be generated. As with all usernames and passwords, you will need to save or remember these. You can always change the password after logging in. There are many best practice recommendations, but they're all pretty straightforward. You want to follow the typical best practices for creating a VM, which includes using a firewall and the default network settings, as well as installing antivirus software. Additionally, you would configure the SQL Server portion in a way that's very similar to how you would on a local machine. Namely, you would want to separate disks for the log and data files, as well as for tempdb and the Windows paging files. Using SSDs is more expensive, but offers better performance. If you can't use SSDs for all of the drives, it's probably most important to use them for the tempdb and paging files first. Also, although you have the server in the cloud and have turned on snapshots, you can't skip normal SQL Server maintenance. You still need to set up regular database and log backups and schedule them with SQL Server agent. As an added convenience, instead of just using a pre-made Google image to build your own machine, you can search the marketplace and find fully pre-configured machines based on Windows or on Linux. Base your decisions on how much time you want to invest in building your image and how much custom configuration you need. If you're worried about the main SQL Server instance going down and need a solution that guarantees the server is always available, you can use the SQL Server always on availability groups. This requires an active directory domain controller. One server would be designated as the main server. Then, one or more replicated servers would be in a standby mode, constantly keeping their own copy of the data in sync with the main. If the main fails, one of the standbys would take over as the main server and continue the workload uninterrupted. Eventually, you can fix the main, resync the data, and switch back to it. Always on availability requires the more expensive enterprise edition of SQL Server. So if you need this feature, make sure you consider price when choosing the image to build your VM. Configuring all of this is exactly as it would be for an on-premises version of SQL Server. You set up the main and the active directory, configure the failover cluster manager on the main server, and add databases to the availability groups to enable replication. The Google documentation contains a good tutorial on setting this up, so please refer to that documentation for more details. Another slightly different option for high availability is SQL Server failover cluster instances. Using this option, you set up a single shared storage device. This would be like a virtual SAN setup using Microsoft Storage Spaces Direct. Then, you have a main instance of SQL Server that uses that virtual SAN, but if it fails, a standby node would become active and attached to the shared SAN to continue the work uninterrupted. A load balancer is used to send requests to the active server. This option also requires the enterprise edition of SQL Server, as well as the data center version's Windows Server. Also, you need an active directory domain controller and an internal Google Cloud Load Balancer. Choosing between availability groups and failover cluster instances is a tricky choice full of pros and cons for each. Microsoft offers both of these choices to provide flexibility in meeting your needs. As with availability groups, there is a good tutorial in the Google Cloud documentation for setting up failover cluster instances.

### Video - [Lab intro: Creating SQL Server Databases on Compute Engine](https://www.cloudskillsboost.google/course_templates/145/video/515139)

- [YouTube: Lab intro: Creating SQL Server Databases on Compute Engine](https://www.youtube.com/watch?v=MICvCtI-hEc)

In this lab, you create a SQL Server VM and admin and client VMs. You then do some basic administration and make a connection from a client to the database.

### Lab - [Creating SQL Server Databases on Compute Engine](https://www.cloudskillsboost.google/course_templates/145/labs/515140)

In this lab, you will provision a SQL Server machine in a private network. Then, you will create a Windows machine in a public network that you can use to adminstrate the database server. You will also create a Linux client in the public network that can be used to connect to the database. Lastly, you will setup a firewall rule that allows access to the SQL Server database only from the private network.

- [ ] [Creating SQL Server Databases on Compute Engine](../labs/Creating-SQL-Server-Databases-on-Compute-Engine.md)

### Video - [Lab review: Creating SQL Server Databases on Compute Engine](https://www.cloudskillsboost.google/course_templates/145/video/515141)

- [YouTube: Lab review: Creating SQL Server Databases on Compute Engine](https://www.youtube.com/watch?v=Pqf0yCNv6lI)

In this lab, you created a SQL Server database plus admitting client VMs. You also administered your SQL Server database and connected to the database from the client. Compute Engine allows you to deploy SQL Server databases with no limitations. Using Compute Engine, you can configure your SQL Server databases just as you would on-premises. It's ideal for when you want to migrate using a left-and-shift approach.

### Video - [SQL Server on Cloud SQL](https://www.cloudskillsboost.google/course_templates/145/video/515142)

- [YouTube: SQL Server on Cloud SQL](https://www.youtube.com/watch?v=Z5O2uWlnOwY)

SQL Server on Cloud SQL offers an easier alternative to fully building out a VM. It comes with all the benefits of using a fully managed service compared to your own VM, but as always, you have to make some trade-offs for that easier option. The first notable difference is that the server runs on Linux, not Windows. Google handles all the networking and backup and maintenance for you, and also makes it easy to set up failover servers. The downside of not having a Windows-based instance is that not all versions of SQL Server are supported. Also, some of the features that rely on Windows are not supported. Until Microsoft's recent association with Linux, SQL Server ran exclusively on Windows, so many features heavily relied on Windows features. This slide lists some of the features of SQL Server that Microsoft currently doesn't support on Linux, and are not supported on Cloud SQL. The complete list can be found in the Google Cloud documentation. It is very quick and easy to create a Cloud SQL instance of SQL Server. There's a lot less to configure than when building your own VM. Basically, you just need an instance ID and a password and to choose the region for it. There are additional options, but these are the ones you always need to set. It is also very easy to create a Cloud SQL machine using the CLI. Compare the gcloud command for creating this instance to creating a VM, and you can see how much simpler it is. Or, if you are using Terraform, Cloud SQL databases can be created with a simple template. As you saw earlier in the course, Terraform makes it easier to configure your data center resources by defining what you want to configure with a simple configuration language. You can also choose among several SQL Server editions and machine configurations. The choice of version is largely influenced by your database size needs. Pricing is significantly different, so it's important to plan ahead, otherwise, you might have to migrate the databases to another server later, or you might spend much more than you need. There are four editions of SQL Server: Express, Web, Standard, and Enterprise. Express edition has no license charge, but is limited to a ten gigabyte database. The other three editions require licensing, Enterprise being the most expensive. See the table on the slide for vCPU, RAM, and database size features and licensing costs. An important point to note is licensing is per-vCPU Core, but you are always charged for at least four vCPUs. Configure the machine type according to required capacity. Enabling automatic storage increases is a good option to consider because it allows you to start off with smaller disks and grow them as needed. Remember, you pay for what you allocate, not what you use. Cloud SQL instances can have either a private or public IP address, or both. A private IP would make your instance more secure, but you'd have to configure authorized clients to connect to that instance through the private IP using a peered network. Enabling Private IP creates a VPC peering between the Cloud SQL database network and your network. Then, only machines in your peered network can access the database. A firewall is used for protection in the case of public IPs, and, by default, only allows machines within the project to have access to the server. However, you could authorize one or more external connections. As with configuring any firewall rule, use CIDR notation to allow IP addresses or ranges. Cloud SQL provides its own maintenance options and automation. Backups should be made during a period of low utilization. You can also easily create a failover cluster by just enabling the High Availability option. The service will also automate typical maintenance tasks like patch management. As with backups, choose a maintenance window when there is low utilization. If maintenance requires a reboot, the server will be unavailable for a short period of time. You can also manually perform a backup or restore with either the console or the CLI. Remember, these are not the same type of backups as native SQL Server database and log backups. The Cloud SQL backup is a snapshot of the persistent disk with the data. It can only be restored to another Cloud SQL database. In addition, you can always run standard SQL Server backups as you would with any SQL Server database.

### Video - [Lab intro: Administering a Highly Available Cloud SQL for SQL Server Database](https://www.cloudskillsboost.google/course_templates/145/video/515143)

- [YouTube: Lab intro: Administering a Highly Available Cloud SQL for SQL Server Database](https://www.youtube.com/watch?v=N1RDdxejMe4)

While Cloud SQL has some limitations, it makes setting up and administering a database much easier. You can create failover servers by simply choosing an option. Backup, restore, and admin tasks are all automated as well. In this lab, you create a SQL Server Cloud SQL instance that is secure and highly available. You'll perform a backup and a restore operation and connect to the instance from a client using its private IP address.

### Lab - [Administering a Highly Available Cloud SQL for SQL Server Database](https://www.cloudskillsboost.google/course_templates/145/labs/515144)

In this lab, you will create a SQL Server database using Google Cloud SQL. You will enable high availability and deploy it securely with only a private IP address. You will then load a sample database into it and connect to the database using a client machine. You will also perform backup and restore operations and test high availability by triggering a failover.

- [ ] [Administering a Highly Available Cloud SQL for SQL Server Database](../labs/Administering-a-Highly-Available-Cloud-SQL-for-SQL-Server-Database.md)

### Video - [Lab review: Administering a Highly Available Cloud SQL for SQL Server Database](https://www.cloudskillsboost.google/course_templates/145/video/515145)

- [YouTube: Lab review: Administering a Highly Available Cloud SQL for SQL Server Database](https://www.youtube.com/watch?v=wfFlVp2JsWI)

In this lab, you created a secure, highly available SQL Server database using Cloud SQL. You also performed backup and resource operations, and you connected to the database using its private IP address. Cloud SQL allows you to deploy SQL Server in a managed environment. Administration like backups and patches are all automated for you, and you can create a failover machine just by setting a parameter, and Google manages a secure network for you.

### Video - [SQL Server on GKE](https://www.cloudskillsboost.google/course_templates/145/video/515146)

- [YouTube: SQL Server on GKE](https://www.youtube.com/watch?v=UQ-VD30Ooec)

A third option for running SQL Server is to deploy it on GKE. Running SQL Server on GKE offers some significant benefits over the other options. Most notable is the ability to run the server across various cloud platforms or in a hybrid on-premises or off-premises option. Using Kubernetes simplifies the automation of deploying cloud resources and applications. Additionally, you have a higher degree of automation available for creating CI-CD pipelines. It also provides the ability to move the server to the same cluster as the application for better performance. You configure SQL Server databases to run in a Kubernetes cluster by using YAML files. Persistent volume planes are used to allocate disk space for your database running in the cluster. A deployment configuration is used to define the Docker image, environment variables, and instance resources. And a service configuration is used to provide access to your SQL Server from its clients. Here is an example of using a Microsoft standard Docker image for your server. This image is maintained by Microsoft and stored in Docker Hub. Alternatively, you could build your own Docker image with SQL Server installed if you needed a higher degree of customization. Many Microsoft pre-built Docker images are available. Choose the version that suits your needs and make sure to set the required environment variables. You use license agreements to specify an initial SA password, the edition, and so on. More information can be found at the Docker Hub web page for each image. Just as in a normal SQL Server configuration, you want the software, data, log, and tempdb in different physical volumes. So here you would configure different persistent volume planes for each. Note that each persistent volume plane has a unique name and the amount of disk space for each is specified. Finally, put all the pieces together in a deployment. The persistent volume planes from the previous slide are added as volumes in the pod you're configuring. Then the volumes are mounted using the mount path property when you configure the SQL Server container that will also run in the pod. Note that path names are Linux, not Windows paths. You can also configure the required environment variables here. Check out the SA password variable. This is being set from a Kubernetes secret. Storing sensitive data in plain text in a configuration file is never a good idea. So you can use secrets to store the actual password and use the secret in the config file. Here the password is being set using the CLI. Run this code before the deployment. The service configuration provides access to the database running inside the cluster. Note the port and target port variables. 1433 is the default SQL Server port. The service type load balancer creates a public IP address, allowing the database to be accessed from outside of the cluster. If you set the type to cluster IP or if you didn't set the type property at all, the service would only have a private IP address. Thus, the database would only be available from inside the cluster. After you configure the various files, you can use the kubectl command line interface to put them all together and to deploy the server. Deploy each of the YAML files with the command kubectl apply dash F and the name of the file. You can delete everything by running kubectl delete.

### Video - [Lab intro: Running SQL Server on Google Kubernetes Engine](https://www.cloudskillsboost.google/course_templates/145/video/515147)

- [YouTube: Lab intro: Running SQL Server on Google Kubernetes Engine](https://www.youtube.com/watch?v=kM-yamO-MU8)

Kubernetes is becoming an increasingly popular way to deploy automated cross-cloud and hybrid cloud applications. Applications, of course, need databases. So if you're deploying your web apps and services to Kubernetes, why not deploy your databases there as well? This slide gives you the opportunity to create a Kubernetes cluster and configure and deploy a SQL Server database in it. Then you connect to the database from a client and look at an alternative way to deploy SQL Server to the cluster using HAL.

### Lab - [Running SQL Server on Google Kubernetes Engine](https://www.cloudskillsboost.google/course_templates/145/labs/515148)

In this lab, you will create a Kubernetes cluster in Google Kubernetes Engine (GKE). You will configure and deploy a Microsoft SQL Server database into the cluster and connect to it from a client machine. Then, you will simplify the deployment using Helm.

- [ ] [Running SQL Server on Google Kubernetes Engine](../labs/Running-SQL-Server-on-Google-Kubernetes-Engine.md)

### Video - [Lab review: Running SQL Server on Google Kubernetes Engine](https://www.cloudskillsboost.google/course_templates/145/video/515149)

- [YouTube: Lab review: Running SQL Server on Google Kubernetes Engine](https://www.youtube.com/watch?v=RGHdl_mWebE)

In this lab, you created a Kubernetes cluster and configured and deployed SQL Server into the cluster. You then connected to the SQL Server database from a client machine. And lastly, you deployed SQL Server to Kubernetes using HAL. Running your SQL Server databases in a Kubernetes cluster is ideal when you want a cross-cloud or hybrid cloud solution that's also automated.

### Quiz - [Module 6 Quiz](https://www.cloudskillsboost.google/course_templates/145/quizzes/515150)

#### Quiz 1.

> [!important]
> **What is an advantage of Cloud SQL over Compute Engine for running SQL Server databases?**
>
> - [ ] All of the above.
> - [ ] Automatic storage increases.
> - [ ] Runs on Windows or Linux servers.
> - [ ] Supports all SQL Server features.

#### Quiz 2.

> [!important]
> **Your company wants to move a large SQL Server database to Google Cloud. They currently run Enterprise edition with a failover cluster, and they want to replicate the database across multiple regions. They also want to integrate with their Active Directory for authentication. What would you recommend?**
>
> - [ ] Deploy SQL Server using Compute Engine.
> - [ ] All of the above would work.
> - [ ] Deploy SQL Server using Kubernetes on GKE.
> - [ ] Deploy SQL Server using Cloud SQL.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/145/video/515151)

- [YouTube: Module Review](https://www.youtube.com/watch?v=-vDhOarvaLM)

To review, in this module, you used Compute Engine to lift and shift SQL Server databases to Google Cloud. You also employed Cloud SQL for managed SQL Server databases. You architected SQL Server for security, high availability, and disaster recovery. And you configured SQL Server to run with Kubernetes on GKE.

## Migrating Oracle Databases to Google Cloud

In this module, you learn about how to migrate Oracle databases to Google Cloud.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/145/video/515152)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=ayqXDPwpW3g)

Damon here again, let's talk about migrating Oracle databases to Google Cloud. In this module, you learn why running Oracle on Google Cloud makes sense. You review the technical specifications of the Bare Metal Solution, and you define common use cases for running Oracle on Google Cloud.

### Video - [Why Oracle on Google Cloud](https://www.cloudskillsboost.google/course_templates/145/video/515153)

- [YouTube: Why Oracle on Google Cloud](https://www.youtube.com/watch?v=zALbwtUB_5Q)

You may have one or more Oracle servers on-premises and want to migrate them off-premises to the cloud. Google provides an excellent solution for this scenario. You may have existing applications that are heavily dependent on Oracle backends and uncoupling them might be difficult. Using Cloud SQL or a SQL Server instance may not be an option. However, you want to benefit from many of Google Cloud's features such as off-site servers and scalability. Google has a solution that allows you to keep your Oracle database and also migrate to the cloud with no restrictions or limitations on the Oracle side of things. It is truly a best-of-all-worlds solution. Bare-metal solution offers many advantages over a traditional on-premises server. Google is fully committed to integrating Oracle workloads in its cloud environment and employs many ex-Oracle engineers. Many enterprises depend on Oracle databases for their mission-critical applications. Google's solution allows customers to run applications in Google Cloud taking advantage of Google innovation while still using the enterprise's Oracle databases. Some features of Oracle require bare-metal servers. So, even though cloud-based services like AWS RDS support Oracle many Oracle features are not supported on that service. There is also a license penalty when running Oracle on virtual machines. When you use virtualized services like RDS Oracle charges for licenses by the virtual CPU. That makes licensing twice as expensive. On bare-metal solution, licenses are charged per physical CPU. Many times, you can't migrate your applications until you also migrate your Oracle database and moving to a different database might require expensive rewrites that are not practical. Google has several key objectives for Oracle workloads. There must be support for all licensable Oracle database options including real application clusters and database and memory. All Oracle database features must be supported without the limits imposed by cloud-managed services. All applications that use Oracle databases, including SAP on Oracle, must be supported. There should be no two-times license fee penalty imposed on cloud services by Oracle. Finally, Google's solution must be compatible with Oracle's license policies. Bare-metal solution has no limitations regarding Oracle versions, features, or configurations and has the same license cost as an on-premises server. Because of the close proximity of the physical Oracle servers to the Google facilities you get the high performance you would expect as if the servers were in the same data center. Additionally, Google handles the security through Google Cloud. Bare-metal solution can also be used by applications running in other clouds. Bare-metal solution is the only solution that provides a co-location data center with a connection to Google Cloud of less than two milliseconds of latency. Plus, vendors also offer high-bandwidth connections to AWS, Azure, Oracle Cloud, and other co-location facilities. Thus, you can host applications and appliances in another partner data center or on-premises and have a high-bandwidth network connection to Google Cloud and bare-metal solution. Bare-metal solution leverages other partners for managed Oracle services and hardware. Other partners like Accenture and Atos provide managed Oracle databases. Non-x86 platforms are offered by partners like IBM. Because bare-metal solution uses Oracle certified bare-metal servers there is no problem with Oracle licensing and support. Bare-metal solution is not a virtualized cloud solution. Thus, all Oracle versions and features are supported. You need to provide your own Oracle licenses and ensure that you are in compliance. You might consider using a third-party compliance firm to help ensure that you conform to license policies from Oracle.

### Video - [Technical Specs](https://www.cloudskillsboost.google/course_templates/145/video/515154)

- [YouTube: Technical Specs](https://www.youtube.com/watch?v=Iz_9XNAuYlk)

So far, we have been talking about running Oracle databases on Google Cloud using Bare Metal Solution. Now, let's look into some of the technical details about how the solution is architected, some of your options, and how you connect to and administer the servers. Bare Metal Solution hardware is traditional physical hardware, except it is located close to the Google hardware and has high-speed connections to the Google Cloud region where it's located. This allows the app layer and backups to perform at the same high speed as if they were other Google-provided database solutions. The Oracle servers are housed in a Google Partner data center. A high-speed interconnect is set up between the partner and your Google region with less than 2ms latency between them. You can create an Oracle server just like you would on-premises. You can choose the operating system you want on your Bare Metal servers when you place the order. Red Hat, Oracle Linux, SUSE, and Windows are all supported. You can also choose whether to install hypervisors. If you do, you can also run other operating systems. You manage and patch your machines as you would on-premises. You bring your own existing licenses. You can choose the hardware that fits your needs. Several standard machine configurations are supported. These servers use the latest hardware and CPUs. Details are in the table shown here, but check the documentation for the latest specs and pricing. Data Storage uses NetApp best practices for tuning the storage. You specify your desired IOPS, storage requirements, and LUN configuration. Up to 400,000 IOPS is guaranteed. You should use Cloud Storage for low-cost blob storage and backups. All data is encrypted at at-rest and over the network. You can backup your databases to Cloud Storage for an inexpensive solution. You pick which Cloud Storage class makes sense for your backups. You can automate snapshots for the database binary's NOS and set a retention period in Cloud Storage. And you can use lifecycle rules to automatically clean up storage over time. Hot backups can be written to local disk-based storage. You can use any traditional Oracle backup tool. There is a dedicated high-speed interconnect between the Google Data Center and the regional extension where the Oracle machines are located. You choose how fast your interconnect has to be. Interconnect between Google Cloud and the regional extension costs $247 per month, per 10 gigabits per second, up to 100 gigabits per second. Network egress between Google Cloud and the regional extension is free. And egress for data that leaves a Google Cloud region is billed at normal egress rates. Clients connect to your Oracle instance via Google Managed VPC. As you did in an earlier lab,a network peering is created between your Google Cloud VPC and the Google Managed VPC that provides access to the bare-metal servers. Connect to a jump server in your customer-managed VPC via SSH through Cloud VPN or a dedicated interconnect. From the jump server, you can use SSH to connect to your Oracle servers through the VPC peering provided by Google. Google will provide the IP addresses and connection information to your servers. You can request specific IP address ranges when placing the order for the servers. If the Oracle server needs access to the internet, that can be provided through NAT. The Oracle servers themselves have no internet access or external IP addresses. Use the most appropriate tool for migrating data to bare-metal solution. To get your data into the Oracle server, you can use a variety of tools, such as Oracle native tools you may be familiar with already. Google provided tools can also be used to move Oracle backups into cloud storage buckets. Depending upon how much data you have, you can use Google Storage Transfer Service. For very large amounts of data, you can order a transfer appliance. You can even use third-party partner tools if they offer a better solution than Oracle or Google provides. Stream is a commonly used third-party migration tool, or maybe you've written one yourself that will do the job. All your familiar tools can be used to connect to the Oracle server just as if it were on-premises. You can connect any VM in your VPC using either SSH or RDP. You can also create new applications using managed services and connect them to your Oracle server. You can monitor the Oracle processes and logs with standard methods. You can even configure Oracle Database Rack if you need high availability. It's exactly like doing it with your own hardware. Rack provides an automatic failover if your node crashes. Because the bare-metal solution runs on physical machines and has shared storage provided by NetApp, Rack can run just like it would on your own hardware. This can't be done running Oracle in AWS or Azure. This allows you to create a highly available cloud-based Oracle solution. To make your disaster recovery solution more robust, you can deploy the Rack to multiple regions.

### Video - [Use Cases](https://www.cloudskillsboost.google/course_templates/145/video/515155)

- [YouTube: Use Cases](https://www.youtube.com/watch?v=7p10alWC9Xw)

Now that you understand the technical details, let's explore some use cases. There are many use cases for running Oracle on Bare Metal Solution. These include development and test environments, disaster recovery, backup and recovery, and cloud migration. Instead of building out a physical server on-premises for development and testing, you can build out what you need on the cloud and use it for however long you need. This allows you to prototype and test out building Google Cloud solutions. The cloud can provide a great environment for testing and a sandbox for research and development. Application developers can use the cloud for increased productivity, flexible resource allocation and faster development cycles. You can also use the cloud as a safe space for testing patches and new versions before putting them in production. Disaster recovery is also simplified in the cloud. You can deploy Oracle in the cloud to get Oracle functionality combined with Google Cloud features and backup and restore the databases to Google Cloud Storage. You can create a cross-region solution that provides greater protection against failure in a single region. It is also easier to periodically test and validate the backups. Backups can be centralized and stored in Google Cloud Storage buckets. Use the appropriate storage class to reduce costs. Automating the backups and taking advantage of deduplication and incremental backups reduces admin costs. Plus, when you need to recover, there is lower latency, which means you can afford to do more frequent validation of your backups to ensure DR readiness. Leverage lifecycle rules in Cloud Storage to move objects to different storage classes as they age. At some point, backups become so old you would never restore them. You can also use lifecycle rules to delete them. For hybrid scenarios, you may be running your production workloads on-premises, but you can securely copy your backups into the cloud using a cloud interconnect or VPN connection, depending on the amount of bandwidth required. Use a Bare Metal Solution in a regional extension to run a test database, or in the event of a disaster, use it to spin up a failover server. Another use case is to move away from an on-premises Oracle server to host it in the cloud. Then you can continue to run your traditional apps as is, but also start to develop new cloud-based apps to leverage Google's innovations. There are many options for migrating, so it is a good idea to develop a detailed migration plan and pick the tools that work best for you. Follow Google's cloud migration process. Start with a detailed assessment and develop a detailed migration plan with tools, solutions, or partners. Then automate your migration. After your migration, look for ways to establish operational best practices and enhance existing applications by moving to a microservice architecture that leverages managed cloud services. During the migration phase, you will probably need to replicate your on-premises Oracle databases with your databases running in Bare Metal Solution. You can use Oracle Data Guard for native database replication.

### Video - [Lab intro: Oracle on Bare Metal Solution](https://www.cloudskillsboost.google/course_templates/145/video/515156)

- [YouTube: Lab intro: Oracle on Bare Metal Solution](https://www.youtube.com/watch?v=p2ZzntcObm8)

This lab demonstrates what you will need to do when managing an Oracle database running on the Bare Metal Solution. This is a simulation in that there is no Bare Metal server for you to connect to. Rather, you will install Oracle on a virtual machine. The lab architecture does replicate closely what you would do in a real BMS environment, however. In this lab, you configure a VPC network. You download and install Oracle 18C on a Bare Metal Solution server and connect to it from Google Cloud.

### Lab - [Deploying the Oracle on Bare Metal Solution](https://www.cloudskillsboost.google/course_templates/145/labs/515157)

In this lab, you will enable and test connectivity between Google Cloud and an Oracle Database 19c instance on a Bare Metal Server.

- [ ] [Deploying the Oracle on Bare Metal Solution](../labs/Deploying-the-Oracle-on-Bare-Metal-Solution.md)

### Video - [Lab review: Oracle on Bare Metal Solution](https://www.cloudskillsboost.google/course_templates/145/video/515158)

- [YouTube: Lab review: Oracle on Bare Metal Solution](https://www.youtube.com/watch?v=JEaaBMtnNRg)

In this lab, you configured VPC network peering between projects. You downloaded the Oracle 18c Installer from the Oracle Technology Network and installed an Oracle 18c database on a simulated Bare Metal server. Lastly, you verified connectivity to the Oracle 18c database from Google Cloud's AI platform.

### Quiz - [Module 7 Quiz](https://www.cloudskillsboost.google/course_templates/145/quizzes/515159)

#### Quiz 1.

> [!important]
> **You want to download a patch from your Oracle bare metal server and you can't access the internet. What is likely the solution?**
>
> - [ ] Add a firewall rule that allows FTP ingress from all sources.
> - [ ] Add a firewall rule that allows egress to the internet.
> - [ ] Add an internet gateway to Google's host network.
> - [ ] Set up a NAT gateway in your peered VPC network.

#### Quiz 2.

> [!important]
> **What is an advantage of running Oracle on Bare Metal Solution over a managed cloud platform like AWS RDS?**
>
> - [ ] Supports all versions of Oracle.
> - [ ] All of the above.
> - [ ] Supports Oracle RAC.
> - [ ] Cheaper license costs.

#### Quiz 3.

> [!important]
> **How do you connect to your Oracle database servers running in Bare Metal Solution?**
>
> - [ ] Set up a VPN to Google's host network and SSH via the VPN.
> - [ ] Connect using SSH from a Jump Server running in your peered network.
> - [ ] All of the above would work depending on how you configured it.
> - [ ] Connect using SSH directly to the server after adding a firewall rule.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/145/video/515160)

- [YouTube: Module Review](https://www.youtube.com/watch?v=JMI22xVRBZA)

In this module, you learn why running Oracle on Google Cloud makes sense, reviewed the technical specifications of the Bare Metal Solution, and defined common use cases for running Oracle on Google Cloud.

## Testing and Monitoring SQL Server Databases in Google Cloud

In this module, you learn about testing and monitoring options for SQL Server databases deployed on Google Cloud.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/145/video/515161)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=9a8AA_7WgyY)

Hi there, this is Julianne again. Welcome back to our course on Enterprise Database Migration. Testing is important to ensure your migration is done correctly. Monitoring will help keep your migrated applications and databases running smoothly over time. Now that you have your SQL Server instance in the Cloud, let's take a look at some things you can do to monitor and test it. Taking a software development or DevOps approach to migration using unit integration and regression testing can ensure a successful migration process. Aside from testing, there's also a need to monitor the entire migration, and Google has tools for that as well.

### Video - [Testing](https://www.cloudskillsboost.google/course_templates/145/video/515162)

- [YouTube: Testing](https://www.youtube.com/watch?v=B4We10ma38c)

Thorough automated testing is an important part of any IT project. Without effective testing, there's no way to know if your database has been migrated correctly. There are many things you need to verify. Was the database schema migrated correctly? Has all the data been migrated? How about user logins? Can all of the users still connect and can users only access the data they're permitted to access? Let's start off by exploring testing. Complete testing is the surest way to facilitate a successful migration and can even speed up the process. It is critical to ensure that applications continue to work on the new server and that the new server has sufficient resources to meet the demands of the application. Before starting a long migration process, you want to make sure that the tools and scripts work properly. There are basically three categories of testing that need to be considered. Structural, functional, and non-functional. Structural testing validates that everything that should have been moved has successfully been moved. It checks to make sure that every table, procedure, index, foreign key, trigger, login, and more has been brought over and that the new environment is structurally the same as the original. Write automated structural tests to ensure that all tables have been migrated, that fields, data types, links, and constraints were migrated as expected, that all indexes have been created, primary foreign key relationships were created, primary and foreign keys matched correctly in related tables, all stored procedures and triggers were migrated, and lastly, that user logins were migrated successfully. Functional testing goes to the next level and makes sure that all of the data has come across correctly. It also makes sure that all of the views and stored procedures work as intended and that users can log in and see what they are supposed to, but not what they shouldn't. Write automated functional tests to ensure that all data was migrated correctly, stored procedures and triggers work as expected, user logins work as expected, users can only perform operations and access data as permitted, and that queries and views continue to return expected results. After you know that everything works as intended, the next step is to make sure it performs well and can withstand peak usage periods. Load and stress testing will push the server to its limit to see whether it can handle the intended workload and just how far it will go before it breaks. This helps determine whether more resources need to be allocated to the server.

### Video - [Monitoring](https://www.cloudskillsboost.google/course_templates/145/video/515163)

- [YouTube: Monitoring](https://www.youtube.com/watch?v=JfmWh_ufRGA)

After a functional server is up and running, you want to monitor its activity to make sure there are no surprises. Google provides a web-based monitoring tool with a default console for each service. You can create your own custom dashboards and monitor resources in both Google Cloud and AWS. There is basic VM monitoring by default, but that just shows big-picture stats such as CPU usage, network traffic, and disk I.O. To really understand what's going on inside of a database server, you typically need to look at more metrics such as memory usage and swap file usage. You can install the Ops Agent to collect those metrics from different services and send them to the monitoring system. The agents collect the data from the OS and the SQL Server and store it in the Google Cloud Logging Service. Once there, it can be searched, filtered, and analyzed. You can export the logged data to Cloud Storage, BigQuery, or PubSub, and even create alerts if the metrics meet a certain condition. Of course, you can also display custom charts of the metrics to visualize the data. You can even create log metrics and trigger alerts when there are anomalies in the log stream. You should install the Logging Agent if you are running your database on a virtual machine. The Logging Agent is based on Fluentd and captures machine logs and the logs of third-party software, including SQL Server. This works on both Windows and Linux. The Logs Viewer provides a convenient web-based interface to filter the logged data or you can export it and explore it using any other tools. To export logs, create a filter that collects the log information you are interested in and then create an export sync. The export destination can be Cloud Storage if you just want to save the log data or BigQuery, if you want to use SQL for log analysis, or send the data to PubSub for real-time log analysis. If you use Cloud SQL, instead of building your own VMs, you get monitoring by default. There's no need to install any agents. You get multiple standard metrics and can view this from the console and the monitor dashboards. The built-in metrics include CPU utilization, storage usage, memory usage, read-write operations, and both ingress and egress bytes. These are the metrics you would probably want to monitor when managing a database. The monitoring system automatically creates dashboards for resources in your projects. It analyzes what resources have been created in a project and builds dashboards accordingly. This is a great way to get started. Often, the hardest part of monitoring is figuring out what you want to monitor. After you get some experience, you can take this a step further by creating your own custom dashboards.

### Quiz - [Module 8 Quiz](https://www.cloudskillsboost.google/course_templates/145/quizzes/515164)

#### Quiz 1.

> [!important]
> **Which monitoring metric below requires the Monitoring Agent to be installed 
on a virtual machine?**
>
> - [ ] CPU usage.
> - [ ] Disk I/O.
> - [ ] Memory usage.
> - [ ] All of the above.

#### Quiz 2.

> [!important]
> **Log data can be exported to where?**
>
> - [ ] BigQuery.
> - [ ] Pub/Sub.
> - [ ] Cloud Storage.
> - [ ] All of the above.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/145/video/515165)

- [YouTube: Module Review](https://www.youtube.com/watch?v=eftVjduymuc)

To review, in this module you learned to use automated unit integration and regression testing techniques to ensure database migration success and monitor your migration projects with Google tools.

## Google Cloud Data Migration Services

In this module, you learn about options and tools available to assist your data migration activties to Google Cloud.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/145/video/515166)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=AN7Fm40dxP0)

Hello everyone, Rashmi here. If you just have a few GBs of data in a SQL backup file that you want to restore in a database in Google Cloud, you can probably skip this module. But life is not always that simple. If you have terabytes of data or even petabytes of data, getting it in the cloud is more complicated. If you want to alter the data before loading it into the new database, that can be complicated too. Or you may be moving from one type of database to another. That requires some work as well. Google and our third parties have tools to help when things get difficult. In this module, you learn to move large amounts of data to the cloud using Google Transfer Services. You also learn to program data processing and ETL pipelines using Cloud Data Fusion and workflows using Cloud Composer. You also automate a database migration using a third-party tool called Stream.

### Video - [Google Cloud Data Migration Services](https://www.cloudskillsboost.google/course_templates/145/video/515167)

- [YouTube: Google Cloud Data Migration Services](https://www.youtube.com/watch?v=8omngJHWh5A)

Let's start with data migration. When you have a small amount of data, you can load it directly from your environment into the target database. For large amounts of data, you should use Cloud Storage as a staging area. After you get the data into Cloud Storage, you can then move it around at Google speed. Also, many Google Cloud tools are designed to work with Cloud Storage. For example, if you want to restore a database into Cloud SQL, it asks you to specify the location of the backup in a Cloud Storage bucket. Transferring data into Google Cloud is not a trivial operation. Multiple factors must be considered, including cost, time, offline versus online transfer options, transfer tools and technologies, and security and privacy. Although transfer into Google Cloud is free, there will be costs with the storage of the data, possibly hardware costs, and possibly egress costs if transferring from another cloud provider. If you have huge datasets, the time required for transfer across a network may be unrealistic. Even if it is realistic, the effects on your organization's infrastructure may be damaging while the transfer is taking place. This needs to be considered. The table above shows the challenge of moving large datasets. You have to decide whether to transfer data over the network or use a hardware appliance. Several tools are available to support network transfer, when only low bandwidth is available, such as Storage Transfer Service. Google also offers a hardware solution known as Transfer Appliance. Here Google ships you hardware that you fill with data from your data center and ship back, where it is transferred to Cloud Storage. The data is encrypted until you choose to decrypt it. Company policies might prevent transferring data over a public internet. In that case, direct peering or cloud interconnect are possible solutions. There are other considerations, such as protecting the data at rest, authorization and access to the source and destination storage system, protecting data while in transit, and protecting access to the transfer product. The general approach is to have the data encrypted and access gained only via access keys. Use gcloud storage to interact with Cloud Storage. gcloud storage commands are designed to look as much as possible, like Linux file system commands. To create a bucket, use the buckets create command. To copy files, use gcloud storage cp. To see files in a bucket, use alice. To delete files, use im. To synchronize a folder in a bucket, use the async command. Storage Transfer Service is a product that enables you to Move or backup data to a Cloud Storage bucket, either from other Cloud Storage providers, or from your on-premises storage. Move data from one Cloud Storage bucket to another, so that it is available to different groups of users of applications. And periodically move data as part of a data processing pipeline or analytical workflow. Storage Transfer Service provides options that make data transfers and synchronization easier. For example, you can Schedule one-time transfer operations or recurring transfer operations. Delete existing objects in the destination bucket if they don't have a corresponding object in the source. Delete data source objects after transferring them. And schedule periodic synchronization from a data source to a data sync with advanced filters, based on file creation dates, file name filters, and the times of day you prefer to import data. The gcloud storage utility also allows transfer of data between Cloud Storage and other locations. In addition, Google has Transfer Service for on-premises data. To help decide which tool to use, consider the following. If transferring from another Cloud Storage provider, use Storage Transfer Service. If transferring less than 1TB from on-premises, use gcloud storage. If transferring more than 1TB from on-premises, use Storage Transfer Service. If transferring less than 1TB from another Cloud Storage region, use gcloud storage. And if transferring more than 1TB from another Cloud Storage region, use Storage Transfer Service. Configuration and data transfer using Storage Transfer Service is easy. First, specify the source. The source can be another Cloud Storage bucket, an S3 bucket, Azure Storage, or an on-premises server that Transfer Service can access. Second, specify the bucket in which you want to transfer the data. Finally, specify when you want to run the job. The job can be run immediately or scheduled for a later time. You can also schedule recurring jobs. The Transfer Service for on-premises data allows large-scale online data transfers from on-premises storage to Cloud Storage. With this service, data validation, encryption, error retries, and fault tolerance are built in. On-premises software is installed. It comes as a Docker container, and then via the cloud console, directories to be transferred to Cloud Storage are selected. Once data transfer begins, the servers will parallelize the transfer across many agents. Via the cloud console, a user can view detailed transfer logs, as well as the creation, management, and monitoring of transfer jobs. To use the Transfer Service for on-premises data, a POSIX-compliant source and a network connection of at least 300 Mbps are required. Also, a Docker-supported Linux server that can access the data to be transferred is required with ports 80 and 443 open for outbound connections. The use case is for on-premises transfer of data greater in size than 1TB. Transfer Appliance is a secure, rackable, high-capacity storage server that you set up in your data center. You fill it with data and ship it to an ingest location, where the data is uploaded to Cloud Storage. Data is encrypted automatically and remains safe until decrypted. Two sizes of appliance are available, 100TB and 480TB. Transfer Appliance is easily mounted in rack space in a data center and can be mounted as network-attached storage. A simple user interface is provided to guide users through local data capture, and the cloud console is used to decrypt and ingest data. The process for using Transfer Appliance is that you request an appliance and it is shipped in a tamper-evident case. Data is transferred to the appliance, the appliance is shipped back to Google, data is loaded to Cloud Storage, and you are notified that it is available. Google uses tamper-evident seals on shipping cases to and from the data-ingest site. Data is encrypted to AES256 standards at the moment of capture. After the transfer is complete, the appliance is erased per NIST-800-88 standards.

### Video - [Programming Data Processing Pipelines with Data Fusion](https://www.cloudskillsboost.google/course_templates/145/video/515168)

- [YouTube: Programming Data Processing Pipelines with Data Fusion](https://www.youtube.com/watch?v=PkiT0Fzfuwo)

Data processing pipelines allow you to grab data from a source, alter it, and then save it to a sink or target. This is sometimes called an ETL job, that is extract, transform, and load. You can write a data processing pipeline in your favorite programming language, but there are also tools that help make it easier. Cloud Data Fusion is one of those tools provided by Google Cloud. Cloud Data Fusion is a fully managed cloud-native enterprise data integration service for quickly building and managing data pipelines. You can use it to cleanse, match, dedupe, blend, transform, or partition data. It will automate the execution of the job and allow you to monitor the job progress. A visual interface is available for drag-and-drop building of pipelines. You can also quickly test and debug pipelines with a small subset of your data. When you deploy pipelines, Cloud Data Fusion automatically provisions Google Cloud infrastructure to actually run the job. There is deep integration with Google Cloud, so you can run your data processing jobs at scale. Cloud Data Fusion's integration with Google Cloud simplifies data security and ensures that data is immediately available for analysis. Whether you're curating a data lake with Cloud Storage and Dataproc, moving data into BigQuery for data warehousing, or transforming data to export it to a relational store like Spanner, Cloud Data Fusion's integration makes development and iteration fast and easy. You build data pipelines with a friendly UI. The rich graphical interface allows for drag-and-drop visualization of pipelines. There are over 100 built-in plugins, connectors, transforms, actions, and support for many legacy data sources. The UI allows you to write pipelines without coding. You can test and debug pipelines, and pre-built images are available to get you started. For those who want to write code or for use cases where significant customization is required, there is also a developer SDK. Cloud Data Fusion instances are completely managed environments for building pipelines running in Google Cloud. There are two editions you can choose from when creating the Dataflow environment. Basic Edition is recommended for development and costs $1.80 per hour. Enterprise Edition is recommended for production and streaming pipelines. It costs $4.20 per hour. Additionally, pipelines run on a Dataproc cluster, so you are also charged for that. Cloud Data Fusion is based on an open-source project called CDAP. This is important because an investment in learning Cloud Data Fusion is not only useful in Google Cloud, but will also be useful in on-prem and hybrid environments. Pipelines provide an interface for building ETL jobs. You first connect it to a source, then transform the source data, and finally write to a sink. Wranglers provide a visual interface for specific data transformations. These are built-in drag-and-drop objects for manipulating the data. Cloud Data Fusion Hub provides access to drivers, plugins, and pre-configured pipelines. This is available from your Cloud Data Fusion instance when you started in your Google Cloud project. Pipeline represent a series of stages arranged in a directed acyclic graph or a DAG. Stage nodes in the pipeline graph can be of different types. Nonlinear pipelines are supported. The nodes can fork where output from a node can be sent to two or more stages, and then two or more forked nodes can merge at a transform or a sink node. There is also a scheduler where you can set up recurring batch jobs at an appropriate interval. Data analysts can explore data sets in Wrangler and preview the results of their transformation. This code-free visual environment is especially useful for data analysts who are often less comfortable with programming.

### Video - [Creating Workflows with Composer](https://www.cloudskillsboost.google/course_templates/145/video/515169)

- [YouTube: Creating Workflows with Composer](https://www.youtube.com/watch?v=WFireMuL8OM)

Cloud Data Fusion helps you build data processing jobs, but sometimes you want to coordinate larger workflows, which may include an ETL pipeline as just one of the steps. This is where Cloud Composer comes in. This sort of a workflow can be programmed, managed, and executed using Cloud Composer. Cloud Composer helps with the orchestration of various steps. Cloud Composer really is just a fully managed environment running Apache Airflow. Airflow is an open-source workflow engine. You can create your workflows with some simple Python code. You can find more information about Airflow at airflow.apache.org. Similar to Cloud Data Fusion, because Airflow is an open-source project and will run anywhere, your investment in learning it is not restricted to Google Cloud. Each Airflow environment hosts a separate web server to access the Airflow UI. There's also a folder created in Google Cloud Storage, where you place the Python code files for your pipelines. The DAGs folder is simply a Cloud Storage bucket where you load your pipeline code. To run a workflow, just save its code file into the Cloud Storage bucket. Airflow detects the new file and runs it on the schedule you specified in the file. Here is an example of a simple workflow. At the top, the workflow is defined. Inside the workflow are a series of steps which make up the DAG or directed acyclic graph. Each step is implemented as an operator and there are different types of operators. In this example, the first operator is a Python operator. It is used to invoke the Python greeting function above it. The second operator is a Bash operator. It is used to run a shell command or script. If you can run Python functions and shell scripts, you can do almost anything you need in Google Cloud. Remember, every Google Cloud resource can be created with a shell script. But there are many other types of operators as well. At the very bottom, after the workflow is defined, the order of the operators is specified. In this example, the operator Hello Python runs first, followed by what time is it operated. Airflow provides many operators to orchestrate Google Cloud and other services. The Airflow website allows you to monitor your workflows. You can see the history, how long each step took, whether there were errors, and so on. The console is available as part of the Google Cloud Composer environment and it is automatically created by the service.

### Video - [Third-Party Tools](https://www.cloudskillsboost.google/course_templates/145/video/515170)

- [YouTube: Third-Party Tools](https://www.youtube.com/watch?v=4Er18q6rHSU)

Google recommends leveraging third-party tools and hiring strategic partners to help with your database migration projects. Let's talk about a couple of those tools now. migVisor is an automated assessment tool. Recall that earlier in the course, you learned about Google's implementation methodology, which consists of four steps, assess, plan, deploy, and optimize. migVisor helps with those first two steps. It helps find dependencies and dependents, and makes recommendations for which Google databases and APIs you should pick as targets. Striim, however, will help with the deployment step. Striim allows you to do online database migrations, and it handles the transferring and synchronization of databases. migVisor's automated tool analyzes source database configurations, attributes, schema objects, and proprietary features. This helps build that initial plan required for a successful migration project. Striim provides an easy-to-use interface for transferring data between databases. For detailed information, go to Striim's website at www.striim.com. Striim allows you to stream data between a target and a source. Many targets and sources are supported. You can also do data transformations within the pipelines using SQL. Typically, an initial data transfer is done to move the bulk of the historical data. Then Striim captures data changes on the source in real time and synchronizes them with the new target database. During the synchronization period, you can migrate clients to the new database. Eventually, all of the old databases will have no clients, and it can be retired. Using Striim, you can even transfer data between different database types. Striim supports many different heterogeneous targets and sources. The table here lists some common targets and sources. Notice that it is not limited to only relational databases. It can be used with big data, data warehousing, and streaming analytics targets, as well as sources. Striim can be easily installed in Google Cloud from Marketplace. After the server is up, a simple configuration is run.

### Video - [Lab intro: Online Data Migration to Cloud Spanner using Striim](https://www.cloudskillsboost.google/course_templates/145/video/515171)

- [YouTube: Lab intro: Online Data Migration to Cloud Spanner using Striim](https://www.youtube.com/watch?v=NWL83FN3bQo)

In this lab, you use Striim to perform an automated data migration from Cloud SQL to Spanner. What you learn here applies to any target or source database. In this lab, you will deploy Striim through the Google Cloud Marketplace and set it up. Then you will use Striim to read from a SQL Server database running in Cloud SQL and perform an initial batch migration. Lastly, you will use Striim to continuously replicate from Cloud SQL to Spanner until you're ready to make the switch from the old database to the new one.

### Lab - [Performing an Online Database Migration](https://www.cloudskillsboost.google/course_templates/145/labs/515172)

This lab demonstrates how to perform an online migration of a SQL Server database to Cloud Spanner using Google Cloud's Data Migration Partner, Striim.

- [ ] [Performing an Online Database Migration](../labs/Performing-an-Online-Database-Migration.md)

### Video - [Lab review: Online Data Migration to Cloud Spanner using Striim](https://www.cloudskillsboost.google/course_templates/145/video/515173)

- [YouTube: Lab review: Online Data Migration to Cloud Spanner using Striim](https://www.youtube.com/watch?v=BVy9pLUpwcI)

In this lab, you deployed Striim through the Google Cloud Marketplace. Then you use Striim to read from a source SQL Database and continuously replicate data from SQL Server to Spanner. Striim is an automated tool that supports homogeneous and heterogeneous data migrations. It supports many different targets and sources. It can greatly simplify your database migration projects and make them more automated and reliable.

### Quiz - [Module 9 Quiz](https://www.cloudskillsboost.google/course_templates/145/quizzes/515174)

#### Quiz 1.

> [!important]
> **Your company needs to upload about 500 TB of data to upload to Google Cloud Storage. Which data transfer service would you recommend?**
>
> - [ ] Storage Transfer Service.
> - [ ] Cloud Data Fusion.
> - [ ] Transfer service for on-premises data.
> - [ ] Transfer Appliance.

#### Quiz 2.

> [!important]
> **You want to automate a recurring job that runs every night. The job needs to extract data from an on-premises database, copy it to Cloud Storage, then run a Dataflow job to transform the data and load it into BigQuery. Which would be best?**
>
> - [ ] Shell script written with gcloud storage.
> - [ ] Cloud Data Fusion.
> - [ ] Storage Transfer Service.
> - [ ] Cloud Composer.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/145/video/515175)

- [YouTube: Module Review](https://www.youtube.com/watch?v=94VmCsxYVQk)

In this module, you learned how to move large amounts of data to the cloud using Google Transfer Services. You also learned to program data processing and ETL pipelines using Cloud Data Fusion and Cloud Composer. Finally, you used Striim to automate the migration of an enterprise SQL Server database into Spanner.

## Making the Business Case for Moving to Google Cloud

In this module, you learn how to write a business case for a Google Cloud database migration project. 

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/145/video/515176)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=ukM4tXJOfDY)

Hi, Damon again. A business case outlines the pros, cons, risks, and rewards of doing a project. Let's talk about how you might write one for a Google Cloud database migration project. In this module, you learn to write a business case to justify a database migration. You perform risk and cost benefit analysis on a cloud migration project and estimate the costs associated with database migration.

### Video - [Writing a Business Case](https://www.cloudskillsboost.google/course_templates/145/video/515177)

- [YouTube: Writing a Business Case](https://www.youtube.com/watch?v=RSobQ-35qSg)

Management has many potential projects and a limited supply of resources to accomplish them. Priorities need to be made, and management must decide which are worthwhile endeavors. A business case is used to justify why the company should take on your project. Let's start by learning why a business case is important and learn what you should include when creating one. A business case answers key questions required to start a cloud journey. What are the goals? There may be many goals: improved efficiency, innovation, lower costs, increased productivity, and many others. The business case allows you to express and prioritize what your motivations are. The business case should include a plan for achieving your goals. Will you lift and shift your applications, improve then move them, rebuild everything, or some combination of all of these? Business people need to evaluate the risks versus the rewards. What will it cost compared to how much it will save or how much will we profit? Provide them an estimate of the time and budget required. Spending time upfront on assessment and planning will help deployment go faster. This doesn't mean you need to spend a year planning every detail, but at least you need a clear understanding of where you are and where you want to get to. The goals might be clear, but the path to get there might not be. Start by defining where you are and define why you want to move. If there are no problems with the current environment, migrating away from it wouldn't make sense. Maybe the current environment is too expensive, maybe it is too difficult to maintain. There might be innovations you want to take advantage of that the current environment makes impossible or impractical. Define thoroughly where you want to get to and why. What is the end goal? Maybe the goal was to be in Google Cloud, running cloud native, microservice applications, and managed services when possible. But why is this beneficial? It might be obvious to you, but not necessarily to the business stakeholders, so tell them these reasons need to be included and outlined in your business case. Earlier in the course, you learned about different strategies for migrating to the cloud. One strategy is lift and shift. Essentially, that means to just pick up your applications and databases as they are and then move them into the cloud. After your applications are in the cloud, you can start optimizing them or migrate functionality to manage services. As discussed earlier, you can optimize a monolithic application a little at a time. Eventually, the monolith disappears as a collection of microservices grows. This is Martin Fowler's Strangler pattern. There are times when lift and shift is not worth it. The cost and complexity of moving applications and databases to the cloud and then trying to optimize them might be too great. In the case of mainframe applications, a lift and shift approach might not be possible. Sometimes improve and move is a better strategy. Instead of migrating the applications and then optimizing them, refactor the applications on-premises and then move them to the cloud. For some apps, a rebuild would be better. There's a good argument to be made that you should reimagine the business. If you were just starting out, how would the business be different given the current technology? How would your applications be different if they were designed to run on today's hyperscale cloud platforms instead of yesterday's internal data center? Sometimes a complete redesign, rearchitect, and rebuild approach will yield better results and be cheaper over time. Companies that use this approach essentially leave the old systems where they are, build shiny new systems in the cloud, and gradually move users to the new system. Eventually, the old systems can be retired. For most organizations, there are multiple strategies. The best approach depends on the application, its users, and its importance to the organization. If you're just starting out or looking for a proof of concept, identify what apps would be the best to move first. Don't start with the most mission critical customer facing application you can find. That is too risky. Look for an app or apps that are less risky. You are also trying to learn and build a knowledge base, so don't look for the unusual app that is an edge case. Find something more typical. Pick a first mover that will be tolerant of a little downtime when the switchover happens. As you work on your business case, a careful application analysis will help manage the risk. Categorize applications based on both how hard they will be to migrate and how important they are to the business. Take a look at the chart on the screen. You would certainly be more comfortable choosing the marketing website or the dev test environment as the first movers as opposed to the ERP system or the OLTP database, which are harder and more critical. Plan your migration a few applications at a time, starting with a few easier migrations where you can learn and gain confidence and experience. Gradually, move on to the more critical and difficult systems. As you learn, you will be able to refine and optimize your migration processes. As part of your business case, you will be expected to give time and budget estimates. We all know that at the beginning of a project, this is often guesswork. However, as you gain experience with your initial proof of concept followed by a few first movers, you will be better equipped to refine those estimates. Business people will understand if your earlier estimates are wrong, as long as you are honest and refine those estimates over time. Your estimates will improve over time if you first build a POC, then migrate some simple applications. You will learn from previous efforts automate as much as possible and always be looking for ways to optimize your deployment processes. Running applications will help refine your cost estimates. Use the Google Cloud price calculator for initial estimates. Compare estimates with actual costs and analyze real costs to help improve estimates for future migrations. Use a standard approach to risk assessment and mitigation. This type of estimate shouldn't be very complicated. Estimate the total cost of ownership both in the cloud and on-premises. Don't just calculate the cost of running an application on Google Cloud and compare that to buying a server from Dell or HP. The total cost of ownership includes many things: administrative cost, the build, power, licenses, support, and so on. Estimate how much the migration will cost and how long it will take. Give a simple high, medium, or low estimate of risk and value. This kind of analysis gives the business people the information they need to make the right decisions for the organization. You will likely encounter pushback when moving to a new environment. Developers are invested in their current environments, which they devoted considerable time to learning. Moving to something new may seem risky and unnecessary to them. This requires a cultural shift. Develop cross-functional teams and train them so they understand the benefits of Google Cloud and are comfortable and knowledgeable using it. Help them understand the benefits, the processes, and goals. Identify a champion within the organization, someone with authority who will fight for you when you get pushback. The Google Cloud Adoption Framework assesses organizational readiness to migrate to the cloud. The A|doption Framework builds a structure on the rubric of people, processes, and technology that you can work with, providing a solid assessment of where you are in your journey to the cloud and actionable programs that get you to where you want to be. It is influenced by Google's own evolution in the cloud and many years of experience helping customers. The adoption framework has four themes and three phases. The four themes are learn, lead, scale, and secure. The learn theme evaluates the quality and scale of the learning programs you have in place to upskill your technical teams and your ability to augment your IT staff with experienced partners. Who is engaged? How widespread is that engagement? How concerted is the effort? How effective are the results? The lead theme measures the extent to which IT teams are supported by a mandate from leadership to migrate to the cloud, and the degree to which the teams themselves are cross-functional, collaborative, and self-motivated. How are the teams structured? Do they have executive sponsorship? How are cloud projects budgeted, governed, and assessed? The scale theme is the extent to which you use cloud native services that reduce operational overhead and automate manual processes and policies. How are cloud-based services provisioned? How is capacity for workloads allocated? How are application updates managed? The secure theme is the capability to protect your services from unauthorized and inappropriate access with a multi-layered identity-centric security model. It is also dependent on the advanced maturity of the other three themes. What controls are in place? What technologies are used? What strategies govern the whole? For each theme, there are three phases: tactical, strategic, and transformational. In the tactical phase, individual workloads are in place, but there is no coherent plan encompassing all of them with a strategy for building out to the future. In the strategic phase, a broader vision governs individual workloads, which are designed and developed with an eye to future needs and scale. You have begun to embrace change, and the people and processes portion of the equation is now involved. IT teams are both efficient and effective, increasing the value of harnessing the cloud for your business operations. In the transformational phase, cloud operations are functioning smoothly. You've turned your attention to integrating the data and insights garnered from working now in the cloud. Existing data is transparently shared. New data is collected and analyzed. The predictive and prescriptive analytics of machine learning are applied. Your people and processes are being transformed, which further supports the technological changes. IT is no longer a call center but has become instead a partner to the business. In this activity, you begin the process of creating a business case from moving to Google Cloud. Open the enterprise database migration case study from earlier in the course. Read the case study and then follow the instructions for Activity 2: Making the business case.

### Document - [Enterprise Database Migration Case Study](https://www.cloudskillsboost.google/course_templates/145/documents/515178)

### Video - [Activity review Writing a Business Case](https://www.cloudskillsboost.google/course_templates/145/video/515179)

- [YouTube: Activity review Writing a Business Case](https://www.youtube.com/watch?v=sMvTehCDUZ4)

In this activity, you were asked to create a value versus risk assessment for the real health systems case study. Obviously, some of the numbers are really just guesses, given we don't have enough information. But, this is the sort of assessment you would want to undertake when justifying a move to the cloud when doing a business case. Here is an example of a completed business case. You may pause the video to review it in detail.

### Quiz - [Module 10 Quiz](https://www.cloudskillsboost.google/course_templates/145/quizzes/515180)

#### Quiz 1.

> [!important]
> **What would be a good criterion for deciding on the best candidate application for moving to the cloud first?**
>
> - [ ] All of the above.
> - [ ] Few dependencies.
> - [ ] Not mission-critical.
> - [ ] No compliance issues.

#### Quiz 2.

> [!important]
> **Which is considered the best strategy for moving to the cloud?**
>
> - [ ] Improve on-premises, then move to the cloud.
> - [ ] It depends on the application.
> - [ ] Lift and shift, then optimize.
> - [ ] Rewrite as a cloud-native application.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/145/video/515181)

- [YouTube: Module Review](https://www.youtube.com/watch?v=eAvC_0SYbfk)

In this module, you learn how to write a business case to justify a database migration, perform risk and cost-benefit analysis on a cloud migration project, and establish the costs associated with database migration.

### Video - [Course Review](https://www.cloudskillsboost.google/course_templates/145/video/515182)

- [YouTube: Course Review](https://www.youtube.com/watch?v=IPwHGmxFzd0)

Thank you for attending this course on Enterprise Database Migration to Google Cloud. This video is the final lecture. In this course, you learned how to migrate on-premises enterprise databases to Google Cloud. You evaluated on-premises database architectures and planned migrations. You learned how to choose the appropriate Google Cloud services to run your database on. These include Compute Engine, Kubernetes Engine, Cloud SQL, and Bare Metal Solution. You learned about Google's methodology for migrating to Google Cloud and the steps in the methodology, which are assess, plan, deploy, and optimize. You deployed a secure network architecture using Terraform to automate resource creation. You learned how to run and administer SQL Server and Oracle databases on Google Cloud. Lastly, you learned to test and monitor database migrations and leverage tools to automate data transfer.

## Course Resources

PDF links to all modules

### Document - [Course Resources](https://www.cloudskillsboost.google/course_templates/145/documents/515183)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
