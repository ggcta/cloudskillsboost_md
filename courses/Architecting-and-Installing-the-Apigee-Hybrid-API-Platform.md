---
id: 167
name: 'Architecting and Installing the Apigee Hybrid API Platform'
datePublished: 2023-10-27
topics:
- GKE
- API
- Apigee
type: Course
url: https://www.cloudskillsboost.google/course_templates/167
---

# [Architecting and Installing the Apigee Hybrid API Platform](https://www.cloudskillsboost.google/course_templates/167)

**Description:**

This course introduces you to the fundamentals and practices used to install and manage Google Cloud's Apigee API Platform for hybrid cloud.

Through a combination of lectures, a hands-on lab, and supplemental materials, you will learn how to install and operate the Apigee API Platform.

**Objectives:**

- Identify the purpose and value of Apigee API Platform.
- Describe basic concepts and capabilities of Google Cloud, Kubernetes, REST and Anthos.
- Discuss Apigee API Platform architecture and recommended practices for topology design.
- Explain Apigee terminology and logical organizational structures.
- Install Apigee hybrid on Google Kubernetes Engine.

## Introduction

Introduction to the fundamentals and installation course on Google Cloud's Apigee hybrid API platform.

### Video - [Course series introduction](https://www.cloudskillsboost.google/course_templates/167/video/418355)

- [YouTube: Course series introduction](https://www.youtube.com/watch?v=bn4Cqpp2-FI)

Hi, I'm Hansel Miranda, a course developer at Google. I’d like to welcome you to Managing Google Cloud’s Apigee API Platform for Hybrid Cloud. This is a series of three courses that teach you the concepts and skills required to install and manage the Apigee hybrid API management platform. In the first course, we discuss fundamental concepts, the hybrid architecture, and you learn how to install the Apigee hybrid platform. In the second course, you learn how to manage the platform, and about the practices used to secure the hybrid runtime components. The third course teaches you about upgrading, monitoring, and troubleshooting Apigee hybrid. These courses are intended for cloud architects and operations specialists who want to install, manage, and operate the Apigee API platform using the hybrid deployment model. You must be familiar with Linux commands and have a basic understanding of Google Cloud, Kubernetes Engine, and networking, as well as common web protocols and standards like REST and HTTP. Through presentations, demos, and a hands-on lab, you will learn how to install and operate the Apigee hybrid platform on Google Cloud. The course will cover the architecture, the installation process, and the techniques used to manage and monitor the hybrid runtime platform. Specifically, you will learn about the product fundamentals, the Apigee hybrid architecture, and how to install and operate Apigee hybrid. You will learn about API proxy deployment and environment management, how hybrid components are secured, and capacity planning and scaling the hybrid platform. You will also learn how to upgrade Apigee hybrid and how to use hybrid logging and monitoring to troubleshoot issues with the platform. The lab in this course will be done using Qwiklabs. Qwiklabs provisions you with Google account credentials, so you can access the Google Cloud Console for the lab at no cost. For each lab, Qwiklabs offers a free set of resources for a fixed amount of time and a clean environment with required permissions. This course provides you with the knowledge and skills needed to install and manage Apigee hybrid. Take time to understand the concepts introduced in the lectures and the techniques used in the lab. The lectures, together with the lab and demos, teach you how to install, manage, and perform common operational tasks like scaling, monitoring, and troubleshooting Apigee hybrid. We hope you enjoy the course.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/167/video/418356)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=K-YJXl6y_Fk)

Hi. I'm Greg Kuelgen, Customer Success Engineer at Google Cloud. I'm filming from my home studio in Portland, Oregon, and I'd like to welcome you to the fundamentals, architecture and installation course of Google Cloud's Apigee Hybrid platform. This course will introduce you to Apigee and it's features. You will learn about the Apigee deployment models and get an overview of Apigee Hybrid. We will also cover some of the basic concepts of Google Cloud, Kubernetes, and Anthos services, that are used to manage Apigee Hybrid. We will discuss Apigee terminology, the organizational structure, and the management API used in Apigee Hybrid. You will learn about the architecture of Apigee Hybrid and the networking between hybrid components. You will learn how to install Apigee Hybrid on Kubernetes, and do a lab to install Apigee Hybrid on Google Kubernetes engine. You will also learn about the tools used to manage the hybrid platform. Finally, we will discuss the process to backup and restore your hybrid run-time data. Welcome to the course.

## Fundamentals

This module provides an introduction to Apigee services, and an overview of Google Cloud, Kubernetes and Anthos.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/167/video/418357)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=aYrCTLhPmrw)

Hello, and welcome to Fundamentals. In this module, we'll introduce the digital value chain, which explains how APIs can be used to implement connected digital experiences. You will learn about Apigee services and the hybrid deployment model that is used to install the Apigee platform. This module provides a high-level overview of Google Cloud, Kubernetes, and Anthos, which are the building blocks needed to install and manage Apigee hybrid. And as a bonus, you'll also get an introduction to REST API concepts.

### Video - [Apigee Overview](https://www.cloudskillsboost.google/course_templates/167/video/418358)

- [YouTube: Apigee Overview](https://www.youtube.com/watch?v=rJmIvbg4HLY)

Let's start with an overview of Apigee. Google Cloud's API management Platform. We'll discuss the business problems that can be solved using Apigee, see many of the features that Apigee provides, and introduce the components of Apigee and the deployment options for the Apigee platform. In later lectures, you will learn about the Google Cloud Platform and get a high-level overview of Kubernetes and Anthos. As a bonus, we will also discuss REST concepts in this module. Modern applications provide features and capabilities that legacy systems cannot directly provide. The pace of change in these digital experiences is much faster than in traditional systems, and so a gap exists between applications that provide connected experiences and those that provide the business and data systems of record. Applications that provide these connected experiences to end users must be able to do so securely and at scale. They have to manage the entire application lifecycle, connect to different backend systems, and be able to track and analyze the interactions between consumers and producers of data. Apigee is a fully featured API management platform that bridges the gap and enables application developers and API providers to create connected digital experiences for end users. The digital value chain helps enterprises provide customers or end users with connected digital experiences using their backend data and services. Web or mobile apps are built by internal enterprise developers or external third party companies. App developers leverage the APIs offered by your company. These APIs are built and managed by the API team within the enterprise. APIs integrate with backend services and other service endpoints, thus providing a clean and seamless interface to applications. By focusing on the consumption side of the digital value chain, Apigee improves the app developer experience and makes it easier for your APIs to be consumed successfully. The Apigee platform includes an API services layer that provides the runtime API gateway functionality. This layer includes the API processing engine, which enables you to add policies and programing to your API proxy. During proxy execution, these policies are applied to requests from apps and responses from backend systems. API Services also provide security functionality in the form of OAuth and other security policies for identity verification, authentication and access control. It supports features for request and response mediation, revision control and versioning, orchestration and much more. To enable the digital value chain, the Apigee platform includes Developer Services. This includes a developer portal that enables app developers to consume your APIs. App developers can register their applications and view the API catalog. Developers can browse your API documentation and even try out your API using Apigee's SmartDocs feature. Any API program must be able to measure and track its performance. The Apigee platform includes Analytics Services which enable enterprises to report on various aspects of the API program. It has many dashboards that chart various dimensions and metrics, including API proxy traffic, response times, target response times, cache performance, developer engagement, and traffic composition. You can view these dashboards in the Apigee UI or retrieve analytics data using the management API. You can also create your own custom reports to chart dimensions that are not in the default dashboard reports, or even add your own custom dimensions. Analytics Services is part of the core platform and is a critical component used in API management. Using this service enables you to gauge the operational health of the platform and determine the overall business value and success of the API program. API Services provide the runtime execution context for your API. To develop the API proxy logic, you include policies in the proxy or target endpoints of API proxy and within the request or response flows. A policy is a configuration file written in XML that provides a specific functionality. Apigee has various pre-built policies that implement traffic management , request, response mediation and security. You can also write custom code that can be executed by extension policies. Apigee supports three developer portal solutions: an integrated portal, a Drupal-based portal and a do-it-yourself, or DIY, portal. The integrated portal is a lightweight portal available in the API hybrid UI. It's the fastest and easiest way to onboard app developers and enable them to register their apps, browse API documentation and start using your APIs. You can customize the look and feel of the integrated portal using custom themes, menus and JavaScript code. The Drupal portal is a fully customizable portal that you host and manage. In addition to the customization options available within the integrated portal, you can implement Apigee monetization, include blogs and forums and have custom developer and app registration flows. The DIY portal is a fully customizable portal that you develop and host using the Apigee management APIs. It involves the most effort and the highest risk and takes the longest to implement. Apigee analytics provides pre-built dashboards for the entire API digital value chain. These dashboards include charts that report on analytic dimensions such as the top apps by API consumption, device type platform, user agent and geographic location. Developer analytics dashboards include charts on the top developers, API products and API proxies consumed. API Proxy analytics dashboards have charts on proxy performance, cache performance, error codes and target performance. These charts report on different metrics such as total traffic response times and cache hits. Apigee offers flexible platform deployment options. Apigee's Google-managed software as a service deployment simplifies customer adoption and dramatically accelerates time to market for new APIs. Developers can immediately start building and running APIs at scale. This is the most popular deployment option. It allows customers to focus on addressing business needs while letting Google manage the operational overhead of running the software at scale and in a secure and reliable way. For customers who are willing to manage the Apigee infrastructure, Apigee can be deployed and managed in a private cloud. Apigee can be installed in a customer data center or in an infrastructure as a service cloud. Customers who want to reduce the infrastructure management cost but retain the flexibility of running APIs in multiple clouds or on-premises can choose the hybrid deployment model. This model allows the customer to manage and deploy containerized versions of API runtime wherever necessary, while delegating the management plane operations to Google. Apigee's Hybrid deployment model takes advantage of Anthos, Google Cloud's hybrid and multi-cloud application platform, which simplifies the management of runtime components across multiple cloud and data centers. This course teaches you how to install and manage the Apigee hybrid deployment model. The Apigee management API is a fully featured REST API that is hosted in the Apigee hybrid management plane. The API must be enabled for your Google Cloud project. The API uses OAuth 2.0 for user authentication. OAuth tokens can be generated using Cloud credentials. The Apigee API enables you to manage entities such as organizations, environments, developers and API products. Apigee hybrid enables you to deploy APIs within your data center on-premises, on Google Cloud and on other cloud providers. Apigee hybrid consists of a management plane hosted and managed by Google and a runtime plane that you install on Kubernetes using GKE, Anthos, and other platforms. Hybrid supports a multi-cloud strategy that enables you to manage enterprise API gateways in your data center or by using multiple cloud providers. All API traffic is processed within the boundaries of your network and under your control, but management services such as the UI, API and analytics run in the cloud and are maintained by Google. There are many advantages of using Apigee hybrid. You are required to manage fewer software components with this deployment model than in a fully developed private cloud deployment, thus reducing the cost to install, manage and operate the runtime plane. The software components run in containers on Kubernetes, so you get all of the benefits of a container orchestration platform, including rolling updates and autoscaling. By installing the hybrid runtime plane in close proximity to backend services, you achieve reduced latency with your APIs. Because you can install the hybrid runtime plane in your data center or private cloud, it can be used to process both internal APIs and public externally facing APIs. Finally, you have complete control over the management and operations of the hybrid runtime plane.

### Video - [Introduction to Google Cloud](https://www.cloudskillsboost.google/course_templates/167/video/418359)

- [YouTube: Introduction to Google Cloud](https://www.youtube.com/watch?v=nbNnF6CQrQI)

In this lecture, we'll discuss Google Cloud and some of the features that are used by Apigee hybrid. Google Cloud is a suite of compute, network, storage, and other services that run on Google's infrastructure. With data centers around the globe, you can provision VMs, storage, networking and other services such as machine learning on Google Cloud. These data centers are located in regions and each region is composed of one or more zones. Cloud resources are deployed in one or more zones within a region. You can think of zones as isolated locations within a region, each with its own power, cooling, etc.. The distribution of resources provides several benefits, including redundancy in case of failure and reduced latency because resources are closer to clients. Google Cloud has regions all over the world. This provides a lot of flexibility to locate compute and other resources based on your application requirements. For instance, you might want to place compute resources for certain applications close to your home office, while choosing other geographic regions for applications that serve customers located globally. We give you a range of regions around the world: Asia, Australia, North America, South America and Europe. So you can place resources close to where you need them. Google Cloud also continues to invest in new regions. Each region also includes at least three availability zones, so you can build highly reliable applications even if you experience large-scale system failures. Apigee hybrid can be deployed in any available region. When building our Cloud, we focused on three fundamental principles. One, provide the tools to operate your infrastructure and application securely. Two, enable you to accelerate development with intelligence and speed. Three, focus on the pieces that affect your business in an open and cost-efficient manner. Google Cloud is a comprehensive platform that supports ingestion of data, whether in real time through Pub/Sub or with a data transfer service where we ship a device to your data center to transfer information. We give you the tools for building data pipelines, for data wrangling and for cleaning up data. There are tools for data warehousing like BigQuery, as well as a variety of tools for doing analytics using spreadsheets, taking advantage of AI and ML and enabling data scientists. Using Google Cloud, you can implement your application migration and modernization strategy incrementally and in a cost-efficient manner by running applications and services across multiple clouds and on-premises. Regions are independent geographic areas that consist of zones. A zone is a deployment area for a resource within a region. For high availibility, applications are deployed across multiple zones in a region. Google Cloud resources are fundamental components that make up all Google Cloud services. Some services and resources can be accessed globally (cloud storage, disk images, networks); some only within a region. (static external IP addresses); and others within a zone (VMs, disks). In Google Cloud, resources are organized in a hierarchy. Companies typically map their organizational structure to the Cloud resource hierarchy, which provides logical attach points for IAM access policies. The organization resource is the root node of this resource hierarchy. It provides central visibility and control over every other resource that belongs to the organization. An additional and optional grouping mechanism under the organization node is the folder. Folders can be mapped to departments, teams or business units within an organization. Under the folder or organization node is the project resource. The Google Cloud project is the base level entity for organizing resources. It is required to create resources, use Cloud APIs and services, manage permissions enable billing, etc.. To access the organization and other cloud resources, you need a Google Cloud account. The account must be granted the appropriate IAM roles in order to access organization resources. For example, to access the organization resource, the user account must be granted the organization admin role. Access control for resources is managed by IAM policies and IAM policy contains bindings of user accounts to roles in addition to other metadata. It is attached to a resource and is inherited by any child resources. There are three basic methods you can use to interact with Google Cloud Services and resources. These are the Google Cloud Console, the Cloud Shell CLI (Command Line Interface) and client software libraries. From the Google Cloud Console, you can create a new project or choose an existing project and use the resources that you create in the context of that project. The Cloud SDK provides the gcloud command line tool that you use in Cloud Shell. It can also be used to programmatically manage cloud resources. To provision and start using Apigee hybrid. Follow these steps. First, you will need a Google Cloud account and organization which are created when you sign up to use Google Cloud services. You will also use a Google Cloud identity account and organization Apigee hybrid. Next, create a project under the Cloud organization via the Cloud Console. You will need to enable Apigee APIs in your project via the console. Apigee APIs are used by the hybrid services to communicate with other cloud services. Use the Apigee API to create an Apigee Hybrid organization. This is a separate resource under the Cloud organization. The Apigee Hybrid Organization ID must match the Google Cloud Project ID. Note that trial hybrid organizations are valid for 60 days. Create one or more environments via the Apigee hybrid UI. An environment is needed to deploy and run your API proxies. Create an environment group via the Apigee hybrid UI and assign one or more environments to the group. Environment groups allow you to group environments together and provide the domain names for routing API traffic to the proxies deployed to the environments within the group. You must have a domain name that you can use for your Apigee hybrid installation and you must create at least one environment group. Apigee hybrid uses service accounts to perform various tasks during normal operation, such as sending logs and metrics data to the management plane, connecting to the management plane, downloading proxy bundles and executing backups. As part of the hybrid runtime plane installation process, you create several service accounts to perform these tasks. These service accounts are assigned roles with the minimum set of permissions that are required to perform the task. Apigee provides the create-service-account tool to create all of the service accounts needed for installation and operation of hybrid.

### Video - [Introduction to Kubernetes and Anthos](https://www.cloudskillsboost.google/course_templates/167/video/418360)

- [YouTube: Introduction to Kubernetes and Anthos](https://www.youtube.com/watch?v=LmzncavfJnI)

In this lecture, we will discuss some of the basic concepts of Kubernetes and Anthos. In the early days of server virtualization, you could run multiple servers and operating systems on the same physical hardware using a hypervisor. Today you create virtual machine images and deploy them fairly quickly, thus improving hardware utilization and deployment time. But with a VM, you are still bundling the application, dependencies, and operating system together. This isn’t fully portable because you can’t move your VM to an environment running a different hypervisor, such as the cloud or your laptop. And every time you start a VM, the OS has to boot, which takes time. But this was a great step forward from dedicated servers because it raised the abstraction layer, and the VMs allowed you to abstract away the hardware and hypervisor. The VM-centric way to solve the app dependency and runtime isolation problem is to run a VM for each application with its own set of dependencies. The result is that multiple copies of the kernel are running, which makes this deployment model redundant and wasteful. When you raise the abstraction level and virtualize the OS, the application and its dependencies can be packaged and run together in a container. You can run a container anywhere that runs the Linux kernel, which makes it highly portable. It starts up quickly, so your app scales quickly. It is lightweight because it doesn’t carry a full OS and it can use the underlying hardware very efficiently. This is the right level of abstraction and is the next step in the evolution of managing code. However, with containers, you still need a system to orchestrate and manage them in production environments. Kubernetes is a container orchestration system that is used to deploy and run application containers. It comprises a set of control processes that continuously drive the current state of application workloads toward the desired state. It can run on laptops, multi-node clusters, in public clouds, on-premises, on VMs, and on bare metal. Kubernetes manages containerized applications that run on a set of machines known as a cluster. In GKE, a cluster consists of at least one control plane and multiple worker machines, called nodes. These control planes and node machines run the Kubernetes cluster orchestration system. In GKE clusters can be a single zone, multi-zone or regional. A single-zone cluster has a single control plane running in one zone. This control plane manages workloads on nodes running in the same zone. A multi-zonal cluster has a single control plane running in one zone and has nodes running in multiple zones. Multi-zonal clusters balance availability and cost for consistent workloads. A regional cluster has multiple replicas of the control plane running in multiple zones within a given region. Nodes also run in each zone where a replica of the control plane runs. Because a regional cluster replicates the control plane and nodes, it consumes more Compute Engine resources than a similar single-zone or multi-zonal cluster. Regional clusters have control planes and nodes spread across zones for high availability and resilience from single zone failure with no downtime during control plane upgrades. The cluster control plane processes include the Kubernetes API server, scheduler, and core resource controllers. All interactions with the cluster are done via Kubernetes API calls that are handled by the Kubernetes API Server running on the control plane. The control plane is responsible for scheduling and managing the lifecycle, scaling, and upgrade of containerized application workloads on the nodes. A node runs the container runtime, Kubernetes, node agent (kubelet), and other services. Kubelet communicates with the control plane and is responsible for starting and running containers scheduled on that node. kube-proxy is a network proxy that maintains the network rules that allow communication with the pods running on the nodes. A pod is the smallest deployable unit of compute that you can create and manage in Kubernetes. A pod is a group of one or more containers with shared storage and network resources and a specification for how to run the containers. Pods are not usually created directly, but instead are created using Kubernetes workload resources like Deployments or Jobs. Pods in a Kubernetes cluster are used in two main ways: Pods that run a single container, and Pods that run multiple containers that work together in a sidecar pattern. These co-located containers form a single cohesive unit of service. In Apigee hybrid, some of the runtime components use sidecar containers to generate and authenticate OAuth tokens while communicating with the management plane. Pods are ephemeral, disposable entities. When a pod is created, it is scheduled to run on a node in the cluster. The Pod remains on that node until it finishes execution, the Pod is deleted, the Pod is evicted for lack of resources, or the node fails. A deployment is a declarative way to create and manage pods in Kubernetes. It defines a ReplicaSet that specifies the desired number of pod replicas needed. The Deployment Controller in Kubernetes changes the actual state of the deployment to the desired state at a controlled rate. The purpose of a ReplicaSet is to maintain a stable set of replica pods running at any given time. A deployment is defined using a YAML file that specifies the desired number of pods and a selector label that identifies the pods to be included in the deployment. It also includes a specification that identifies the containers that will run in the pod. The example Deployment in the slide manages 4 pod replicas with the role label set to “web.” A StatefulSet is the workload object that is used to manage stateful applications. It manages the deployment and scaling of a set of pods, and provides guarantees regarding the ordering and uniqueness of these Pods. Like a Deployment, a StatefulSet manages pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a persistent identity for each of the pods, which is maintained across any rescheduling. These identifiers make it easier to match existing volumes to new pods that replace any that have failed. Apigee hybrid uses a StatefulSet to deploy Cassandra, the datastore that stores objects used by the runtime plane. A DaemonSet ensures that all (or some) nodes in the cluster run a copy of a pod. As nodes are added to the cluster, the DaemonSet pods are added to them. As nodes are removed from the cluster, those pods are garbage collected. Typical uses of a DaemonSet include running a logs collection daemon on every node or running a node-monitoring daemon on every node. In Kubernetes, a service is an abstraction that defines a logical set of pods and a policy by which to access them. The set of pods targeted by a service is usually determined by a selector. A service has a fixed IP address that lasts for the life of the service, even as the IP addresses of the member pods change. Because a pod is ephemeral, its IP address changes as it is deleted and re-created. Therefore it doesn't make sense to use Pod IP addresses directly. Clients call the service IP address instead, and their requests are load-balanced across the pods that are members of the service. Like other Kubernetes resources, a service is defined in a manifest file in YAML format. In the example shown, the kind of resource is specified as a Service. The service is given a name; in this example it is named frontend-svc. The service also has a selector set to the role: web. This selects all pods that have a role label set to web and are part of the frontend service. The service type is defined as LoadBalancer. A load balancer service is externally accessible and can be reached by clients that know the DNS name or IP address of the service. In addition to LoadBalancer, two other types of services that are commonly used in Kubernetes are ClusterIP and NodePort. External clients call the service by using the load balancer's IP address and the TCP port specified by port value in the YAML. The request is forwarded to one of the member pods on the TCP port specified by the targetPort. Labels are key/value pairs that are attached to Kubernetes objects, such as pods. Labels are used to specify identifying attributes of objects that are meaningful and relevant to users. They can be used to organize and to select subsets of objects. In the example shown, there are 4 pods with 3 labels attached to each of them: app, env, and role. To list all of the pods running in the myapp application in production, you could filter requests with the app =myapp, and env=prod label selectors. To list all the pods running in the myapp application in the test environment, you could filter requests with the app = myapp, env=test label selectors. A Kubernetes volume is a directory that is accessible to all of the containers in a pod. To use a volume, a pod specifies what volumes to provide for the pod and where to mount those into containers. After the volume is mounted, the containers in the pod are brought online and the rest of pod initialization is completed. There are many different types of volumes in Kubernetes. Volumes, such as those backed by block stores, may outlive the pod. They will be unmounted when a pod goes away and are re-mounted on new pods if needed. Some volumes, like ConfigMaps and Secrets, are coupled to the life of the pod and cease to exist when the pod ceases to exist. One class of volume is a persistent volume which has a life cycle of its own, independent of the pod. A Custom resource is an extension of the Kubernetes API that represents a customization of a particular Kubernetes installation. After a custom resource is installed, users can create and access its objects using kubectl, just as they do for built -in resources like pods. When combined with a custom controller, a custom resource provides a declarative API that allows you to specify the desired state of the resource, which is then kept in sync with the current state of the resource in the cluster. A custom resource can be created by defining a CustomResourceDefintion or CRD object in Kubernetes using a specified name and schema. Apigee hybrid defines a custom resource definition called ApigeeDeployment. The ApigeeDeployment CRD is used to create, update, and release stateless hybrid components such as message processors in a Kubernetes cluster. We discussed some of the Kubernetes resources that are used by Apigee hybrid. Lets now talk about Anthos, the hybrid and multi-cloud platform also used by Apigee hybrid. Anthos is a modern application management platform that provides a consistent development and operations experience for cloud and on-premises environments. Anthos is used to manage Kubernetes installations for deploying and managing workloads across the enterprise. The primary computing environment for Anthos uses Anthos clusters which extend GKE for use on Google Cloud, on premises or other cloud providers. Anthos comprises a set of core components that implement various features in your hybrid cloud installation. All of these components run on Google Cloud and on-premises, while some of them also run on other cloud providers currently supported by Anthos. These components implement infrastructure and cluster management, configuration management, workload migration, service management, serverless workloads, logging and monitoring, and the application marketplace. For infrastructure and cluster management, Anthos uses GKE on Google Cloud, Anthos clusters on other supported cloud providers, and Anthos clusters on VMware for clusters running on-premises. For configuration management, Anthos Config Management is used on Google Cloud, other supported cloud providers, and on-premises. To migrate workloads running on-premises to Google Cloud, there is Migrate for Anthos. Services on Google Cloud, other supported cloud providers, and on-premises can be managed using Anthos Service Mesh. Cloud Run for Anthos enables you to run serverless workloads on-premises or on Google Cloud. For logging and monitoring, Cloud Logging and Cloud Monitoring is available on Google Cloud and on-premises. The Kubernetes applications in Cloud Marketplace are available for Google Cloud and on-premises. The primary computing environment for Anthos relies on GKE on Google Cloud, on-premises, or other supported clouds to manage Kubernetes installations in the environments where you deploy your applications. With Anthos on Google Cloud, GKE manages the node components in the customer’s project. With Anthos clusters on VMware, all components are hosted in the customer's on-premises environment, and with Anthos clusters on other supported clouds, all components are hosted in the customer’s cloud environment. To manage service communication, networking, authentication, and other features within your clusters, install Anthos Service Mesh. Anthos Service Mesh is based on Istio, which is an open source implementation of the service mesh infrastructure layer. To declare and manage cluster resources consistently across multiple environments, use Anthos Config Management. Cloud Run for Anthos is powered by Knative which is an open source project that supports serverless workloads on Kubernetes. It provides a developer-focused experience for creating and running modern applications on-premises or on Google Cloud Logging and Cloud Monitoring provide a unified place to view logs and monitor the health of your cluster resources. Anthos environments outside of Google Cloud must be able to reach Google’s API endpoints for Cloud Logging and Cloud Monitoring services and to register clusters with the Google Cloud Console. You can connect your on-premises, other cloud, and Google Cloud environments in various ways. The easiest way is by implementing a site-to-site VPN between the environments using Cloud VPN. For more stringent latency and throughput requirements, you can choose between Dedicated Interconnect and Partner Interconnect. As the number of clusters increases and starts spanning multiple environments, the cluster administrator has additional complexity to manage in terms of resources and consistency. With Anthos Config Management, you create a common configuration for all administrative policies that apply to your Kubernetes clusters both on-premises and in the cloud. Configuration is treated as data that is stored in a git repository and serves as a single source of truth for your cluster configurations. Anthos Config Management evaluates changes and rolls them out to all clusters so that your desired state is always reflected in a consistent manner. Cloud Logging and Cloud Monitoring are the built-in observability solutions for Google Cloud. They offer a fully managed logging solution, metrics collection, monitoring, dashboards, and alerting. Cloud Monitoring monitors GKE on-prem clusters in a way similar to GKE clusters on Google Cloud. It is a powerful cloud-based observability solution that is easy to configure and provides visibility to your Anthos environments running on-premises or on Google Cloud. Istio is an open source implementation of the service mesh model that lets you connect, secure, control, and observe services running in your cluster. Istio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and other functionality with few or no code changes in service code. You can also add Istio support to services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between services, and then configure and manage Istio using its control plane. The Istio control plane contains these components: Pilot provides service discovery and propagates configuration to the different proxy services. Galley is used for configuration management in Istio on Kubernetes and other platforms. Citadel is used for TLS certificate management within the service mesh. Cloud Run for Anthos is a standalone product that works with Anthos as well as on GKE without an Anthos subscription. Cloud Run is powered by Knative, an open source project that supports serverless workloads on Kubernetes. Cloud Run for Anthos abstracts away complex Kubernetes concepts, allowing you to easily leverage the benefits of Kubernetes and serverless together. It provides access to custom machine types, additional networking support, and Google Compute Engine hardware accelerators. It allows you to run your workloads on-premises or on Google Cloud. Cloud Run for Anthos provides you with more control over your services because it allows you to access your VPC network and run your services in all GKE regions. Anthos Service Mesh is a suite of tools that helps you monitor and manage a reliable service mesh. A service mesh is an infrastructure layer that enables managed, observable, and secure communication across services running in your cluster. Anthos Service Mesh is based on Istio, the open source service mesh platform, and is deployed as a uniform layer across your entire infrastructure. It enables you to create, deploy, and manage services on Google Cloud, GKE on-prem, and other supported clouds. The Anthos dashboard in the Google Cloud Console provides you with a secure, unified user interface to view and manage your Kubernetes clusters running on Google Cloud, on-premises, or on other supported clouds. It includes an out-of-the-box structured view of all your Anthos resources. The dashboard landing page gives you a runtime status summary for all your services and clusters. It also includes the Anthos Service Mesh Dashboard, which provides service-level metrics for observability into the health and performance of your cluster services. For easy installation and management of third-party applications, you can use Google Cloud Marketplace. The marketplace contains apps that deploy to your Anthos clusters regardless of where they are running. You can also search and filter for Anthos apps and easily find Anthos-compatible solutions. Cloud Marketplace solutions have direct integration with your existing Google Cloud billing and are supported directly by the software vendor. Anthos provides a wide range of benefits for both development and operations. For development, Anthos provides a container management platform based on Kubernetes. Developers can use this platform to quickly and easily build and deploy existing container-based applications and microservices-based architectures. They can build CI/CD workflows for configuration and code and have code-free instrumentation with logging and monitoring. Operational benefits include centralized configuration management, centralized deployment and management of clusters, and single-pane-of-glass visibility across all infrastructure. With Anthos, you can enforce security standards on clusters using auditable configuration and secure microservices using mutual TLS.

### Video - [REST Concepts](https://www.cloudskillsboost.google/course_templates/167/video/418361)

- [YouTube: REST Concepts](https://www.youtube.com/watch?v=WdUdOeJY6d8)

Let's discuss some high level concepts of REST and the HTTP protocol that is used by REST APIs. HTTP isn't application layer protocol that allows a client to fetch and update resources from a server. It could be used to fetch hypertext documents, text, and binary data. It can also be used to create, update, and delete data on a server. Clients and servers use a request response message pair to facilitate this data exchange over HTTP. A request consists of an HTTP method or verb that defines the client operation, the path of the resource, and the form of a URL, the HTTP version, and any optional headers. Depending on the method used, a request body is also included in the request. A server response includes the HTTP version, status code of the operation, a status message, HTTP headers, and optionally, a response. The request and response message formats contains optional headers and an optional message body, depending on the HTTP method used. For methods post, put, or patch, the creator update resources on the server, a body is typically included in the request and response messages. In addition, the request message usually contains standard headers to indicate the format and length of the body data in the request and response. REST was defined in a 2000 PhD doctoral dissertation at the University of California, Irvine. Roy Fielding and his colleagues designed the REST architectural style or version 1.1 of HTTP was being designed. Roy Fielding and his team used HTTP concepts to create a simple pattern for APIs. REST APIs leverage common web HTTP concepts like URLs, verbs and status codes. Request and response message payloads are generally specified using JavaScript Object Notation or JSON. The use of these common web patterns and REST APIs tend to reduce the app developer learning curve, and helps make REST APIs self-documenting and easy to adopt. REST APIs use well-defined URLs that contain the resource type being operated on. URLs contain the plural noun form of the resource type. For example, owners or dogs. If the API operates on a specific resource object, the idea of the resources included in the URL path following the resource now. For APIs that operate on a subset of resources, query parameters are used in the URL. There are techniques to request partial responses an pagination using URL query parameters. When you need to version, a best practice is to version your APIs using a version string as a path fragment in the URL. JSON is a lightweight data interchange text format that is easy for humans to read and write. It is also easy for machines to parse and generate. It is based on a subset of the JavaScript programming language standard. JSON is built on two structures, a collection of name value pairs and an ordered list of values. Curl is a command line tool and library to get and send data using URL syntax. It supports multiple protocols and performs SSL certificate validation when used with secure protocols like HTTPS. Through the Libcurl library, curl supports standard HTTP methods such as git, post, and put. It's a popular tool used to test REST APIs from the command line. Here are some examples of using curl. You can easily make a GET call to a URL by specifying it as an argument to the curl command. The output of the command contains the response from the URL. When you specify the -v option, curl outputs verbose information as it submits the request and receive the response from the requested URL. You can also supply authentication options like a username and password on the curl command line. And you can specify which HTTP methods to use when making API calls by using curl with the -x option. Each option is used to supply optional headers in the curl request. Curl supports a wide variety of command line options that are documented on the Curl Man page.

### Quiz - [Fundamentals](https://www.cloudskillsboost.google/course_templates/167/quizzes/418362)

#### Quiz 1.

> [!important]
> **How does Apigee help in the digital transformation of an enterprise?**
>
> - [ ] Using Apigee, you can easily and securely integrate with backend services.
> - [ ] Apigee enables the entire digital value chain, from exposure to consumption of services and data, using APIs.
> - [ ] Apigee can be used to break up a monolithic application into microservices.
> - [ ] Apigee provides capabilities of a service mesh that you can use to run microservices.

#### Quiz 2.

> [!important]
> **Which set of HTTP methods contains one that is invalid?**
>
> - [ ] PUT, TAIL, GET
> - [ ] OPTIONS, TRACE, PATCH
> - [ ] GET, POST, PUT
> - [ ] PATCH, HEAD, DELETE

#### Quiz 3.

> [!important]
> **What are three benefits of using Apigee hybrid?**
>
> - [ ] With Apigee hybrid, you operate your API runtime plane with fewer software services than in a private cloud deployment, resulting in reduced cost of ownership.
> - [ ] The hybrid runtime plane is deployed to Google Cloud, to other clouds, or in your own data center, giving you complete control over its management and operations.
> - [ ] Apigee hybrid runs on Kubernetes, so you can achieve autoscaling and other operational benefits of a containerized system.
> - [ ] Installing the Apigee hybrid runtime plane on Google Kubernetes Engine provides automatic access to all other Google Cloud APIs and services.

#### Quiz 4.

> [!important]
> **Which three deployment models are supported by Apigee?**
>
> - [ ] Customer-managed management plane and Apigee-managed runtime plane
> - [ ] Apigee-managed management plane and customer-managed runtime plane
> - [ ] Customer-managed management and runtime planes
> - [ ] Apigee-managed management and runtime planes

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/167/video/418363)

- [YouTube: Module Review](https://www.youtube.com/watch?v=bGg8xP3o4io)

In this module, you learned about Apigee and the services it provides to help customers implement their digital value chain. You also learned about Apigee Hybrid as one of the deployment models for installing Apigee. I gave you a high-level overview of Google Cloud, Kubernetes, and Anthos, which are the building blocks needed to install and manage Apigee Hybrid. We also covered a brief introduction to rest concepts. In the next module, you'll learn about Apigee Hybrid architecture, Apigee terminology, and networking concepts that are needed to install and manage the Hybrid platform.

## Architecture

This module discusses the Apigee hybrid achitecture, terminology and networking.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/167/video/418364)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=tR8MsO8tTUk)

In this module, you will learn about hybrid terminology and core concepts like the Apigee organization and environment. The Apigee Hybrid Architecture, its components, and their operations are discussed. You will also learn about the hybrid management plane, the hybrid UI, and the management API, and how they are used to manage your hybrid platform. You will learn about networking an Apigee hybrid and how virtual hosts are configured for environments.

### Video - [Terminology and Organizational structure](https://www.cloudskillsboost.google/course_templates/167/video/418365)

- [YouTube: Terminology and Organizational structure](https://www.youtube.com/watch?v=as7ImROCKp8)

In this lecture, we will discuss the terminology and some of the core concepts used in Apigee. We will also discuss organizational structure and the entities that are required in Apigee hybrid. In later lectures, you will learn about the Apigee hybrid architecture, networking, and the Management API. An API is an interface that enables an application to consume capabilities or data from another application. By defining stable entry points to application logic and data, APIs enable developers to easily access and reuse application logic built by other developers over the network. An API also implies a contract. The contract provide some level of assurance that over time the API will change in a predictable manner. In Apigee an API proxy is a bundle of XML configuration files and optional resources that decouples the application facing API from your backend service. It enables the backend to undergo changes to the service while allowing apps to continue to call the same API without any interruption. An Apigee policy is a module that implements a specific management function as part of the proxy in its request or response flow. Policies enable you to add common types of capabilities to an API easily and reliably without the need to write code. They provide features such as security, rate-limiting, transformation, and mediation. You can control the behavior of your API by implementing policies in your API proxy. In order for an API proxy to receive requests in Apigee hybrid, it must be deployed to an environment that is mapped to a virtual host. A virtual host is a piece of configuration that allows hybrid to route API requests using multiple host aliases. The host alias is typically the DNS domain name that maps to an IP address and it is used as the endpoint by client applications that send requests to the API. A target server is a piece of configuration that abstracts the location of a backend resource out of an API proxy. You create a target server configuration by providing its logical name, the hostname and port of the target server, and optional TLS configuration. The API proxy forwards requests from client applications to the target backend by referencing the name of the target server in the target endpoint configuration. An API proxy configuration consists of two types of endpoints; the proxy endpoint and target endpoint. The proxy endpoint defines the way client apps consume your APIs. You configure the proxy endpoint to define the URL of your API proxy and whether apps access the API or HTTP or HTTPS. You usually attach policies to the proxy endpoint to enforce security, quota checks, rate-limiting, and access control. Policies are placed in flows in the proxy endpoint. Flows are sequential stages along the API request and response processing path. You build the logic of the API proxy by placing policies in one or more flows. Some flows are conditional and execute when their condition evaluates to true at runtime. The target endpoint defines the way the API proxy interacts with your backend service. You configure the target endpoint to forward requests to the proper backend service, including any security settings, protocol, and other connection information. You can attach policies to the target endpoint to augment the requests for the backend service, add authentication data, or rewrite the target URL if needed. You can also reformat the response from the backend using policies in the target endpoint. The same types of flows can be used in the proxy and target endpoints. Apigee hybrid comes with a set of policies available for use in your API proxies. They are grouped into four categories. The traffic management policies enable you to configure caching, control quotas, and mitigate the effects of traffic spikes to your API proxy. Using mediation policies, you perform message transformation, parsing, validation of the API requests and response, and raise faults and alerts. You can also access the metadata of various entities used by our API Proxy, such as API products, developers, and apps. With security policies, you can control access to your APIs with OAuth and API key validation and implement threat protection features to detect malicious data in the request sent to the API. If handling XML payloads, you can generate SAML tokens and verify SAML assertions in your API proxy. There are policies to generate, decode and verify, sign JSON Web Signatures and JSON Web Tokens, which can be used to secure access to your API proxy. The extension group of policies let you define custom functionality by allowing you to plug-in code written in Java, JavaScript, and Python to your API proxy. You can also log messages to assist log server and orchestrate calls to other third party endpoints from within your proxy. Shared flows are common, reusable sets of policies that can be used by multiple API proxies. Using the flow colored policy in this group, you can invoke a shared flow from within your API proxy. You should be familiar with a few top-level entities when using Apigee Hybrid. At the top is the Google Cloud organization. This is the root node in the resource hierarchy that is used in Apigee Hybrid. It provides central visibility and control over all its child resources. Under the organization, a Google Cloud project is used. The project is the base entity under which all cloud resources are organized. You can create a project using the Google Cloud Console. The Apigee Hybrid runtime plane resources are deployed in a cluster that runs on Kubernetes. The cluster comprises a set of virtual machines that run the Apigee Hybrid workloads and other system components. It can be created using the Cloud Console or the GCloud command line interface and is associated to the Google Cloud project. The hybrid organization is a resource under the Google Cloud project and is a prerequisite for installing and running Apigee Hybrid. It serves as a logical boundary of data and facilitates multi-tenancy within the Apigee system. There is a one-to-one relationship between the hybrid organization and the Cloud project. You can create a hybrid organization using the Apigee API. Environment provides a runtime execution contexts for API proxies. There can be more than one environment under an Apigee Hybrid organization. You can create an environment using the Apigee Hybrid UI or Apigee API. More than one organization can be created in Apigee Hybrid to support multitenancy. A hybrid runtime can be installed in Kubernetes clusters in different regions with the organizations and environments spanning the installation. You must consider the cost and availability of cluster resources when implementing multi-tenancy in your hybrid installation. A hybrid organization is the top-level entity in Apigee Hybrid. The hybrid organization resource is used to scope API entities that are used during the execution of the API proxy. You can have more than one Apigee Hybrid organization. A separate organization is usually used to manage API entities and proxies in production installations. Data is not directly shared between organizations. You can however, export and import data between organizations using the Apigee APIs. API proxies are defined at the organizational level and are deployed to environments. Shared flows are reusable sets of commonly used policies that can be used by more than one API proxy and like API proxies, they are defined at the organization level. A shared flow must be deployed to an environment before it can be used. A user represents an authenticated account that can access a hybrid organization and the entities within that organization, such as environments, API proxies and keystones. To add a new user to your Apigee organization, you grant access to the user's account first in the Google Cloud project by assigning desired roles to that account. You can define access for the user in individual hybrid environments by using the Apigee Hybrid UI to assign roles. You package your API proxies into API products for consumption by app developers on the Apigee developer portal. Using open API specifications, you can create documentation for your APIs and publish it on the developer portal. Developers register their apps by subscribing to one or more API products on the portal. Analytics data is collected for all your APIs and API products and viewable in the Apigee Hybrid UI. Apigee Hybrid writes admin activity audit logs which record operations that modify the configuration or metadata of hybrid resources in the Google Cloud project. The log entries are viewable in the Google Cloud Console. API keys and or tokens are generated and linked to apps when developers register them to use an API product. When an app presents a key or token in the API request, it can be identified and authenticated, which allows access to the API product and its proxies. The API product specifies the environments in which it is available and can be used. You can configure an API product with OAuth scopes, when using OAuth and use quota settings to rate limit the number of requests made to the API proxies that make up the product. In Apigee Hybrid, you assign one or more environments to an environment group. Environment groups allow you to group environments together and provide the host aliases or domain names for routing API traffic to the proxies deployed to the environments within the group. You must create at least one environment group, and you must assign at least one host alias or domain name to the group. In the hybrid runtime plane configuration, a virtual host is associated to the name of an environment group. All environments added to an environment group share the same host aliases or domain names that are configured for that group. You can group environments by business function, by domain name, by region, or by any other dimension. An environment group defines one or more host aliases or domain names that map to the environments in the group. This allows Apigee Hybrid to route incoming API request to the correct runtime environment and API proxy that is deployed to that environment. An Apigee Hybrid environment can belong to multiple environment groups. Domain names that are configured must be unique to only one environment group. An API proxy that is deployed to an environment that belongs in multiple groups, can be invoked using different domain names. An environment provides a runtime execution context for API proxies. A proxy must be deployed to an environment in order to process API request. You can have multiple environments in an organization in Apigee Hybrid. API proxies with related functionality can be deployed to the same environment. Environments are also used to support the API development life cycle. A proxy revision can be promoted from a development environment through the testing environment and eventually into production, using a CICD process. A proxy revision is deployed to an environment. The Proxy can then receive and process API request. A shared flow revision must also be deployed to an environment. It is then available for use by any proxy that is deployed to that same environment. Each environment in Apigee Hybrid can have its own set of key value maps, which are collections of name value string pairs. This data can be used by API proxies in that environment at runtime. The KVM can be created by using the hybrid UI or API. It's data can be populated using the key value map operations policy. KVMs can be encrypted to store passwords or other sensitive information. Property sets are another way to store name value string pairs. They can be created and populated, while the Apigee Hybrid UI or API and stored in the management plane. They are downloaded to the runtime plane for use by the API proxies. Other environments scope configurations include flow hooks that execute shared flows and resources like JavaScript files. Caches are used to store API responses or other data for use by the API proxy at runtime. They can eliminate unnecessary requests to backends or third party services by the API proxy. Thus resulting in improved latency and throughput and better scalability. Cached data should not be shared between proxies in different environments. They are only scoped at the environment level. Targets servers are used to decouple back-end URL configuration from the API proxy code. This allows the proxy to connect to environment specific backends without modifying the proxy. Keystores and truststores store private keys and certificates to support communication over TLS between client apps and Apigee and between Apigee and back-end servers. When you make a change to an API proxy, you can deploy it as a new revision to a specific environment in your cluster. A proxy deployment is an association between an API proxy revision and an environment. API products bundle a set of API proxies with optional quotas and other metadata and offer them to app developers. App developers register their apps to use one or more API products on the Apigee developer portal. Each registered app receives an API key that associates the app to the set of API products. Apigee Hybrid enforces access control to the API proxies in the product. When the app provides the API key in the request to the API.

### Video - [Hybrid Architecture](https://www.cloudskillsboost.google/course_templates/167/video/418366)

- [YouTube: Hybrid Architecture](https://www.youtube.com/watch?v=WiXloXbUutA)

In this lecture, we will discuss the architecture of Apigee hybrid and the various components that are used to operate the API management platform. Apigee hybrid consists of a control or management plane that is hosted on Google Cloud and maintained by Google. The management plane consists of the Apigee hybrid UI, the Apigee management API, and analytics services. Apigee hybrid also consists of a runtime plane that is installed and maintained by the customer. The runtime plane is installed in a cluster running on a Kubernetes platform. All API traffic passes through the runtime plane and is processed within this plane. The runtime plane components are containerized so you can achieve auto-scaling and other operational benefits of containers. The management plane interacts with other Google APIs and services running in Google Cloud, such as Identity and Access Management. The runtime plane also interacts with Cloud Logging and Cloud Monitoring services. You can view runtime component logs and monitor the health of your runtime cluster using these Cloud services. The Apigee hybrid user interface is a service that runs in the management plane. It enables API developers to create and deploy API proxies, configure policies, create API products, and create developer apps. Administrators use the Apigee hybrid UI to monitor proxy deployment status and create and manage environments and other configuration objects. The management server provides a set of Apigee APIs. These APIs provide a programmatic interface to manage your hybrid organization, environments, and other resources, such as proxy deployments, apps, and API products. You can use these APIs in CI/CD workflows to automate hybrid management tasks. The unified analytics platform receives and processes analytics and deployment status data from the runtime plane. It makes this data available to the Apigee hybrid UI and API for developers and administrators to consume. The integrated developer portal is a simple self-service portal development tool available in the Apigee hybrid UI. It enables API developers and administrators to quickly create a developer portal and publish API products and their API documentation. Application developers can browse the API catalog, read the documentation, and self-register their apps to consume the API products on the developer portal. API developers and administrators can perform various tasks using the Apigee hybrid UI, based on their role and access permissions: You can manage environments and environment groups using the hybrid UI. To create, update, and delete environment resources, you must also update the hybrid runtime plane configuration separately. You can refine user access to each environment by assigning user roles that are different from what is granted at the project level You can deploy or undeploy proxy revisions to and from the hybrid environment and view deployment status of the proxy. You can create trace sessions to debug your API proxy and download trace session data for viewing You can create and manage other objects, such as target servers, key-value maps, and shared flows, for use by your API proxies. The runtime plane runs in a Kubernetes cluster that you maintain and deploy on a supported Kubernetes platform. The primary services that execute on the runtime plane are: Message Processors: Message Processors process API traffic from client applications and integrate with backend services. Runtime Ingress gateway is used to route API requests that come from outside the cluster to the runtime message processor pods. It uses a combination of the domain name and base path to map the request to the correct runtime pod. The synchronizer fetches configuration data about an API environment from the management plane and propagates it across the runtime plane. Cassandra is the hybrid runtime plane datastore that provides data persistence in the runtime plane. The management API for runtime data, or MART, is a service used by the management plane to access and update entities in the runtime plane datastore. The Apigee Connect component allows the hybrid management plane to connect securely to the MART service in the runtime plane, without requiring you to expose the MART endpoint outside the cluster. The Universal Data Collection Agent is a service running within the data collection pod in the runtime plane. It receives analytics and trace data from the Message Processors and sends it to the Unified Analytics Platform in the management plane. Log information is collected from the runtime components and sent to Cloud Logging. All runtime components generate metrics that are collected and sent to Cloud Monitoring. The Apigee Watcher component retrieves virtual host changes for an organization and propagates these changes to the Runtime Ingress gateway running in the cluster. Hybrid message processors provide API request processing and policy execution in the runtime plane. Message processors load proxies, shared flows and other API resources from the synchronizer component. Other entities, such as API keys and tokens, are accessed from the runtime data store. A message processor deployment is scoped to a single Apigee environment. A Runtime Ingress controller routes API requests from outside the cluster to the message processors. The Synchronizer runtime component fetches configuration data about an API environment from the management plane and propagates it across the runtime plane. It periodically polls the management server for changes and downloads a new configuration whenever changes are detected. The configuration data is retrieved and stored locally on the file system and in Cassandra, where the Message Processors accesses it. The downloaded configuration data allows the runtime plane to function independently from the management plane. If the connection between the management and runtime plane fails, services on the runtime plane continue to function. The configuration data downloaded by the Synchronizer includes Proxy bundles, shared flows, API resources, target server, and other configuration needed by the API proxies in the runtime plane. Apache Cassandra is the runtime datastore that provides data persistence for the runtime plane. It is deployed as a StatefulSet on a node pool in your Kubernetes cluster. The Cassandra database stores information about the following entities: Key management system (KMS) data, such as developers, apps, API keys, and API products. Key Value Map data, Response cache, OAuth tokens, and Quotas. Locating these entities close to the runtime processing services helps support security and scalability requirements. Apigee hybrid uses dynamically created persistent volumes to store data in the Cassandra database. Cassandra pods bind to the existing persistent volumes on restart. Data that is used by your API proxies at runtime is stored in the Cassandra data store. This includes data on apps, app developers, API products, and API keys and data stored in key-value maps, property sets, etc. You use the Apigee hybrid UI or API to create and update this data. These management operations use API calls to the MART service that process this data in the runtime datastore. Let’s review the sequence of a MART operation. An API developer or administrator calls an Apigee management API or uses the hybrid UI to process runtime data. Using Google's IAM services, the management server authenticates and authorizes the caller. If the caller is authenticated, the management server uses a pre-configured trusted service account to generate an OAuth token. It then makes a request to the MART server via Apigee Connect and includes the OAuth token in the request. MART receives the request, authenticates and authorizes it, and then validates the request. If the request is valid, MART processes the request by reading or updating the Cassandra runtime datastore. MART then logs its activity on the host file system. Apigee Connect allows the hybrid management plane to securely connect to the MART service in the runtime plane without requiring you to expose the MART endpoint outside the cluster. Apigee Connect is a gRPC service that registers with the management plane on startup. When a hybrid UI or API call that processes runtime API data is made, a management plane service selects an active connection and makes a call to Apigee Connect. Apigee Connect uses persistent connections. The connection remains open, and the client—in this case Apigee Connect—is responsible for re-establishing it if it fails. To use Apigee Connect, you must enable the Apigee Connect API in the Google Cloud API library and configure a service account with the Apigee Connect Agent role as part of your cluster setup. The Universal Data Collection Agent is a service that runs within the data collection pod in the runtime plane. All message processor services in Apigee hybrid stream trace and analytics data to a data collection pod in the cluster. The data collection pod stores the streamed data on the pod's file system via a fluentd service. The Universal Data Collection Agent periodically extracts the stored data and sends it to the Unified Analytics Platform service in the management plane. The UAP processes the incoming analytics and deployment status data and makes it available to you via the hybrid UI or the Apigee API. UDCA is scoped to an environment or organization. Let's review the sequence of a UDCA operation. The Message Processors stream trace and analytics data to the data collection pod in the hybrid runtime plane. This pod contains the fluentd and UDCA services. The fluentd service buffers the data and writes it to the pod file system. UDCA continuously polls the pod file system for new data files. The polling interval can be configured for your hybrid installation. When a new data file is available, UDCA quickly uploads it to the UAP service in the management plane. You then access the data via the Apigee hybrid UI or API. The Watcher component fetches virtualhost and basepath configuration from the management plane and propagates that configuration to the ingress gateway using ApigeeRoute resource objects. Watcher also sends API proxy deployment status data from the runtime message processors to the management plane. Watcher authenticates to the management plane using a separate service account that has the Apigee Runtime Agent role. Like the other hybrid runtime components, it generates logs and metrics that can be used for debugging and monitoring purposes using Cloud Logging and Cloud Monitoring. Apigee hybrid uses various Google Cloud services for its operation. It uses Cloud Identity for user authentication and service account authorization and uses IAM roles and permissions for access management. Apigee hybrid uses default Apigee roles created in IAM. All hybrid resources are organized under the cloud project for management and billing. Cloud Logging and Cloud Monitoring are used by hybrid components for logging, metrics collection, monitoring, and troubleshooting. This diagram shows a typical hybrid runtime installation. In production systems there are 2 nodepools: one for data that is used by cassandra, and the second one for the other runtime components. In this deployment pattern there is one Kubernetes cluster. All components are hosted within the cluster, including the runtime message processor components that service 2 hybrid environments. Ingress is via the Runtime Ingress gateway, which exposes the runtime service outside the cluster and routes traffic to the environments. Developers and Admins access management functions through Apigee Services (UI and Apigee API) and via the Google Cloud Console. This hybrid installation shows a more complex deployment pattern. Connections to the management plane are only shown from one environment for clarity, but each environment would have the same kind of outbound and inbound connections. The setup is identical in terms of traffic that flows from the runtime plane to the management plane, and vice versa. With this pattern, we introduce a global traffic manager and load balancer to distribute traffic between the two runtime plane clusters. Additionally, Cassandra replication enables all runtime plane data to be synced across each cluster. The Apigee hybrid runtime plane can be deployed in multiple regions. It can be deployed in an active-active topology for low latency API responses from your multi-regional services. It can also be deployed in an active-passive topology for failover or disaster recovery scenarios. To set up Apigee hybrid in multiple regions, you set up Kubernetes clusters in each region using separate CIDR blocks. You must also open Cassandra ports for cross-region communication because Cassandra propagates hybrid data to all clusters. Details on how to expand an existing Cassandra cluster to a new region and configure a multi-region seed host are provided at the link included on the slide. You can install the Apigee hybrid runtime plane using a multi-cloud deployment model. Here is a high-level list of tasks needed to accomplish this: Plan your network layout in each cloud region and ensure that your subnets do not use overlapping IP address ranges. Set up VPN gateways and a VPN tunnel to connect resources within each network and create required firewall rules. Enable the Google Cloud and Apigee APIs that are needed to install Apigee hybrid. Create Kubernetes clusters in each cloud infrastructure, and attach non-GKE clusters to Anthos for cluster management. Verify cluster connectivity across the cloud providers. Install certificate manager in each cluster. These are prerequisites for running Apigee hybrid. Install the Apigee hybrid runtime distribution in each cluster, and configure Cassandra ring expansion.

### Video - [Hybrid Networking](https://www.cloudskillsboost.google/course_templates/167/video/418367)

- [YouTube: Hybrid Networking](https://www.youtube.com/watch?v=EVoRIj1ALI8)

In this lecture, we will discuss Apigee hybrid networking within the runtime plane and between the runtime and management plans. It is important to understand the networking and port requirements needed to operate Apigee hybrid. Individual runtime plane services use slightly different security protocols and different ports to communicate with each other. Port numbers used by the runtime services are documented on the Apigee hybrid website at docs.apigee.com/hybrid/ports. The MART and message processor runtime services connect to Cassandra using mutual TLS. The Runtime Ingress gateway connects to the message processors on port 8443 using 1-way TLS. External connections to the hybrid runtime plane can be grouped into inbound or outbound connections. Inbound connections include API calls from the management plane to MART via Apigee Connect, and client app request to the Runtime Ingress gateway. Management API calls OAuth over TLS. You can configure the ingress gateway to use TLS based on your API requirements. Outbound connections include egress traffic to external backend services and connections to the management plane and to Google Cloud Services. You can configure egress traffic from the message processors to your backend services to use 1-way or 2-way TLS. Runtime components use port 443 to communicate with the management plane and services in Google Cloud over TLS. The Runtime Ingress gateway manages inbound requests from client apps. It is used to route traffic to the hybrid environments that are configured in the cluster. Virtual hosts allow Apigee hybrid to handle API requests to multiple domain names and route API proxy request to the specific hybrid environments The virtualhost runtime plane configuration enables you to map an environment group using the virtualhost.name property. This configuration enables the ingress gateway to route an API proxy request to the environment group and runtime service that handles the requests. Here is a sample runtime plane configuration file that defines two virtual hosts named devgrp and testgrp. Each virtualhost maps to an environment group with the same name. One or more hostAliases (domain names) must be configured when creating the environment group. The hostAliases must be unique across multiple environment groups. The virtualhost property requires a TLS and certificate. The key and certificate are used to provide secure communication with the ingress gateway and must be compatible with the host aliases defined in the specified environment group. You can use selector labels to map a virtual host to an Apigee ingress gateway deployment. You must specify both of the selectors: selector.app and selector.ingress_name to properly apply an ingress gateway to a virtual host, and the ingress gateways must be properly labeled. VPC Service Controls helps mitigate the risk of data exfiltration from Google Cloud services. With VPC Service Controls, you create perimeters that protect the resources and data of services that you explicitly specify. Apigee hybrid integrates with VPC Service Controls to isolate resources in your Google Cloud projects. Resources within a perimeter are accessed only from clients within authorized VPC networks using Private Google Access from either Google Cloud or on-premises. Clients that have private access to resources within a perimeter do not have access to unauthorized (potentially public) resources outside the perimeter. VPC Service Controls provides an additional layer of security defense for Google Cloud services that is independent of Identity and Access Management (IAM). To set up VPC Service Controls with Apigee hybrid, you must first set up private connectivity for your hybrid hosts. This involves configuring routes, firewall rules, and DNS entries to let the Google APIs access those private IPs. Next, you must secure Apigee services and additional services within the service perimeter. The additional services include: Cloud Monitoring, Google Kubernetes Engine (if you are running on GKE), and Container Registry (if you're using this as your local repository). You then copy the Apigee hybrid images to the private repository by downloading a specific version of the signed Apigee images, tagging the images, and then uploading them to your image repository. A tool is available in the Apigee hybrid distribution that helps you implement this task. Next, you update your overrides configuration file to refer to the image URLs to the private repository for the hybrid runtime components. Finally, you apply your overrides configuration file changes to the hybrid runtime cluster. Apigee hybrid uses multiple Kubernetes pods in its runtime plane that communicate with each other on specific IP addresses and ports within the cluster. However, not all pods need to communicate with every other pod. You can use Kubernetes network policies to control the traffic between pods at the IP address or port level. The entities that a Pod can communicate with are identified through a combination of other pods that are allowed, namespaces that are allowed, and IP blocks. Then defining a pod or namespace based NetworkPolicy, you use a selector to specify what traffic is allowed to and from the pods that match the selector. With IP-based network policies, you define policies based on IP blocks (CIDR ranges). To use network policies, you must use a network plugin or controller that supports the NetworkPolicy resource. In the Apigee hybrid runtime plane, not every pod communicates with every other pod in the cluster. It is a good practice to create network policies for your hybrid component pods to control pod traffic flow, based on their scope of operation. You create network policies to restrict the component pods to only communicate with the other hybrid runtime pods based on their function. For example, to secure access to the apigee-cassandra pod, create network policies to allow only ingress traffic from specific hybrid components. These components include apigee-mart, apigee-runtime, the other apigee-cassandra pods, and the apigee-metrics components. Similarly, you create additional network policies to secure access to other hybrid runtime components. Here is a sample NetworkPolicy configuration to secure access to apigee-cassandra from apigee-mart. Note that the podSelector property identifies the apigee-cassandra component to apply the policy to. The direction of traffic (policyType) is configured as Ingress. The source pod from which traffic is allowed uses the podSelector label of apigee-mart. The protocol and ports are as specified for the apigee-cassandra component. To apply the configuration to your cluster, install a CNI plugin like Calico and apply the network policy using the kubectl command.

### Video - [Apigee API](https://www.cloudskillsboost.google/course_templates/167/video/418368)

- [YouTube: Apigee API](https://www.youtube.com/watch?v=7Vb9wgEZepU)

In this lecture, we briefly discuss the Apigee API. The Apigee API is a Management API that you use to programmatically manage Apigee hybrid entities. The Apigee API is hosted by Apigee in the management plane on Google Cloud. It must be enabled for your project. To use the Apigee API, you must have a valid OAuth 2.0 access token. The token can be generated by using your Google Cloud account or service account credentials. A service account with the appropriate role can be created using IAM in the Cloud Console and can be used to generate the token. The token expires in 60 minutes, after which a new token must be generated. The Apigee API can be used to programmatically manage various Apigee entities such as API proxies, API products, developers, and apps. Using the Apigee API for CI/CD workflows is a typical use case. As part of these workflows, you can perform CRUD operations: create, read, update, or delete on API proxies and other configuration objects by using scripts that invoke the Apigee APIs. Apigee APIs are hosted in Google Cloud at the apigee.googleapis.com endpoint. The APIs expose operations on various resources as rest endpoints. Organizations, environments, APIs, and API products are some of the resources that can be managed using these APIs. The complete list of Apigee APIs is available on the Google Cloud Apigee API documentation site. Here is an example of invoking the /v1/organizations Apigee API. The Apigee API is enabled using the Google Cloud Console and OAuth 2.0 access token is generated using service account credentials. The service account in your Google Cloud project with the Apigee organization admin role is used. Using curl, the /v1/organizations API is invoked by passing in the token value as a bearer token in the request header and passing in the name of the hybrid organization in the URL path. This API returns information about the organization in the response.

### Quiz - [Architecture](https://www.cloudskillsboost.google/course_templates/167/quizzes/418369)

#### Quiz 1.

> [!important]
> **Which three services are part of the Apigee hybrid management plane?**
>
> - [ ] Apigee API
> - [ ] Identity and Access Management (IAM)
> - [ ] Unified Analytics Platform (UAP)
> - [ ] Hybrid UI

#### Quiz 2.

> [!important]
> **In Apigee hybrid, which mechanism is used to map a domain name to an environment?**
>
> - [ ] Create an environment group with one or more domain names, and assign the environment to the group. Configure a virtual host in the runtime plane to use the environment group.
> - [ ] Define an environment group property in the overrides file configuration of the runtime plane, and map the environment to this property.
> - [ ] Define a virtualhost property in the overrides file configuration of the runtime plane, and map the environment to this property.
> - [ ] Define an environment property in the overrides file configuration of the runtime plane, and map one or more domain names to this property.

#### Quiz 3.

> [!important]
> **Which hybrid runtime component provides an API interface to the management plane?**
>
> - [ ] MART
> - [ ] Message Processor
> - [ ] UDCA
> - [ ] Synchronizer

#### Quiz 4.

> [!important]
> **Which mechanism is used by the Apigee API to authenticate and authorize management API requests in hybrid?**
>
> - [ ] Service accounts
> - [ ] Username/password
> - [ ] OAuth
> - [ ] Client certificate

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/167/video/418370)

- [YouTube: Module Review](https://www.youtube.com/watch?v=ns1PyUxY1nY)

In this module, you learned about the terminology and core concepts such as the Apigee organization and environment used in Apigee hybrid. We discussed the Apigee Hybrid architecture, it's components and their operation. You learned about the hybrid management plane, the hybrid UI, and the Apigee API, and how they are used to manage your hybrid platform. You also learned about networking in Apigee Hybrid and how virtual hosts are configured for environments. Next, you will learn about the installation process and complete a lab to install Apigee Hybrid on Google Kubernetes Engine.

## Installation and Platform Operation

This module discusses the Apigee hybrid installation process, and the tools used to manage and configure the hybrid runtime plane.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/167/video/418371)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=nZZbncj2ig0)

In this module, you will learn the hybrid installation process. And complete the lab to install Apogee hybrid on Google Kubernetes engine. You will also learn about the tools to manage the hybrid runtime components and configure your cluster resources. We will also discuss the process to back up and restore your hybrid runtime data.

### Video - [Installation Process](https://www.cloudskillsboost.google/course_templates/167/video/418372)

- [YouTube: Installation Process](https://www.youtube.com/watch?v=-AwGlx53Pbw)

Let’s start with the installation process. This process guides you through the high-level steps needed to install the Apigee hybrid runtime plane. In this lecture, we will review these steps and the contents of the hybrid software distribution package. We will also discuss the strategy used to configure the hybrid runtime plane. The process to install Apigee hybrid is as follows: Set up your Google Cloud account, project, and organization. Enable relevant APIs and create an Apigee organization, environment group, and environment. Download apigeectl, the hybrid software package for the version you plan to install. Determine the size of your Kubernetes clusters based on your API proxy workload and performance requirements, and then create the clusters. Install the cert-manager required by Apigee hybrid. The hybrid software includes configuration for most resources deployed in a cluster. You use an overrides file to override some of the configuration and provide any required properties. The configuration specified in this file is then applied to your hybrid clusters. Expose the Apigee ingress gateway. When all the hybrid services are running, deploy a simple API proxy to verify the installation. You download the current or a specific version of the hybrid software distribution from a public storage location on Google Cloud. The location URL contains the version number of the distribution and the platform on which you plan to install hybrid. You can get the list of version numbers from the Apigee hybrid release website at docs.apigee.com/release. The hybrid distribution package contains a top-level directory with a name that begins with apigeectl, followed by the version and platform. Let’s review the contents of this directory... apigeectl is a command-line executable that is used to install and manage Apigee hybrid in a Kubernetes cluster. apigeectl is a wrapper for kubectl, which is a tool that lets you manage Kubernetes clusters. You must have kubectl installed and set up on your Kubernetes administration machine to use the apigeectl command. We will discuss the usage of apigeectl later in this module. The config subdirectory contains the deployment specification of the Apigee hybrid runtime resources, and their configuration in YAML file format. For example, this file contains the replicaCountMin and replicaCountMax properties of the runtime message processor component installed in your cluster. You usually override some of these configuration properties in separate files when installing and configuring Apigee hybrid. The examples subdirectory contains sample override.yaml files for small-, medium-, and large-sized deployments of Apigee hybrid. These files can be used as initial configurations for your type of deployment and should be updated as the requirements of your hybrid deployment change over time. The directory also contains a sample private overrides.yaml file that is used for hybrid runtime images from a different repository. The plugins subdirectory contains deployment specifications for Apigee hybrid system components that are installed in the cluster. This includes the apigee-operators, apigee-resources, and apigee-envoyfilter configuration files. These system components manage the creation and release of hybrid components in the cluster, provide certificate management for hybrid components, and customize the Runtime Ingress gateway configuration. The templates subdirectory contains template deployment specifications for the hybrid runtime components. These templates are used to generate the values of the configuration properties for each component during installation of the hybrid runtime plane. The hybrid runtime component configuration is organized into template files based on their scope of operation: organization, environment, and cluster. The tools subdirectory contains the create-service-account tool that is used to create various Google Cloud service accounts. These service accounts are used by the runtime components to securely access the Apigee hybrid management plane and Google Cloud services. Other tools included in this directory enable you to pull and push runtime images to a user defined repository, clean up cassandra pods, and dump pod logs from your Kubernetes cluster. After you download the hybrid software distribution, follow these steps to install the runtime plane: First, based on your sizing requirements, create the cluster on your Kubernetes platform. Apigee hybrid uses cert-manager to manage certificates used by the various components in the runtime plane. Cert-manager is a native Kubernetes certificate management controller that helps with issuing and renewing of certificates. You install cert-manager from a GitHub repository using the kubectl apply command. Next, unpack the hybrid software distribution in a suitable directory on your administration machine. In the example shown, the distribution is unpacked in the apigee directory under the user’s home directory. Set up shell environment variables for your Google Cloud project ID, region, zone, hybrid organization, etc. These variables are used for cluster authentication and to run other hybrid installation commands. Next, set up your administration machine with a recommended directory structure on the file system. Create a subdirectory named hybrid-files under the HOME/apigee directory. You can give the directory any name you want. This is used to store files for TLS certificates, service accounts, override configuration, and hybrid tools, which are all used in the hybrid runtime installation process. Create symbolic links to the tools, config, templates, and plugins subdirectories from within the hybrid-files directory. These links enable you to run the apigeectl command line tool from there. A TLS certificate and key are needed for the runtime ingress gateway that handles API proxy traffic. For trial installations of Apigee hybrid, you can use a self-signed certificate. Components in the hybrid runtime plane use Google Cloud service accounts to communicate with the management plane and with other Google Cloud services. You can create these service accounts by using the create-service-account tool that is included in the Apigee hybrid distribution package. The Apigee hybrid installer: apigeectl uses defaults for most cluster configuration settings for the hybrid runtime plane components. However, a few settings do not have defaults, and you can override others. You provide values for these settings in an overrides.yaml configuration file. You can use one of the sample overrides.yaml files provided in the hybrid distribution as a starting point for your cluster configuration. The overrides.yaml file is usually created in a subdirectory named “overrides” in the hybrid-files directory. After the overrides.yaml file is created, you enable the required permissions for the synchronizer runtime component to authenticate against the hybrid management plane. Run the apigeectl command from within the hybrid-files directory to install Apigee hybrid. You first initialize the hybrid system namespaces by running the apigeectl init command. This installs the hybrid system components in the cluster. After all the pods are running, you run the apigeectl apply command. The apply command installs the Apigee hybrid runtime components in your cluster. You can use the apigeectl check-ready command to test whether all the pods in the cluster are ready. Finally, set up the Kubernetes service for your Apigee ingress gateway. The Kubernetes service is needed for each ingress deployment to obtain an IP address that can be exposed. Client calls to proxies will invoke a hostname that will resolve to this IP address. There are two options for providing a Kubernetes service to assign the IP address. The recommended option for production environments is to create a custom Kubernetes service for each Apigee ingress gateway. For non-production environments or to test initial traffic through the Apigee ingress gateway, Apigee hybrid provides a default Kubernetes service for each ingress deployment. When you install Apigee hybrid, you provide configuration properties that are used to configure hybrid components in your cluster. The properties override the default values that come with the hybrid software distribution. Using this strategy, you can have multiple override files that configure your cluster with different resource configurations, based on certain criteria. For example, you may have different config settings for a production cluster versus a non-production cluster. You can also have separate override files where each file configures all the components needed for a single runtime environment in the cluster. It is a best practice to maintain your override.yaml files in source control. This allows versioning of your configuration and makes it easier to roll back to a previous version if needed. The Apigee hybrid installation process creates multiple namespaces in your Kubernetes cluster. These namespaces logically group the runtime components by the type of functionality they provide. All the hybrid workload components are deployed in the Apigee namespace in your cluster. After Apigee hybrid is installed, you can verify that it is operating as expected. Create a test environment in the hybrid UI and configure this environment in the hybrid runtime plane. Then create or upload a simple API proxy using the hybrid UI and deploy it to the test environment. This test API proxy can use a mock backend as the target of the API. Send multiple requests to the API. You can use curl, Postman, or any REST client to generate HTTP requests. Confirm that the API proxy is returning the expected response from the backend. You can add API key verification or quota policies to the test API proxy to verify that the runtime datastore is functioning as expected. This will confirm that the MART, Message Processor, and Cassandra runtime components are functioning as expected. In addition, you can start Trace sessions to debug your proxy and view Analytics in the hybrid UI. Using the Google Cloud Console, you can view log messages to troubleshoot the hybrid runtime components and view metrics to track resource usage of the cluster components.

### Video - [Lab Introduction: Installing and Managing Apigee Hybrid](https://www.cloudskillsboost.google/course_templates/167/video/418373)

- [YouTube: Lab Introduction: Installing and Managing Apigee Hybrid](https://www.youtube.com/watch?v=qCk3kUTyC5s)

You will now complete a lab: Installing and Managing Apigee Hybrid. In this lab you install and configure the Apigee hybrid runtime plane in a Kubernetes cluster on GKE. You then create Apigee hybrid environments and an environment group using the Apigee hybrid UI. Next, you deploy simple API proxies to the Apigee hybrid environments and validate the environments by testing the API proxies. Finally, you view API proxy analytics in the Apigee hybrid UI.

### Lab - [Installing and Managing Apigee Hybrid](https://www.cloudskillsboost.google/course_templates/167/labs/418374)

The hybrid deployment model enables you to deploy and manage the Apigee runtime in Kubernetes containers, while Apigee hosts the management plane in Google Cloud.

- [ ] [Installing and Managing Apigee Hybrid](../labs/Installing-and-Managing-Apigee-Hybrid.md)

### Video - [Lab Review: Installing and Managing Apigee Hybrid](https://www.cloudskillsboost.google/course_templates/167/video/418375)

- [YouTube: Lab Review: Installing and Managing Apigee Hybrid](https://www.youtube.com/watch?v=8zkQLm2vu00)

In this lab you installed and configured the Apigee hybrid runtime plane in a Kubernetes cluster on GKE. You then created Apigee hybrid environments and an environment group using the Apigee hybrid UI. Next, you deployed simple API proxies to the Apigee hybrid environments and validated the environments by testing the API proxies. Finally, you viewed API proxy analytics in the Apigee hybrid UI.

### Document - [Reading: Installation notes](https://www.cloudskillsboost.google/course_templates/167/documents/418376)

### Video - [Platform Operation](https://www.cloudskillsboost.google/course_templates/167/video/418377)

- [YouTube: Platform Operation](https://www.youtube.com/watch?v=c-cJ515uyW4)

In this lecture, we discuss the tools used to operate the Apigee hybrid runtime platform. The apigeectl executable is included in the distribution package and installed in your apigeectl directory. You use this tool to initialize and configure your Kubernetes cluster with the hybrid runtime system components and workloads. When running the tool, provide the command name and any optional flags on the command line. The command specifies the operation you want to perform, for example; init, apply, or check-ready. Flags specify command parameters. To initialize the cluster with the hybrid system components, use the init command. You use the apply command to apply your configuration and install the Apigee hybrid workloads into the cluster. The check-ready command can be used to check whether all of the components in the cluster are running. The complete list of commands is documented on the CLI reference page on the Apigee documentation website at cloud.google.com. You can control the scope of the command used with apigeectl by using optional flags. The flags shown here determine the set of runtime components that the apigeectl command applies the configuration to. The datastore flag applies the configuration changes to the Cassandra component in the cluster. The env flag applies the configuration changes to the Runtime, Synchronizer, and UCDA cluster components for the specified environment. The all-envs flag applies the configuration changes to these components for all environments under the organization specified in your overrides config file. The org flag applies the configuration changes to the Apigee Connect, MART, and Watcher components for the specified organization. The telemetry flag applies the configuration changes to the logger and metrics components in the cluster. The redis flag applies the configuration to the in-memory data storage scope (Redis). The all flag is used with the delete command and deletes the entire Apigee hybrid installation from your cluster. The dry-run flag allows you to check for any errors in the configuration without applying them to your cluster. When used with the print-yaml flag, the complete cluster configuration for your hybrid runtime plane can be output to a file. You can then save the file in source control. The f flag allows you to provide a specific override yaml configuration file for your cluster components. If the file is not specified, the command looks for a file named overrides.yaml in the current directory. The settings flag is used to apply changes to the virtualhosts property to the cluster. The complete list of flags is documented on the CLI reference page on the Apigee Documentation website at cloud.google.com. Here is an example of how to use the apigeectl command. In this example, you would like to change the minimum and maximum number of replicas for the runtime message processor component in your cluster. You first change the overrides.yaml configuration file on your file system. Then, run the apigeectl apply command, passing in the environment name, and the name and location of the overrides config file. You can then use the check-ready command to determine whether the component pods are up and running with the new configuration. apigeectl uses the Kubernetes command line tool: kubectl. Kubectl enables you to manage and control Kubernetes clusters. You can use kubectl to list and describe cluster resources like pods and check on the current status. You can also start command shells to access the containers running in those pods for debugging purposes. The Kubernetes CustomResourceDefinition API resource allows you to define custom resources in your cluster. Using a CRD object, you can create a custom resource with a name and schema that you specify. Apigee hybrid uses a CustomResourceDefinition called ApigeeDeployment. The ApigeeDeployment objects can be managed by using kubectl commands. Along with the Apigee Operators custom controller, they validate deployment configurations and deploy hybrid runtime resources in the cluster. Apigee hybrid uses a layered approach to configure your cluster. The default configuration is stored in a file called values.yaml in the config subdirectory under the apigeectl directory in the hybrid distribution. To install Apigee hybrid, you create an overrides configuration file that includes mandatory properties and other properties that override the default values in the values.yaml file. Not all default properties can be overridden. For those that can and should be modified, the best practice is to provide the new values in the overrides.yaml config file instead of directly modifying the values.yaml file. This approach preserves your modified values during product upgrades. It is also a best practice to maintain multiple override files in source control. You can have one overrides file for your production clusters and a separate one for a non-production cluster. You can also have separate override files for each hybrid environment in the cluster. A good practice is to maintain a set of override files where each file contains only the configuration required for each group of components. For example, you can have a common.yaml overrides file for components like cassandra, logging, and metrics. Another file can contain the configuration properties for the hybrid organization and resources like mart and apigee connect. Environment-level properties for the runtime synchronizor, and ucda components can be in their own overrides file, one file per hybrid environment. Virtual hosts configuration can also be in its own overrides configuration file. Using this strategy, you can control changes to a subset of components in the cluster by updating and applying only the specific overrides file. The apigeectl tool contains useful command line flags called dry-run and print-yaml. You can use the dry-run flag with the apply command to check for errors in your configuration without making any changes to the cluster. When the dry-run flag is combined with the print-yaml flag, you can generate your entire cluster configuration. You can then save this configuration to a file and maintain it in source control for reference. Automated issue surfacing (AIS) provides quick information about ongoing issues detected within your Hybrid cluster. Starting with Apigee hybrid v1.10, the Apigee runtime watcher component automatically scans the control plane and Kubernetes API server state to determine if there are any configuration issues. By default, the scanning happens every 60 seconds. You can change the interval or disable scanning if you prefer. When AIS detects an issue, it creates a new instance of ApigeeIssue within the Kubernetes API server. These instances contain information about the issues and links to documentation on the specific issues. When you resolve issues, they will automatically be deleted from the Kubernetes API server when the scanning determines they are no longer occurring. Automated issue surfacing only looks for known, common, system detectable issues and cannot detect all the issues within a cluster. You can check for any existing issues with the kubectl get apigeeissues command. Annotations are key value maps used to attach metadata to Apigee hybrid Kubernetes pods. You can create custom annotations for the following properties: cassandra, connectAgent, logger, mart, metrics, runtime, synchronizer, udca, and watcher. To add a custom annotation, add a stanza to the overrides.yaml file for the respective component.

### Video - [Backup and Restore](https://www.cloudskillsboost.google/course_templates/167/video/418378)

- [YouTube: Backup and Restore](https://www.youtube.com/watch?v=Sd7FM8Yrgq0)

In this lecture, we discuss procedures to perform backups of your hybrid runtime plane datastore. We will also discuss how to restore your cluster from a backup. Apigee hybrid uses a Cassandra database to store your hybrid runtime data. Cassandra is a replicated database that is configured to have at least 3 copies of your data in each cluster. Cassandra uses streaming replication and read repairs to maintain the data replicas at any given point. The Cassandra runtime data store is deployed to your Kubernetes cluster as a StatefulSet. The Cassandra pod reads and writes data using a PersistentVolume. If the pod is deleted and re-created, it will continue to have access to the PersistentVolume. In hybrid, Cassandra backups are not enabled by default. It is a good practice, however, to enable Cassandra backups in case your data is accidentally deleted. You can use one of two methods to configure backups. Apigee hybrid CSI backup and restore uses Kubernetes CSI (Container Storage Interface) snapshots. This method is recommended for hybrid instances hosted in Google Cloud, AWS, or Azure. Non-CSI hybrid backup and restore copies schema and other data to backup files. This is the recommended method for on-prem installations. For both methods, you'll configure backup in your overrides.yaml file. Apigee recommends you make a copy of the overrides.yaml file, so that you can reuse it during the recovery process. Let’s take a look at using the Apigee hybrid CSI backup and restore method. Starting with Apigee hybrid 1.9, you can backup and restore your hybrid data using CSI (Container Storage Interface) snapshots. CSI backup generates disk snapshots and stores them as encrypted data in cloud storage. CSI backup does not need a Google Cloud Storage bucket or a remote server to store backup data. CSI backup is recommended for hybrid instances hosted in Google Cloud, AWS, or Azure. For more information, see the documentation at Cassandra CSI backup and restore. Now let’s take a look at how to schedule backups in Cloud Storage and in a remote server, using non-csi backup and restore methods. Cassandra non-CSI backups are best suited to schedule backups in Cloud Storage and in a remote server. The entities that are backed up in this process include the Cassandra schema, partition token information per node, and a daily snapshot of the data. The backup is stored in a Cloud Storage bucket that you must create before enabling the Cassandra backup. You can also back up your datastore to a server file system. This topic is covered in later slides in this section. To schedule backups, you must perform a set of steps. First, create a service account with the storage.objectAdmin role. This service account role allows you to back up data to Cloud Storage. To create this service account, use the create-service-account tool in the tools directory in the hybrid distribution. Name the service account apigee-cassandra, and store the service account json key file in the service-accounts subdirectory. Create a storage bucket in Cloud Storage. Specify a data retention policy for the bucket. 15 days is a reasonable period. Update your overrides.yaml file and add the backup configuration properties to the cassandra stanza. These properties enable the backup, set the path to the service account file and the Cloud Storage bucket, and configure a cron schedule to run the backup. Use the apigeectl command to apply the override configuration changes to the cluster. Pass in the datastore flag that indicates the cassandra component and the path to the overrides.yaml file that contains your backup config properties. Allow for sufficient time in the cron schedule for the backup to begin after the configuration changes are applied to the cluster. To verify the backup job, run the following command: kubectl get cronjob -n apigee. The backup job creates a schema.cql file and uploads it to the Cloud Storage bucket. It then instructs the Cassandra node to back up the data and waits for the data to be uploaded. You can monitor a backup job that has started based on your configured cron schedule. First, use the kubectl get pods command to determine the name of the pod running the backup job. Run the kubectl logs command, passing in the name of the pod running the backup job. You will see output in the pod log as the backup job executes. Note the backup snapshot timestamp. It is used to configure your override properties when your backup is restored. In a single region deployment, Apigee hybrid is deployed in a single data center or a region. If you have multiple Apigee organizations in your deployment, the restore process restores data for all the organizations. In a multi-organization setup, you cannot restore a specific organization. In your configuration, the Cassandra backup can reside either on Cloud Storage or on a remote server. Ensure that the version of Apigee hybrid is the same version that created the backup files. Confirm that the Kubernetes cluster you are restoring to does not have a prior Apigee hybrid installation. If you are restoring to the existing cluster, use the commands shown to delete the existing Apigee hybrid installation. Modify the overrides yaml file to specify the namespace element. Add the restore configuration properties to the cassandra stanza. Specify the timestamp of the backup snapshot to restore, the path to the service account file, and the Cloud Storage bucket. Create a new hybrid runtime deployment. This will create a new Cassandra cluster and begin restoring the backup data into the cluster. Verify the restoration job progress and confirm that apigeeds and all the other pods are up. Upon successful completion of the restore and confirmation that the runtime components are healthy, it is recommended to configure a backup on the cluster. To monitor the progress of the restore job, first use the kubectl get pods command to determine the name of the pod running the job. Run the kubectl logs command, passing in the name of the pod running the restore job. You will see output in the pod log as the restore job executes and completes. You can store backups of your Cassandra database to compressed files in the file system of a server you specify. Backups occur on a schedule that you specify in your overrides file. The connection to the server is by secure SSH. To configure a file system backup, you must first set up a server and install and configure SSH. As part of this process, you create a key pair. The public key is copied to the server, and the location of the private key is configured in your overrides config file. Test the SSH connection between your Cassandra pods in the runtime cluster and your backup server. Next, you configure your overrides.yaml file with the backup configuration. Edit your overrides.yaml configuration file to add the backup properties. Set the enabled property to true, and provide the path to the SSH private key file and the backup server IP address. Set the directory location where the backup files are to be stored on the backup server. Set the required cloudProvider property to HYBRID, and set the cron schedule for the backup. Apply the configuration changes to your cluster using the apigeectl command with the datastore flag. Finally, verify the backup job. The Cassandra backup can also reside on a remote server, and the process to restore is the same as mentioned in the previous slides on restoring cassandra in a single region. For non-cloud storage restores, modify the overrides.yaml file to specify the namespace element. Add the restore configuration properties to the cassandra stanza, for example, the path to the SSH private key file, the file system server IP address, the storage location on the file system that contains the backup, and the cloudProvider property with value "HYBRID". Specify the timestamp of the backup snapshot to restore. Then continue the restore process as you would for restoring Cassandra in a single region.

### Quiz - [Installation and Platform Operation](https://www.cloudskillsboost.google/course_templates/167/quizzes/418379)

#### Quiz 1.

> [!important]
> **Which of the following is a true statement regarding backup of the Cassandra datastore in the hybrid runtime plane?**
>
> - [ ]  Cassandra backup is enabled by default.
> - [ ]  The Cassandra backup process does not need authentication to perform the backup.
> - [ ]  The Cassandra backup schedule is configured in the overrides configuration file using standard crontab syntax.
> - [ ]  Apigee hybrid can only back up Cassandra to a Cloud Storage bucket.

#### Quiz 2.

> [!important]
> **Which of the following properties cannot be configured in the overrides.yaml file used in the hybrid installation process?
**
>
> - [ ]  API proxy definition
> - [ ]  Service account files
> - [ ]  Google Cloud project ID
> - [ ]  Namespace

#### Quiz 3.

> [!important]
> **Which of the following methods of using the apigeectl command is invalid?
**
>
> - [ ]  apigeectl apply -f ./overrides.yaml
> - [ ]  apigeectl init -f ./overrides.yaml -c environment
> - [ ]  apigeectl delete -f ./overrides.yaml
> - [ ]  apigeectl check-ready -f ./overrides.yaml

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/167/video/418380)

- [YouTube: Module Review](https://www.youtube.com/watch?v=xnQLMeahzMk)

In this module you learned how to install and configure Apogee hybrid on a Kubernetes platform. You install hybrid on Google Kubernetes engine in a lab. You also learned how to use the Apogee CDL tool to initialize and configure the hybrid runtime cluster. Finally, you learned about the backup and restore process for the runtime data store component used in hybrid.

### Video - [Course Review](https://www.cloudskillsboost.google/course_templates/167/video/418381)

- [YouTube: Course Review](https://www.youtube.com/watch?v=zgdIWXkr21E)

Thank you for taking the fundamentals and installation course on Google Cloud's Apigee Hybrid API platform. In this course you will introduce to the Apigee API management platform. You learned about the Apigee deployment models and got an overview of Apigee Hybrid. We discussed some of the basic concepts used in Google Cloud, Kubernetes, and Anthos. In the architecture module, Apigee terminology, the organizational structure, and the Apigee API was discussed. And you learned about the hybrid architecture and the network connections used between hybrid components. You learned how to install Apigee Hybrid on Kubernetes and completed the lab to install Apigee Hybrid on Google Kubernetes engine. You also learned about the tools used to operate the Apigee hybrid platform and the process to back up and restore your hybrid runtime data. Congratulations on completing this course on the fundamentals and installation of Google Cloud's Apigee Hybrid API platform. We recommend you continue with the next course in this series on the management and security of the Apigee Hybrid API platform. In that course you will learn about API proxy deployment and environment management on the Apigee hybrid platform. You will also learn how to secure the runtime plane components, plan capacity, an scale the hybrid runtime plane.

## Course Resources

PDF links to all modules

### Document - [Architecting and Installing the Apigee Hybrid API Platform Course Resources](https://www.cloudskillsboost.google/course_templates/167/documents/418382)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
