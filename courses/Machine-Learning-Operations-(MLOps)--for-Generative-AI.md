---
id: 927
name: 'Machine Learning Operations (MLOps)  for Generative AI'
datePublished: 2024-10-14
topics:
- Machine Learning
- Data Management
- Machine Learning Operations
type: Course
url: https://www.cloudskillsboost.google/course_templates/927
---

# [Machine Learning Operations (MLOps)  for Generative AI](https://www.cloudskillsboost.google/course_templates/927)

**Description:**

This course is dedicated to equipping you with the knowledge and tools needed to uncover the unique challenges faced by MLOps teams when deploying and managing Generative AI models, and exploring how Vertex AI empowers AI teams to streamline MLOps processes and achieve success in Generative AI projects.

**Objectives:**

- Recognize the evolution of MLOps Explore the MLOps framework for Generative AI Explain the unique challenges of Generative AI Explore how Vertex AI empowers AI teams

## Machine Learning Operations (MLOps)  for Generative AI

This course is dedicated to equipping you with the knowledge and tools needed to uncover the unique challenges faced by MLOps teams when deploying and managing Generative AI models, and exploring how Vertex AI empowers AI teams to streamline MLOps processes and achieve success in Generative AI projects.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/927/video/511832)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=X312OSFJPrI)

Welcome to the Machine Learning Operations (MLOps) for Generative AI course. In this course, you'll enter the complex landscape of MLOps strategies that are designed specifically for generative AI While the realm of generative AI is brimming with excitement and exploration. There are unique challenges in transitioning from captivating demos to real world production. This course will equip you with the knowledge and tools to overcome these challenges and harness the transformative power of generative AI. By completing this course, you'll be able to recognize the evolution of MLOps and its role in shaping the future of AI development. Explore the emerging framework of ML Ops specifically tailored for generative AI applications. Explain the unique challenges faced by MLOps teams when deploying and managing generative A.I. models and explore how Vertex AI empowers AI teams to streamline MLOps processes and achieve success in generative AI projects.

### Video - [The evolution of MLOps](https://www.cloudskillsboost.google/course_templates/927/video/511833)

- [YouTube: The evolution of MLOps](https://www.youtube.com/watch?v=PG4itEzqGBE)

Let's start by revisiting the essence of MLOps and delve into the reasons for its ongoing evolution. If you've worked with ML models before, you know that training and deploying them can be a time consuming and iterative process. To solve this problem, organizations need to build the necessary ML engineering, culture and capability. MLOps, with its standardized processes and technology infrastructure, provides the answer, enabling you to rapidly and reliably build, deploy and operationalize ML systems. There's no universally accepted definition for the distinction between non generative AI and generative AI, but we're not going to enter that discussion in this course. Instead, we'll focus on the core concepts about each kind of AI. before we talk about AML Ops, In order to highlight the difference between non-generative AI and generative, AI will use two terms: we'll call non generative AI “predictive A.I.,” and we'll also talk about “generative AI.” Predictive AI refers to a type of artificial intelligence that focuses on performing specific tasks or solving specific problems. It doesn't have the ability to create new content or generates original ideas. Instead, it relies on preexisting data and algorithms to make predictions. Classifications or decisions. For example, regression, classification, object detection, or forecasting tasks can be listed as predictive A.I. tasks Within the AI research community, the term generative is primarily used for sophisticated models capable of producing high quality human like outputs. Therefore, the term “generative A.I. can be used for a type of artificial intelligence that focuses on creating new and original content such as text, images or music by learning from patterns in existing data. Building upon the existing MLOps infrastructure, our objective is to seamlessly extend its capabilities to support the rapidly growing field of generative A.I. While predictive AI and generative. A.I. share some fundamental principles, the latter introduces a unique set of challenges. Our goal is to adapt and refine established MLOps processes to effectively address these challenges and fully harness the potential of generative AI. Let's revisit the two key phases of the traditional ML workflow: training and serving. The training phase commences with an experimental stage, where labeled data, ML algorithms, and trained models are employed to operationalize the training step. The serving or inference phase involves deploying the model to an endpoint and obtaining predictions through an inference pipeline utilizing new data, the served model, and prediction requests. If we focus on the training and serving stages, the traditional ML workflow can be simplified into four distinct phases: experimentation, training, deployment and prediction. However, over the past few years, we've recognized the importance of evaluating and monitoring these models consistently across all these phases. Considering the ever changing nature of data and real world applications. As a result, we've incorporated evaluation and monitoring phases into the overall workflow, encompassing both the training and serving stages. In parallel, at the industry level, we've collectively fine-tuned our practices in logging diverse artifacts such as models in model registries, features in feature stores, and even pipeline directed Acyclic graphs in a template gallery. Simultaneously, we've maintained consistent data and artifact governance throughout the entire training and serving lifecycle. With a solid understanding of the core phases of the ML workflow and the importance of evaluation and monitoring, you're now equipped to dive deeper into the specific complexities of MLOPs for generative AI. Join us in the next lesson as we delve into the specific challenges faced by MLOps teams when deploying and managing generative AI models, and discover how Vertex AI empowers you to overcome these hurdles and achieve success.

### Video - [MLOps framework for Generative AI](https://www.cloudskillsboost.google/course_templates/927/video/511834)

- [YouTube: MLOps framework for Generative AI](https://www.youtube.com/watch?v=DpOyYdPmLtc)

Now that you're familiar with the traditional or predictive ML workflow, let's focus on how MLOps can be specifically tailored for generative AI applications. MLOps for Gen AI can be defined as a set of practices, techniques and technologies that extend MLOps principles to the operationaliization of generative AI applications. While the foundation of MLOps remains essential, generative AI introduces unique considerations that require additional layers and refinements to the traditional workflow. Let's start with the experimentation phase. This phase is distinct in two ways. Pre-trained model discovery, where the emphasis shifts from building models from scratch to discovering and leveraging Pre-trained models and the direct prediction shortcut, where a direct path from the discovery and experimentation phase enables immediate prediction execution using natural language prompts. After the discovery and experimentation phase, we proceed with training and serving our models as we would in any ML workflow. However, with generative AI, we introduced two new phases customization and tuning and curated data. Customization and tuning instead of training models from scratch, generative AI focuses on customizing and fine tuning pre-trained models for specific tasks. This approach allows us to leverage the power of Pre-trained models while tailoring them to our specific needs. Curated data: Generative AI relies on curated datasets rather than massive amounts of labeled data. This necessitates a reevaluation of data lake and data warehouse strategies to ensure that we have the right data available for training and serving our models. The other necessary adaptation of MLOps for Gen AI is in the governing artifacts phase. Generative AI introduces additional artifacts such as tuning jobs, adaptive layers, and embeddings, that require governance. The next adaptation is in the evaluating and monitoring phases. Traditional metrics like accuracy and precision might not suffice for evaluating generative AI models. New metrics that consider fluency, factuality and brand reputation are often required. Check the documentation for further details of new evaluation metrics and services. Vertex AI offers. In the final phase curate data or incorporating enterprise data phase, it appears that foundation models are somewhat static, confined the knowledge they acquired during pre-training. The true potential lies in accessing data beyond their original training sets, allowing them to perform more sophisticated tasks. However, integrating additional data introduces new evaluation and monitoring challenges, increasing the overall complexity. With these insights in hand, let's move on to explore the practical tools and strategies Vertex AI offers to streamline your generative AI MLOps journey.

### Video - [Navigating Generative AI: Embracing Nuances and Adapting MLOps](https://www.cloudskillsboost.google/course_templates/927/video/511835)

- [YouTube: Navigating Generative AI: Embracing Nuances and Adapting MLOps](https://www.youtube.com/watch?v=tWUhAQ-w4Zw)

In this section, we'll explore the unique challenges faced by MLPps teams when deploying and managing generative AI models. We'll also learn how Vertex AI empowers AI teams to streamline MLOps processes and achieve success in generative AI projects. The main idea here is that your existing MLOps investments remain relevant and valuable. You can leverage the same platform for generative AI tasks like code and text generation and summarization. However, effectively integrating generative AI requires a thorough grasp of the unique challenges it presents. While generative, AI holds immense promise, its integration into the MLOps framework presents unique challenges that require careful consideration and tailored solutions. Let's examine these challenges and explore the effective mitigation strategies to seamlessly integrate generative AI within the MLOps ecosystem. The first challenge is increased AI infrastructure needs for pre-trained multitask models. Generative AI models, especially large language models, demand substantial computational resources due to their complex architecture and extensive pre-training requirements. This necessitates a robust A.I. infrastructure equipped with GPUs and TPUs to support training, experimentation and deployment. One effective mitigation strategy might be leveraging Vertex AI for efficient generative AI development. Vertex AI provides a comprehensive platform for generative AI development, offering pre-trained models, a user friendly interface for model customization and optimized infrastructure for training and experimentation. Vertex AI streamlines the exploration and experimentation phase for A.I. applications in a couple of ways. First is the Model Garden. Model Garden serves as your discovery gateway, providing access to a diverse array of models, including Google models, open source models, and third party models. It can help you understand which models best suits your use case without extensive data collection or labeling millions of examples. Imagine effortlessly integrating functionalities like classification, summarization and entity extraction into your applications with just simple text instructions instead of dedicated ML pipelines and datasets for each! Model Garden users can get started immediately with code examples to learn the best practice for training models on GPUs and TPUs. Many of the models in the Model Garden could also be trained in a distributed fashion. The second is Vertex AI Generative Studio. This simplifies model customization by enabling you to fine-tune models with your own data. It provides a user-friendly interface for testing sample prompts, designing custom prompts, and adapting foundation models to align with your specific application requirements. Vertex AI Generative Studio is a fully managed environment, which means that Google takes care of the underlying infrastructure. This frees up developers and data scientists to focus on building and deploying applications. You can also leverage Vertex AI Training and Prediction for fully managed compute infrastructure. You just upload your data and Vertex Training takes care of everything to run and scale with high availability. This allows you to scale your ML training efficiently and focus on what matters most: building and deploying powerful A.I. models. The second challenge is customizing and tuning Generative AI models. Generative AI models often require fine tuning to align with specific tasks and domains. This involves supervised tuning, reinforcement learning with human feedback and extensive data curation. Three services offered by Vertex AI can be leveraged to mitigate the challenges associated with customizing and tuning generative AI models. Supervised Tuning, which lets you leverage existing supervised learning techniques for tasks with well-defined outputs. Reinforcement Learning with Human Feedback (RLHF) can be employed for tasks where defining the expected output is challenging, such as  summarization and chat applications. RLHF is offered for both Google PALM models and open source models like llama2. And data curation, which means augmenting generic pre-training data with domain-specific data to enhance model performance. Let's move on to the next challenge. Generative AI introduces additional artifacts such as prompts, embeddings and adaptive layers that require efficient management. Managing those additional artifacts is another challenge for generative AI models. At this point, existing tools and best practices on Vertex AI are still very useful. The first is prompt management and analysis. Prompt engineering is the art of crafting the perfect instructions to guide language models towards generating the desired output. While it can be a meticulous process, there are valuable tools and frameworks available to assist in-depth prompt analysis and debugging. For instance, tools like Langchain and Weight & Biases empower users to design prompts for a diverse range of tasks, from straightforward text completion to more intricate natural language processing applications such as text summarization and code generation. These tools enable users to provide specific instructions, context, input data and outputs indicators ensuring that the language model receives the necessary guidance to produce the desired outcome. Second is embedding management. Embeddings transform unstructured data like text images and videos into dense vector representations, enabling powerful applications such as search recommendations and similarity matching. You can leverage Vertex AI Feature Store to streamline the storage, management, and serving of these embeddings. Next is adaptive layer management. Adaptive tuning means updating only a small set of weights within those models. Adapter layers are actually only tens of megabytes in file and then get passed to our foundation model alongside your prompts to generate an inference. You can employ Vertex Model Registry to manage your predictive AI models and generative AI models, including adaptive layers. Finally, is tuning job management. Tuning jobs involve optimizing your models for improved performance. Tuning generative AI models using pipelines necessitates robust reproducibility, lineage tracking and metadata management. Vertex AI Pipelines provides a comprehensive platform for orchestrating and managing generative AI tuning jobs, enabling seamless lineage tracking from dataset to model, regardless of whether you're utilizing Spark, XGBoost, TensorFlow, or PyTorch. Evaluating and monitoring generative A.I. models also poses unique challenges due to the unstructured nature of their outputs. Generative AI models necessitate evaluating and monitoring not a single output, but rather extensive chunks of generated output, such as images and chat logs. To effectively evaluate and monitor generative AI models, consider implementing strategies that address the unique challenges posed by their unstructured outputs. With evaluation, you can develop metrics that consider factors like fluency, factuality and brand reputation for mitigating evaluation challenges. Vertex AI Evaluation Services offer a range of evaluation, targets, methods, and access to numerous metrics. To evaluate the performance of a model, you first create an evaluation dataset that contains prompts and ground truth pairs. For each pair, the prompt is the prompt that you want to evaluate, and the ground truth is the ideal response for that prompt. During evaluation, the prompt of each pair in the evaluation dataset is passed to the model to produce an output. The output generated by the model and the ground truth from the evaluation dataset are used to compute the evaluation metrics. The metrics used for evaluation depend on the task you're evaluating. Check the official documentation for the supported tasks and the metrics used to evaluate each task. With monitoring, you can utilize safety scores and recitation checking tools to ensure content quality and authenticity during the monitoring phase. When inputs and outputs involve external or internal users, delegating input provision becomes essential. It's crucial to understand the nature of the inputted or generated content. Vertex Palm addresses this challenge by generating an automatic payload with safety scores across more than ten categories for each input and support request. This valuable information facilitates monitoring and enables the setting of appropriate thresholds for various use cases. Recitation checking plays a pivotal role in ensuring content authenticity. To address concerns about the use of unoriginal content built in recitation checkers scan the model's outputs against existing code repositories and web articles. Upon identifying significant matches, the system can either block substantial matches or provide a recitation within the API call output for matches exceeding a specific text character threshold. These unique aspects of model monitoring significantly differ from traditional elements like training and serving skew or tracking feature and prediction drift, offering diverse approaches to enhancing your model monitoring strategy. Integrating generative AI models with enterprise data requires efficient data access and processing mechanisms. To seamlessly integrate enterprise data into generative AI workflows, you can employ Vertex AI services. The first is embeddings and Vector Search. You can utilize embeddings to process and store enterprise data for efficient querying and analysis. Vertex AI offers three core services: A suite of Vertex Embeddings APIs: A suite of embeddings across text and image, allowing projection of both to the same vector space for semantic knowledge comparison. Fully managed vector database: Storing and retrieving these embeddings is facilitated by Vertex Vector search and Vertex Feature Store, supporting embeddings alongside existing ML features. And integration of Vertex Palm with third party tools and frameworks such as Langchain. The second is grounding capabilities. You can employ Vertex Palm’s grounding capabilities to generate responses based on enterprise data, reducing the risk of hallucinations. And the third is Vertex Extensions. You can use Vertex Extensions to author and manage extensions that connect to real-time data and real-world actions. These three capabilities extend the reach of generative AI into enterprise data. Overall, by addressing these challenges and implementing the tailored adaptations discussed, you can effectively integrate generative AI into MLOps processes, unlocking the transformative potential of this technology.

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/927/video/511836)

- [YouTube: Summary](https://www.youtube.com/watch?v=R0mgWq41tGI)

Congratulations! You've made it to the end of the MLOps for generative AI course. Through this course, we recognized the evolution of MOps and its pivotal role in shaping the future of A.I. development. We explored the emerging framework of ML Ops, specifically tailored for generative AI applications, and explained the unique challenges faced by MLOps teams when deploying and managing these models. Finally, equipped with this knowledge, we explored how Vertex AI empowers teams to streamline MLOps processes and achieve success with generative AI projects. This course is just another step in your machine learning operations journey. For more training and hands-on practice with ML and AI, please explore the options available at cloud.google.com/training/machinelearning-ai And if you're interested in validating your expertise and showcasing your ability to transform businesses with Google Cloud Technology, you might consider working toward a Google Cloud certification. You can learn more about Google Cloud certification offerings at cloud.google.com/certifications. Thanks for completing this course. We'll see you next time!

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
