---
id: 1143
name: 'Networking in Google Cloud: Load Balancing'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1143
date_published: 2024-08-21
topics:
  - Networking
---

# [Networking in Google Cloud: Load Balancing](https://www.cloudskillsboost.google/course_templates/1143)

**Description:**

This training course builds on the concepts covered in the Networking in Google Cloud: Fundamentals course. Through presentations, demonstrations, and labs, participants explore and implement Cloud Load Balancing.

**Objectives:**

* Implement hybrid load balancing.
* Manage traffic to the load balancers.
* Use Google Cloud Armor with your load balancers.

## Welcome to Networking in Google Cloud

Welcome to the Networking in Google Cloud: Routing and Addressing course.

### Video - [Networking in Google Cloud Introduction](https://www.cloudskillsboost.google/course_templates/1143/video/500639)

* [YouTube: Networking in Google Cloud Introduction](https://www.youtube.com/watch?v=Ye5AyIpHdI8)

Welcome to the fifth course of the networking in Google Cloud series, Load Balancing. In this course, we will cover the topics listed on the screen. We'll begin with an overview of Google Cloud load balancing. We will continue with a discussion of hybrid load balancing, in other words, load balancing between Google Cloud, other public clouds, and on premises environments. We will follow with a discussion of traffic management, which provides enhanced features to route traffic based on criteria that you specify. After that, you will apply what you've learned in a traffic management lab exercise. Next, we'll discuss using internal application load balancers as next hops, including benefits, caveats, and some use cases. We'll continue by discussing how to use the Google Global Edge network to serve content closer to users with the cloud content delivery network, or CDN. We'll follow that with a lab exercise and a brief quiz. Let's get started.

## Hybrid Load Balancing and Traffic Management

This module covers load balancing between environments inside and outside of Google Cloud. You will also learn how to configure traffic controls.

### Video - [Load balancing](https://www.cloudskillsboost.google/course_templates/1143/video/500640)

* [YouTube: Load balancing](https://www.youtube.com/watch?v=FECM3ppfruE)

Welcome to the hybrid load balancing and traffic management module of the networking and Google Cloud load balancing course. In this module, we will cover the topics listed on the screen. We'll begin with an note review of cloud load balancing. We will continue with the discussion of hybrid load balancing. In other words, load balancing between Google Cloud, other public clouds, and on-premises environments. We will follow with a discussion on traffic management, which provides enhanced features to route traffic based on criteria that you specify. After that, you'll apply what you've learned in a traffic management lab exercise. Let's get started. A load balancer, as the name suggests, balances load across multiple instances of your applications. Cloud load balancing receives client traffic. This traffic can be external or internal depending on the load balancer you use. A backend configuration distributes requests to healthy backends. Some load balancers also support backend buckets. One or more back ends must be connected to the backend service or backend bucket. Backend configuration defines how traffic is distributed, which health check to use, if session affinity is used, and which other services are used, such as Cloud CDN or Identity-Aware Proxy. Cloud load balancing can route traffic to either a backend service or a backend bucket. The backend services define how to handle the traffic. For example, backend services define how the traffic is distributed, which health check to use, and if session affinity is used. Backend services also define which other Google Cloud services to use, such as Cloud CDN or Identity-Aware Proxy. On the other hand, backend buckets direct incoming traffic to Cloud Storage buckets. Backend buckets are useful in serving static content. We will discuss this in more detail in the upcoming section. Some of the backend services include a managed instance group or a network endpoint group or NEG. In this module, we are going to look at some special features related to network endpoint groups. Google Cloud Platform offers a range of load balancing solutions that can be classified based on the OSI model layer that they operate at and their specific functionalities. Application load balancers, these load balancers are designed to handle HTTP and HTTPS traffic, making them ideal for web applications and services that require advanced features like content based routing and SSL or TSL termination. Application load balancers operate as reverse proxies, distributing incoming traffic across multiple backend instances based on rules that you define. They are highly flexible and can be configured for both Internet facing external and internal applications. Network load balancers. Network load balancers operate at the transport layer and efficiently handle TCP, UDP, and other IP protocols. They can be further classified into two types, proxy load balancers. These also function as reverse proxies, terminating client connections and establishing new ones to backend services. They offer advanced traffic management capabilities and support backends located both on-premises and in various cloud environments and passthrough load balancers. Unlike proxy load balancers, these do not modify or terminate connections. Instead, they directly forward traffic to the backend while preserving the original source IP address. This type is well suited for applications that require direct server return or need to handle a wide range of IP protocols. A network endpoint group or NEG is a configuration object that specifies a group of backend endpoints or services. A common use case for this configuration is deploying services and containers, as in Google Kubernetes Engine. The load balancer must be able to select a pod from the container, as shown in the example. You can also distribute traffic in a granular fashion to workloads and services that run on your backend hosts. You can use NEGs as backends for some load balancers with traffic director. There are five types of NEGs. Zonal NEGs define how end points should be reached, whether they are reachable and where they are located. A zonal NEG contains one or more endpoints that can be compute engine virtual machines, VMs, or services that run on VMs. Each endpoint is specified either by an IP address or an IP port combination. An Internet NEG contains a single endpoint that is hosted outside of Google Cloud. This endpoint is specified by host name FQDN port or IP port. A serverless NEG is a backend that points to a cloud-run app engine, cloud functions or API gateway service that resides in the same region as the NEG. A private service NEG contains a single endpoint. That endpoint resolves to either a Google managed regional API endpoint or a managed service published by using Private Service Connect. A hybrid connectivity NEG points to traffic director services that run outside of Google Cloud, on-premises or other public cloud backends. The focus in this module is on hybrid connectivity NEGs. For more information on using NEGs and a complete list of supported load balancers, please refer to the network endpoint groups overview in the Google Cloud documentation.

### Video - [Hybrid load balancing](https://www.cloudskillsboost.google/course_templates/1143/video/500641)

* [YouTube: Hybrid load balancing](https://www.youtube.com/watch?v=bMF_Livd8qw)

Let's next discuss hybrid load balancing. A hybrid load balancer, lets you extend Cloud load balancing to workloads that run on your existing infrastructure outside of Google Cloud. A hybrid strategy is a pragmatic solution for you to adapt to changing market demands and incrementally modernize the backend services that run your workloads. You can create a hybrid deployment to enable migration to a modern Cloud based solution or a permanent fixture of your organization IT infrastructure. Next, let's look at a few general use cases of hybrid load balancing. Jia, a Cloud network engineer, needs to modernize the symbol corporation IT infrastructure to improve application performance and scalability. Currently, Symbol runs a mix of on-premises applications and applications deployed in Google Kubernetes Engine, or GKE. Traffic is often unpredictable with sudden surges during sales or promotions, overwhelming their on-premises servers. Azumi knows they need a way to handle these spikes without compromising performance or spending a fortune on excess capacity that sits idle most of the time. Additionally, managing two separate load balancing solutions for the premises and GKE environments is becoming increasingly cumbersome. In this example, traffic from clients on the public Internet enters your private on-premises environment, and traffic from another public Cloud provider enters through a Cloud load balancer. The load balancer also gets requests from internal clients. The load balancer sends requests to the services that run your workloads. These services are the load balancer endpoints, and they can be located inside or outside of Google Cloud. You can figure a load balancer backend service to communicate to the external endpoints by using a hybrid NEG. The external environments can use Cloud Interconnect or Cloud VPN to communicate with Google Cloud. The load balancer must be able to reach each service with a valid IP address port combination. The example shows a load balancer backend service with a hybrid and a Zonal NEG. The hybrid NEG connects to endpoints that are on-premises and in other public Clouds. The Zonal NEG points to the Cloud endpoints in the same subnet and zone. A hybrid load balancer requires special configuration only for the backend service. The frontend configuration is the same as any other load balancer. To configure the backend services outside of Google Cloud, first configure one or more hybrid connectivity network endpoint groups or NEGs. Add each non Google Cloud Network endpoint IP port combination to a hybrid connectivity network endpoint group or NEG. Ensure that the IP address and port are reachable from Google Cloud. For hybrid connectivity NEGs, you set the network endpoint type to non GCP Private IP Port. Create the NEG in a Google Cloud zone that is as close as possible to your other environment. For example, if you're hosting a service in a non premises environment in Bengaluru, India, can place the NEG in the Asia South 1A Google Cloud zone, as shown in the example. Next, add a health check to the NEG. Add the hybrid connectivity NEGs to a hybrid load balancer backend. A hybrid connectivity NEG must only include endpoints outside Google Cloud. Traffic might be dropped if a hybrid NEG includes endpoints for resources within a Google Cloud VPC network. You can use hybrid load balancing with the following, global external application load balancer, classic application load balancer, regional external application load balancer, cross region internal application load balancer, regional internal application load balancer, external proxy network load balancer, global and regional, regional internal proxy network load balancer, and cross region internal proxy network load balancer. You choose a load balancer depending on your needs, such as where the clients and workloads are located. To create, delete, or manage a load balancer with mixed zonal and hybrid connectivity NEG backends in a single backend service, you must use the Google Cloud CLI or the Rest API. Regional dynamic routing, and static routes are not supported. The Cloud router used for hybrid connectivity must be enabled with global dynamic routing. The internal application load balancer and hybrid connectivity must be configured in the same region. If they are configured in different regions, you might see backends as healthy, but client requests will not be forwarded to the backend. Ensure that you also review the security settings on your hybrid connectivity configuration. Currently, HA Cloud VPN connections are encrypted by default, using IPSec encryption. Cloud Interconnect connections are not encrypted by default. For more details, go to the encryption in transit in Google Cloud on the Google Cloud website.

### Video - [Traffic management](https://www.cloudskillsboost.google/course_templates/1143/video/500642)

* [YouTube: Traffic management](https://www.youtube.com/watch?v=Acj2HdPdh-g)

Traffic management is key to ensuring optimal network performance and user experience. In this section, we'll delve into a real world use case and its implementation. Bola, a Cloud network engineer at Cymbal Corporation, is responsible for maintaining their video streaming platform. The website accesses the Cymbal store infrastructure using Cloud load balancing. To optimize performance and costs, they want to route user traffic to different server backends based on the requested video quality. For example, 4K versus HD. What should Bola do? Bola can benefit from traffic management. Traffic management provides enhanced features to route load balancer traffic based on criteria that you specify. With traffic management, you can; implement traffic steering based on HTTPS parameters, such as the host, path, headers, and other request parameters. Perform request-based and response-based actions, such as redirects and header transformations, and use traffic policies to fine tune load balancing behavior, such as retry policies, request mirroring, and cross origin resource sharing or cores. The traffic features that are available can vary per load balancer. Check the Google Cloud documentation for details. For example, traffic management overview for a classic application load balancer, traffic management overview for global external application load balancers, and traffic management overview for regional external application load balancers. Recall that in addition to traffic management, Cloud load balancing offers backend services like health checks, session affinity, balancing mode, and capacity scaling. These load balancers support traffic management. Global external Application Load Balancer, Global classic Application Load Balancer, Internal Application Load Balancer, and the Regional external Application Load Balancer. Other load balancers have access to only traffic features available in backend services, such as balancing mode and session affinity. Not all load balancers support all traffic management features. For a complete list of traffic management features supported for each load balancer, refer to the routing and traffic management in the Google Cloud documentation. The URL map contains rules that define the criteria to use to route incoming traffic to a backend service or a backend bucket. Traffic management features are configured in a URL map. In other words, the load balancer uses the URL map to determine where to route incoming traffic. When you configure routing, you can choose between the following modes; simple host and path rule or advanced host path and route rule. Each URL map can contain only one mode or the other mode. A request is first evaluated based on the host rule. If the domain matches a defined host rule, the system uses its associated path matcher to further refine routing. Each host rule consists of a list of one or more hosts and a single path matcher. If no host rules are defined, the request is routed to the default service. The request path is compared against available path rules with the longest, most specific match taking priority. Path rules are evaluated on a longest path matches first basis. You can specify the path rules in any order. Once the best match is found, the request is sent to the corresponding backend service. If no specific host and path rules apply, the request is handled by the default backend service. This setup allows you to create custom routing rules like sending video related requests to a dedicated service while directing general traffic to a different backend. Going back to the scenario, Bola can use the URL map to distribute traffic. The host rule is cymbal.com, which means any host other than cymbal.com, for example, cymbal.org or cymbal.net are directed to the default service. Once the host name cymbal.com matches, the URL is matched for a path rule. The default backend service is video site. Requests with the exact URL path video-hd are directed to the video HD backend service. Requests with the exact URL path video-4k are directed to the video-4k backend service. On this slide, you see a URL map that routes video traffic to the example we covered in the previous slides. This example is shown by using a YAML file. You can also use the Google Cloud Console to configure URL maps. Let's look at how this example works. Default service. The default service defines a service where traffic should be routed when no matching URL is found. You must specify a default service or a backend bucket. The host rules defines a list of host names that are processed by this rule. In this example, there's only one item in the list, which means that only one host rule is defined. This host rule contains an asterisk. The asterisk is a wildcard, which matches all hosts. To see where to find the matching logic to use, look at the value of path matcher. For this host rule, pathMatcher is set to pathmap. Here's a path matchers list, which contains a list of path matching rules. The only element in this list is path map. Each match rule defines logic to process the traffic that is sent to the backend service. In this example, there are two sets of paths. One paths list defines valid URL paths for the video hd. The other paths list defines valid URL paths for the video 4K. If the URL contains a match for one of these paths lists, the load balancer routes the traffic to the corresponding service. If the traffic contains a path that matches none of the paths lists, then it's sent to the default backend video site. In other words, the traffic is sent to the service denoted by pathmatchers/defaultservice. For additional details, refer to the documentation. The advanced routing mode can choose a rule based on a defined priority and includes additional configuration options. Instead of path rules, advanced routing uses route rules. Each URL map can include either simple or advanced rules, but not both. This URL map contains rules that route 95% of the traffic to Service A, and 5% of the traffic is routed to Service B. The example shows a YAML implementation of an advanced routing mode. You can also use the Google Cloud Console to configure URL maps. Let's look at how this example works. When no matching host rule is found, the default service defines a service to use. The field default service is required. The host rules works the same way for simple routing mode. As in the previous example, this host rule uses the asterisk to match all hosts. Because path matcher is set to matcher 1, /pathMatchers/matcher1 defines the matching logic. /pathMatchers/matcher1 contains a list of route rules. The route rules contain a list of one or more match rules and a route action. When URL satisfy the match rules, their traffic is processed by the route action. In this example, there's only one item in match rules. Where prefix match equals an empty string, the prefix match condition matches the URL path prefix, URLs that start with the same string match. In the example, the prefix match is the empty string, which matches all URLs. In other words, all URLs trigger this match rule, and the route action is applied. The route action defines how the traffic is routed. In the example, the route action is set to weightedBackendServices. Weighted backend services is a list of backend services. A weight value is specified for each backend service, representing a percentage of the total traffic. Ninety-five percent of the traffic is sent to Service A, and 5% of the traffic is sent to Service B. The route action can also define traffic policies such as retry policies and cores. For a complete list of route action values, refer to the Google Cloud documentation for the load balancer that you're using. In this example, you might notice that there are two default service key value pairs. One default service is associated with the host rules and the other is associated with the route rules. If there's no matching host rule, the first default service is used. If there's no matching route rule, the second default service is used. Not all load balancers support all traffic management features. For a complete list of traffic management features supported for each load balancer, refer to routing and traffic management in the Google Cloud documentation. Wild cards are supported, but only after a forward slash. For example, /video/* is valid and /video* is invalid. Rule matching does not use regular expressions or substring matching. For example, /videos/hd* does not match /videos/hd-pdq, because -pdq is a substring and also because it comes after the forward slash. /video/* matches /video/hd-pdq. You can also use Gemini to learn more. One trick to get better responses is to assign Gemini a role. Prime the model to assume a specific role, also known as a persona. Adding a role is not always necessary, but can enforce a certain level of expertise when generating a response. Improve performance, and tailor's communication style. This technique is particularly useful for getting the model to perform highly technical tasks or enforcing specific communication styles. The example on the slide shows a sample prompt where Gemini assumes the role of a Google Cloud technical support engineer.

### Video - [Lab Intro: Configuring Traffic Management with a Load Balancer](https://www.cloudskillsboost.google/course_templates/1143/video/500643)

* [YouTube: Lab Intro: Configuring Traffic Management with a Load Balancer](https://www.youtube.com/watch?v=he_52s8txsE)

In this lab, you will create a regional internal application load balancer with two back ends. Each back end will be an instance group. You will configure the load balancer to create a blue green deployment; the blue deployment refers to the current version of your application. The green deployment refers to a new application version. In this lab exercise, you configure the load balancer to send 70% of the traffic to the blue deployment, and 30% to the green deployment.

### Lab - [Configuring Traffic Management with a Load Balancer](https://www.cloudskillsboost.google/course_templates/1143/labs/500644)

In this lab, you create two managed instance groups in the same region. Then, you configure an Internal Load Balancer with the instances groups as the backends.

* [ ] [Configuring Traffic Management with a Load Balancer](../labs/Configuring-Traffic-Management-with-a-Load-Balancer.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/1143/quizzes/500645)

#### Quiz 1.

> [!important]
> **You can use hybrid load balancing to connect these environments:**
>
> * [ ] Google Cloud and on-premises
> * [ ] Google Cloud and AWS
> * [ ] Google Cloud, AWS, and on-premises
> * [ ] Google Cloud, other public clouds, and on-premises

#### Quiz 2.

> [!important]
> **Where would you configure traffic management for a load balancer?**
>
> * [ ] In the load balancer frontend
> * [ ] In the load balancer backend
> * [ ] In the URL map
> * [ ] In the load descriptor

#### Quiz 3.

> [!important]
> **When you use the internal IP address of the forwarding rule to specify an internal Network Load Balancer next hop, the load balancer can only be:**
>
> * [ ] In the same VPC network as the next hop route.
> * [ ] In the same VPC network as the next hop route or in a peered VPC network.
> * [ ] In the same subnet as the next hop route.
> * [ ] In the same subnet as the next hop route or a Shared VPC network.

### Video - [Debrief](https://www.cloudskillsboost.google/course_templates/1143/video/500646)

* [YouTube: Debrief](https://www.youtube.com/watch?v=aUSbwi6QLOU)

In this module, we began with an overview of load balancing in Google Cloud. We continued with the discussion of hybrid load balancing. You learned that hybrid load balancing can be used to migrate your workloads into Google Cloud or to provide multiple platforms for your workloads. We covered the load balancers that support hybrid load balancing and an overview of the components that you must configure. We then talked about using traffic management with your load balancers. You learned which load balancers support traffic management features. You were introduced to the URL map, where you configured traffic management features. We walked through a simple example of traffic management. In the example, you saw how to configure a URL map to match against incoming traffic and specify where the traffic should be sent. You then applied what you learned in a lab exercise. Finally, you took a brief quiz to test your knowledge.

## Caching and Optimizing Load Balancing

This module introduces Google Cloud Armor and Cloud CDN. It also covers caching content near the user and the benefits of using TCP/UDP load balancers as next hops, and load balancer optimization strategies.

### Video - [Internal Network Load Balancers as next hops](https://www.cloudskillsboost.google/course_templates/1143/video/500647)

* [YouTube: Internal Network Load Balancers as next hops](https://www.youtube.com/watch?v=l7u4l0Fa-zI)

Let's cover some additional concepts regarding load balancing in this module. In this module, we will discuss using internal network load balancers as next hops, including benefits, caveats, and some use cases. We will also introduce Google Cloud Armor and cloud CDN. We will cover content caching and share some strategies to optimize load balancing. Let's get started. Before we cover how to use an internal network load balancer as a routing next hop, let's discuss why these load balancers are useful. They're fast. Internal network load balancers don't have the overhead associated with other types of cloud load balancers. The reduced overhead makes them fast. An internal network load balancer routes connections directly from clients to the healthy backend without any interruption. There's no immediate device or single point of failure. Client requests to the load balancer IP address go directly to the healthy backend VMs. Unlike other types of load balancers, there's minimal processing of the incoming traffic. Responses from the healthy backend VMs go directly to the clients, not back through the load balancer. TCP responses use direct server return. For more information, see IP addresses for request and return packets in the Google Cloud documentation. Lets consider some use cases for internal network load balancers. You can load balance traffic across multiple VMs that are functioning as gateway or router VMs. You can use gateway virtual appliances as the next hop for a default route. With this configuration, VM instances in your virtual private cloud or VPC network send traffic to the Internet through a set of load balanced virtual gateway VMs. You can send traffic through multiple load balancers in two or more directions by using the same set of multi-NIC gateway or router VMs as backends. To accomplish this result, you create a load balancer and use it as the next hop for a custom static route in each VPC network. Each internal network load balancer operates within a single VPC network, distributing traffic to the network interfaces of backend VMs in that network. In these use cases, the backend services are the gateway VMs, gateway virtual appliances, multi-NIC gateways, and router VMs. Because these resources are all internal, it makes sense to access them through an internal network load balancer. As we discussed a moment ago, these load balancers have lower overhead than other load balancers that Google Cloud offers. Next, let's consider how to access these backends even faster. To specify the next hop, you have three choices as shown in the table. The main difference concerns the location of the next hop load balancer. If the next hop load balancer is in the same VPC network, you can specify the forwarding rule name and the load balancer region. To use a next hop load balancer in a peered VPC network, specify the internal IP address of the forwarding rule. You can also specify a next hop forwarding rule by its resource link. The forwarding rules network must match the routes VPC network. The forwarding rule can be located in either the project that contains the forwarding rules network, a standalone project, or a shared VPC host project, or a shared VPC service project. This use case load balances traffic from internal vms to multiple network address translation gateway instances that route traffic to the Internet. In this example, an internal network load balancer has next hops configured to three compute engine VMs. Each compute engine VM has a NAT gateway that runs on it and has can IP forward set to be true. These VMs then forward traffic to the Internet. Optionally, you can set up the gateways to apply custom logic to fine tune access to the Internet. In addition to exchanging subnet routes, you can configure VPC network peering to export and import custom static and dynamic routes. Custom static routes that have a next hop of the default Internet gateway are excluded. Custom static routes that use next hop internal network load balancers are included. You can configure a hub and spoke topology with your next hop Firewall virtual appliances located in the hub VPC network by doing the following. In the hub VPC network, create an internal network load balancer with Firewall virtual appliances as the backends. In the hub VPC network, create a custom static route with the destination subnet 10.0.2.2.0/24 and set the next hop to be the internal network load balancer, 10.0.1.100. Use VPC network peering to connect the hub VPC network to each of the spoke VPC networks. For each peering, configure the hub network to export its custom routes and configure the corresponding spoke networks to import custom routes. Custom static route created in step two with destination subnet 10.0.2.0/24, the route with the load balancer next hop is one of the routes that the hub network exports. Subject to the routing order, the next hop Firewall appliance load balancer in the hub VPC network is available in the spoke networks. If global access is enabled, the Firewall appliance is available according to the routing order. If global access is disabled, then resources are only available to requesters in the same region. Internal network load balancer 1 shown on the left distributes traffic from the clients to nic0, the primary interface on the backend services. The internal network load balancer 2 shown on the right distributes traffic from clients to nic1, the secondary interface on the backend services. The result is that clients can connect to the backend services through nic0 or nic1. When the load balancer is a next hop for a static route, no special configuration is needed within the client VMs. Client VMs send packets to the load balancer backends through VPC network routing in a bump in the wire fashion. Using an internal pass through network load balancer as a next hop for a static route provides the same benefits as a standalone internal pass through network load balancer. The health check ensures that new connections are routed to healthy backend VMs. By using a managed instance group as a backend, you can configure autoscaling to grow or shrink the set of VMs based on service demand. You must enable global access on the VPC network so that the next hop is usable from all regions. Whether the next hop is usable depends on the global access setting of the load balancer. With global access enabled, the load balancer next hop is accessible in all regions of the VPC network. With global access disabled, the load balancer next hop is only accessible in the same region as the load balancer. With global access disabled, packets sent from another region to a route that uses an internal network load balancer next hop are dropped. Even if all health checks fail, the load balancer next hop is still in effect. Packets processed by the route are sent to one of the next hop load balancer backends. If needed, configure a failover policy. A next hop internal network load balancer must use an IP address that is unique to a load balancer forwarding rule. Only one backend service is unambiguously referenced. Two or more custom static route next hops with the same destination that use different load balancers are never distributed by using ECMP. If the routes have unique priorities, Google Cloud uses the next hop internal network load balancer from the route with the highest priority. If the routes have equal priorities, Google Cloud still selects just one next hop internal network load balancer. For packets with identical source IP addresses routed to the same backend, use the client IP, NO_DESTINATION, CLIENT_IP_NO_DESTINATION session affinity option. There are some additional caveats for using an internal network load balancer as a next hop. For example, pertain to the use of network tags. For additional information on this and other caveats, refer to additional considerations on the internal network load balancers as next hop's page in the Google Cloud documentation.

### Video - [Cloud CDN](https://www.cloudskillsboost.google/course_templates/1143/video/500648)

* [YouTube: Cloud CDN](https://www.youtube.com/watch?v=a5uJqE8rPR4)

Next, let's discuss content delivery network or CDN, which caches content nearer to users. We'll also talk about CDN Interconnect, which lets select third party content delivery network or CDN providers establish direct interconnect links at edge locations in the Google network. Cloud CDN caches content at the edges of the Google network. This caching provides faster content delivery to users while reducing transmission costs. Content can be cached at CDN nodes as shown on the map. There are over 90 of these cache sites spread across metropolitan areas in Asia Pacific, the Americas, and AMIA. For an updated list, see the cache locations in the Google Cloud documentation. For Cloud CDN performance measured by Stexis, please see the reports on the Citrix website. When setting up the back end service of an application load balancer, you can enable Cloud CDN with a checkbox. Using cache modes, you can control the factors that determine whether Cloud CDN caches your content. Cloud CDN offers three cache modes. The cache modes define how responses are cached, whether Cloud CDN respects cache directives sent by the origin, and how cache TTLs are applied. The available cache modes are USE_ORIGIN_HEADERS, CACHE_ALL_STATIC, and FORCE_CACHE_ALL. USE_ORIGIN_HEADERS mode requires origin responses to set valid cache directives and valid caching headers. CACHE_ALL_STATIC mode automatically caches static content that doesn't have the no store private or no cache directive. Origin responses that set valid caching directives are also cached. FORCE_CACHE_ALL mode unconditionally caches responses, overriding any cache directive set by the origin. If you use a shared back-end with this mode configured, ensure that you don't cache private per user content, such as dynamic HTML or API responses. Let's walk through the Cloud CDN response flow with this diagram. In this example, the application load balancer has two types of back-ends. There are managed VM instance groups in the US central one and Asia East one regions, and there's a Cloud storage bucket in US East one. A URL map decides which back-end to send the content to. The Cloud storage bucket could be used to serve static content, and the instance groups could handle PHP traffic. When a user in San Francisco is the first to access content, the cache site in San Francisco sees that it can't fulfill the request. This situation is called a cache miss. If content is in a nearby cache, Cloud CDN might attempt to get the content from it. For example, if a user in Los Angeles has already accessed the content. Otherwise, the request is forwarded to the application load balancer, which in turn forwards the request to one of your back-ends. Depending on the content that is being served, the request will be forwarded to the US Central One Instance Group or the US East one storage bucket. If the content from the back end is cacheable, the cache site in San Francisco can store it for future requests. In other words, if another user requests the same content in San Francisco, the cache site might now be able to serve that content. This approach shortens the round trip time and saves the origin server from having to process the request. This is called a cache hit. For more information on what content can be cached, please refer to caching overview in the Google Cloud documentation. Each Cloud CDN request is automatically logged within Google Cloud. These logs will indicate a cache hit or cache miss status for each HTTP request of the load balancer. You will explore such logs in the next lab. Cache modes, let you control how content is cached. CDN Interconnect lets select third party content delivery network providers establish direct interconnect links at edge locations in the Google Edge network. These connections let you direct your traffic from your VPC networks to a CDN provider network. For a complete list of CDN providers, refer to the CDN Interconnect overview in the Google Cloud documentation. CDN Interconnect lets you connect directly to select CDN providers from Google Cloud. Your network traffic that egresses from Google Cloud through one of these links benefits from the direct connectivity to supported CDN providers. CDN Interconnect reduces your Cloud CDN cache population costs. If you have a high volume of egress traffic, consider using CDN Interconnect. You can use the CDN interconnect links between Google Cloud and selected providers to automatically optimize the egress traffic and save money. If you're populating the Cloud CDN cache locations with large data files from Google Cloud, this optimization can be especially helpful. Frequent content updates are another typical CDN Interconnect use case. Cloud workloads that frequently update data stored in Cloud CDN cache locations benefit from using CDN Interconnect. The direct link to the Cloud CDN provider reduces latency. Ingress traffic is free for all regions. Egress traffic rates apply only to data that leaves compute engine or Cloud storage. Egress charges for CDN interconnect appear on the invoice as compute engine network egress via carrier peering network. The special pricing for your traffic that egress from Google Cloud to a CDN provider is automatic. Google works with approved CDN partners and supported locations to accept provider IP addresses. Any data that you send to your allow listed CDN provider from Google Cloud is charged at the reduced price. This reduced price applies only to IPv4 traffic. It does not apply to IPv6 traffic. Intra-region pricing for CDN Interconnect applies only to intra-region egress traffic that is sent to Google Approved CDN providers at specific locations. CDN Interconnect does not require any configuration or integration with Cloud load balancing. If your CDN provider is already part of the program, you don't need to do anything. Traffic from supported Google Cloud locations to your CDN provider automatically benefits from the direct connection and reduced pricing. Work with your supported CDN provider to learn what locations are supported. Your supported CDN service provider can also help you correctly configure your deployment to use intra-region egress routes, which costs less than intra-region egress traffic.

### Video - [Google Cloud Armor](https://www.cloudskillsboost.google/course_templates/1143/video/500649)

* [YouTube: Google Cloud Armor](https://www.youtube.com/watch?v=tht2EnfFniU)

Next, let's discuss the benefit of using Google Cloud Armor with Cloud CDN. Google Cloud Armor and Cloud CDN work in tandem to provide a secure and optimized web experience. Imagine Google Cloud Armor as your application's security guard. It inspects incoming traffic, filters out malicious requests like DDoS attacks and web exploits, and safeguards your backend servers from harm. This protection ensures your application stays up and running, even under attack. Meanwhile, Cloud CDN stores copies of your website's content, images, videos, etc, around the globe. When a user accesses your website, Cloud CDN delivers the content from the location closest to them. This significantly speeds up page load times, providing a seamless experience for your users while also reducing the strain on your primary servers. To protect CDN origin servers, you can use Google Cloud Armor with Cloud CDN. Google Cloud Armor protects your CDN origin server from application attacks, mitigates OWASP top ten risks, and enforces layer 7 filtering policies. There are two types of security policy that affect how Google Cloud Armor works with Cloud CDN, Edge security policies and backend security policies. Use Edge security policies to filter requests before content is served from cache. Use Google Cloud Armor backend security policies to protect requests routed to the backend service. Next, you will explore a lab covering how to use Google Cloud Armor to defend Edge cache.

### Lab - [Defending Edge Cache with Cloud Armor](https://www.cloudskillsboost.google/course_templates/1143/labs/500650)

Apply Google Cloud Armor security policies to restrict access to cached objects on Cloud CDN and Cloud Storage, and block the traffic before it reaches the load balanced backend services or buckets.

* [ ] [Defending Edge Cache with Cloud Armor](../labs/Defending-Edge-Cache-with-Cloud-Armor.md)

### Video - [Load balancer optimization strategies](https://www.cloudskillsboost.google/course_templates/1143/video/500651)

* [YouTube: Load balancer optimization strategies](https://www.youtube.com/watch?v=8RbGAzWww8E)

Next, let's discuss some load balancer optimization strategies. Optimizing your cloud load balancer configuration can significantly reduce costs without impacting performance or availability. Here are some key strategies. Autoscaling, dynamically adjust resources enable autoscaling to automatically scale backend instances based on real-time traffic demands. This ensures you only pay for the resources you need, eliminating over provisioning and unnecessary costs. Define scaling thresholds. Set clear thresholds for scaling up and down to optimize resource utilization and prevent unnecessary scaling events. And utilize custom metrics, leverage custom metrics like CPU usage or response time to trigger autoscaling, ensuring resources are allocated based on specific performance indicators. Right sizing sources, choose the right load balancer type. Select the most appropriate load balancer type, for example, application, proxy network, or pass through network based on your traffic type and requirements. Avoid using a more expensive type than necessary. Match resources to workload. Select the appropriate machine type and size for your backend instances based on their anticipated workload. Over provisioning resources will lead to higher costs while under provisioning can impact performance. Regularly review resource utilization. Monitor the CPU, memory, and network utilization of your backend instances and load balancers. If utilization is consistently low, consider downsizing resources for cost savings. Use cloud monitoring and cost management tools. Use cloud monitoring, use Cloud Monitoring to gain insights into your load balancer and backend instance performance. Analyze metrics like CPU usage, memory consumption, and network bandwidth to identify areas for optimization. Leverage Cloud cost management tools. Use tools like Cloud Billing and Cloud cost management to track your load balancing costs and identify potential savings opportunities. These tools can provide detailed cost breakdowns by project, service, and resource, helping you identify underutilized resources or areas for potential consolidation. Cost allocation tags, implement cost allocation tags to categorize your load balancing costs by department, project, or any other relevant criteria. This allows for more granular cost tracking and facilitates cost optimization efforts. Additional strategies, scheduled downtimes. Consider scheduling downtimes for non-critical workloads during periods of low traffic to reduce costs. Reserved instances, use reserved instances for predictable workloads to obtain significant discounts on load balancer resources. Spot instances, explore using spot instances for non-critical workloads to take advantage of discounted compute resources. Cloud CDN integration, integrate Cloud CDN with your load balancer to reduce the load on your backend instances and potentially reduce load balancer costs. Remember, cost optimization is an ongoing process. By implementing these strategies and continuously monitoring your cloud resources, you can achieve significant cost savings while ensuring your cloud infrastructure remains efficient and scalable.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/1143/quizzes/500652)

#### Quiz 1.

> [!important]
> **When you use the internal IP address of the forwarding rule to specify an internal Network Load Balancer next hop, the load balancer can only be:**
>
> * [ ] In the same subnet as the next hop route.
> * [ ] In the same VPC network as the next hop route or in a peered VPC network.
> * [ ] In the same subnet as the next hop route or a shared VPC network.
> * [ ] In the same VPC network as the next hop route.

#### Quiz 2.

> [!important]
> **CDN Interconnect provides:**
>
> * [ ] A direct peering connection between third-party content delivery networks (CDNs) and Google's edge network.
> * [ ] A virtual private network (VPN) tunnel between your VPC network and Google's global network.
> * [ ] A direct connection between your origin servers and Google's Cloud Load Balancing service.
> * [ ] A private connection between your on-premises network and Google Cloud.

#### Quiz 3.

> [!important]
> **Which of the following best practices help optimize load balancing cost?**
>
> * [ ] Ignoring load balancer health checks to avoid additional API calls.
> * [ ] Selecting the most expensive load balancer type for maximum performance.
> * [ ] Overprovisioning load balancer resources to handle peak traffic loads.
> * [ ] Implementing a caching layer with a content delivery network (CDN).

### Video - [Debrief](https://www.cloudskillsboost.google/course_templates/1143/video/500653)

* [YouTube: Debrief](https://www.youtube.com/watch?v=usXadL-Maeo)

In this module, we began with an overview of load balancing in Google Cloud. We continued with a discussion of hybrid load balancing. You learned that hybrid load balancing can be used to migrate your workloads into Google Cloud or to provide multiple platforms for your workloads. We covered the load balancers that support hybrid load balancing and an overview of the components you must configure. We then talked about using specific management with your load balancers. You learned which load balancers support traffic management features. You were introduced to the URL map where you configure traffic management features. We walked through a simple example of traffic management. In the example you saw how to configure a URL map to match incoming traffic and specify where the traffic should be sent. You then applied what you learned in a lab exercise. Next, we covered using internal network load balancers as next hops. You learned about some of the major use cases and some simple topologies were shown. We continued by discussing using Cloud CDN to get content to your clients faster. You also learned about CDN Interconnect to direct traffic from your VPC networks to a supported CDN provider network. You also learned how Cloud CDN Interconnect optimizes your Cloud CDN cache population costs. Finally, you took a brief quiz to test your knowledge.

## Course Resources

Student PDF links to all modules

### Document - [Networking in Google Cloud: Load Balancing Course Resources](https://www.cloudskillsboost.google/course_templates/1143/documents/500654)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
