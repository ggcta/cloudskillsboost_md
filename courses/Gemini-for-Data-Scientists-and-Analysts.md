---
id: 879
name: 'Gemini for Data Scientists and Analysts'
type: Course
url: https://www.cloudskillsboost.google/course_templates/879
date_published: 2024-05-14
topics:

---

# [Gemini for Data Scientists and Analysts](https://www.cloudskillsboost.google/course_templates/879)

**Description:**

In this course, you learn how Gemini, a generative AI-powered collaborator from Google Cloud, helps analyze customer data and predict product sales. You also learn how to identify, categorize, and develop new customers using customer data in BigQuery. Using hands-on labs, you experience how Gemini improves data analysis and machine learning workflows.

Duet AI was renamed to Gemini, our next-generation model.

**Objectives:**

* Use Gemini and BigQuery to analyze data and predict product sales.
* Identify and categorize new customers using BigQuery and Gemini.
* Use Gemini, Vertex AI and BigQuery to generate useful next steps for a marketing campaign.

## Gemini for Data Scientists and Analysts

Learn how to analyze data in BigQuery and predict product sales with help from Gemini. identify and categorize new customers using customer data in BigQuery, and generate useful next steps for a marketing campaign.

### Video - [Introducing Gemini for data professionals](https://www.cloudskillsboost.google/course_templates/879/video/476687)

* [YouTube: Introducing Gemini for data professionals](https://www.youtube.com/watch?v=_pFAmZBwels)

[MUSIC] This is a data professional. They might be a data analyst, data engineer, or data scientist. We'll refer to them as the data pro. Let's learn how you can use BigQuery, Gemini, and Vertex AI to analyze your data to categorize new customers or gain insights like predicted product sales. Later, you'll use Gemini, Vertex AI, and BigQuery to generate useful next steps for a marketing campaign. They've recently joined a new company, and part of their role requires them to turn their teams manufacturing plant dashboard from a reactive one into a proactive one so that their team can anticipate issues with equipment. The data pro is here to help ensure that the equipment is functioning properly. They do that by using data to create reporting mechanisms that predict when maintenance should occur. And Gemini is here to help the data pro. The first step is to open the manufacturing plant dashboard powered by Looker. All of the plants real-time structured and unstructured data is loaded into Google Cloud, analyzed and presented here. They can identify that one of the plants is having some issues, so they click into it to explore. Immediately, they notice that throughput has been below target for a number of hours, starting at 2 PM. The data pro decides to use Gemini in Looker to learn why throughput is down. In the chat window, they prompt Gemini, why did the throughput drop at 2 PM? Gemini responds with the production line that throughput dropped four. And because it knows the context of the situation, Gemini has gone one step further and suggested additional questions to ask, like this one, which metrics correlate with the dip? From the response, the data pro realizes that there are some equipment issues. Gemini has gathered from routine diagnostics some machine statistics that our pro will use to start communicating the physical fix to technicians. This is really helpful for our data pro. Now, our data pro can understand why they were tasked with making this dashboard proactive instead of reactive. They need to build a preventative maintenance model to predict future downtime and avoid it. To build and ship a data model, they start in BigQuery Studio, a simple, unified environment where they can build anything they want, including in Python. First, they determine what data already exists so they don't have to start from the beginning. There are two data sets, one in Google Cloud and another in a different cloud. They notice a throughput table that might be a promising starting point. They open it to validate the usefulness and quality of the data. When they open the table, they use the data profile tool to perform some light exploratory data analysis. They also note that there are automated quality checks in place, which is a good sign, so they know this data is actively maintained. There is even an insights tab where Gemini has already analyzed the data and how it's being used, and proactively suggests insights for business questions that our data pro might want to answer. They decide to open and run one of the queries in BigQuery to further explore the insights. They appreciate how helpful these insights are to build their understanding of this data quickly. It seems that another data set also has some reference data, so they'll pull that into the model, too. Because they're building multiple ML projects, they decide to do this in a Python Notebook so collaboration is easier. Gemini supports SQL generation and completion, so they ask it to use the multicloud data to help build a model that can predict throughput based on the attributes currently available. In the Python Notebook itself, they prompt Gemini by clicking + Code and typing in a prompt to create a logistics regression model. And just like that, the SQL is ready, that saved our data pro a lot of time. It also interconnected the data across multiple clouds by using a simple SQL join in real time without the need for complex operations or processes here. Gemini helps our data pro code faster, discover the data, answer business questions, and generate insights. Amazing, our data pro wants to ensure that the model will actually run on real-time data because it was trained on a table, not a data stream. First, they use a few lines to verify that the model is actively running against the production data and with predictions, so the model is ready. Predictions are a good start, but the data pro wants to go further and actually help the plant operators by giving them recommendations for how they can keep the plant working optimally. Luckily, there is some more data available this time unstructured inspector reports that may help. They use retrieval augmented generation, which means that they will use their own data and an LLM, a large language model, to build an agent using tools to connect to their data. These tools will help improve the reliability of responses from the LLM and reduce hallucinations, which happen when the LLM produces inaccurate responses. Sounds like a lot of work, but they're going to do it in a few lines of code. First, they'll bring AI to the data in BigQuery. They use LLMs powered by Vertex AI and BigQuery's new ML inference engine to connect to all these amazing models. Summarize the reports and create vectors or numeric representations of their text to help with searches and groupings. Next, they pass a question or prompt into the LLM to get the vectors and summary. Then they use the summaries to ask the LLM for recommendations to get the predictive actions and the results are displayed. This would have taken our data pro a lot of time and effort to do if they used traditional data science methods. Here they did it with a few functions and in a couple of minutes by using the power of generative AI and all in bigquery where their data remains secure. Now the pipeline is deployed. Here are the pipeline results back in the plant dashboard where the predictive results are now displayed. Our data pro can even set actions by using Looker so that the right people are notified to take action on the new recommendations. Our data pro has succeeded in helping the equipment monitoring dashboard move from reactive to proactive. And with all that time saved, they can now finally get around to playing the ukulele. Thanks, Gemini. What will you build? [MUSIC]

### Lab - [Analyze data with Gemini assistance](https://www.cloudskillsboost.google/course_templates/879/labs/476688)

In this lab, you'll analyze BigQuery data with assistance from Gemini.

* [ ] [Analyze data with Gemini assistance](../labs/Analyze-data-with-Gemini-assistance.md)

### Video - [How to analyze data with Gemini](https://www.cloudskillsboost.google/course_templates/879/video/476689)

* [YouTube: How to analyze data with Gemini](https://www.youtube.com/watch?v=iObPJDtGsRs)

I just joined a sales team as a data analyst, and my first assignment is to create a sales time series forecasting model to help predict product sales. Before I do that, I want to get familiar with some of the sales data that's available. I'm going to use Gemini in Big Query to help me do that since I'm new and Gemini in Big Query is trained on Google Cloud-specific data and best practices. I'll start in Big Query Studio, a unified collaborative workspace that helps accelerate data to AI workflows in the Cloud Console. No, I have already granted my user with the appropriate role and enabled the Gemini API. The first thing I'll do is create a dataset, which I'll use to create a model later. Then I want to double-check that the Gemini features I want to use are enabled. Here, in the upper right of the screen, I can click on the Gemini icon in the toolbar and make sure that auto-completion, auto-generation, and explanation are all checked. Anytime I want to prompt Gemini through the chat. I click this Gemini icon in the console toolbar, and the chat will open up on the right. One of the things that Gemini can help with is knowing what data I can access for analysis and how to analyze that data. In order to familiarize myself with the sales data that's available to me on this new team, I'm going to use Gemini to explore a bit so I'm going to prompt it with something like, how do I learn which dataset and tables are available to me in big query. It's telling me that there are a few different ways that I can learn which datasets and tables are available to me. I want to do this in the console, so I'll ask a follow-up question. How can I do this in the console, and I get my answer with step-by-step instructions? To do this, I'll go to the Explorer pane and click "Add", and I can select public datasets, which is down here and I can see there are a variety of public datasets that I can choose from. But I know there's a dataset that my team has been using. I'm going to look it up. Here it is, and I can see some information about the dataset here. But I'll go ahead and view the dataset. On the left, I can see the tables that are in the dataset and if I click on one of these tables, I can see the schema, the data type and if I click Preview, I can see a preview of the data that's in there, which is really helpful for me to look up and see what kind of data I'll be working with. Now, I'll hop back into Big Query Studio. Now that I know what data is available to me, I can ask a follow-up question, how can I load different types of data into big query for use via the console? Gemini will tell me how I can do that. This is a small example of the things that I can ask Gemini for help with in relation to my data but I can also ask something like, how do I get started with big query at the most basic level up to something more complex, like, how do I create a time series forecasting model in Big Query, which we'll actually be getting help with in a bit. The best part of this is that I don't have to leave the console to look up how to do these things. I just ask Gemini and it can tell me. Now, I want to dig in a bit to some of the queries that I know are available to me. There's a query that my team uses that I can see right off the bat, tells me which tables are being queried from the dataset but it has some additional sections that would take me some more time to fully understand so I'll ask Gemini to help me with that. I'll highlight the query and click the tooltip to the left here that says, Explain this query. You can also right-click on the highlighted query and select Explain this query. The response in Gemini chat lets me know the intent of the query is to find the top 10 users by average sale price with additional info about how the code is structured to get me that end result. Here, I can compare the response from Gemini with the query and understand how that query is working and what it's meant to do. Then if I run the query, I can see that. In fact, it does give me the top 10 users by average sales price. This saved me probably 3-5 minutes of looking at the query to understand what it was doing. This is a relatively simple query, but I can already see how helpful in time-saving this will be for me when I'm looking through queries that have 30-plus lines. Next, I want to get a bit deeper into the data and instead of just knowing the top 10 customers, I want to know, what are the total sales for each product on each day. I'm able to do that right in the editor so I'm going to close out the chat and the explorer pane for now, and I can prompt Gemini in the editor to generate a query for me by starting my prompt with the pound sign and writing my prompt. Something along the lines of select sum of sales by date and products casted today from this order items table joined with products and hit "Enter". Gemini suggests the query, and it'll give me the query so I'll hit tab on my keyboard to accept it and there it is. I'm going to modify it a bit because I know the created underscore at is the table I want to use for the date and I'll put it in a descending order. Now I can run the query and get the results and I can see here the total sales for each product for each day, which can be useful to identify trends like which products are selling best on which days. I've used Gemini to explain queries and help me generate queries to help better understand the data I'm working with. Now I can focus on actually building a forecasting model so I can help predict sales for my team. I have this query that a teammate shared with me from their saved queries, and it's going to help me create a time series forecasting model. Before I run it, I'm going to ask Gemini to quickly explain this query to me. It tells me the intent is to create a time series forecasting model using the ARIMA underscore plus algorithm and the data that the model will be trained on, as well as the dataset the model will be created in, which is good to confirm that it's the dataset I created in the beginning. Again, I can thank Gemini for quickly catching me up on that query, versus having to do it myself or ask my teammate to explain it. Now, I'll run this query and while that's running, I'm actually wondering, what is an ARIMA underscore plus model type? What does that mean? I've never heard that before. I'll prompt Gemini in the chat. What is an ARIMA underscore plus model type? What functionalities does it have? It's a model type for time series forecasting, which is good. It's the most computationally expensive but also provides the most accurate results, which is good to know that I'm getting my money's worth using it and that it has these functionalities. Again, this is pretty cool because I don't have to leave the console to Google it. I can see in the results pane the model has been created, and now I want to get a forecast from it so I'm actually going to ask Gemini to help me with that. I'll prompt in the chat how can I get a forecast in SQL from my model. Gemini gives me a query, so I'll copy it into my editor and I want to change this a little bit because I want my horizon to be seven, and I actually want confidence level to be 0.95. Hit "Run". Voila, the query results pop up here for the time series forecasting model. Now I have a basic forecasting model that I can query and share the results with my team. If I want to, I can go even further to visualize the results in Looker Studio but I'll save that for another time. I went from data ingestion to understanding, analysis, and forecasting in about 15-20 minutes. The best part, I didn't have to leave the console once to look anything up. That's the power of Gemini.

### Lab - [Gemini for Data Scientists](https://www.cloudskillsboost.google/course_templates/879/labs/476690)

This lab shows you how to use Gemini, an AI-powered collaborator in Google Cloud, to create, visualize and summarize a k-means clustering model with ecommerce data to generate useful next steps for a marketing campaign. This lab is intended for data scientists of any experience level.

* [ ] [Gemini for Data Scientists](../labs/Gemini-for-Data-Scientists.md)

### Video - [Designing an LLM connected model with Gemini](https://www.cloudskillsboost.google/course_templates/879/video/476691)

* [YouTube: Designing an LLM connected model with Gemini](https://www.youtube.com/watch?v=Wek8P-M1Wjw)

I'm a data scientist for an e-commerce company, and I've been asked by my management to help the marketing team develop a group of target audiences for their next campaign. They've requested that customers be split into five groups based on order behaviors, and for some, descriptive statistics about each group. To do this, I'll create and summarize a k-means clustering model, and even create a visualization for it using the available e-commerce data we have. Additionally, I'd like to take it one step further and see if I can provide the marketing team with some actionable next steps to take with each of these groups to streamline and speed up their work efforts a bit. Using Gemini, I'm confident that I'll be able to do all this and take that extra step without too much additional time needed from IAM. I'll need to use BigQuery Studio to complete my task. First, I need to enable it. Note, I have already enabled the Cloud AI companion for Gemini, and the necessary IAM roles for my account. Starting in the Google Cloud console, I'll go to the BigQuery page, and I can see in the lower left-hand corner, there's an option that says new code-management features of BigQuery Studio in preview. Note that you may or may not see this depending on when you're accessing BigQuery Studio, as it may be out of preview by the time that you check it out. I'll click "Enable Now". A list of required APIs will appear, and I'll click "Enable All". Skip the permission section and enable all of the additional APIs as well. Once complete, I will close the enable features pop-up. Back in BigQuery Studio, at the top of the page, I'll click this dropdown arrow and select "Create Python Notebook". You'll have to choose a region to store your code assets from the dropdown and click "Select". A new sample notebook will pop up with some example cells, which you can check out if you want, but I'm actually going to delete them so I can use this notebook for my task. To do this, I can click the garbage can icon that appears when I hover over each cell. Now, I'll need to connect to a runtime. In the upper right-hand corner of the notebook, I'll click the dropdown and select "Connect to a runtime", then select "Create a new runtime", click "Create Default runtime". That can take a little while if it's your first time connecting to a runtime, or if it's been more than a few hours since you last connected. If it's taking some time, I can still move on to the next step, which is creating a BigQuery dataset. To do that, I can click the three dots next to my project in the explorer pane and select "Create data set". I'll name the dataset ecommerce and select the multi-region location type, and US, multiple regions in the United States. Click "Create Data Set", and I can see in the explorer pane that my dataset has been created. Now, back in the notebook, I'm going to start pasting in some code that I have prepared to import the necessary libraries, define the variables I use in the upcoming steps, and initiate the connection for BigQuery and Vertex AI. To add code into the notebook, I'll click this + Code button that appears when I hover at the top of the window, and the first set of code I'll paste in is to import the necessary libraries. Next, I'll paste in some code to define the variables like dataset name, model name, and table name, and to initiate the BigQuery and Vertex AI connection. Now my runtime is connected, which I know because I can see this green checkmark in the upper corner here, so I can run these code cells by clicking the play button to the left of each cell. I can click multiple at once, and it'll just queue them up. My initial steps are complete, so next I can create and import a base table as a BigQuery DataFrame. This table will serve as my base table moving forward, and the Gemini magic here will allow me to run BigQuery SQL right from a notebook cell. Again, I'll create a new code cell and paste in my code and run it. Now I'll use the generate AI feature by adding a new code cell and clicking "Generate with AI". I want to create a BigQuery DataFrame with the table I just created and have it show me the top 10 records, and all I have to do is insert a natural language prompt with a table I want referenced and hit "Enter". Here we go. It's generated some code for me to use. This looks right, so I'm going to go ahead and run the code. Boom, I can see the DataFrame output displayed right here in my notebook. Now that I was able to get my customer data in a BigQuery DataFrame, I can create a k-means clustering model to split the customer data into clusters based on different variables, like recency, order count, and spend, and then visualize the data. In a new code cell, I'll use the generate with AI feature again and enter a prompt asking for a few things; first, for the DataFrame to be split into test and training data for a k-means clustering algorithm, second, to create a k-means clustering model with five clusters, and lastly, ask to save the model to BigQuery. Press "Enter" and see what it gives me. Great. It popped out some code, and it has the steps that I want with what looks like the right code, so I'll run it. Now my model should be created. If I refresh the content of my explorer panel by clicking the three dots and refresh content, the model should pop up under my ecommerce dataset. There it is, customer_segmentation_model under the ecommerce dataset. The next step is to call the k-means model of the df DataFrame and store the results as predictions_df DataFrame. We'll use the generate with AI feature and enter a prompt to create a new DataFrame. Again, some nice code output. Looks good, so I'll run it to show me the top 10 records again, like I did before. I can see here in the table that the cluster column has been added. Now we can create a basic visualization to see what the data looks like. I'll go for a scatter plot using the predictions DataFrame I created to look at the relationship between the days since last order by average spend and colored by cluster, which was just generated using our k-means model. I'll prompt generate with AI to generate a scatter plot, and I'll tell it what to put on each axis, as well as a title, attribute grouped by k-means cluster. It gives me the output. I'll double-check that everything looks good , and run the code. There it is, the scatter plot, visual attributes grouped by k-means cluster with average spend on the y-axis and days since last order on the x-axis. It's pretty awesome that I can split my data into clusters and visualize it right here inside this Python notebook in BigQuery Studio. But let's take it that one step further and use the magic of LLMs to really help the marketing team with their campaign. We can use the cluster information to ask the LLM for help generating insights on each cluster. To do this, first we'll need to summarize the clusters so we can see what they look like. In a new code cell, I'll paste this code that defines a new DataFrame, which displays summary statistics for each of the clusters, and I'll run it. Already, just from looking at the results here, I can immediately glean some insights from the table. I have five clusters, and I can see some clusters have a higher spend while others have a higher count of orders and more. This is great information, but it's a lot of metrics and cluster info. I'm going to use some code to convert the DataFrame into a string so that I can pass it into the LLM call. I'll run this. It'll give me this output that is a string format and is called cluster_info as I defined in my input. Now, I can take this cluster_info and use it to define a prompt so that the LLM, text-bison, knows what I'm looking for. To define this prompt, I'll paste in some code that says this information will be referred to as prompt, and then what I want from the model. In this case, I'll say it's a creative brand strategist, and given the following clusters, it should come up with a persona, title, and next marketing action for each cluster using that cluster_info that was previously created. Then I'll run it. There won't be an output, because I'm just defining the variable called prompt that I'll use to call the LLM. So far, we've created a k-means model, assigned each cluster a cluster from the model, generated summary statistics for each cluster, and defined a prompt. With my prompt defined, I can now call the text-bison model to generate customer insights and next steps for the marketing campaign. Using generate with AI, I'll enter a prompt to call the text-bison model and have it generate a marketing campaign using the variable prompt that I just defined. It suggests code to create the model and the desired response. I'll run it. Awesome. This is really cool. There's information about each cluster, title, persona, and next marketing step for each cluster, like a personalized email or a special offer, and it's in a format that is easily sharable with the marketing team. Normally, it would take hours of manual work to gather customized information, accounting for taste, spend, and by frequency as a data professional. Now, I was able to do this in minutes by combining generative AI with the data in BigQuery.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/879/quizzes/476692)

#### Quiz 1.

> [!important]
> **Which Vertex AI model can be used to identify customer clusters or segments?**
>
> * [ ] K-means clustering model
> * [ ] Text-bison (PaLM 2 for Text)
> * [ ] Multmodalembedding (Embeddings for Multimodal)
> * [ ] Chat-bison (PaLM 2 for Chat)

#### Quiz 2.

> [!important]
> **How can you use Gemini in BigQuery to analyze data? Select three.**
>
> * [ ] Complete a SQL query.
> * [ ] Generate SQL queries to analyze your data from a description you write in English.
> * [ ] Create a dataset in BigQuery.
> * [ ] Schedule queries.
> * [ ] Explain how a SQL query works.
> * [ ] Grant access to a dataset.

### Document - [Next Steps](https://www.cloudskillsboost.google/course_templates/879/documents/476693)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
