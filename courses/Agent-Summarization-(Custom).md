---
id: 1102
name: 'Agent Summarization (Custom)'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1102
date_published: 2024-07-10
topics:
  - Large Language Models
  - Data Preparation
  - Machine Learning
---

# [Agent Summarization (Custom)](https://www.cloudskillsboost.google/course_templates/1102)

**Description:**

In this course you will learn how Contact Center AI Agent Assist can help distill complex customer interactions into concise and clear summaries. 

**Objectives:**

* Understand conversation summarization in a nutshell.
* Learn how to evaluate the required steps to prepare data.
* Understand how to Identify the best practices for reviewing steps for labelling data.
* Learn the important model details and steps for building a model.
* Develop a clear understanding of model deployment.
* Understand the key metrics to evaluate a model.
* Understand the common issues with your model and identify ways for improving it.
* Learn LLM with its custom sections feature.

## Conversation Summarization in a Nutshell

After completing this module, you will be able to explain conversation summarization for Agent Assist.

### Video - [Conversation summarization in a nutshell](https://www.cloudskillsboost.google/course_templates/1102/video/491461)

* [YouTube: Conversation summarization in a nutshell](https://www.youtube.com/watch?v=ED8wD6c6Ym0)

Welcome to the Agent Summarization Customer User Journey? Today, we're diving into a really exciting topic-Agent Summarization, specifically tailored to your customers' needs. Now, you might be wondering, "What exactly is Agent Summarization?" In simple terms, it's a powerful tool we use to distill complex customer interactions into concise, clear summaries. Think of it as a way to capture the essence of a conversation without losing key details. Why is this important? Well, in our fast-paced world, efficiency is key. With custom agent summarization, we're not just saving time; we're enhancing our understanding of customer needs and improving our responses. Let's look at Agent summarization in more detail. Let's review our roadmap for today's session at the CCAI Academy. First up, we'll break down 'Conversation Summarization in a Nutshell. This is where we'll lay the groundwork and understand the basics. Next, we'll move onto 'Data Preparation'. Think of this as the kitchen prep before cooking a meal. We'll see how the right ingredients - or in our case, data -  are essential for a successful outcome. Then, it's 'Data Labeling' time. Much like categorizing ingredients, we'll explore how labeling our data correctly sets us up for success. Following that, we step into the world of 'Model Building'. This is where we start cooking! We'll learn how all our prep works work leads to the creation of a robust (C- What model ?) model. After our model is built, we'll move into 'Model Evaluation'. Just like tasting a dish to ensure it's just right, we must evaluate our model to make sure it meets our standards. Once we're satisfied, we'll discuss 'Model Deployment'- serving our dish to the world, or in our case, implementing our model in real-world scenarios. 'Model Improvement' is next We'll look at ways to refine and enhance our model, much like tweaking a recipe to perfection. And finally, we'll wrap up with 'Custom LLM Summarization', where we tailor our conversation summarization model to fit our unique needs perfectly. Let's get started! Let's begin by examining the basics of conversation summarization. First, let's review "How it works". Picture a state-of-the-art abstract summarization model. It delves deep, understanding the context and nuances of conversations to create summaries that really capture their essence. Now, let's explore "What it does." Imagine having the ability to condense lengthy dialogues into accurate, concise narratives. That's our API's superpower. It's like having an expert storyteller who can distill a lengthy conversation into a brief, meaningful summary. And the exciting part is "Use Cases." Think about how this can transform your workflow. For agents, it suggests a summary draft right at the end of a conversation, drastically reducing the effort and time needed to craft a well-formatted summary. It's like having a helpful assistant who's always one step ahead. For returning customers, this API means agents can quickly review past conversations, ensuring continuity and personalization in customer service. And for QA and managers? It's a game-changer. They can efficiently monitor and review service quality, ensuring that every customer interaction meets our high standards. Let's walk through the essential steps to get you up and running: First it all begins with 'Preparing and Uploading Training Data'. Think of this as laying the foundation. We'll use the conversation dataset service to upload our data. Next up is 'Configuring a Conversation Profile with Agent Assist. This is where you set the parameters. It's all about tailoring the system to understand and respond to your specific needs. Then comes 'Integrating Your Runtime with Agent Assist APIs. Whether it's chat or voice, this step ensures that your user interface speaks seamlessly with the system. It's like connecting the dots to ensure a smooth conversation flow. Now, the exciting part - 'Getting Summarizations at Runtime via API'. This is where you get real-time summaries of your conversations, a tool that can transform how you interact and respond. Lastly, we have 'Submitting Feedback Through Answer Record Service'. Your input is invaluable. This step ensures continuous improvement and adaptation to your needs.

## Data Preparation

After completing this module, you will be able to describe and evaluate the steps required to prepare data in conversation summarization for Agent Assist.

### Video - [Data preparation](https://www.cloudskillsboost.google/course_templates/1102/video/491462)

* [YouTube: Data preparation](https://www.youtube.com/watch?v=c3ZjeiX0a-s)

Next, let's have a look at Data preparation. Let's begin with a review of the data preparation steps. Data preparation is a crucial stage of our journey. This step is where we set the stage for effective Conversation Summarization. First off, it's about 'Acquiring Customer Conversation Data'. We're aiming for 2,000 conversations with high-quality summaries. But, if that sounds like a tall order, don't worry. Starting with 1,000 conversations can also set us on the right path. Next, we step into 'Labeling Data'. Here we're not just labeling; we're crafting art. We ensure our summaries are concise, accurate, and written in proper English. It's like fine-tuning a precision instrument. Remember, a bit of noise in the source conversation is okay. Then comes 'Reviewing Labeled Data'. This isn't just a review; it's a quality check. We need our text conversation data in JSON- formatted files, each referring to a single conversation. Finally, 'Conversion to TF Record and JSON Conversations'. We then store this prepared data in Google Cloud buckets, ensuring it's ready for our next steps. With our data prepared and primed, we're all set to begin building. You'll find the links to best practices and a jupyter notebook for sample code in the additional resources document.

### Quiz - [Data Preparation Quiz](https://www.cloudskillsboost.google/course_templates/1102/quizzes/491463)

## Data Labelling

This module explores the steps involved in data labelling for the creation of impactful summaries through conversation summarization in Agent Assist.

### Video - [Data labelling](https://www.cloudskillsboost.google/course_templates/1102/video/491464)

* [YouTube: Data labelling](https://www.youtube.com/watch?v=nLvn08J_hwM)

Ok, now let's review the steps to label data. Before we dive into writing those impactful summaries, let's align on some best practices to ensure we're creating something truly valuable. Summary Sections First, let's talk about 'Summary Sections'. What should a good summary contain? Think of your summary like a story with up to 3 sections - for example, Situation, Action, and Outcome. It's about crafting a narrative that's informative and to the point. Vocabulary Next up is 'Vocabulary'. It's crucial to use the right language. If you have a document with preferred vocabulary, use it as your guide. Let's say we're using the same model for voice and messaging. In that case, a neutral term like 'conversation' is more fitting than 'call'. Pre-defined sentences for Common Events Then 'Pre-defined Sentences for Common Events'. Consistency is key here. Using the same sentences for frequent events not only reduces errors and typos but also ensures a more consistent model output. For instance, for resolved issues, have a standard phrase like 'Issue resolved with no further steps needed' or Resolved without customer confirmation' when the customer leaves before acknowledging the solution. PII Data And finally, 'PII Data'. Be vigilant about not including sensitive Personal Identifiable Information like names or personal identifiers in summaries. However, less sensitive data, like account-related dates, might be necessary. For example, including a new due date for a payment extension is crucial. Our DLP system is smart enough to differentiate between sensitive dates, like birth dates, and non-sensitive ones. Level of details Continuing with our best practices for writing top-notch summaries, let's move onto 'Level of Details' and 'Conversation Extract'. Starting with 'Level of Details', it's about getting the granularity just right for each use case. For instance, in a payment extension scenario, outline the situation briefly, like 'Customer requested a payment extension'. Then specify the action, such as 'Extended by [number of days] until [date]'. It's like painting a picture with just enough detail to convey the scene clearly. Conversation extract Now, onto 'Conversation Extract'. To enhance our model's quality, variety is key. We recommend pulling random conversations from a broad time frame, say 6 months to a year. This ensures we capture diverse scenarios, like a major outage or underrepresented events. Also, think about the types of conversations you need. Are they messaging, voice, or bot- contained? Are they from specific areas like sales or the financial department? Lastly, don't forget to create a comprehensive document for your labelers. This should include all instructions, clear guidelines, and examples. It's like giving them a map for this journey, ensuring they can navigate through the data labeling process efficiently and accurately. As we refine our approach to writing impactful summaries, let's focus on three key areas: sentence structure, pronoun usage, and the process of continuous improvement. Firstly, 'Make Sentences Short'. Remember, our goal is clarity and brevity. Short sentences are powerful and straightforward. Avoid the passive voice and don't repeat information across sections. For example, instead of saying, "Answers have been provided by an expert to the customer," simplify it to "Provided customer answers". It's like distilling the essence of our message into its most potent form. Next, 'Use Gender Neutral Pronouns'. In our summaries, we steer clear of guessing a customer's gender. It's not just about accuracy; it's about respecting privacy and focusing on what truly adds value to our summaries. Finally, 'Continuous Improvement'. This is a journey, not a destination. We recommend organizing weekly feedback sessions after reviewing the conversations and associated summaries written by our labelers. It's like fine-tuning a musical instrument- the more we practice and adjust, the sweeter the melody. As we continue to refine our approach to creating effective summaries, let's focus on the crucial aspects of 'Data Source' and 'Annotation Tasks'. First up, 'Data Source'. Envision a spreadsheet, a canvas where each row represents an utterance. This spreadsheet will not only detail the content of each utterance but also the speaker's role and the transcript_id it belongs to. It's organized meticulously, with utterances from a sample transcript sorted in sequence, all linked by a common transcript_id. This organization is key to understanding the flow and context of each conversation. Now let's talk about 'Annotation Tasks'. In a separate tab of the same spreadsheet, your task is to compose summaries. Each row here corresponds to a transcript_id, linking back to the source conversation. Your challenge is to distill these conversations into summaries that are both informative and concise. It's like extracting the essence of a conversation, capturing all the important information in a clear and succinct manner. Remember, the art of summarization lies in balancing detail with brevity. As you work through these annotation tasks, think of yourself as an artist, crafting a miniature but complete portrait of each conversation. As we delve deeper into crafting effective summaries, let's focus on the format, which is divided into four key sections: Situation, Action, Outcome, and Next Steps. First is 'Situation'. This is where you describe the customer's issue. Note the main problems the customer presents. For example, "The customer wanted to cancel an order and check account balances". It's important to capture all major issues but leave out the sub-questions. Our goal is to highlight the core of the customer's concern. Next is 'Action'. This section is about what the agent did to help the customer. Sometimes, like in our Situation section, there might be multiple actions. For instance, "Canceed the order and provided detailed account information". Remember, we're focusing on main actions, not the subactions. The third section is 'Outcome'. Here, we describe what outcome resulted from the agent's actions. Keep it simple and clear, like "Issue Resolved" or "Escalated to Supervisor". If the customer left prematurely, note that as well, such as "Customer disconnected". Lastly is 'Next Steps". This is where we outline any follow-up actions suggested by the agent. If the agent proposed a callback, that's what we include here. And if there were no next steps mentioned, simply put "N.A." This structure approach ensures our summaries are not only comprehensive but also clear and focused. This is an example of labeled data with the situation, action and outcome. So for each conversation, in the first row, where the Turn ID is 1, we need to add a manually written summary in the desired section. For example, Situation, Action and outcome, in separate columns. This is an example of the template for preparation within sheets. Now, let's look at the nuts and bolts of how our summarization process works with actual conversation data. It's like looking under the hood of a car to understand how the engine runs. Take a look at this example. You can see a series of entries, each capturing a slice of the conversation- some from the agent, some from the customer. Like "I cannot login," from a customer, followed by the agent's response. It's a back-and-forth dance each step documented. Now let's look at the 'Conversation Info' section. Based on our conversation, we create annotations that neatly fit into our summary format: Situation, Action, and Outcome. For instance, the 'Situation' here is "Customer was able to login to account". It's straightforward capturing the customer's issue. Then, the 'Action' taken is "Agent sent an email with password reset instructions". And finally, the 'Outcome' is "Problem was resolved". This structured approach to summarization ensures that we're not just capturing words but also the essence and resolution of each conversation. It's about turning these dialogues into insightful, concise narratives that provide clear and actionable information.

### Quiz - [Data Labeling Quiz](https://www.cloudskillsboost.google/course_templates/1102/quizzes/491465)

## Building a Custom Model

This module explores the steps involved in building a conversation summarization model and how to access it via the Agent Assist UI console.

### Video - [Building a custom model](https://www.cloudskillsboost.google/course_templates/1102/video/491466)

* [YouTube: Building a custom model](https://www.youtube.com/watch?v=uuJlfyGPdXk)

Next, let’s explore how to build a custom model. In the next section we will: Understand the important model details Identify the steps for building a model And lastly, learn how to use the Agent Assist UI console Let's shift gears and explore the technical backbone of our conversation summarization model. Understanding the model's architecture and training process is crucial for harnessing its full potential. Model architecture Firstly, the 'Model Architecture.' We're using LongT5, a robust and sophisticated model designed for handling complex tasks like ours. Max Input Length Regarding 'Max Input Length,' our model can process up to 4,000 tokens. This means it can handle quite extensive conversations, ensuring no detail is missed. Training Data Best Practices Next, let's talk about 'Training Data Best Practices.' There are some key guidelines here: Keep inputs under 4,000 tokens and outputs under 120. Ensure there's no PII (Personal Identifiable Information) or toxicity in the summaries. And importantly, training summaries should only be sourced from actual conversations to maintain authenticity. Training data distribution Another crucial aspect is 'Training Data Distribution.' It should mirror live traffic as closely as possible. This ensures the model is well-acquainted with the kind of data it will encounter in real-world scenarios. For those using voice data, 'Check STT Quality.' It's important to ensure the speech-to-text conversion is accurate, as this forms the basis of our training data. Quantity And quantity matters too! Over 1,000 training examples is recommended for good quality. It's like seasoning a dish just right – the more balanced, the better the taste. Summary Writing Lastly, remember, 'Summary Writing Rules' apply across the board. Consistency in how summaries are written is key to creating training a model that performs reliably and accurately. So, with these details in mind, we're not just building a model; we're crafting a sophisticated tool that understands and summarizes conversations with precision and insight. Embarking on building a custom model is like setting off on an exciting expedition. Each step is crucial to reaching our destination – which is, “a finely-tuned, custom conversation summarization model”. Let's break down these steps. First, 'Create GCP project.' owned by the customer. Next, Privacy is paramount, 'Redact PII data'. Now, 'Acquire conversations.' Aim for 1000 to 2,000 conversations with high-quality summaries. Store them in a GCS bucket. Remember, the more conversations, the richer the data pool. Then, 'Label data using best practices.' After labeling, you must 'Review the data’ to ensure that it meets requirements. Then copy the data into approved Google storage with security and IAM control. Next, 'Train the Model and internal evaluation.' Finally, 'Evaluate the output.' This could be done by both Customer and Google. With these steps, we're not just building a model; we're crafting a tailored tool that understands and summarizes your specific conversational nuances. Now that we've prepared our training data, it's time to bring it to life through the Agent Assist UI Console. 'Point to the Cloud Storage path in the Agent Assist console.' This is where you tell the system where to find its training material. Once you've set the path, the Agent Assist console becomes your control center for custom model training. The upcoming slides are your guidebook. They'll take you step-by-step through this process, complete with screenshots, to ensure that training and deploying your model in your dev environment is as smooth as possible. Remember, each step is integral to the success of your model. This is the UI for the summarization to create a custom model On this screen, we choose whether the conversation type is Chat or Voice, and add a Model name along with the language. Here we choose the source where our labelled data is present. Labelled data here refers to the conversation transcripts along with the labelled summaries in a JSON format we discussed earlier. We can also choose the option to try the sample data here. This page is for creating the annotated dataset for the summary model if the data is present in Cloud Storage. Here we train the model by clicking the “Begin training” button. We can choose to provide an evaluation data here. The file format of evaluation data would be the same as that of training data discussed earlier. The evaluation is done using the metric “ROUGE-L” (Recall-Oriented Understudy for Gisting Evaluation). It measures the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans and generates a score. Usually, “ROUGE-L”>40% is considered good for a custom summarization model. Once the model is finalized, assign the model to a conversation profile. Here we provide a conversation profile name to link the summarization model. The conversation profile name can be copied by clicking the three dots in front of the newly trained model and clicking ‘Copy Profile ID’.

### Quiz - [Building a Custom Model Quiz](https://www.cloudskillsboost.google/course_templates/1102/quizzes/491467)

## Model Deployment

This module explores the requirements for the deployment of a conversation summarization model in Agent Assist.

### Video - [Model Deployment](https://www.cloudskillsboost.google/course_templates/1102/video/491468)

* [YouTube: Model Deployment](https://www.youtube.com/watch?v=qlqVNh2Ws9Y)

Next step is the model deployment. Once the custom model has been trained, we need to deploy the model in a client environment. So let's learn how to do that in the next section. As we navigate the final stages of our model deployment, it's important to understand that this process is a trilogy of critical steps. Each step is essential, like pieces of a puzzle that come together to complete the picture. First there's 'Model Creation'. Once trained and evaluated in the dev environment, copy the mode to the Client Tenant Environment. Here, it becomes accessible in the client's Agent Assist console. You can use the 'From Saved Model' section of the Model Management Colab for this. Next up, 'Deploying the Model', This can be done either directly through the UI console or via backend using the Model Management Colab. Finally 'Linking the Model to the Conversation Profile', For our model to be truly effective, it needs to be integrated into the conversation ecosystem. This is achieved by linking it to a new or existing conversation profile through the Agent Assist console. The integration id of this profile is then used to call the AA APIs. It's the final step in making sure our model is not just functional but also fully integrated and operational. Remember, each of these steps is critical to ensuring that our model is not only deployed successfully but also optimized for performance in the real world.

### Quiz - [Model Deployment Quiz](https://www.cloudskillsboost.google/course_templates/1102/quizzes/491469)

## Model Evaluation

This module explores the key steps for the evaluation of a custom trained model in Agent Assist.

### Video - [Model evaluation](https://www.cloudskillsboost.google/course_templates/1102/video/491470)

* [YouTube: Model evaluation](https://www.youtube.com/watch?v=zAQTFJ101Z0)

In this section, we are going to focus on how the custom trained model can be evaluated. The objective is to give you an understanding of how to identify the key metrics that define the success of our conversation summaries. Understanding these metrics is like having a compass that guides us towards creating high-quality, effective summaries. There are four key metrics that you always want to keep in mind: Accuracy Completeness Compliance Grammar and Spelling First let's dive into 'Accuracy'. This metric is all about truthfulness. Does the summary accurately reflect the conversation? It's scored on a scale from 1 to 5, with 1 being "Not linked at all to the conversation", and 5 meaning "Every detail is true". Next is 'Completeness'. Here, we can assess if any important details were missed in the summary. Again, we use a 1 to 5 scale. A score of 1 means all details were missed, while a 5 indicates a comprehensive summary. Then there's 'Compliance'. This measures adherence to writing rules. The scoring ranges from 1, indicating complete non-compliance, to 5 which is perfect adherence. Lastly, we have 'Grammar and Spelling'. This is about the linguistic quality of the summary. Scores range from 1 nonsensical sentences, to 5, perfect grammar and spelling. It's the polish that makes the summary not just readable but professional. Remember these metrics are guidelines that ensure our summaries are not just coherent, but also respectful, unbiased, and informative. They help us maintain a high standard of quality in every summary we create. This is an example of what the results can look like.

### Quiz - [Model Evaluation Quiz](https://www.cloudskillsboost.google/course_templates/1102/quizzes/491471)

## Model Improvement

This module explores ways to continuously improve a custom trained model built in Agent Assist and identify common issues within the model deployment process.

### Video - [Model improvement](https://www.cloudskillsboost.google/course_templates/1102/video/491472)

* [YouTube: Model improvement](https://www.youtube.com/watch?v=fyoB4dRQFQs)

Once the model has been deployed and is in use, we need to think about how we can improve it over time. So let's talk about the Model Improvement. In this section we are going to: Learn how to continuously improve your model Help you identify common issues and troubleshoot Continuous improvement is like gardening; regular care and attention ensures healthy growth. Firstly, let's talk about using agent-edited summaries. If agents' edits align with the rules of our training dataset, their summaries can be a goldmine for retraining. However, if there's uncertainty about the quality of the agents' edits, then a more analytical approach is needed. Start by identifying areas where the model underperforms. Is it a specific agent skill or a certain topic? Think of this as diagnosing the problem areas. Then, dive into the data. Extract conversations where summaries were significantly edited, as indicated by the ROUGE score. It's detective work, pinpointing exactly where changes were made. The next step is rewriting. Create 'perfect' summaries for these conversations, that ideally reflect both the content and the spirit of the interaction. Finally, merge these new summaries with your previous training dataset and retain your model. Continuous improvement is about evolution, not revolution. It's a process of regular, incremental changes that cumulatively lead to significant enhancements in your model's performance. In our journey of building and refining the conversation summarization model, we've encountered a few challenges along the way. It's important to recognize these hurdles as opportunities to learn and improve. Firstly, the 'DLP Template'. This is crucial. Remember, summarization happens after message redaction. It means any redacted information can't appear in your summaries. Next, the 'Token Limit'. Our model considers up to 2048 tokens for summaries. So, for longer conversations, anything beyond this limit is ignored. Another issue is 'Incorporating Bot Conversations'. You'll want to include the entire conversation in the summarization, including the bot's part. Linking the bot and human agent parts can be tricky, but it's essential for a complete understanding. Then, there's 'Feedback and Reporting'. Although customers send feedback through the API, there isn't an automatic report to track KPIs like the percentage of summaries edited. 'Changing Agent Habits' is also a hurdle. Some agents might add extra information, like a conversation ID, to the summary, which can increase the edit rate. Lastly, a rare but notable issue is 'Model Repetition'. Occasionally, the model may start repeating itself. However, with the advent of T5-based models, this behavior has become less common. Each of these challenges teaches us something valuable. Post-deployment of our conversation summarization model, we enter the critical phase of ensuring the model's effectiveness and refining it. Firstly, 'Evaluation'. Remember, evaluating a summary's quality isn't straightforward Unlike math problems, there isn't just one correct answer. Multiple summaries can accurately reflect the same conversation. It's like interpreting a poem - there can be several valid interpretations. For example, "Answers have been provided by an expert to the "customer" can be summarized as "Provided customer answers" or "Checked and provided information" simplified to "Provided information". It's about capturing the essence in fewer words. Next, 'Identifying New Data for Model Iteration'. Caution is advised when using agent feedback for new datasets as this feedback can impact the model negatively. Agents some times add extra information, reference numbers, or even Private Identifiable Information, which shouldn't be in the summary. Also, they might make grammar mistakes, use abbreviations, or omit crucial details, like a promised credit to a customer. Therefore, the best practice is to extract and review summaries edited by agents. Have a labeler or a subject matter expert (SME) verify the legitimacy of these edits. If necessary, rewrite them to adhere to established guidelines. It's like curating content and ensuring it's accurate, relevant, and follows the rules. This phase is about striking a balance between leveraging agent input and maintaining the quality and integrity of our summaries. It's a continuous process of learning, adapting, and improving.

### Quiz - [Model Improvement Quiz](https://www.cloudskillsboost.google/course_templates/1102/quizzes/491473)

## Custom LLM Summarization

After completing this module, you will be able to use the custom sections feature of LLM Summarization.

### Video - [Custom LLM Summarization](https://www.cloudskillsboost.google/course_templates/1102/video/491474)

* [YouTube: Custom LLM Summarization](https://www.youtube.com/watch?v=ZqcnGcIE-uw)

Finally lets review Custom LLM Summarization. In this section, you’ll review the LLM summarization with custom sections feature. You also have the power to add custom sections to your summaries. Moreover, custom LLM Summarization allows you to personalize any summary section based on text instructions. Imagine defining the tone of the text, or even replacing terms like "agent" with "advocate." It's a level of customization that allows your summaries to truly reflect your organization's voice and style. And there's more. You can provide sample summaries to help the model learn the format and structure of your ideal summaries. To bring all this to life, there's a full demo video available. With custom LLM Summarization, the power to shape our summaries is in our hands.

### Quiz - [Custom LLM Summarization Quiz](https://www.cloudskillsboost.google/course_templates/1102/quizzes/491475)

## Additional Resources

This module includes the list of additional resources that complement the course learning.

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/1102/documents/491476)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
