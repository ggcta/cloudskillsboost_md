---
id: 72
name: 'Preparing for your Professional Data Engineer Journey'
datePublished: 2024-12-13
topics:
- Data Pipelines
- Machine Learning
- Data Engineering
type: Course
url: https://www.cloudskillsboost.google/course_templates/72
---

# [Preparing for your Professional Data Engineer Journey](https://www.cloudskillsboost.google/course_templates/72)

**Description:**

This course helps learners create a study plan for the PDE (Professional Data Engineer) certification exam. Learners explore the breadth and scope of the domains covered in the exam. Learners assess their exam readiness and create their individual study plan.

**Objectives:**

- List the domains covered on the Professional Data Engineer (PDE) certification exam.
- Identify gaps in your knowledge and skills for each domain.

## Introduction to the Professional Data Engineer (PDE) Certification

Welcome to Preparing for the Professional Cloud Data Engineer Journey. In this introductory module, you learn about the role of a Professional Data Engineer. We discuss the types of resources available to support your study, and how to use the workbook to create your study plan. 

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/72/video/517249)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=wtWPqyZr7oE)

Are you ready to start preparing for the Professional Data Engineer certification exam? In this course, you will assess your exam readiness, and form a personalized workbook that will guide you through your certification journey. Hi, I’m Ajay, a Technical Curriculum Developer at Google Cloud. I’ll introduce you to the Data Engineer certification, and how to prepare for it. This content is designed for experienced Data Engineers that are ready to become Google Cloud certified. Before taking this course, you should have experience designing and managing data solutions using Google Cloud. In this course, you’ll be exposed to and engage with exam topics through a series of lectures and quizzes. Upon completion of this course you’ll understand the domains covered on the exam, And you’ll be equipped to identify gaps in your knowledge and skills for each domain. Enroll in this course to prepare to become a certified Google Cloud Data Engineer.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/72/video/517250)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=fQL7gNS0F2A)

Welcome to Preparing for Your Professional Data Engineer Journey. In this course, you learn about the skills covered on the Professional Data Engineer certification exam. Modules 1 through 5 each point to one section of the exam guide. In Module 6, you prepare for the next steps in your journey and create a study plan. However, it’s important to clarify that this course by itself will not prepare you to take the certification exam. This course is not a “cram session.” The exam is purposefully calibrated to test your ability to apply the knowledge required of a Professional Data Engineer, not merely repeat it. Cram sessions have minimal impact on your ability to pass the exam. Instead, the goal of this course is to help you better structure your preparation time for the exam. You learn about the scope of each exam section and assess your current knowledge and skills using diagnostic questions. You then review where to find additional tools and resources to include in your study plan. In this introductory module, you learn about the role of a Professional Data Engineer. We discuss the types of resources available to support your study, and how to use the workbook to create your study plan.

### Video - [Introduction to the Professional Data Engineer role](https://www.cloudskillsboost.google/course_templates/72/video/517251)

- [YouTube: Introduction to the Professional Data Engineer role](https://www.youtube.com/watch?v=FFRC0v1WXO8)

So… what exactly is the role of a Professional Data Engineer? Let’s review the job role description: A Professional Data Engineer makes data usable and valuable for others by collecting, transforming, and publishing data. This individual evaluates and selects products and services to meet business and regulatory requirements. A Professional Data Engineer creates and manages robust data processing systems. This includes the ability to design, build, deploy, monitor, maintain, and secure data processing workloads. For more information, visit the Professional Data Engineer Certification page. In this course, you put yourself in the role of a Professional Data Engineer working for Cymbal Retail, a fictional company. Cymbal Retail is amongst the largest B2C retailers in the world serving customers in 52 countries and territories. The founders started with brick and mortar stores and had the foresight to continuously invest in technologies over many decades. Cymbal Retail has seen significant changes in purchasing behavior, with accelerated growth in online purchases in recent years even as they continue to grow their brick and mortar presence. Let’s explore some of what your role will involve at Cymbal Retail. Cymbal Retail has multiple data centers of their own across the world. Cymbal Retail is in the process of transitioning these data centers to being entirely on Google Cloud, with a significant portion of their data and applications already moved. When migrating and operating, as a Professional Data Engineer you will need to ensure that Cymbal Retail complies with all local regulations. You will also need knowledge of security around data and the tools available for compliance. Customer delight has been the guiding purpose for Cymbal Retail through generations. The company has made multiple acquisitions across the world to serve customers with agility. As a Professional Data Engineer, you must be able to integrate products and solutions available within Google Cloud with technologies employed by the acquired company. You have four key technology areas of focus that drive your short term technology strategy: rapidity in the entire supply chain, accuracy of analytics and predictions, centralized control and security, and controlled costs. Throughout every decision, the data engineering team has to find the right balance to meet the business needs. Cymbal Retail’s management wants to be on top of all activities across the organization. The management team requires up to date information at their fingertips in easily understandable visuals. They also want the rest of the organization to employ data in their own decision making. Analysts should have the tools to ask questions of the data. It is one of your responsibilities as a Professional Data Engineer to make data available to all parts of the organization that need it. Data should be available quickly and in easily consumable ways. As you continue through this course, you will continue to explore the role of a Professional Data Engineer at Cymbal Retail. We use this scenario to illustrate the types of considerations and tasks that correspond to each section of the exam guide. Cymbal Retail’s business needs also provide context for many of the diagnostic questions you encounter along the way, and highlight areas that you should spend more time learning about.

### Video - [Certification value and benefits](https://www.cloudskillsboost.google/course_templates/72/video/517252)

- [YouTube: Certification value and benefits](https://www.youtube.com/watch?v=Whj0qA30f3w)

Why become a Google Cloud Certified Professional Data Engineer? Certification value has skyrocketed. Becoming Google certified gives you industry recognition. It validates your technical expertise and can be the starting point to for the next phase of your career. You may be curious about what differentiates a “professional” cloud certification from an “associate” level one. The professional level certification expects the exam taker to know how to evaluate case studies and design solutions to meet business requirements—in addition to knowing about technical requirements—for customer implementation.

### Video - [Certification process](https://www.cloudskillsboost.google/course_templates/72/video/517253)

- [YouTube: Certification process](https://www.youtube.com/watch?v=bRYBorPuO00)

As you explore the role of the Professional Data Engineer at Cymbal Healthcare in this course, you also explore different sections of the exam guide which forms the basis for the certification exam. In the following modules, you take diagnostic questions to assess your knowledge of each section of the exam guide. The exam guide for the Professional Data Engineer is divided into five sections, each containing one or more objectives. We focus on where you can find resources at the section objective level. You can find the exam guide on the certification page at cloud.google.com/learn/certification/guides/data-engineer. The certification page will always have the latest available version of the exam guide. It's important to note that there are separate teams developing the Professional Data Engineer exam questions and developing the courses and exam preparation materials. The course developers and instructors don’t know what questions will be on your certification exam. The goal of the course is to determine what you know and what you don’t know to help you prepare a study plan and get ready for the Professional Data Engineer job role and exam. Later, when you take the exam, you will demonstrate whether you have the skills and knowledge required to earn the certification. Throughout this course, you are pointed to specific resources and documentation that can help you fill the gaps you identify through the diagnostic questions. Let’s go over the types of resources you may want to include in your study plan. Google provides resources to help you develop your skills and experience with Google Cloud products and services. The learning path for this certification includes online courses, online practice labs, and practice questions. The courses recommended for the Professional Data Engineer certification can be taken on demand or as instructor-led courses. The instructor-led course Data Engineering on Google Cloud is equivalent to the following series of on demand courses: Modernizing Data Lakes and Data Warehouses with Google Cloud; Building Batch Data Pipelines on Google Cloud; Building Resilient Streaming Analytics Systems on Google Cloud; and Smart Analytics, Machine Learning, and AI on Google Cloud. The instructor-led course Serverless Data Processing with Dataflow is available as an on demand 3-course series: Serverless Data Processing with Dataflow: Foundations; Serverless Data Processing with Dataflow: Develop Pipelines; and Serverless Data Processing with Dataflow: Operations. You learn more about how these courses relate to the sections of the exam guide as you complete the modules in this course. The skill badges provide hands-on experience working in Google Cloud. Skill badges are learning paths made up of labs that give you practice with Google Cloud services or solutions. Pass the challenge lab and you will receive a shareable credential that recognizes your ability to solve real-world problems with your cloud knowledge. As we review the diagnostic questions in this course, you also get recommendations for skill badges to include in your study plan. You can access the Professional Data Engineer Learning Path at www.cloudskillsboost.google/paths/16. Sample questions are another resource that you can use to prepare. The diagnostic questions in this course are designed to help you identify your knowledge gaps. On the certification page, Google provides a set of sample questions that can help you familiarize yourself with the format of the exam questions. Once you complete the question set, you will receive feedback describing the rationale for the correct answers. The sample questions provide a good opportunity to practice taking the type of scenario-based, application-level questions on the exam. The exam questions present you with a scenario, explains the goal or what you are trying to achieve, and asks you what you would do in that situation. Google also supplies official public documentation for its products and services. In each of the following modules, you learn about specific documentation resources to help you study that section in preparation for the exam.

### Video - [Creating your study plan](https://www.cloudskillsboost.google/course_templates/72/video/517254)

- [YouTube: Creating your study plan](https://www.youtube.com/watch?v=sG4KnNODyOQ)

One of the primary goals for this course is to help you devise a study strategy that focuses on the areas you need to work on. Let’s quickly explore how the course is set up. The course - and your course workbook - focuses on each section of the exam guide in turn. To help you craft a study strategy, you take diagnostic questions as part of each module. Many of these questions relate to our Cymbal Retail scenario and ask you to apply concepts you will need to be familiar with as an Professional Data Engineer. You can take these questions using an on-demand quiz or using your workbook. Keep in mind that these diagnostic questions are meant to help you identify gaps in your knowledge, but they don’t represent all possible topics on the exam. Remember, we don’t expect that you answer all these questions correctly right now. This is meant to be a course that you take toward the beginning of your preparation journey, and many of you may not be experts yet. If there are any terms or concepts that are confusing, please ask your instructor. After you individually answer the diagnostic questions in each module, you review the questions and the correct answers to identify what you need to study and where you can find more information. As you review the questions, there are hints with bolded text as to what is important in the question and how you might evaluate the responses. Remember these tips for multiple choice questions: Read the question stem carefully. Make sure you understand exactly what the question is asking. Try to anticipate the correct answer before looking at the options. You should be able to come up with the correct answer just from reading the question stem. You may find that more than one answer may be possible on multiple choice tests. Take questions at face value. If certain details are omitted, then they are unlikely to contribute to the selection of the best answer. Pay attention to qualifiers ("usually", "all", "never", "none") and key words ("the best", "the least", "except"). We review select questions related to each section objective. As we cover each objective, you learn more about where the key concepts appear in Google Cloud documentation, specific courses and modules, or specific skill badges. At the end of each section objective, there is a list of related resources. Mark or highlight the specific resources you need in your study plan. In the final part of your workbook there is a template to help you identify weekly goals and study activities. We discuss more about putting together weekly goals at the end of this course. Now that you know about the overall setup of this course and how to use the workbook, let’s get started by exploring section 1 of the exam guide.

### Document - [Course Workbook](https://www.cloudskillsboost.google/course_templates/72/documents/517255)

## Designing Data Processing Systems

In this module, you'll explore considerations for designing data processing systems, which correspond to section 1 of the Professional Data Engineer Exam Guide. You start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you'll assess your skills in this section through 10 diagnostic questions. Then you'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan. 

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/72/video/517256)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=rZCCM4XKlr0)

Welcome to Module 1: Designing Data Processing Systems In this module, you’ll explore considerations for designing data processing systems, which correspond to section 1 of the Professional Data Engineer Exam Guide. Let’s start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you’ll assess your skills in this section through 10 diagnostic questions. Then you’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

### Video - [Migrating data from private data centers to Google Cloud for Cymbal Retail](https://www.cloudskillsboost.google/course_templates/72/video/517257)

- [YouTube: Migrating data from private data centers to Google Cloud for Cymbal Retail](https://www.youtube.com/watch?v=74tZfSZIQFw)

Let’s begin by exploring the role of a Professional Data Engineer in migrating Cymbal Retail’s data from its private data centers to Google Cloud. Let’s look at the various challenges a Professional Data Engineer will face in accomplishing this task. Cymbal Retail’s existing private data centers are going to be decommissioned in the next two years. You need to help migrate existing data and data processing systems to Google Cloud. Similar migrations have to also be conducted in acquired companies with their own private data centers. As a Professional Data Engineer, your role involves ensuring that there is a uniform approach to managing and securing the data. Controlling access at a granular level based on employee role is critical to complying with regional laws and industry regulations. You need to understand and deploy options to control the security of data via encryption. Whether during migration or in the course of regular operations, certain data has to be captured and stored, but never revealed to other parts of the organization, for example, analysts. You need to employ tools to redact information, and you need be able to extend these tools as required to meet future requirements. You will be required to move data reliably from external sources into Google Cloud. Sometimes, the data movement might be done over many days. In other cases, data needs to be quickly moved as and when it arrives in other data sources for up to date analytics. As data is moved into Google Cloud, it will also need to be assessed for quality and cataloged for easy discoverability by consumers of the data. Cymbal Retail’s data is used simultaneously by different divisions to manage their business—finance, marketing, purchasing, sales, compliance, legal, store operations, and more. However, all of these groups do not need access to all the data. You should ensure fine-grained control on the data which is strictly on a need-to-know basis.

### Video - [Introduction: Diagnostic questions](https://www.cloudskillsboost.google/course_templates/72/video/517258)

- [YouTube: Introduction: Diagnostic questions](https://www.youtube.com/watch?v=w2QMljr6L-M)

It’s your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, these questions are intended to help you understand or diagnose which areas you'll want to focus on in your study plan. So we don't expect you to know all the answers yet. Please complete the diagnostic questions that are presented next. If your learning platform does not support assessments or quizzes, kindly use your Course Workbook.

### Video - [Diagnostic Questions Practice Demo](https://www.cloudskillsboost.google/course_templates/72/video/517259)

- [YouTube: Diagnostic Questions Practice Demo](https://www.youtube.com/watch?v=2mk2U5-PJKw)

The diagnostic questions in this course can be challenging, and some of them have several layers or decisions to make. Let’s examine some approaches you can use when analyzing complicated questions through a detailed discussion of a few diagnostic questions from this first module. Let’s start with question 3. First, consider the question stem. We can break the stem up into multiple parts. The first part of the stem is the scenario, which gives you context for the question: “You are migrating on-premises data to a data warehouse on Google Cloud. This data will be made available to business analysts. Local regulations require that customer information including credit card numbers, phone numbers, and email IDs be captured, but not used in analysis.” In this scenario you learn some key information: Your data includes personally identifiable information (PII), but it cannot be used in analysis. Next, you can identify the goal statement, or what it is you need to accomplish: “You need to use a reliable, recommended solution to redact the sensitive data.” From this goal, you know to look for options that are reliable (accurate and consistent in performance) and follow Google-recommended practices. So, what should you do? Start by noticing patterns in the responses. Two options, A and D, have you use the Cloud Data Loss Prevention (DLP) API. Option B has you manually delete columns, and Option C has you create a regular expression to delete information. You can eliminate option B right away - it is not reliable. Columns might contain sensitive data, even if it is not indicated by the title. For example, there could be a comments column with sensitive private information. You can also eliminate option C. Determining the regular expressions that match different data types can be more tedious and less accurate than using Cloud DLP. This leaves you with the two options using Cloud DLP, and you need to determine the correct usage. Look at the differences between them. In option A, you identify and redact data that matches certain infoTypes. In option B, you perform date shifting of entries. You can eliminate option D, because date shifting the entries does not redact the personal identifiable information (PII) like credit card numbers, phone numbers, and email IDs. This leaves you with option A. Cloud Data Loss Prevention helps you discover, classify, and protect your most sensitive data. There are predefined infoTypes that you can employ to identify and redact specific data types. Now, let’s examine question 4. First, let’s break the stem up into multiple parts. It begins with the scenario: “Your data and applications reside in multiple geographies on Google Cloud. Some regional laws require you to hold your own keys outside of the cloud provider environment, whereas other laws are less restrictive and allow storing keys with the same provider who stores the data.” There is a key point here - to comply with all the laws of some regions your organization operates in, you need to hold your own keys outside of the cloud provider environment. Next, you can identify the goal: you need a solution that can centrally manage all your keys. So, what should you do? In this question, each of the responses has you choose a different option. You can eliminate one of the responses right away… … because confidential computing does not affect the storage of data. Of the remaining responses, all three use services to help manage your keys. You need to determine where the keys will be stored. Options B and C store keys on Google Cloud. Option D stores keys on an external key management partner. You can eliminate the options that store keys on Google Cloud, because they won’t meet the requirement for the data and the keys to be stored in different provider environments. That leaves you with option D as the correct answer. Because you need a single solution that also has to store keys externally, this would be the appropriate choice. Use the approach that helps you as you answer diagnostic questions throughout this course. Remember to read the question stem carefully, and identify the key points and requirements. Some questions have multiple parts. Consider each of the requirements posed by the question. Find the similarities and differences in the responses, and eliminate responses that do not meet the requirements.

### Quiz - [Diagnostic questions](https://www.cloudskillsboost.google/course_templates/72/quizzes/517260)

#### Quiz 1.

> [!important]
> **You are using Dataproc to process a large number of CSV files. The storage option you choose needs to be flexible to serve many worker nodes in multiple clusters. These worker nodes will read the data and also write to it for intermediate storage between processing jobs. What is the recommended storage option on Google Cloud?**
>
> - [ ] Cloud Storage
> - [ ] Cloud SQL
> - [ ] Zonal persistent disks
> - [ ] Local SSD

#### Quiz 2.

> [!important]
> **Cymbal Retail is migrating its private data centers to Google Cloud. Over many years, hundreds of terabytes of data were accumulated. You currently have a 100 Mbps line and you need to transfer this data reliably before commencing operations on Google Cloud in 45 days. What should you do?**
>
> - [ ] Zip and upload the data to Cloud Storage buckets by using the Google Cloud console.
> - [ ] Store the data in an HTTPS endpoint, and configure Storage Transfer Service to copy the data to Cloud Storage.
> - [ ] Upload the data to Cloud Storage by using gcloud storage.
> - [ ] Order a transfer appliance, export the data to it, and ship it to Google.

#### Quiz 3.

> [!important]
> **Cymbal Retail has a team of business analysts who need to fix and enhance a set of large input data files. For example, duplicates need to be removed, erroneous rows should be deleted, and missing data should be added. These steps need to be performed on all the present set of files and any files received in the future in a repeatable, automated process. The business analysts are not adept at programming. What should they do?**
>
> - [ ] Load the data into Google Sheets, explore the data, and fix the data as needed.
> - [ ] Load the data into Dataprep, explore the data, and edit the transformations as needed.
> - [ ] Create a Dataproc job to perform the data fixes you need.
> - [ ] Create a Dataflow pipeline with the data fixes you need.

#### Quiz 4.

> [!important]
> **Your data and applications reside in multiple geographies on Google Cloud. Some regional laws require you to hold your own keys outside of the cloud provider environment, whereas other laws are less restrictive and allow storing keys with the same provider who stores the data. The management of these keys has increased in complexity, and you need a solution that can centrally manage all your keys. What should you do?**
>
> - [ ] Store your keys on a supported external key management partner, and use Cloud External Key Manager (Cloud EKM) to get keys when required.
> - [ ] Enable confidential computing for all your virtual machines.
> - [ ] Store keys in Cloud Key Management Service (Cloud KMS), and reduce the number of days for automatic key rotation.
> - [ ] Store your keys in Cloud Hardware Security Module (Cloud HSM), and retrieve keys from it when required.

#### Quiz 5.

> [!important]
> **You are running a user-supplied DoFn method signature pipeline in Dataflow. The function has been defined by you. The code is running slow and you want to further examine the pipeline code to get better visibility of why. What should you do?**
>
> - [ ] Use Cloud Profiler
> - [ ] Use Cloud Monitoring
> - [ ] Use Cloud Logging
> - [ ] Use Cloud Audit Logs

#### Quiz 6.

> [!important]
> **Business analysts in your team need to run analysis on data that was loaded into BigQuery. You need to follow recommended practices and grant permissions. What role should you grant the business analysts?**
>
> - [ ] bigquery.resourceViewer and bigquery.dataViewer
> - [ ] bigquery.user and bigquery.dataViewer
> - [ ] bigquery.dataOwner
> - [ ] storage.objectViewer and bigquery.user

#### Quiz 7.

> [!important]
> **You are managing the data for Cymbal Retail, which consists of multiple teams including retail, sales, marketing, and legal. These teams are consuming data from multiple producers including point of sales systems, industry data, orders, and more. Currently, teams that consume data have to repeatedly ask the teams that produce it to verify the most up-to-date data and to clarify other questions about the data, such as source and ownership. This process is unreliable and time-consuming and often leads to repeated escalations. You need to implement a centralized solution that gains a unified view of the organization's data and improves searchability. What should you do?**
>
> - [ ] Implement Looker dashboards that provide views of the data that meet each teams' requirements.
> - [ ] Implement a data warehouse by using BigQuery, and create datasets for each team such as retail, sales, marketing.
> - [ ] Implement a data mesh with Dataplex and have producers tag data when created.
> - [ ] Implement a data lake with Cloud Storage, and create buckets for each team such as retail, sales, marketing.

#### Quiz 8.

> [!important]
> **Cymbal Retail has acquired another company in Europe. Data access permissions and policies in this new region differ from those in Cymbal Retail's headquarters, which is in North America. You need to define a consistent set of policies for projects in each region that follow recommended practices. What should you do?**
>
> - [ ] Create a new organization for all projects in Europe and assign policies in each organization that comply with regional laws.
> - [ ] Implement a flat hierarchy, and assign policies to each project according to its region.
> - [ ] Create top level folders for each region, and assign policies at the folder level.
> - [ ] Implement policies at the resource level that comply with regional laws.

#### Quiz 9.

> [!important]
> **Laws in the region where you operate require that files related to all orders made each day are stored immutably for 365 days. The solution that you recommend has to be cost-effective. What should you do?**
>
> - [ ] Store the data in a Cloud Storage bucket, and specify a retention period.
> - [ ] Store the data in a Cloud Storage bucket, and set a lifecycle policy to delete the file after 365 days.
> - [ ] Store the data in a Cloud Storage bucket, enable object versioning, and delete any version greater than 365.
> - [ ] Store the data in a Cloud Storage bucket, and enable object versioning and delete any version older than 365 days.

#### Quiz 10.

> [!important]
> **You are migrating on-premises data to a data warehouse on Google Cloud. This data will be made available to business analysts. Local regulations require that customer information including credit card numbers, phone numbers, and email IDs be captured, but not used in analysis. You need to use a reliable, recommended solution to redact the sensitive data. What should you do?**
>
> - [ ] Use the Cloud Data Loss Prevention API (DLP API) to perform date shifting of any entries with credit card numbers, phone numbers, and email IDs.
> - [ ] Create a regular expression to identify and delete patterns that resemble credit card numbers, phone numbers, and email IDs.
> - [ ] Use the Cloud Data Loss Prevention API (DLP API) to identify and redact data that matches infoTypes like credit card numbers, phone numbers, and email IDs.
> - [ ] Delete all columns with a title similar to "credit card," "phone," and "email."

### Video - [Your study plan](https://www.cloudskillsboost.google/course_templates/72/video/517261)

- [YouTube: Your study plan](https://www.youtube.com/watch?v=oR14ZjequKc)

Now let’s review how to use these diagnostic questions to help you identify what to include in your study plan. As a reminder - this course isn’t designed to teach you everything you need to know for the exam - and the diagnostic questions don’t cover everything that could be on the exam. Instead, this activity is meant to give you a better sense of the scope of this section and the different skills you’ll want to develop as you prepare for the certification. You’ll approach this review by looking at the objectives of this exam section and the questions you just answered about each one. Let’s introduce an objective, briefly review the answers to the related questions, then explain where you can find out more in the learning resources and/or in Google documentation. As you go through each section objective, use the page in your workbook to mark the specific documentation, courses, and skill badges you’ll want to emphasize in your study plan.

### Document - [Study plan resources](https://www.cloudskillsboost.google/course_templates/72/documents/517262)

### Quiz - [Knowledge Check](https://www.cloudskillsboost.google/course_templates/72/quizzes/517263)

#### Quiz 1.

> [!important]
> **A company collects lots of consumer data from online marketing campaigns. Company plans to use Google Cloud to store this collected data. The top management is worried about exposing personally identifiable information (PII) that may be present in this data.  What should you do to reduce the risk of exposing PII data?**
>
> - [ ] Use the Cloud Data Loss Prevention API (DLP API) to inspect and redact PII data.
> - [ ] Ensure that all PII data is removed from the collected data before storing it on Google Cloud.
> - [ ] Store all data in BigQuery and turn on column level access to protect sensitive data.
> - [ ] Ensure that all stored data is monitored by Security Command Center.

#### Quiz 2.

> [!important]
> **Your company is very serious about data protection and hence decides to implement the Principle of Least Privilege. What should you do to comply with this policy?**
>
> - [ ] Ensure that the access permissions are given strictly based on the person's title and job role.
> - [ ] Give just enough permissions to get the task done.
> - [ ] Ensure that the users are verified every time they request access, even if they were authenticated earlier.
> - [ ] When a task is assigned, ensure that it gets assigned to a person with the minimum privileges.

## Ingesting and Processing  Data

In this module you'll explore considerations for ingesting and processing data, which corresponds to section 2 of the Professional Data Engineer Exam Guide. You'll start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you'll assess your skills in this section through 10 diagnostic questions. Then you'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan. 

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/72/video/517264)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=DYLErQEGf-A)

Welcome to Module 2: Ingesting and Processing Data. In this module you’ll explore considerations for ingesting and processing data, which corresponds to section 2 of the Professional Data Engineer Exam Guide. Let’s start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you’ll assess your skills in this section through 10 diagnostic questions. Then you’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

### Video - [Ingesting and processing data for Cymbal Retail](https://www.cloudskillsboost.google/course_templates/72/video/517265)

- [YouTube: Ingesting and processing data for Cymbal Retail](https://www.youtube.com/watch?v=Levno2tlK-c)

Let’s begin by exploring the role of a Professional Data Engineer in handling data ingestion and data processing for Cymbal Retail, which includes using the various services provided by Google Cloud to support data ingestion and data processing. Cymbal Retail receives data from multiple sources, both internal and external. As the business has grown, the volume of ingested data has also increased exponentially. Processing the data has become increasingly complex and costly. In Cymbal Retail’s current on-premises data centers, Spark and Hadoop jobs are executed on pre-configured, static infrastructure. Part of your role involves determining how to lift-and-shift these jobs to Google Cloud. You have to design the architecture of the data ingestion and processing. Some data can be directly loaded into data warehouses using an extract and load approach, while others might be transformed before being uploaded into the data warehouse. Building, deploying, and operating effective flexible data pipelines for all the stages of data processing is a primary expectation from you as a Professional Data Engineer. You need to identify and deploy the right approach between EL, ETL, or ELT and choose the right Google Cloud tools for the job. Cymbal Retail’s customers want features that require the increased use of real-time data. In this regard, tools like open source Apache Beam and the hosted Dataflow are important skills for a data professional. Your knowledge of ways to apply different types of windowing for various use cases will provide the right approach to analyze streaming data. Your role also requires you to optimize all data ingestions and data processings tasks. Your optimizations should bring considerable savings on effort and cost, while improving availability and responsiveness. As the volume of data and scale of processing increases, Cymbal Retail does not want the latency, effort, or cost to increase linearly, or worse, exponentially. Your early design decisions on automation and orchestration could reduce effort later on.

### Video - [Introduction: Diagnostic questions](https://www.cloudskillsboost.google/course_templates/72/video/517266)

- [YouTube: Introduction: Diagnostic questions](https://www.youtube.com/watch?v=2iIs-19De-w)

It’s your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, you’re not expected to know all the answers yet. Please complete the diagnostic questions that are presented next. If your learning platform does not support assessments, kindly use your Course Workbook.

### Quiz - [Diagnostic questions](https://www.cloudskillsboost.google/course_templates/72/quizzes/517267)

#### Quiz 1.

> [!important]
> **You are running Dataflow jobs for data processing. When developers update the code in Cloud Source Repositories, you need to test and deploy the updated code with minimal effort. Which of these would you use to build your continuous integration and delivery (CI/CD) pipeline for data processing?**
>
> - [ ] Cloud Code
> - [ ] Terraform
> - [ ] Compute Engine
> - [ ] Cloud Build

#### Quiz 2.

> [!important]
> **You manage a PySpark batch data pipeline by using Dataproc: You want to take a hands-off approach to running the workload, and you do not want to provision and manage your own cluster. What should you do?**
>
> - [ ] Rewrite the job in Spark SQL.
> - [ ] Rewrite the job in Dataflow with SQL.
> - [ ] Configure the job to run on Dataproc Serverless.
> - [ ] Configure the job to run with Spot VMs.

#### Quiz 3.

> [!important]
> **Your company has multiple data analysts but a limited data engineering team. You need to choose a tool where the analysts can build data pipelines themselves with a graphical user interface. Which of these products is the most appropriate?**
>
> - [ ] Cloud Composer
> - [ ] Dataflow
> - [ ] Cloud Data Fusion
> - [ ] Dataproc

#### Quiz 4.

> [!important]
> **You need to run batch jobs, which could take many days to complete. You do not want to manage the infrastructure provisioning. What should you do?**
>
> - [ ] Use Cloud Run to run the jobs.
> - [ ] Use Cloud Scheduler to run the jobs.
> - [ ] Use Workflows to run the jobs.
> - [ ] Run the jobs on Batch.

#### Quiz 5.

> [!important]
> **You have a data pipeline that requires you to monitor a Cloud Storage bucket for a file, start a Dataflow job to process data in the file, run a shell script to validate the processed data in BigQuery, and then delete the original file. You need to orchestrate this pipeline by using recommended tools. Which product should you choose?**
>
> - [ ] Cloud Scheduler
> - [ ] Cloud Tasks
> - [ ] Cloud Composer
> - [ ] Cloud Run

#### Quiz 6.

> [!important]
> **The first stage of your data pipeline processes tens of terabytes of financial data and creates a sparse, time-series dataset as a key-value pair. Which of these is a suitable sink for the pipeline's first stage?**
>
> - [ ] Cloud Storage
> - [ ] Cloud SQL
> - [ ] AlloyDB
> - [ ] Bigtable

#### Quiz 7.

> [!important]
> **Your data engineering team receives data in JSON format from external sources at the end of each day. You need to design the data pipeline. What should you do?**
>
> - [ ] Store the data in persistent disks and create an ETL pipeline.
> - [ ] Create a public API to allow external applications to add the data to your warehouse.
> - [ ] Store the data in Cloud Storage and create an extract, transform, and load (ETL) pipeline.
> - [ ] Make your BigQuery data warehouse public and ask the external sources to insert the data.

#### Quiz 8.

> [!important]
> **You are processing large amounts of input data in BigQuery. You need to combine this data with a small amount of frequently changing data that is available in Cloud SQL. What should you do?
**
>
> - [ ] Create a Dataflow pipeline to combine the BigQuery and Cloud SQL data when the Cloud SQL data changes.
> - [ ] Use a federated query to get data from Cloud SQL.
> - [ ] Copy the data from Cloud SQL to a new BigQuery table hourly.
> - [ ] Copy the data from Cloud SQL and create a combined, normalized table hourly.

#### Quiz 9.

> [!important]
> **You want to build a streaming data analytics pipeline in Google Cloud. You need to choose the right products that support streaming data. Which of these would you choose?**
>
> - [ ] Cloud Storage, Dataflow, Cloud SQL
> - [ ] Cloud Storage, Dataprep, AlloyDB
> - [ ] Pub/Sub, Dataflow, BigQuery
> - [ ] Pub/Sub, Dataprep, BigQuery

#### Quiz 10.

> [!important]
> **You are creating a data pipeline for streaming data on Dataflow for Cymbal Retail's point of sales data. You want to calculate the total sales per hour on a continuous basis. Which of these windowing options should you use?**
>
> - [ ] Hopping windows (sliding windows in Apache Beam)
> - [ ] Tumbling windows (fixed windows in Apache Beam)
> - [ ] Session windows
> - [ ] Global window

### Video - [Your study plan](https://www.cloudskillsboost.google/course_templates/72/video/517268)

- [YouTube: Your study plan](https://www.youtube.com/watch?v=fj7IO0f_kuo)

Now let’s review how to use these diagnostic questions to help you identify what to include in your study plan. As a reminder - this course isn’t designed to teach you everything you need to know for the exam - and the diagnostic questions don’t cover everything that could be on the exam. Instead, this activity is meant to give you a better sense of the scope of this section and the different skills you’ll want to develop as you prepare for the certification. You’ll approach this review by looking at the objectives of this exam section and the questions you just answered about each one. Let’s introduce an objective, briefly review the answers to the related questions, then explain where you can find out more in the learning resources and/or in Google documentation. As you go through each section objective, use the page in your workbook to mark the specific documentation, courses, and skill badges you’ll want to emphasize in your study plan.

### Document - [Study plan resources](https://www.cloudskillsboost.google/course_templates/72/documents/517269)

### Quiz - [Knowledge Check](https://www.cloudskillsboost.google/course_templates/72/quizzes/517270)

#### Quiz 1.

> [!important]
> **A company collects large amounts of data that is useful for improving business operations. The collected data is already clean and is in a format that is suitable for further analysis. The company uses BigQuery as a data warehouse. What approach will you recommend to move this data to BigQuery?**
>
> - [ ] Implement Extract, Transform and Load (ETL) pipelines using tools like Dataflow.
> - [ ] Split the data into smaller files and then move to Google Cloud.
> - [ ] Do transformation using Extract, Load & Transform  (ELT).
> - [ ] Directly load the data using Extract and Load approach ( EL).

#### Quiz 2.

> [!important]
> **A company wants to improve productivity and decides to programmatically schedule and monitor workflows. What tool can you use to automate your workflows?**
>
> - [ ] Apache Beam and Dataflow
> - [ ] Dataproc
> - [ ] Data Fusion
> - [ ] Cloud Composer

## Storing Data

In this module you'll explore considerations for storing data, which corresponds to section 3 of the Professional Data Engineer Exam Guide. You start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you'll assess your skills in this section through 10 diagnostic questions. Then you'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan. 

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/72/video/517271)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=Zcqaarey_0w)

Welcome to Module 3: Storing the Data. In this module you’ll explore considerations for storing data, which corresponds to section 3 of the Professional Data Engineer Exam Guide. Let’s start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you’ll assess your skills in this section through 10 diagnostic questions. Then you’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

### Video - [Storing Cymbal Retail’s data](https://www.cloudskillsboost.google/course_templates/72/video/517272)

- [YouTube: Storing Cymbal Retail’s data](https://www.youtube.com/watch?v=Cr0-hRCqXIo)

Let’s begin by exploring the role of a Professional Data Engineer in storing data in Google Cloud for Cymbal Retail, including data architecture, database selection, and data lifecycle management. As a Professional Data Engineer, you role is deeply intertwined with how Cymbal Retail uses data. Cymbal Retail ingests, stores, processes, and analyzes many types of data, including documents, images, text, and video. There are close to a thousand databases used across the organization containing structured and unstructured data. Google Cloud has a variety of databases, both SQL and NoSQL, and also products suitable for data lakes, data warehouses, and data meshes. You need to know the difference between each of them and choose the appropriate storage solution, which is essential to ensuring efficiency of access and cost. All of this data, running into petabytes, has to meet the multi-modal purpose of cost-efficient storage, quick retrieval for analytics, and secure access. To comply with industry and regional data privacy laws, Cymbal Retail needs to have clear and demonstrable governance controls in place. Your data engineering team must ensure that data follows a well-defined lifecycle. Each region's laws define a strict timeline for the retention and deletion of customer data. You need to set controls in place so that individuals do not accidentally breach rules. It is also your responsibility as a Professional Data Engineer to continue to maintain Cymbal Retail’s data for effective use. As business requirements change, you need to evolve the data architecture to meet demand. Appreciation of the goals of the business and the ability to map them to data decisions make an excellent data engineer.

### Video - [Introduction: Diagnostic questions](https://www.cloudskillsboost.google/course_templates/72/video/517273)

- [YouTube: Introduction: Diagnostic questions](https://www.youtube.com/watch?v=SNXPV8gCEhA)

It’s your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, these questions are intended to help you understand or diagnose which areas you'll want to focus on in your study plan. So we don't expect you to know all the answers yet. Please complete the diagnostic questions that are presented next. If your learning platform does not support assessments or quizzes, kindly use your Course Workbook.

### Quiz - [Diagnostic questions](https://www.cloudskillsboost.google/course_templates/72/quizzes/517274)

#### Quiz 1.

> [!important]
> **A manager at Cymbal Retail expresses concern about unauthorized access to objects in your Cloud Storage bucket. You need to evaluate all access on all objects in the bucket. What should you do?**
>
> - [ ] Enable and then review the Data Access audit logs.
> - [ ] Change the permissions on the bucket to only trusted employees.
> - [ ] Route the Admin Activity logs to a BigQuery sink and analyze the logs with SQL queries.
> - [ ] Review the Admin Activity audit logs.

#### Quiz 2.

> [!important]
> **Your analysts repeatedly run the same complex queries that combine and filter through a lot of data on BigQuery. The data changes frequently. You need to reduce the effort for the analysts. What should you do?**
>
> - [ ] Create a view of the frequently queried data.
> - [ ] Export the frequently queried data into a new table.
> - [ ] Export the frequently queried data into Cloud SQL.
> - [ ] Create a dataset with the data that is frequently queried.

#### Quiz 3.

> [!important]
> **You have large amounts of data stored on Cloud Storage and BigQuery. Some of it is processed, but some is yet unprocessed. You have a data mesh created in Dataplex. You need to make it convenient for internal users of the data to discover and use the data. What should you do?**
>
> - [ ] Create a lake for BigQuery data and a zone for Cloud Storage data.
> - [ ] Create a lake for unprocessed data and assets for processed data.
> - [ ] Create a lake for Cloud Storage data and a zone for BigQuery data.
> - [ ] Create a raw zone for the unprocessed data and a curated zone for the processed data.

#### Quiz 4.

> [!important]
> **You have several large tables in your transaction databases. You need to move all the data to BigQuery for the business analysts to explore and analyze the data. How should you design the schema in BigQuery?**
>
> - [ ] Redesign the schema to denormalize the data with nested and repeated data.
> - [ ] Redesign the schema to normalize the data by removing all redundancies.
> - [ ] Retain the data on BigQuery with the same schema as the source.
> - [ ] Combine all the transactional database tables into a single table using outer joins.

#### Quiz 5.

> [!important]
> **You have data that is ingested daily and frequently analyzed in the first month. Thereafter, the data is retained only for audits, which happen occasionally every few years. You need to configure cost-effective storage. What should you do?**
>
> - [ ] Configure a data retention policy on Cloud Storage.
> - [ ] Configure a lifecycle policy on Cloud Storage.
> - [ ] Create a bucket on Cloud Storage with object versioning configured.
> - [ ] Create a bucket on Cloud Storage with Autoclass configured.

#### Quiz 6.

> [!important]
> **Cymbal Retail has accumulated a large amount of data. Analysts and leadership are finding it difficult to understand the meaning of the data, such as BigQuery columns. Users of the data don't know who owns what. You need to improve the searchability of the data. What should you do?**
>
> - [ ] Add a description column corresponding to each data column.
> - [ ] Create tags for data entries in Cloud Catalog.
> - [ ] Export the data to Cloud Storage with descriptive file names.
> - [ ] Rename BigQuery columns with more descriptive names.

#### Quiz 7.

> [!important]
> **You have data stored in a Cloud Storage bucket. You are using both Identity and Access Management (IAM) and Access Control Lists (ACLs) to configure access control. Which statement describes a user's access to objects in the bucket?**
>
> - [ ] The user only has access if both IAM and ACLs grant a permission.
> - [ ] The user has access if either IAM or ACLs grant a permission.
> - [ ] The user has no access if IAM denies the permission.
> - [ ] The user has no access if either IAM or ACLs deny a permission.

#### Quiz 8.

> [!important]
> **You need to store data long term and use it to create quarterly reports. What storage class should you choose?**
>
> - [ ] Nearline
> - [ ] Coldline
> - [ ] Archive
> - [ ] Standard

#### Quiz 9.

> [!important]
> **You are ingesting data that is spread out over a wide range of dates into BigQuery at a fast rate. You need to partition the table to make queries performant. What should you do?**
>
> - [ ] Create an ingestion-time partitioned table with daily partitioning type.
> - [ ] Create an integer-range partitioned table.
> - [ ] Create an ingestion-time partitioned table with yearly partitioning type.
> - [ ] Create a time-unit column-partitioned table with yearly partitioning type.

#### Quiz 10.

> [!important]
> **You need to choose a data storage solution to support a transactional system. Your customers are primarily based in one region. You want to reduce your administration tasks and focus engineering effort on building your business application. What should you do?**
>
> - [ ] Use Cloud SQL.
> - [ ] Install a database of your choice on a Compute Engine VM.
> - [ ] Create a Cloud Storage bucket with a regional bucket.
> - [ ] Use Spanner.

### Video - [Your study plan](https://www.cloudskillsboost.google/course_templates/72/video/517275)

- [YouTube: Your study plan](https://www.youtube.com/watch?v=_k0FP8e2rSA)

Now let’s review how to use these diagnostic questions to help you identify what to include in your study plan. As a reminder, this course isn’t designed to teach you everything you need to know for the exam, and the diagnostic questions don’t cover everything that could be on the exam. Instead, this activity is meant to give you a better sense of the scope of this section and the different skills you’ll want to develop as you prepare for the certification. You’ll approach this review by looking at the objectives of this exam section and the questions you just answered about each one. Let’s introduce an objective, briefly review the answers to the related questions, then explain where you can find out more in the learning resources and/or in Google documentation. As you go through each section objective, use the page in your workbook to mark the specific documentation, courses, and skill badges you’ll want to emphasize in your study plan.

### Document - [Study plan resources](https://www.cloudskillsboost.google/course_templates/72/documents/517276)

### Quiz - [Knowledge Check](https://www.cloudskillsboost.google/course_templates/72/quizzes/517277)

#### Quiz 1.

> [!important]
> **Cymbal Retail also collects large amounts of structured, semistructured, and unstructured data. The company wants a centralized repository to store this data in a cost-effective manner using Google Cloud. What tool can you use to meet these requirements?**
>
> - [ ] Cloud Storage
> - [ ] Bigtable
> - [ ] Dataflow
> - [ ] Cloud SQL

#### Quiz 2.

> [!important]
> **Cymbal Retail collects large amounts of data that is useful for improving business operations. The company wants to store and analyze this data in a serverless and cost-effective manner using Google Cloud. The analysts need to use SQL to write the queries. 
What tool can you use to meet these requirements?**
>
> - [ ] Data Fusion
> - [ ] BigQuery
> - [ ] Memorystore
> - [ ] Spanner

## Preparing and Using Data for Analysis

In this module, you'll explore considerations when preparing and using data for analysis, which corresponds to section 4 of the Professional Data Engineer Exam Guide. You start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you'll assess your skills in this section through 10 diagnostic questions. Then you'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan. 

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/72/video/517278)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=IOmMZS3pafk)

Welcome to Module 4: Preparing and Using Data for Analysis. In this module, you’ll explore considerations when preparing and using data for analysis, which corresponds to section 4 of the Professional Data Engineer Exam Guide. Let’s start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you’ll assess your skills in this section through 10 diagnostic questions. Then you’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

### Video - [Preparing and using Cymbal Retail’s data for analysis](https://www.cloudskillsboost.google/course_templates/72/video/517279)

- [YouTube: Preparing and using Cymbal Retail’s data for analysis](https://www.youtube.com/watch?v=WmiI9OCnn4U)

Let’s begin by exploring the role of a Professional Data Engineer in preparing and analysing data for Cymbal Retail. The management at Cymbal Retail understands that data becomes most useful when it can be used to make decisions. They have employed advances in data based decision-making for several decades. The company's leaders need analytics and decision support systems to be flexible, agile, and customizable to their needs. One of your key responsibilities as a Professional Data Engineer is to help support the business in effectively using data to make decisions. As much as up-to-date analytics is useful for Cymbal Retail, the management realizes that their partners would benefit from it too. You need to ensure data is shared in a secure, limited way. Partner-specific reports are available. These partner accounts are controlled for access and granularity by the data engineering team. Industry and government bodies frequently request data from Cymbal Retail, which the company is obliged to provide in a short turnaround duration. Any data access is limited strictly on a need to know basis. As a Professional Data Engineer, your role also involves supporting Cymbal Retail’s goals of advancing its machine learning technologies. Fine-tuned predictions in estimating demand and individual customer behavior has helped reduce inventory while increasing sales. The adoption of the latest machine learning and artificial intelligence technologies has been key to this development. As Cymbal Retail is investing more in the adoption of relevant machine learning technologies, data engineering must process the data to make it usable to build machine learning models and build such models.

### Video - [Introduction: Diagnostic questions](https://www.cloudskillsboost.google/course_templates/72/video/517280)

- [YouTube: Introduction: Diagnostic questions](https://www.youtube.com/watch?v=Rt9asxyTHYA)

It’s your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, these questions are intended to help you understand or diagnose which areas you'll want to focus on in your study plan. So we don't expect you to know all the answers yet. Please complete the diagnostic questions that are presented next. If your learning platform does not support assessments or quizzes, kindly use your Course Workbook.

### Quiz - [Diagnostic questions](https://www.cloudskillsboost.google/course_templates/72/quizzes/517281)

#### Quiz 1.

> [!important]
> **You have a complex set of data that comes from multiple sources. The analysts in your team need to analyze the data, visualize it, and publish reports to internal and external stakeholders. You need to make it easier for the analysts to work with the data by abstracting the multiple data sources. What tool do you recommend?**
>
> - [ ] Looker
> - [ ] Looker Studio
> - [ ] Connected Sheets
> - [ ] D3.js library

#### Quiz 2.

> [!important]
> **You built machine learning (ML) models based on your own data. In production, the ML models are not giving satisfactory results. When you examine the data, it appears that the existing data is not sufficiently representing the business goals. You need to create a more accurate machine learning model. What should you do?**
>
> - [ ] Perform feature engineering, and use domain knowledge to enhance the column data.
> - [ ] Train the model with more of similar data.
> - [ ] Perform L2 regularization.
> - [ ] Train the model with the same data, but use more epochs.

#### Quiz 3.

> [!important]
> **You repeatedly run the same queries by joining multiple tables. The original tables change about ten times per day. You want an optimized querying approach. Which feature should you use?**
>
> - [ ] Views
> - [ ] Partitions
> - [ ] Materialized views
> - [ ] Federated queries

#### Quiz 4.

> [!important]
> **You have analytics data stored in BigQuery. You need an efficient way to compute values across a group of rows and return a single result for each row. What should you do?**
>
> - [ ] Use a window function with an OVER clause.
> - [ ] Use an aggregate function.
> - [ ] Use a UDF (user-defined function).
> - [ ] Use BigQuery ML.

#### Quiz 5.

> [!important]
> **Your business has collected industry-relevant data over many years. The processed data is useful for your partners and they are willing to pay for its usage. You need to ensure proper access control over the data. What should you do?**
>
> - [ ] Host the data on Cloud SQL.
> - [ ] Export the data to zip files and share it through Cloud Storage.
> - [ ] Host the data on Analytics Hub.
> - [ ] Export the data to persistent disks and share it through an FTP endpoint.

#### Quiz 6.

> [!important]
> **Your company uses Google Workspace and your leadership team is familiar with its business apps and collaboration tools. They want a cost-effective solution that uses their existing knowledge to evaluate, analyze, filter, and visualize data that is stored in BigQuery. What should you do to create a solution for the leadership team?**
>
> - [ ] Create models in Looker.
> - [ ] Configure Tableau.
> - [ ] Configure Connected Sheets.
> - [ ] Configure Looker Studio.

#### Quiz 7.

> [!important]
> **You need to optimize the performance of queries in BigQuery. Your tables are not partitioned or clustered. What optimization technique can you use?**
>
> - [ ] Filter data as late as possible.
> - [ ] Batch your updates and inserts.
> - [ ] Use the LIMIT clause to reduce the data read.
> - [ ] Perform self-joins on data.

#### Quiz 8.

> [!important]
> **You used Dataplex to create lakes and zones for your business data. However, some files are not being discovered. What could be the issue?**
>
> - [ ] The files are in Parquet format.
> - [ ] You have an exclude pattern that matches the files.
> - [ ] You have scheduled discovery to run every hour.
> - [ ] The files are in ORC format.

#### Quiz 9.

> [!important]
> **You have data in PostgreSQL that was designed to reduce redundancy. You are transferring this data to BigQuery for analytics. The source data is hierarchical and frequently queried together. You need to design a BigQuery schema that is performant. What should you do?**
>
> - [ ] Use nested and repeated fields.
> - [ ] Copy the normalized data into partitions.
> - [ ] Retain the data in normalized form always.
> - [ ] Copy the primary tables and use federated queries for secondary tables.

#### Quiz 10.

> [!important]
> **Your data in BigQuery has some columns that are extremely sensitive. You need to enable only some users to see certain columns. What should you do?**
>
> - [ ] Create a new table with the column's data.
> - [ ] Use policy tags.
> - [ ] Use Identity and Access Management (IAM) permissions.
> - [ ] Create a new dataset with the column's data.

### Video - [Your study plan](https://www.cloudskillsboost.google/course_templates/72/video/517282)

- [YouTube: Your study plan](https://www.youtube.com/watch?v=6p-yTUTjwm8)

Now let’s review how to use these diagnostic questions to help you identify what to include in your study plan. As a reminder, this course isn’t designed to teach you everything you need to know for the exam—and the diagnostic questions don’t cover everything that could be on the exam. Instead, this activity is meant to give you a better sense of the scope of this section and the different skills you’ll want to develop as you prepare for the certification. You’ll approach this review by looking at the objectives of this exam section and the questions you just answered about each one. Let’s introduce an objective, briefly review the answers to the related questions, then explain where you can find out more in the learning resources and/or in Google documentation. As you go through each section objective, use the page in your workbook to mark the specific documentation, courses, and Skill Badges you’ll want to emphasize in your study plan.

### Document - [Study plan resources](https://www.cloudskillsboost.google/course_templates/72/documents/517283)

### Quiz - [Knowledge Check](https://www.cloudskillsboost.google/course_templates/72/quizzes/517284)

#### Quiz 1.

> [!important]
> **You need to share inventory data from Cymbal Retail with a partner company that uses BigQuery to store and analyze its data. What tool can you use to securely and efficiently share the data?**
>
> - [ ] Cloud Storage
> - [ ] Cloud Data Loss Prevention (Cloud DLP)
> - [ ] Analytics Hub
> - [ ] Data Catalog

#### Quiz 2.

> [!important]
> **Cymbal Retail has a team of ML engineers that builds and maintains machine learning models. 
As a Professional Data Engineer, how will you support this team?**
>
> - [ ] Finalize the type of machine learning model to use.
> - [ ] Keep on improving the machine learning model after initial deployment.
> - [ ] Identify what type of data is required to build ML models.
> - [ ] Process and prepare existing data to enable feature engineering.

## Maintaining and Automating Data Workloads

In this module you'll explore considerations for maintaining and automating data workloads, which corresponds to section 5 of the Professional Data Engineer Exam Guide. You start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you'll assess your skills in this section through 10 diagnostic questions. Then you'll review these questions. Based on the areas you need to learn more about, you'll identify resources to include in your study plan. 

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/72/video/517285)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=fbrYYLyH96U)

Welcome to Module 5: Maintaining and Automating Data Workloads. In this module you’ll explore considerations for maintaining and automating data workloads, which corresponds to section 5 of the Professional Data Engineer Exam Guide. Let’s start by discussing how a Professional Data Engineer performs this role at Cymbal Retail. Next, you’ll assess your skills in this section through 10 diagnostic questions. Then you’ll review these questions. Based on the areas you need to learn more about, you’ll identify resources to include in your study plan.

### Video - [Maintaining and automating data workloads at Cymbal Retail](https://www.cloudskillsboost.google/course_templates/72/video/517286)

- [YouTube: Maintaining and automating data workloads at Cymbal Retail](https://www.youtube.com/watch?v=wYGRaqPM2bg)

Let’s begin by exploring the role of a Professional Data Engineer in helping Cymbal Retail maintain and automate its data workloads. As part of the Data Engineering team at Cymbal Retail you are responsible for ensuring cost effectiveness and correctness of data. The team has understood that automation and observability are cornerstones to providing a reliable and scalable service to end customers, internal teams, and to vendors. You need to be familiar with monitoring and managing active workloads to balance compute capacity, error handling, and costs. Cost control, though extremely important, cannot be detrimental to the end user experience. Web pages and purchase workflows have to be fast and frictionless. Availability has to be high, as a customer who has even a one-time bad experience is a customer lost. In order to balance the competing demands of user experience and cost-efficiency, you need to research and obtain the best pricing available from Google Cloud. To help achieve both reliability and cost control, Cymbal Retail wants to automate workloads and scale repeatable data processing. As a Professional Data Engineer, you need to regularly look for opportunities to optimize through automation and to boost efficiency. Taking a hands off approach to maintenance with appropriately set notifications and thresholds can save considerable time and effort while achieving scale. Marketing and sales teams need the flexibility to run campaigns quickly. These deals could bring in big spikes in demand for a short period, which results in valuable sales. Previously the marketing and sales teams would inform the data engineering team of such events weeks or months in advance, which would help them arrange the required computing and data resources. This has been found to be prohibitive. You need to find solutions that give considerable autonomy and independence to the marketing and sales teams to run their campaigns without being restricted by the technology demands.

### Video - [Introduction: Diagnostic questions](https://www.cloudskillsboost.google/course_templates/72/video/517287)

- [YouTube: Introduction: Diagnostic questions](https://www.youtube.com/watch?v=go9vNAmQ0P0)

It’s your turn to assess your experience and skills related to this section with some diagnostic questions. Remember, these questions are intended to help you understand or diagnose which areas you'll want to focus on in your study plan. So we don't expect you to know all the answers yet. Please complete the diagnostic questions that are presented next. If your learning platform does not support assessments or quizzes, kindly use your Course Workbook.

### Quiz - [Diagnostic questions](https://www.cloudskillsboost.google/course_templates/72/quizzes/517288)

#### Quiz 1.

> [!important]
> **You need to create repeatable data processing tasks by using Cloud Composer. You need to follow best practices and recommended approaches. What should you do?**
>
> - [ ] Combine multiple functionalities in a single task execution.
> - [ ] Write each task to be responsible for one operation.
> - [ ] Update data with INSERT statements during the task run.
> - [ ] Use current time with the now() function for computation.

#### Quiz 2.

> [!important]
> **You have a team of data analysts that run queries interactively on BigQuery during work hours. You also have thousands of report generation queries that run simultaneously. You often see an error: Exceeded rate limits: too many concurrent queries for this project_and_region. How would you resolve this issue?**
>
> - [ ] Create a yearly reservation of BigQuery slots.
> - [ ] Run the report generation queries in batch mode.
> - [ ] Run all queries in interactive mode.
> - [ ] Create a view to run the queries.

#### Quiz 3.

> [!important]
> **You need to design a Dataproc cluster to run multiple small jobs. Many jobs (but not all) are of high priority. What should you do?**
>
> - [ ] Reuse the same cluster to run all jobs in parallel.
> - [ ] Use ephemeral clusters.
> - [ ] Reuse the same cluster and run each job in sequence.
> - [ ] Use cluster autoscaling.

#### Quiz 4.

> [!important]
> **Cymbal Retail processes streaming data on Dataflow with Pub/Sub as a source. You need to plan for disaster recovery and protect against zonal failures. What should you do?**
>
> - [ ] Create Dataflow jobs from templates.
> - [ ] Enable Dataflow shuffle.
> - [ ] Enable vertical autoscaling.
> - [ ] Take Dataflow snapshots periodically.

#### Quiz 5.

> [!important]
> **You have a Dataflow pipeline in production. For certain data, the system seems to be stuck longer than usual. This is causing delays in the pipeline execution. You want to reliably and proactively track and resolve such issues. What should you do?**
>
> - [ ] Review the Dataflow logs regularly.
> - [ ] Review the Cloud Monitoring dashboard regularly.
> - [ ] Set up alerts on Cloud Monitoring based on system lag.
> - [ ] Set up alerts with Cloud Run functions code that reviews the audit logs regularly.

#### Quiz 6.

> [!important]
> **You are running a Dataflow pipeline in production. The input data for this pipeline is occasionally inconsistent. Separately from processing the valid data, you want to efficiently capture the erroneous input data for analysis. What should you do?**
>
> - [ ] Create a side output for the erroneous data.
> - [ ] Re-read the input data and create separate outputs for valid and erroneous data.
> - [ ] Check for the erroneous data in the logs.
> - [ ] Read the data once, and split it into two pipelines, one to output valid data and another to output erroneous data.

#### Quiz 7.

> [!important]
> **You run a Cloud SQL instance for a business that requires that the database is accessible for transactions. You need to ensure minimal downtime for database transactions. What should you do?**
>
> - [ ] Configure replication.
> - [ ] Configure backups.
> - [ ] Configure high availability.
> - [ ] Configure backups and increase the number of backups.

#### Quiz 8.

> [!important]
> **Multiple analysts need to prepare reports on Monday mornings due to which there is heavy utilization of BigQuery. You want to take a cost-effective approach to managing this demand. What should you do?**
>
> - [ ] Use Flex Slots.
> - [ ] Use BigQuery Enterprise Plus edition with a three-year commitment.
> - [ ] Use BigQuery Enterprise edition with a one-year commitment.
> - [ ] Use on-demand pricing.

#### Quiz 9.

> [!important]
> **A colleague at Cymbal Retail asks you about the configuration of Dataproc autoscaling for a project. What would be the Google-recommended situation when you should enable autoscaling?**
>
> - [ ] When there are different size workloads on the cluster.
> - [ ] When you want to scale on-cluster Hadoop Distributed File System (HDFS).
> - [ ] When you want to down-scale idle clusters to minimum size.
> - [ ] When you want to scale out single-job clusters.

#### Quiz 10.

> [!important]
> **When running Dataflow jobs, you see this error in the logs: "A hot key HOT_KEY_NAME was detected in…". You need to resolve this issue and make the workload performant. What should you do?**
>
> - [ ] Add more compute instances for processing.
> - [ ] Increase the data with the hot key.
> - [ ] Ensure that your data is evenly distributed.
> - [ ] Disable Dataflow shuffle.

### Video - [Your study plan](https://www.cloudskillsboost.google/course_templates/72/video/517289)

- [YouTube: Your study plan](https://www.youtube.com/watch?v=QJo_FfqLNJY)

Now let’s review how to use these diagnostic questions to help you identify what to include in your study plan. As a reminder - this course isn’t designed to teach you everything you need to know for the exam - and the diagnostic questions don’t cover everything that could be on the exam. Instead, this activity is meant to give you a better sense of the scope of this section and the different skills you’ll want to develop as you prepare for the certification. You’ll approach this review by looking at the objectives of this exam section and the questions you just answered about each one. Let’s introduce an objective, briefly review the answers to the related questions, then explain where you can find out more in the learning resources and/or in Google documentation. As you go through each section objective, use the page in your workbook to mark the specific documentation, courses, and skill badges you’ll want to emphasize in your study plan.

### Document - [Study plan resources](https://www.cloudskillsboost.google/course_templates/72/documents/517290)

### Quiz - [Knowledge Check](https://www.cloudskillsboost.google/course_templates/72/quizzes/517291)

#### Quiz 1.

> [!important]
> **Cymbal Retail uses Google Cloud and has automated repeatable data processing workloads to achieve reliability and cost efficiency. You want out-of-the-box metric collection dashboards and the ability to generate alerts when  specific conditions are met. What tool can you use?**
>
> - [ ] Cloud Monitoring
> - [ ] Data Catalog
> - [ ] Cloud Composer
> - [ ] Cloud Data Loss Prevention (Cloud DLP)

#### Quiz 2.

> [!important]
> **Your company recently migrated to Google Cloud and started using BigQuery. The team members don't know how much querying they are going to do, and they need to be efficient with their spend. As a Professional Data Engineer, what pricing model would you recommend?**
>
> - [ ] Decide how much compute capacity you need and reserve it using capacity pricing.
> - [ ] Use IAM service to block access to BigQuery till the team figures out how much querying they are going to do.
> - [ ] Use BigQuery's on-demand pricing model.
> - [ ] Create a pool of resources using BigQuery Reservations.

## Summary

 In this module, you focus on creating your individualized study plan. You will use the notes you've been taking throughout this course to put together a study plan for each week in your Professional Data Engineer journey.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/72/video/517292)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=aCZnh9ZU9vU)

Welcome to Module 6: Your Next Steps. In this module, you focus on creating your individualized study plan. In this module, you use the notes you’ve been taking throughout this course to put together a study plan for each week in your Professional Data Engineer journey.

### Video - [Weekly Study Goals](https://www.cloudskillsboost.google/course_templates/72/video/517293)

- [YouTube: Weekly Study Goals](https://www.youtube.com/watch?v=PpvOPjtBumw)

Now that you’ve explored all five sections of the exam guide, consider what you’ve learned about your knowledge and skills through the diagnostic questions in this course. You should have a better understanding of what areas you need to focus on and what resources are available. Think about the answers to these questions: When will you take the exam? How many weeks does that give you to prepare? How many hours can you realistically spend preparing for the exam each week? How many total hours will you prepare? Be sure to leave enough time at the end of your plan to retake the diagnostic questions and the sample questions, and fill in any gaps in your knowledge that may remain. Take a few minutes to think about how much time you will allocate to preparing for the exam and note your answers in the workbook. The number of weeks in your preparation journey will depend on various factors, such as your prior experience collecting, transforming, and publishing data in Google Cloud, your comfort level with different products and services, and how much time you have available to dedicate to studying each week. You might choose to focus on specific courses or Skill Badges each week, such as in this sample study plan, or instead focus your study on a specific topic, such as data processing with Dataflow. Once you have a high-level idea of how many weeks that you have to study and how you want to determine your weekly focus, you want to build out a plan with weekly goals and study activities. Use the template in your workbook to plan your study goals for each week. Consider: What exam guide sections or topic areas will you focus on? What courses (or specific modules) will help you learn more? What Skill Badges or labs will you use to practice your skills? What documentation links will you review? What additional resources will you use, such as sample questions? You may do some or all of these study activities each week. Let’s review an example. For example, if you’ve identified using BigQuery for data warehousing as a particular area you need to study, you might choose to structure your study for a week to include targeted modules from the on-demand training, a related skill badge for hands-on practice, and documentation. Alternatively, you might choose one week to complete an entire course, and another week to focus on a skill badge. You can determine the approach that fits your existing skill set. Find the weekly study template at the end of your workbook. Duplicate the weekly template for the number of weeks in your individual preparation journey. Remember, you may need to adjust your plans based on the areas where you need to learn more. For more information about the resources we’ve discussed in this course, refer to your notes and the student copy of the slides. To register for the exam, follow the link on the Professional Data Engineer certification information page, at https://cloud.google.com/learn/certification/data-engineer. And with that, good luck with your preparation journey, and all the very best for your exam!

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
