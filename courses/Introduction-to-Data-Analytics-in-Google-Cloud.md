---
id: 961
name: 'Introduction to Data Analytics in Google Cloud'
type: Course
url: https://www.cloudskillsboost.google/course_templates/961
date_published: 2024-10-15
topics:
  - Data Processing
  - Big Data
  - Data Management
---

# [Introduction to Data Analytics in Google Cloud](https://www.cloudskillsboost.google/course_templates/961)

**Description:**

This is the first of five courses in the Google Cloud Data Analytics Certificate. In this course, you'll define the field of cloud data analysis and describe roles and responsibilities of a cloud data analyst as they relate to data acquisition, storage, processing, and visualization. You'll explore the architecture of Google Cloud-based tools, like BigQuery and Cloud Storage, and how they are used to effectively structure, present, and report data. 

**Objectives:**

* Define data analytics as it applies to cloud computing.
* Explain the benefits of cloud computing.
* Identify common tools used by entry-level cloud data analysts.
* Describe key roles that interact with data within an organization.

## Introduction to cloud computing

In this module, you'll receive an introduction to the Google Cloud Data Analytics Certificate program. You'll meet the instructors and familiarize yourself with the content types. This module investigates the question, "What is cloud data analytics?"—spanning its history through its current defining characteristics. 

### Video - [Welcome to the Google Cloud Data Analytics Certificate](https://www.cloudskillsboost.google/course_templates/961/video/512082)

* [YouTube: Welcome to the Google Cloud Data Analytics Certificate](https://www.youtube.com/watch?v=g_96253Svgg)

There's something happening right now everywhere in the world that's changing all of our lives in every way. It's cloud computing. How can cloud computing make that much change? By connecting us, people, with data quickly, easily, and anywhere at any time. It affects how we communicate, work, shop, plan, and even how we relax and have fun. The cloud is changing people's lives, and it’s also completely reshaping and improving business. These days, data is the cornerstone of all kinds of organizations. They depend on nonstop information about sales transactions, consumer feedback, inventory and purchase orders, customer service interactions, market research statistics, and so much more. Uninterrupted access to business data is a must for organizations, which creates another must: people who can assess that data and put it to work. And this is why the demand for cloud data analytics professionals keeps growing and growing. We need people like you to help organizations understand their customers, collaborate with partners, strategize for the future, mitigate risk, and become more flexible and resilient. The content in this program will equip you with the knowledge and skills required of entry-level roles in the field of cloud data analytics. Hi, I’m Joey! Here, at Google, I am an analytics manager. This means that I lead a team of business analysts whose job is to provide data-backed insights to inform key business decisions. I’m so happy to welcome you to the program! I’m your course one instructor, and I’ll be by your side the whole way through this course. I grew up in a single-parent household in the Los Angeles area, with strong roots in my Mexican-American heritage. Living in a big, diverse, complicated city like LA —while challenging at times— definitely instilled in me a passion for connecting with diverse groups and helped me build interpersonal skills that have been super valuable in life and in my career. My career path wasn't linear or planned, but through an internship and an early career-rotational program, I discovered a passion for data analytics, and specifically using data to help people make better decisions or gain knowledge they wouldn't otherwise have. As an analyst, I want to show folks that data is for everyone, and make technical work less intimidating and more approachable to all. The program is divided into courses based on different cloud data analytics processes. The course topics include an introduction to cloud computing and data analytics, cloud storage and data management, data processing and analysis in the cloud, and visualization of data in the cloud. I encourage you to complete the courses in order, as each topic builds on what you've learned before. The final course is the capstone. It’s a great opportunity to demonstrate the knowledge and skills you gain throughout your academic journey. We’ve got videos and readings to teach you cloud data analytics concepts and skills. Then, interactive activities and labs will let you practice those concepts and skills. You can take the labs more than once, so if you hit some trouble spots, just keep at it! You’ll also have quizzes to confirm your understanding and glossaries to help you prepare to do your very best. And career resources, including resume and interview prep, will help you prepare to apply for jobs and impress hiring managers. You'll hear from Googlers like me working in cloud computing. We'll give you an inside perspective at what it's like in our industry and share personal stories of how we got into the field. Some of these Googlers are going to join me in guiding you through the courses. Let’s take a second to meet them now. [ERIC:] Hello! I’m Eric and I’m a Product Analyst at Google. In course 2, you’ll explore how data is structured and organized. You’ll gain experience with data lakehouse architecture and cloud components like BigQuery, Google Cloud Storage, and DataProc to efficiently store, and analyze, and process large datasets. Next, you’ll meet my colleague Alyx. [ALYX:] Hey there, I’m Alyx, and I’m a Data Analytics Customer Engineer. I’m really looking forward to spending time with you as you learn all about the data journey, from collection to insights. You’ll explore data transformation and practice strategies to transform real data sets to solve business needs. [CJ:] Hi, I’m C.J., and I work in data analytics here, at Google. I’ll guide you through the key stages of visualizing data in the cloud: storytelling, planning, exploring data, designing visualizations, and collaborating with data. You’ll use Looker to create data visualizations and build dashboards. [CHRISTINE:] I’m Christine, your course 5 instructor. Together, we’ll put everything you learned from across courses 1 through 4 into action in a capstone project. And you’ll create impressive work examples to share with future employers. All of us are thrilled to introduce you to the fascinating and rewarding field of cloud data analytics. So, let's get started on your cloud journey.

### Document - [Google Cloud Data Analytics Certificate overview](https://www.cloudskillsboost.google/course_templates/961/documents/512083)

### Video - [Introduction to Course 1](https://www.cloudskillsboost.google/course_templates/961/video/512084)

* [YouTube: Introduction to Course 1](https://www.youtube.com/watch?v=vcJHeRbg12U)

Not too long ago, when a company stored data or ran programs, it needed a huge room filled with a bunch of gigantic, noisy computers humming away right there in the office. But, in the 1960s, a group of engineers asked, “What if we share computing power among many users, so not everyone needs their own computer?” Fast-forward a few decades, and here we are with remote data centers ready to store our data, run our apps, help us with analysis, and so much more. In this course, you’ll start your journey into the world of cloud computing and gain the fundamental knowledge you need to be successful in the field. Whether you’re a cloud newcomer or seeking to level-up your cloud skills, you’ve come to the right place! This course will provide you with a solid foundation in key concepts, skills, and tools used for data analysis with Google Cloud. I started my career as a Philosophy graduate with no professional experience, unsure of how my education would translate into a job. But after being exposed to a few different roles at Google, I found a passion for data analysis and engineering, where a lot of entry-level tasks were like mini-logic puzzles that I was paid to solve. I looked forward to the technical challenges that the role offered, which provided the building blocks for my current career path. I first learned to write SQL in my role as an HR Analyst at Google. One of my first responsibilities was as a primary responder on our team's ticket queue. Each day, we'd receive requests from internal business partners to produce data reports, usually in the form of big spreadsheets with custom logic based on real business questions and problems. I really enjoyed the process of fulfilling these requests: starting with transforming the business request into an analytics problem, using SQL to mold the data into an answer, and providing a dataset that was understandable and approachable to our non-technical users. It felt great to offer a service to our users and give them information that they couldn't otherwise obtain. It was fun. As more organizations adopt cloud-based solutions, there’s a growing need for skilled cloud professionals to help them make the transformation. To get you on your way, I’ll introduce you to the program, let you know what to expect moving forward, and share some great tips for successfully completing the certificate. You’ll learn how to define cloud computing, identify its components, and differentiate between cloud and traditional computing. You’ll explore cloud data analysis compared to on-premises physical data analysis. And you’ll learn about the impact of cloud data analytics on all kinds of businesses with a special focus on the Google Cloud Architecture Framework. Then, you’ll discover the inner workings of data management and the data lifecycle, and the cloud data analyst’s role in keeping both running smoothly. You’ll also explore how cloud professionals collaborate to create some really cool business projects together. Finally, you’ll discover key tools in the cloud data analyst’s toolbox and learn about the importance of process management in cloud computing. As you progress, you’ll be introduced to Google cloud-based tools, including BigQuery and Dataproc. And after completing this course, you’ll know about cloud data tools and be able to understand and communicate cloud benefits, share timely insights, and so much more. I’m so excited to be part of your cloud data analytics exploration and I'm here to guide you every step of the way. Let’s keep the momentum going and head on over to the next topic.

### Document - [Course 1 overview](https://www.cloudskillsboost.google/course_templates/961/documents/512085)

### Document - [Helpful resources and tips](https://www.cloudskillsboost.google/course_templates/961/documents/512086)

### Document - [Lab technical tips](https://www.cloudskillsboost.google/course_templates/961/documents/512087)

### Document - [Explore your Course 1 scenario: TheLook eCommerce](https://www.cloudskillsboost.google/course_templates/961/documents/512088)

### Video - [Welcome to module 1](https://www.cloudskillsboost.google/course_templates/961/video/512089)

* [YouTube: Welcome to module 1](https://www.youtube.com/watch?v=0iI8E-rCksc)

Hey there! Coming up, we have many exciting things to discover about the cloud! Here’s a quick breakdown. First, we’ll check out the basics of cloud data analytics. You’ll learn about its history and explore the many benefits of cloud computing. After that, you’ll consider the differences between cloud computing and traditional computing. This includes their network infrastructures, defining characteristics, and advantages and limitations. Next, you’ll tap into the program resources so you can make a plan to be successful and career-ready. To wrap up, peruse the glossary, with key terms and definitions. Meet you again soon! Hi, I’m Joey! Here, at Google, I am an analytics manager. This means that I lead a team of business analysts whose job is to provide data-backed insights to inform key business decisions. One of my favorite projects has been our support of future workplace design, where we've partnered with architects, program managers, and UX teams to design a new office experience for Googlers. We’ve been able to use data to better understand what Googlers need from the office, and translate those needs into real spaces and services used by close to 200,000 people. I grew up in a single parent household in the Los Angeles area, with strong roots in my Mexican-American heritage. Living in a big, diverse, complicated city like LA —while challenging at times— definitely instilled in me a passion for connecting with diverse groups and helped me build interpersonal skills that have been super valuable in life and in my career. My career path wasn't linear or planned, but through an internship and an early career-rotational program, I discovered a passion for data analytics, and specifically using data to help people make better decisions or gain knowledge they wouldn't have otherwise. I studied Philosophy and worked at a pizza place for three summers! I did a non-technical internship at Google and that turned into a job and allowed me to eventually shift into a technical role. I’ve gotten to try out a lot of different fields like HR Operations, K-12 Education Outreach, and Business Intelligence. My goal is to become a role model to other underrepresented groups. I want to inspire people to discover new passions and achieve what they may not have thought was possible. As an analyst, I want to show folks that data is for everyone, and make technical work less intimidating and more approachable to everyone.

### Video - [Ben: The cloud's impact on education](https://www.cloudskillsboost.google/course_templates/961/video/512090)

* [YouTube: Ben: The cloud's impact on education](https://www.youtube.com/watch?v=1iCEDkleHhI)

My name is Ben, and I'm the senior vice president of Learning and Sustainability at Google. I'd always been interested in learning because for me, my mom, my mother was a school teacher, and I felt that learning is really what enables people to reach a different point than they otherwise would. I know it enabled me to go to a place I could not have dreamt of being, were it not for the education I got. And I think that's incredibly important for people to have access to that kind of opportunity. And growing up in India, I did have access to a good school. I did not come from wealthy family but I had access to a good school, and I saw the difference it made in my life. And I think today, with the help of technology, we can hopefully bring more of that opportunity to more people in the world. The cloud is really important because it's a trajectory of where computation is going. If you think about all of the major products that you use, almost all of them are now based in some online cloud data center, and they have access to all these amazing computing resources, and they enable these services to really provide amazing things for their users. So studying cloud technologies enables you to participate in that whole economy of jobs and of opportunities that consist of building these powerful facilities in the cloud that are being used by people around the world. One of the really interesting ways in which education is evolving is allowing people to build and learn individual skills through various skilling courses. Many aspects of education are not available to everybody everywhere, unfortunately. But it's possible to build the basic skills that one needs from an education more piecemeal today. And I think the approach of skilling can allow people to build up the pieces of the education that they really need in the way that they have access to, in the way that they have time for, in a way that they have the resources for. The initial parts of learning anything are learning the basics and the fundamentals, whether it is a sport or a physical skill like carpentry, whatever. The first steps are learning the basics. So persevere with it and it'll get really interesting. I've been working with computers for, what is it now? Over 30, 35 years, and it is still fascinating every day.

### Video - [Cloud computing infrastructure](https://www.cloudskillsboost.google/course_templates/961/video/512091)

* [YouTube: Cloud computing infrastructure](https://www.youtube.com/watch?v=KYAA1CR4VaM)

Hello, cloud enthusiast! Get ready to learn exactly what cloud computing is all about, including how it works, the components of a cloud infrastructure, and different cloud service models. So, first up, what exactly is cloud computing? Cloud computing is the practice of using on-demand computing resources as services hosted over the internet. “Over the internet” is what makes up the cloud part. It eliminates the need for organizations to find, set up, or manage resources themselves. And they only pay for what they use. Cloud computing uses a network to connect users to a cloud platform. This is a virtual space where they can access and borrow computing services. A primary computer handles all communication between devices and servers to share information. And there are privacy and security measures to keep everything safe. Here’s another way to think about it. Picture cloud computing like a shared kitchen in a rental apartment, owned by a property management company or, in the case of the cloud, a third-party service host. The kitchen has many appliances and cooking tools, just as the cloud platform has servers, storage, hardware, and software. So, when someone in the apartment gets hungry, they just go ahead and cook a meal in the well-equipped kitchen. They don’t each have to buy their own wooden spoons or toaster oven. Likewise, the cloud enables organizations to access computing resources on-demand, without spending time and money buying and maintaining their own storage, hardware, and software. It’s the unique infrastructure of a cloud computing model that makes all of this possible. This infrastructure has four main components: hardware, storage, network, and virtualization. Let’s check out hardware first. Types of hardware include servers, processors, and memory; network switches, routers, and cables; firewalls and load balancers; cooling systems; and power supplies. These are the physical items needed to keep things running. Now, data storage in a cloud computing infrastructure can occur in three main ways: file, object, or block. File storage keeps data in one place and organizes it in a simple, easy-to-understand way through a hierarchy of files in folders. This is the oldest and most widely used data storage system, but it’s a bit cumbersome and can only accomplish so much. Next, object storage holds unstructured data, along with its metadata. Metadata is just data about data. For example, a picture taken with a smartphone might contain information about the location, the date, and the type of device that captured the image. This is really useful for understanding the photo, just as metadata explains what its own data is all about. Lastly, block storage divides large volumes of data into smaller pieces, optimally organized, with unique labels. An advantage of block storage is that the data is easily accessible, but it can be expensive and has limited capability to handle metadata. All right, now we have the network. After all, cloud computing infrastructure needs a way to connect to its backend resources, and this connection is made possible through a network of the physical hardware. Through this network, users tap into cloud resources using some of the hardware mentioned earlier, including routers and firewalls. Basically, the physical network setup is what enables the virtual one to operate. Finally, there is virtualization, which is technology that creates a virtual version of physical infrastructure, like servers, storage, and networks. This is what lets the service work without a connection.

### Video - [Cloud computing service models](https://www.cloudskillsboost.google/course_templates/961/video/512092)

* [YouTube: Cloud computing service models](https://www.youtube.com/watch?v=QmginIC3oWc)

Here, at Google, we have many data centers. A data center is a physical building that contains servers, computer systems, and associated components. These facilities provide a centralized location for vast amounts of data. And skilled cloud analysts access this valuable information right through the cloud! For all sorts of business tasks and projects, cloud analysts select and extract relevant data, then prepare it for processing and examination. They know how to expertly analyze, visualize, and share data discoveries to uncover valuable insights and make smart business decisions. So it’s really important to know that there are three primary models to choose from. Each offers a different level of flexibility and control. These models are infrastructure as a service, or IaaS; platform as a service, PaaS; and software as a service, SaaS. First, IaaS is a cloud computing module that offers on-demand access to Information Technology, or IT infrastructure services, including hardware, storage, network, and virtualization tools. With IaaS, a service provider hosts, maintains, and updates the infrastructure. Your organization would manage everything else: your operating system, your data, and your applications. An IaaS model provides the highest level of control over your IT resources and works a lot like traditional on-premises IT. An example of an IaaS model is cloud storage, like emails you’ve sorted into an online folder. Here’s another example. When someone leases a car, it’s like they’re borrowing it for a while, having fun driving it around, but they have to give it back when the lease agreement is up. Well, IaaS is kind of like that. A user picks the infrastructure they want, uses it for the contracted period, but they do not own it. Next, PaaS provides hardware and software tools to create an environment for the development of cloud applications, simplifying the application-development process. PaaS is all about helping users build apps. So, your organization would enjoy being able to fully focus on app development without the burden of managing and maintaining the underlying infrastructure. Your developers would create, test, troubleshoot, launch, host, and maintain your app all on the platform. PaaS is like hopping into a taxi and telling the driver where to take you. You’re not behind the wheel, but you trust the driver to get you to where you need to be. Lastly, SaaS provides users with a licensed subscription to a complete software package. This includes the infrastructure, maintenance and updates, and the application itself. Other users also have access to use the same services. Using SaaS, you’d just connect to the app through the internet. Think of SaaS like riding the bus —you pick your stop from routes that are set already and you share the bus ride with other people. Remember that these examples are meant to demonstrate the level of individual customization in IaaS, PaaS, and SaaS. They do not refer to any actual hardware or software details. Whew! We’ve covered a lot about cloud computing, infrastructure, and service models. I think we’ve earned ourselves a study snack. I’m gonna go cook something up in my cloud kitchen. Catch ya later!

### Document - [The history of cloud computing](https://www.cloudskillsboost.google/course_templates/961/documents/512093)

### Video - [Benefits of cloud computing](https://www.cloudskillsboost.google/course_templates/961/video/512094)

* [YouTube: Benefits of cloud computing](https://www.youtube.com/watch?v=3D0R4OiMUgw)

I love today's module because I get to do one of my most favorite things: nerd out about the cloud. I'm a huge fan, but I also know that, as a cloud data professional, my enthusiasm level may come on a little strong for folks who don't have a cloud computing background. That's why it's important to really understand the cloud and its many advantages, so that you can explain it clearly to others in a way that is easy to understand, and hopefully exciting! Let’s first learn about accessibility. One of the big advantages of a cloud computing model is that organizations can access and manage data, software, storage, and cloud infrastructure from any location at any time through the internet. They don’t need to be physically present where the hardware and software are installed. And they don’t need their cloud service provider’s assistance when they need more data. Next is scalability, which means to easily expand or upgrade computing resources to meet changing needs. Scalability eliminates physical computing limitations. Now, the benefit of cost savings is pretty straightforward. Organizations only pay for the computing resources used. In a cloud computing model, organizations get what’s called a measured service. Similar to household utilities like electricity and water, users are charged only for what they use, based on the number of transactions, the storage volume, and the amount of data transferred. This helps make all kinds of business initiatives more profitable and sustainable. The advantage of security is also pretty straightforward. With cloud computing, an organization’s systems, data, and computing resources are protected from theft, damage, loss, and unauthorized use. Cloud computing security is generally recognized as stronger than the security of a traditional network infrastructure. This is because data is located in data centers that few people have access to. Plus, the information stored on cloud servers is encrypted, meaning it’s not something that’s easily broken into. Ok, moving on to efficiency. There’s a lot that’s efficient about cloud computing, but one of the main advantages is that organizations can provide immediate access to new and upgraded applications. There’s no time wasted worrying about the state of network infrastructure or going through a costly or time-consuming implementation process. There are tons of amazing things about the cloud! And now we’ve come to freeing resources, so users can focus on more value-added tasks. In the cloud field, we refer to this as managed services. A managed service involves a third-party provider taking care of the ongoing maintenance, management, and support of an organization’s cloud infrastructure and applications. This, in turn, gives users lots more time to focus on other work. It’s like a mechanic who automatically comes to you for annual inspections and services, rather than you spending many hours in a mechanic shop waiting for services. That's because all of the ongoing maintenance and management from the cloud happens automatically in the background; a user doesn’t have to initiate it. Because cloud computing is super versatile, it offers a wide range of common uses including disaster recovery, data storage, and large-scale data analysis that provides users with significant benefits. Let’s start with disaster recovery. Using cloud computing in disaster recovery means having access to more data centers to ensure that data and information is safe and secure during an emergency. The next benefit is data storage. Data storage helps streamline data centers by storing large volumes of data, which enables easier access to the data, analysis of the data, and backup of the data. Then, we have large-scale data analysis. Large-scale analysis offers easy and quick access to multiple data sources, and intuitive user interfaces to query and explore the data. This speeds up the overall process of discovering data-driven insights. Isn’t cloud computing amazing? Users can say goodbye to the limitations of traditional data storage and computing methods, and enjoy the world of advantages that it offers. And data analysts can help users seize these advantages with expert cloud data analysis skills!

### Document - [Introduction to artificial intelligence](https://www.cloudskillsboost.google/course_templates/961/documents/512095)

### Quiz - [Test your knowledge: Foundations of cloud computing](https://www.cloudskillsboost.google/course_templates/961/quizzes/512096)

### Video - [Traditional versus cloud computing](https://www.cloudskillsboost.google/course_templates/961/video/512097)

* [YouTube: Traditional versus cloud computing](https://www.youtube.com/watch?v=9aLfxPzlGgs)

When cloud computing was first introduced, many people resisted the idea of losing physical control over important files, cherished photos, and all sorts of other data. People were used to keeping these things close by, under their own roofs. So let’s use those cherished photos as an example. Putting a photo in an album that you keep on your bookshelf does offer control, convenience, and security to a certain extent. Control can be limited by resources. You need money, materials, and time to print out a photo and purchase a frame or album. You can also only fit so many physical items in your space. As far as security goes, well, that physical memento could be damaged. Now let’s think about how we can enjoy that photo if it’s in the cloud. You can view and share it anytime, anywhere. And if you still want a physical copy, you can make one and feel comfort knowing that you have backup, just in case. Choosing between traditional and cloud computing is also a trade-off. Both have advantages and limitations, and both can have a place in business, depending on what the priorities are. In this video, you’ll learn about traditional computing, how it works, and its defining characteristics. You’ll then compare traditional and cloud computing, which will be valuable to know in the role as a cloud data analyst. So, what’s traditional computing? Traditional computing is a computing model that enables data storage, access, and management through the use of physical hardware and software within a network infrastructure, typically located on-premises. Here’s how that all comes together. First, hardware is set up in a dedicated space or room by IT professionals. Next, the required software —operating systems, applications, and security tools— are purchased and installed. Once the hardware and software are operational, IT personnel are responsible for maintaining and managing the entire system. This infrastructure gives an organization sole control and access to its data and equipment. So, with traditional computing, everything you need is located in one location, on-premises, and can’t be accessed anywhere else. This defining characteristic offers four key advantages: control, security, compliance, and no reliance on the internet. Let’s explore each of these. First, with traditional computing, organizations have full control over their hardware, software, and data. They can customize their localized network infrastructure to meet their specific needs. And because of this control, users usually feel more confident about the second advantage —security— if properly maintained. This is because they have sole access to their systems and sensitive information. Third, traditional computing might be the only viable option if a business is in an industry that requires data to be stored on-premises. This is an example of compliance, which means that a company must follow certain regulations, rules, and laws. In this case, ones that deal with data security. Lastly, traditional computing does not rely on an internet connection when users want to access the network or the data it contains. So, important information can be accessed even if internet service goes down. But just as with our photo album example, there are some downsides. First, with a traditional computing system, data access is limited to the device and location where the hardware and software are installed. Also, scaling up in a traditional computing model is challenging. Software limitations, the time needed to purchase and set up hardware, and the physical space required make it difficult to scale and expensive. Besides scaling up expenses, traditional computing involves buying hardware and software, plus ongoing maintenance of network infrastructure. Lastly, traditional computing can be inefficient, as each user’s software must be purchased, rather than shared. And, again, the software is not automatically updated. These are just some of the reasons why many organizations are moving to the cloud for their computing needs. The cloud is more accessible, scalable, and offers tons of savings. It’s also super-secure, efficient, and frees up staff to work on more projects. It’s picture-perfect! Get it? Ha ha.

### Video - [Cloud data warehouses](https://www.cloudskillsboost.google/course_templates/961/video/512098)

* [YouTube: Cloud data warehouses](https://www.youtube.com/watch?v=OcheWZ9yTPI)

Thanks for joining me as we venture into the wide world of cloud data warehousing! There’s so much data out there, it’s truly dizzying. So it’s no surprise that businesses have struggled to figure out where to keep it all. The fact is, traditional databases struggled to keep up with the evolving demands of data analytics. Luckily, cloud data warehouses are emerging to fill the need. How do they do it? Well, that’s what we’re going to learn about in this video. First, a cloud data warehouse is a large-scale, data storage solution hosted on remote servers by a cloud service provider. To understand this better, picture it like a huge warehouse where large amounts of different types of containers from various places are stored. The cloud data warehouse can collect, store, integrate, and analyze data. There are many advantages to this structure. Cloud data warehouses are typically fully managed by the cloud provider. This means that the cloud provider takes care of various operational tasks and maintenance, allowing users to focus on utilizing the data and gathering insights rather than handling the underlying infrastructure. This saves time, money, and resources. Cloud data warehouses also have more uptime compared to on-premises data warehouses. Uptime is the amount of time a machine is operational. And, of course, only working computers have the ability to scale and support increased demands for data. Next, cloud warehouses can integrate separated data by gathering data from various structured sources within an organization, like sales systems, email lists, and websites, and pulling it all into one place. This integrated data then can be analyzed for some pretty exciting and useful business insights. Another big advantage is that cloud data warehouses provide real-time analytics, ensuring users have quick access to the latest information. And in business, being fast is usually the key to outperforming the competition. Cloud data warehouses also offer some really cool artificial intelligence, or AI, and machine learning, or ML, capabilities. And when you apply AI and ML to your data analysis, this really powers up the possibilities. My team worked on a recent project where we built a predictive model to help Google anticipate demand for office amenities, such as its cafés, and help save money and reduce waste. Using ML tools, we were able to test over 30 factors across months and months of data and build a model that could forecast demand with enough accuracy and time to allow on-the-ground teams to adjust accordingly. Pretty cool, right? Last but not least, cloud data warehouses enable custom reporting and analysis. This means that users can analyze and generate reports specifically from historical data because it is stored on a separate server from data related to current business transactions and day-to-day operations. As you've probably figured out, the types and amounts of data that companies need to organize are only growing, which means so is the demand for data storage. Luckily, cloud data warehouses are up for the challenge with the added benefits of management and analysis to make it even easier to use the data you have.

### Document - [Advantages of cloud data warehouses](https://www.cloudskillsboost.google/course_templates/961/documents/512099)

### Video - [Introduction to BigQuery](https://www.cloudskillsboost.google/course_templates/961/video/512100)

* [YouTube: Introduction to BigQuery](https://www.youtube.com/watch?v=8-c7ynONvr8)

Okay, data enthusiasts, now that we know what cloud-based data warehouses are, we should probably figure out which one suits our needs. And I've got a great one to introduce to you. Meet BigQuery: Google's powerhouse of storage and analysis. An organization’s data is vital to its business success. And data warehousing helps make the most of that data by providing quick and easy access to information, which leads to ideas, insights, and, best of all, data-driven decision-making. BigQuery is a data warehouse on Google Cloud that helps users store and analyze data right within BigQuery. They can query data, filter large datasets, aggregate results, and perform some really complex operations. BigQuery works with SQL, or structured query language. This is a computer programming language used to communicate with the database. It allows users to search through massive amounts of data —and find information they are searching for— incredibly quickly using Google infrastructure. As a cloud data analyst, BigQuery's integrated SQL interface and machine learning capabilities will help you discover, implement, and manage data tools to inform critical business decisions. The output of your work in BigQuery can integrate with typical business intelligence tools or spreadsheets. But there’s a lot more to explore. Another feature is BigQuery’s ability to easily migrate existing data warehouses from other cloud service providers. This is a huge timesaver! One of my favorite things about BigQuery is its dry-run parameter. This lets you check your query thought process and plan before actually running it. And BigQuery will tell you the number of bytes the query will run so you can estimate the cost before actually querying the database. It’s like a practice swing in golf to help you make sure your ball goes in the hole! You can also use BigQuery to store, explore, and run queries on data gathered from servers, sensors, and other devices. And scheduled queries can be used to automatically refresh data and keep tables up to date. Data can be updated hourly, daily, or weekly, so you’ll deliver the most dynamic, timely metrics to your stakeholders. On my team, we use BigQuery almost daily to query, transform, and report on data. Using BigQuery, we're able to tap into a multitude of data sources, which allow us to support our users with the most relevant insights. We use SQL to join datasets and transform the data, creating tables and charts that provide answers. And when we land on an answer that can be useful in the future, we scale it, building self-service reports and dashboards that allow users to retrieve the same answer over and over again in a timely manner. For my team, BigQuery helps us create a bridge between the data that exists and the problems folks are trying to solve or questions they're trying to answer. With its smooth integration with other tools, user-friendly interface, and the use of SQL for effective programming, BigQuery makes the discovery of valuable information within complex datasets simple and productive. It’s an essential part of the cloud data analyst’s toolbox. There’s tons to explore, so have some big fun getting to know BigQuery. This will be an invaluable tool for your cloud career!

### Quiz - [Test your knowledge: Cloud computing versus traditional computing](https://www.cloudskillsboost.google/course_templates/961/quizzes/512101)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/961/video/512102)

* [YouTube: Wrap-up](https://www.youtube.com/watch?v=oRoRhNzOKwI)

It’s been a blast introducing you to the field of cloud data analytics! You’ve learned that cloud computing is an advanced and powerful computing model that resolves many limitations of traditional computing. It also addresses evolving data computing needs of people and businesses all across the globe. Cloud computing provides on-demand availability of computing resources as services over the internet, which offers accessibility, scalability, cost-savings, security, and efficiency. And it frees up your time and resources so you and your colleagues can focus on the kinds of tasks that bring more value to your team and organization. You began this course with an introduction to cloud computing. You then learned about its history, current defining characteristics, and the advantages of using a cloud computing model. Next, you examined the differences between cloud computing and traditional computing, like a physical network infrastructure in traditional computing versus a cloud network infrastructure in cloud computing. You've got this. And remember to celebrate your hard work in a favorite way: a yummy snack, a comfort show, or a touchdown celebration.

### Document - [Glossary terms from module 1](https://www.cloudskillsboost.google/course_templates/961/documents/512103)

### Quiz - [Module 1 challenge](https://www.cloudskillsboost.google/course_templates/961/quizzes/512104)

## Data analytics in the cloud

In this module, you'll explore the difference between traditional data analysis and cloud data analysis as well as the impact of cloud data analytics. You'll also examine various billing models in cloud computing and Google's cloud architecture framework.

### Video - [Welcome to module 2](https://www.cloudskillsboost.google/course_templates/961/video/512105)

* [YouTube: Welcome to module 2](https://www.youtube.com/watch?v=pOJNX8Ly12Y)

Hi there! Welcome to the next section, where you’ll continue learning about data analytics in the cloud! In this section, you’ll explore migrating data to the cloud from on-premises systems, and together, we’ll get into the differences between on-premises, hybrid, and cloud data system architectures. You’ll also learn a lot about the Google Cloud Architecture Framework throughout. You'll witness cloud’s impact on data analytics and many other industries. And you’ll check out strategies for cloud cost optimization and its benefits for users. You’ll also explore the cost of storage, running queries, and resource provisioning, along with the different billing models. This information will help you guide your future employer toward the most cost-effective cloud solution for their particular needs. Meet you in the next video!

### Video - [Steps for effective cloud migration](https://www.cloudskillsboost.google/course_templates/961/video/512106)

* [YouTube: Steps for effective cloud migration](https://www.youtube.com/watch?v=1PkDnk1bkqo)

Anyone who’s been through a move knows that it’s a lot of work. There’s emptying shelves, sorting items for packing or donation, carefully boxing everything up, loading the boxes into a moving van, and then you have to unpack and get everything organized in its proper place once you get to the new place. Whew! An organization migrating its physical computing infrastructure to the cloud also requires careful planning and a great amount of effort to ensure a successful move. Fortunately, third-party cloud service providers like Google Cloud can help make everything easier. In this video, we'll discuss the process of migrating an on-premises computing network infrastructure to a cloud platform. You'll learn the steps to follow, cloud data migration strategies, and important factors to consider during migration. All right, the first step is to think about some key factors. These include choosing the right cloud environment for your organization. Then, think about how much data will be transferred to the cloud. This is important because large amounts of data can take a long time to move, which can delay operations. Next, consider how much downtime your organization can deal with during migration. Obviously, no business wants to shut down their systems any longer than necessary, so this decision should be agreed on by all stakeholders. The next step is to choose your migration strategy. Options include rehosting, also called lift and shift, replatforming, repurchasing, refactoring, or retiring. Let’s break those down. Rehosting is a cloud-migration strategy that involves moving an entire on-premises system to the cloud without changing anything else about the system. An exact copy of the current setup is created in the cloud, which helps the organization quickly achieve a return on investment as they use the enhanced efficiency of their operations, the robust and reliable nature of the cloud infrastructure, and the innovative technologies that are built into cloud-based solutions. Replatforming is a cloud-migration strategy that involves making small changes to the on-premises system once it’s migrated to the cloud. So, the main structure of the system’s applications remains the same, but a few things are improved for better performance. Repurchasing is a cloud-migration strategy that involves moving applications to a new, cloud-based service platform, usually a software-as-a-service platform. The cloud service will be an all-new experience, so this requires some team member training. Refactoring is a cloud-migration strategy that involves building all-new applications from scratch and discarding old applications. This is ideal when organizations need new features, like serverless computing, that their current systems don’t have. Retiring occurs when applications that are no longer useful are turned off. The next step in the migration process is choosing your migration partner. As a cloud professional, you’ll want to help your organization find a cloud service provider that offers the right infrastructure for your particular business, that offers valuable services and tools, and that invests in development to keep things fresh. It’s also important to examine the provider’s customer support and service level agreement, or SLA, so you get reliable and prompt support. Obviously, I’m a big fan of what we do here at Google —especially how we help our partners prepare for cloud migration success! We’ve got something called the Google Cloud Adoption Framework, which helps users assess their organization’s readiness to adopt cloud technologies. This framework acts like a map from current capabilities to their ideal cloud destination. The Google Cloud Adoption Framework evaluates four themes. First, 'learn' refers to the quality and scale of an organization’s learning programs. 'Lead' describes the level of support from leadership given to an organization’s IT department when migrating to Google Cloud. 'Scale' is the extent to which an organization uses cloud-based services and how much automation they need to manage their system. And 'secure' ensures an organization’s ability to protect their cloud environment from unauthorized and inappropriate access. Google Cloud provides a migration path that also has four phases: assess, plan, deploy, and optimize. In the 'assess' phase, users perform a thorough review of their existing network infrastructure. Then, in the 'plan' phase, users set up a basic cloud infrastructure on Google Cloud, where their workloads will exist. When it’s time to 'deploy', the workloads are actually moved to Google Cloud. Lastly, the 'optimize' phase is when organizations begin using cloud-based technologies and features. And this is where they start to really enjoy the improved accessibility, scalability, cost-savings, security, and efficiency. Like any move, cloud migration requires careful planning. As a cloud data analytics professional, the ability to help your organization follow the steps in this video will help ensure everything gets to where it needs to be so you can enjoy your beautiful new place in the cloud!

### Video - [Explore cloud deployment models](https://www.cloudskillsboost.google/course_templates/961/video/512107)

* [YouTube: Explore cloud deployment models](https://www.youtube.com/watch?v=meDkRuMlnS4)

Welcome to this intro to cloud deployment models! We’re going to explore how to advise any organization in choosing the right environment for their unique business needs. There are three primary models: public clouds, private clouds, and hybrid clouds. As a cloud data analytics professional, it will be important to understand how each works. Then, you can help your organization select a model that’s flexible, adaptable, and helps users quickly respond and adjust to changing conditions. First up, a public cloud is a cloud model that delivers computing, storage, and network resources through the internet. In this model, these resources are shared among multiple users and organizations, granting them on-demand access and utilization. Public cloud services are overseen and maintained by third-party cloud service providers, who not only manage the infrastructure but also operate their own data centers. Next, a private cloud is a cloud model that dedicates all cloud resources to a single user or organization and is created, managed, and owned within on-premises data centers. Finally, hybrid clouds are a combination of the public and private models. They enable organizations to enjoy both cloud services and the control features of on-premises cloud models. Think of these three cloud models as different ways to heat a building. Public clouds are like an electricity company that delivers the power you use to generate heat. You can choose to turn it up when you’re chilly or turn it off when it’s warm outside. You only pay for the power used. And, as a customer, you don’t need to worry about the maintenance of the power lines and generators. Private clouds are like having your own solar panels to generate power and heat. You need to buy the panels, have a place to install them, and you’re responsible for their proper care and maintenance. And hybrid clouds are like using the services of an electricity company, but also owning and using solar panels. This gives you more options over how heat is delivered when the temperatures drop. You can choose when the power company is right to use and when solar panels are the better option. Ok, now let’s discuss the advantages and disadvantages of each model. With public clouds, you pick and choose the resources you need, and pay only for what you use. Public clouds can easily scale up or down based on demand. And there are no maintenance worries because the cloud service provider handles all of that for you. Another key advantage is reliability. Public clouds have vast networks of servers and can quickly redirect resources in an emergency. Speed and ease of deployment are also benefits of a public cloud model. Adoption occurs faster and more simply because the cloud infrastructure is already in place. Lastly, public clouds offer new services and frequent updates that enable users to benefit from the latest innovations like artificial intelligence and machine learning, or AI and ML. Now, private clouds come with higher maintenance and management costs, but offer a few critical advantages. The first is that, as the name suggests, they offer private and secure networks if protected properly. Without proper security measures put in place, it can be vulnerable to hackers. Second, private clouds help with any required regulations and compliance because you control where your data is stored and where computing takes place. Private clouds also provide consistent performance because hardware isn’t shared among other organizations. Going hybrid can be a bit tricky, as blending public and private models adds complexity. But there are some key benefits. A hybrid cloud model allows you to add a public cloud provider to your existing on-premises infrastructure, which increases cloud computing power without adding data center expenses. A hybrid cloud model also gives you access to the latest innovations, like AI and ML, which can really be business game changers. Because you choose where your applications sit and where computing happens, there are key security and compliance advantages. Likewise, hybrid computing occurs closer to the actual users, so it enables faster performance. There’s also greater flexibility because you can choose whichever cloud environment is best for the specific task at hand. Whether your organization is best suited to a public, private, or hybrid cloud model, your guidance will help choose a great option. All three offer some really cool ways to advance any organization’s computing power!

### Document - [Benefits of cloud deployment models](https://www.cloudskillsboost.google/course_templates/961/documents/512108)

### Quiz - [Test your knowledge: Cloud data analysis versus on-premises data analysis](https://www.cloudskillsboost.google/course_templates/961/quizzes/512109)

### Video - [Cloud's role in advancing data analytics](https://www.cloudskillsboost.google/course_templates/961/video/512110)

* [YouTube: Cloud's role in advancing data analytics](https://www.youtube.com/watch?v=p2MKGUZ3_Dk)

Keeping track of business data used to be incredibly labor-intensive. From handwritten ledgers to complex filing systems to typing facts and figures into a spreadsheet. Collecting, cleaning, organizing, and storing information was a huge and resource-heavy task. But cloud data analytics has automated and enhanced these processes, making data management much more efficient and less prone to human error. In this video, we’ll dive into how the cloud enables data from a variety of sources to be smoothly integrated, creating a single source that users can access and analyze in real time. First, cloud data can be managed with data integration or data ingestion. Data integration combines data from different sources into a single, usable data source. This integration can happen through the ETL process, or extract, transform, and load; or the ELT process: extract, load, and transform. ETL and ELT are cloud-based approaches that use the power of cloud data warehouses like Google BigQuery to transform data. ETL transforms the data before it’s loaded into the warehouse and ELT transforms it after. But either way, it’s ready for further processing or analysis. Data ingestion obtains, imports, and processes data for later use or storage. The information is obtained from various sources and processed through stream or batch ingestion. Stream ingestion involves real-time continuous processing of data as soon as it is collected from various sources. Batch ingestion processes data in predefined intervals or larger chunks. Those are just a few of the ways cloud data analytics has transformed how organizations access their data. There are also web interfaces, application programming interfaces or APIs, SQL, other ingestion tools like Pub/Sub, and business intelligence solutions, like Looker and Jupyter Notebooks. All of these help users access data that’s stored in the cloud anywhere, anytime. And while we’re discussing data in the cloud, cloud data analytics also makes it possible to store different types of data like files, objects, or blocks. File data is information that’s stored in a file on your computer or another storage device. Object data is a piece of information with a unique identifier which you can find no matter where it’s stored. And block data is a piece of information that has been cut from a larger piece of information and given its own file path. So many data analysis activities have greatly benefited from cloud data analytics processes: big-data analysis, the ability for visualization of multiple data sources asynchronously, AI and ML, custom report analysis, data mining, data science. The list goes on and on! In today's world, innovation is the driving force for many companies. And there’s no doubt that data fuels these innovations. It’s really amazing how much cloud analytics has advanced the field of data analytics, making powerful analytical tools and processes available to organizations of all kinds. At the same time, it enhances the analytics process, making it easier, faster, and more cost-effective for users to discover valuable insights from data.

### Document - [Benefits of cloud data analytics](https://www.cloudskillsboost.google/course_templates/961/documents/512111)

### Video - [Cost considerations of cloud services](https://www.cloudskillsboost.google/course_templates/961/video/512112)

* [YouTube: Cost considerations of cloud services](https://www.youtube.com/watch?v=SgXP4WZg_L0)

Hello, future cloud pro! Thanks for being with me for this rundown on the key features affecting cloud costs: resource provisioning, storage, and running queries. Let’s start with an example. Say you’re headed to the market, so you create a shopping list. You think about what you’ll need in the coming week, then write down exactly those items. The list helps ensure you don’t overspend on items you don’t need, or that might go bad later. Well, managing the costs of cloud data analytics is kind of like that. The key is to be a super savvy shopper, knowing exactly what resources you need, and how much. This saves money and prevents waste. The first method cloud professionals use to achieve these goals is resource provisioning. This is the process of a user selecting appropriate software and hardware resources and the cloud service provider setting them up and managing them while in use. The resource provisioning process occurs through one of three delivery models: advanced provisioning, dynamic provisioning, and self-provisioning. Each delivery model is different based on the types of resources an organization buys, how and when it receives these resources, and how it pays for them. In advance provisioning, the user signs a formal agreement with the cloud service provider and either pays a set price or is billed monthly. Then, the provider gathers the agreed upon resources and delivers them to the user. In dynamic provisioning, resources are adjusted based on the user’s changing needs and they’re only charged for what they use. This means they can easily scale up or down based on usage demands. With self-provisioning, also known as cloud self-service, the user purchases resources from the cloud provider through a website or online portal, and then the resources are quickly made available for the user, usually within hours or even minutes! Although users only pay for the resources they use, how they choose to receive these resources affects how much they pay. Payment can be arranged with one of three rates: fixed, pay-as-you-go, or instant purchase. Now let’s move to storage costs. Storage is ranked as one of the top three cloud expenses for many organizations, and the demand for more storage capacity only continues to grow. Storage costs vary based on data storage, data processing, and network use. As the term suggests, data storage is the amount of data kept in storage. In the cloud profession, we refer to these as buckets. Exactly how much an organization pays for this storage can actually change based on where those buckets are located in the world and the type, or “class,” of the data being stored. For example, coldline storage data is great for data you read or change once a quarter, but archive storage data keeps data that is only meant for backup or archiving purposes, and it’s the cheapest form of data storage. Now, data processing is the step where raw data is cleaned, organized, and changed into a format for easy analysis. Data processing can significantly affect storage costs in several ways, like the more data you process the more storage space you’ll need. And faster data processing requires more advanced storage solutions. Finally, network use refers to the amount of data that is read or moved between storage buckets. Ok, let’s dive into the costs associated with running queries on stored data. One interesting point to understand is that most cloud service providers charge for query processes based on the amount of data processed, not the amount returned after the query is run. So, every time a cloud data analyst runs a query to retrieve, update, or analyze data, a bill is generated based on the total amount of data that query processes. With Google BigQuery, for example, there’s a choice of two pricing models for running queries: on-demand pricing and capacity pricing. On-demand pricing is based on the amount of data, in bytes, that each query processes. The on-demand pricing model is ideal for users whose compute and storage needs fluctuate based on business priorities. And capacity pricing is based on the computing power used to run queries over time, measured in virtual central processing units, called slots. The capacity pricing model is ideal for users who want a more predictable, controllable cost for their queries. As a cloud data analytics professional, you can bring a ton of value to your employer by using this knowledge to get the best performance and value from cost-effective cloud resources and services. Best of all, you'll be helping them optimize resources, processes, and the bottom line!

### Video - [Cloud data analytics in different industries](https://www.cloudskillsboost.google/course_templates/961/video/512113)

* [YouTube: Cloud data analytics in different industries](https://www.youtube.com/watch?v=myIBYs-XYgw)

It wasn’t too long ago that patient charts were kept on premises at a doctor’s office. This created a limitation to information access: Important health information couldn’t be easily shared with other medical offices or hospitals. A patient might not know if they were up to date on vaccinations or be able to access their medical history. Similarly, people working on a manufacturing shop floor constantly had to count inventory parts one by one, entering the totals into a physical spreadsheet. That’s how they knew they had what they needed to get products manufactured for their customers. But today, cloud data analytics is revolutionizing healthcare and manufacturing. In this video, we’ll focus on these industries, and we’ll also dive into the fields of education and transportation. You’ll discover the cloud’s far-reaching impact and learn how you, as a cloud professional, can help all kinds of businesses predict the next big trend, discover patterns that lead to innovation, and make quick decisions that improve operations, systems, and customer satisfaction. Some fascinating new personalized medicine and predictive analytics opportunities are significantly improving patient outcomes, helping people live healthier lives. Cloud data analytics in healthcare lets us ask big questions about many patients’ data. We can learn how a medical product works over time and with different people, or see trends in how often certain medical products are prescribed. And this is just a snippet of the transformative advances that cloud data analytics have had within the healthcare industry. Now, let’s check out cloud data analytics in manufacturing. Today, the manufacturing industry faces challenges like unpredictable demand fluctuations and supply chain disruptions. The industry is also emerging innovative methods of operations, and improving overall efficiency for manufacturers, clients, and customers. The capacity for manufacturing companies to quickly adapt and respond to these conditions is more critical than ever. Integrating cloud data analytics into manufacturing processes is becoming an essential part of the industry. The real-time analysis of massive amounts of data retrieved from manufacturing processes and client and customer interactions helps companies optimize their operations for reliability and efficiency. Analysis of cloud data also offers organizations the ability to build more targeted and innovative products. Cloud data analytics in manufacturing helps keep factory and warehouse operations running smoothly. For example, smart technologies identify issues or need for repairs, and flag things that don’t meet product standards. AI checks quality at every step of production to keep standards high. Cloud data analytics also improves the customer experience by making sure manufacturers produce exactly what their customers want. Plus, it makes the supply chain transparent. You know what’s happening with your customers, partners, suppliers, and even their suppliers. Again, smart technologies can prevent and help track supply chain disruptions. And it helps create products based on customer-driven data, improving efficiency and increasing sales. Ok, let’s review how cloud data analytics impacts the education field. Technological advancements —including cloud data analytics— enable educational institutions to better equip learners with the knowledge and practical skills necessary for career readiness and typical workplace scenarios. For example, consider a university that wants to optimize its course offerings based on student preferences and performance data. By leveraging cloud data analytics, the institution can analyze enrollment patterns, student feedback, and academic performance metrics. Cloud data analytics in education enables a more thorough evaluation of the validity and effectiveness of courses and course content. It also helps educators design learning experiences for each student based on prior interactions with content. Next up, an example from the transportation industry. Transportation companies typically cover large geographical areas and make frequent trips to and from major destinations. This not only requires a large number of vehicles, such as planes and ships, but also careful and strategic planning. By using real-time cloud data analysis for effective decision-making, more efficient routes can be designed that save time, money, and resources. The cloud also helps these businesses predict possible delays, optimize operations, increase reliability, and improve customer service. Cloud data analytics in transportation can improve transport vehicle maintenance. Just as in manufacturing, technologies installed in vehicles monitor their operational status and provide maintenance alerts. Smart technologies also enable AI and ML integration. AI and ML capabilities are used by transportation companies to analyze historical travel data and discover valuable insights that are then used to predict future travel trends. And finally, cloud data enables logistics planning. With cloud data analytics, transportation companies can make the most of the latest virtual modeling software to help plan intelligent logistics and efficient routes. Although we’ve only highlighted four industries in this video, cloud data analytics has had a profound impact on almost any field you can think of. And its impact will only continue to grow, which is why demand for cloud professionals is so high, no matter which industry you choose to pursue in your cloud career!

### Quiz - [Test your knowledge: The impact of cloud data analytics](https://www.cloudskillsboost.google/course_templates/961/quizzes/512114)

### Video - [Overview of cloud architecture](https://www.cloudskillsboost.google/course_templates/961/video/512115)

* [YouTube: Overview of cloud architecture](https://www.youtube.com/watch?v=DCdAsqvGCX0)

Hi there! In this video we’re going to dive into cloud architecture. And to do that, we’re heading to the shopping mall! Ok, so picture a huge shopping mall with all kinds of stores, each selling different products and services. Whatever you need, you'll likely find it. The best part? As a shopper, you have access to all of these cool things without having to worry about the building’s utilities, maintenance, or security. The mall handles all of that. Well, cloud architecture is like a shopping mall. Thinking about this in business, an organization may need to store files. That’s no problem! Need a lot of computing power? Easy as can be. Want to analyze data? Right this way. And behind-the-scenes activities are up to the cloud provider. The organization chooses the services they need, and the cloud provider puts the services to work. Cloud architecture is what builds the cloud. And how the various components and technologies are arranged is what lets organizations pool, share, and scale resources over a virtual network. The components that make up the structure include a frontend platform, a backend platform, a cloud-based delivery model, and a network. Frontend platforms include the parts of the architecture that users interact with: the screen design, apps, and home or work internet networks. Frontend platforms allow users to connect and use cloud computing services. For example, say a user opens the web browser on their mobile phone and edits a Google Doc. In this case, the browser, phone, and Google Docs app are all frontend cloud architecture components. In contrast, backend platforms are the components that make up the cloud itself, including computing resources, storage, security mechanisms, and management. A primary backend component is the application. This behind-the-scenes software is accessed by a user from the frontend and then the backend component completes the task. When you use an online shopping app, you may browse the products first, add them to your cart, and then make a purchase. All these actions are possible because the backend application is at work behind the scenes to complete the shopping experience. Another key backend component is service. You can think of service as the heart of a cloud architecture because it takes care of all the tasks happening in the computing system and manages the resources users access. The third primary backend component is runtime cloud. Runtime cloud provides a space for all cloud services to perform efficiently. It’s similar to your laptop or phone’s operating system and ensures cloud services run smoothly. Runtime cloud technology creates monitors for all services, like applications, servers where data is stored, space for storing files, and network connections. Next, we have storage. This is where cloud service providers keep the data to run systems. Different cloud service providers offer flexible and scalable storage designed to hold and organize vast amounts of data in the cloud. Infrastructure is probably the most well-known component of cloud architecture. It’s made up of important hardware that allows the cloud to work, and it keeps systems at their best, including the central processing unit or CPU, the graphics processing unit or GPU, and the network devices. The final primary backend component is security. Because more and more organizations are adopting cloud computing, it’s essential to ensure that everything is kept safe. Planning and designing security for data and networks provides critical supervision of systems, stops data loss, and avoids downtime. Cloud architecture is the blueprint of how all of these components can deliver a highly agile and scalable solution. Understanding cloud architecture is an important step toward becoming a skilled cloud professional, and you’re well on your way!

### Document - [The Google Cloud Architecture Framework](https://www.cloudskillsboost.google/course_templates/961/documents/512116)

### Video - [Cost optimization for the cloud](https://www.cloudskillsboost.google/course_templates/961/video/512117)

* [YouTube: Cost optimization for the cloud](https://www.youtube.com/watch?v=cypjSk8qbZk)

Hi! It’s great to be with you learning about cost optimization in the cloud. Optimizing costs —in other words, saving money— is something that’s important to all businesses, whether it’s lowering the price of cloud computing or just planning and making smart decisions. As an everyday example, say you were heading out of town for a week. You wouldn’t keep all your lights on, leave perishable food in the fridge, or schedule important deliveries. That would be a total waste of money! Cloud cost optimization is a lot like that. It involves planning and managing resources so you’re not paying for what you don’t need or spending more on services than necessary. In this video, we'll define cloud cost optimization as it relates to using and managing cloud resources, discuss the benefits of cloud optimization, and dive into some cloud cost optimization strategies. Let’s get started! So, first, what exactly is cloud cost optimization? Cloud cost optimization is the process of reducing cloud expenses by implementing cost-reduction strategies. But cloud cost optimization does more than just save money for organizations. It also offers ways to enhance workload efficiency and ensures cloud resources are giving users maximum value. It does this in a few key ways. The first is cost visibility. This means the organization knows exactly what they spend their money on and how specific cloud services are billed. It’s all about being able to justify why a certain amount of money was spent to achieve an operational goal. The next benefit is improved performance of applications. By optimizing cloud resources, organizations can guarantee that apps run smoothly, which improves the user experience while reducing cloud expenses. Another really critical advantage is cutting carbon emissions. Like most business activities, using the cloud affects the environment. So, as a cloud specialist, you can play an important role in reducing your organization’s carbon footprint by optimizing cloud resources. Ok, now that we’ve examined some cost optimization benefits, let’s discuss some strategies that businesses can use to ensure they’re getting the best return on their cloud investments. First, rightsizing. This is the process of adjusting computing resources, like processing power and storage, to fit the exact needs of an application or workload. This optimizes usage. Similarly, auto-scaling is a cloud service that monitors applications, automatically scaling up or down according to the computing resources needed to meet user demands. Finally, reserved instances is a cloud payment model in which an organization purchases a specific amount of resources for a certain time and receives a discount in return for this commitment. Organizations can use cloud resources long term, when they do not expect to have to scale up or down during a specific time period. Then, the organization can benefit from reserved instances. When implemented, these cost optimization strategies significantly reduce expenses, improve app efficiency, cut waste, increase environmental responsibility, and provide a better return on cloud investments. Plus, all of these advantages have the potential to create something any business will enjoy: cost savings. Now, that's a great way to demonstrate the value of talented cloud data analytics professionals!

### Document - [Cost optimization tips](https://www.cloudskillsboost.google/course_templates/961/documents/512118)

### Quiz - [Test your knowledge: Google Cloud Architecture Framework](https://www.cloudskillsboost.google/course_templates/961/quizzes/512119)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/961/video/512120)

* [YouTube: Wrap-up](https://www.youtube.com/watch?v=Ew418FdTh8g)

You’ve learned so much already and are making great strides toward your future career. You’ve discovered the fascinating process of migrating to the cloud. And how important it is to choose the right data system environment. You also learned about the cloud’s impact on both the data analytics field and other industries. Then, you reviewed costs and billing models, exploring cloud cost optimization and its many benefits. You also learned about the costs of storage, running queries, resource provisioning, and the different billing models. This know-how will help you guide any employer toward valuable savings. Lastly, you got an overview of cloud architecture and considered how it’s a blueprint for everything that’s possible in the incredible cloud. Catch you in the next section!

### Document - [Glossary terms from module 2](https://www.cloudskillsboost.google/course_templates/961/documents/512121)

### Quiz - [Module 2 challenge](https://www.cloudskillsboost.google/course_templates/961/quizzes/512122)

## The data lifecycle

In this module, you'll be introduced to the stages of the data lifecycle from data entry to data destruction. You'll learn the benefits of lifecycle management, including data protection and disaster recovery. This module also covers how lifecycle management relates to cost control and compliance and governance considerations. 

### Video - [Welcome to module 3](https://www.cloudskillsboost.google/course_templates/961/video/512123)

* [YouTube: Welcome to module 3](https://www.youtube.com/watch?v=STyYDjykpPE)

Hi there! Welcome to the next section in your intro to cloud computing in data analytics! These topics are all about the data lifecycle —from data entry to data destruction and everything in between. To start, we’ll discuss the basics of data management and introduce safety and privacy considerations. Next, we’ll explore the data lifecycle phases. You’ll experience some really cool examples of data in action. Then, we’ll check out the data analyst’s role in the data lifecycle. You’ll learn about many of the different data professionals you might meet in your career and how they all fit into a data lifecycle. Finally, we’ll cover strategies for controlling data lifecycles, including automation, retention policies, and holds and versioning in data management. There’s a lot coming up, and I’m thrilled to be here to guide you through the next exciting step in your cloud journey! Let’s get started!

### Video - [Introduction to data management](https://www.cloudskillsboost.google/course_templates/961/video/512124)

* [YouTube: Introduction to data management](https://www.youtube.com/watch?v=uxXgZSardto)

Imagine that you have carefully organized a bookshelf expecting it to stay as you arranged it. However, the next day, you find that someone has rearranged the books, maybe borrowed the one you were saving, or even replaced it with a different book. It can be frustrating and confusing when you leave something in one state, but soon find out that it’s been changed or removed. This is especially true for data! For example, maybe a company collects data about its users. Some of it's sensitive, and a lot of it only applies to certain departments. And what if every single employee has access to all of the company’s data? This definitely causes all kinds of problems. Some employees might mistakenly alter the data or save multiple new versions. This makes it pretty easy for someone else to begin working with the wrong information. At the same time, other employees might inadvertently decide to delete some data when they’re done with their own projects. That could be catastrophic! This business needs a data management plan. Data management is the process of establishing and communicating a clear plan for collecting, storing, and using data. A data management plan is created to ensure that procedures for each of these steps are understood by all employees. In some organizations, the data management plan may also be referred to as data governance. There are three main reasons why data management is so important. First, it ensures that collaboration is seamless. All users can access data using documented procedures, so the data stays within governance and compliance requirements. Second, data management supports a strong data security program. It helps set parameters to protect from data breaches or data loss. And third, a plan for data management helps with scalability by having a clear procedure in place. As a cloud data analytics professional, you might create data management plans. Or, you could work within one that’s already been established. Either way, some key aspects to consider include access, data types, storage, and archives. Let’s dive into each of those. First, access. A data management plan defines the roles of anyone who will access the data, including each user’s level of access. This can also include the type of data they have access to. It may even be as granular as certain rows or columns. Along the same lines, the plan should also define the data types the organization is allowed to collect, like personally identifiable information, or PII. For example, maybe only the marketing department should have access to user emails to send notifications. Or, only analysts have access to user birthdates to segment reports by age brackets. Next, a data management plan should include the types of storage allowed. For example, BigQuery projects and Google Drive folders, and backup plans for outages. And finally, archives. Procedures for archiving or deleting data should be considered and follow business guidelines and any external regulations. It’s also important to call out any exceptions to archive rules. For example, litigation data may be exempt from these procedures. Each of these elements of data management are crucial to protecting information and ensuring user privacy. While data management plans vary from organization to organization, they should begin by clearly defining the business objectives. These inform the rest of the plan. Objectives help define what type of data will be collected and which teams can access it. They determine how long data should be saved and how it will be used. And they help prioritize the entire team’s goal to effectively share all relevant data management information. Now, there are some key policies to follow when generating a plan. First, a retention policy may be created for your organization as a whole or for each individual project. Retention policies outline how data is archived and deleted. They may be driven by compliance regulations, legal requirements, and General Data Protection Regulation, or GDPR guidelines. Next, a data-collection policy is an outline that creates rules around how data is collected and what resources may be used in the process. The archival policy outlines where and how data is stored once an analysis project is complete. And lastly, a deletion policy is an outline of when and how data is permanently destroyed. Of course, if employees don’t know about the data management plan and how to access it, that’s just as bad as not having one at all. A business needs to clearly communicate the plan long before there is any data to manage. And a business should be sure to educate all users about the plan, and the specific permissions and procedures. That’s all for now. Stay curious. Hope to continue exploring the cloud with you again soon!

### Document - [Importance of data management](https://www.cloudskillsboost.google/course_templates/961/documents/512125)

### Document - [[Supplemental] Store and use data with SQL](https://www.cloudskillsboost.google/course_templates/961/documents/512126)

### Video - [Safety and privacy in the cloud](https://www.cloudskillsboost.google/course_templates/961/video/512127)

* [YouTube: Safety and privacy in the cloud](https://www.youtube.com/watch?v=f-blLID-dlQ)

Anyone who works with data inherits the responsibility to protect private or personal data, especially personally identifiable information, or PII. This is all about data privacy, which we’ll explore in this video. PII and other privacy standards have legal implications. So it’s important to call out: Our discussion in this course should not be considered legal advice. So what is data privacy? Data privacy is preserving a data subject’s information any time a data transaction occurs. Any organization that collects data knows that earning users’ trust is an essential part of working with their data. As a cloud data professional, one of your primary goals will be to use data honestly, transparently, and fairly. PII is data that is unique to an individual and therefore can be used to distinguish one person from another. A user’s email address, mailing address, phone number, precise location, birthday, and full name are all examples of PII —and must be safeguarded. Along those same lines, personal health data is PHI, or protected health information. PHI is health data that can identify an individual, like information about patient demographics, mental or physical health diagnoses or treatments, and payment records related to health care. It's also important to know about the General Data Protection Regulations, or GDPR. This is privacy legislation for organizations in Europe. It regulates how they can collect, use, and store personal data. It also has requirements around reporting to increase accountability and fines for those that fail to comply. A pro tip is that, even if your employer isn’t located in Europe, the rules may apply to any business that collects data from European citizens. Whether complying with GDPR or trying to follow best practices in PII and PHI data security, there are several ways to help your organization avoid fraud and identity theft. One strategy is identity access management. This is a process that gives each employee who interacts with the company’s system access to specifically defined programs and datasets. As a data professional, you may be required to only access the bare minimum of data needed to do your job. This is usually referred to as NTK, or “need to know.” Accessing data just out of curiosity or for a non-valid business reason may be prohibited. Another proven strategy is having internal data stewards. This privacy team is in charge of keeping data access in check. Frequent audits within your organization are an additional security measure. An audit is a formal examination of how users are accessing data in order to ensure safe and appropriate access while identifying and solving any data concerns. Another way to protect data is through the use of security keys for accessing software, accounts, and datasets. This is an authentication method that uses a physical digital signature, or key, to verify a user’s identity before they access these resources. Finally, encryption is another proven method. This is just the process of encoding information. Encryption protects data by scrambling it into a unique secret code that can only be unlocked with a digital key. Data may be encrypted on an individual machine and unencrypted when you work with it. Or it may be encrypted when it's shared or sent from one machine to another. Having access to data —especially data about people— is a big responsibility! So it’s really important for cloud data professionals to know the best practices of data privacy and safety.

### Quiz - [Test your knowledge: Introduction to data management](https://www.cloudskillsboost.google/course_templates/961/quizzes/512128)

### Video - [Stages of the data lifecycle](https://www.cloudskillsboost.google/course_templates/961/video/512129)

* [YouTube: Stages of the data lifecycle](https://www.youtube.com/watch?v=YO5AhNmh7J4)

Hello, and welcome to this exploration of the data lifecycle! Understanding how data moves through its entire life will help you, as a cloud professional, stay organized and keep your work as efficient as possible. Let’s begin with an example. Maybe your analytics team is assigned a new project. It can be tempting for everyone to want to get right to work. But organizing project phases and creating a plan before you begin helps you get much better results. Likewise, knowing who’s responsible for which tasks creates a streamlined project! Any data project will benefit from understanding the data lifecycle. This is the sequence of stages that data experiences, which include: plan, capture, manage, analyze, archive, and destroy. Let’s dive into each one a bit further. As its name suggests, the plan stage begins before you start any analysis. Usually it involves answering a particular business question or meeting an objective. It's important that all project outcomes are focused on this key point. A typical business question is “How can we increase user engagement with our product?.” And an objective might be “We will reduce our plastic use by 50 percent by using only recycled materials in our package design.” Next, team members decide what data types to collect, which data management processes to follow, and who is responsible for each stage. During the planning stage, the ideal project outcomes and how to measure success are defined. Next is the capture stage, when data is collected from different sources. This may include outside resources, like publicly available datasets, or it may be data that’s generated from within the organization. This stage is also when gaps are identified in an organization’s current data collection and are improved through iteration. For example, maybe business leaders want to know how many users have downloaded their app. But when reviewing the existing data collection plan, an analyst notes that they only capture data about how many users have created an account, but not always how many app downloads have occurred. The analyst may need to work with the engineering team to figure out if it's possible to get this data logged. Then, it’s time to process the captured data. This is important because raw data directly from the source is usually not in a usable format for analytics. So, a data engineer needs to clean the data and transform the data. They may also compress the data and encrypt the data for security purposes. Now, we’ve come to the manage stage. The goal of this stage is to ensure proper data maintenance. This includes safe and secure data storage. Keep in mind that the manage stage is an ongoing process throughout the project. In other words, the raw data collected in the capture stage, the transformed data from the process stage, and any business logic that analysts establish all must be managed securely on an ongoing basis. Next, is the analyze stage. This is when analysts use data to answer the business question or help meet the business objective generated during planning. At this point, team members review the compiled data to find trends, create visualizations, and suggest business recommendations based on data insights. Then, in the archive stage, the data engineer may store data for later use, if needed. Finally, when data is no longer useful to the organization, it's destroyed. Although it might seem like a waste to destroy data, this is an important stage to ensure that sensitive data cannot be stolen, to support privacy protection guidelines, and meet other compliance requirements and regulations. For example, this may keep data practices in compliance with GDPR, the General Data Protection Regulation, in the European Union. This regulation covers the data that an organization is allowed to hold. Ok, one final point about the data lifecycle: None of these stages work without data team members. You just learned that data analysts primarily work during the analysis stage. They collaborate with data architects, who design the plan for managing the data. Data engineers build the infrastructure for the data. And data scientists use the data to create models to understand the data. With these talented people in place, your organization will have a point person for each stage of the data lifecycle. And everyone will have a clear idea of where projects and data are headed!

### Document - [Summary of the data lifecycle](https://www.cloudskillsboost.google/course_templates/961/documents/512130)

### Video - [The data lifecycle in action](https://www.cloudskillsboost.google/course_templates/961/video/512131)

* [YouTube: The data lifecycle in action](https://www.youtube.com/watch?v=XUgOES01oZY)

Hello, and welcome to this video about effective data reporting! Being able to share data insights with others is an essential part of data analytics. After all, an amazing insight to the business question is only valuable when the information is shared! As a cloud data analyst, you'll likely create a variety of reports to share your analysis and provide summaries for decision-makers. The data lifecycle —plan, capture, manage, analyze, archive, and destroy— will help you create engaging, data-driven communications. Now, before we go further, it’s important to note that the data lifecycle and the data analysis process are two distinct concepts. When we think about the data lifecycle, this is how the data itself moves and changes throughout its existence. When we think about the data analysis process, this relates to how data analysts interact with the data. Now that we’ve covered that! Let’s dive into the data lifecycle. Let’s shift to an on-the-job example with a data analyst, named Kayla. A stakeholder at Kayla’s company requests a report of all payments the company received in the last month. They would like to analyze several invoices to ensure they're not missing any payments. The stakeholder also wants to see the percentage of payments that were made on time and the payment methods. To generate this report, Kayla will motion through the entire data cycle. The first step is the plan stage. So, Kayla will review the business questions and objectives. Then, she will determine what fields need to appear on the report. Kayla considers questions like what filters are needed to extract or separate the data and how far back in time should the data go. Now, she moves on to the capture stage. Here, Kayla will implement the plan she’s created and assess the metrics she needs to capture. Then, Kayla will confirm the data is available and then gather the data. Data can come from a variety of sources. In the scenario of the payments report, Kayla will need user information and payment details. Sometimes the data you need doesn’t exist. If that’s the case, you may need to work with the product or engineering team to figure out how to fill in the gaps. You’ll also want to establish a reasonable timeline for the data export. Let’s get back to Kayla’s process. The manage stage is all about data management. In this stage, Kayla considers how to store the data and prioritize security. Kayla will also determine whether she’s working with any PII, or personally identifiable information. When sensitive data is involved, it’s critical to take steps to protect people’s privacy. The report Kayla is generating will likely include user credit card numbers, addresses, and phone numbers. If she determines this information to be necessary for her project, then she will ensure it’s secure. Otherwise, she will omit the PII at the start and simplify her data security needs. Kayla can also ensure that the data is accessed on a need-to-know basis so that only people who need the data can access it. In this phase, Kayla can also perform quality checks to confirm no required values are null or missing. In this payments report project, let’s say she has multiple physical stores collecting payments in addition to an online website. So, it’s important to check that the same fields are using the same kind of data so everything combines seamlessly during table joins. For example, the field “account number” will need to match to the correct user account. And both need to match in field types like string values, for example, CUST12345. Next, it’s time to analyze the data. In this stage, Kayla uses the data to solve problems and support business goals. She can do this through the collection, transformation, and organization of data in order to draw conclusions, make predictions, and drive informed decision-making. One of the most exciting parts of the analyze stage is sharing reports of your insights! Now, Kayla will create visualizations to help others understand the insights. She will also go back to the security protocol to ensure that she’s not sharing any PII with people who should not have access. Next, in the archive stage, Kayla will determine what needs to be saved. She may need to save data to share it with additional stakeholders later on. Or, she might save it to compare with other reports. Finally, once Kayla determines she doesn’t need the data anymore, she moves on into the destroy phase. Destroying data completes the cycle! Congratulations, you’ve just experienced the whole data lifecycle! You’ve met your business objectives and kept all of your company’s data safe and sound.

### Document - [Activity: Design a data lifecycle plan](https://www.cloudskillsboost.google/course_templates/961/documents/512132)

### Quiz - [Activity Quiz: Design a data lifecycle plan](https://www.cloudskillsboost.google/course_templates/961/quizzes/512133)

### Document - [Activity Exemplar: Design a data lifecycle plan](https://www.cloudskillsboost.google/course_templates/961/documents/512134)

### Quiz - [Test your knowledge: Introduction to the data lifecycle](https://www.cloudskillsboost.google/course_templates/961/quizzes/512135)

### Video - [Common roles on a data team](https://www.cloudskillsboost.google/course_templates/961/video/512136)

* [YouTube: Common roles on a data team](https://www.youtube.com/watch?v=K3CJLVkHa_w)

Have you ever played on a sports team or experienced a live sporting event? For example, take the sport of basketball. In basketball, there are two teams, each with five players. The players on each team work together, trying to score points by throwing a ball through a net. They're all basketball players, but each has an individual position and distinct role to play. The same is true for a data team. Everyone’s on the team and each one plays a distinct role when completing a project. In this video, you’ll discover the interesting jobs of people who work with data. Based on what you learn, you may even decide to pursue one of these new career paths instead of cloud data analytics. That’s great! Whatever direction you end up taking, knowing all about these different jobs will be invaluable. First up, data analysts. These are truly dynamic people with a wide skill set. They know how to seamlessly switch between roles on a data team, acting as a data detective, a translator, even an artist. Data analysts blend the science of analytics with the art of storytelling. They work directly with the data. Data analysts perform data analysis workflows, including importing, manipulating, calculating, and reporting on business data. When reporting, they visualize and communicate results to stakeholders,which includes creating presentations and dashboards for sharing data. Data analysts also perform statistical analyses. In working through a data cycle, some of the tasks they may complete include analyzing and reviewing databases, using scripting languages like Python or SQL to query data, creating visualizations in dashboards in programs, maybe Tableau or Looker, and presenting findings to stakeholders. Data analysts also play a role in the management and sharing of data, under the direction of a data architect or engineer. They work with data engineers to clean data and ensure that it's focused on answering the business questions generated during the planning stages. If all of this sounds exciting to you, then put on your detective’s hat, start translating information, and choose your artist’s paint palette. This just might be the career path for you! Ok, let’s meet the data engineer next. A data engineer is a professional who transforms data into a useful format for analysis and gives it a reliable infrastructure. They design, create, manage, migrate, and troubleshoot databases used by applications to store and retrieve data. Much of their work involves developing the structure the team uses to collect, store, and analyze data. The data engineer is also the one who builds the data pipelines. Pipelines are a series of processes that transport data from different sources to a destination for storage and analysis. Engineers also test and maintain pipelines once they have been built. Data engineers are also tasked with the important job of ensuring that data is accessible to all other members of the team, as permitted by their company’s access policies. Like all members of a data team, engineers must be effective communicators. They collaborate with team members and other stakeholders to ensure company objectives are met. It’s an exciting career path that makes a powerful impact on everyone in the organization! Next, we’ll get to know data scientists. These data professionals analyze data, do statistical analysis, and usually build and train machine learning models. The data scientist is someone who works primarily in data research, identifying business questions, collecting data from multiple sources, organizing it, and finding answers. Now, let’s consider how a data scientist and a data engineer are alike, and how they differ. Like a data engineer, a data scientist uses coding and scripting skills to clean and summarize data. They differ in that an engineer typically builds a data pipeline from start to finish, and a data scientist uses the data from the pipeline to draw conclusions. Data scientists create data analysis workflows. Along with data analysts and engineers, data scientists import data from various sources, clean it, and use data to perform calculations. They also understand the business applications of data, and focus on statistics, machine learning, and modeling. The role of data scientist is another wonderful option for your own data career. All right, last but definitely not least, we have data architects. These professionals collaborate with data analysts, data scientists, and data engineers to design the infrastructure of the database. The data architect helps their team to plan the overall data lifecycle of a solution. The data architect will usually produce a data or solution architecture diagram to support the management of a data lifecycle in collaboration with the analyst and scientist. Then, they relay it to the data engineer to build the database! Every business is different, but those are some of the most common data team roles. However you make your way into the field, keeping your career options open will allow you to grow and make your own valuable contributions!

### Document - [Overview of data team roles](https://www.cloudskillsboost.google/course_templates/961/documents/512137)

### Video - [Safa: Data analysts are essential in the cloud](https://www.cloudskillsboost.google/course_templates/961/video/512138)

* [YouTube: Safa: Data analysts are essential in the cloud](https://www.youtube.com/watch?v=L2BxttzneYI)

Hi, I'm Safa. I'm a product analyst at Alphabet. Doing work that feels meaningful is what excites me the most about my job. One thing that my parents did give me.. . Their intentions in giving me a computer was for me to do well in my studies so that I can research things, do my homework better. But what they didn't realize was that the computer was a vortex into online connections. And I think just by virtue of spending so much time on the computer, I found myself really good at tech skills. When it came time to going to college and deciding, "Oh shoot, what do I wanna study now?," I knew I wanted to do something within tech. I think cloud is an exciting career space because if we're talking, especially around data analytics and we're thinking about where we can host data on the cloud. There's gonna be trillions of records of data on the cloud and we're gonna need folks who have that analytics or data knack to be able to know how to report on it, to be able to know how to engineer ways for that data to live on the cloud. And so I think if someone is interested in being a part of the cloud, especially as a data analyst, I feel like there would be a lot of opportunities to mend those two ways to together. Cloud makes it possible for a lot of people to do their job well, (chuckling) so it is an exciting place to be at. In my job, especially as a data analyst, it's really important to have a reliable and robust way for having our data organized, for having our data able to be accessed, to have an intuitive way to query and report on that information. The cloud makes that possible for us because we're able to use tools like BigQuery that allows me to query that data and find where those tables are accessed and it creates like a platform for us to have our data warehouse on it. And it's fast and it makes it a lot easier. And I've been in jobs before where we didn't have something like that. I think a lot of folks, especially at Google, 'cause everyone is so smart and just so hardworking, I think we all probably are going through something or another and doubting ourselves and feeling like, "I'm not as smart as a person next to me. I'm not as hardworking as a person next to me." When I had first started at Google, I mean I, like many other folks, I had so much imposter syndrome. I thought they made a mistake hiring me. 'Cause I was not the traditional person that I had attributed as someone who gets hired into Google, which is the Harvard graduate, the person who got amazing grades, who was valedictorian. And here I was, high school dropout. If you want a product to reach multiple people as Google products do, then you need diverse people working on it. So yeah, I think it's super important for folks of non-traditional backgrounds to have their voices be heard and their opinions to be inputted and woven into products because it makes for a better product. A piece of advice that I would share for folks doing the certificate when moments of self-doubt come up, is reminding yourself that you are incredible. Trusting that gut instinct that led you to this certificate in the first place is important to hear and honor. I feel like for me, coming back to that place of, there's a reason why I'm doing this and I know I like this stuff. And I know myself well enough to know that this is just a momentary wave and if I ride this wave through, I'll go back to that same place of feeling excited by this topic again.

### Document - [[Supplemental] Overview of data analysis and business intelligence](https://www.cloudskillsboost.google/course_templates/961/documents/512139)

### Quiz - [Test your knowledge: The data team's role in the data lifecycle](https://www.cloudskillsboost.google/course_templates/961/quizzes/512140)

### Video - [Importance of automation in the data lifecycle](https://www.cloudskillsboost.google/course_templates/961/video/512141)

* [YouTube: Importance of automation in the data lifecycle](https://www.youtube.com/watch?v=bMjDYSgkQsY)

Business demands are always changing and, as a data professional, being able to adapt and respond is invaluable. Sometimes data professionals will wish they can multitask or be in two places at once. This actually applies to many jobs! For example, consider a farmer. When the farm is small, a farmer might water crops by herself every morning. As the farm grows, the farmer no longer has time to water the crops as she once did. She wants to figure out how to water a huge field’s worth of crops efficiently. How can she possibly achieve this? She invests in a timing system that waters her crops at the same time every day for a preset interval. This means that all of her crops are watered simultaneously and she harvests a record number of crops. In data management, you’ll sometimes encounter similar situations! You might need to ingest data on a set schedule or you may need to automatically cap the amount of data ingested. You might be asked to be more efficient with a set amount of resources. There are many ways you could be asked to scale or streamline data processing. In this video, we’ll explore how data management systems can be implemented and automated to respond to ever-changing business needs. Automation is the use of software, scripting, or machine learning to perform data analysis processes without human work. For example, consider a business that has over two hundred employees to pay twice per month. Someone could verify the hours worked for all employees. Then, another person could calculate the amount of each paycheck. And a third person could write and sign the checks. Most businesses don’t operate this way! Instead, the payroll process is usually automated. Verification of hours worked and calculation of pay is all done automatically by a computer program. In some cases, payment is automatically transferred to each employee’s bank account by an automated process. Automation saves time and resources —and ensures that people are paid on time! One of the key benefits of automation is that it reduces mistakes during data collection through automatic checks for errors or incomplete data. This helps ensure your dataset is complete and accurate. In situations where compliance is key, automation ensures uniformity and accuracy important for compliance. Automation scales up workloads —ingesting or processing more data at once— without losing key information. Finally, automation allows for efficiency and process improvement, which can help to preserve resources —both human and technical— and controls costs. Having a clear process for moving data through a cycle makes it easier to control your outcomes. With automation added at key areas, you'll be able to move data through its lifecycle quickly and efficiently. Now that you’ve automated repetitive tasks, this frees up your time to focus on the meaningful work of data analysis!

### Document - [Types of automation for the data lifecycle](https://www.cloudskillsboost.google/course_templates/961/documents/512142)

### Video - [Introduction to data retention policies](https://www.cloudskillsboost.google/course_templates/961/video/512143)

* [YouTube: Introduction to data retention policies](https://www.youtube.com/watch?v=BMrD7O5ukxs)

Have you ever cleaned out your closet and donated some clothes to a charity? Or maybe you’ve cleared out old pictures from your smartphone so your phone is faster and more efficient. Now, what happens if you donate a sweater you actually meant to keep or deleted a photo that you really wish you still had? Whatever type of cleaning you’re doing, it’s important to have a plan in place so you don’t accidentally throw away something you need! A clear process for cleaning, or for deleting items, makes it easier to keep track of everything. In your role as a data analyst, you'll likely encounter something called a data retention policy at your organization. This is a key part of data management, which deals with how and when data is saved or deleted. Data retention is the collection, storage, and management of data. Every organization that collects data should have a policy for its retention. These retention policies may consider internal needs, industry regulations, and laws that apply to the particular business. A data retention policy should clearly define the scope of the policy and the reason for saving information. It also defines whose data may be collected, both internally and externally. And, as applicable, the policy may list any laws and regulations the organization follows. The policy provides a detailed description of the organization’s data retention process. For example, a schedule for saving and deleting data, rules for keeping data safe, and processes for destroying data. It also includes guidelines for data breaches. Other guidelines that may be a part of a data retention policy include the format of the data, whether the organization archives or destroys data, and who has authority to make decisions about deletion. Now that you have an idea of what goes into the data retention policy, let’s review some of the regulations that may affect your data, depending on the industry you work in. A business may need to adhere to the following regulations or standards. It’s important to call out: Our discussion in this course should not be considered legal advice. If it’s a publicly traded company in the United States, it must adhere to the Sarbanes-Oxley Act for data retention. Sarbanes-Oxley requires organizations to maintain internal control systems that protect financial data of consumers. A business that accepts credit card payments must have policies that follow the Payment Card Industry Data Security Standard (PCI DSS). This standard requires organizations to build secure networks, protect cardholder data, and implement strong access control measures. A business must also regularly monitor and test all of these elements. Certain healthcare organizations in the United States are subject to the Health Insurance and Portability and Accountability Act (HIPAA), or HIPPA. HIPAA establishes requirements to protect an individual's identifiable health information. And any organization that processes personal data of European Union citizens, even if the organization is based outside the European Union, must follow the General Data Protection Regulation, or GDPR. GDPR requires that reasons for personal data collection must be transparent, and a business may only use data for the stated collection purpose. GDPR also provides European Union citizens with certain rights and requires that businesses securely store personal data. When using a cloud service like Google Cloud, a data professional will usually activate a Bucket Lock feature. A data professional can use this as a tool to create a data retention policy for each storage bucket and define how long it will be retained. Google Cloud’s Bucket Lock also provides access to a detailed audit log, which gives users a clear picture of data requests and responses. This helps a business maintain compliance with policies about WHO accesses data and when. Finally, object lifecycle management is an automated way a data professional can manage a data retention policy. In the cloud, an “object” is data stored in a cloud object storage service. The cloud service provider stores the data in large chunks with associated metadata. This management process enables a data professional to create parameters for object storage, to delete objects or DATA, or move it into different buckets. With predefined parameters, automation makes it easy for the business to keep track of important information! You can use the strategies in this video to make your data more organized and secure. You’ll keep what you need for as long as you need it. And get rid of what you don’t to keep processes moving efficiently for speedy data insights!

### Video - [Overview of version control and holds](https://www.cloudskillsboost.google/course_templates/961/video/512144)

* [YouTube: Overview of version control and holds](https://www.youtube.com/watch?v=sm5Kn5oH83o)

Have you ever worked on a document and saved multiple versions of it along the way? It can be really difficult to find the most up-to-date version. It’s also sometimes challenging to keep track of a document while multiple people are working on it at the same time. And, worst-case scenario, someone might delete the doc altogether, thinking it’s no longer useful! In this video, we’ll discuss version control and holds as effective ways to ensure a document’s current version is the most accurate for all document editors. In the data analytics field, data professionals use versioning and holds to keep data current and make sure everyone has access to the accurate version. Let’s start with versioning. Versioning is the process of creating a unique way of referring to data. This may be through an identification number, a query, or date and time identification. The main goal is to label a file or dataset with the date and time that it was created. This means that, even if you have multiple versions of the same data, you can always refer to the date and time to ensure you’re working with the most recent version. This is especially important if you’re on a team where multiple people have access to the same files and may be creating versions. The process of versioning can also include saving a dataset with a new name or saving data in a new filepath. With clear directions about how and where to save files, you can track current versions with all of your teammates. There are several benefits of versioning to help ensure a data professional can perform accurate and reliable tests on the data. Along with making sure you have the latest version of data, versioning can also support quality control. Sometimes there may be an editing error, or an error in data import. Being able to revert to a previous version allows you to return to the most recent accurate dataset. And it’s valuable when addressing compliance concerns. You can version data that lives in databases or warehouses and cycle it out by version. Plus, sometimes when you find an error in code, you can go back to the original code to find where edits are incorrect. Ok, now let’s move on to holds. A hold is a policy placed on a dataset that prevents its deletion or prevents deletion capabilities for certain accounts. Sometimes, a dataset will have an organization-wide hold, meaning no one can delete it. There are several benefits of holds. Most importantly, they prevent accidental deletion and preserve data indefinitely, which means someone can’t remove data that may be needed in the future. Holds policies will allow the business to determine and define each user’s responsibility for datasets. For example, some users may have access to view and edit data, but are prevented from deleting it. A business may put both versioning and hold policies in place if there are security concerns about sensitive information. A business may also create policies to help manage storage costs. Ultimately, both versioning and holds support increased collaboration, accuracy, and better business results. Implementing these policies is just one way that data professionals help their organizations achieve data excellence!

### Document - [Versioning and holds for accurate data](https://www.cloudskillsboost.google/course_templates/961/documents/512145)

### Quiz - [Test your knowledge: Control data flow in the cloud](https://www.cloudskillsboost.google/course_templates/961/quizzes/512146)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/961/video/512147)

* [YouTube: Wrap-up](https://www.youtube.com/watch?v=KvgFr3Z-TCM)

Nice job! You’ve made it to the end of this section! Before moving on, let’s review what you’ve learned in this section. You explored data management when working on a data project. You also found out about data teams and data cycles —and how they work together. Now, let’s review some specific key points. You learned about the importance of data security and safety, and retention and deletion policies. Then, you explored the stages of the data lifecycle. And you considered the roles in the data lifecycle you might play, plus where your team members make their own contributions. You then went on to identify how automating the data lifecycle can resolve issues and boost efficiency. Finally, you identified strategies for safeguarding and controlling data, including retention policies, versioning, and holds. Congratulations on all of your progress so far! I can’t wait to welcome you to the incredible field of data analytics.

### Document - [Glossary terms from module 3](https://www.cloudskillsboost.google/course_templates/961/documents/512148)

### Quiz - [Module 3 challenge](https://www.cloudskillsboost.google/course_templates/961/quizzes/512149)

## The role of a cloud data analyst

In this module, you'll explore the key differences and similarities between data analytics on-prem and data analytics in the cloud. You'll also be introduced to the role a data analyst plays in an overall organization, who they will interact with as a data analyst, and key aspects of process management for answering business data questions. 

### Video - [Welcome to module 4](https://www.cloudskillsboost.google/course_templates/961/video/512150)

* [YouTube: Welcome to module 4](https://www.youtube.com/watch?v=3S3z19pa9PE)

Hi there! Welcome to the next stage of your cloud data analytics journey. In this section, you’re going to dive deeper into data analyst roles and responsibilities; cloud data tools; and data, identity, and access management. You’ll first check out the unique ways the data analyst's role operates in cloud data analytics. Then, you’ll learn more about other key team members who interact with data. This will help you collaborate like a pro! Next, you’ll explore Google Cloud’s data and analytics portfolio. You'll get a high-level introduction to the tools: BigQuery, Looker, Dataproc, and Dataflow. Your final section will focus on data access management. You’ll understand why this is an important part of the data analyst’s role. You‘ll also learn about identity access management. We’re going to cover a lot in this section, and it will be a great glimpse into the day-to-day activities of your future role in the cloud. Let’s do it!

### Video - [Data analysis in the cloud](https://www.cloudskillsboost.google/course_templates/961/video/512151)

* [YouTube: Data analysis in the cloud](https://www.youtube.com/watch?v=MtpLCaQaKls)

Wherever your data journey may take you, the cloud is going to have a big impact on your data career. To make the most of all the cloud has to offer, you’ll need some specialized knowledge about many of the unique aspects of data analysis in the cloud. And that’s what we’re going to explore in this video. First, let’s understand cloud data analytics. Cloud data analytics is the process of analyzing and extracting meaningful insights from large volumes of data using cloud-based services and solutions. It enables data professionals to analyze data —and use services and systems— all hosted on the cloud. Cloud data analytics allows analysts to access large amounts of data from multiple sources without having to worry about setting up the infrastructure and security for each dataset. Even for organizations that also have data on premises on physical servers, with cloud data analytics, a major portion of their data analytics is hosted in the cloud. This includes the data itself and the systems used to manipulate and analyze it. Let’s check out some of the key advantages of working with data in the cloud. One big benefit is having quick and easy access to real-time data from different sources. Plus, analyzing large datasets is simplified because computing and processing power don’t rely on local machines. Plus, data professionals can aggregate and analyze data right in the cloud. This is especially valuable, as cloud data analytics continues to have a huge impact on website data, sales data, financial data, and performance data roles. For example, if a web server is hosted in the cloud, much of the data will be generated right there. So, it makes sense that aggregation and analysis also happen in the cloud. Here are some specific ways data analytics works in the cloud: sentiment analysis and customer segmentation. On a sentiment analysis project, you can monitor the feelings of customers, employees, and competitors on social media. Using cloud analytics, you can bring in data from all major social media platforms and create summaries of themes in feedback. When working with customers, you can use cloud analytics for customer segmentation to group customers by their behaviors, needs, and preferences. With cloud databases and tools, you can automatically ingest large amounts of data and train your system to generate themes and summaries. Much of this is made possible by a cloud-based data pipeline. Through that pipeline, information moves seamlessly —from creating data to archiving data. Pro tip: In a professional setting, some data analysts may never have to access data based on physical or on-premises hardware at all! In your career, you’ll probably build SQL data pipelines in a variety of cloud-based tools, like BigQuery. You’ll create visualizations in Tableau or Looker and maybe use predictive analytics from the latest artificial intelligence offerings. You might even find yourself creating exciting machine learning models! Of course, you’ll perform the typical analytics tasks of data ingestion, cleaning, and transformation. A benefit to hosting versus on-premises is that, with hosting, you’ll have cloud tools and platforms to help you with those tasks! Whether your future role lives on-premises or in the cloud, you now know how the cloud affects many facets of the data analyst’s daily work. This know-how will help you maximize the cloud’s amazing tools and solutions!

### Document - [Data analyst job responsibilities](https://www.cloudskillsboost.google/course_templates/961/documents/512152)

### Video - [Overview of Google's cloud data analytics tools](https://www.cloudskillsboost.google/course_templates/961/video/512153)

* [YouTube: Overview of Google's cloud data analytics tools](https://www.youtube.com/watch?v=qUOwo62Hd3o)

As the cloud continues becoming a more prominent reach in the data world, all kinds of exciting new data tools will arise. As a data professional, it’s important to know that cloud data tools will usually be specific to each stage of the data lifecycle. And there will probably be a lot of choices for each stage. So, how do you choose the right solution for a particular task? In this video, we’ll introduce you to some of the tools offered by Google and explore how they can support many types of projects and initiatives. First is BigQuery, a serverless data warehouse. This tool is pretty popular in the data world because it works across many major cloud platforms. With BigQuery, you use SQL to query and organize data. There are also some really cool machine learning and artificial intelligence tools integrated within the platform. And BigQuery includes built-in business intelligence engines to create interactive and responsive data insights. As a data analyst, you may use BigQuery to process seemingly unlimited amounts of data —think terabytes and even more terabytes— without having to manage the database. Another solution that may become a part of your toolkit is Looker. Looker organizes business data and builds workflows and applications based on data insights. But it’s primarily a data visualization and reporting tool. As a data analyst, you might use Looker to integrate various file types like CSV, JSON, and Excel into a single application. You may also use it to publish data in a variety of dashboards. Next is Dataproc. This service is fully managed and allows you to run Apache Hadoop, Apache Spark, and Apache Flink, along with many other open-source tools and frameworks. Dataproc can modernize data lakes and perform ETL functions —or extract, transform, and load— for massive amounts of data. As a data professional, you might use Dataproc to process large data workloads and place them in clusters that allow you to access your data quickly. Then, there’s Dataflow, which gives you the ability to stream and batch process data in a serverless application. Data professionals may use Dataflow to develop data processing pipelines. It can assist with reading the data from its original source, transforming it, and writing it back to your database. You may also work with Cloud Data Fusion, another fully managed service that allows you to integrate multiple datasets of any size. As a cloud data analyst, you might use Cloud Data Fusion because it allows you to use a graphical user interface instead of code to manage your data pipelines. Dataplex also allows you to work with multiple data sources. It creates a central hub for managing and monitoring data to work across various data lakes, data warehouses, and data marts. As a data analyst, you may access Dataplex with BigQuery. This will allow you to access data from a variety of databases and platforms with a single interface. Finally, BigLake is a storage engine that you can use to unify data warehouses and lakes with BigQuery and open-source frameworks. It also provides options for access control, and multi-cloud storage. These are just some of the standard tools you might encounter as a data analyst, and there are even more on the horizon.

### Document - [End-to-end data management](https://www.cloudskillsboost.google/course_templates/961/documents/512154)

### Video - [Data access management for secure data](https://www.cloudskillsboost.google/course_templates/961/video/512155)

* [YouTube: Data access management for secure data](https://www.youtube.com/watch?v=NNIJh47ExTQ)

Hello, data enthusiast! I’m so happy you’ve joined me for this investigation into data access controls and best practices of data management. As you’re about to discover, a major part of data management is ensuring that data is secure. And security, in this context, is about where the data is stored and who has access. Let’s begin with an example. Picture an office manager with a security device at their office entryway. They need to provide an up-to-date access code to their staff and trusted vendors. In these circumstances, the office manager can give other people the access code to allow them in. In other words, sharing this code means they can give access to only people they trust —and keep out those who shouldn’t enter. Chances are, you use unique identifiers like this access code all the time. Other types of unique identifiers include the code to access a phone or a computer. Data access management works in a similar way. As a data professional, you give specific people access to your data and your data processes —and keep out those who shouldn’t have access. Ok, first a quick definition: Data access management is the process of implementing features including password protection, user permissions, and encryption to protect data and related processes. In the context of the cloud, it includes the creation of individual user roles or user groups. And the access for each role or group is pre-defined by an administrator. The act of assigning this access is called identity access management, or IAM. IAM is the process of assigning certain individual roles or groups with access to particular resources. And some key resources may include software hosted in the cloud, datasets, databases, or entire data warehouses. The level of access is determined by the role of the user. There are three main components of identity access management: principal, role, and policy. First, principal is the account or accounts that can access a resource. You can think of this as the account attached to each username. It’s important that each account has a unique identifier. The second aspect of identity access management is role. This is the collection of permissions that lists the elements that an account has access to. Each principal may have a different role. And finally, policy is the collection of roles that are attached to a principal. Let’s explore how this might work in an organization. As a data analyst administrator, you’re setting up a security group for the entire data team in the Finance organization. The principal is a group that contains all the individual usernames for those on the data team. The roles in this group include both data writer and data reader. Both roles are attached to the principal. The policy is both data writer and data reader attached to the principal’s username. Now, universal access, access by role, and access by environment are the primary access types you’ll encounter. Here’s how they work. Universal access —as the name suggests— is access that everyone needs. This may be universal access to an entire project or to one portion. Next, access by roles is access for each individual role assigned to a project. Creating profiles by roles helps determine what access is needed for the tasks each role will perform. Finally, access by environment is determined by user location. So, a remote user may have different access than someone who’s onsite. There are some best practices that help ensure data is accessed according to assigned permissions. First, be sure to audit data access and monitor user tasks. Second, put additional permissions on remote access. Also, verify and stop unusual activities. And finally, add two-factor authentication or strong passwords to accounts to ensure security. It’s also a good idea to regularly check access roles. Users may not need as much access as originally provided. Alternatively, a user may need increased permissions to complete a project. And it’s always possible that users change departments and take on new roles, and need an update to their permissions. Along the way, it’s very important to document user access so you have a record of who has access and why. This also helps audits go smoothly and is critical for compliance documentation. Consider sharing this information in a team playbook so access issues don’t become blockers to completing work. I hope you enjoyed this rundown of data access management! With effective access policies in place, you’ll know you’re helping your employer keep its data safe and sound!

### Document - [Types of data access management](https://www.cloudskillsboost.google/course_templates/961/documents/512156)

### Quiz - [Test your knowledge: Data management in the cloud](https://www.cloudskillsboost.google/course_templates/961/quizzes/512157)

### Video - [Introduction to business data requests](https://www.cloudskillsboost.google/course_templates/961/video/512158)

* [YouTube: Introduction to business data requests](https://www.youtube.com/watch?v=b1IkAiFhe0c)

Welcome, future data pro! Thanks for joining me for this investigation into business data requests. The business data request is the kickoff to help launch a successful project. It's where all the fun starts! A business data request is any business question that can be answered with data. Whether the request involves data that already exists, or new data. As a data analyst, you’ll typically receive this request through a ticketing system. This is a tool for recording requests and assigning them to the appropriate team member. Data requests can be simple or complex. Some can be responded to by a single data analyst. Others will require a team of data analysts to work together to generate a larger report. Typically, data requests will vary from organization to organization, but data requests will have some similar elements. First, requests can be internal, from employees within the organization, or external, from outside. External requests may come from government or regulatory agencies, vendors, users, or clients. Types of requests might include answers to a data-related question or the creation of a data report, an extract, or a dashboard. And requests usually include information or structure specifics. Here’s a case to consider. Drew, a data analyst, works for a company that runs clinical trials for a health care product. Stakeholders want to know how long participants spend filling out surveys during the trial. So, Drew receives a business data request asking for a report about survey timing. This is an internal request for a data report. Drew can also potentially add the report information to a dashboard. As Drew begins to work on this request, there are several things he keeps in mind. First, the overarching goal. In this case, the goal is to find out how much time participants are spending on surveys. And, it’s also a great practice to go beyond this and try to understand why the stakeholders need the data. An example question Drew might ask about the goal is: Do the stakeholders need to correlate the length of time taken with the quality of the response? Then, Drew will determine what’s being measured. In this case, it’s pretty straightforward —the length of time participants take to finish a survey. Next, Drew figures out what data is needed and how much. Does he consider all survey responses times? Or, only the surveys that launched in the last year? In this case, he determines he wants all surveys, but segmented by survey name. Drew reviews the survey completions to identify any errors or outliers in the data. For example, if the average survey completion time was 10 minutes and a participant took 24 hours, is that a bug or did they actually take that long? Finally, Drew searches for trends to identify, within the survey name segment. For example, does one particular survey take longer than the rest? Next, he gathers the data and creates a summary report of all the information. This enables Drew to find connections and draw conclusions about how long participants spent filling out surveys during the trial engagement. This is just one example of a data request, and you’re sure to come across lots more throughout your data career. But no matter what question you’re trying to answer, working through these steps will help ensure that you gather the right data and use it to come up with a brilliant solution!

### Document - [Communicate and collaborate with stakeholders](https://www.cloudskillsboost.google/course_templates/961/documents/512159)

### Video - [Tips for ensuring accurate data](https://www.cloudskillsboost.google/course_templates/961/video/512160)

* [YouTube: Tips for ensuring accurate data](https://www.youtube.com/watch?v=iNK1FrMsp1k)

Hello, data detective! Always being curious, enjoying investigation, and committing to tracking down necessary information are all big parts of the data analysis process. Yet sometimes, that information just isn’t as useful as we’d hope! In this video, you’ll gain some great strategies for figuring out how to fix this problem. Maybe, as a data analyst, you’re asked to respond to a business data request. This is a business question that you can answer with data. Your first step is to assess the request to make sure you understand what’s involved, and the goal or conclusion desired. Let’s take up this case study. The request involves a supply chain organization that wants to determine if supply partners have a successful customer experience when purchasing inventory from their online store. Now, there are a few ways to confirm whether you have the data you need to respond to this request. You could use analytics software to check the structure of the database. Then, determine if it is possible to learn when supply partners began searching for a specific product compared to when they clicked the purchase button. You might also track timestamps on their visits to your website. This can help confirm that partners are able to efficiently find and buy what they need. And, it lets you know that they’re actively engaging with your website. Next, you move on to data cleaning —sometimes called data wrangling. In this phase, you correct or eliminate any inaccurate or repeat records from the dataset. For example, maybe a supply partner omits important details from an order, then creates the same order again to correct their mistake. You’d want to remove the duplicate. Also during this phase, you have the opportunity to learn more about your supply partner and define specific segments. And then, break down the dataset into smaller sections. Now it’s time to check for any outliers or deviations in the dataset. If a partner mistakenly enters an order for one thousand products, but they really only need ten, you’d want to address that outlier. Here, you also integrate multiple data sources —or use data from multiple methods or sources— to ensure information is accurate. In this example, you’d use data from your website, your customer management system, and your purchase orders. If available, you could also check summary statistics. You can create these in a dashboard or spreadsheet that calculates the average, mean, median, minimum, maximum, and other various relevant statistics and calculations. You can also run a quick SQL query that does the same. Finally, if you’ve done this type of analysis in the past, it might be helpful to compare the two projects. If the results vary wildly, this likely indicates that you need to recheck your dataset and your process. Better yet, consistent results help confirm that you’re working with valuable, useful data. By putting on your detective’s cap and viewing data from many different perspectives, you can be sure you’re using the best possible data! Your stakeholders will appreciate your investigative spirit when their requests are fulfilled skillfully and thoroughly!

### Document - [The data request process](https://www.cloudskillsboost.google/course_templates/961/documents/512161)

### Quiz - [Test your knowledge: Align data with stakeholder requests](https://www.cloudskillsboost.google/course_templates/961/quizzes/512162)

### Video - [Cloud data storage and management tools](https://www.cloudskillsboost.google/course_templates/961/video/512163)

* [YouTube: Cloud data storage and management tools](https://www.youtube.com/watch?v=iA4kfLpcguw)

Hi there, and thanks for joining me as we explore a business case about data management and storage tools! Data analytics in the cloud brings with it a library of tools that can assist with each phase of the analysis process. In this video, you’ll work through a business data request and explore the tools you might use to fulfill it. In this example, you’re a data analyst working for an app-based gaming company. After a few months on the job, you receive a business data request from the advertising team. They want to know how many users, and what percent of users, are clicking on ads during gameplay. The advertising team will use your findings to decide whether they should continue showing ads during gameplay or, instead, just charge users more for the app. Your game app is hosted on various platforms, and users are able to purchase a subscription if they want fewer ads. So, you'll need to gather data from multiple data sources. The first data management tool you use is Google Cloud Storage. This will host all data once it’s compiled. With this tool, you can upload data from remote servers through an internet connection. While working toward your business question, you’re able to move your application databases, analytics, and data science tools into one place. You also use Dataflow in this project to create a data pipeline. Dataflow gives you the option of either stream or batch processing. This makes it possible to stream data from your various apps into one database. You can also use the batch processing feature to import any data you already stored on your computer. Cloud Data Fusion is the tool you use to build a data pipeline. You can also create data integrations, run high-volume data pipelines, and continuously integrate user data from your applications into your database. Once you have data from all sources streaming into your Google Cloud Storage, you use BigQuery to write SQL queries to join data and clean the data to ensure it’s complete, has no duplicates, and will be useful for your project. Next, DataProc allows you to use open-source data analytics tools at a large scale. And you can apply programming languages and algorithms to data also at a large scale. With these tools, you’re able to move your data to the next phase and start drawing conclusions. After analyzing the data using the relevant tools, you share your discoveries with the advertising team, highlighting how many users click on ads and what percentage of total gamers that equals. With this information, the advertising team can effectively choose whether to keep using ads to generate revenue or charge users a fee. You’ve just experienced a typical business case involving data management and storage. Along the way, you successfully integrated multiple tools to complete a data workflow and find important answers for your stakeholders. Cloud data professionals are excellent data stewards. And, with the strategies from this example case, you now understand how to use key tools and solutions to protect and control all kinds of data!

### Document - [Google Cloud's data storage and management tools](https://www.cloudskillsboost.google/course_templates/961/documents/512164)

### Video - [Introduction to Dataproc](https://www.cloudskillsboost.google/course_templates/961/video/512165)

* [YouTube: Introduction to Dataproc](https://www.youtube.com/watch?v=lNQPWglz84Y)

Hello, future data expert! Welcome to this video all about Dataproc! Together, we’ll explore what makes it such a powerful tool and how you can implement it in order to both save time and increase the reliability of your data. Let’s begin with an example. Maybe you work for a major online retailer that sells more than five hundred products and sources them from about 2,000 suppliers. Obviously, this adds up to a lot of data —both structured and unstructured. This retailer is working on a strategy to stay competitive in the fast-paced world of online retail. Staying ahead of the competition is a top priority. That's why your company is so focused on building a cutting-edge data system that revolutionizes the way they process data and customer reviews. To help achieve this objective, you and your data team utilize Dataproc to manage the unstructured data. Dataproc is a fully managed service that maximizes open-source data tools for batch processing, querying, streaming, and machine learning. One of the benefits of Dataproc is that you can increase and decrease compute resources based on the project, so then you don’t need to guess, underestimate, or overestimate resources. This means your online retailer can dedicate cloud computing resources that focus only on processing customer reviews. And you can have separate resources that focus only on processing sales information. You also learn that Dataproc works especially well with Apache Hadoop, a framework that distributes the processing of data across clusters of computers. This also means that we can scale up to use the power of multiple computers, even thousands, to process our data in real time when we need it. Dataproc uses its open-source tools on virtual machines that scale up and down as needed. And it can install a Hadoop cluster in 90 seconds. Let’s learn a bit about how Dataproc scaling works! Dataproc disaggregates storage and compute services, so both can be created and terminated as needed. This means that, once you have data to store, your storage will remain. But if you’re not actively processing data, the compute services will terminate. For a more specific example, let’s go back to our online retailer. So, your company is going to store external application logs in cloud storage. Then, the data will be processed by Dataproc, which can write it back to Google Cloud Storage or BigQuery. You can also send the data to a data science notebook for analysis or to data scientists who can build and train AI models. Because storage is separate from processing, your organization will save money by grouping tasks. So, you’ll be using exactly the right amount of memory and storage space to meet all your data analysis needs. Your company has always stored data in different servers and on some hardware-based servers. You learn that Dataproc has pre-built templates that can help you move your data from your existing structure into Google Cloud Services, or GCS. Some of the common templates include Snowflake to GCS, Redshift to GCS, S3 To BigQuery, and Kafka to BigQuery. This means your databases in Snowflake, Redshift, or Amazon can be automatically moved into Dataproc with these templates. Plus, Dataproc makes it easier to manage Hadoop, and you can integrate organization-wide security. Plus, you can enable data users through integrations, giving varied permissions to different users. All right, now that your company has integrated Dataproc, it’s able to provide data on all five hundred products to all of its vendors. This has given your data team increased visibility into the best products to source and sell! Best of all, your suppliers receive a smoother, consistent flow of valuable recommendations, enabling each vendor to tap into the waves of user demand like never before. With real-time feedback and actionable insights, suppliers can fine-tune their product descriptions, ensuring they resonate with users on a whole new level!

### Document - [Uses of generative AI in data analytics](https://www.cloudskillsboost.google/course_templates/961/documents/512166)

### Lab - [Navigate BigQuery](https://www.cloudskillsboost.google/course_templates/961/labs/512167)

Explore public datasets

* [ ] [Navigate BigQuery](../labs/Navigate-BigQuery.md)

### Quiz - [Test your knowledge: Cloud data tools](https://www.cloudskillsboost.google/course_templates/961/quizzes/512168)

### Video - [Introduction to process management](https://www.cloudskillsboost.google/course_templates/961/video/512169)

* [YouTube: Introduction to process management](https://www.youtube.com/watch?v=KgktfZu41Tc)

Hello, and welcome to this video about process management! We’re going to dive into some great strategies for keeping your data projects progressing smoothly. Data analysts usually have several projects going on at the same time, all at different stages, and involving different tasks and teams. Managing this work simultaneously requires the talent of a skilled juggler, gracefully keeping several balls in the air at once. Each project represents a different ball, and you must carefully coordinate your movements to maintain control over all of them. With a well-organized internal process workflow, you can become a masterful juggler, skillfully switching between tasks and preventing any balls from dropping. There are three key parts of a typical data analyst workflow: using a data request central system, checking in code, and keeping an internal record of your work. Let’s review each one now. First, it's important to have a data request central system to store and manage business data requests and related team conversations. This system provides relevant documentation, enables collaboration, and preserves historical records. Documentation is a process for preserving all of the details and conversations about each request in one place, making information easy to find when a data analyst needs to find or request it. Collaboration happens when members of the data analytics team visit the central system to review data requests, including queries and data delivery methods. Historical records are past data or information that’s been stored and can be accessed or retrieved when needed. This supports collaboration and helps refresh your own memory. For example, maybe you can’t remember why you wrote a specific query a year ago. If it’s documented, you can go back to the original request, review your notes, and remind yourself of why you wrote it. This can be a great time-saver for new projects! And while we’re on the subject of queries, it’s also best practice for any teams who write code to check their code into some sort of central repository. Github is a popular tool for this kind of sharing. To check in code is to upload code to a main repository so others can access it and review it. You can check out a copy of the code, and teammates will know that you are actively working on a copy of the code. Then, when you are ready, you can request a code review so you can get sign-off from your teammates. Once they approve the code, you can “check in” the code, and now everyone will have access to the changes you made. You just unpacked the importance of checking in code for effective teamwork. Now you’ll explore more benefits of using a check-in system with a central repository to help you manage the code you check in. First, more effective collaboration. Another analyst on your team can review your code to catch potential oversights or errors before using it in production. Next, if a data report comes from a centralized repository, updates can be made there directly, avoiding the need to rewrite the report or create inconsistencies with changes in one place and not in others. Third, having code reviews and keeping all the live code in one place can improve code quality by ensuring consistency, following a style guide, and enhancing code readability in terms of using appropriate syntax and well-placed comments. For major changes to dashboards or reports, a code-sharing system lets you track code easily. The next advantage is having an easier way to revert code. This system lets you undo changes with a single click, making it quick and simple to troubleshoot and fix code errors. Next, revision history reveals when and who changed or added code, helping you trace its origins and reach out to the relevant data analyst if you need clarification. Finally, keeping all your code in one repository makes it easier to locate. After all, you and your team will always need to know where to find your code files! As a final strategy, keeping an internal record of your work is a great time-saver. You can use the same system when stakeholders submit data requests to create and track your own tasks for different projects. And logging your tasks in a central place not only keeps you organized, but also allows you and your team to share progress. Juggling multiple projects is all part of a day in the work life of a data analyst. But here’s the good news: If you’ve got an efficient system that handles data requests, manages code check-ins, and tracks all your work, then you've got this! You’ll have a super organized workflow and deliver top-quality work on all your projects.

### Video - [Strategies for handling data requests](https://www.cloudskillsboost.google/course_templates/961/video/512170)

* [YouTube: Strategies for handling data requests](https://www.youtube.com/watch?v=cFu9m2kweOA)

Welcome, data whiz! Thanks for coming along for this journey into data management. In your role as a data analyst, you may receive data requests. In this video, we’re going to focus on strategies for expertly handling these business data requests. A business data request is any business question that can be answered with data. So, when a stakeholder needs help from the data team, they’ll typically use a ticketing system to submit their request. Then, the data analyst uses that same ticketing system to track and prioritize the request. While ticketing systems vary, usually they allow people in the business to submit a question or issue, or ask for a new computing feature. The system then helps people track their request from start to finish. Other examples of data requests may include asking for a new report or asking for modification to an existing dashboard or report. Generally, each ticket submitted will have a details page. Comments are written about each item, so all communication about the request exists in one place. The general elements of each data request are type, priority, and status. The type category can be set by your organization to help group requests together. You might group requests about a similar issue or by the person who’s responding. Grouping the requests allows you to address all aspects of an issue as different people may point out different problems. The issue priority category does exactly what it says —helps people determine the priority for each issue. And the issue status allows the data analyst to provide a real-time status update about how the request is progressing. Besides the ticketing system, it’s important to intake issues thoroughly. This means collecting as many details as you can upfront to avoid a lot of back-and-forth later. When taking requests, consider what, when, who, where, why and two hows. Let’s illustrate all of this with an example. Maybe, as an analyst, you’re working for a nonprofit solar energy initiative. Your organization is tracking how a particular program is affecting smaller communities outside a city. You’ve been taking data requests from all stakeholders in any form, and it’s starting to get confusing! So, your data team institutes a ticketing system. Through this new system, stakeholders can now submit business data requests to ask for reports, data clarifications, reference data, data extracts, and more. All right, here comes your first issue in the tracker! You receive a data request asking for a count of communities affected by the solar energy initiative. So you ask the following: First, the what: Should all solar energy initiatives be counted or only certain types? Second, when: How far back should the data go? Just this calendar year? The last three hundred sixty-five days? Or for all time? Next, who: Will all communities be included in the dataset or only a subset? Ok, now you’ve come to whereE: So you ask, should the data be stratified by any particular region or zip codes? When you stratify something, you divide it into groups. All right, now why: It’s important to understand the business context in which the stakeholder is coming to you with this request. For example, what are the business questions this report would help answer? Thinking about context also helps guide additional questions you may have missed asking in the data intake process. Next is the first how: This how is about refreshing the data. So you ask, how often should this data be refreshed? Is this just a one-time pull, or will it be needed on an ongoing basis? And finally, the second how is about data delivery. For this, you consider how the stakeholder wants the data delivered. They might prefer a lightweight static report, like a spreadsheet, or a more robust dynamic dashboard. These are a lot of details to cover with your requestor! Thankfully, having a system allows you to document the details so you can easily refer back and iterate, if needed. With this info, you’re able to get back to your stakeholders with valuable data insights. But wait! You’ve just received another ticket. Now, one of your stakeholders wants to know how many buildings have reduced energy costs compared to buildings that didn't opt into the solar initiative. When you start to research this question, you realize that you need to first create a report listing which buildings received solar energy in which year. Then, you’ll need a report that segments those buildings by start date. For these tasks, you create something called a parent-child relationship. Luckily, this is another great feature of ticketing systems! A parent-child relationship enables users to divide a single ticket into subtickets, which can be worked on simultaneously —sometimes even by different team members. So, for our example, identifying which buildings received solar energy over the past year is a separate task that needs to be finished before you can answer the main question related to overall solar savings. With this in mind, you create a child request for each new report and attach them to the parent —the original request you received. In the ticketing system, you then define the relationship between the requests and order them in a way that makes sense. Great work! Now you’re prepared to answer the stakeholder’s latest question. As a final point, let’s go over how to track all of the requests in a ticketing system. For this, you’ll use status fields. Some common status fields are: assigned, start work, in progress, fixed, verify, and reopen. Your organization may customize these based on their needs. But the key is that the status fields enable everyone to easily understand the status of a request. Also, saving fixed and verified requests creates a handy database of previously answered questions. So, for the purposes of your solar energy project, you’d indicate the issue regarding the number of buildings using solar that needs to be verified and the request regarding the total amount of savings. Nice! Using an efficient tracking system enables you to share data quickly and accurately. It improves overall communication and collaboration. And it’ll definitely make your life easier when you have numerous requests coming in for your expert data work!

### Document - [Elements of a data request](https://www.cloudskillsboost.google/course_templates/961/documents/512171)

### Video - [Clear documentation benefits the whole team](https://www.cloudskillsboost.google/course_templates/961/video/512172)

* [YouTube: Clear documentation benefits the whole team](https://www.youtube.com/watch?v=G45GiGilhDY)

Hello, and thanks for being with me to discover some great ways to keep data team members on the same page. And I do mean “page” because this video is all about data documentation! Data documentation is a written guide to the data contained in a dataset, how it was collected, and how it's organized. Data documentation typically includes the purpose of collecting the data, the procedures followed, the date and time data was collected, the structure of the dataset, and any notes about data validation or quality assurance. Data documentation may come in the form of readme files, a data dictionary, a codebook, a lab notebook, a spreadsheet, and more. Let’s get into this with an example. Maybe you’ve just joined a data team as an analyst at a cutting-edge renewable energy company. As you start to become familiar with the data available to you, you notice that wind and geothermal energy are recorded on different spreadsheets with different columns and notations. And you notice that the different departments within your organization are keeping their own data tables and integrating them together for reporting. You also wonder why your team only has access to data for the last two years when there is at least ten years worth of data available. Just then, one of your new coworkers hands you the team’s data playbook and documentation for the datasets you’re using. They explain that the playbook documentation defines the data, which will help you understand the information you’re working with. Full of curiosity and momentum, you delve into the playbook, ready to uncover solutions that will bring clarity to the data puzzle! You discover that the team’s data playbook provides the company’s overall plan for handling and managing data, with team-specific processes. For a data analyst, the most important elements of the team playbook are information about how to request data access, how to grant data access to others, where your team stores data tables in BigQuery, and how to carry out common tasks. As an added bonus, sometimes a team playbook includes sample queries for common requests like merging data. As you review the playbook, you start to understand why the wind and geothermal data are kept in separate files. You find out that there are certain individual projects that are crucial to the environmental success of the company. And, in some of the case studies included in the playbook, you find some great examples of how the data has been merged together in the past for other initiatives. You join your team in a retrospective and discuss the benefits of sharing the playbook to complete your data analyst tasks. You learn that the data playbook is a living document, meaning it’s updated as needed. So, when you become more accustomed to working on the team, they look forward to your contributions as well! Now, you understand the importance of data documentation, like a data playbook, and how teams follow documentation procedures to collect and manage data. Having an up-to-date playbook will help ensure you and your data team stay on the same page!

### Quiz - [Test your knowledge: Process management as a data analyst](https://www.cloudskillsboost.google/course_templates/961/quizzes/512173)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/961/video/512174)

* [YouTube: Wrap-up](https://www.youtube.com/watch?v=mqlPuYvDmaY)

Congratulations! You’ve accomplished so much during this section of the program! You’re well on your way to becoming a cloud data analyst! Now you understand lots more about the work of data analysts in cloud data, and how you might collaborate on a data team. You also discovered the basics of some key cloud data tools and processes. Along the way, you focused on data gathering, plus data access management and storage tools. This also included an investigation into data processing and transformation. And this section covered a lot about data requests, data documentation, and team playbooks. Thanks again for joining me, and well done! You’re making outstanding progress!

### Video - [Vince: Characteristics of strong data analyst candidates](https://www.cloudskillsboost.google/course_templates/961/video/512175)

* [YouTube: Vince: Characteristics of strong data analyst candidates](https://www.youtube.com/watch?v=lmOulSgfPoo)

As a hiring manager, I really wanna screen people in rather than screen people out. We're always looking for reasons to say, "Yes," to people rather than to say, "No." I'm Vince and I'm a Cloud Data Engineering Manager. That means that I work with a team of data engineers to help customers solve data and analytics challenges. But the qualities we look for in candidates are general facility with data, you know? A little bit of an analytics background is always good, familiarity with technologies like SQL, big data technologies and batch and stream processing. Workplace skills are critical. Workplace skills are almost everything else that you do other than the hard technical skills that you exercise in your job. So the ability to collaborate, the ability to find compromise, the ability to lead, all of these things go into being a really strong employee. When it comes to showcasing your workplace skills, it's not just important what you would do to solve a problem, but how you would solve the problem. Helping your interviewer understand your process and how you get to a solution is just as important as a solution itself. I think confidence is really important. When I'm interviewing a candidate, I like to see that a candidate is comfortable in a situation like interviewing, which is inherently kind of uncomfortable. It's a good signal if someone shows up with a high degree of confidence to an interview that they'll show up well when they're in high stakes situations with customers. Candidates have a lot of power in the interview process. If you've made it to the point of an interview, then the company views you as someone who might be someone who they want to work there. I think it's important to ask questions of your interviewer that would give you some insight into what it's actually like to work at the company you're interviewing for, questions that you might have about things like work-life balance. One question that I always like getting from candidates is, "What's one thing you would change about the company "that you work for?" If you're considering getting started in a career in data analytics, I would really encourage you to find some public data and use some of the great open source tooling out there and start playing with some data. It's a great way to build experience and also see if it's something you'd like to build a career around.

### Video - [Vince and Monica: Interview role play](https://www.cloudskillsboost.google/course_templates/961/video/512176)

* [YouTube: Vince and Monica: Interview role play](https://www.youtube.com/watch?v=Htn1TyW6Wno)

Hi, I'm Vince. Hi, I am Monica. Congratulations on making it through this course. Now you're gonna get a sneak peek on what an interview on the topics in this course would be like. We hope this will let you know what to expect in your next interview. What interests you in a career in cloud data analytics? I have always been very passionate with numbers, I understood and worked them very well through high school. I remember math being one of my subjects with my highest score in my report card. And then, when I have to choose for a career path, I decided to do computer information system, which was like the business and also like the technical side of analysis. One of the projects that I work while in the university was to create a dataset schema, and I remember then I was amazed about everything data. So that's what got me into data analytics. So tell me about your work experience and how that prepares you for a role in data and analytics. A previous role that I had was an IT technician. Part of my role was to be responsible, creating a dashboard to show the usability of a tool. After the analysis, we concluded that the tool was being used, but just by promoting it a little bit more could be more useful and decrease the time of troubleshooting for the techs. - What are some ways that cloud computing can be challenging for a company? - Cloud computing can be challenging for a company integrating data from these different sources. Also, compliances, they have to meet the strict regulators and the privacy standard. - Can you talk a little bit about your experience with either of those two areas? - Sure, I would say, when I created the dashboard that I was talking earlier, all the information that I was using was for different data source. So I have to basically clean and then join them using the same attributes. - So imagine this scenario, a stakeholder approaches you and says that they need a report on company payments. As an analyst, how would you intake this request? - If a stakeholder comes to me asking for a report of payments, I will ask some questions. Some of the questions that I would ask is, what is the objective of this analysis? So I know the purpose of it. I would also ask the times, does he want to do an analysis of a year, of two years, of like just a month, so I can gather the information, the data that I just need for that dashboard. And lastly, I would ask if there's any PII or data security request for this dashboard. All right, so you've completed your requirements gathering, what are a couple of things you might need to start the analysis? So the first thing that I would do is to make sure the data is valid, the one that I'm working with. And after that, I would just analyze it using query. What would you query the data with? I will use SQL. Okay. To query the data. What's an example of a system that might enable you to query it with SQL? Sure, one example that could be BigQuery. Okay, Monica, I wanna thank you for interviewing with us. I really wish you good luck with the rest of the process. Thank you, Vincent, thank you so much for your time and consideration. In this scenario, Monica showed how to talk through your answers and show your thinking. By sharing your thought process, you demonstrate what you know and how you solve problems. Problem solving is a top skill hiring managers are looking for. That's it for now, stay tuned for more tips.

### Document - [Interview tip: Demonstrate your thought process](https://www.cloudskillsboost.google/course_templates/961/documents/512177)

### Document - [Glossary terms from module 4](https://www.cloudskillsboost.google/course_templates/961/documents/512178)

### Quiz - [Module 4 challenge](https://www.cloudskillsboost.google/course_templates/961/quizzes/512179)

### Video - [Course wrap-up](https://www.cloudskillsboost.google/course_templates/961/video/512180)

* [YouTube: Course wrap-up](https://www.youtube.com/watch?v=piTh8M-dOkc)

Congratulations on finishing this course! You are well on your way to accomplishing great things in cloud data analytics! My favorite part of working in cloud data analytics is the opportunity it provides to offer answers and insights that a user, stakeholder, or customer may have never thought possible. Making data available, usable, and accessible is no small feat, and with the right tools and skills you can really change how people think about problems and make decisions. As someone who loves helping people, that's exciting! Now, let’s take a minute to go over all the things you learned in this course. You began by exploring an introduction to the program and some tips for successfully completing the certificate. You also learned about cloud computing, its components, and cloud computing versus traditional computing. Then, you explored cloud data analysis versus on-premises data analysis. And you learned about the impact of cloud data analytics on all kinds of businesses, with a special focus on the Google Cloud Architecture Framework. Next, you discovered the inner workings of data management and the data lifecycle, and the cloud data analyst roles in keeping both running smoothly. You also explored cloud team collaboration and how this helps teams to create some really cool business projects together. Finally, you discovered what cloud data tools are in an analyst’s toolbox and learned about the importance of data documentation in data analytics. You now know about cloud data tools, and can understand and communicate cloud benefits, share timely insights, and so much more! Congratulations on your progress so far! You’re off to a great start!

### Document - [Course 1 resources and citations](https://www.cloudskillsboost.google/course_templates/961/documents/512181)

### Document - [Glossary terms from Course 1](https://www.cloudskillsboost.google/course_templates/961/documents/512182)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
