---
id: 155
name: 'Google Cloud Computing Foundations: Networking & Security in Google Cloud'
datePublished: 2024-12-18
topics:
- Networking
- Google Cloud Services
- Automation
type: Course
url: https://www.cloudskillsboost.google/course_templates/155
---

# [Google Cloud Computing Foundations: Networking & Security in Google Cloud](https://www.cloudskillsboost.google/course_templates/155)

**Description:**

The Google Cloud Computing Foundations courses are for individuals with little to no background or experience in cloud computing. They provide an overview of concepts central to cloud basics, big data, and machine learning, and where and how Google Cloud fits in.

By the end of the series of courses, learners will be able to articulate these concepts and demonstrate some hands-on skills.

The courses should be completed in the following order:
1. Google Cloud Computing Foundations: Cloud Computing Fundamentals
2. Google Cloud Computing Foundations: Infrastructure in Google Cloud
3. Google Cloud Computing Foundations: Networking and Security in Google Cloud
4. Google Cloud Computing Foundations: Data, ML, and AI in Google Cloud

This third course covers cloud automation and management tools and building secure networks.

**Objectives:**

- Demonstrate how to build secure networks in the cloud.
- Identify cloud automation and management tools.

## It helps to network

Networks: Demonstrate how to build secure networks in the cloud

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/155/video/518197)

- [YouTube: Introduction](https://www.youtube.com/watch?v=LvrymILM-RI)

Welcome to Module 7 of the Google Cloud Computing Foundations course, It helps to network. In this section of the course, you’ll learn how networking in Google Cloud works and what you must consider before setting up networks. This means you’ll: explore Virtual Private Clouds (VPCs), examine Google’s network architecture, learn how to use multiple VPC networks, explore options to build hybrid clouds, and examine load balancing options. The module agenda follows the objectives. You’ll begin with an introduction to networking in the cloud. Next, you’ll learn what a Virtual Private Cloud is, including the basics of public and internal IP addresses and Google’s network architecture. Next, you’ll explore how routes and firewall rules work in the cloud. After that, you’ll examine how multiple VPC networks can be used, which is supported by two labs. In the first, you’ll create VPC networks and VM instances, and in the other, you’ll practice controlling access in a VPC. From there, you’ll learn about the option to build hybrid clouds using VPNs, interconnecting, and direct peering. You’ll then complete a lab where you’ll configure an Application Load Balancer with Google Cloud Armor. Finally, the module concludes with a short quiz and a recap of topics covered. Let’s get started!

### Video - [Networking in the cloud](https://www.cloudskillsboost.google/course_templates/155/video/518198)

- [YouTube: Networking in the cloud](https://www.youtube.com/watch?v=SeFFMKQhDSY)

Let’s start with an introduction to networking in the cloud. Computers communicate with each other on a network. The computers in a single location, like an office, are connected on a local area network (LAN). Multiple locations can have their LANs connected to a wide area network, or WAN. Most networks today are connected to the internet, which enables millions of personal computers, servers, smartphones, and other devices to communicate, provide, and consume IT services. Since around 2004, Google has been a leader in building fast, powerful, high-quality cloud infrastructures. Google’s high-quality private network connects regional locations to more than 100 global network points of presence close to users. Google Cloud also uses state-of-the-art software-defined networking and distributed systems technologies to host and deliver services around the world. When every millisecond of latency counts, Google ensures that content is delivered with the highest throughput. https://cloud.google.com/about/locations/ Here’s an example network diagram for an application that bridges an organization's physical data center and Google Cloud. The web client, or end user, performs a Domain Name System lookup, which is served by Cloud DNS. Content Delivery Networks, or CDNs, are locations within the Google network that are used to cache content closer to the users to improve application performance. The user requests a page. If the CDN option was selected when Cloud Load Balancing was configured, then the request will go to the closest CDN location, which will serve that page without getting it from the servers if the CDN has the page stored. For a page that the CDN doesn’t have, it will call Cloud Load Balancing. Cloud Load Balancing will pick an appropriate frontend server to serve the request, and return the page to the user. In Google Cloud, a "network" is an isolated global resource holding network configuration. Instances are deployed in regional subnetworks, however the policy (firewall, routing, and so on), and access through VPN, are configured at the global network level. Depending on the content of the page, the frontend Compute Engine server may have to talk to a Compute Engine backend server, which is regularly updated with data feeds from other sites. The subnetworks are configured so that the frontend subnet cannot talk directly to the data center or colocation facility.

### Video - [Virtual Private Clouds (VPCs)](https://www.cloudskillsboost.google/course_templates/155/video/518199)

- [YouTube: Virtual Private Clouds (VPCs)](https://www.youtube.com/watch?v=BTQe___TjTM)

Now let’s explore what a Virtual Private Cloud is. A Virtual Private Cloud, or VPC, is a secure, individual, private cloud-computing model hosted within a public cloud (like Google Cloud!). On a VPC, customers can run code, store data, host websites, and do anything else they could do in an ordinary private cloud, but this private cloud is hosted remotely by a public cloud provider. This means that VPCs combine the scalability and convenience of public cloud computing with the data isolation of private cloud computing. VPC networks connect Google Cloud resources to each other and to the internet. This includes tasks such as segmenting networks, using firewall rules to restrict access to instances, and creating static routes to forward traffic to specific destinations. Here's something that tends to surprise many new Google Cloud users: Google VPC networks are global. They can also have subnets, which is a segmented piece of the larger network, in any Google Cloud region worldwide. Subnets can span the zones that make up a region. This architecture makes it easy to define network layouts with global scope. Resources can even be in different zones on the same subnet. The size of a subnet can be increased by expanding the range of IP addresses allocated to it. And doing so doesn’t affect the already configured virtual machines. For example, let’s take a VPC with one network that currently has one subnet defined in Google Cloud’s us-east1 region. If the VPC has two Compute Engine VMs attached to it, it means they’re neighbors on the same subnet, although they are in different zones! This capability can be used to build solutions that are resilient to disruptions but retain a simple network layout. Google Cloud offers two types of VPC networks, determined by their subnet creation mode: auto subnet mode and custom subnet mode. When an auto mode VPC network is created, one subnet from each region is automatically created within it. As new Google Cloud regions become available, new subnets in those regions are automatically added to auto mode VPC networks. The automatically created subnets use a set of predefined IP ranges and default firewall rules can be applied. In addition to the automatically created subnets, you can add more subnets manually to auto mode VPC networks, in the regions you choose, by using IP ranges outside a set of predefined IP ranges. When you expand the IP range in an auto mode VPC network, the broadest prefix you can use is /16. Any prefix broader than /16 would conflict with the primary IP ranges of other automatically created subnets. Due to its limited flexibility, an auto mode VPC network is better suited to isolated use cases, such as proof of concepts, and testing. A custom mode network does not automatically create subnets. This type of network provides you with complete control over its subnets and IP ranges. You decide which subnets to create, in regions you choose, and using IP ranges you specify. These IP ranges cannot overlap between subnets of the same network. Custom mode VPC networks are therefore a lot more flexible and are better suited to production environments. While you can switch a network from auto mode to custom mode, this conversion is one way. Custom mode VPC networks cannot be changed to auto mode VPC networks.

### Video - [The basics of public and internal IP addresses](https://www.cloudskillsboost.google/course_templates/155/video/518200)

- [YouTube: The basics of public and internal IP addresses](https://www.youtube.com/watch?v=Zy8g3BjADFg)

In this next section, you’ll learn the basics of public and internal IP addresses in the cloud. A Virtual Private Cloud (VPC) is composed of subnetworks, or subnets, and each subnet must be configured with a private IP CIDR address. CIDR stands for Classless Inter-domain Routing. The CIDR range will determine what internal IP addresses will be used by virtual machines in the subnet. Internal IP addresses are only used for communication within the VPC and cannot be routed to the internet. Each octet in an IP address is represented by 8 binary bits. So a typical IPV4 address is 32-bits long. The number at the end of the range determines how many bits will be static or frozen. This number determines how many IP addresses are available with a CIDR address. The CIDR range determines how many IP addresses are available. A /16 range will provide 65,536 available IP addresses. Every time you add “1” to the last number, the number of available IP addresses is cut in half. So, what’s the difference between public and internal IP addresses? Public, or external IP addresses can be ephemeral or reserved. They are assigned from a pool of IP addresses associated with the region. If you allocate a reserved IP address but don't attach it to a virtual machine, you will be billed for the IP address. Virtual machines are unaware of their public IP address, which means that if you look at the operating system network configuration, the virtual machine will only display the internal IP address. Private, or internal IP addresses, however, are allocated to VMs by a Dynamic Host Configuration Protocol (DHCP) service. The lease for the IPs is renewed every 24 hours. The name of the virtual machine is the hostname, and the hostname will be associated with the internal IP address through a network-scoped DNS service.

### Video - [The Google Cloud network](https://www.cloudskillsboost.google/course_templates/155/video/518201)

- [YouTube: The Google Cloud network](https://www.youtube.com/watch?v=AgGtTtc2Mo8)

Now it’s time to learn about Google’s network architecture. This map represents the Google Cloud network at a high level. Google Cloud consists of regions, represented by the markers in blue, together with proposed future regions in white. A region is a specific geographical location where you can run your resources. The number on each region represents the zones within that region. Points of presence, or PoPs, are represented by the dark gray dots. The PoPs are where the Google network is connected to the rest of the internet. By operating an extensive global network of interconnection points, Google Cloud can bring its traffic closer to its peers, which reduces costs and provides users with a better experience. Google’s global private network is represented by the blue lines. The network connects regions and PoPs and is composed of hundreds of thousands of miles of fiber optic cable and several submarine cable investments. The cables that have a year next to them are our latest investments. The last component that makes up the architecture is Google’s services themselves. https://cloud.google.com/about/locations/ Here are five of Google Cloud’s networking products available to users. Google Cloud VPC is a comprehensive set of networking capabilities and infrastructure that’s managed by Google. With Virtual Private Cloud, you can connect your Google Cloud resources in a Virtual Private Cloud and isolate them from each other for purposes of security, compliance, and development versus test versus production. Cloud Load Balancing provides high performance, scalable load balancing for Google Cloud to ensure consistent performance for users. Cloud CDN is a content delivery network that serves content to end users with high availability and high performance, usually by storing files close to the user. With Cloud CDN, Google’s global network provides low-latency, low-cost content delivery. Cloud Interconnect lets you connect your own infrastructure to Google’s network edge with enterprise-grade connections. Google's partner network-service providers offer connections, and can offer higher service levels than standard internet connections. Cloud DNS (domain name system) translates requests for domain names into IP addresses. Google provides the infrastructure to publish specific domain names in high-volume DNS services suitable for production applications.

### Video - [Routes and firewall rules in the cloud](https://www.cloudskillsboost.google/course_templates/155/video/518202)

- [YouTube: Routes and firewall rules in the cloud](https://www.youtube.com/watch?v=bgNRHMD-ZFA)

Next, let’s consider routes and how firewall rules allow traffic to flow within a VPC. Much like physical networks, VPCs have routing tables. A routing table is a data table of router locations and their IP addresses, stored in the memory of a router or a network host. It lists the routes to particular network destinations, and sometimes, a metric value (number of “hops” needed to get to that address) which aids the router in choosing the most efficient route. VPC routing tables are built-in so you don’t have to provision or manage a router. They are used to forward traffic from one instance to another within the same network, across subnetworks, or even between Google Cloud zones, without requiring an external IP address. Another thing you don’t have to provision or manage for Google Cloud is a firewall. VPCs provide a global distributed firewall, which can be controlled to restrict access to instances through both incoming and outgoing traffic. It’s convenient to define firewall rules through metadata tags on Compute Engine instances. For example, you can tag all your web servers with “WEB” and write a firewall rule that says that traffic on ports 80 or 443 is allowed into all VMs with the “WEB” tag, no matter what their IP address happens to be.

### Video - [Multiple VPC networks](https://www.cloudskillsboost.google/course_templates/155/video/518203)

- [YouTube: Multiple VPC networks](https://www.youtube.com/watch?v=0_L8ZOIicLI)

So, how can you use multiple VPCs to build a robust networking solution? You’ll answer that question next. You’ll remember that VPCs belong to Google Cloud projects, but what if your company has several Google Cloud projects and the VPCs need to talk to each other? With VPC peering, a relationship between two VPCs can be established to exchange traffic. It allows private RFC 1918 connectivity across two VPC networks, regardless of whether they belong to the same project or organization. Now, remember that each VPC network will have firewall rules that define what traffic is allowed or denied between the networks. VPC peering is a decentralized or distributed approach to multi-project networking, because each VPC network can remain under the control of separate administrator groups and maintain its own global firewall and routing tables. Historically, such projects would consider external IP addresses or VPNs to facilitate private communication between VPC networks. However, VPC peering does not incur the network latency, security, and the cost disadvantages of using external IP addresses or VPNs. Alternatively, to use the full power of Identity and Access Management (IAM) to control who and what in one project can interact with a VPC in another, then you can configure Shared VPC. Shared VPC allows an organization to connect resources from multiple projects to a common VPC network. This allows the resources to communicate with each other securely and efficiently by using internal IPs from that network. When you use Shared VPC, you designate a project as a host project and attach one or more other service projects to it. The overall VPC network is called the Shared VPC network.

### Video - [Lab Intro: Multiple VPC Networks](https://www.cloudskillsboost.google/course_templates/155/video/518204)

- [YouTube: Lab Intro: Multiple VPC Networks](https://www.youtube.com/watch?v=8322Vb4Hkug)

You’ll now apply what you’ve learned about multiple VPC networks to a hands-on lab. In the lab titled, “Multiple VPC Networks,” you’ll create several VPC networks and virtual machine instances, and then test connectivity across networks. During this lab, you’ll: Create custom mode VPC networks with firewall rules. Use Compute Engine to create virtual machine instances. Explore the connectivity for virtual machine instances across VPC networks. Create a virtual machine instance with multiple network interfaces.

### Lab - [Multiple VPC Networks](https://www.cloudskillsboost.google/course_templates/155/labs/518205)

In this lab, you create several VPC networks and VM instances, then test connectivity across networks.

- [ ] [Multiple VPC Networks](../labs/Multiple-VPC-Networks.md)

### Video - [Lab Intro: VPC Networks - Controlling Access](https://www.cloudskillsboost.google/course_templates/155/video/518206)

- [YouTube: Lab Intro: VPC Networks - Controlling Access](https://www.youtube.com/watch?v=lHcNyFgwjoQ)

Let’s go straight into another hands-on lab. This time you’ll practice controlling access in a VPC. In the lab titled “VPC Networks - Controlling Access,” you’ll create NGINX web servers, control external HTTP access to the web servers using tagged firewall rules, and explore IAM roles and service accounts. During this lab, you’ll: Create an NGINX web server. Create tagged firewall rules. Create a service account with IAM roles. Explore permissions for the Network Admin and Security Admin roles.

### Lab - [VPC Networks - Controlling Access](https://www.cloudskillsboost.google/course_templates/155/labs/518207)

In this lab, you create two nginx web servers and control external HTTP access to the web servers using tagged firewall rules. Then, you explore IAM policies and service accounts.

- [ ] [VPC Networks - Controlling Access](../labs/VPC-Networks-Controlling-Access.md)

### Video - [Building hybrid clouds](https://www.cloudskillsboost.google/course_templates/155/video/518208)

- [YouTube: Building hybrid clouds](https://www.youtube.com/watch?v=5zFMgUzRW-A)

In this next section, you’ll learn about different options available to build a hybrid cloud by using Google Cloud. Many Google Cloud customers want to connect their Google Virtual Private Clouds to other networks in their system, such as on-premises networks or networks in other clouds. There are several effective ways to achieve this. One option is to start with a virtual private network connection over the internet and use the IPsec VPN protocol to create a “tunnel” connection. To make the connection dynamic, a Google Cloud feature called Cloud Router can be used. Cloud Router lets other networks and Google VPC exchange route information over the VPN by using the Border Gateway Protocol. With this method, if you add a new subnet to your Google VPC, your on-premises network will automatically get routes to it. But using the internet to connect networks isn't always the best option for everyone, either because of security concerns or because of bandwidth reliability. A second option is to consider “peering” with Google by using Direct Peering. Peering means putting a router in the same public data center as a Google point of presence and using it to exchange traffic between networks. Google has more than 100 points of presence (PoPs) around the world. Customers who aren’t already in a point of presence can contract with a partner in the Carrier Peering program to be connected. Carrier Peering gives you direct access from your on-premises network through a service provider's network to Google Workspace and to Google Cloud products that can be exposed through one or more public IP addresses. One downside of peering, though, is that it isn’t covered by a Google service level agreement. If getting the highest uptimes for interconnection is important, then using Dedicated Interconnect would be a good solution. This option allows for one or more direct, private connections to Google. If these connections have topologies that meet Google’s specifications, they can be covered by up to a 99.99% SLA. Also, these connections can be backed up by a VPN for even greater reliability. And the final option we’ll explore is Partner Interconnect, which provides connectivity between an on-premises network and a VPC network through a supported service provider. A Partner Interconnect connection is useful if a data center is in a physical location that can't reach a Dedicated Interconnect colocation facility, or if the data needs doesn’t warrant an entire 10 Gbps connection. Depending on availability needs, Partner Interconnect can be configured to support mission-critical services or applications that can tolerate some downtime. As with Dedicated Interconnect, if these connections have topologies that meet Google’s specifications, they can be covered by up to a 99.99% SLA, but note that Google is not responsible for any aspects of Partner Interconnect provided by the third-party service provider nor any issues outside of Google's network.

### Video - [Load balancing options](https://www.cloudskillsboost.google/course_templates/155/video/518209)

- [YouTube: Load balancing options](https://www.youtube.com/watch?v=Y2gmPRR98GU)

Now let’s explore the different load balancing options available with Google Cloud. The job of a load balancer is to distribute user traffic across multiple instances of an application. By spreading the load, load balancing reduces the risk that applications experience performance issues. Cloud Load Balancing is a fully distributed, software-defined, managed service for all your traffic. And because the load balancers don’t run in VMs that you have to manage, you don’t have to worry about scaling or managing them. You can put Cloud Load Balancing in front of all of your traffic: HTTP(S), other TCP and SSL traffic, and UDP traffic too. Cloud Load Balancing provides cross-region load balancing, including automatic multi-region failover, which gently moves traffic in fractions if backends become unhealthy. Cloud Load Balancing reacts quickly to changes in users, traffic, network, backend health, and other related conditions. And what if you anticipate a huge spike in demand? Say, your online game is already a hit; do you need to file a support ticket to warn Google of the incoming load? No. No so-called “pre-warming” is required. Application Load Balancers and Network Load Balancers are two primary types of load balancers offered by Google Cloud, each designed for specific use cases. Application Load Balancers operate at the application layer (Layer 7) of the OSI model. They are ideal for applications that require load balancing based on HTTP(S) headers, cookies, or URL paths. Application Load Balancers provide features like SSL/TLS termination, session affinity, and content-based routing. Network Load Balancers operate at the network layer (Layer 4) of the OSI model. They are suitable for load balancing based on IP addresses and ports. Network Load Balancers are often used for TCP and UDP traffic, as well as for scenarios where low latency and high throughput are critical. They support features like TCP/UDP load balancing and health checks.

### Video - [Lab Intro: Application Load Balancer with Google Cloud Armor](https://www.cloudskillsboost.google/course_templates/155/video/518210)

- [YouTube: Lab Intro: Application Load Balancer with Google Cloud Armor](https://www.youtube.com/watch?v=jYBlr5E-A6E)

In this next lab, you’ll configure an Application Load Balancer with Google Cloud Armor. Google Cloud Armor is used to help mitigate Distributed Denial-of-Service (DDoS) attacks. In the lab titled “Application Load Balancer with Google Cloud Armor,” you’ll configure an Application Load Balancer with global backends and stress test the load balancer and blocklist the stress test IP with Google Cloud Armor. During this lab, you’ll: Create HTTP and health check firewall rules. Configure two instance templates. Create two managed instance groups. Configure an Application Load Balancer with IPv4 and IPv6. Stress test an Application Load Balancer. And blocklist an IP address to restrict access to an Application Load Balancer.

### Lab - [Application Load Balancer with Cloud Armor](https://www.cloudskillsboost.google/course_templates/155/labs/518211)

Configure an Application Load Balancer with global backends, stress test the Load Balancer, and denylist the stress test IP with Cloud Armor.

- [ ] [Application Load Balancer with Cloud Armor](../labs/Application-Load-Balancer-with-Cloud-Armor.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/155/quizzes/518212)

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/155/video/518213)

- [YouTube: Summary](https://www.youtube.com/watch?v=n8Kf_-pNsJ8)

This brings us to the end of the seventh module of the Google Cloud Computing Foundations course. Let’s do a quick summary. You explored Virtual Private Clouds (VPCs). A Virtual Private Cloud, or VPC, is a secure, individual, private cloud-computing model hosted within a public cloud (like Google Cloud!). A Virtual Private Cloud is made up of subnetworks, or subnets, and each subnet must be configured with a private IP CIDR address. You examined Google’s network architecture, including regions and points of presence. You also learned about five of Google Cloud networking products: Google Cloud VPC, Cloud Load Balancing, Cloud CDN, Cloud Interconnect, and Cloud DNS. Learned how to use multiple VPC networks with VPC peering and shared VPC. Explored options to build hybrid clouds. Options include IPsec VPN protocol, Direct Peering, Carrier Peering, Dedicated Interconnect, and Partner Interconnect. Examine load balancing options, including Cloud Load Balancing.

## Keeping an eye on things

Automation: Identify cloud automation and management tools

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/155/video/518214)

- [YouTube: Introduction](https://www.youtube.com/watch?v=Tg_JNyFqhow)

Welcome to module 8 of the Google Cloud Computing Foundations course: Keeping an eye on things. In this section of the course, you’ll look at the role that Google Cloud can play in automating the creation and management of your Google Cloud resources, and ensuring that your infrastructure and applications run optimally. This means that you’ll: Learn about Infrastructure as Code (IaC). Explore Terraform as an IaC option. Examine the role of monitoring, logging, error reporting, tracing, and profiling in the cloud. And learn how to use Google Cloud Observability for monitoring, logging, error reporting, tracing, and profiling. The module agenda follows the objectives. You’ll begin with an introduction to Infrastructure as Code before you move on to Terraform as an IaC tool. You’ll then learn about the importance of monitoring and managing existing services, applications, and infrastructure. Next, you’ll look at the integrated tools that comprise Google Cloud Observability. From there you’ll learn how to use Cloud Monitoring through a hands-on lab. And finally, the module concludes with a short quiz and a recap of the topics covered.

### Video - [Introduction to Infrastructure as Code (IaC)](https://www.cloudskillsboost.google/course_templates/155/video/518215)

- [YouTube: Introduction to Infrastructure as Code (IaC)](https://www.youtube.com/watch?v=px3g0ezSlmc)

Let's start with the first topic, where we’ll discuss the concept of Infrastructure as Code. Creating an environment in Google Cloud can mean lots of work, like setting up a compute network and storage resources, and then monitoring their configurations. This process can be done manually by writing the commands you need to set up your environment the way you want. However, this is labor-intensive. If you want to change the environment, it requires updating commands. Or if you want to clone an environment, you must manually write new commands. As the name implies, infrastructure as code, or IaC, is taking what a required infrastructure needs to look like and defining it as code. You capture the code in a template file that is both human readable and machine consumable. IaC tools allow for the creation of entire architectures by using templates. The template is a configuration file that supplies the details of the infrastructure. Rather than having to use a console or run commands manually to build all parts of the system, the template is used to automatically build the infrastructure. The same template enables resources to be automatically updated and/or deleted as required. Because templates are treated as code, they can be stored in repositories, tracked using version control systems, and shared with other users. Templates can also be used for disaster recovery. If for any reason the infrastructure must be rebuilt, the templates can be used to automatically recover.

### Video - [Terraform](https://www.cloudskillsboost.google/course_templates/155/video/518216)

- [YouTube: Terraform](https://www.youtube.com/watch?v=ammhpymRZsA)

Now let’s explore Terraform, an open source Infrastructure as Code tool. You saw in the last section how manually creating an environment in Google Cloud can be a labor-intensive task, and how using a template can make the process more efficient. Terraform is an open source tool that lets you use templates to provision Google Cloud resources. To use Terraform, you create a template file by using HashiCorp configuration language (HCL), which describes what the components of the environment should look like. Terraform then uses that template to determine the actions needed to create the environment your template describes. If you need to change the environment, you can edit your template and then use Terraform to update the environment to match the change. A deployment can be repeated as many times as required with consistent results, and you can delete an entire deployment with one command or click. Terraform uses the underlying APIs of each Google Cloud service to deploy your resources. This enables you to deploy almost everything we have seen so far, from instances, instance templates, and groups, to VPC networks, firewall rules, VPN tunnels, Cloud Routers, and load balancers. You can also store and version-control your Terraform templates in Cloud Source Repositories. In addition to Terraform, Google Cloud also provides support for other third-party, open source IaC tools.

### Video - [Monitoring and managing services, applications, and infrastructure](https://www.cloudskillsboost.google/course_templates/155/video/518217)

- [YouTube: Monitoring and managing services, applications, and infrastructure](https://www.youtube.com/watch?v=0ZNS_daW6oU)

In this section, you’ll learn which activities need to be performed to monitor and manage existing services, applications, and infrastructure. Monitoring is the foundation of product reliability. It reveals what needs urgent attention and shows trends in application usage patterns, which can yield better capacity planning, and generally help improve an application client's experience and minimize their pain. In Google's Site Reliability Engineering book, which is available at landing.google.com/sre/books, monitoring is defined as: "Collecting, processing, aggregating, and displaying real-time quantitative data about a system, such as query counts and types, error counts and types, processing times, and server lifetimes." With monitoring, you can ensure continued system operations, uncover trend analyses over time, build dashboards, alert personnel when systems violate predefined service-level objectives (SLOs), compare systems and systems changed, and provide data for improved incident response–just to name a few tasks. An application client normally only sees the public side of a product, and as a result, developers and business stakeholders both tend to think that the most important way to make the client happy is by spending the most time and effort on developing that part of the product. However, to be truly reliable, even the best products must be deployed into environments with enough capacity to handle the anticipated client load. Great products also need thorough testing, preferably automated, and a refined continuous integration/continuous development (CI/CD) release pipeline. Blameless postmortems and root cause analyses are the DevOps team's way of letting the client know why an incident happened and why it is unlikely to happen again. In this context, we are discussing a system or software failure, but the term “incident” can also be used to describe a breach of security. Transparency here is crucial to building trust.

### Video - [Google Cloud Observability](https://www.cloudskillsboost.google/course_templates/155/video/518218)

- [YouTube: Google Cloud Observability](https://www.youtube.com/watch?v=8auEBHY7UNc)

Now let’s explore Google Cloud Observability. Google Cloud Observability provides powerful monitoring, logging, error reporting, and debugging for applications in the cloud. It equips you with insight into the health, performance, and availability of cloud-powered apps; enabling you to find and fix issues faster. When DevOps personnel want to track exactly what's happening inside Google Cloud projects, they often first think of monitoring. As stated previously, monitoring starts with signal data. Metrics take measurements and use math to align those measurements over time. For example, it might be taking raw CPU usage measurement values and averaging them to produce a single value per minute. Google Cloud, by default, collects more than a thousand different streams of metric data, which can be incorporated into dashboards, alerts, and several other key tools. When data scientists run massive, scalable queries in BigQuery, it’s important for them to know how many queries are currently in flight, how many bytes have been scanned and added to the bill, and data slot usage patterns. It could also be critical to DevOps teams that run containerized applications in Cloud Run to know CPU and memory utilization, and app bill time. And if those same DevOps teams want to enhance the signal metrics from their custom application wherever it's running, they could use open source OpenTelemetry and create their own metrics. Workloads on Compute Engine will benefit from CPU and memory utilization data, along with uptime, disk throughput, and many other metrics. Cloud Monitoring provides visibility into the performance, uptime, and overall health of cloud-powered applications. It collects metrics, events, and metadata from projects, logs, services, systems, agents, custom code, and various common application components, including Cassandra, NGINX, Apache Web Server, and Elasticsearch. Monitoring ingests that data and generates insights through dashboards, metrics explorer charts, and automated alerts. Most log analysis starts with Google Cloud’s integrated Logs Explorer. Logging entries can also be exported to several destinations for alternative or further analysis. Pub/Sub messages can be analyzed in near-real time by using custom code or stream processing technologies like Dataflow. BigQuery allows analysts to examine logging data through SQL queries. And archived log files in Cloud Storage can be analyzed with several tools and techniques. Log data can be exported as files to Cloud Storage, as messages through Pub/Sub, or into BigQuery tables. Logs-based metrics can be created and integrated into Cloud Monitoring dashboards, alerts, and service SLOs. Default log retention in Cloud Logging depends on the log type. Data access logs are retained by default for 30 days, but this is configurable for 3650 days. Admin logs are stored by default for 400 days. Export logs to Cloud Storage or BigQuery to extend retention. The Google Cloud logs that are visible to you in Cloud Logging vary, depending on which Google Cloud resources you use in your Google Cloud project or organization. Four main log categories are audit logs, agent logs, network logs, and service logs. Cloud Audit Logs helps answer the question "Who did what, where, and when?" Admin activity tracks configuration changes. Data access tracks calls that read the configuration or metadata of resources and user-driven calls that create, modify, or read user-provided resource data. System events are non-human Google Cloud administrative actions that change the configuration of resources. Access Transparency provides you with logs that capture the actions Google personnel take when they access your content. Agent logs use a Google-customized and packaged Fluentd agent that can be installed on any AWS or Google Cloud VM to ingest log data from Google Cloud instances– for example, Compute Engine, managed VMs, or containers, and AWS EC2 instances. Network logs provide both network and security operations with in-depth network service telemetry. VPC Flow Logs records samples of VPC network flow and can be used for network monitoring, forensics, real-time security analysis, and expense optimization. Firewall Rules Logging lets you audit, verify, and analyze the effects of your firewall rules. NAT Gateway logs capture information on NAT network connections and errors. Service logs provide access to logs created by developers who deploy code to Google Cloud. For example, if they use Node.js to build a container and deploy it to Cloud Run, any logging to standard out or standard error will automatically be sent to Cloud Logging for easy, centralized viewing. Error Reporting counts, analyzes, and aggregates the crashes in your running cloud services. Crashes in most modern languages are “exceptions,” which are not caught and handled by the code itself. Its management interface displays the results with sorting and filtering capabilities. A dedicated view shows the error details: time chart, occurrences, affected user count, first and last-seen dates, and a cleaned exception stack trace. You can also create alerts to receive notifications on new errors. Cloud Trace, based on the tools Google uses on its production services, is a tracing system that collects latency data from your distributed applications and displays it in the Google Cloud console. Cloud Trace can capture traces from applications deployed on App Engine, Compute Engine VMs, and Kubernetes Engine containers. Performance insights are provided in near-real time, and Cloud Trace automatically analyzes all of your application's traces to generate in-depth latency reports to surface degradations. Cloud Trace continuously gathers and analyzes trace data to automatically identify recent changes to your application's performance. Poorly performing code increases the latency and cost of applications and web services every day, but nobody knows or does anything about it. Cloud Profiler changes this by using statistical techniques and extremely low-impact instrumentation that runs across all production application instances to provide a complete CPU and heap picture of an application without slowing it down. With broad platform support that includes Compute Engine VMs, App Engine, and Kubernetes, Cloud Profiler lets developers analyze applications that run anywhere, including Google Cloud, other cloud platforms, or on-premises, with support for Java, Go, Python, and Node.js. Cloud Profiler presents the call hierarchy and resource consumption of the relevant function in an interactive flame graph that helps developers understand which paths consume the most resources and the different ways in which their code is actually called.

### Video - [Lab Intro: Cloud Monitoring: Qwik Start](https://www.cloudskillsboost.google/course_templates/155/video/518219)

- [YouTube: Lab Intro: Cloud Monitoring: Qwik Start](https://www.youtube.com/watch?v=LfgsMV5z28w)

Let’s learn about Cloud Monitoring with this hands-on lab. In the lab titled “Cloud Monitoring: Qwik Start,” you’ll monitor a Compute Engine virtual machine instance with Cloud Monitoring. During this lab you’ll: Create a Compute Engine instance. Add an Apache2 HTTP server to the instance. Create an uptime check and alerting policy. Create a dashboard and chart. View logs. Check the uptime check results and triggered alerts.

### Lab - [Cloud Monitoring: Qwik Start](https://www.cloudskillsboost.google/course_templates/155/labs/518220)

This lab shows you how to monitor a Compute Engine virtual machine (VM) instance with Cloud Monitoring.

- [ ] [Cloud Monitoring: Qwik Start](../labs/Cloud-Monitoring-Qwik-Start.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/155/quizzes/518221)

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/155/video/518222)

- [YouTube: Summary](https://www.youtube.com/watch?v=iQkXCDZasII)

This brings us to the end of the eighth module of the Google Cloud Computing Foundations course. Let’s do a quick summary. First, you learned about Infrastructure as Code (IaC). And then you explored Terraform as an IaC option. Next, you examined the role of monitoring, logging, error reporting, tracing, and profiling in the cloud. And finally, you learned how to use Google Cloud Observability for monitoring, logging, error reporting, tracing, and profiling.

### Document - [Demonstrate your capabilities further!](https://www.cloudskillsboost.google/course_templates/155/documents/518223)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
