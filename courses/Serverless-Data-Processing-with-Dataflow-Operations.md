---
id: 264
name: 'Serverless Data Processing with Dataflow: Operations'
type: Course
url: https://www.cloudskillsboost.google/course_templates/264
date_published: 2025-01-20
topics:
  - Reliability
  - CI/CD
---

# [Serverless Data Processing with Dataflow: Operations](https://www.cloudskillsboost.google/course_templates/264)

**Description:**

In the last installment of the Dataflow course series, we will introduce the components of the Dataflow operational model. We will examine tools and techniques for troubleshooting and optimizing pipeline performance. We will then review testing, deployment, and reliability best practices for Dataflow pipelines. We will conclude with a review of Templates, which makes it easy to scale Dataflow pipelines to organizations with hundreds of users. These lessons will help ensure that your data platform is stable and resilient to unanticipated circumstances.

**Objectives:**

* Perform monitoring, troubleshooting, testing and CI/CD on Dataflow pipelines.
* Deploy Dataflow pipelines with reliability in mind to maximize stability for your data processing platform

## Introduction

This module covers the course outline

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/264/video/521558)

* [YouTube: Course Introduction](https://www.youtube.com/watch?v=J4Zc2VFFNR0)

Mehran: Hi, and welcome to the third installment of the Dataflow series: Dataflow Operations. My name is Mehran Nazir, and I'm a product manager with Google Cloud Dataflow. You have arrived at the final course of the Dataflow course series, which seeks to provide you all of the skills needed to build your modern data platform on Dataflow. In the Foundations course, we learned about the building blocks of Dataflow, including Shuffle, Streaming Engine, Flexible Resource Scheduling, and Beam portability. We also covered horizontal integrations with Dataflow, including IAM, quotas, and security features. We then move to Developing Pipelines, which explored how you can turn your business logic into a Dataflow Pipeline. We reexamined the building blocks of the Beam SDK, introduced advanced features like state and timers, reviewed best practices, and concluded with a deep dive on SQL, data frames, and notebooks. In the last installment of the Dataflow course series, we will introduce the components the Dataflow operational model. These lessons will help ensure that your data platform is stable and resilient to unanticipated circumstances. Let's review the outline for the Dataflow Operations course. First, we'll explore Dataflow monitoring, which includes a walkthrough of the various screens of the Dataflow console experience. Next, we'll discuss the logging and Error Reporting integrations, a critical piece of the Dataflow Operations stack. We will review our recommended approach for troubleshooting and debugging Dataflow Pipelines, then explore common causes for Pipeline errors. From there, we will do a thorough examination of performance optimization techniques for Dataflow Pipelines. This module will help you get the most out of your Dataflow jobs. We will discuss testing and continuous integration/continuous deployment, otherwise known as CI/CD, with Dataflow which will help you safely test and roll out changes to your Pipelines. We will move on to reliability with Dataflow Pipelines and discuss methods for building systems that are resilient to corrupted data and data center outages. The final module of this course will cover Flex Templates, a feature that helps data engineering teams standardize and reuse Dataflow Pipeline code. Many operational challenges can be solved with Flex Templates. We will conclude the course with a recap of all the key lessons from the modules. Let's get started.

## Monitoring

In this module, we learn how to use the Jobs List page to filter for jobs that we want to monitor or investigate. We look at how the Job Graph, Job Info, and Job Metrics tabs collectively provide a comprehensive summary of your Dataflow job. Lastly, we learn how we can use Dataflow's integration with Metrics Explorer to create alerting policies for Dataflow metrics.

### Video - [Job List](https://www.cloudskillsboost.google/course_templates/264/video/521559)

* [YouTube: Job List](https://www.youtube.com/watch?v=iJtZcQ8PqV4)

Omar: Hey, there, my name is Omar Ismail, a solutions developer at Google Cloud. You have launched your Apache Beam pipeline onto Dataflow, and now you want to view its usage. What monitoring resources exist? There is a Dataflow monitoring page which you will learn to use for both batch and streaming jobs. Dataflow also integrates with cloud monitoring. Cloud monitoring is mostly used as a back end to build the Dataflow UI, and you can use it as well to build your own custom dashboards. Let's start by looking at the job list page in Dataflow. This is the first page you see when you visit Dataflow. The job list page shows all the jobs that have run over the last 30 days. If we click on the running button... We see only the jobs that are running. In my project, I currently have two. if we toggle back to show all jobs and enable sorting... the columns become sortable. For example, I can sort by the start time... or by pipeline status. Let's switch off sorting... and view how filtering works. Click into the filter text box, and you can choose any of the columns to filter by. I'm going to select status. And that shows me all the available statuses. Let me select all the ones that failed. I can use more than one filter and set its relationship to the previous one. "And" is implied by default. So if you want your condition to be an "or," you'll need to select the "or" option. I am not going to set an "or." Instead, I'm going to filter by name... And find jobs that have the word "Wikipedia" in them. Imagine a use case where you want to check the status of all of your failed jobs that ran with the word "Wikipedia" in them. Instead of having to add that filter every time, you can bookmark that link and revisit those jobs every time. If I take the link in the URL... Copy it... Open a new tab... And then paste in go... We can see that it takes us back to the filtered view that we had. We can then go ahead and add the link as a bookmark.

### Video - [Job Info](https://www.cloudskillsboost.google/course_templates/264/video/521560)

* [YouTube: Job Info](https://www.youtube.com/watch?v=IfzjMEWu-No)

person: Now that we have seen where all the data flow jobs are listed, let us explore how we can monitor one job. The first page you see when you click on a job is the job graph page. Let's go over what we see on the screen. To the right, we have the job Info panel. This shows basic metadata about the job, such as the regional endpoint being used, the worker location, and the encryption type. Below that, we see the number of resources that are in use. And below that, we see the parameters used to run the pipeline.

### Video - [Job Graph](https://www.cloudskillsboost.google/course_templates/264/video/521561)

* [YouTube: Job Graph](https://www.youtube.com/watch?v=de96TzefOjI)

person: Now let's focus on what we see on the job graph. In the middle of the page, we see a visual representation on the steps from my Beam code. I am running a batch pipeline that reads data from a BigQuery table, reshuffles it, and writes it to a cloud storage bucket in TensorFlowRecord format. The records are split across training, validation, and test sets. In Beam, some steps are made up of substeps. We can see these by expanding each step. As this is a batch job, steps are executed sequentially. The next part should not start until the one before it finishes. DataFlow optimizes your pipeline for both streaming and batch pipelines. Part of the optimization includes fusing multiple steps in your pipeline into single steps. If we press on any step, we can see how DataFlow splits each stage into a number of optimized stages. Some stages will be shared between different steps. For example, if I press on the RecordToExample step and view its stages, and then press on ReshuffleResults and view its stages, we can see they share a common stage. When that stage starts, the UI will show both RecordToExample and ReshuffleResults running. If we were running a streaming job, all the stages and steps would run concurrently. Pressing on each step not only shows us the optimized stages, but also throughput info for each step across time. Below that, we see the total number of elements added and the estimated size. Another metric available at each step and substep is the wall time. This shows the total amount of time by the assigned workers to run each step. This can be a useful metric to look at when you want to see where your workers are spending the most amount of time. Eventually, the batch job will complete. In batch jobs, as we are dealing with a known amount of data, jobs do get completed. Once a job finishes, all steps should be marked with a green check mark, as shown here. If a job fails, the steps that failed will be shown in red with an error symbol. As streaming jobs process unbounded collections, there is no completion time for the job unless you cancel or drain it. Beam lets you set custom metrics for your pipeline. The metrics class has three methods that can be used: counter, distribution, and gauge. The counter method lets you increment and decrement any variable or event you are interested in checking. The distribution method is not a histogram, but tracks for you the count, minimum, maximum, and the mean. The gauge method lets you see the latest value of their variable you set it to track. Please review our public docs to see which custom metric types are supported in DataFlow. The DataFlow UI displays any custom metric on the right pane of the Job Graph page. For example, this is a pipeline run of the Beam Java word count example. On the DataFlow UI, we see the custom metrics associated with the job here. We count the number of empty lines and the length distribution of each line.

### Video - [Job Metrics](https://www.cloudskillsboost.google/course_templates/264/video/521562)

* [YouTube: Job Metrics](https://www.youtube.com/watch?v=c-yufsjsoSA)

person: The other page available on the Dataflow UI is the Job Metrics tab. This shows us time series data for our job. This page varies between batch and streaming. Let's look at the Job Metrics for the BigQuery two tensor flow records batch pipeline we ran earlier. The first graph shows the number of workers that ran across the lifetime of the job with auto scaling enabled. At certain points during the job, we can see that the Dataflow service decided that more workers were needed to increase the job throughput. The green line shows how many workers are needed, and the blue line shows the current number of workers. There will be a small time gap between the two as each new worker needs time to spin up and for work to be assigned to it. The second graph shows the throughput for each sub step versus time. Recall that your beam steps are made up of sub steps. And here we see the throughput of each one. In the Job Graph tab, we discussed how batch pipelines do not run all the steps concurrently. We can see that on this graph here. The first hump shows the records being read, and the second one shows the records being partitioned and saved to Google Cloud Storage. The third graph shows each CPU utilization percentage. In our job run, we see the all workers reached near 100% CPU utilization. A healthy pipeline should have all the workers running around the same CPU utilization rate. If you see that a couple of your workers are running at 100% and the rest of the workers have low utilization, your pipeline is likely unhealthy and suffering from an uneven distribution of workload. Some beam operations like group by key cannot be split across workers. Each worker will be assigned a range of keys to group. If your data is heavily skewed, one worker could end up doing all the work while the others do nothing. On the CPU utilization graph, we see this as a couple of workers having a high CPU utilization, while the others have low CPU utilization. The last graph in batch pipelines is the worker error log count. As the name suggests, this shows the number of log entries from the workers that had a level of error. In batch jobs. If processing an element fails four times in a row, the whole batch pipeline fails. Let us now look at a streaming pipelines Job Metrics page. This is for a pipeline I ran that reads from Pub/Sub and syncs to BigQuery. Just like batch pipelines, there are graphs for auto scaling throughput, CPU utilization, and worker error log count. In addition to these graphs, there are a few graphs for streaming jobs. Let us start with the first two, the data freshness and system latency graphs. These graphs are great to measure the health of a streaming pipeline. The data freshness graph shows the difference between real time and the output watermark. The output watermark is a timestamp where any time step prior to the watermark is nearly guaranteed to have been processed. For example, if the current time is 9:26 a.m., and the data freshness graphs value at that time is six minutes, that means that all elements with a timestamp of 9:20 a.m. or earlier have arrived and have been processed by the pipeline. The system latency graph shows how long it takes elements to go through the pipeline. If the pipeline is blocked at any stage, the latency will increase. For example, imagine our pipeline reads from Pub/Sub, does some beam transformation on the elements, then syncs them into Spanner. Suddenly, Spanner goes down for five minutes. When this happens, Pub/Sub won't receive confirmation from Dataflow that an element has been sunk into Spanner. This confirmation is needed for Pub/Sub to delete that element. As there is no confirmation, the system latency and data freshness graphs will both rise to five minutes. Once the Spanner service comes back, all the elements will be written into Spanner and data flow will confirm that with Pub/Sub, returning the system latency and data freshness graphs to normal. In addition to the data freshness and system latency graphs, streaming jobs can also have an input and output metrics at the bottom of the metrics page. Input metrics and output metrics are displayed if your streaming Dataflow job has read or written records using Pub/Sub. In my case, I only had Pub/Sub as an input so I can only see input metrics. If I have more than one Pub/Sub source or sink, I can view the metrics of any one of them by clicking on the drop down and choosing the Pub/Sub source or sink I want. In my case, I only have one Pub/Sub source and that is my subscription name data flow fund. The first graph we talked about is the request per second graph. Requests per seconds is the rate of API request to read or write data by the source or sink over time. If this rate drops to zero or decreases significantly for an extended period relative to your expected behavior, then the pipeline might be blocked performing certain operations, or there is no data to read. If this happens, you should review steps that have a high system watermark to see where the blockage is happening. Also, examine the worker logs for errors or indications that slow processing is occurring. The second graph is the response errors per seconds by error type graph. Response errors per second by type error is the rate of failed API requests to read or write data by the source or sink over time. If errors occur frequently and repeatedly, see what they are and cross reference them to the specific error code documentation on Pub/Sub error codes. For all pipelines, you can restrict the timeline for the graphs and logs using the time selector tool. Right now I have a job that has been running for a few hours. How do I focus on a specific time interval? This is where the time selector tool comes in, and I'll show you how to use it. Open the time selector tool by pressing on the button showing the current time range selected. This will open a drop down menu, you can select a time range for the charts and logs ranging from hours to the maximum lifetime of the pipeline. You can even choose a custom time range by setting the start and end time you want to view. Let's click the max time for the pipeline to see how the graphs change across the pipeline's entire time. And press apply to see the change. Keep an eye on the data freshness and system latency graphs. At the beginning of our run, the pipeline had a lot of data to read. If I bring the cursor near the peak of the data freshness graph, we can see the pipeline was approximately 16 hours behind wall time when it started. This is because I first sent data to a Pub/Sub subscription for 16 hours before starting the pipeline. If I want to zoom into a specific time period from the graph, I press on the start point I am interested in and drag and hold to the end of the time period I am interested in. Once I released the pointer all the graphs will be zoomed into the time range highlighted. If I want to exit the zoomed view, I press on the Reset Zoom button at the top.

### Video - [Metrics Explorer](https://www.cloudskillsboost.google/course_templates/264/video/521563)

* [YouTube: Metrics Explorer](https://www.youtube.com/watch?v=sUaiJ_SnfMc)

person: We've explored the DataFlow UI and its monitoring capabilities. Let's look at how Cloud Monitoring Metrics Explorer can be used with DataFlow. Cloud Monitoring Metrics Explorer is a sink into which all the DataFlow metrics we've seen are exported. You can explore DataFlow metrics using the Metrics Explorer and build custom dashboards to view them. The available metrics range from plotting the job status to custom metrics that you create. Here is an example of a custom dashboard using Cloud Monitoring. This dashboard shows the data watermark leg across all pipelines that start with a specific name. While other metrics can be used, it depends on what you want to measure. Some metrics that can be used are shown on this page. For example, if you want to see if your job failed, set is_failed to greater than 0 and filter by job name. If you want to see if a dependency is failing, set up a counter to measure the number of times the dependency is called and plot the results using the user counter metric. You may have noticed that most of the graphs we looked at had a Create Alerting Policy button. Cloud Monitoring gives you the ability to create alerts and be notified when a certain metric crosses a specific threshold. Where can this be useful? In streaming pipelines, if an element fails to get processed, it is retried indefinitely. Streaming pipelines have no concept of failure unless you specifically cancel or drain a job. You can catch indefinite retries by setting an alert if system latency increases over a predefined value. Every time an alert is triggered, an incident and a corresponding event are created. If you specified a notification mechanism in the alert, such as an email or SMS, you will also receive a notification. The alerting policies provided are on a per-pipeline basis, but you can build your own custom alerting policy grouping more than one pipeline using Cloud Monitoring. This is the end of this module. You should now be able to: navigate the DataFlow job details UI, interpret job metric graphs to diagnose pipeline regressions, and set up alerts on DataFlow jobs using Cloud Monitoring.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/264/quizzes/521564)

#### Quiz 1.

> [!important]
> **You would like to set up an alerting policy to catch whether the processed data is still fresh in a streaming pipeline. Which metrics can be used to monitor whether the processed data is still fresh?**
>
> * [ ] job/per_stage_data_watermark_age
> * [ ] job/is_failed
> * [ ] job/data_watermark_age
> * [ ] job/system_lag

#### Quiz 2.

> [!important]
> **You have a Pub/Sub subscription with data that hasn't been processed for 3 days. You set up a streaming pipeline that reads data from the subscription, does a few Beam transformations, and then sinks to Cloud Storage. When the pipeline is launched, it is able to read from Pub/Sub, but cannot sink data to Cloud Storage due to the service account missing permissions to write to the bucket. When viewing the Job Metrics tab, what do you expect to see in the data freshness graph?**
>
> * [ ] Initial start point at 3 days, with a downward sloping line.
> * [ ] Initial start point at 0 days, with an upward sloping line to 3 days and beyond.
> * [ ] Initial start point at 3 days, with a flat horizontal line.
> * [ ] Initial start point at 3 days, with an upward sloping line.

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/264/documents/521565)

## Logging and Error Reporting

In this module, we learn how to use the Log panel at the bottom of both the Job Graph and Job Metrics pages, and learn about the centralized Error Reporting page.

### Video - [Logging](https://www.cloudskillsboost.google/course_templates/264/video/521566)

* [YouTube: Logging](https://www.youtube.com/watch?v=t4wX59vIz8Q)

Omar: Hello there. My name is Omar Ismail, a solutions developer at Google Cloud. In this module, we learn about the Logs panel at the bottom of the Job Graph and Job Metrics pages, as well as the centralized error reporting page. Let's get started with logging. I've created and canceled a streaming pipeline that I designed not to work so we can see some error logs. I set the sync to a bucket I do not have access to. How can I get logging info about this job? Let's start by expanding the Logs panel at the bottom of the page. The first tab is the job logs tab. These are messages that are from the data flow service. We can filter to show a minimum log level. For example, if I want to see logs with a severity of error and above, I would select error. If I'm looking for a specific message, I can type it in the filter text box. The next tab I can click is the Worker Logs tab. Worker logs are from the VMs running the worker. Just like in the Job Logs tab, we can filter by log level and by message. If I want to see logs from a specific step, or any of its sub steps, I click on the sub step or step in the Jobs Graph page. Here I have selected the WriteShardedBundlesToTempFiles sub step that is part of the TextIO.Write step. If I want to return to looking at the general log view, I click on the whitespace outside of this step. This returns us to the Worker Logs tab. We now move on to the Diagnostics tab. This tab shows the frequency of each error across time in your entire project, as well as when it was first seen and last seen. Clicking on the error takes you to the error reporting page, which provides more detailed information. We talk about the error reporting page in the next module. Not only does a Diagnostics tab shows errors coming from user exceptions, it also provides job insights for you. During the pipeline's life, Dataflow analyzes the logs that are part of your job and highlights important ones in the Diagnostics tab. A few of these are listed on this slide. If the jar file provided to the worker were missing required classes, you will get a worker JAR file misconfiguration error message. The Diagnostic tab also shows if the worker VM had to kill a process or shut down due to the JVM crashing. If your code is running a step that is taking a long time to perform operations, you might see a lengthy operation message in the Diagnostic tab. If the slow processing is due to a hotkey, then the Diagnostic tab will show you that a hotkey was detected. In streaming scenarios, your pipeline might fail to process if you are grouping a huge amount of data without using a combined transform, or are producing a large amount of data from a single input element. If this happens, the Diagnostic tab will tell you that the commit key request exceeds the size limit. Finally, if there was a high rate of log messages from the job, and some of them were not sent to cloud logging, throttling logger worker will appear in your Diagnostic tab. In this example, I have a batch job that failed. Upon checking the Diagnostic tab, I can see that the JVM crashed due to memory pressure. Without sifting through the logs, I can find the cause of failure just by looking at the Diagnostics tab. If your pipeline reads from or loads data into BigQuery, there is one more tab that can be viewed. This is the BigQuery Jobs tab, and it can be used for troubleshooting and monitoring BigQuery jobs that are part of your pipeline. This tab will appear if you are using Beam 2.24 and larger and have the BigQuery admin role. Your Beam code can either read an entire BigQuery table or issue a query to read parts of a table. When the former is used, BigQuery exports the table as a JSON file to GCS using an extract job. When the latter is used, BigQuery exports the selected rows as JSON files to GCS using a query job. If either of the two read methods are used, they will appear in the BigQuery jobs tab. The BigQuery IO supports two methods of inserting data into BigQuery, load jobs and streaming inserts. By default, the BigQuery IO uses load jobs when you sync bounded p collections and streaming inserts when you sync unbounded p collections. Only load jobs will appear in the BigQuery Jobs tab. Let's look at a batch job that read and wrote data using BigQuery. The pipeline I ran read from a BigQuery table with stats on tornadoes computed the number of tornadoes in each month and outputted the results to a different BigQuery table. The pipeline read from BigQuery using an extract job and wrote to BigQuery using a load job. Let's view them in the BigQuery Jobs tab. First, select the location to pull BQ jobs from. BigQuery jobs run in the same location as the data set they read from or write to. Let's retrieve the jobs. Depending on how many BigQuery jobs the pipeline run, it may take a few minutes to retrieve the job list. As my job was quick, the jobs are retrieved almost immediately. As we can see, the pipeline ran two BigQuery jobs. The extract job read the BigQuery table and exported the results to GCS, and the load job wrote to BigQuery via GCS as well. If I want more detailed information about each job, I can press on the command line button, and a pop-up window will appear, showing a G Cloud command to run. Let us run it in Cloud Shell and view the results. Some of the statistics available are the destination URL, the table we are reading from, and the length of time the job took to run. We can also see how many bytes were read, the timeline of the job, and if it ran in a BigQuery reservation.

### Video - [Error Reporting](https://www.cloudskillsboost.google/course_templates/264/video/521567)

* [YouTube: Error Reporting](https://www.youtube.com/watch?v=1cERq7EI5-A)

person: Now let's talk about the Error Reporting page. In the last module, we showed the different types of tabs available in the logs pane. One of them was the diagnostic tab, which shows you a list of frequently occurring errors for your job. pressing on the error takes you to the Error Reporting page. Error Reporting aggregates and displays errors produced in your running cloud services. For Dataflow, Error Reporting shows you your most frequent or new errors that occur across all the Dataflow pipelines in your project. You can see how many times the error occurred across a specific timeframe by pressing on the different time ranges on the top right. You can view the jobs where the error occurred. We can see the error occurred in a couple of Dataflow jobs that I ran. If we scroll down, we can see the full stack trace for the error. If your company has an issue tracker, you can link this error reporting page to it by pasting your issue tracker link here. Once you've seen an error and have it handled, you can change its status from open to either acknowledged, resolved or muted. This is the end of this module. You should now be able to use the Dataflow logs and diagnostic widgets to troubleshoot pipeline issues.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/264/quizzes/521568)

#### Quiz 1.

> [!important]
> **Your batch job has failed, and when viewing the Diagnostic tab, you see the following insights:Which of the options below is the best one to undertake to resolve the issue?**
>
> * [ ] Increase the number of machines used
> * [ ] Switch Beam code from Java to Python
> * [ ] Use a larger machine size
> * [ ] Increase persistent disk size

#### Quiz 2.

> [!important]
> **Select all that apply - the BigQuery Jobs tab shows jobs from:**
>
> * [ ] Query jobs
> * [ ] Load jobs
> * [ ] Streaming Extracts
> * [ ] Streaming Inserts

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/264/documents/521569)

## Troubleshooting and Debug

In this module, we learn how to troubleshoot and debug Dataflow pipelines. We will also review the four common modes of failure for Dataflow: failure to build the pipeline, failure to start the pipeline on Dataflow, failure during pipeline execution, and performance issues.

### Video - [Troubleshooting workflow](https://www.cloudskillsboost.google/course_templates/264/video/521570)

* [YouTube: Troubleshooting workflow](https://www.youtube.com/watch?v=OAuXXpTv4wY)

Omar: Hey there. My name is Omar Ismail, a solutions developer at Google Cloud. In this module, we'll learn how to troubleshoot and debug Dataflow Pipelines. We will begin by looking at the general troubleshooting workflow for Dataflow. The general troubleshooting workflow involves two steps: first, checking for errors in the job, and second, looking for anomalies in the Job Metrics tab. Let us begin by looking at how to check for errors in a job. The most common first step is to look in the Dataflow Jobs page and notice the status of the job in question. If the job is in the failed state, we can then click into the job and dive deeper into the root cause. It is also important to note that not all problematic jobs will be in a failed state, it is possible that certain problematic jobs are still in the running state. In the Job Graph view page, the most common place to look for errors is the error notification above the job graph. If the job failed, then you will also see the individual step in the job graph that failed. More detailed error messages can be found by expanding the Log section, as shown below. Click the open icon to view the full logs in Cloud Logging. Cloud logging provides a simple UI to filter and search for logs within the job. The next part of this section involves looking for anomalies in the job using the Job Metrics tab. Data freshness and system latency are good indicators of the performance of a streaming Dataflow job. Increasing data freshness indicates that the Pipeline workers are unable to keep up with the data being ingested into the Pipeline. Increasing system latency indicates that a certain work item within the Pipeline is taking a long time to get processed. For all Dataflow jobs, the CPU utilization graph is a good indicator of the parallelism in a job and can also indicate if a job is CPU-bound. The latter half of the CPU graph shown here is a good example of limited parallelism in a Pipeline where only one or fewer workers have a high CPU utilization with others close to zero.

### Video - [Types of troubles](https://www.cloudskillsboost.google/course_templates/264/video/521571)

* [YouTube: Types of troubles](https://www.youtube.com/watch?v=65TdpfdySjA)

person: Let's move on to the different types of troubles seen in data flow pipelines. Typically, a failed or problematic Apache Beam pipeline run can be attributed to one of the following causes; one, graph or pipeline construction errors when building the job graph. Two, errors in job validation by the Dataflow service. Three, exceptions during pipeline execution. And finally, slow running pipelines or lack of output that affect the performance of the pipeline. Let's start by looking at the first type of trouble. These types of errors occur while Apache Beam is building your Dataflow pipeline, and validating the beam aspects as well as the input/output specifications of your pipeline. These errors can typically be reproduced using the direct runner and can be tested against using unit tests. Keep in mind that no job will be created on the Dataflow service if there is an error while building the pipeline. The example shown here depicts an error where the pipeline code is performing an illegal operation that is checked and caught while building the code on the beam side. This message should be visible in the console or terminal window where you ran your Apache Beam pipeline. Let's move on to the second type of job troubles. Once the data flow service has received your pipelines graph, the service will attempt to validate your job. This validation includes the following, making sure the service can access your jobs associated cloud storage buckets for file staging and temporary output. Checking the required permissions in your Google Cloud project, making sure the service can access input and output sources such as files. If your job fails the validation process, you'll see an error message in the data flow monitoring interface, as well as in your console or terminal window if you are using blocking execution. The error displayed is an example of a situation in which the pipeline code passed its validation, but the pipeline was rejected by Dataflow due to lack of permissions in the project where the job was tried to run. These errors cannot be reproduced with the direct runner. They require the Dataflow runner and potentially the Dataflow service. To iterate quickly and protect against regression, build a small test that runs your pipeline or a fragment of it. Since the error does not depend on scale, run it on a tiny amount of data, so isn't costly. Let's move on to the third type of job troubles. While your job is running, you may encounter errors or exceptions in your worker code. These errors generally mean that the do functions in your pipeline code have generated unhandled exceptions, which result in failed tasks in your Dataflow job. Exceptions in your user code are reported in the Dataflow monitoring interface. You can investigate these exceptions using the general troubleshooting workflow described in the beginning of this module. The above screenshot shows that using cloud logging to see the error from the data flow interface gives us a more detailed stack trace on the exception. Consider guarding against errors in your code by adding exception handlers. For example, if you'd like to drop elements that fail some custom input validation done in a ParDo, handle the exception within your DoFn and drop the elements or return it separately. More details on this can be found in the best practices module of the developing pipelines with Dataflow course. You can also track failing elements in a few different ways. You can log the failing elements and check the output using cloud logging. You can check the Dataflow worker and worker startup logs for warnings or errors related to work item failures. And finally, you can have your ParDo write the failing elements to an additional output for later inspection. It is important to note that batch and streaming pipelines have different behaviors and handle exceptions differently. In batch pipelines, the Dataflow service retries failed tasks up to four times. In streaming pipelines, your failed job may stall indefinitely. You will need to use other signals to troubleshoot your job. High data freshness, job logs, cloud monitoring, metrics for pipeline progress, and error accounts. Let's move on to the final type of job troubles. Multiple factors such as pipeline design, data shape, interactions with sources, sinks, and external systems can affect the performance of a pipeline. These will be covered in further detail in the performance module of this course. The user interface provides useful information to debug performance problems at a step level. Use the user interface to identify expensive steps. The step info section can provide useful information including wall time, input elements, input bytes, output elements, and output bytes. Wall time for a step provides the total approximate time spent across all threads in all workers on the following actions: initializing the step, processing data, shuffling data, and ending the step. The input element count is the approximate number of elements that the step received, and the estimated size provides the total volume of data that was received. Similarly, the output element count is the approximate number of elements produced by the step, and the estimated size provides the total volume of data that was produced. This is the end of this module. You should now be able to use a structured approach to debug Dataflow pipelines and examine common causes for pipeline failures.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/264/quizzes/521572)

#### Quiz 1.

> [!important]
> **Which two of the following statements are true for failures while building the pipeline?**
>
> * [ ] The error message is visible in Dataflow.
> * [ ] The failure can be caused by insufficient permissions granted to the controller service account.
> * [ ] The failure is reproducible with the Direct Runner.
> * [ ] The failure can be caused by incorrect input/output specifications.

#### Quiz 2.

> [!important]
> **Your Dataflow batch job fails after running for close to 5 hours. Which two of the following troubleshooting steps would you take to understand the root cause of the failure?**
>
> * [ ] Log the failing elements and check the output using Cloud Logging.
> * [ ] Investigate the wall time of the individual steps in the job.
> * [ ] Monitor the Data Freshness and System Latency graphs to understand the job performance.
> * [ ] Check the Dataflow worker logs for warnings or errors related to work item failures.

### Lab - [Serverless Data Processing with Dataflow - Monitoring, Logging and Error Reporting for Dataflow Jobs](https://www.cloudskillsboost.google/course_templates/264/labs/521573)

In this lab, you a)configure logging for your dataflow jobs, and b)explore the Error Reporting page 

* [ ] [Serverless Data Processing with Dataflow - Monitoring, Logging and Error Reporting for Dataflow Jobs](../labs/Serverless-Data-Processing-with-Dataflow-Monitoring-Logging-and-Error-Reporting-for-Dataflow-Jobs.md)

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/264/documents/521574)

## Performance

In this module, we will discuss performance considerations we should be aware of while developing batch and streaming pipelines in Dataflow.

### Video - [Pipeline Design](https://www.cloudskillsboost.google/course_templates/264/video/521575)

* [YouTube: Pipeline Design](https://www.youtube.com/watch?v=4bV0pNalCB0)

Hi, I am Ajay, I am a Strategic Cloud Engineer at Google. In this module, we will discuss performance considerations we should be aware of while developing batch and streaming pipelines in Dataflow. We will begin with some general pipeline design considerations, then we will discuss effects of data shape on performance of a pipeline. Next we will see impact of external systems on a dataflow pipeline. In the end we will wrap up this section with insights of Dataflow specific performance optimization options. Let’s begin with pipeline design decisions which impacts performance of a pipeline. We might sometimes underestimate simple considerations that are critical to a pipeline’s performance. Filtering data early might be considered one such option. It is recommended to place transformations that reduce the volume of data as high up on the graph as possible. This includes placing them above window operations, even though the Window transform itself does nothing more than tag elements in preparation for the next aggregation step in the DAG. Choose coders that provide good performance. For example, in the Java SDK, do not use SerializableCoder. Choose a more efficient coder, for example ProtoCoder or Schemas. Pro Tip: Encoding and decoding are a large source of overhead. Thus, if you have a large blob but only need part of it, you could selectively decode just that part. For example, 'com.google.protobuf.FieldMask' in Protobufs enables reading specific bits of information without deserializing whole blob. If your pipeline has large windows aggregating large volumes of data, you can create smaller Window + Combine patterns before the main sliding window to reduce the volume of elements to be processed when the window slides. Runners may support fusion as part of graph optimization. Such optimizations can include fusing multiple steps or transforms in your pipeline's execution graph into a single phase. There are a few cases in your pipeline where you may want to prevent the Dataflow service from performing fusion optimizations. Before we discuss graph optimization further let’s first understand what a fanout transformation is. In a fanout transformation a single element can output hundreds or thousands of times as many elements. For example in this diagram, a single input element (key1 and value1) outputs multiple output elements. The primary example where fusion is not desirable is a large fanout transform. To prevent Fusion you can insert a Reshuffle after your first ParDo. The Dataflow service never fuses ParDo operations across an aggregation. Alternatively you can pass your intermediate PCollection as a side input to another ParDo. The Dataflow service always materializes side inputs. One of the common service ticket resolutions for performance comes from that old favorite: too much logging! In the Dataflow runner, logs are sent from all workers to a central location in Stackdriver. A thousand machines, all pushing hundreds of logs per second, can cause massive back pressure! Log.info should almost always be avoided against PCollection element granularity. These will rarely be useful in logs. Log.error should also be carefully considered. Using a dead letter pattern followed by a count per window of 5 minutes may be better suited for reporting data errors.

### Video - [Data Shape](https://www.cloudskillsboost.google/course_templates/264/video/521576)

* [YouTube: Data Shape](https://www.youtube.com/watch?v=Rr0MNKbrstg)

In this section we will discuss the effects of data shape on a pipeline’s performance. Unique characteristics of data you are processing also effects the performance of Dataflow pipelines. For example, data skew often results in unbalanced data processing. As discussed in the “Serverless Data Processing with Dataflow” course, operations like groupByKey merge multiple PCollections into one. During a GroupByKey or combine operation, keys will be shuffled to workers. All values related to one key will be sent to the same machine throughout the process. This can be a problem when the data you are processing is skewed. For example, columns used as keys that are @nullable often end up being hot keys. To mitigate the hot key issue, we can use one of the following three techniques. The first one is to use the helper API “withFanout(int)”. This allows for the definition of intermediate workers before the final combine step. Another similar API is withHotKeyFanout(Sfn). It is available for Combine.perkey and allows for a function to determine intermediate steps. Using Dataflow Shuffle for batch or Streaming Service also alleviates this issue. The Dataflow shuffle or streaming service offloads the shuffle operation to a backend service. This means that shuffle operation is not constrained by resources available on a single worker machine. The Dataflow service makes it easier to detect and surface hot keys. To do so, set “hotKeyLoggingEnabled” flag to true. Enabling this flag will print the specific key that is your bottleneck, which can help Dataflow developers to implement custom logic for that specific key. Without the flag, Dataflow will print if they think they've detected a hot key, but cannot reveal what that key is. Key space used in your pipeline also has an impact on its performance. For example, the maximum amount of parallelism is determined by the number of keys. More machines will not be able to do any more work if key space is limited. Below are some general guidelines regarding key space: Too few keys is bad for performance. Limited keyspace will make it hard to share workload, and per-key ordering will kill performance. Too many keys can be bad too as overhead starts to creep in. If the key space is very large, consider using hashes separating keys out internally. This is especially useful if keys carry date/time information. In this case you can "re-use" processing keys from the past that are no longer active, essentially for free. Pro Tip! If windows are distinct, the window can be added as part of the key to shard work across more workers. Adding the window to the key improves the ability of the system to parallelize processing since those keys can now be processed in parallel on different machines since they are now recognized as unrelated.

### Video - [Source,  Sinks & external systems](https://www.cloudskillsboost.google/course_templates/264/video/521577)

* [YouTube: Source,  Sinks & external systems](https://www.youtube.com/watch?v=6wxDRex1EZ0)

SPEAKER: In this section, we'll discuss the impact of external systems on a Dataflow pipeline. In the Dataflow service, most sources and sinks abstract the user from the need to deal with read stage parallelism. Sometimes this hides underlying issues that impact a pipeline's performance. For example, if you are reading gzip files via TextIO, gzip files can't be read in parallel. A single thread will deal with each file. This will have three negative effects. First one is that only one machine can do the Read I/O operation. After the read stage, all few stages will need to run on the same worker that read the data. In any shuffle stage, a single machine will need to push all the data from the file to all other machines. This single-host network becomes the bottleneck. Switch to uncompressed files while using TextIO or switch to compressed Avro format. Beam runners are designed to be able to rapidly chew through parallel work, pin up many threads across many machines to achieve this goal. This can easily swamp an external system. This is an issue for both batch and streaming pipelines. The effect on the external system are often more pronounced in batch or during backlog processing in a streaming pipeline. To alleviate this issue, make use of a batch mechanism in the call to external system and use a mechanism like group into batches, transforms, or start bundle and finish bundle. You should also provision external services to handle the peak volume of the data flow pipeline. While working in cloud, it's sometimes easy to forget the impact of the simple choices we make while developing applications. Co-location is one such aspect. Using services and resources from the same region usually means relatively lower latency for interservice communication. This lower latency may result in significant performance gains, especially when the pipeline involves significant interaction with external services like BigQuery, Bigtable, or any other service outside of Dataflow.

### Video - [Shuffle and streaming engine](https://www.cloudskillsboost.google/course_templates/264/video/521578)

* [YouTube: Shuffle and streaming engine](https://www.youtube.com/watch?v=1K-9PTUn8qQ)

Now let’s explore some Dataflow-specific performance optimization options. Dataflow Shuffle is the base operation behind Dataflow transforms such as GroupByKey, CoGroupByKey, and Combine. The Dataflow Shuffle operation partitions and groups data by key in a scalable, efficient, fault-tolerant manner. Currently, Dataflow uses a shuffle implementation that runs entirely on worker virtual machines and consumes worker CPU, memory, and persistent disk storage. The service-based Dataflow Shuffle feature, available for batch pipelines only, moves the shuffle operation out of the worker VMs and into the Dataflow service backend. The service-based Dataflow Shuffle has the following benefits: Faster execution time of batch pipelines for the majority of pipeline job types. A reduction in consumed CPU, memory, and persistent disk storage resources on the worker VMs. Better autoscaling, since VMs no longer hold any shuffle data and can therefore be scaled down earlier. Better fault tolerance. An unhealthy VM holding Dataflow Shuffle data will not cause the entire job to fail, as would happen if not using the feature. Dataflow Shuffle and the Streaming Engine feature offloads the window state storage operation from the persistent disks (PDs) attached to workers, to a backend service. It also implements an efficient shuffle for streaming cases. The Dataflow Shuffle service is applicable to batch pipelines, while the Streaming Engine service is built for streaming pipelines. No code changes are required to get the benefits of these features. Worker nodes continue running your user code that implements data transforms, and transparently communicate with the Streaming or Shuffle engine to store the pipeline state. Many scalability and autoscaling issues can be resolved by enabling Shuffle and Streaming Engine for your batch and streaming pipelines, respectively. This is the end of this module. You should now be able to: Understand performance considerations for pipelines, and Consider how the shape of your data can affect pipeline performance.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/264/quizzes/521579)

#### Quiz 1.

> [!important]
> **Select options we can use to mitigate data skew in Dataflow pipelines?**
>
> * [ ] Add more worker machines.
> * [ ] Add composite windows and triggers.
> * [ ] Use api like "withFanout" or "withHotKeyFanout".
> * [ ] Use Dataflow shuffle for batch pipelines and Dataflow streaming option for streaming pipelines.

#### Quiz 2.

> [!important]
> **Which one of the following is not a consideration for designing performant pipelines in Dataflow?**
>
> * [ ] Logging
> * [ ] Coders and decoders used in pipeline.
> * [ ] Filtering data early.
> * [ ] SDK used for developing the pipeline.

#### Quiz 3.

> [!important]
> **When we should avoid fusion in a Dataflow pipeline?**
>
> * [ ] Never
> * [ ] Only in specific scenarios, like if your pipeline involves massive fanouts.
> * [ ] Always

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/264/documents/521580)

## Testing and CI/CD

This module will discuss unit testing your Dataflow pipelines. We also introduce frameworks and features available to streamline your CI/CD workflow for Dataflow pipelines.

### Video - [Testing and CI/CD Overview](https://www.cloudskillsboost.google/course_templates/264/video/521581)

* [YouTube: Testing and CI/CD Overview](https://www.youtube.com/watch?v=kA3LO8Cr13M)

Hi! My name is Vince Gonzalez, and I am a data engineer with Google Cloud. This module will introduce frameworks and features available to streamline your CI/CD workflow for Dataflow pipelines. In this module, we'll cover an overview of testing and CI/CD. We'll discuss unit testing your Beam pipelines, integration tests, artifact building, and considerations around deploying your pipelines. Let's get into the overview. Software engineers are no stranger to application lifecycle management. After all, that is how we keep applications fresh and up to date. Dataflow pipelines are no different. Dataflow pipelines are authored according to well-understood best practices within software engineering. First, Dataflow pipelines need a comprehensive testing strategy. So we should be implementing unit tests, integration tests, and end-to-end tests to ensure that our pipeline behaves as we expect. The approach to deployment should also be well structured. A haphazard rollout can result in corrupted data being written to the sink or disruptions to your downstream applications. Finally, data engineers should strive to validate changes made to pipeline logic, and have a rollback plan if there is a bad release. While all these considerations are similar to general application development, there are some key differences to point out. Data pipelines often aggregate data, and this makes them stateful, in that they must accumulate the result of some aggregation over time. This means that if you need to update your pipeline, you need to consider any state that may exist in the pipeline you're updating. We'll discuss this in more detail later, but when you change your pipeline, you'll need to account for existing state, as well as any changes to the pipeline logic and topology. Changes you make need to be compatible with the pipeline you're updating. If they are not, you will have to devise alternate migration strategies that might require reprocessing data. If you do roll out a bad configuration, you could be dealing with more than just an unpleasant experience for end users. If your pipeline makes non-idempotent side effects to external systems, you will have to account for those effects after a rollback. This raises the stakes for ensuring safe releases. Now that we understand some of the challenges that come with testing and deploying data processing applications, let’s take a look at what testing and CI/CD look like with Beam and Dataflow. Testing in Beam is summed up well by this diagram. You can read this from the center out, starting with the Beam pipeline itself, and some hand crafted test inputs, then moving to other PTransforms and DoFn subclasses, before considering integration testing, which involve real data sources and sinks. So let's talk about Unit tests. All pipelines revolve around transforms, and the lowest level we typically deal with in Beam is the DoFn. Since these are essentially functions, we validate their behavior with unit tests that operate on input datasets. They produce output datasets that we validate with assertions. Similarly, we can provide test inputs to the entire pipeline, which might contain our DoFns as well as other PTransforms and DoFn subclasses. We also assert that the results of the entire pipeline are what we expect. For system integration tests, we incorporate a small amount of test data using the actual I/Os. This should be a small amount of data, since our goal is to ensure the interaction with the IOs produces the expected results. Finally, end-to-end tests use a full testing dataset, which is more representative of the data our pipeline will see in production. Whatever tooling you're using in your CI/CD testing environment, you'll make use of the Direct Runner, which runs on your local machine, and your production runners, which run on the cloud service of your choice, like Dataflow. The Direct Runner will be used for local development, unit tests, and small integration tests with your data sources. You'll use your production runner when it's time to do larger integration tests, when you want to test performance, and when you want to test pipeline deployment and rollback. More broadly, the CI/CD lifecycle looks something like this. It's iterative, and moves through a cycle of development, building artifacts and testing, followed by deployment. In the development part of the lifecycle, we write our code, executing unit tests locally using the direct runner and executing integration tests using the Dataflow runner. As we develop and test, we're committing to source repositories along the way. These commits and pushes trigger the continuous integration system to compile and test our code in an automated manner, using Cloud Build or a similar CI system. Once the builds complete successfully, artifacts are deployed, first to a preproduction environment where end-to-end tests are run. If these succeed, we deploy to our production environment.

### Video - [Unit Testing](https://www.cloudskillsboost.google/course_templates/264/video/521582)

* [YouTube: Unit Testing](https://www.youtube.com/watch?v=PrN__co9DwM)

person: Now that we've gotten the overview of the lifecycle, let's take a look at unit testing in some detail. Unit tests are fundamental to all software development, and beam is no exception. We use unit tests in Beam to assert behavior of one small testable piece of your production pipeline. These small portions are usually either individual DoFns or PTransforms. These tests should complete quickly, and they should run locally with no dependencies on external systems. To get started with unit testing in Beam Java pipelines, we need a few dependencies. Beam uses JUnit 4 for unit testing. And here you can see an example of the dependency section of a Maven POM for a Beam pipeline. In a regular Beam pipeline, the pipeline object represents your pipeline. Test pipeline is a class included in the beam SDK specifically for testing transforms. So when writing tests use TestPipeline in place of Pipeline where you would create a Pipeline object. Unlike Pipeline.create, TestPipeline.create handles setting pipeline options internally. PAssert is another class provided as part of Beam that lets you check the output of your transforms. Assertions on the contents of a PCollection are incorporated into the pipeline. These assertions can be checked no matter what kind of pipeline runner is used. PAssert becomes part of your pipeline alongside the transforms. PAssert works on both local and production runners. Putting this all together, unit tests help you ensure the correct functioning of your pipeline. Your pipeline is made up of DoFns sub classes and composite transforms, which might combine multiple DoFns. Unit tests lets you provide known input to your DoFns and composite transforms, then compare the output of those transforms with a verified set of expected outputs. The Apache Beam SDK provides a JUnit rule called TestPipeline for unit testing individual transforms like your DoFns subclasses, composite transforms, like your PTransform subclasses, and entire pipelines. You can use TestPipeline on a Beam pipeline runner such as the direct runner or the Dataflow runner to apply assertions on the contents of PCollection objects using PAssert as shown in the code snippet here. In this minimal example, we instantiate a TestPipeline, then create a test PCollection containing some data, then we assert that the PCollection contains the data we expect in any order. One thing to keep in mind when you're developing your pipeline is that it's an anti-pattern to design your pipeline too much around anonymous subclasses of DoFn. Anonymous subclasses make it impossible to test the correctness of the transform without duplicating the code in the test, which will quickly become a challenge to maintain. Anonymous classes are also not as reusable as named subclasses would be. So prefer named subclasses to anonymous ones. When we compare the right-hand code block to the anti-pattern in the left block, we see that the DoFns are now named subclasses. In other words, rather than putting the DoFn code in line in our ParDo, we create an instance of the Transform. Named subclasses are easily testable, so we can validate their behavior independently without having to execute the entire pipeline. In addition to the functionality of our transforms, we can and should test our assumptions about how window transforms will behave. Beam provides a create.timestamped method which can be used to create timestamped elements in a testing PCollection. You can manipulate the timestamp directly as we do in this example by adding the window duration to the timestamp of the last element. Then in the right-hand code block, you can see that we apply fixed windows of window duration and perform a count on the windowed elements. We can then assert that the resulting PCollection contains the windowed calculations we expect. Note that windowing takes place in both batch and streaming pipelines. Testing how your window transforms behave is useful in both types of pipelines. Speaking of streaming pipelines, let's talk about how to test those. Test stream is a testing input that generates an unbounded PCollection of elements advancing the watermark and processing time as elements are emitted. After all of the specified elements are emitted, test stream stops producing output. Each call to a TestStream.builder method will only be reflected in the state of the pipeline after each method before it has completed and no more progress can be made by the pipeline. The pipeline runner must ensure that no more progress can be made in the pipeline before advancing the state of the test stream. For streaming pipeline tests, we'll use TestStream to create a pipeline object that enables you to model the effect of element timings. Let's take a look at what this looks like in code. To use TestStream you first create a TestStream instance. Then you add timestamped elements to it. You can manipulate the timestamps in the TestStream by adjusting the timestamp object and manipulate the position of the watermark using an instant object. Advancing the watermark to infinity closes all windows so that you can perform your windowed calculation and assert on the result. Test stream is supported by the direct runner and the Dataflow runner. Use both to carry out your streaming pipeline tests. We can also test more complex streaming interactions. In this example, we're asserting on the presence of certain elements in particular panes of a window. We also ensure that for all the panes none have less than three elements or more than five.

### Video - [Integration Testing](https://www.cloudskillsboost.google/course_templates/264/video/521583)

* [YouTube: Integration Testing](https://www.youtube.com/watch?v=mVeOcJs2GSM)

person: Now that we've gotten through the overview of unit testing, let's take a look at integration testing in some detail. An actual pipeline reads from two data sources and writes to BigQuery. Integration tests create a smaller amount of data, and assert that the output of the transforms are what we expect. For large integration tests, we work with data on closer to a production scale. To do this, we can clone data from a production project to a non production project. This diagram looks at a batch pipeline reading from two sources on cloud storage and writing to BigQuery. We can use the storage transfer service to copy cloud storage data. We can copy a BigQuery data set or even work with the production data set is read only. Let's take a look at large integration testing for streaming pipelines. One of the nice things about streaming data sources like cloud pub sub is that you can easily attach extra subscriptions to a topic. This comes at an extra cost. But for any major updates, you should consider cloning the production environment and running through the various lifecycle events. To clone the pub sub stream, you can simply create a new subscription against the production topic. You may also consider doing this activity on a regular cadence, such as after you have had a certain number of minor updates to your pipelines. The other option this brings is the ability to carry out AB testing. This can be dependent on the pipeline and on the update. But if the data you're streaming can be split, for example, on entry to the topic, and the syncs can tolerate different versions of the transforms, then this gives you a great way to ensure everything goes smoothly in production. And integration tests, we typically test the entire pipeline without sources and sinks. In this example, we see a p transform subclass called weather stats pipeline that summarizes integers representing weather data. We create a test pipeline instance and test weather stats pipeline by creating a p collection of integers and asserting that the result of the pipeline transformations match the data we expect.

### Video - [Artifact Building](https://www.cloudskillsboost.google/course_templates/264/video/521584)

* [YouTube: Artifact Building](https://www.youtube.com/watch?v=Lr4TAdG6vmQ)

person: In this section we'll review artifact building. Specifically, we'll cover how we should package Apache Beam with the rest of our pipeline artifacts. Apache Beam uses semantic versioning. Version numbers use the form major dot minor dot incremental and are incremented as follows. Major versions are incremented for incompatible API changes. Minor versions are incremented for new functionality added in a backward incompatible manner. Finally, the incremental version is incremented for forward compatible bug fixes. Build artifacts necessary for your Java pipelines are available on Maven Central. There are lots of packages available for beam and you'll usually need more than just the core. The build system has Maven or Gradle. Here we're showing snippets of a Maven palm. It's typical that you'll need to pull in more than just the core. Notice here that we're pulling in other dependencies for the Dataflow runner and for GCP IOs. We recommend that you use beam 2.26 and higher Versions from 2.226 use the Google Cloud libraries bomb to specify Google related library versions which reduce the potential for dependency conflicts.

### Video - [Deployment](https://www.cloudskillsboost.google/course_templates/264/video/521585)

* [YouTube: Deployment](https://www.youtube.com/watch?v=9dmhnu84iw0)

Having tested, built, and established our environment, let's talk about deployment. We’ll look at three distinct stages of the pipeline lifecycle: deployment, in-flight actions, and termination. Let’s begin with deployment. There are two ways to deploy your Dataflow job. We can use a direct launch, in which we run the pipeline directly from the development environment. For Java pipelines this means running the pipeline from Gradle or Maven. For Python, this means running the python script directly. This method can be used for both batch and streaming pipelines. We can also use templates. Templates allow us to launch a pipeline without having access to a developer environment. Templates are built and deployed as an artifact on Cloud Storage, and can be used for both batch and streaming pipelines. Separating the development and execution environments makes it easy to automate your Dataflow deployments and enable non-technical users. If you're using an external scheduler, like Airflow, you'll be able to use Airflow's built-in support for Dataflow, which calls a template when invoked. You would supply your pipeline options via the operator arguments. More generically, if you're orchestrating the deployment via CLI tooling, you can provide the options via gcloud commands. You might notice that Dataflow SQL is not on this list. Dataflow SQL is a special case of a templated deployment—it’s actually a user interface built on top of a flex template. When we're deploying our pipeline for the first time, we need to submit the pipeline to the Dataflow service with a unique name. This name will be used by the monitoring tools when you look at the monitoring page in the console. Next, we’ll move onto pipeline actions that can be taken on in-flight pipelines. These actions are only available to streaming pipelines, since batch jobs can simply be relaunched. As a streaming pipeline processes data, it accumulates state. It's useful to have ways to preserve the state so that we can manage changes to our pipeline without the risk of permanent data loss. We can use snapshots for this. With snapshots, we can save the state of an executing pipeline before launching a new pipeline. This way, if we need to roll our pipeline back to a previous version, we can do that without losing the data processed by the version being rolled back. Since streaming pipelines are long-running applications, we’re likely to need to modify our pipeline from time to time. Now that we’ve saved the state of the running pipeline with a snapshot, we can safely update it. When you update a job on a Dataflow service, you replace the existing job with a new job that runs your updated pipeline code. The Dataflow service retains the job name, but runs the replacement job with an updated job ID. If for any reason you are not happy with how the replacement job is running, you can roll back to the prior version by creating a job from a snapshot. Let’s explore these two actions a little more deeply. Dataflow snapshots are useful for several scenarios. As mentioned, Dataflow Snapshots provides a copy of intermediate state of your pipeline at the moment the snapshot is taken. You can use that snapshot to validate a pipeline update, or use it as checkpoint for you to roll back your pipeline in the event of an unhealthy release. You can also use Snapshots for backups and recovery use cases. We’ll explore this in more depth in the Reliability module. Lastly, Snapshots create a safe path for migrating pipelines to Streaming Engine. If you want to reap the benefits of smoother autoscaling and superior performance, you can take a snapshot and create a job from that snapshot. The new job will run on Streaming Engine. The flip side of this is that jobs created from Snapshots cannot be run with Streaming Engine disabled. Let’s see how we use Dataflow Snapshots. We’ll cover two Snapshot workflows: creating snapshots, and creating a job from a snapshot. We can create a snapshot in the UI or using the CLI. We can navigate to the Job Details page of the pipeline of interest. You’ll see a Create Snapshot button to initiate the snapshot. After you click on the button, you’ll be prompted to select if your snapshot will or will not be created with a source snapshot. If you are using Pub/Sub, creating a snapshot with source will allow you to create a coordinated snapshot between your unread messages and accumulated state. This makes it easier to roll back your pipeline to a known point in time. The pipeline will pause processing while the snapshot is being taken. Depending on how much state is buffered, it could take a matter of minutes. We recommend planning to take snapshots during periods of the day when latency can be tolerated, such as non-business hours. You can also create snapshots using the CLI or API. These methods allow you to automate snapshots of your Dataflow pipelines, which lends itself nicely to scheduling snapshots on a weekly cadence. To create a job from a snapshot, we have to pass in two extra parameters, as seen in the sample command here. We have to enable Streaming Engine with the --enableStreamingEngine flag. Secondly, we pass in the Snapshot ID into the createFromSnapshot parameter. If you are creating a job from the snapshot for a modified graph, the new graph must be compatible with the prior job. We’ll discuss update compatibility shortly. Now that we’ve snapshotted our pipeline, we’re ready to update our pipeline. There are various reasons why you might want to update your Dataflow job: One is to enhance or otherwise improve your pipeline code. Another is to fix bugs in your pipeline code. You might also want to update your pipeline to handle changes in the data format. Finally, you might want to change your pipeline to account for version and other changes in your data source. To update your pipeline, you’ll need to do a couple of things. First, you need to pass the "update" and "jobName" options when you submit the new pipeline. You’ll have to set jobName to the name of the existing pipeline, or else the old job will not be replaced. This tells Dataflow that you're updating the job, rather than deploying a new pipeline. Second, if you added, removed, or changed any transform names, you'll need to tell Dataflow about these changes by providing a transformNameMapping. The replacement job will preserve any intermediate state data from the prior job. Note, however, that changing the windowing or triggering strategies will not affect data that's already buffered or already being processed by the pipeline. "In-flight" data will still be processed by the transforms in your new pipeline. Additional transforms that you add in your replacement pipeline code may or may not take effect, depending on where the records are buffered. Updates can also be triggered via the API. This can enable continuous deployment contingent on other tests passing. When you update your job, the Dataflow service performs a compatibility check between your currently running job and your potential replacement job. The compatibility check ensures that things like intermediate state information and buffered data can be transferred from your prior job to your replacement job. This means that some changes are not possible with streaming update. Let’s review the most common compatibility breaks. Modifying your pipeline without providing a transform mapping will fail the compatibility check. When you update a job, the Dataflow service tries to match your transforms from your prior job to your new job so that intermediate state data for each step can be fully processed. If your changes have renamed or removed any steps, you will have to provide a transform mapping so that Dataflow can match the state. Adding or removing side inputs will also cause the check to fail. Changing coders. The Dataflow service isn’t able to serialize or deserialize records if your updated job uses different data encoding. Running your job with a new zone and a new region will also cause your compatibility check to fail. Your replacement job must run in the same zone in which you ran your prior job. Lastly, removing stateful operations. Dataflow fuses multiple steps together for efficiency, but if you’ve removed a state-dependent operation from within a fused step, the check will fail. If your pipeline requires any of these changes, we recommend draining your pipeline, then launching a new job with the updated code. Now that we’ve discussed actions you can take on your streaming pipeline, we’ll discuss two ways that you can terminate your pipeline. First, we start with drain. Selecting drain will tell the Dataflow service to stop consuming messages from your source and finish processing all buffered data. After the last record is processed, the Dataflow workers are torn down. This action is only applicable to streaming pipelines. Secondly, we can cancel the job. Using the Cancel option ceases all data processing and drops any intermediate, unprocessed data. We can cancel both batch and streaming jobs. Let’s take a closer look at both of these options. We can terminate our job from the UI. When you navigate to the Job Details page of your job, you will find a Stop button in the menu bar. You’ll be prompted between two options for stopping your job: canceling your job, or draining your job. Let’s explore each of these options. When we drain the pipeline, it will stop pulling data from the source, and it will finish processing data that has already been read into the pipeline. This provides an advantage from cancelling your job outright, since no record is dropped. When you relaunch your streaming job, it will continue processing unacknowledged messages from your source. However, when a streaming pipeline is drained, the watermark is moved to infinity, which closes all windows. Closing all the windows in this way will result in incomplete aggregations, since draining the pipeline will not wait for open windows to be closed before stopping pulling from the source system. Consider the impact of incomplete aggregations on downstream systems when draining your pipelines. Beam attaches a PaneInfo object that provides information about the pane an element belongs to, as every pane is implicitly associated with a window. You can use PaneInfo to identify incomplete windows and choose to write that data elsewhere, which can save you the hassle of reconciling incomplete aggregations with your production dataset. When you cancel a job, Dataflow will immediately begin shutting down the resources associated with your job. The pipeline will stop pulling and processing data immediately, which means you may lose any data that was still being processed when the pipeline was canceled. If your use case can tolerate data loss, then cancelling your job will fit your purpose. So, to summarize the lifecycle of a streaming pipeline, let's review our deployment options. First, if it's the first time deploying the pipeline, there's no existing state to consider. So you just deploy the pipeline. If there's an existing pipeline, and you want to update it, you should take a snapshot of your pipeline. This ensures that you have a working state you can revert your pipeline to if you observe an issue with your new deployment. Once you’ve taken a snapshot, you’re ready to update your job. You need to account for any changes to the names of the pipeline's transformations by providing the mapping from old names to the new. If the updated pipeline is compatible, the update will succeed, and you'll get a new pipeline in place of the old, without losing the state of the previous version of the pipeline. If the update is not possible, then you’ll need to choose between drain and cancel options. If you can replay the source, then you can choose to cancel the pipeline, which will drop any in-flight data. You can then deploy the new pipeline, and replay the data from the source. Note that we cannot use a Dataflow snapshot if the pipeline modifications are not update compatible, but taking a snapshot of the source with a Pub/Sub snapshot, you can minimize unnecessary reprocessing. If replay is not possible, then you can drain the pipeline. This will not lose data, but you may end up with incomplete aggregations in your output sink. Your downstream business logic should inform the appropriate approach to handling this. Once the pipeline has been drained, you can relaunch the pipeline. This is the end of the module. You should now be able to: Execute test approaches with your Dataflow pipeline, Snapshot and update your Dataflow pipelines, And drain and cancel your Dataflow pipelines.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/264/quizzes/521586)

#### Quiz 1.

> [!important]
> **Using anonymous subclasses in your ParDos is an anti-pattern because:**
>
> * [ ] ParDos are required to contain a concrete subclass of a DoFn.
> * [ ] Anonymous subclasses are bad for the performance of your pipeline.
> * [ ] Anonymous subclasses are harder to test than concrete subclasses.

#### Quiz 2.

> [!important]
> **When draining a streaming pipeline, what should you expect to happen?**
>
> * [ ] Ingestion stops immediately, windows are closed, and processing of in flight elements will be allowed to complete.
> * [ ] A snapshot is taken of the source, then ingestion is stopped. Windows are closed and processing of in-flight elements will be allowed to complete.
> * [ ] Any open windows will wait for new data so that aggregations are completed. Then the pipeline will be canceled.
> * [ ] Both processing and ingestion stop immediately.

### Lab - [Serverless Data Processing with Dataflow - Testing with Apache Beam (Java)](https://www.cloudskillsboost.google/course_templates/264/labs/521587)

In this lab you will learn and implement testing concepts for Cloud Dataflow.

* [ ] [Serverless Data Processing with Dataflow - Testing with Apache Beam (Java)](../labs/Serverless-Data-Processing-with-Dataflow-Testing-with-Apache-Beam-(Java).md)

### Lab - [Serverless Data Processing with Dataflow - Testing with Apache Beam (Python)](https://www.cloudskillsboost.google/course_templates/264/labs/521588)

In this lab you will learn and implement testing concepts for Dataflow.

* [ ] [Serverless Data Processing with Dataflow - Testing with Apache Beam (Python)](../labs/Serverless-Data-Processing-with-Dataflow-Testing-with-Apache-Beam-(Python).md)

### Lab - [Serverless Data Processing with Dataflow - CI/CD with Dataflow](https://www.cloudskillsboost.google/course_templates/264/labs/521589)

In this lab you will learn and implement testing and CI/CD concepts for Cloud Dataflow.

* [ ] [Serverless Data Processing with Dataflow - CI/CD with Dataflow](../labs/Serverless-Data-Processing-with-Dataflow-CI-CD-with-Dataflow.md)

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/264/documents/521590)

## Reliability

In this module we will discuss methods for building systems that are resilient to corrupted data and data center outages.

### Video - [Introduction to Reliability](https://www.cloudskillsboost.google/course_templates/264/video/521591)

* [YouTube: Introduction to Reliability](https://www.youtube.com/watch?v=PMhsYU7sYL0)

Federico: Hi, I'm Federico Patota, a cloud consultant here at Google. In this module, we will learn how to implement reliability in Dataflow pipelines. There are different approaches for reliability based on the type of pipeline you are running. Batch jobs are simple. If a batch job does not launch or if it fails during execution, you can always rerun the job. Source data is not lost and partial data written to sinks can be rewritten, if it was written at all. Streaming jobs, on the other hand, are more complex. Streaming jobs are continuously processing data and behave like a long-lived application. Thus, reliability is of the utmost importance. You must be vigilant for various failure modes, and when a failure inevitably occurs, you must act fast to minimize data loss and downtime. Most of the reliability best practices in this specific module are for streaming pipelines. In particular, the second half of this module focuses on disaster recovery and high availability configurations. With that being said, lessons covered in the monitoring and geolocation sections are also relevant for your batch workloads. We can classify the pipeline's failures in two broad categories, failures related to user code and data shapes, and failures caused by outages. Software bugs are a reality of software engineering, including data processing applications. Transient errors and corrupted data can impact your data processing jobs. So it's important to know how to mitigate the adverse effects that can be produced unintentionally by software bugs. Dataflow sits at the center of multiple Google Cloud services. That also means that Dataflow is susceptible to various outage modalities, including service outages, zonal outages, and regional outages. If a network service is down, Dataflow will likely be impacted by it. Similarly, if Compute Engine instances are inaccessible in a particular zone or region where Dataflow workers are running, the data processing jobs will be affected. Since Dataflow is often connected between different parts of a customer application, running on GCP, users need to be especially vigilant for any service disruptions. In the following modules, we will discuss different strategies to mitigate the risk of these incidents and increase the reliability of your Dataflow workloads.

### Video - [Monitoring](https://www.cloudskillsboost.google/course_templates/264/video/521592)

* [YouTube: Monitoring](https://www.youtube.com/watch?v=kEdN0I0xlCw)

person: Now we will discuss how a good monitoring strategy can improve the reliability of your Dataflow workloads. First, we start with a reminder that for batch jobs, tasks with failing items are retried up to four times. After four failures, the job will fail. As mentioned before, a batch job can be rerun with little fear of data loss or interruption of existing services, as long as this is run within a user desired service level objective or resolution. For streaming pipelines, failing work items will be retried indefinitely. The rest of this module, we will discuss techniques to prevent your streaming workloads from being stuck forever. Erroneous records may cause your pipeline to get stuck or fail outright. As described in previous modules, we highly recommend implementing a dead-letter queue, and error logging to prevent these failure modes. This can help catch problems in your code or in data shapes. This code snippet shows an example of a pattern written in Java, There are a few things to know here. We wrapped user code inside a process element function with a try catch block. Inside of the catch block, we do not log every error or exception, as it may overwhelm the whole pipeline. Instead, we send the erroneous record to an alternative dead-letter sink. We use tuple tags so that we can write to multiple outputs in the resulting p collection. This helps us write to downstream p collections as well as send raw data to a persistent storage medium like BigQuery or cloud storage, so that we can inspect them offline. To maximize the reliability of your workloads, it is essential to implement a robust monitoring and alerting strategy. Monitoring and alerting policies can help you catch issues with your data processing before they bring down production systems. It lets you combine different types of metrics and observe important Service Level Indicators or SLS of pipeline performance. If you are comparing those SLIs against acceptable threshold, monitoring can give you critical insights for early detection of potential issues. Dataflow provides a web based monitoring interface that can be used to view and manage jobs. You can create metrics based alerts with a couple of clicks. We've covered this in our monitoring module. In addition, data flows integration with cloud monitoring provides extensive flexibility for pipeline monitoring. You can collect custom metrics that point to health conditions that are relevant for your use case, like the number of erroneous records that have been detected. The possibilities are endless with Dataflow's monitoring integration. For batch workloads, you might be interested in the overall runtime of your job. If the job runs on a recurring schedule, you might want to ensure that the job completes successfully within a given period of time. Some variance in pipeline execution time is expected across runs due to a variety of factors, but if they are violating your service level objectives, or SLOs, you need to be notified right away. With cloud monitoring, you can track the elapsed time for a job and create an alert that goes off if the elapsed time exceeds a threshold that is equivalent to your SLO. This can be entirely done using its integration with Dataflow. For streaming pipelines, you want steady and sustained data processing. Dataflow provides standard metrics like data freshness and system latency that make it easy to track whether your pipeline is falling behind. You can create an alert with a couple of clicks from the Dataflow monitoring UI that will be triggered if this selected metrics fall behind the specified threshold. This is an example of a simple alerting policy. You can combine it with custom metrics or with other statistics to determine the failure condition that matters to your workload. These alerts are essential for improving the reliability posture of your Dataflow pipelines.

### Video - [Geolocation](https://www.cloudskillsboost.google/course_templates/264/video/521593)

* [YouTube: Geolocation](https://www.youtube.com/watch?v=QbAVtgKptJo)

person: In this section we'll talk about how selecting the location of your data for processing can impact the reliability of your pipeline. Dataflow is a regional service. When a user submits a job to a regional endpoint without explicitly specifying end zone, the Dataflow service routes the job to a zone in the specified region based on resource availability. In other words, Dataflow will pick the best zone for the job based on available capacity. If you explicitly specify a zone, you will not get this benefit. If a job submission fails due to a zone issue, retrying without explicitly specifying a zone will usually fix the issue. This is a helpful technique in the event of a zonal outage. Note that you cannot change the location of a job after you got started. If it is a streaming job, you will have to drain or cancel the pipeline first before launching it again. This applies to when you relaunch a job in the same region without the zone specified or if you choose to relaunch a job in an entirely different region. When thinking about the locations of your Dataflow job, there are three elements to be aware: Your sources, your processing, and your sinks. You should always locate your resources in the same region. For an additional layer of reliability, you can also elect to use multiregional sources and sinks. Services like Google Cloud Storage, BigQuery, and Pub/Sub provide geo-redundant options that make your data seamlessly accessible in multiple regions. Dataflow processing can only occur in one region. But in the event of a regional outage, using multiregional sources and sinks allows you to move your data processing to a different region without suffering from performance penalty. You should try to avoid any configurations that have critical cross-region dependency. If you have a pipeline that has a critical dependency on services from multiple regions, your pipeline is likely to be affected by a failure in any of those regions. For example, a pipeline that is reading from my Cloud Storage bucket in us-central1 and writing for BigQuery table in us-east4 could go down if either one of those two regions are down.

### Video - [Disaster Recovery](https://www.cloudskillsboost.google/course_templates/264/video/521594)

* [YouTube: Disaster Recovery](https://www.youtube.com/watch?v=9vg-UbBnB3w)

SPEAKER: In this video, we will look at disaster-recovery methods with Dataflow. These methods only apply to streaming pipelines. Data is your most prized asset, which is why it is essential to have a disaster-recovery strategy in place for your production systems. One way is to take snapshots of your data source. This capability is supported in many popular relational databases and data warehouses. But what if you are using a messaging application? Pub/Sub offers this capability. You can implement the disaster-recovery strategies with two features-- Pub/Sub Snapshots, which allows you to capture the message acknowledgment state of a subscription, and Pub/Sub Seek, which allows you to alter the acknowledgment state of messages in bulk. If you are using this strategy, you will have to reprocess messages in the event of a pipeline failure. This means you will have to consider how to reconcile this in your data sink and the duplicate sum records that have been written twice. Let's go over what we need to do to use Pub/Sub Snapshot to support our disaster-recovery requirements. First, you should take snapshots of the Pub/Sub subscription. To do this, you can use the Command Line Interface, CLI in short, or the Cloud console. After your Pub/Sub snapshot has been created, you can stop and drain your Dataflow pipeline. You can do this using the command line interface or in the Job Details page of the Dataflow UI. Once your pipeline has stopped processing messages, you can use Pub/Sub's Seek functionality to revert the acknowledgment of messages in your subscription. Again, you can achieve this using the command line tool. Finally, you are ready to resubmit your pipeline. You can launch your pipeline using any of the ways that you use to deploy your Dataflow job, either directly from your development environment or by using the command line tool to launch a template. The example on this slide shows a sample command for a templated job that is being launched with the command line interface. An important caveat to consider is that Pub/Sub messages have a maximum data retention of seven days. This means that, after seven days, a Pub/Sub Snapshot no longer has any use for your stream processing. If you choose to use Pub/Sub Snapshots for your disaster recovery, we recommend that you take snapshots weekly, at a minimum, to ensure that you do not lose any data in the event of a pipeline failure. Using Pub/Sub Snapshots in conjunction with Seek is a good starting point. But when you are using Pub/Sub and Dataflow for your streaming analytics, there are important things to consider. When you use Pub/Sub Seek to restart your data pipeline from a Pub/Sub Snapshot, messages will be reprocessed. This creates a few challenges. First, you might observe duplicate records in your sink. The amount of duplication depends on how many messages were processed between the time of when the Snapshot was taken and the time the product line was terminated. In addition to that, data that has been read by your pipeline but yet to be processed and written to sink will need to be processed over again. Remember that Dataflow acknowledges a message from Pub/Sub when it has read the message not when the record has been written to the sink. This presents a challenge for pipelines with complex transformation logic. For example, if your pipeline is processing millions of messages per second and goes through multiple processing steps, having to reprocess the data represents a significant amount of lost compute. Lastly, if your pipeline has implemented exactly once processing, windowing logic will be interrupted when you drain and restart your pipeline. Since you have to lose the buffered state when you drain your pipeline, you must conduct a tedious reconciliation exercise if exactly once processing is a requirement for your use case. Luckily, Dataflow also has Snapshot capabilities. If you recall, we introduced Dataflow Snapshots as a useful tool for testing and rolling back updates to streaming pipelines in our testing and CI/CD module. Dataflow Snapshots can also be used to offer disaster-recovery scenarios. Since Dataflow Snapshots save streaming pipeline's state, we can restart the pipeline without reprocessing in-flight data. This saves you money whenever you have to restart your pipeline. Moreover, you can restore your pipeline much faster than using the Pub/Sub Snapshots and Seek strategy. This ensures that you have minimal downtime. Dataflow Snapshots can be created with a corresponding Pub/Sub source Snapshot. This helps you coordinate the Snapshot of your pipeline with your source. In other words, you can pick up your processing where you left off when you restart the pipeline. This saves you the hassle of having to manage Pub/Sub Snapshots. Let's take a look at how we can use Dataflow Snapshots for disaster-recovery scenarios. Our first step involves creating the Snapshot of the Dataflow pipeline. We can do this directly in the UI with the Create Snapshot button in the Menu bar. You will be prompted to create a Snapshot with or without sources. If your pipeline is using Pub/Sub, we recommend that you select the With Sources option. You can also create a Snapshot using the command line interface. Next, we need to stop and drain your Dataflow pipeline. This is also possible in both the UI and using the command line interface. Lastly, we create a new job from the Snapshot. This is accomplished by passing in the Snapshot ID into a parameter when you deploy your job from your deployment environment. Since Dataflow Snapshots, like its Pub/Sub counterpart, has a maximum retention of seven days, we recommend scheduling a coordinated Dataflow and Pub/Sub Snapshot at least once a week. This means that, if your pipeline goes down, you have a point in time in the past seven days from which you can restart processing, ensuring you can almost always avoid any data-loss scenario. You can use Cloud Composer or Cloud Scheduler to schedule this weekly snapshot. Snapshots are located in the region of the origin job. When you create a job from a Snapshot, you must launch the job in the same region. This is useful for zonal outages. If a zone goes down, you can relaunch the job from a Snapshot in a different zone in the same region. This protects your workloads against zonal outages. However, Dataflow Snapshots cannot help you migrate in a different region in the event of a regional outage. The best action to take in that event is to wait for the region to come back online or to relaunch the job in a new region without a Snapshot. If you've taken a Snapshot, though, you can ensure that your data is not lost.

### Video - [High Availability](https://www.cloudskillsboost.google/course_templates/264/video/521595)

* [YouTube: High Availability](https://www.youtube.com/watch?v=bPt-DWtFzr4)

person: In this final section we will look at different high availability configurations for your dataflow pipelines. This section only applies to streaming pipelines. High availability is a hard requirement for some use cases. If you are processing financial transactions or identifying cybersecurity threats in an event stream, there are very real external risks if your pipeline goes down. your specific needs should be assessed when considering how to implement a highly available architecture of your own. When considering high availability, you need to take three factors into consideration. First downtime. How much downtime can your operation tolerate without breaking business continuity? Many organizations define recovery time objectives, or RTO to articulate this upper link. Second data loss. How much of your data can your application afford to lose in the event of an outage? ET managers will often use the term recovery point objectives or RPOs to describe this requirement. Third, cost. Running in a highly available configuration doesn't come for free, and it is important to consider how much your business is willing to pay to ensure that their data pipelines reach sufficient reliability standards. Now that we've discussed the consideration, let's look at a couple of possible configurations on dataflow. You can choose to make redundant sources that are available in multiple regions. In this example architecture, you can maintain two independent subscriptions in two different regions that are reading from the same topic. If a regional outage occurs, you can start a replacement pipeline in the second region and have the pipeline consume data from the backup subscription. If a region goes offline, you can start a new pipeline in a different region immediately to continue processing. Your application might drop data in the process as the intermediate data in the original pipeline will be dropped. However, you can replay the backup subscription to an appropriate time to keep data loss at a minimum if you're coordinating pub/sub snapshots between the two subscriptions. Using a multi-regional sync can also ensure that your new pipeline will be able to write to the sync without degrading latency. Downstream applications must know how to switch to the running pipelines output. Since only the source data is duplicated, it is more cost efficient than other alternative high availability configurations. If your application cannot tolerate data loss, run duplicate web pipelines in parallel in two different regions. Your pipelines will consume the same data from two different subscriptions, process data using workers in different regions, and write to multi-regional sinks in each location. This architectures provides geographical redundancy and fault tolerance. We have already discussed how pub/sub's global architecture makes setting up redundant subscriptions in different regions really easy. Dataflow workers can only work in one zone per job. By running parallel pipelines in separate Google Cloud regions, you can insulate your jobs from failures that affect a single region. Using multi-regional storage locations for your data syncs is not a requirement, but provides you one extra degree of fault tolerance. Applications that feed from the process data sets must have a way to switch to the running pipelines output. In the example above, we are running pipelines in two different continents, America and Europe. However, this would be the approach if you were running on the same continent, but in different regions. For example, running redundant pipelines in US Central one and US East one. This architecture basically offers you zero downtime, even where we'll have multiple instances of your pipeline running. Similarly, as your data is being processed in multiple regions, data loss is extremely unlikely. However, since you are duplicating resources across the entire stack, this approach is the most expensive high availability configuration. This is the end of this module. You should now be able to take snapshots of your data fill pipeline for disaster recovery requirements.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/264/quizzes/521596)

#### Quiz 1.

> [!important]
> **You want to launch a streaming Dataflow job in europe-west4 and want to protect your pipeline from zonal stockouts. Which launch command will achieve these requirements?**
>
> * [ ] $ python3 -m apache_beam.examples.wordcount \  --input gs://dataflow-samples/shakespeare/kinglear.txt \  --output gs://$BUCKET/results/outputs --runner DataflowRunner  \  --project $PROJECT  --temp_location gs://$BUCKET/tmp/  \  --region europe-west4
> * [ ] $ python3 -m apache_beam.examples.wordcount \  --input gs://dataflow-samples/shakespeare/kinglear.txt \  --output gs://$BUCKET/results/outputs --runner DataflowRunner  \  --project $PROJECT  --temp_location gs://$BUCKET/tmp/  \  --region europe-west4  --worker_zone europe-west4-b
> * [ ] $ python3 -m apache_beam.examples.wordcount \  --input gs://dataflow-samples/shakespeare/kinglear.txt \  --output gs://$BUCKET/results/outputs --runner DataflowRunner  \  --project $PROJECT  --temp_location gs://$BUCKET/tmp/  \

#### Quiz 2.

> [!important]
> **How long is the retention for Dataflow Snapshots?**
>
> * [ ] Three days
> * [ ] Indefinitely
> * [ ] Seven days

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/264/documents/521597)

## Flex Templates

This module covers Flex Templates, a feature that helps data engineering teams standardize and reuse Dataflow pipeline code. Many operational challenges can be solved with Flex Templates.

### Video - [Classic templates](https://www.cloudskillsboost.google/course_templates/264/video/521598)

* [YouTube: Classic templates](https://www.youtube.com/watch?v=aCH_aKTeZds)

Hi, my name is Prathap. I am a Cloud Data Engineer at Google. Welcome to Dataflow Flex Templates module. In this module, we will cover: Review of Dataflow templates, What are flex templates? How to create a flex template, and Google-provided templates. Let’s start with classic templates. You might already know about Dataflow templates and why are they required, as covered in our earlier course “Serverless Data Processing with Dataflow.” Let us do a quick recap: In normal cases, when a developer executes a Beam pipeline, the SDK stages all the pipeline dependencies on Google Cloud Storage and calls the Dataflow Jobs API to create a job by passing a job request object. To launch a pipeline, this workflow requires runtime dependencies to be installed, which can be challenging for non-technical users. This dependency also limits using cloud-native services like Cloud Scheduler for scheduling. Dataflow templates enable the separation of the development activities and the execution activities. The initial version of templates are now referred to as classic templates. With classic templates, the developer stages the pipeline as a template file on Google Cloud Storage. Now users can launch the pipeline, referring to the template file without the need for any runtime dependencies. This new approach facilitates more opportunities for automation and reusability of pipelines across the teams.

### Video - [Flex templates](https://www.cloudskillsboost.google/course_templates/264/video/521599)

* [YouTube: Flex templates](https://www.youtube.com/watch?v=DInHnQ17-x4)

SPEAKER: Classic templates have a couple of challenges-- ValueProvider support for beam I/O transforms and lack of support for dynamic DAG. Let us check each of them in detail. Pipeline options or compile time parameters. To make these options available to users, while launching templates, pipeline options have to be converted to runtime parameters. The ValueProvider interface allows the templates to accept runtime parameter values provided by end users. ValueProvider interface support has been added for several Google and non-Google I/Os, but there are few open source I/Os that lack ValueProvider support, which means that pipelines developed using these I/Os cannot be converted to classic templates. In classic templates, the pipeline graph gets built when the developer converts the pipeline into a template. Due to this, the shape of the graph cannot be changed based on user-provided options. An example would be if you developed a pipeline that consumes events from Pub/Sub and would like to load these events either to BigQuery or cloud storage, depending on the user's choice at runtime. With classic templates, since the dynamic selection of sync node at template launch time is not possible, you need to create two separate variations of this template, one for writing to BigQuery and the other for cloud storage. To address these challenges, Google has built the next generation of templates, referred to as flex templates. With flex templates, the pipeline developer packages the pipeline artifacts into docker image and stages the image on Google Container Registry. In addition, the developer creates a metadata specification file on cloud storage. Users can launch a template referring to a metadata spec file stored on cloud storage by passing appropriate parameter values. Behind the scenes, the template launcher service reads the metadata spec file, downloads the docker image, and invokes the pipeline using user-supplied values. It's important to note that, with flex templates, the job graph is generated when the end user launches the templates, whereas with classic templates, the graph is generated when the templates are created. This distinction makes flex templates more flexible than classic templates.

### Video - [Using flex templates](https://www.cloudskillsboost.google/course_templates/264/video/521600)

* [YouTube: Using flex templates](https://www.youtube.com/watch?v=Qko94OJcEgU)

SPEAKER: Now, let us check out how to create a flex template and run the Dataflow job from a flex template. Turning a Dataflow pipeline into a flex template is easy and straightforward. First, create a metadata file indicating the pipeline parameters. Next, run the flex-template build gcloud command. Let us cover these steps in detail. Create a metadata.json file with details like the pipeline name, description, and parameter details. You can also specify regexes for any parameters that need to be validated against user-supplied values before the template is launched. This fail-fast approach avoids the overhead of launching a job that may potentially fail due to incorrect parameter values. The gcloud dataflow flex-template build command can be used to build a flex template. In this command, you will provide the path to store the Docker image and template specification file. Refer to the metadata file created in previous step. Specify the artifact details-- such as .jar file, in this example-- and entry point for execution. Executing this command will package the pipeline artifacts into a Docker image, pushes the image into Google Container Registry, creates a template spec file on Cloud Storage containing the image URL and parameter details. For detailed instructions, check out our public documentation. Flex templates can be launched through several channels, like Google Cloud console, gcloud, REST API, or Cloud Scheduler. To launch a flex template from the Google Cloud console, select the Custom Template from Dataflow Template dropdown. Enter the Cloud Storage path referring to a template specification file. Provide the pipeline options, and run the job. To launch a flex template from gcloud, use the flex-template run command by referring to the template specification file and passing the template parameters as required by the pipeline. Alternatively, you can also run the template with a REST API request. Remember that classic templates and flex templates use different endpoints. Flex templates can also be scheduled using the native Cloud Scheduler. In this example, the template has been scheduled to run every 30 minutes. This table provides the detailed comparison of classic and flex templates. Google recommends using flex templates for any Dataflow pipeline that you would like to reuse.

### Video - [Google provided templates](https://www.cloudskillsboost.google/course_templates/264/video/521601)

* [YouTube: Google provided templates](https://www.youtube.com/watch?v=0aathYy7cmo)

SPEAKER: We have reached the final section of our module, Google-provided Templates. Google readily provides a large collection of templates to Dataflow users. The good news is that you can use them without writing a single line of code. These templates can be used for transferring data between different systems. You can also add simple transformations through a JavaScript user-defined function. Google has also open sourced all the templates, with the full code available on GitHub. This repository also serves as a starting point for Dataflow developers to learn best practices for writing and testing beam pipelines. With active community support, we encourage you to contribute either new templates or enhancements to existing templates. Similar to user-developed templates, Google-provided templates can be launched through Console, G Cloud, REST API, or Scheduler. You can create a job using one of the templates by clicking on Create Job from Template option on the Dataflow job screen. Choose the appropriate template, provide the pipeline options, and run the job. You can also find usage instructions in the information pane on the right-hand side by clicking on Open Tutorial. When you select a template, you can also view the graph on the right-hand side for all classic templates. For flex templates, the graph is not rendered, as the final graph might change based on the user options. Based on whether graph is displayed or not, you can also determine if the template is a classic template or a flex template. Google-provided templates are classified into streaming, batch, and utility templates. The list provided here represents a sample of available templates. Some of the popular streaming templates are Pub/Sub to BigQuery and Data Masking Using DLP, whereas in batch, the templates like BigQuery export to Parquet and Spanner export to Cloud Storage are widely used. Utility templates like Streaming Data Generator are helpful to generate synthetic records used during proof of concept or for evaluating performance benchmarks. You've reached the end of Flex Templates Module. Now you are ready to convert any Dataflow pipeline into a flex template and share with others.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/264/quizzes/521602)

#### Quiz 1.

> [!important]
> **How are Flex Templates packaged?**
>
> * [ ] Jar/Pex
> * [ ] Tar file
> * [ ] Docker image
> * [ ] ProtoBuf binary

#### Quiz 2.

> [!important]
> **Into which of the following categories are the Google-provided templates classified?**
>
> * [ ] Streaming and utility only
> * [ ] Batch and streaming only
> * [ ] Batch, streaming, and utility
> * [ ] Batch and utility only

#### Quiz 3.

> [!important]
> **Which of the following is a challenge associated with classic templates?**
>
> * [ ] Increased latency while launching templates.
> * [ ] Lack of support for Dynamic DAG (Directed Acyclic Graph).
> * [ ] Lack of support for runtime parameters.

### Lab - [Serverless Data Processing with Dataflow - Custom Dataflow Flex Templates (Java)](https://www.cloudskillsboost.google/course_templates/264/labs/521603)

In this lab you (i)convert a custom pipeline into a Dataflow Flex Template, ii) and run a Dataflow Flex Template.

* [ ] [Serverless Data Processing with Dataflow - Custom Dataflow Flex Templates (Java)](../labs/Serverless-Data-Processing-with-Dataflow-Custom-Dataflow-Flex-Templates-(Java).md)

### Lab - [Serverless Data Processing with Dataflow - Custom Dataflow Flex Templates (Python)](https://www.cloudskillsboost.google/course_templates/264/labs/521604)

In this lab, you (i)convert a custom pipeline into a Dataflow Flex Template, (ii)run a Dataflow Flex Template.

* [ ] [Serverless Data Processing with Dataflow - Custom Dataflow Flex Templates (Python)](../labs/Serverless-Data-Processing-with-Dataflow-Custom-Dataflow-Flex-Templates-(Python).md)

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/264/documents/521605)

## Summary

This module reviews the topics covered in the course

### Video - [Course Summary](https://www.cloudskillsboost.google/course_templates/264/video/521606)

* [YouTube: Course Summary](https://www.youtube.com/watch?v=I5KljnQo2Ok)

Mehran: Congratulations, you've made it to the end of the operations course. In the final installment of the data flow series, you're ready to build a modern data platform now. Before we do that, let's summarize the main concepts we covered in each of the modules in the operations course. We started this course with the walkthrough of the data flow monitoring experience. We learn how to use the jobs list page to filter for jobs that we want to monitor or investigate. We looked at how the job graph job info and job metrics tabs collectively provide a comprehensive summary of your Dataflow job. Lastly, we learn how we can use Dataflow integration with metrics Explorer for creating alerting policies for data flow metrics. We explored two important integrations in the data flow, operational toolkit, logging and error reporting. The logging panel helps you sift through job and work logs provides a diagnostics tab that surfaces errors, you can click through to the error reporting interface, investigate the frequency of these errors, and examine the full stack traces of these errors. We use the monitoring logging and error reporting capabilities and incorporated them into our recommended troubleshooting workflow, which leverages data flows integrated Error Reporting in jobs metrics tab. We then reviewed four common modes of failure for data flow, failure to build the pipeline, failure to start the pipeline and data flow, failure during pipeline execution, and performance issues. Performance is a key consideration for any data engineer operating a data processing system. In this module, we review how pipeline design can impact your performance that topology, coders, windows and logging that you implement can have adverse impacts on your pipeline performance if not taken into careful consideration the shape of your data, specifically if your keyspaces skewed can cause worker imbalances and cause under utilization for your pipeline. Your Dataflow pipeline will interact with sources, sinks and external systems. A well-tuned pipeline will take the limitations and constraints of these pieces into account. Lastly, shuffle and streaming engine can help offload data storage from worker attached disks onto a highly scalable back end that will deliver performance benefits to your pipeline. As your data requirements evolve, so do your data flow pipelines. A robust Dataflow architecture implements testing at various abstraction layers, starting with the do functions at the lowest level, then P transforms, then pipelines. And finally, for entire end to end systems. Dataflow's continuous integration continuous deployment model requires using the direct runner to validate your pipeline in a local environment, followed by testing it on a production runner before it is pushed to production. Beam provides helpful functions like p assert, test, pipeline and test stream to help implement this testing architecture. Dataflow offers features such as update, drain snapshots and cancel so that you can adjust the deployment of your streaming pipelines as needed. Next, we discussed how to implement reliability best practices for your Dataflow pipelines. Monitoring dashboards and alerts can help notify you when your system is encountering a bottleneck. And using dead letter queues and error logging can prevent pipelines from going down when corrupted data enters the pipeline. Protecting your pipelines from zonal and regional outages require thoughtfulness about how you specify the location of your sources, sinks, and Dataflow job, but data loss can be mitigated with pub sub and data flow snapshots. High Availability can be implemented by running redundant pipelines in different zones or regions. Our last module is covers flex templates, which makes it easy to share and standardized data flow pipelines for your organization. Templates allow you to call data flow pipelines by making an API call without the fuss of installing runtime dependencies in your development environment. Google offers a variety of templates directly in Cloud Console, which allows you to launch Dataflow job without writing a single line of code. Flex templates offers advantages over classic Dataflow templates, and are encouraged for all templating needs. To conclude, data flow offers a whole suite of features that makes it easy to manage your data processing system. This operational toolkit will help you focus your efforts on insights, not infrastructure, and ensure that you can spend your time creating value for your customers not keeping the lights on. We're excited to see what your organization builds on data flow.

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
