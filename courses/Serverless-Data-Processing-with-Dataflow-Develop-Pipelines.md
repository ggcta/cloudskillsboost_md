---
id: 229
name: 'Serverless Data Processing with Dataflow: Develop Pipelines'
type: Course
url: https://www.cloudskillsboost.google/course_templates/229
date_published: 2025-02-24
topics:
  - SQL
  - Apache Beam
---

# [Serverless Data Processing with Dataflow: Develop Pipelines](https://www.cloudskillsboost.google/course_templates/229)

**Description:**

In this second installment of the Dataflow course series, we are going to be diving deeper on developing pipelines using the Beam SDK. We start with a review of Apache Beam concepts. Next, we discuss processing streaming data using windows, watermarks and triggers. We then cover options for sources and sinks in your pipelines, schemas to express your structured data, and how to do stateful transformations using State and Timer APIs. We move onto reviewing best practices that help maximize your pipeline performance. Towards the end of the course, we introduce SQL and Dataframes to represent your business logic in Beam and how to iteratively develop pipelines using Beam notebooks.

**Objectives:**

* Review the main Apache Beam concepts covered in the Data Engineering on Google Cloud course
* Review core streaming concepts covered in DE (unbounded PCollections, windows, watermarks, and triggers)
* Select & tune the I/O of your choice for your Dataflow pipeline
* Use schemas to simplify your Beam code & improve the performance of your pipeline
* Implement best practices for Dataflow pipelines
* Develop a Beam pipeline using SQL & DataFrames

## Introduction

This module introduces the course and course outline

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/229/video/526215)

* [YouTube: Course Introduction](https://www.youtube.com/watch?v=LUockM4IAQ8)

Hi, and welcome to the second installment of the Serverless Data Processing with Dataflow series, Developing Pipelines on Dataflow. My name is Mehran Nazir, and I am a product manager with Dataflow. If you’ve been following the data engineering progression thus far, you’ve learned about different Google Cloud services you can use for your data processing needs. You might have chosen to go deeper on Dataflow, Google Cloud’s unified batch and stream processing engine that’s serverless, fast, and cost-effective. If you’ve taken the Dataflow Foundations course, the first part of this course series, you likely understand Dataflow’s IAM, quotas and security model, and also have a conceptual grasp of the Beam Portability Framework and how Dataflow separates compute and storage with Shuffle and Streaming Engine. If you remember, there are three ways to launch a Dataflow pipeline: Launching a template using the Create Job Wizard in Cloud Console. You don’t have to write code with this option—all you have to do is select your desired template from a drop-down menu, fill out a few fields, and your job can be deployed. We covered this workflow briefly in the Building Batch Pipelines course in the data engineering curriculum. 2. Authoring a pipeline using the Apache Beam SDK and launching from your development environment. This can mean writing a pipeline using the Java SDK in an interactive development environment (IDE) like IntelliJ, or using a read-eval-print-loop workflow with the Python SDK using a Jupyter notebook. We introduced the building blocks of the Apache Beam SDK in the data engineering course. 3. Writing a SQL statement and launching it in the Dataflow SQL UI. Dataflow SQL lets you launch Dataflow jobs using the familiar semantics of SQL, and includes streaming extensions that allow you to express logic for handling data in real time. In this second installment of the Dataflow course series, we are going to be diving deeper on number 2 (developing pipelines using the Beam SDK) and will dedicate one module to number 3 (Dataflow SQL). Developing your pipelines using the SDK allows you to tap into the full suite of possibilities afforded by the Beam model, and is often the choice of our most advanced users. Let’s take a look at what we’ll be covering in the Developing Pipelines with Dataflow course. We will first spend some time refreshing the concepts covered in earlier courses. More specifically, we will be reviewing the building blocks of the Beam programming model. We will then review watermarks and triggers, introduced in our Building Resilient Streaming Analytics Systems course and expanded upon in this course. Next, we will review sources and sinks, which represent the “Extract” and the “Load” of your Extract-Transform-Load (or ETL) pattern. From there, we will introduce schemas, which give developers a way to express structured data in their Beam pipelines. In the next module, we will cover state and timers. These powerful primitives unlock new use cases by giving developers fine-grained control over in-flight data. After we have laid the foundations of the Beam SDK, we will discuss best practices and review common patterns that maximize performance for your Dataflow pipelines. We will dive into two domain-specific languages, SQL and DataFrames. We’ll explore how SQL is implemented with Beam and Dataflow, then examine Beam DataFrames, an API that gives developers a similar interface to the popular pandas open-source project. Our last module will cover Beam notebooks, an interface for Python developers to onboard onto the Beam SDK and develop their pipelines iteratively in a Jupyter notebook environment. We’ll wrap up the course with a summary of all of the concepts covered.

## Beam Concepts Review

Review main concepts of Apache Beam, and how to apply them to write your own data processing pipelines.

### Video - [Beam Basics](https://www.cloudskillsboost.google/course_templates/229/video/526216)

* [YouTube: Beam Basics](https://www.youtube.com/watch?v=YVxAIeQwVgY)

Israel: Hello, my name is Israel Herraiz, and I work as a strategic cloud engineer at Google. In this video, you will learn the main concept of Apache Beam and how to apply them to write your own data processing pipelines. Let's start with the main concerns of Apache Beam. The genius of Beam is that it provides instructions that unify traditional batch programing concepts and stream processing concepts. Unifying batch programming and [indistinct] processing is a big innovation in data engineering. The four main concepts are, Beam transforms P collections, pipelines, and pipeline runners. ♠A pipeline identifies the data to be processed and the actions to be taken on the data. The data is held on a distributed data instruction called a P collection. A P collection is immutable. Any change that happens in a pipeline receives one P collection as input and creates a new P collection as output. It doesn't change the incoming P collection. The actions are contained in an instruction called a P transform. A P transform handles input, transformation and output of the data. The data in a P collection is passed along the graph from one P transform to another. Pipeline runners are analagous to container hosts, such as Kubernetes Engine. The integral pipeline can be run on a local computer, in a virtual machine, in a data center or in a service in the cloud, such as Dataflow. The only differences are scale and access to platform specific services. For instance, Google Cloud Storage. Imutable data is one of the key differences between batch programing and testing processing. The assumption in the von Neumann architecture was that data would be operated on and change in place. This was very memory efficient, and this made sense when memory was expensive and scarce. So making a copy of data was expensive. Nowadays, in distributed systems, imitable data where each form results in a new copy means that there is no need to coordinate access, control or sharing of the original ingested data. So it enables, or at least it simplifies distributed processing.

### Video - [Utility Transforms](https://www.cloudskillsboost.google/course_templates/229/video/526217)

* [YouTube: Utility Transforms](https://www.youtube.com/watch?v=4jESr1MDAMU)

Apache Beam comes with a set of transforms that you can use as the building blocks of your pipeline. Let's learn about those transforms. By combining these blocks, you can build a complex process in logic that is applied at scale by Dataflow. ParDo lets you apply a function to each one of the elements of a P collection. GroupByKey and Combine are similar. With GroupByKey, you put all the elements with the same key together in the same worker. If your group is very large or the data is very skewed, you have a so-called hotkey and you're going to apply a commutative and associative operation, you can use Combine instead. Combine will make the transformation in a hierarchy of several steps. For large groups, this will have much better performance than GroupByKey. GroupByKey let you join two P collections by a common key. You can create a left or right, outer join, inner join and so on using GroupByKey. Flatten also receives two or more input P collections and fuses them together. But please do not confuse flattened with joins or with GroupByKey. If two P collections contain exactly the same type, they can be fused together in just one P collection using the Flatten transform. However, with joins with GroupByKey, you have two P collections, but typically with different value types that share a common key. Partition is in a way the opposite of Flatten. It divides your P collection into several output P collections by applying a function that assigns a group to ID each element in the input P collection.

### Video - [DoFn Lifecycle](https://www.cloudskillsboost.google/course_templates/229/video/526218)

* [YouTube: DoFn Lifecycle](https://www.youtube.com/watch?v=juDY2t6cKRA)

person: One of the main features of Butterbean is the richness of possibilities that you can implement in a Pardoo and they do function. Pardoo seems like a simple map or a filter, but it is actually a very powerful and versatile transform. We will not always need all this power and versatility. Bin offers and convenience versions of Pardieu! Transforms for these situations. If you need to feel better or just map or flat map the elements of a collection or add keys or extract keys or values, you can use this higher level, more convenient transforms. Just don't forget that the functions offered you very powerful possibilities. Let's see those more detail elements in a collection are processing bundles. The division of the collection in the bundle is arbitrary and selected by the runner. This allows the runner to choose an appropriate middle ground between persistent results after every element and having to retract everything. If there is a failure, for example, a streamlined runner may prefer to process and commit small bundles, and the match runner may prefer the process. Larger bundles when processing and values. A single bundle may contain several different keys, and the function has several methods that can be overdriven to control how your code interacts with each day to bundle the main method, this process where each one of the elements is transformed. But there are other methods, other call at different moments during the life cycle of the function. These methods enable you to control how the data bundles are processed in the function in combination with side inputs and outputs. This opens a myriad of possibilities for writing your functions. Let's see how these methods work. When a worker starts, it creates an instance of the function right after creating that Eastnor instance, it calls the setup method. This method is called once per worker. This is a good place to start. Objects such as data connections, network connections or any other kind of helper process that will be used with all the data Rundle's. Every time the function receives a new data bundle, the runner calls the start bundle method of the function. This is a good place to start tracking your data bundle if you need to. For instance, for instance, Marable's or matrix purposes. After I start the bundle for every element, the runner will call the process method of the function. This is where the transition takes place. For that transform the process method may redistribute or receive side inputs from the process method. You can also update the state and this will be shown in the state and damaged sections later in this course. If you define Demerse, this may be called more than once per bundle, depending on the value of the timer. They stayed on timer sections, covered this in more detail to once they do function, transforms the last element of the bundle. The runner calls the method finish one, though this method is a good place to do match calls for. For instance, if you're advocating an external system, if the function is, I think finally when all the data bundles are processed and the worker is not needed anymore, the runner calls the teardown method. If you started any connection in your setup method, this is the method where you should close those connections. Beware when we did in a standard estate, in your function as a generic rule, always mutator state using state variables rather than class members, the runner may recycle you the function or process. The same bundle in different workers for redundancy do not moutet external state from your process method ensure that any state variable is clear in the bundle method. Otherwise they could contain state for the previous bundle. And remember, a bundle may contain several keys. So Estoril State maps based on that key.

### Lab - [Serverless Data Processing with Dataflow - Writing an ETL pipeline using Apache Beam and Dataflow (Java)](https://www.cloudskillsboost.google/course_templates/229/labs/526219)

In this lab, you a) build a batch ETL pipeline in Apache Beam, which takes raw data from Google Cloud Storage and writes it to BigQuery b) run the Apache Beam pipeline on Dataflow and c) parameterize the execution of the pipeline.

* [ ] [Serverless Data Processing with Dataflow - Writing an ETL pipeline using Apache Beam and Dataflow (Java)](../labs/Serverless-Data-Processing-with-Dataflow-Writing-an-ETL-pipeline-using-Apache-Beam-and-Dataflow-(Java).md)

### Lab - [Serverless Data Processing with Dataflow - Writing an ETL Pipeline using Apache Beam and Dataflow (Python)](https://www.cloudskillsboost.google/course_templates/229/labs/526220)

In this lab, you a) build a batch ETL pipeline in Apache Beam, which takes raw data from Google Cloud Storage and writes it to BigQuery b) run the Apache Beam pipeline on Dataflow and c) parameterize the execution of the pipeline.

* [ ] [Serverless Data Processing with Dataflow - Writing an ETL Pipeline using Apache Beam and Dataflow (Python)](../labs/Serverless-Data-Processing-with-Dataflow-Writing-an-ETL-Pipeline-using-Apache-Beam-and-Dataflow-(Python).md)

### Quiz - [Quiz 1 - Beam Concepts Review](https://www.cloudskillsboost.google/course_templates/229/quizzes/526221)

#### Quiz 1.

> [!important]
> **What happens when a PTransform receives a PCollection?**
>
> * [ ] It creates a new PCollection as output, and it does not change the incoming PCollection.
> * [ ] You may select whether the PTransform will modify the PCollection or not.
> * [ ] It modifies the PCollection to apply the required transformations.

#### Quiz 2.

> [!important]
> **How many times will the process/process Element method of a DoFn be called?**
>
> * [ ] As many times as there are data bundles in the PCollection.
> * [ ] As many times as there are elements in the PCollection.
> * [ ] This is a runner-specific value. It depends on the runner.

#### Quiz 3.

> [!important]
> **What is CoGroupByKey used for?**
>
> * [ ] To join data in different PCollections that share a common key.
> * [ ] To group by a key when there are more than two key groups (with two key groups, you use GroupByKey).
> * [ ] To join data in different PCollections that share a common key and a common value type.

### Document - [Module Resources](https://www.cloudskillsboost.google/course_templates/229/documents/526222)

## Windows, Watermarks, and Triggers

In this module, you will learn about how to process data in streaming with Dataflow. For that, there are three main concepts that you need to learn: how to group data in windows, the importance of watermark to know when the window is ready to produce results, and how you can control when and how many times the window will emit output.

### Video - [Windows](https://www.cloudskillsboost.google/course_templates/229/video/526223)

* [YouTube: Windows](https://www.youtube.com/watch?v=BFpw6jqHq9A)

person: Hi, it's me again. Israeli strategic engineer at Google, in this video, you would learn about how to process data in streaming with data flow. For that, there are three main concepts that you need to learn how to group data and windows, the importance of watermarks to know when the window is ready to produce results and how you can control when and how many times the window will emit output. Let's start talking about the windows. It is likely that your first experience with data processing pipelines is processing data, much, much pipelines are often run on schedule. For instance, why are they so? They produce fresh results with frequency running batch pipelines with a certain frequency is also a way to Chank data. So when we have large amounts of data, we can divide the processing and handle all the data by doing batches in situations like this. What you probably need a extremely pipeline. It is likely that your data is not discretionary. Despite being processing matches, the batches are artificially split to simplify the processing of data. If your data is not distortionary, how party will lets you handle it as a stream of continuous data. However, dealing with a string is not only a matter of continuity and making a split to process data. There are other inherent problems to processing data. One of the main problems you have to deal with when processing processing data is the lack of order. Imagine a situation where you are processing events coming from the mobile application. One of your routers here, Shaunessy Green Square. I started using the application athon at 8:00 in the morning. You receive some messages in your pipeline, but then another user shown here at a yellow hexagon did the same. But this user was driving the subway in a tunnel with no phone coverage when the user returned to the surface and they phone let you get the message with Sandile. But the wait may be worse. Yet another user, the second blue here, was using your fantastic application while flagging on a very long transcontinental flight using their mobile phone in airplane mode. This user enabled the phone signal when they arrive at the destination and suddenly you start getting more messages that were produced at 8:00 in the morning, but that you are only seen hours later. How can you deal with out of the data and how can you make a split to process data? The answer to both is windows. But these windows are not just simple groups or batches of data. Let's see where. So a window is just a way to divide it in groups in order to do it, and that's what happens with the data when the wind divides data into time based, finite chunks. Windows are required when doing aggregations of about unbounded data is being primitives such as a group bickie or combinat. However, you can also do aggregations within a state and time without having to use a window instrument pipelines. There are two dimensions of time processing time and event time in processing time. Data flow assigns the current timestamp to every new message in event time. We use instead the tiny stamp of the messages, as it was said in the original source when the message was produced. If you get messages by processing time, this is the same as micro matching messages that were produced around the same time. If they arrive out of order will be assigned to different batches. Processing done is fine, depending on the kind of calculations you want to perform. But when time enables you to apply a more complex aggregation logic to the data in Aventine, messages are grouped together depending on the systems generated at the source, not depending on the moment of their arrival. For instance, one message may be late and arrive very closely to another on time message. These two messages belong to different Windows Barrat arriving at approximately the same time, Dataflow reads the messages. Direness Times determines that one of the messages was actually late and assigns it back to the proper window, assuming the windows is still open or waiting for later. By doing this, we can record the order and groups of data as they were producing the source, even if they arrive out of order to flow. This is a very powerful feature of a streaming pipeline. And here in lies the possibilities of doing complex and sophisticated calculations in streaming pipelines, even in the case of out of order delivery. Butterbean includes three different types of windows that are available by default, fix is Liveing and sessions. We can also create custom window types. Fix windows are those that are divided into tiny slices. For example, hourly, daily, monthly, fixed time windows consists of consistent, non overlapping, overlapping intervals, sliding down windows, also representing intervals in the data stream. However, sliding down windows may overlap. For example, each window we make up captured 60 seconds worth of data, but a new window will start every 30 seconds. The frequency with which a sliding windows begin is called a period. A typical application of a sliding windows will be to calculate a moving average session based windows capture bars of user activity. Session windows are defined by a minimum gap duration, and the timing is triggered by another element. Such and windows are data dependent windows that are not known ahead of time. You need to look at the data to figure that out. Examples are intercessions you should never to and website, etc..

### Video - [Watermarks](https://www.cloudskillsboost.google/course_templates/229/video/526224)

* [YouTube: Watermarks](https://www.youtube.com/watch?v=bYNTyTPjOk4)

person: With windows, you decide where you put the message, but you now need to make another additional decision. When is the window going to meet the results? At the first glance, you may decide just to meet the results when the window closes. This is a very intuitive is you have a fixed window, but in other situations, like a session windows, it might not be so obvious. In addition to this, you will also receive late data. So you need to decide how to trigger output in the case of laded. But how would they define if they are windowing by Aventine, your messages will be within the boundaries of the window. How do you decide if a message is laid or that you have waited long enough for LAYTH data? This is where the concept of a watermark becomes useful. Let's focus on how windows work when there is no latency and no later data, in an ideal world, if there were no latency and everything was instantaneous, then these fixed windows would just flash at the close of the window at the very microsecond that the time it to begins, a one minute window terminates and flashes all the data. But this is only if there is no latency. But in the real world, in assuming pipelines, the order of our data will be altor. Even if you receive data in perfect order when it is processed in the pipeline, in a distributed system, different messages will take different processing times and that order will be lost. How can you decide that the window can be closed if the data is out of order? How can you be sure that no further and order messages will be received in estimated pipelines that are two dimensions of time? The relationship between the two defines what it is called the watermark. The watermark is the relationship between the processing timestamp and the event. DINNERSTEIN The processing timestamp is the moment the message arrives at the pipeline. Ideally, the both should be the same with no delays. However, this rarely happens. There are always delays, latencies and so on. Any message that arrives before the watermark is set to be Everleigh. This happens too, if it arrives right after the watermark is said to be on time and if it arrives later, then it is late to date. So the watermark is what defines whether a message to circulate the watermark can be calculated because it depends on messages we have not yet seen. So data flow estimates the watermark as the oldest is timestamp waiting to be processed. This estimation is continuously updated with every new message that is received. Now, why do you need to keep two dimensions of paint and a watermark definition? Let's see how watermarks help decide when a window is complete and you can proceed with your calculations. In a real war setting, data will always arrive with Stalactite, this lag time is the difference between when the data was suspected and when the data is actually arriving these days. On Friday, the expectation is what we call the watermark. They will keep track of the lack of every message and will try to predict the value of the watermark that they lack in the future. When the Dennis stamp of the last message is after or add the value of the watermark, then it means that the window can be considered complete. Any message received after this moment will be considered late. In this example, data one is late because it is arriving much later than when it was expected that it's arriving much later than the watermark. So data is only late when it compared to the watermark. It doesn't make sense to talk about later data unless we have a watermark. Data flow will wait until the watermark is trespass to close the window while it actually waits for some additional time as a form of buffer. But after that, the windows flash and the result is omitted. Any message coming after this moment will be considered late. You will need to make a decision about what to do with the data. The default behavior is to drop late date, but as you will see in the trigger section, you can choose to wait for data and limit results again if there are any later messages. When you run a stream pipeline in the floor, the jump into the flow contains some details about the watermark values. The data freshness metric is actually related to the watermark of your input data. When you are processing fresh data, the data value decreases when the wind is close and those messages will are considered now fully. Process data has not waited until it has been processed by the law. In these situations, the watermark will be close to real time. They the freshness is the difference between real time and the stamp of the oldest message waiting to be processed, the watermark is actually a tiny stamp of the message that has not been processed yet. So they the freshness is a measurement of how far the oldest messages is far from the current moment when you see a monotonically increase in value. It means that data has the weight of the input for more time waiting to start to be processed. There could be two reasons for the additional weight. It could be because the pipeline is busy processing messages, or it could be because the input has increased very quickly and data is accumulating at the input. Or it could be because of how can we distinguish between both situation for that system? Latency is a useful metric system. Latency measures the time it takes to fully process a message. This includes any weighting down in the input source. If for some reason the pipeline needs more time to process a message, then system latency will increase. For instance, because the pipeline is missing, processing a complex message. When seasonless latency keeps increasing and data freshness keeps increasing to it means that the pipeline cannot process more messages until it does not finish processing the current messages. You are not necessarily receiving a lot of more messages. The pipeline is just taking longer to process the current messages. But if Sistan latency remains constant or reduces and does not monotonically increase and data freshness value is monotonically increasing, that could be because there are many more messages at the input. For instance, we have received like a peek at the input the pipeline gives processing data at the same pace. So latency doesn't increase system latency. But unless data flow adds more workers to the pipeline, it will not be able to catch up with the input peak and the freshness increases. If you are running without the scaling in this situation, data flow will spin up more workers to process this additional data for the. So although we don't get actual watermark values from the floor with the freshness and Lattanzi metrics, we can written about the situation of our pipeline and diagnose if we are getting more input or if the pipeline is busy doing more calculations, dataflow itself uses these metrics to decide when to upscale or downscale to our data the amount of resource use to the actual demand of data processing. The ideal situation for a streaming pipeline is to have both a stable data freshness and then latency values if they have monotonically increases and latency doesn't increase that evidence that you're receiving more input data data flow will spin up new workers because of the size of the backlog. To be processed is increasing. If latency increases and data freshness is stable, then messages are taking more time in the pipeline to be processed. CPU usage will probably increase, so data flow will spin up new workers. But you may also see other situations that increase latency, but no super cheap usage. For instance, if you are accessing an external service or API and the service may be overloaded and taking a long time to respond to ship usage will not be high. But the latency will increase in that situation. Outas killing would not create more workers. They would be useless anyways to accelerate the pipeline in that situation. And if both metrics monotonically increase, then your backlog is increasing and your pipeline is also taking more time to process A16 messages. All those should create more workers to adapt to the increasing demand.

### Video - [Triggers](https://www.cloudskillsboost.google/course_templates/229/video/526225)

* [YouTube: Triggers](https://www.youtube.com/watch?v=D4eU4HdIzgs)

>> You have seen how the water mark is useful to have an idea of data completeness and how to use metrics to see how the water mark is evolving, the default behavior is to trigger the results when the water mark is sparse. But that can be often a very long time. Do you have to wait that long before you can see the results coming out of your windows? No, you don't. Three years are useful to define in precise detail when we want to see the results of our window. Let's see how things work. By using three years, you can control the Lattanzi to produce a result or you can ensure the data completeness before you emit a result or a combination of both triggers can be based on Aventine, for instance, immediate results after 30 seconds as measured by the messages, timestamps or on processing time. For instance, Emet results every 30 seconds as measured by the workers clock, regardless of their messages thunderstorms. And they can also be based on data, for instance, and meet some results after seeing 25 messages. Or you can use any combination of the above three years with a composite trigger, we can employ and implement a very complex logic for deciding when and how many times to trigger the results of our windows. The default behavior is to trigger the watermark. So if you don't specify a trigger, you are actually using the trigger after watermark after watermark is an event time trigger. We could also apply any other triggers event time. The message is that are used to measure time with these triggers, but we could also add custom triggers if the trigger is based on processing time. The actual clock real time is used to decide when to meet the results. For instance, you could decide to meet exactly every 30 seconds, regardless of the timestamps of the messages that have arrived. The window after count is an example of a data driven trigger rather than immediate results based on time here with trigger based on the amount of data that has arrived within the window. The combination of several types of triggers, openside worth of possibilities with a streaming pipeline, we may need some results using after processing time and then again at the watermark when data's complete and then for the next five messages that arrive late after the watermark. In summary, you can integrate to make sure that Doumitt results early, which is to say with minimal latency, or you can use them to make sure that you process data and that your results include all the relevant messages, even if those messages are delayed. Or you can combine the two conditions. For instance, make sure that the immediate results early and late repeat the calculation and new results when data is complete. But when you hand me the results several times, how will that aperture have been rapid? The calculation, you can actually control that. Let's talk about accumulation modes. When you trigger the the window several times, you have to decide on the desired accumulation mode. There are two accumulation modes in a batch of accumulate and this car with accumulate every time you trigger it again in the same window. The calculation is just repeated with all the messages that have been included in the window so far with this car. Once some messages have been used for a calculation. Those messages are discarded. If new messages arrive later and there is a new trigger, the result will only include the new messages and those messages will be discarded again. Sure, there will be any additional triggers later. Let's see how these models work with an example. This example is using fixed windows of 10 minutes, but you don't want to wait so long until you see results. So the trigger is set to Aventine every couple of minutes in the first trigger, the window has only seen two messages and we immediately at the containing just two messages by the ten. Then by the time the next trigger fires up, the window has received four more messages. Now the trigger amidst the list, again containing the previous messages and the new messages. The third trigger, again, includes all the previous messages and the new messages. If your window is very wide, using accumulate as the accumulation mode may consume considerable resources as the accumulated output has to be stored when the window is still open, the windows and the message here are the same as in the previous slide. But now we have said the accumulation mode to discard every new trigger will only use the new messages that the window has received so far. And once the result is emitted, it will discard those messages. If the calculation you need to make with the windows is associative and commutative, you can safely update that calculation. Using this commode without any loss of accuracy in the output storage where you store the partial results, you should be able to aggregate the partial results to get the actual calculation value. The main advantage of using the discard mode is that the performance will not suffer even if you use a very wide window, because no state, no accumulation is stored for very long, only until the next trigger is released. Let's see how the specified triggers in a butterbean, these examples are in Python, in the example at the top, the pipeline triggers more than once per window. There will be a trigger 30 seconds after opening the window and then again once the watermark is reached after that, for every late message within the first two days, there will be an additional three or so. Every window will produce more than one output. Bear that in mind when designing Utøya index sample at the bottom, the window is not trigger the watermark, but whenever the window sees a hundred messages or 60 seconds have passed, whichever happens first. The trigger only computes that dealt us. Once it really is calculated, the previous messages are discarded because the window allows for two days to wait for little data. The trigger will produce output if we have left messages within two days after the watermark. So even if we are not triggering the watermark, the watermark is still important. You may have heard, by the way, that the Python SDK for Baturin doesn't support setting a value for allowed liveness. That was the case sometime ago, but they're now allowed. The place is fully supported in Python, in dataflow and other runit. These are the same examples, but in Java here, you need to specify the type of the collection we are assuming this extreme battery could be actually any other type, even a custom class. The API is different than in the case of Python, more adapted to the customs of Java. But the concepts are exactly the same as those shown in the producers like you have learned how to process detainee stimming with a Butterbean instrument is not only about the continuity of data, it has other important features as well. The most prominent lack of order when receiving data windows can help with that window by event time lets you recover the natural order of the data. But you can also use processing time, which is more like MacRobertson to omit results in a window. Watermarks are important to know if our data is complete or whether we should still wait for more data. If data arrives after the watermark, that data will be considered late. That's not a big deal with triggers. We can decide how to deal with data. Remember that Couston triggers. Let you decide when to meet results early, when the data is complete and so on, and the window may have several triggers.

### Lab - [Serverless Data Processing with Dataflow - Batch Analytics Pipelines with Dataflow (Java)](https://www.cloudskillsboost.google/course_templates/229/labs/526226)

In this lab you write a pipeline that aggregates site traffic by user and write a pipeline that aggregates site traffic by minute.

* [ ] [Serverless Data Processing with Dataflow - Batch Analytics Pipelines with Dataflow (Java)](../labs/Serverless-Data-Processing-with-Dataflow-Batch-Analytics-Pipelines-with-Dataflow-(Java).md)

### Lab - [Serverless Data Processing with Dataflow - Batch Analytics Pipelines with Dataflow (Python)](https://www.cloudskillsboost.google/course_templates/229/labs/526227)

In this lab you write a pipeline that aggregates site traffic by user and write a pipeline that aggregates site traffic by minute.

* [ ] [Serverless Data Processing with Dataflow - Batch Analytics Pipelines with Dataflow (Python)](../labs/Serverless-Data-Processing-with-Dataflow-Batch-Analytics-Pipelines-with-Dataflow-(Python).md)

### Lab - [Serverless Data Processing with Dataflow - Using Dataflow for Streaming Analytics (Java)](https://www.cloudskillsboost.google/course_templates/229/labs/526228)

In this lab you read data from a streaming source, perform the same aggregations you performed before, and write out results in a streaming fashion to BigQuery. You will also experiment with the difference in processing time and event time with lagging (late) data.

* [ ] [Serverless Data Processing with Dataflow - Using Dataflow for Streaming Analytics (Java)](../labs/Serverless-Data-Processing-with-Dataflow-Using-Dataflow-for-Streaming-Analytics-(Java).md)

### Lab - [Serverless Data Processing with Dataflow - Using Dataflow for Streaming Analytics (Python)](https://www.cloudskillsboost.google/course_templates/229/labs/526229)

In this lab you read data from a streaming source, perform the same aggregations you performed before, and write out results in a streaming fashion to BigQuery. You will also experiment with the difference in processing time and event time with lagging (late) data.

* [ ] [Serverless Data Processing with Dataflow - Using Dataflow for Streaming Analytics (Python)](../labs/Serverless-Data-Processing-with-Dataflow-Using-Dataflow-for-Streaming-Analytics-(Python).md)

### Quiz - [Quiz 2 - Windows, Watermarks Triggers](https://www.cloudskillsboost.google/course_templates/229/quizzes/526230)

#### Quiz 1.

> [!important]
> **What are the types of windows that you can use with Beam?**
>
> * [ ] Open and closed windows.
> * [ ] It depends on the runner, because each runner has different types of windows.
> * [ ] Fixed, sliding, and session windows.

#### Quiz 2.

> [!important]
> **How many triggers can a window have?**
>
> * [ ] As many as we set.
> * [ ] One or none.
> * [ ] Exactly one.

#### Quiz 3.

> [!important]
> **How does Apache Beam decide that a message is late?**
>
> * [ ] This is a runner-specific value. It depends on the runner.
> * [ ] A message is late if its timestamp is after the watermark.
> * [ ] A message is late if its timestamp is before the clock of the worker where it is processed.

#### Quiz 4.

> [!important]
> **What can you do if two messages arrive at your pipeline out of order?**
>
> * [ ] You can recover the order of the messages with a window using event time.
> * [ ] You cannot do anything to recover the order of the messages.
> * [ ] You can recover the order of the messages with a window using processing time.

### Document - [Module Resources](https://www.cloudskillsboost.google/course_templates/229/documents/526231)

## Sources and Sinks

In this module, you will learn about what makes sources and sinks in Dataflow. The module will go over some examples of TextIO, FileIO, BigQueryIO, PubsubIO, KafKaIO, BigtableIO, Avro IO, and Splittable DoFn. The module will also point out some useful features associated with each I/O.

### Video - [Sources & Sinks](https://www.cloudskillsboost.google/course_templates/229/video/526232)

* [YouTube: Sources & Sinks](https://www.youtube.com/watch?v=R45FQrb2I24)

Welcome to the Sources and Sinks for Dataflow module. My name is Wei Hsia and I’m a Customer Engineer for Google Cloud. In this module, you will learn about what makes sources and sinks in Dataflow. The module will go over some examples of TextIO, FileIO, BigQueryIO, PubsubIO, KafKaIO, BigtableIO, AvroIO, and Splittable DoFn. The module will also point out some useful features associated with each I/O. In this video, we talk about sources and sinks. In a data pipeline, there’s generally an input and an output. In Beam, these are called sources and sinks. A source is when you read input data into a Beam pipeline. Sources generally appear at the beginning of a pipeline—but that doesn’t necessarily need to be the case, as you will see later in this module. A sink is where you would write output data from your Beam pipeline. A sink is a PTransform that performs a write to the specified destination. A PTransform is an operation that takes an input and provides an output. A common output for a sink is PDone, which signals that the branch of the pipe is done. A bounded source is a source that reads a finite amount of input. This is commonly associated with batch processing. A bounded source will be responsible for splitting up the work of reading an input into bundles. Bundles are groupings of elements in the pipeline for a unit of work. The bounded source will also provide estimates to the service and number of bytes to be processed. Because the input is finite, there is a known start and a known end. If the bundles can be broken down into smaller chunks, Dataflow will dynamically rebalance work to achieve better performance. An unbounded source is a source that reads from an unbounded amount of input. An unbounded source is commonly associated with streaming. Checkpoints allow for the ability to bookmark where the data has been read in the source, which means that data that has been processed in the stream doesn’t need to be re-read. Watermarks from sources can provide the point in time estimates for a piece of data. Finally, some unbounded sources, for example PubsubIO, have the ability to pass a record ID to allow deduplication of messages. Dataflow will keep track of the message IDs for 10 minutes and automatically discard the record if duplicated. Sinks are often PTranformations that write data to an end system. You can check the code out for the various sinks to see this. In general, sinks will emit a PDone value to signify the completion of the transform. There are some, such as BigtableIO, that also allow you to continue processing data after the success, as you will see later. If you have a need for continued processing, you can also write your own PTransform sink to write out the data and have an output. Apache Beam is open-source, so there are many developments contributed by the community and by Google. There are a lot of various open source I/Os, and the list continues to grow. For an updated list of various I/O connectors, refer to the official Apache Beam documentation page.

### Video - [TextIO & FileIO](https://www.cloudskillsboost.google/course_templates/229/video/526233)

* [YouTube: TextIO & FileIO](https://www.youtube.com/watch?v=MYjMPHRrzts)

TextIO and FileIO are used for sources and sinks when you need to work with text or files, respectively. Let's dive into an example in Java for TextIO. This is a simple read using TextIO. There are variant different read methods available for TextIO. Now let's take a look at a couple of Python examples. In the first example, you provide a file name as the first step and pass that into the second step, which uses the file name to read from a file. This is an example where the second step is also a source and not necessarily the first step. For the second example, you're reading from a file by passing in the file name. Use the method that best suits your needs. Here's an example of FileIO in Java using a file match pattern. The match keyword and the argument filepattern will allow you to search for a pattern as seen here. You're able to use the filename and other metadata as part of the read. In the Python example shown here, you're mapping the file to the variable x and using the object to access the contents and the metadata. File pattern matching is useful when you need to grab a range of files. Beam FileIO is also able to continuously monitor a location for a particular pattern. This Java example will monitor the location every 30 seconds for an hour to see if new files match the pattern provided. This pattern is useful for times when you have files flowing in but have a sliding window in which they will arrive. Here's another example, but rather than providing the filenames, you read off a message queue such as Pub/Sub using PubsubIO. Certain systems like Cloud Storage have the native ability to trigger a message to Pub/Sub on metadata changes. The message is then parsed to return the filename for the subsequent step that can now target a file for a read. This method lets you read a stream of files. Contextual I/O provides many mechanisms to enhance the behavior of text reading. Historically, when more complicated TextIO reads are required, you are relegated to using FileIO. Contextual I/O lets you use TextIO for those use cases. For example, you're able to return things such as ordinal position or read multi-line CSV records. This is an example of a sink written in Java using TextIO to write your output to a file. You can write to many different file systems and object stores. Here is another example of a simple write using TextIO, this time in Python. Dynamic destinations allow you to determine the sink destination at run time. You can invoke this with the writeDynamic function in Java. This example allows you to use the transaction type to determine the file name. In this Python example, you take the dynamic destinations a step further by writing to different sinks depending on the characteristics of the data. In this example, the record type determines the destination file type. Dynamic destinations are useful when you are unsure of the specific destination at run time. In these examples, you can expand the range of your destinations without altering the code.

### Video - [BigQueryIO](https://www.cloudskillsboost.google/course_templates/229/video/526234)

* [YouTube: BigQueryIO](https://www.youtube.com/watch?v=nZUf0slvqjs)

BigQueryIO is a useful connector for BigQuery, a scalable and serverless data warehouse. In this example, you read from BigQuery and use a standard SQL statement to retrieve results from BigQuery. When using standard SQL, Dataflow will submit the query to BigQuery and first retrieve metadata. BigQuery will then export the results to a temporary staging location in Cloud Storage and then Dataflow will read the contents from there, prioritizing throughput. Once the data is read from Cloud Storage, you can then map the results to be used in the data pipeline. The BigQuery Storage API is built to facilitate consumption from distributed processing frameworks, such as Beam. This Storage API allows you to achieve very high throughput when reading from BigQuery. To invoke the Storage API read method, use the DIRECT_READ method. In this example, you are using the column projection feature, withSelectedFields, to reduce the number of columns accessed by BigQuery. There is also a simple predicate filter that you can pass through for row filtering by invoking the withRowRestriction clause. You can write to BigQuery with BigQueryIO. There are multiple ways to specify schema, but using the built-in schema functionality is a quick and efficient way to specify it. You will be exploring schemas in more depth later on in this course. In this Java example, you are utilizing dynamic destinations to route your writes to various tables in BigQuery. Dynamic destinations allow you to write to multiple destinations-- in this case tables-- with varying schemas as well. BigQueryIO can write to BigQuery with streams or batches. Your streaming job can write to BigQuery in batches by windowing your data and using the FILE_LOADS method. By default, your streaming job will default to the streaming write method. Here's a Python example of a BigQuery dynamic destination write. You can define the function to return the destination you would like the data routed to.

### Video - [PubsubIO](https://www.cloudskillsboost.google/course_templates/229/video/526235)

* [YouTube: PubsubIO](https://www.youtube.com/watch?v=E2LtcrrtDpM)

Pub/Sub is Google’s highly scalable and robust messaging service. Dataflow and Pub/Sub often go hand in hand, and you can connect them using PubsubIO. In this Java example, you are reading from a Pub/Sub topic using PubsubIO. This read method automatically creates a subscription when the Dataflow job is deployed, and is destroyed upon termination of the job. If you would like to have a subscription remain upon termination of the job, create a subscription and use the fromSubscription method. Dataflow PubsubIO automatically acknowledges the messages when the data is durably persisted in Dataflow, in other words when it is materialized in the service. By default, Pub/Sub’s message timestamp is used for windowing features. It is possible to reassign that value to take advantage of other timestamps. One example would be if you had a reported timestamp from the source, versus the published timestamp in the message. You could use the reported timestamp to compute your windows and other calculations. A common scenario in any data pipeline is the ability to capture failures. You want to be able to capture these failures so that you can act upon them. One method is to use a dead letter queue. A dead letter queue is a queue where you divert messages that meet one or more criteria. This could be a failure or a piece of data that you may need a second look at. Here’s an example of reading from a Kafka system and publishing it to Pub/Sub. There are two scenarios and you can see the failed messages are diverted to a second topic.

### Video - [KafkaIO](https://www.cloudskillsboost.google/course_templates/229/video/526236)

* [YouTube: KafkaIO](https://www.youtube.com/watch?v=01s5I3LyGDg)

KafkaIO is an unbounded source and is generally used for streaming. You're able to use checkpoints with Kafka to bookmark your read so you can resume where you left off. Kafka topics are where streams of records are stored. You can choose which topics to subscribe to with KafkaIO by submitting the parameter withTopics and a list of topics of interest. Alternatively, you can use withTopic to submit a read for a single topic. KafkaIO is built in Java, but Beam has a concept of cross-language transforms. Python's KafkaIO module uses the cross-language transform to enable KafkaIO on Python. The cross-language transformation enables transforms, like sources and sinks, across multiple software development kits, or SDKs. You can see in this example that the call uses a Python function from a library that implements the external calls.

### Video - [BigtableIO](https://www.cloudskillsboost.google/course_templates/229/video/526237)

* [YouTube: BigtableIO](https://www.youtube.com/watch?v=O-kIWT4Hzro)

Bigtable is a scalable, NoSQL database service by Google. Both Bigtable and Dataflow are designed for high throughput and scalability. BigtableIO serves as the module that will communicate between Bigtable and Dataflow. You are able to include a row filter when reading from Bigtable. This allows you to filter out rows based on criteria. You can invoke this with the withRowFilter clause and passing the filter criteria. Another important aspect of NoSQL databases is the ability to use the index. withKeyRange will allow you to enable a prefix scan on the index to quickly arrive at the desired prefix. There are times where you will want to continue on a pipeline after a sink rate has completed. BigtableIO has this ability by triggering the Wait.on function in Beam by sending a signal to the Wait function. This enables you to continue with an additional transformation after a write has completed.

### Video - [AvroIO](https://www.cloudskillsboost.google/course_templates/229/video/526238)

* [YouTube: AvroIO](https://www.youtube.com/watch?v=VVPFdkcsyL4)

Avro is a file format that is self-describing. It's a very popular format for big data. AvroIO allows you to read and write to that file type. Avro files provide the schema and the data so the files can be self-describing. You can use the built-in functions from AvroIO to retrieve the schema into your Beam pipeline. You can also use wild cards to read multiple files as seen in this Python example.

### Video - [Splittable DoFn](https://www.cloudskillsboost.google/course_templates/229/video/526239)

* [YouTube: Splittable DoFn](https://www.youtube.com/watch?v=mrIXRzqDSJg)

Splittable Do Functions, or DoFns, enhance the sources and generic Do Function capabilities. Sources going forward will be able to utilize splittable Do Functions for both streaming and batch. This brings Beam's unified batch and streaming programming model closer to fruition. Splittable Do Functions are a generalization of a Do Function that gives it core capabilities of a source, splittability, and the ability to report back the metrics that you learned about earlier in this module, such as progress. Progress and other metrics then allow you to know how far along a bundle is and how far it has to go. This enables the ability for the work to be split into multiple bundles. The splittable Do Function also keeps the syntax, flexibility, modularity, and ease of coding from the Do Function syntax. When you read a file, the splittable Do Function allows you to set the restrictions, such as a sequence of blocks, on where the files are read to. Splittable Do Functions allow you to build custom sources with ease. The function is a Do Function with additional parameters, such as RestrictionTracker, as shown in this Java example. You need to define an initial restriction that will create a restriction describing a complete unit of work. This is shown with the function "def initial_restriction" in this pipeline example. One way to accelerate your development of a Dataflow pipeline is to refer to the open source code as a basis for your code. There are many examples in Python and in Java in the links provided. Thanks for listening.

### Quiz - [Quiz 3 - Sources & Sinks](https://www.cloudskillsboost.google/course_templates/229/quizzes/526240)

#### Quiz 1.

> [!important]
> **What kinds of data are a bounded and an unbounded source respectively associated with?**
>
> * [ ] Time-series data and graph data.
> * [ ] Small data and Big Data.
> * [ ] Structured data and unstructured data.
> * [ ] Batch data and streaming data.

#### Quiz 2.

> [!important]
> **What is the simplest form of a sink?**
>
> * [ ] Built-in primitive function
> * [ ] PTransform
> * [ ] PCollection
> * [ ] PSink

### Document - [Module Resources](https://www.cloudskillsboost.google/course_templates/229/documents/526241)

## Schemas

This module will introduce schemas, which give developers a way to express structured data in their Beam pipelines.

### Video - [Beam schemas](https://www.cloudskillsboost.google/course_templates/229/video/526242)

* [YouTube: Beam schemas](https://www.youtube.com/watch?v=pdvO6pn6_y4)

David: Hi there, my name is David Sabater, and I work as outbound product manager for data analytics at Google Cloud. In this model, I'm going to introduce Beam schemas and also provide some code examples. This model about schemas is part of the DataFlow developing pipelines course. Let us start introducing schemas before we look at some examples. A P collection must consist of elements of the same type. For example, it could consist of many JSON objects or of other available object types like byte stream, also known as PlainText, Avro or protocol buffer. To Beam, these collections of types are blocks that are passed between transforms. However, to support this [indistinct] processing, Beam needs to be able to encode each individual element-- for example, as a byte stream-- so elements can be read and passed around to distributed workers. Common Beam sources can produce JSON, Avro, Proto [indistinct], or database raw objects. All of these types have well-defined structures: structures that can often be determined by examining the type. Even within an SDK pipeline, simple Java protos or [indistinct] equivalent structures in other languages are often used as [indistinct] types. These are also have a clear structure that you can infer by applying a custom coder and inspecting the class. As we have seen in our previous slide, the types of records being processed typically have an obvious structure. By understanding the structure of a pipeline's records, we can provide much more concise APIs for data processing. And actually, database folks have known this since the '70s, using schemas. Schemas to the rescue. Most structured records share some common characteristics which can be represented as schemas. They can be subdivided into separate name fields and values. Fields usually have string names, but sometimes, as in the case of index, [indistinct] have numerical indices instead. There is a finite list of primitive types that a field can have. These often match primitive types in most programing languages: int, long, string, and so on. Often a field type can be marked as optional, sometimes referred to as nullable or requited. Often records have a nested structure. A nested structure occurs when a field itself has two fields. So the type of the field itself has a schema. These structure records have some commonly feature array or map type fields. Now, in order to take advantage of schemas, your P collection must have a schema attached to it. Often the source itself will attach a schema to the P collection. For example, when using Avro IO to read Avro files, the source can automatically infer a Beam schema from the average schema and attach that to the Beam P collection. However, not all sources produce schemas. In addition, Beam pipelines often have intermediate stages and types, and those also can benefit from the expressiveness of schemas.

### Video - [Code examples](https://www.cloudskillsboost.google/course_templates/229/video/526243)

* [YouTube: Code examples](https://www.youtube.com/watch?v=ktI9_9qybqw)

person: Let's now show some code examples. Let's show with an example how to use these small, concise APIs for typical data processing examples. We'll start by filtering purchases based on geolocation using longitude and latitude coordinates. Please note that these examples only cover the Java SDK. For Python, SQL and [indistinct] APIs are used instead and will be introduced later in the course. Feel free to check out the [indistinct] documentation on the Apache Beam website for guidance on using schemas in Python as well. We can see how concise the code for filtering is in both cases. This is possible thanks to features introducing Java SDK 8 to filter streams with lambdas. It's easy to understand the logic we are trying to implement with and without using schemas. Now let's see a more verbose example to implement joins in the Java SDK. Let's talk about joining datasets using this example. We're going to join transactions with purchases from an online system. It's important to point out that one transaction always has one or more purchases so we can use an inner join. We want to calculate the total purchases per transaction grouped by user ID. We can see how expressive the solution is when using schemas compared with actual Java without schemas, focusing on the business logic instead of having to embed steps to [indistinct] types, right? Schemas make your code more readable and easier to manage--whoo-hoo.

### Lab - [Serverless Data Processing with Dataflow - Branching Pipelines (Java)](https://www.cloudskillsboost.google/course_templates/229/labs/526244)

In this lab you a) implement a pipeline that has branches b) filter data before write c) add custom command line parameters to a pipeline

* [ ] [Serverless Data Processing with Dataflow - Branching Pipelines (Java)](../labs/Serverless-Data-Processing-with-Dataflow-Branching-Pipelines-(Java).md)

### Lab - [Serverless Data Processing with Dataflow - Branching Pipelines (Python)](https://www.cloudskillsboost.google/course_templates/229/labs/526245)

In this lab you a) implement a pipeline that has branches b) filter data before write c) add custom command line parameters to a pipeline d) Convert a custom pipeline into a Dataflow Flex Template and e) run a Dataflow Flex Template.

* [ ] [Serverless Data Processing with Dataflow - Branching Pipelines (Python)](../labs/Serverless-Data-Processing-with-Dataflow-Branching-Pipelines-(Python).md)

### Quiz - [Quiz 4 - Schemas](https://www.cloudskillsboost.google/course_templates/229/quizzes/526246)

#### Quiz 1.

> [!important]
> **Is it possible to mix elements in sSchema PCollections inside a single Beam pipeline? (Select the two correct answers.)**
>
> * [ ] Not possible within the same PCollection
> * [ ] Yes, in all scenarios
> * [ ] Yes, but only across different PCollections
> * [ ] Not at all

#### Quiz 2.

> [!important]
> **Which of the following element types can be encoded as a schema from a PCollection? (select ALL that apply.)**
>
> * [ ] Avro objects
> * [ ] A single list of JSON objects
> * [ ] Protobuf objects
> * [ ] Byte string objects

### Document - [Module Resources](https://www.cloudskillsboost.google/course_templates/229/documents/526247)

## State and Timers

This module covers State and Timers, two powerful features that you can use in your DoFn to implement stateful transformations.

### Video - [State API](https://www.cloudskillsboost.google/course_templates/229/video/526248)

* [YouTube: State API](https://www.youtube.com/watch?v=7q7TW-JLpf8)

Israel: Hello, my name is Israel Herraiz, and I work as a strategic cloud engineer at Google. In this video, you would learn about state and timers, two powerful features that you can use in your DoFn to implement stateful transformations. Let's start learning about the state API. Apache Beam pipelines can aggregate data with two main types of transformers, using GroupByKey or Combine. ParDos cannot do aggregations. This is because normally ParDos' states transform. You can map from one to zero, one to one, or one to many elements, but you cannot aggregate them together. Or can you? For instance, if you want to count the number of messages you have seen per key, is it possible to do that with a ParDo? Apache Beam comes with additional possibilities for ParDos: stateful transformations. You can have a state variables that can be reused across elements to do any kind of calculation that requires accumulating a state from several different messages. With stateful ParDos, there are many aggregations that can be implemented without having to use a combiner or a GroupByKey. State is maintained per key. So the input should be part of key values. For streaming pipelines, the state is also maintained per window. The state can be read and mutated during the processing of each one of the elements. The state is local to each one of the transformers. For instance, two different keys process, and two different workers are not able to share a common state, but all elements in the same key can share a common state. In this ParDo, we are receiving squares and we are doing a element by element transform to circles. We are also calculating the total area of the square scene so far. To do this calculation, we are accumulating the partial results in a state variable. The state variables are parsed through the process element as any other variable and can be safely mutated from within the process element method. You might be wondering that if you create your own DoFn class, why not using class members assisted variables? In case of reprocessing for instance, because of errors, dataflow takes care of making sure that the mutation of the state is consistent and safe. It will discount any mutation that has not been fully processed. If you use normal class members for keeping that state, you would have to implement that logic to discount that and ensure that your state mutation is safe and consistent. And that's actually very complex. Finally, in step three, after reaching the limit, the state values read will produce an output circles using the accumulated state value triangles. We could have just produced the state values as output, but in general we can produce any output that requires that value. In this example, the pipeline is calling an external service to enrich the data that is being processed. If you are running a large pipeline in data flow, accessing an external service for every element that you are processing in the ParDo can be problematic. You would be making millions of calls per second from hundreds of different workers. It is very easy to overwhelm an external service in that situation. How can you overcome this problem? The state variables allow you to batch the request by accumulating elements in a buffer and making batch calls to the external services, that, for example, you can make a call every 10 messages. Let's see the code for this example. This example is in Python. There are two state variables: buffer state and count state. These are passed through the process method of your DoFn as additional input arguments. In the buffer state, the DoFn is adding new elements right after the DoFn increases the count. When the count surpasses the max buffer size, the call to the external service is made. In this example, we have omitted this call for simplicity. It is important to remember to clear the state once it has been used. The DoFn will not finish entirely unless the whole state has been clear. This is the same example, but in Java. In Java, you need to annotate the state variables using state ID. And they are also passed as additional input arguments to the process method of the DoFn. Again, we have omitted the actual call to the external service for the purpose of simplicity. Now, both these code and the Python version shown in the previous slide have a problem. For the last message that we receive for the last messages that we receive, what if the buffer does not reach this max buffer size? The DoFn would keep that state indefinitely. And the DoFn would never finish. How can you solve that problem? Let's introduce the concept of timers.

### Video - [Timer API](https://www.cloudskillsboost.google/course_templates/229/video/526249)

* [YouTube: Timer API](https://www.youtube.com/watch?v=C6arToRWCzM)

person: In addition to the state API, you can also use timers in your stateful transformations. The combination of both state and timers enables you to get rich and complex stateful transformations. Let's see how state and timers work together. Timers are used in combination with state variables to ensure that the state is clear at regular intervals of time. As with the case of windows and triggers, we can define timers either in event time or processing time. Event time timers depend on the watermark value. Processing time timers depend on the clock of the workers, and not on any timestamp included in the messages that are being processed. With a timer, we can solve the problem of a state being kept indefinitely. When the DoFn is processing the last messages of the last bundle, it is likely that the last buffer will not reach the threshold value set in the code. Also, if messages are coming in slowly, it may take a long time for the buffer to fill up. In both situations, you probably will want to produce some results rather than wait for a long time until more messages show up. Timers are useful for that. Let's revisit our example, but now let's add a timer to avoid having the state wait indefinitely. This example is in Python. The logic for the state is the same as in the previous examples, but now the DoFn has also an event time timer. When the watermark processes the value of allowed lateness, the timer expires. Then the state will be processed and clear, even if the buffer has not reached the limit size yet. By using a timer, you ensure that your state is not kept indefinitely and that the DoFn will finish even if no new messages are coming. Without the timer, the DoFn will have to be waiting indefinitely until the size of the buffer reaches the limit. Note that you need to have expiry method and annotated with on timer. This is the method that is called when the timer expires. This is the same example, but in Java. You need to use the timer ID annotation to create a timer, and the on timer annotation for the method that is called when the timer expires. The logic is the same as in the Python example. The timer expiry method ensures that the bundles are processed, even if the count does not reach the minimum count size or if the messages are coming in very slowly. In summary, this is that DoFn implemented with the examples. In addition to the two state variables, the DoFn has now a timer, which is called when the watermark goes over a certain value of allowed lateness. This is the typical pattern combining a state variable and timers. Remember that you have two options for timers: event time and processing time. You should use event time timers when you want to do the callback based on the watermark and the timestamps of your data, that is, when data is a state. Event time timers will be influenced by the rate of progress of the input data. Processing time timers will expire regardless of the progress of your data. The timer will trigger at regular intervals. Either event or processing time timers can be used for the example shown in the previous slides. You need to decide whether you want to fire always at regular intervals or depending on the pace of progress of your data and use the timers accordingly.

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/229/video/526250)

* [YouTube: Summary](https://www.youtube.com/watch?v=QYjocoiVpIQ)

person: The previous videos, you have learned about how you can use state and timers to implement stateful transformations. Remember, there are two types of timers available in a [inaudible]. Processing time timers are good for implementing timeouts. And event time timers are good for output based on data completeness. The time in processing time is relative to the previous messages and you will have periodic outputs based on that relative time. Event time timers are based on the timestamps of the messages being processed. If you want to make sure that you are emitting output when data is complete and you don't expect more data, then use event timers. A word of warning with event timers. Always make sure to clear the state after emitting the output. If you leave the state behind, then the function will keep waiting for new data and that will consume resources in your pipeline. In summary, for short and predictable latencies with maybe incomplete results, use processing time. For complete outputs with possible high latency, use event timers. Depending on the kind of state that you want to accumulate, you can use a different type of state variables. Value state is genetic. It can hold any kind of value of any type. If you want to add several elements, use a bag state for a more efficient pipeline. Bag will return the objects that were added previously, but with no guarantee of order. Appending objects to a bag is very fast. For any kind of aggregation that is associative and commutative, it is better to use the combining state. And if you are going to maintain a set of key values, a dictionary or map, use map state. With a map, you have random access given a key. Map state is more efficient than other state variables for retrieving specific keys. Finally, the set state, available in the patching programming model, but not supported in data flow. You may use the bag state for similar purposes instead. In summary, state and timers open a lot of new possibilities for due functions. You could implement domain-specific triggering of result not only based on time, but based on anything you may think of. There are also applications for slowly changing dimensions when you keep a dimension table and only a reference to every dimension in a large collection of data. How do you update your large collection of data when the dimension changes? Yes, with state and timers. Joins in a streaming or joining all the elements of a graph with all elements of another graph, so-called biclique. You can also apply the state and timers to implement such a join logic. In any situation where you need fine control on how the aggregation of the elements is done, state and timers allow you to implement a precise and complex logic. In general, any workflow that should be applied per key can be expressed as state and timers. State and timers are a very powerful feature of [inaudible]. You can implement complex logic in a due function and do much more than just map and filters. We could almost say that the limit of the difference is you can do with state and timers is imagination.

### Quiz - [Quiz 5 - State and Timers](https://www.cloudskillsboost.google/course_templates/229/quizzes/526251)

#### Quiz 1.

> [!important]
> **What is the use case for timers in Beam's State & Timers API?**
>
> * [ ] You can use timers instead of state variables to do timely aggregations.
> * [ ] Timers are used in combination with state variables to ensure that the state is cleared at regular time intervals.

#### Quiz 2.

> [!important]
> **Can you do aggregations with ParDo?**
>
> * [ ] No, you cannot do any type of aggregations with ParDo.
> * [ ] You can do aggregations using state variables in a DoFn.

### Document - [Module Resources](https://www.cloudskillsboost.google/course_templates/229/documents/526252)

## Best Practices

This module will discuss best practices and review common patterns that maximize performance for your Dataflow pipelines.

### Video - [Schemas](https://www.cloudskillsboost.google/course_templates/229/video/526253)

* [YouTube: Schemas](https://www.youtube.com/watch?v=d8ioF8rP9YY)

Ajay: Hi, my name is Ajay. I'm a strategic cloud engineer at Google. And now you have covered different building blocks of a DataFlow pipeline, like Beam basic concepts, windows, watermarks and triggers and their usage in creating data processing pipelines; different sources and sinks supported in DataFlow; Beam schemas for processing structured data; pipeline state and timers. In this chapter, we'll take a deep dive into some of the best practices involved in DataFlow. We will begin with the introduction of Beam schemas. We will explore how, using schemas, we can process structured data more efficiently. Then we'll explore best practices for handling unprocessable or erroneous records in a pipeline. We will also cover some best practices around error handling and generation of POJOs, also known as plain old Java objects. We will wrap up this section with an overview of DAG optimization and ways to exploit the life cycle of DoFn to do batch processing. Let's start by looking into Beam schemas. As we have discussed in previous chapters, a schema describes a type in terms of fields and values. Each field is named and has a type. Schemas can be nested arbitrarily and can contain repeated or complex fields as well. When you use schemas in DataFlow jobs, you make your code more readable and easier to manage. Also, it allows the DataFlow service to make optimizations behind the scenes as it is aware of the type and structure of data being processed. For example, the DataFlow service optimizes the encoder and decoder required for [indistinct] and deceleration of data as it moves from one phase to another. Here is an example of using schemas in Java and Python SDK. Each code snippet shows an example of a class with the name, purchase, and in Java and Python respectively. It has five fields: user ID, item ID, shipping address, cost cents, and transactions.

### Video - [Handling un-processable data](https://www.cloudskillsboost.google/course_templates/229/video/526254)

* [YouTube: Handling un-processable data](https://www.youtube.com/watch?v=QDkfWSaLXts)

person: Next section is handling un-processable or erroneous records in a pipeline. When working in a real world use case, we should design our pipelines to handle data that is not in the desired or expected format. Erroneous records may cause our pipeline to fail if not handled properly. When faced with erroneous records, rather than just log the issue, send the [inaudible] to a persistent storage medium, such as BigQuery or Cloud storage to handle them separately. Take--use double tags to access multiple outputs from the resulting P collection. Here we can see the [inaudible] code to implement the dead-letter sink pattern. Consider wrapping user code inside a process element function with a try-catch block. Inside the try-catch block, avoid logging every error exception, as it may overwhelm the whole pipeline. Especially when presenting your [inaudible], increases, instead, send the erroneous records to an alternative dead-letter sink. Line 11 in this snippet shows the erroneous records is being sent to a side output using dead-letter tag. Finally, it is written to a different sink at line 15.

### Video - [Error handling](https://www.cloudskillsboost.google/course_templates/229/video/526255)

* [YouTube: Error handling](https://www.youtube.com/watch?v=qFctZg4fax0)

The next section covers error handling. In this section, we'll explore best practices to handle errors and exceptions in a Dataflow pipeline. How should I handle errors and exceptions in my Dataflow pipeline? As you might already know, errors and exceptions are part of any data processing pipeline. To write a performant, fault-tolerant pipeline, it is important to handle them appropriately. Always read the user code in DoFn functions with a try-catch block. Handle the different exceptions according to their severity. In the exception block, rather than just log the issue, send the raw data out as SideOutput into a storage medium such as BigQuery or Bigtable using a String column to store the raw, unprocessed data. You can use tuple tags to write output to multiple sinks. In case of erroneous records, you can use tuple tags to send data to a dead letter queue.

### Video - [AutoValue code generator](https://www.cloudskillsboost.google/course_templates/229/video/526256)

* [YouTube: AutoValue code generator](https://www.youtube.com/watch?v=hY-jRTjX2jA)

person: In this section, we will explore some of the utility classes that the Beam SDK provides for generating POJOs, which stands for Plain or Java Object. Overall, Apache Beam schemas are the best way to represent objects in a pipeline because of the intuitive way they allow you to work with structure data. However, there are still places where a POJO is needed while developing pipelines in Java, for example, when dealing with key value objects or handling the object state. Hand-building POJOs require you to code appropriate overrides for the equal and hash code matters, which can be time consuming and error-prone. You can end up with inconsistent applications easily. To avoid this, use the AutoValue class builder to generate POJOs. This ensures that the necessary overrides are covered and lets you avoid the potential errors introduced by hand-coding. AutoValue is heavily used within the Apache Beam code base, so familiarity with it is useful if you want to develop Apache Beam pipelines on dataflow using Java SDK. AutoValue can also be used in concert with Apache Beam schemas if you add on @DefaultSchemas annotation. For more information, see "Creating Schemas" in reference section in the end. For more information on AutoValue, see AutoValue docs in the reference section. Remember, this is only applicable to Java-based pipelines.

### Video - [JSON data handling](https://www.cloudskillsboost.google/course_templates/229/video/526257)

* [YouTube: JSON data handling](https://www.youtube.com/watch?v=RlyOHCxYJ1M)

person: In this section, we'll explore best practices involving handling JSON data. Processing JSON strings in dataflow is a common need, for example, when processing click stream information captured from web applications. To process JSON strings, you often need to convert them into either rows or plain old Java objects, also known as POJOs, for the duration of the pipeline processing. The Apache Beam built-in transform JsonToRow is a good solution for converting JSON strings to rows. If you need to convert JSON strings to a POJO using AutoValue, register a schema for the type by using the @DefaultSchema annotation. Then, use the Convert utility class so you end up with code similar to the following code snippet. The structure of JSON data may change frequently. Use Deadletter pattern to handle unsuccessful messages resulting from unexpected structures or schemas. For more details, refer to Queueing Unprocessable Data for further analysis.

### Video - [Utilize DoFn lifecycle](https://www.cloudskillsboost.google/course_templates/229/video/526258)

* [YouTube: Utilize DoFn lifecycle](https://www.youtube.com/watch?v=3fKYk515Wn0)

person: DoFns play an important role in dataflow pipelines. They allow users to transform each input element. In this section, we'll explore how we can reference the lifecycle of DoFn objects for micro batching if required. It's common to invoke external APIs as part of your pipeline. While working on big data use cases, it is easy to overwhelm an external service endpoint if you make a single call for each element flowing through the system, especially if you haven't applied any reducing functions. If you remember what we covered in the Beam concepts review module, you will remember this is what the lifecycle of a DoFn looks like. We recommend batching calls to external systems by leveraging @StartBundle and @FinishBundle lifecycle elements. The code snippets here shows surer code to override @StartBundle and @FinishBundle functions of DoFn. For micro batching, you can initialize or reset the batch in @StartBundle and commit it in the @FinishBundle function. Remember, depending on runner implementation, @StartBundle and @FinishBundle may be called multiple times to process more than one bundle. It is important to reset variables appropriately while using lifecycle functions of DoFn.

### Video - [Pipeline Optimizations](https://www.cloudskillsboost.google/course_templates/229/video/526259)

* [YouTube: Pipeline Optimizations](https://www.youtube.com/watch?v=wtl4G62dD3A)

person: This is our last section on dataflow best practices. Here we'll explore a few things that we should keep in mind while designing our dataflow pipelines. Let's look into some general guidelines we should consider while developing dataflow pipelines. Whenever possible, filter data early in the pipeline and move any steps that reduce data volume up in your pipeline. This will reduce the overall data volume flowing through the pipeline, enabling efficient use of the pipeline resources. This includes placing them about window operations as well, even though the window transform itself does nothing more than DAG element in preparation for the next aggregation step in the DAG. Data collected from external systems often needs cleaning, since a single message can suffer from multiple issues that needs correction. Think carefully about the direct acyclic graph or DAG you will need. If an element contains data with multiple effects, you must ensure that the elements flows through all of the appropriate transforms whenever possible. Applied data transformation serially to let the Dataflow service optimize data for you. Whenever transformations are applied serially, they can be merged together in single stage, enabling them to be processed in the same worker nodes and reducing costly IO network operations. If your pipeline interacts with external systems, look out for back pressure and external systems. May be a key value store like BigTable or [indistinct] used for lookups in a pipeline or an [indistinct] sink your pipeline writes to. It is recommended that you ensure the appropriate capacity of external systems to avoid back pressure issues. Enabling auto scaling for Dataflow pipelines is also a good idea. If for some reason your [indistinct] system is backlogged, your Dataflow pipeline can scale down instead of underutilizing pipeline resources. In this module, we started with intro to Beam schemas. We discussed its usefulness while dealing with structured data. Then we looked into best practices for handling unprocessable or erroneous records. Next, we covered best practices around error handling and generation of modules. We wrapped up this session with overview of DAG optimizations, and ways to exploit lifecycle of [indistinct] to do batch processing. Thanks for joining.

### Lab - [Serverless Data Processing with Dataflow - Advanced Streaming Analytics Pipeline with Dataflow (Java)](https://www.cloudskillsboost.google/course_templates/229/labs/526260)

In this lab you read deal with late and malformed streaming data using advanced Apache Beam concepts.

* [ ] [Serverless Data Processing with Dataflow - Advanced Streaming Analytics Pipeline with Dataflow (Java)](../labs/Serverless-Data-Processing-with-Dataflow-Advanced-Streaming-Analytics-Pipeline-with-Dataflow-(Java).md)

### Lab - [Serverless Data Processing with Dataflow - Advanced Streaming Analytics Pipeline with Dataflow (Python)](https://www.cloudskillsboost.google/course_templates/229/labs/526261)

In this lab you read deal with late and malformed streaming data using advanced Apache Beam concepts.

* [ ] [Serverless Data Processing with Dataflow - Advanced Streaming Analytics Pipeline with Dataflow (Python)](../labs/Serverless-Data-Processing-with-Dataflow-Advanced-Streaming-Analytics-Pipeline-with-Dataflow-(Python).md)

### Quiz - [Quiz 6 - Best Practices](https://www.cloudskillsboost.google/course_templates/229/quizzes/526262)

#### Quiz 1.

> [!important]
> **What is the recommended way to convert JSON objects to POJOs?**
>
> * [ ] Use JsonToPOJO
> * [ ] Use JsonToRow

#### Quiz 2.

> [!important]
> **Which functions of the DoFn lifecycle are recommended to be used for micro-batching?**
>
> * [ ] setup and teardown
> * [ ] startBundle and finishBundle
> * [ ] init and destroy

#### Quiz 3.

> [!important]
> **Choose all the applicable options:If your pipelines interact with external systems,**
>
> * [ ] Not provisioning external systems appropriately may impact the performance of your pipeline due to back pressure..
> * [ ] It is important to provision those external systems appropriately (i.e., to handle peak volumes).
> * [ ] External System doesn't impact performance of a Dataflow pipeline as they are run outside the Dataflow environment.
> * [ ] Testing external systems against peak volume is not important.

### Document - [Module Resources](https://www.cloudskillsboost.google/course_templates/229/documents/526263)

## Dataflow SQL and DataFrames

This modules introduces two new APIs to represent your business logic in Beam: SQL and Dataframes.

### Video - [Dataflow and Beam SQL](https://www.cloudskillsboost.google/course_templates/229/video/526264)

* [YouTube: Dataflow and Beam SQL](https://www.youtube.com/watch?v=dviDsKFIwjg)

Hi again, my name is David Sabater Dinter and I work as Outbound Product Manager for data analytics at Google Cloud. This module is part of the Dataflow Developing Pipelines course, and we are going to introduce two new APIs to represent your business logic in Beam, SQL and Dataframes. Let’s start with SQL, available in our Dataflow runner and also within the Beam SDK. You might remember this slide from the Schemas section, where one of the key takeaways was that by understanding the structure of a pipeline’s records, we can provide much more concise APIs for data processing. Do you remember what API is used by Database folks since the ‘70s? SQL, that’s right. Have you ever wondered why? Let’s discuss briefly. SQL reduces boilerplate code, and is easier to understand by implementing simple SQL statements describing your transformations. It can also automatically optimize the pipeline execution: SQL planners can actually optimize on every execution, probably more so than our handwritten code. And workers are also able to perform further optimizations. SQL is a domain-specific language used in programming and designed for managing data held in a relational database management system. Or, more relevant here, for stream processing in a relational data stream management system. The data is accessed through relational algebra, where, for example, projections are used to pick a subset of the columns to query, filter to apply certain conditions to the rows being returned, and finally apply aggregation via the group by clause. It includes also syntax to operate on nested structures. Don’t worry about reading the code! This is just to show you how verbose it can be to write a join in the Java SDK. You might remember the example that was used to introduce schemas in the other section. The important part is to understand what a join is. Fundamentally, this is joining two input datasets to obtain one output dataset. Let’s now show why Dataflow SQL can help you to implement your data processing pipelines. See the amount of lines of code required to implement a join in the Java SDK, without using schemas and SQL. Most of the extra code is required to annotate types, mapping key/values, and so on. If we use the Scala SDK with Scio, developed by Spotify and available as open source Apache license, we can reduce the code verbosity significantly, thanks to lambdas and type inference features from Scala. With SQL, we are providing the ability to translate the business logic written in SQL back to Apache Beam primitives, to be executed in our Dataflow serverless service in a scalable and overall concise way. What are we trying to do here? We can see in this slide how these primitives are derived and chained together in a Directed Acyclic Graph to execute our logic. Before we go into detail about Dataflow SQL, let’s first distinguish some important components associated with the personas and their user journeys or how they interact with the service. A data analyst will typically start interacting with historical data in the BigQuery UI, running SQL statements on historical data to test their hypothesis about what happened, typically referenced as batch data. After testing on historical data, they will ideally want to test the same business logic in the form of SQL statements over real-time data this time. Switching to the Dataflow SQL UI to test the same logic over real-time data involves very few changes—nice! Lastly, once the data analyst is happy with the logic, they would be able to pass those SQL statements to the data engineer, who will be able to implement them with little change in the form of SQLTransforms inside the Beam Java SDK. Please note that Beam SQL and Dataflow SQL are effectively identical, but while Beam SQL offers a programmatic interface, Dataflow SQL also offers a UI interface. Let’s walk through this journey together. First we begin with the common denominator, which is Beam SQL. It allows a Beam user to query bounded and unbounded PCollections with SQL statements, also referred to as querying data in batch and streaming mode. Your SQL query is embedded using SQLTransforms, an encapsulated segment of a Beam pipeline similar to PTransforms. You can freely mix SQLTransforms and other PTransforms in your pipeline if needed. It also supports User-Defined Functions. Beam SQL includes the following dialects to interpret SQL statements, which we will cover later: Apache Calcite SQL, and Google ZetaSQL. Finally it integrates Schema PCollections and supports windowing when aggregating unbounded data. As mentioned earlier, Beam SQL supports two dialects to understand the relational algebra: The Beam Calcite SQL is a variant of Apache Calcite, a dialect widespread in big data processing, compatible with Apache Flink SQL for example. Beam Calcite SQL is the default Beam SQL dialect and supports Java UDFs among other mature features. Beam ZetaSQL is more compatible with BigQuery, so it’s especially useful in pipelines that write to or read from BigQuery tables, for example when using the Dataflow SQL UI writing to BigQuery. Now let's talk about Dataflow SQL. Dataflow SQL integrates with Apache Beam SQL and supports a variant of the ZetaSQL query syntax, using SQLTransforms in a Dataflow Flex template (but all transparent to you, the user!). You can actually write your SQL logic through the UI or gcloud client command line. ZetaSQL provides the same dialect as BigQuery Standard SQL. And lastly, it can optionally be used as a long-running batch engine. As you may remember from the personas we described, one of the core use cases for Dataflow SQL is to help data analysts query streaming data using a common language, SQL. A typical use case will: Select from Pub/Sub, Join with batch data, Aggregate some metrics over a particular Window, And finally publish to BigQuery or Pub/Sub topic. It’s also worth mentioning that the Dataflow SQL is not only restricted to GCP-native services like BigQuery or Pub/Sub. We are also planning to integrate with many others like Kafka and Bigtable. As we described earlier, data analysts can use their existing SQL skills to develop and run streaming pipelines from the BigQuery web UI. You don't need to set up an SDK environment or know how to program in Java or Python. By using the familiar BigQuery UI, one could easily join streams such as Pub/Sub with snapshotted data sets. BigQuery tables are an example, but Kafka and Bigtable are also coming soon as already mentioned. You can query your streams at static datasets with SQL by associating schemas with objects such as tables, files, and Pub/Sub topics. You create a job and specify the output location, for example writing your results into BigQuery tables for analysis and dashboarding. This is just as simple as selecting Dataflow as the execution engine for SQL statements, using the BigQuery web UI and your destination dataset and table. Remember always to ensure the regional endpoint, sources and destination are within the same region if possible. You can directly access the Dataflow UI to monitor the underlying Dataflow job running your query. In case you want to launch Dataflow SQL jobs programmatically instead of relying on the UI, there is also an option to use the gcloud command tool for authoring jobs through the Dataflow SQL CLI. The Dataflow SQL interface is integrated with gcloud to give you that capability through your command-line interface. Finally, as described in the user journeys, the data engineer would be able to apply the SQL logic implemented by the data analyst, within existing pipelines. In this case, note the use of PCOLLECTION as a table name. Named Tuples can also be accessed by name. And before closing this section, you can see here a simplified version of the Dataflow Template being submitted when using the Dataflow SQL UI. Data engineers are free to actually implement a similar template to be able to encapsulate all that logic from the data analysts, in a more programmatic and scalable way.

### Video - [Windowing in SQL](https://www.cloudskillsboost.google/course_templates/229/video/526265)

* [YouTube: Windowing in SQL](https://www.youtube.com/watch?v=UbdLI59r_CA)

person: Let's move now to see how we can apply streaming concepts like windowing in SQL. We're going to see how we can implement the different types of windows for aggregations as we stream data. The first one you will remember from previous sections is the tumbling or fixed window. We can see in this gray how the tumble term is included to incorporate that type of windowing. Then for hopping or sliding windows, the same approach can be done using the hop term. In this case, including two different intervals. One defining the length of the window and the other when the new window begins. Lastly, we can see how session windows will be implemented in SQL. In this case, session will be used, including the interval of time that determines when a new session window must be created.

### Video - [Beam DataFrames](https://www.cloudskillsboost.google/course_templates/229/video/526266)

* [YouTube: Beam DataFrames](https://www.youtube.com/watch?v=LdempJbwwJo)

person: In the last but not least part of this model, let's discover how we can also leverage another Appia used widely data frames the Apache beam Pythonesque provides a data frame API for working with panels like data frame objects. The feature lets you convert a collection to a data frame and then interact with the data frame using the standard methods available on the panels. Data from API like the example in the slides, adding up total prices grouped by a recipe. The data from API is built on top of the panels implementation and Panesar different methods are invoked on subsets of the data sets. In parallel, the big difference between beam data frames and Panesar difference is that operations are deferred by the Beam API to support the beam parallel processing model. You can think of beam data frames as a domain specific language for beam pipelines similar to beam cycle data frames is a DSL built into the Beam Python SDK. Using these DSL, you can create pipelines without referring standard beam constructs like produce and Combine Purkiss. The Beam Data Frame API is intended to provide access to a familiar programing interface within a beam pipeline. In some cases, the data from API can also improve pipeline efficiency by deferring to the highly efficient Becta responder's implementation. Let's introduce the first primitive group by the more primitive group Bickie Combined. Pearcy and Thibeault combined effect are significantly more verbose and less intuitive. You've already seen some examples of these. When we cover schemas with the Java SDK, a group, a group, sorry, a group by operation involves some combination of a combination of a splitting the object, applying a function and combining the results. These can be used to group large amounts of data and compute operations on these groups using an arbitrary expression like the example above producing the expected results. It's also possible to use the data from API by a function to the two data frame transform data frame. Transform is similar to sequel's transform from the beam cycle DSL that we introduced before where sequel Transform translates a sequel query to a P transform a data frame. Transform is a pittance from the plays function that takes on returns data frames. Are they different? Transform can be particularly useful if you have a standalone function that can be called both on beam and an ordinary PARNAS data frames data frame, transform, can accept and return multiple collections by name and by keyword as shown in the following examples. These is last slide demonstrates how simple it is to convert Pikul collections to beam data frames and vice versa. Beam data frames are deferred like the rest of the Beam API, as a result, there are some limitations on what you can do with beam data frames. Compare to the standard policy implementation. Again, because all operations are deferred, the result of a given operation might not be available for contraflow or interactive visualizations. For example, you can compute some, but you can't branch. And the result? Result columns must be computable without access to data, for example, you can't use transpose. Also, big elections are inherently unordered, so panis operations that are sensitive to the ordering of rows are unsupported, for example, or other sensitive operations such as shift comix, a Kumin head and tail are not supported with being data frames. Competition doesn't take place until the pipeline runs. Before that, only the shape or a schema of the result is known, meaning that you can work with the names and types of the columns, but not the result data itself. However, we can see that HelloWallet example in data processing counting words. We first need to map the source data to a schema to be able to see these more expressive APIs. We then need to convert to data frame before we can apply a group by function to aggregate the sum by word to obtain the word count. And lastly, like in pangas data frame, we can directly say the results with the two kesby method. Finally, data frames can also be converted back to schema collections.

### Lab - [Serverless Data Processing with Dataflow - Using Dataflow SQL for Batch Analytics (Java)](https://www.cloudskillsboost.google/course_templates/229/labs/526267)

In this lab you add SQL statements to the previously written Beam pipeline that aggregates site traffic by user and by minute. You also execute a Beam SQL job from the BigQuery UI.

* [ ] [Serverless Data Processing with Dataflow - Using Dataflow SQL for Batch Analytics (Java)](../labs/Serverless-Data-Processing-with-Dataflow-Using-Dataflow-SQL-for-Batch-Analytics-(Java).md)

### Lab - [Serverless Data Processing with Dataflow - Using Dataflow SQL for Streaming Analytics (Java)](https://www.cloudskillsboost.google/course_templates/229/labs/526268)

In this lab you read data from a streaming source, perform the same aggregations you performed before but this time in SQL, and write out results in a streaming fashion to BigQuery. You will also create a Dataflow SQL Streaming job from the BigQuery UI.

* [ ] [Serverless Data Processing with Dataflow - Using Dataflow SQL for Streaming Analytics (Java)](../labs/Serverless-Data-Processing-with-Dataflow-Using-Dataflow-SQL-for-Streaming-Analytics-(Java).md)

### Quiz - [Quiz 7 - Dataflow SQL & DataFrames](https://www.cloudskillsboost.google/course_templates/229/quizzes/526269)

#### Quiz 1.

> [!important]
> **Which two of the following interfaces support Calcite SQL?**
>
> * [ ] Beam SQL client
> * [ ] Dataflow template
> * [ ] Dataflow SQL

#### Quiz 2.

> [!important]
> **What operations can you do in standard Pandas DataFrames that are not possible in Beam DataFrames?**
>
> * [ ] Shift the DataFrame
> * [ ] Write the DataFrame columns as rows
> * [ ] Compute two different aggregates based on the input data

### Document - [Module Resources](https://www.cloudskillsboost.google/course_templates/229/documents/526270)

## Beam Notebooks

This module will cover Beam notebooks, an interface for Python developers to onboard onto the Beam SDK and develop their pipelines iteratively in a Jupyter notebook environment.

### Video - [Beam Notebooks](https://www.cloudskillsboost.google/course_templates/229/video/526271)

* [YouTube: Beam Notebooks](https://www.youtube.com/watch?v=lR4iLBWzh84)

>> Hi, my name is Rosa Orkney and I'm a Google cloud developer advocate for Google Earth dataflow. And today, as you can see on the agenda, we're going to be looking at being notebook's and running them on the dataflow service. We think about the way that we would normally develop a Pache pipeline. The SDK allows us to declaratively described the pipeline that we would like the service to run. As we can see in this example here with the store sales and the online sales, which comes and describes a direct, slightly graphic computation attack. Once we've done this, we actually submitted to the service and the service goes ahead and runs that pipeline for us. The this is fantastic for production use cases because it allows the runner to be able to do fancy optimizations like dataflow fusion, where it can collapse stages together and make it very efficient for the processing that's going to happen. However, it's not the best experience when you're first developing your pipeline because we often get into this right submit job re waiting for logs, waiting for print statements. This is especially true when we're still exploring our data where we would not only like access to the data as we purchased it, but access to the data as our transforms are transforming that data now with Apache Beam. We do have the interactive runner that's available and the interactive runner allows us to be able to get access to the pipeline. So specifically, it gives us access to the intermediate results that are available from our transformations, allowing us to do exploration and the next stages of development. Importantly, as you would expect with Apache Beam, the interactive runner also works with batch and stream sources. So we no longer have to mock out or objects as we're doing our development. We can actually, when needed, run directly against the real data, even if that source is unbounded and industry. So going on to the you know, how we get to run, this will look at the being notebook's and on this slide you actually see the various steps we need to take with the data flow service UI to be able to set up a notebook environment. So we start the data flow notebooks that gives us access to creating new instances. And the nice thing there is that once a new instance is being created, which is the host for our notebooks, all the libraries and things that we need for them already in place so we can immediately start doing our development. The notebook's environment also comes with some ready-Made examples, which are great for exploration and learning, but also very useful for Kosmidis specifically. We'll talk a little bit more about one of the examples, the word count example, and we won't reproduce it completely in this session. That's for you to hopefully do once you start exploring the notebooks directly. But just to take a few snippets and some of the options that will be available when you do start making use of the notebooks. So in this slide, we can actually see some of the transformations in that word. Count example. First, we need to read from our inbounded source, which we do with read from pops up. So this puts it into the words collection. We then apply a fixed window to those elements, a fixed window of 10 seconds. This puts it into the window word collection. And finally we do a count. Now, if we were doing this without the interactive runner at this point, we'd have the fourth transform, would do some lobbying or turn it converted to a sink where we can then look at the data because we are using the interactive running, we can access intermediate results. How do we do need a way to tell the system when to stop reading from this unbounded source of information? And the way that we do that is with a couple of options from the interactive runner, which is the IB options, either recording duration to be set or recording size limit. The duration gives a fixed amount of time for the interactive runner to record data, and the recording size limit gives us a fixed amount of bytes to read. The latter is very useful when you're working with a real stream of information where you might have a very large volume of data and you don't want all of that to be put into your notebook as you're doing the experimentation. The other important factor is that we have the option to actually reuse the stream of information that we've gathered or to get fresh data. So the reuse is useful because then we were exploring data and manipulating it or working with the same data set in terms of looking at those collections and looking at that intermediate result. I did not show allows us to visually see the information, as we can see in this slide example here. The options include window info will also give us some extra metadata about each element. For example, the event time and the window that that data belongs to. Visualization is obviously very useful, but we also want to be able to use this data and manipulate it directly within our code, for that we can use it to collect, which allows us to then output this into a Penders data frame, which then we can do all our manipulation against if we wanted to actually do further visualization of data with graphs, etc.. The notebook comes in built with a feature via IB show, which is to set visualize data to true. This gives us access to the UI that you're seeing here and you can do various visual exploration of the information with these core primitives. We can now start getting out of that right. Submit to service lifecycle when we are dealing with our data. But obviously once we complete our development, we then want to be able to submit the job to the service and we want to do that with as little as code as possible. So finally, until the gain from development to production, there's very little we actually need to do the code, because at the end of the day, we're just writing a Beam SDK code for this whole process. We just need to enable running on the service by importing the data from runner, by providing options, the pipeline options, for example, the project and the staging directory, etc. And then we just do run it on pipeline, which will submit that job to the service. So hopefully with this, you've had a nice overview of what will be available when we when you start working with the notebooks and thank you.

### Quiz - [Quiz 8 - Beam Notebooks](https://www.cloudskillsboost.google/course_templates/229/quizzes/526272)

#### Quiz 1.

> [!important]
> **Which one of these statements is true?**
>
> * [ ] When using the interactive runner, if you want to play with the values from a PCollection within a dataframe, you must access them from within a DoFn.
> * [ ] You can use the  option include_window_info from ib.show to get extra metadata about each element in a Pcollection.
> * [ ] When using the interactive runner, you have to create a logging DoFn to see the values of an intermittent PCollection.

#### Quiz 2.

> [!important]
> **Which two of the following statements are true about using the interactive runner?**
>
> * [ ] You can limit the amount of data the interactive runner records from an unbounded source by setting recording_size_limit.
> * [ ] You can limit the amount of time the interactive runner records data from an unbounded source by using the recording_duration option.
> * [ ] You can limit the number of elements the interactive runner records from an unbounded source by setting the recording_element_count option.

### Document - [Module Resources](https://www.cloudskillsboost.google/course_templates/229/documents/526273)

## Summary

This module provides a recap of the course

### Video - [Course Summary](https://www.cloudskillsboost.google/course_templates/229/video/526274)

* [YouTube: Course Summary](https://www.youtube.com/watch?v=gm8reExV_Vo)

person: You've reached the end of the developing pipelines with dataflow course. Let's do a brief recap of what we've covered. We started by reviewing core Apache beam concepts, defining key terms like pipelines, Peak Collection's P transforms and Rutter's. We also covered utility transformers like Pardoo Group Bickie and Flatting included with the breakdown of the life cycle of a new fund. We can put these pieces together to build basic beam pipelines. We looked at how windows, watermarks and triggers were together to deal with streaming data with the flexibility of the B model. You can decide how your pipeline emits results and manages late arriving data. These concerns can help you translate your business logic into a streaming pipeline that will deliver Real-Time Insights for your applications and end users data for jobs. Read from sources and right to Sync's. And we covered a wide range of issues available to you to the BPM SDK from Textile and File Eyo for text and file says respectively to Google Cloud Io's like McQuary Pub, Sub and Big Table. We also covered popular open source countries like Kafka, IO and Avro. These sources and sinks almost all have their own nuances, so it's important to reference the documentation to review what tuning Premraj available to you for your use case. Your organization might also need to build their own connectors for proprietary Io's with split-Level do funds, you can write your own source that leverages the utilization benefits of distributed processing to maximize throughput schemas. Help express the structure of your data in the language of beam, making your code easier to manage and more efficient to run. State and timers provide a way for developers to manage Purkey State, which gives more fine grained control over aggregations by manipulating the state of inflight data and controlling when data is processed. Using timers, you can effectively enable any use case you can imagine, no longer limited by the limitations of Pardieu in group bickies. We combine all of these building blocks in the best decade to develop pipelines that are executed on the data for service. We share a number of best practices based on years of experience, working with engineers across a wide range of use cases, some of the highlights include implementing a dead letter. Q Which can ensure that your pipeline does not stall indefinitely if it encounters corrupted input data. Devising an air handling strategy for your due funds handle JSON data using beams built in JSON utility transforms. Bache calls to external APIs so you don't disrupt external services and employing various pipeline optimization techniques that are discussed in more detail in the module. We explore an alternative way to launch data for pipelines using sequel. Dataflow school provides an interface integrated into the big queerer UI to select your sources, write a school segment with streaming extensions that describe your windowing logic, then write to a big query table for further analysis. However, if you want to invoke data for jobs via SQL programmatically, we also offer a command line interface to do just that. If you want to integrate sequel into your handcrafted Beahm pipeline, you can do that with Beahm sequel. We introduced the beam data frames, which allows you to convert a collection to a data frame and interact with it using the standard methods available in the popular Panda's data frame API. If you are a Python developer, data scientist, this API can offer a familiar entry point into beam data flow that looks like your existing toolkit. We finished the course by covering Beahm notebooks, which merges the Beam Python SDK with the Jupiter Lab interface, enabling a completely different way of operating beam pipelines. The interactive runner that is deployed on beam notebooks allow you to inspect intermediate P collections so that you can validate your transformations before you launch a pipeline onto the data service. Bime Notebook's also contains source recording features that allow developers to prototype pipelines that read from unbounded data sources a BMW VM can be launched directly from the console UI. If you're just starting with the Python STK, the beam notebook is the place to start. It comes preloaded with several tutorials and walked through to the SDK offering a learning path that is available and no other SDK. In summary, Apache beam data flow offers a compelling platform for all your data processing needs and without the fear of vendor lock in. We're excited to see what applications you built with the concepts from this course.

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
