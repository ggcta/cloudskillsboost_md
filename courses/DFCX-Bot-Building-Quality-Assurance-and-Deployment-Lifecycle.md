---
id: 1105
name: 'DFCX Bot Building Quality Assurance and Deployment Lifecycle'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1105
date_published: 2024-07-10
topics:
  - Deployment
  - CI/CD
---

# [DFCX Bot Building Quality Assurance and Deployment Lifecycle](https://www.cloudskillsboost.google/course_templates/1105)

**Description:**

This course explores the quality assurance best practices and the tools available in Dialogflow CX to ensure production grade quality during Virtual Agent development, as well as the key tenets for the creation of a robust end to end deployment lifecycle

**Objectives:**

* Understand the different levels of testing required to ensure production grade quality in Virtual Agent development in Dialogflow CX
* Understand the importance of creating a Customer User Journey first approach to testing
* Understand how to best create test cases in Dialogflow CX
* Learn how to implement an end-to-end testing plan and highlight additional testing considerations.
* Understand how to best set up a CI/CD pipeline and a robust DevOps automation framework in Dialogflow CX

## Creating a testing strategy

This module explores the importance of adopting a customer user journey (CUJ) "first" approach to testing and the different levels of quality assurance required to ensure production grade quality in Virtual Agent development

### Video - [Creating a testing strategy](https://www.cloudskillsboost.google/course_templates/1105/video/492165)

* [YouTube: Creating a testing strategy](https://www.youtube.com/watch?v=zwSyxHXHnuE)

Welcome to “Dialogflow CX Bot Building Quality Assurance and Deployment Lifecycle”. In this course we are going to provide you with the tools to assess and ensure production grade quality of your virtual agent development. We will also review the key tenets of an end to end deployment lifecycle using the capabilities available in Dialogflow CX. In this course, we will focus primarily on quality assurance and the deployment lifecycle for Dialogflow CX virtual agents. Creating a strong testing strategy is fundamental to delivering high-quality bots. Dialogflow CX has specific features and tools to help you design and implement those tests. We'll discuss those features and show you how to build a test plan. This includes mapping out test cases and understanding the best way to integrate those tests into your development process. After your bot has been thoroughly tested, we'll dive into the deployment lifecycle. We'll explain how to move your bot into upstream environments successfully. Finally, we'll cover continuous integration and continuous delivery, also known as CI/CD, which ensures smooth, error-free automated deployments. Let’s start by going through the framework for creating a solid testing strategy for virtual agent development. Creating a solid testing strategy enables you to: Adopt a customer user journey first approach to testing, and Leverage different approaches to testing depending on the needs and the users in the bot development lifecycle We will go over each of these concepts in detail. Building virtual agents is particularly challenging due to the non trivial business requirements, the large amount of data involved as well as the number of stakeholders involved in the success of the initiative In order to manage this uncertainty with confidence, a solid testing approach is needed to systematically assess the performance of the bot before every launch to production. Systematically assessing performance doesn’t mean checking the robustness of an isolated newly implemented feature, but testing the end to end performance of the agent in solving a customer issue with following a Customer User Journey “first” approach to testing. A customer journey is a depiction of the experience that a customer goes through to acquire the goods and services that they need. Whether that be completing a transactional task or just obtaining the answer to a question, traversing the end-to-end journey for a customer helps gain a better understanding of their needs, and can help businesses identify how to best satisfy them. Customer User Journeys, or CUJs, are a fundamental tool in software development to ensure production grade quality and readiness for a release. So what are the steps for the creation of CUJs? First, establish a finite catalog. Establishing a finite list of scenarios for what the customer experience should look like is critical in setting up a robust testing strategy. These scenarios should be defined and agreed upon by all stakeholders. This ensures the existence of a formal agreement between the Quality Assurance team and the rest of the program around what should be tested. Adopting a Customer User Journeys “first” approach to testing, provides a level of assurance that the core user journey functionalities were covered in the verification by the Quality Assurance team, prior to the sign off of every new release to production. So treat a set of explicit CUJs with testing protocols like a “contract” between development and QA which makes the testing strategy: Reproducible Deterministic and Give us High Confidence that the experiences built work. The bottom line is, the more agreement around CUJs the lower risk of encountering problems close to production deployment. The second step to keep in mind while creating a Customer User Journeys catalog is prioritization. Customer User Journeys can be prioritized according to business needs, production traffic volumes, and complexity. When possible, the key driver should be production traffic. This is because some edge cases do not require as much attention compared to journeys that encompass a higher share of traffic. You need high confidence that your top Customer User Journeys work. Once prioritization is defined, a level of reporting can be generated around the catalog of CUJs. The Quality Assurance team should be able to generate reports around CUJs to highlight what was tested, which CUJ rendered the best test results and which had the most problems. Documenting “Failures” allows development team reproduce problems and resolve them. Successful tests are important as well so that, when encountering failures later a callout can be made pointing at the time when it last worked. This is to make sure that once a functionality is fixed, it stays fixed. Once the CUJs catalog is built, a Requirements Traceability Matrix, or RTM for short, can help the QA team by providing a mapping between the Customer User Journeys steps and the test cases created. The goal is to have adequate test coverage for all requirements with as many variations as possible. The RTM provides insight into the coverage and acts as a checklist for the complete coverage of all items prior to every release by: Ensuring test coverage aligns with real user needs Helping prioritize testing efforts based on critical user paths, and Catching potential failures early in the development cycle. One last critical element to mention in the creation of a solid test strategy is interacting with the bot adversarially. This is one of the key responsibilities of the Quality Assurance during bot building which is often overlooked. The user’s behaviour is not always friendly to the bot or consistent to the bot’s objectives. Therefore, having test cases configured in a way for you to spot where the bot may fall off the customer “happy path” to resolution, allows you to catch problems early in the development lifecycle. This promptly rectifies them and provides you with the confidence in signing off any new delivery.

### Video - [Methods of testing](https://www.cloudskillsboost.google/course_templates/1105/video/492166)

* [YouTube: Methods of testing](https://www.youtube.com/watch?v=EGwZ1H6GQRw)

Now it’s time to explore the various methods of testing a virtual agent. We will learn what methods options are most used, their purpose, what stage of development they occur in and who is responsible for administering each testing phase. Let’s dive right in! When thinking about methods of testing, it is important to consider the sequence and interaction of all the systems that engage with Dialogflow CX and the customer. Other systems that interact with Dialogflow CX typically are: The telephony Integration component, which handles a seamless call transitions. The client services application, like a web or mobile application, which helps render messages for user interfaces. And lastly, is the backend systems which provides more in depth information about a customer. It typically facilitates the ability to perform specific actions, like payments or updating the caller’s information. In this course, we will concentrate on the testing methods directly related to Dialogflow CX, which are focused on intent recognition, routing accuracy, and response generation. Let’s explore each of them. The most successful testing strategies for virtual agents in Dialogflow CX are based on a four tiered approach: one - Unit Tests, two - Integration tests, three - End to End tests and four - A/B tests. It is best practice for the development team to own the responsibility of the first two types of testing - unit tests and integration tests - which occur earlier in the development lifecycle. The Quality Assurance team focuses more on End to End and A/B tests which occur closer to production deployment. Developers are responsible to ensure no tests cases are failing before and after any development work. They also have ownership to build new tests to check bot behavior as compared to the defined requirements of a new feature. Also, keep in mind that the degree of automation may decrease along the development lifecycle. Unit and integration tests are expected to be mostly automated for ease of confirming full coverage of utterances, the absence of regressions and the correct functioning of APIs. End to end and A/B tests will require a higher degree of manual effort to ensure more careful assessment of most complex use cases. Regardless of the stage of the development lifecycle, testing should be done in an environment as similar as possible to the production environment. There are two types of unit tests. Routing and NLU. Routing Unit tests ensure that the logical transitions between steps in a customer user journey function as expected. All pages and transitions should be covered by at least one unit test. As mentioned, this process should be owned by the development team that built those routes. Here are some key factors to consider when designing routing unit tests: Routing unit tests provide a way to prove that the logic of a specific unit performs correctly against functional requirements. They also provides developers with feedback about whether updates to the virtual agent have broken any existing behavior. Routing unit testing begins in the development team’s sandbox environment and it entails an exhaustive test of one module, segment, or application in isolation from other applications or solution components. Teams should ensure all happy paths have unit test cases developed. Some routing unit tests may include interactions with other units within an application. If a webhook is not available in the development environment, they can be mocked using Webhook Parrot. Natural language understanding, or NLU, unit testing is the second foundational component of unit testing. This testing ensures that the NLU model for the virtual agent is performing optimally by correctly identifying the intent of diverse user utterances. This includes variations in phrasing, synonyms, and informal language. This type of testing covers intent recognition and entity extraction. Entities can include dates, locations, and product names from a wide range of utterances. Large volumes of sample utterances should be used to identify areas where the NLU can be fine tuned by adding, moving, or deleting training phrases from the model. A key factor in achieving full NLU coverage during unit testing is to ensure that the agent has appropriate responses for user inputs that is not expected at any given point in the customer user journey. This includes tailoring no-match responses at the page level to repeat the prompt or options they are being asked to select. Consider a use case where the virtual agent needs to determine which device a user is contacting customer support about. To accomplish this, the user must input the last 4 digits of the devices serial number. However, the caller could respond with “I don’t know”, “None of them”, or “All of them”. In order to reduce escalations on that page, the virtual agent designer will need to create intents to capture these types of user input and respond accordingly. While this level of detail can be planned for during the bot design phase, you will invariably encounter unexpected ways that users interact with your virtual agent. As new utterances that are not recognized by the existing intents on a page are discovered, they will need to be added as training phrases to the existing intents, or as training phrases to new intents that require different agent responses. Note that the majority of unit tests, both routing and NLU, can be successfully executed in the Dialogflow CX sandbox with the appropriate governance and configurations. A best practice is to ensure a zero failing test cases policy before and after starting any development work. This also helps during regression testing to identify which test cases require fixing. Next is Integration system testing. The objective of this testing is to discover errors in the interfaces between the components and to verify that the modules at each level work together. Integrated system testing is also generally expected to validate as much of the full spectrum of the end-to-end application flows as possible. This will generally include invocations to services, applications, systems, and components which are external to the application or organization in which the Integrated System Testing team resides. Generally, developing in an environment as similar to production significantly eases Integrated System Testing. Integrated System Testing should be done by the development team where possible. Examples include Dual-tone multi-frequency, also known as DTMF, or numerical input, barge in, and webhook timeouts. Developers should also ensure full coverage of test cases that require telephony integration. End to end testing or User Acceptance Testing, UAT for short, allows the quality assurance team to ensure that all the customer experience requirements are met and the Virtual agent can react to real world scenarios. This is accomplished by testing the whole catalog of customer user journeys and by interacting with the virtual agent adversarially. Like integration testing; End to End Testing in a production-like environment significantly eases UAT testing as well. The last, most common type of testing in Dialogflow CX is AB Testing. This is the final phase of the certification process prior to exposure to live customers. AB testing is a UX methodology that involves a randomized experiment to compare the performance of two systems or conversational experiences to verify which versions better meets the functional requirements outlined in the project scope or specifications. In this instance, two conversational experiences you may want to compare through AB testing are: the IVA legacy experience versus the newly developed experience built through Dialogflow CX, or secondly, the customer experience handled by a live agent versus the newly developed experience built through Dialogflow CX. As long as it can show an improvement in the call handling experience. This phase is generally managed by the Quality Assurance team. The diagram shows the sequencing of the four types of testing in the development lifecycle and the environments in which they typically occur. In the development environment, you will have unit and and integration testing. In the QA environment, you will have end to end testing. In the pre-prod environment, you will have A/B testing. As mentioned, performing these tests in an environment as close as possible to production is very important, especially for integration and End to End testing because it allows us to get more accurate feedback regarding issues that could potentially arise in production. Where a similar configuration to the production environment is not possible, a mitigation plan should be on place to minimize risks. One option is to use mock webhooks or mock systems. Another option is to release small batches of changes to a limited number of users to closely monitor their impact in production. In addition to the foundational testing phases already discussed, which are performed regularly during the development lifecycle, there are additional testing strategies that can be leveraged regularly or ad hoc. This assures the agent is production ready at launch, performs as per service level agreement, and has no regression. Typically we see these types of testing managed outside of Dialogflow CX in custom tooling. Design testing ensures that the correct intents are being invoked with the intended utterances in specific scenarios to end users, providing feedback on the UI experience. These types of tests are generally executed by the design team in the Dev environment. The design teams consists of the conversational architects that define the conversational taxonomy and implement the agent flows within Dialogflow CX. Performance testing, or telco infrastructure test, is normally performed by a team of architects, telco engineers, and telephony engineers in QA or pre-prod. They assess the performance of the virtual agent and other infrastructure components under varying loads. This includes testing response times, accuracy under load, and the system's ability to handle multiple simultaneous interactions. Regression testing assures that the existing features still function appropriately after new features are added or updated. As previously mentioned, the Quality assurance team is responsible to assess the absence of regressions, while performing end to end or UAT testing in QA and pre-prod environments. Adversarial testing is another type of testing performed when assessing the end to end performance of the system by the QA team in the QA or pre-prod environment. It deliberately tries to cause the agent to fail in order to identify new vulnerabilities or areas of improvement that need to be implemented to prevent escalations. Failover and recovery testing assesses how the agent handles and recovers from system failures, as well as its ability to maintain data integrity during such events. It can be performed by the QA team, when needed, in the QA and Pre-prod environment. Lastly, Beta testing can be performed prior to a full launch. This allows the agent to be tested in its full implementation without risking exposure of defects to a wide audience. Once all testing activities are complete, it is important to assess the results both at the development and QA level. Dialogflow CX has a test case interface that can be leveraged to assess the testing results at a high level. If you require more customization and dashboarding capabilities, you have the option to develop your own test running scripts and visuals. As the last step in the creation of your testing strategy, it is important to define a tracking system for your testing results. This will allow you to keep a regular pulse on the robustness of your plan. Let’s evaluate each phase of this process. This process starts with granular record keeping which includes the maintenance of a log that categorizes test case results for each release, including pass/fail status, associated defects, and any relevant observations. Keeping track of testing results helps in the analysis identifying trends, patterns, and recurring issues that inform root cause analysis and improvement strategies. Test results can inform a feedback loop process from QA to the development team that iteratively improves the agent. Results can also be compiled for knowledge sharing in root cause resolution forums to help limit future occurrences via preventative measures. Having a robust testing strategy creates a virtuous cycle in your development lifecycle that promotes continuous improvement of the quality of the agent throughput.

### Quiz - [Create a Testing Strategy Quiz](https://www.cloudskillsboost.google/course_templates/1105/quizzes/492167)

## Implementing your testing strategy in Dialogflow CX

This module deep dives into the specifics of each level of testing required during Virtual Agent development (Unit testing, Integration testing, end to end testing and AB testing) 

### Video - [Unit testing](https://www.cloudskillsboost.google/course_templates/1105/video/492168)

* [YouTube: Unit testing](https://www.youtube.com/watch?v=T3uVoglSnnU)

In the following sections, we are going to explore each layer of the Dialogflow CX testing strategy in detail. We will begin with unit testing, followed by integration system testing, End to End testing and A/B testing. Let’s first explore how to implement a unit testing strategy in Dialogflow CX. As previously introduced, Unit tests can be broken down into two main categories: Route testing and NLU testing. Route testing is related to ensuring that various types of page transitions, conditional logic based transitions and intent recognition transitions, are functioning as expected. NLU testing is the process of making sure that various user utterances are invoking the expected intents to trigger the appropriate agent responses and transitions. Let’s explore both types of tests, starting with routing. Route testing is also known as Partial Page Testing because it helps targeting a specific issue that is causing the agent to fail for a certain use case. This testing is typically limited to individual pages of a virtual agent in order to isolate and identify specific issues that are causing the agent to behave unexpectedly at that stage of the conversational flow. Examples of issues that can be discovered during route testing may be a parameter that is typed incorrectly or the user’s input prevents the page from transitioning as expected. This type of testing provides a lightweight approach to identifying, resolving, and testing reproducible bugs. In order to reach full coverage, each route on a page requires an explicit test case. The simulator also has functionality to expedite your route testing needs. If you need to test a route transition on a specific page, you can just instruct the simulator to begin the test on that page instead of the start page. This alleviates the need to work through the entire flow just to reach the one area of the virtual agent that you are currently testing. The other type of unit testing is NLU testing. NLU testing differs by assessing the efficacy or user utterances getting tagged to the correct intent on each page. NLU tests can also be conducted within Dialogflow CX, however it is recommended to create custom scripting to test large batches of utterances. This helps to ensure accurate intent detection for potentially ambiguous utterances. NLU testing requires a wide variety of user utterances to be tested for their corresponding intent as per input page in Dialogflow CX.

### Video - [Unit test driven development](https://www.cloudskillsboost.google/course_templates/1105/video/492169)

* [YouTube: Unit test driven development](https://www.youtube.com/watch?v=vBFgg0qu3SA)

Now that we are aware of the importance that unit tests play in the development lifecycle, let’s unpack how to structure a test driven development strategy for Unit Tests. Test driven development is a core philosophy of the CCAI approach to virtual agent development. It promotes iteratively testing throughout the development lifecycle of a virtual agent. We will review this in the context of Unit tests, but these principles could be easily extended to other types of testing. This philosophy is also coherent with the CUJ “first” approach to testing introduced earlier in aiming to stress the importance to verify the quality of your agent in every step of the development process, and in a way that it aligns to the needs of the end users. Dialogflow CX supports this philosophy by offering built-in test features to uncover bugs and prevent regressions from the design stage to post production CI/CD. The simulator can be used to test updates to the virtual agent as well as define golden test cases to run as an eval set when needed. The experiments feature can also be leveraged to deploy multiple versions of the virtual agent against live traffic. The first step of the process is to understand the requirements and success criteria for the virtual agent you are testing. Before you start writing tests, clearly define what you expect your virtual agent to do. This includes understanding the different types of user inputs or intents, and the expected responses or actions from the agent. Once you have defined your requirements, you can begin writing your test cases. It is best practice to start writing test cases before developing the actual features. For a virtual agent, these test cases can include different scenarios of user-agent interactions, how it handles context switching, or how it manages fallbacks. Next you can begin implementing the agent’s features. This involves defining intents, training phrases, parameters, responses, and fulfillment logic in Dialogflow CX. Step 4 is where you begin running the tests. The standard approach is to use the native menus in Dialogflow CX but you can also use automated scripts for running test cases via the API. Next is refactor and repeat. If tests fail, modify your agent's configuration and logic until all tests pass. Then, refactor your code to improve efficiency, readability, and maintainability. After refactoring, run the tests again to ensure nothing broke during the process. Repeat this cycle as you continue to develop new features. After you have iterated on the virtual agent development and the test cases are passing, you can move on to continuous Integration. This will allow you to automatically run tests whenever changes are made to the virtual agent, ensuring consistent quality and functionality. As a final step, it's crucial to establish a strong monitoring and feedback system. This system should continuously track the agent's performance and gather user feedback and conversation logs. This data will pinpoint areas where the agent can be improved. Use this information to write new tests that target those areas of improvement. These tests should be integrated into a test-driven approach for ongoing development. Having a feedback loop process in place, allows you to instate a virtuous cycle in your development process that feeds itself and iteratively improves the performance of your agent.

### Video - [Integration system testing](https://www.cloudskillsboost.google/course_templates/1105/video/492170)

* [YouTube: Integration system testing](https://www.youtube.com/watch?v=21pKcZqp9Tk)

Next, we will cover the Integration system testing. As a recap, the objective of this type of test is to ensure that your virtual agent works seamlessly with external systems it connects to. The most common type of integration to consider is how Dialogflow CX connects with the end user. The platform offers many options for telephony and text based integrations that can be accessed natively For voice conversations specifically, we will discuss dual tone multi-frequency, also referred to as DTMF, and barge-in. webhooks are the most common integration test both in the voice and the chat channel, DTMF refers to the ability for the user to provide numerical input using the keypad or their device as an alternative to speaking the number to the agent. This can be useful in situations where there is background noise or collecting a long sequential string of numbers like an account number. In order to properly test that your DTMF is working, call the agent to verify the input recognition via the telephony integration that is being utilized for the project. You can design the agent response to the input collection to repeat what has been collected by referencing the DTMF entity. This not only helps with testing, but is a good practice to confirm to the end user that the virtual agent picked up is the correct information. This will also help avoid needing to review the speech to text transcripts to ensure the correct information is being collected. Prior to designing the test cases, review two key best practices for your DTMF implementation: Confirm that numbers such as “1” or “2” are not used as synonyms for the DTMF entity. This may cause conflicts if the page also collects other numbers. Use the name of the special characters such as “pound” or “star” in the entity to avoid erratic behavior for . For example: dtmf_digits_pound Pronunciations: # = pound * = star Another unique voice consideration is when a user barges-in with their request while the virtual agent is still speaking. Barge-in enables end-users to interrupt an agent, allowing the them to move along the flow more quickly if desired. Barge-in can be enabled on Agent level settings, flow level settings, and page level settings. Barge in can be disabled on specific pages where there may be a need or legal require for the user to listen to the entire agent response without skipping. Since Barge-in is a telephony feature, you will need to call the agent through the telephony integration that has been implemented for the project in order to properly test its functionality. You will want to ensure that you have test coverage to test the areas of the agent where you want the barge-in option to be active, as well as testing the pages where you want to make sure that the user cannot skip the agent dialogue. Finally, we want to ensure that our webhook calls are prepared to handle failures as part of the Webhook testing integration testing. To test these failures, intentional failure tests must be created. We can create failure tests by using the test console’s parameter injection feature. This allow us to set parameter values to mimic various webhook failures at the beginning of the test case. When the webhook is invoked, the webhook service will read these specific parameters and trigger the expected failure. At the HTTP level, the webhook service could return any of the error statuses detailed in the table on the slide. Each error status code will trigger a different event in Dialogflow CX. Your test cases should cover each of these different possible events. Remember that the goal of webhook testing is to ensure that every type of failure is handled in your webhook fulfillment code. Test cases will need to be created for all failure types handled within your code. Webhook failures can be simulated by setting session parameters to provoke them. Parameters can be set at the beginning of the test case through injected parameters, which allows the webhook service to read these specific parameters and then trigger the expected failure.

### Video - [End to end (E2E) testing](https://www.cloudskillsboost.google/course_templates/1105/video/492171)

* [YouTube: End to end (E2E) testing](https://www.youtube.com/watch?v=ljj_-dihxhI)

After unit and integration testing, we can now move to exploring end to end testing. Depending on the stage of bot development lifecycle you are in, end to end testing may have different connotations with respect to their scope and purpose. The three main types of end to end testing in order of complexity are: Modular testing. This subset of end to end testing focuses on an individual flow or component that is a subset of the entire customer journey. Focused Journey testing is concerned with testing an entire customer journey utilizing simulated inputs. Full-Stack testing encompasses all layers of the technology stack involved in the customer journey including integration with backend fulfillment systems. Let’s select and focus our attention on Modular testing. As the name suggests, it is larger in scope than page testing as it encompasses an entire Flow from end to end. It’s purpose is to validate the logic and performance of individual flows. This makes modular testing ideal for testing specific features or updates in isolation from the rest of the agent. To perform modular testing, you need to go through a flow from its start to its end, including the handling of intents, fulfillment, and transitions within the flow to test it’s holistic logic and functionality. Modular testing refers to a subset of end to end testing that limits the scope to a specific flow. This is useful when just a portion of the agent has been updated. It is important to consider that modular testing can miss issues related to context management when transitioning to other flows in the agent. The purpose of the focused journey testing is to provide early feedback on specific journeys and isolate problems within workflows for targeted debugging. These tests center on a single feature, critical process, or limited set of interactions within a user journey. In order to perform Focused Journey testing, you need to: First, select a user journey. Then, develop the test cases. Next, execute the tests with controlled inputs. And finally, analyze the results and report the findings for improvement. Focused journey testing is useful when there is a need for end to end testing a specific customer user journeys within a virtual agent in isolation from other system issues. This can save time over full stack testing when you want to test a specific CUJ that has recently been updated without taking the entire scope of the agent into account. This form of end to end testing is most accurate when the simulation for mock inputs are representative of real-world performance and values. Please consider that there is more maintenance to these tests because simulators and mocks need to be kept up to date as the system or its dependencies evolve. Full stack testing encompasses all of the principles of Modular and Focused Journey testing, however it is applied to all of the flows and customer user journeys within the scope of the agent design. In addition, it also encompasses the practices outlined in the integration system testing section. This full agent end to end testing module is the last step before the agent can be handed over for user acceptance testing on the customer end and ultimately production launch. The process for implementing a full stack testing strategy includes combining all of the test cases for all CUJ’s handled by the agent. Once full test coverage for the virtual agent is reached the next step is to add testing to ensure the function of all integrated systems such as the telephony integration and chat user interface. Full stack testing is the most comprehensive stage of testing for the development team. The entire agent design from intent detection to client side system integration is in scope for this phase of end to end testing. Once this stage is complete, the agent is ready for UAT and production launch upon customer sign off. It is important to note that this phase requires the most diverse array of skill amongst the development and testing team. Careful planning is required to cover all areas of the agent, coordinate testing efforts with backend system availability, and stay within the delivery timeframe. When testing the performance of your agent End to End, it is important to assess its ability to gracefully handle unexpected situations or errors. These are commonly referred to Fallback scenarios. The most common being: no-matches, no-inputs and webhook failures. No-match' scenarios occurs when a user's input does not match any of the defined intents in your agent. It's crucial to have a strategy for these circumstances. Your virtual agent should be able to acknowledge the misunderstanding and either ask for clarification or offer alternative options. 'No-inputs' happens when there is no user input, perhaps due to user distraction or confusion. Your virtual agent should be designed to handle these silent moments appropriately, either by prompting the user again or by providing additional guidance or information. Finally, there’s 'Webhook failures.' These occur when a call to an external webhook results in an error or timeout. It's important for your virtual agent to have a contingency plan in these circumstances. Instead of leaving the user in the dark, your virtual agent should inform the user of the issue in a user-friendly manner and offer alternatives, if possible. Fallback scenarios are key to creating a resilient and user-friendly Dialogflow agent. By preparing for these scenarios, you ensure that your agent provides a helpful and consistent experience, even when faced with the unexpected. Let’s explore a couple of examples on how to test for “no-matches” scenarios. Begin by creating scenarios where user inputs are intentionally out of the scope of your virtual agent capabilities. Think of unusual or unrelated queries that a user might input. This step is vital in assessing how your virtual agent deals with unexpected or irrelevant user inputs. Next is to define what will be your virtual agent response. Should it ask for clarification, direct the user to a help menu, or perhaps redirect them to a human operator? The goal is to determine the most helpful and natural response for these situations, so make sure you account for this in your testing journey. Methods for testing no-input scenarios differ when testing by voice over the telephony integration, or through the console via chat. When testing no input for telephony, you may simply not reply at various stages of the conversation where you want to test the no-input fulfillment. For chat testing through the console you can simulate user silence or non-response situations, like no audio by using a space character as input. Observe how your agent handles these cases, ensuring it effectively re-prompts the user or proceeds with predefined default actions for a smooth conversation flow. In our last fallback scenario, webhook failures occur when a backend service is unavailable or incurs a timeout. We want to ensure the virtual agent is able to handle these scenarios in a user-friendly manner. The default contingency plan may be to escalate to a live agent. Depending on the objective of the webhook called, you may develop alternative methods to attempt handling the user’s request. Now that you know what End to End testing entails, keep in mind these guidelines for creating and maintaining End to End test cases. When introducing a new page, test all existing routes and transitions within the flow to ensure they still function correctly on the new page. Always include test cases that specifically cover "No-match" and "No-input" scenarios to verify how the system handles unexpected situations. For NLU updates, such as intents, craft 10 to 15 test cases with variations in how the updated intent is triggered. And lastly, if a new page has several access points, create test cases covering each path to ensure all routes to the page work flawlessly. These best practices will fortify your strategy to preserve the end to end conversational experience of your agent.

### Video - [A/B testing (Experiments)](https://www.cloudskillsboost.google/course_templates/1105/video/492172)

* [YouTube: A/B testing (Experiments)](https://www.youtube.com/watch?v=U-gOSE0eUJ0)

The last type of testing is A/B testing, commonly referred to as Experiments in Dialogflow CX. As previously mentioned, AB testing is a UX methodology that allows to compare the performance of two or more systems or conversational experiences to verify which versions better meets the functional requirements outlined in the project scope or specifications. A/B testing can be accomplished in Dialogflow CX by using the built-in experiments feature. To the greatest extent possible, development and testing should be done in an environment similar to the production environment, commonly referred to as pre-prod. Experiments allow the development team to allocate a portion of live traffic to different flow versions. The most common metrics used to monitor the performance of these experiments in production are: Live agent handoff rate is the count of sessions handed off to a live agent. Session end-rate is the count of sessions that reached END underscore SESSION. Total no-match count is the total count of occurrences of a no-match event. Total turn count is the total number of conversational turns. One end-user input and one agent response is considered a turn. Average turn count involves the average number of turns. Contained is the count of sessions that reached END underscore SESSION without triggering other metrics below. Callback rate is the count of sessions that were restarted by an end-user. Abandoned rate is the count of sessions that were abandoned by an end-user. These metrics can help you inform which experiment are best for your customer and we encourage integrating experiment data into your reporting. Here is an example of how these metrics show up in the Dialogflow CX console. Green colored results suggest a favorable outcome, while red suggests a less favorable result. Notice that in some cases, higher or lower numbers are not necessarily better, the high abandonment rate or low abandonment rate. When you have a new version of an agent, or flow to test, begin by exposing it to a small percentage of the user traffic. If initial results are positive, you can gradually increase the traffic in order to provide a more statistically significant evaluation set. This set will either guides your decision to move forward with the new version, or revert to the existing agent. Also, if you have a need to expedite the testing process, Dialogflow‚Äôs Experiments feature allows us to run multiple experiments at the same time. A/B testing using the Experiments feature enabling designers to test out new ideas about what and how a virtual agents should handle specific circumstances or flows which may challenge conventional wisdom. Instead of relying on intuition, this feature can be greatly helpful to test out new theories and capture quantitative metrics to inform decision making. If initial experiments continue to show positive results as you increase the sample size of the user base exposed to the test version, increase the traffic percentage until you reach a 50/50 split. This will allow you to do an accurate assessment of the relative performance of the two versions. Once these metrics are collected in a controlled setting and their traffic percentages are equal, you can compare two pages or two flows to determine the degree to which the versions differ. AB tests are a great way to foster a data-driven culture that can substantially help expedite the release process and ensure the best conversational experience for the user. For more information about experiments in Dialogflow, please refer to the resource in the Additional resources document.

### Quiz - [Implementing your testing strategy in DFCX Quiz](https://www.cloudskillsboost.google/course_templates/1105/quizzes/492173)

## Understanding Dialogflow CX's testing features

This module explores how to best use the Dialogflow CX's testing feature set (simulator and validation tool) to support each level of testing during Virtual Agent development and how to analyze and increase test coverage.  

### Video - [Reviewing Conversational Agents development](https://www.cloudskillsboost.google/course_templates/1105/video/492174)

* [YouTube: Reviewing Conversational Agents development](https://www.youtube.com/watch?v=JJ9MMM78Iug)

Now that we have a good understanding of each layer of testing, let’s explore how to best use the Dialogflow CX’s testing feature set to support them. In this section we will be covering: Key tenets of Dialogflow CX development, Exploratory testing while developing, How to test an virtual agent using the simulator, How to create a test case, and The importance of complete test coverage. To begin, let’s learn how to review existing flows to inform test case development. Our original CUJ was focused on allowing a customer to check their account balance. We have developed a simple flow illustrating each of the components required to accomplish that goal. With the first component we need to ensure the user’s intent is captured correctly by the virtual agent. It begins on the “Ask why user is calling” page. Identifying the user’s intent for contacting support is the first and most important aspect of handling a customer service request. As soon as the user intent is identified, we can route them to the appropriate flow for their desired use case. The second component requires checking if the user needs to be authenticated to see their account information. Once the user has been authenticated, we can provide them with the information that was requested at the beginning of the conversation, which in this example is “Confirm Balance”. Now that we have identified the three major tasks of the flow, let's move on to the test case mappings for each component. Here is an outline of the various testing methods that are needed for each component of the flow in the example. For the intent recognition component you would want to design NLU unit tests to test the number of utterances related to asking for a balance. For the authorization component, you should confirm that the flow routes appropriately back to the main balance inquiry flow after the webhook call returns a positive response for the authentication. Alternatively, you need to ensure that the customer is prompted to input their credentials if the authentication is not recognized in the system. Lastly, to test the balance confirmation component, you need to ensure that the agent response has the appropriate values for the authenticated user and that the response is being rendered correctly.

### Video - [Exploratory testing while developing](https://www.cloudskillsboost.google/course_templates/1105/video/492175)

* [YouTube: Exploratory testing while developing](https://www.youtube.com/watch?v=ZKk4qx3Y1xA)

Now, let’s address Exploratory testing using the simulator and Dialogflow messenger. Unlike structured tests, exploratory testing encourages you to actively engage with the agent in a “free form”, even adversarially, with the objective to pushing boundaries to unearth unexpected behaviors and potential bugs. Exploratory testing, is a great way for the developer and/or the tester to validate the robustness of the conversational experience and its ability to withstand real life scenarios. Utilizing the Dialogflow CX Simulator, you can perform exploratory testing to simulate real conversations by experimenting with various inputs to evaluate the virtual agent responses. It's an opportunity to be creative and to test scenarios that scripted testing might miss. The Dialogflow CX Simulator can help you mimic these front-end interactions, to do so: First, emulate the front end client using initialization parameters, events, voice, etc. Just keep in mind some possible constraints, for example, the lack of graphic elements rendering. Second, Monitor session state for parameter and context changes. Next, Test webhook functionalities and external data integrations. Lastly step back in the same conversation to check different behaviors and detect issues. Another option is to Test the performance of your agent through Dialogflow Messenger to allow for realistic interactions with the virtual agent through a web interface. Additional functionality that messenger enables includes: Testing across devices or browsers, Checking for consistent agent responses across platforms, Reviewing the rendering of visual elements, and Interacting with the GUI elements. Let's dive into how to setup the simulator in Dialogflow CX. First, select your agent. Then, on the right side of the console press the Test Agent button to open the simulator. Here, select an environment or flow version, and then an active flow and the page. This will allow the draft environment and default start flow to be used for initial testing. Use the settings icon marked with red circle, above the simulator, to configure settings for your testing session. Three options will be at your disposal: Webhook, Partial Response and Sentiment Analysis. Selecting the Webhook Toggle enables or disables webhook calls. This is useful to test how your agent behaves with and without external service integrations. Note that this will not trigger webhook error events. Partial responses enables you to test how your agent handles incomplete user inputs. Finally, turning Sentiment analysis on allows you to analyze the sentiment of user inputs. Now that you have access to the simulator, how can you best interact with it? Start by providing end-user input in the form of text. This means simulating what a real user might input during a conversation. After typing your input, you can either press 'Enter' or click the 'Send' button to submit it to the agent. Text input is just one type of end user input. The Dialogflow CX's simulator also allows the injection of: Parameter values which is useful for testing how your agent handles different contexts or states. You can either introduce new parameter values or use preset values for existing parameters. You can also test Custom events within your agent. Testing with events is key to understanding how your agent reacts to specific triggers or scenarios that are not solely based on user text input. And lastly DTMF, which is essential for testing telephony interactions. DTMF inputs simulate the key presses on a phone keypad, which is a common interaction in voice-based systems. This feature is crucial for ensuring that your agent can handle telephony interactions accurately. The flexibility the Dialogflow CX's simulator offers while testing is crucial to stress test different interaction scenarios. Using the simulator allows the developer to observe and monitor the Session State. This provides valuable information for debugging as you test and build your virtual agent. This includes: Which active flow and page the turn is on, Monitoring of parameter values during the conversation, and The detailed execution steps which are taken by the virtual agent during the conversation. Let’s address each individually. The page section shows the active flow and page name at the current stage of the test or turn you are presently on. The parameter section shows which parameters have been collected and their current values. Lastly, the execution steps show the steps made up to the current point in the test case. By clicking on the clipboard icon you can see the full detail. Alternatively, the dropdown menu will take you to the page component where this step is defined and executed. While the Dialogflow CX simulator is a powerful tool for testing and interacting with your virtual agents, there are some considerations that are important to keep in mind. First is the ability to set parameter values to null. In certain scenarios, it might be necessary to reset or clear a parameter, but currently, Dialogflow CX does not support directly setting a parameter's value to null. Second is multilingual switching. This means that if you want to create a virtual agent that supports multiple languages, you will only be able to test one language at a time. Lastly, session entities entries. Dialogflow CX currently does not support session entities entries which are a powerful tool in customizing and extending the capabilities of system entities per session. . As an additional consideration for using the simulator to conduct voice testing, is that the simulator’s speech translation performance can vary based on accent, speech clarity, and background noise.

### Video - [Creating a test case in Simulator](https://www.cloudskillsboost.google/course_templates/1105/video/492176)

* [YouTube: Creating a test case in Simulator](https://www.youtube.com/watch?v=bPqh2bf_8PQ)

Now let’s go through the steps and best practices for creating a test case in the simulator. Creating a test case in the Dialogflow CX simulator is a three-step process. First, create what are generally referred to as “golden test cases” which are examples of benchmark conversations between users and the virtual agent. These are considered ideal conversations because they match with the training phrases and conversations defined in the design phase. Next, click on the “Save as test case” button to save the scenario. Be sure to include the details you want associated with the test case. Specifically: Enter a unique display name for the test case. Optionally, provide the tag names to help organize your test cases. Make sure that you have a catalog of approved tags to limit variability (example hashtag-pod-name-underscore-flow). Remember that all tags must start with a hashtag. Optionally, provide a note describing the purpose of the test case. Then, select parameters you want to track in the test case. A list of suggested parameters will be provided, but you can also input other parameters to track. When you select tracking parameters, the parameter assertion is checked during the test case run. Once your test case is saved, let’s review where to find it. In Dialogflow CX Console, go to the "Manage" tab and select "Test Cases." This page will display all the test cases you've previously created. You can search for a specific test case by its name or filter test cases using tags in case they were assigned. Once you've located the test case you want to replay, there is a play button on the right hand side that allows you to run it in the simulator. The simulator will execute the test case step by step, showing each user utterance and the agent's response, as per the saved test case. The Dialogflow CX simulator also offers the flexibility to make real-time updates to our test cases. This feature is incredibly useful when you spot issues or see opportunities for enhancements during a test case replay. Another feature in the simulator is the 'Undo Last Conversation Turn' button. This feature is particularly helpful when you want to revert the last step of your conversation. With this button, you can step back, make necessary adjustments, and retest the scenario. After making the necessary changes, you can save the updated version of the test case. Click the "Save" button in the simulator, which might prompt you to enter a new name for the modified test case or overwrite the existing one. Optionally, update the tags and notes for the test case to reflect the changes made. In the test cases table, you can view the test name, tags, the latest test time and environment, as well as the latest test result. Use the tags associated with each test case to filter them. Tags can help you organize and group test cases based on different criteria like test scenarios, functionalities, or other categories, so make sure to define a naming convention in your program, to create a deterministic catalog of approved tags to limit variability. A common way to define a tag is to specify, after the hashtag, the Pod Name and the flow or the page the test belongs to. After filtering and selecting the desired test cases, click "Run" to execute them. Alternatively, you can click "Run all test cases" to execute all the test cases at once. Just remember to select the environment against which you want to run these test cases. You can monitor the status in the task queue after running the test cases. The test result will be updated upon completion. You can click on a specific test case to view detailed results, including comparisons of the golden test case and the latest run conversations. During the test run, the test engine checks virtual agent dialogue, matched intent, current page, and session parameters turn by turn. Any discrepancies or failures will be highlighted, and you can review these to identify potential issues in your virtual agent configuration.

### Video - [Understanding and utilizing the validation tool](https://www.cloudskillsboost.google/course_templates/1105/video/492177)

* [YouTube: Understanding and utilizing the validation tool](https://www.youtube.com/watch?v=GT6E5-po8CI)

Now let’s learn how to use Dialogflow CX’s built-in validation tool. To help virtual agent designers create high-quality virtual agents, Dialogflow CX provides a validation feature. Virtual agent validation is performed on-demand. You can request agent validation from either the Dialogflow Console or the API when you have completed editing your virtual agent and retrained the NLU models. You can also query the latest validation results without performing a new validation. The validation results are informational only. They provide a list of classified validation messages that you can correct to improve the quality and performance of your virtual agent. Virtual agent validation does not affect the behavior of a virtual agent in any way. You can ignore the validation messages and still launch your virtual agent. The validation messages cover the quality of the NLU model training data including the intents and entity types, and the quality of the page-based flow structure such as transition rules. Examples of specific validation messages can be seen in the example on the screen, where the tool alerts the use that the page cannot reach the end flow, end session or another flow. Validation messages can also be conveniently seen on the intents list, the entities list and the pages list. For example, in the pages list, an icon indicating the severity level is shown in the page row when there are validation messages for that page. A tooltip showing the detailed validation messages appears when hovering over the icon. The validation tool has three levels of severity for identified issues: Info severity is the informational category meant to warn users when the agent is not adhering to bot design best practices. Warning severities are given when the agent may not be behaving as expected. And error severity notifies the user about areas where the agent is experiencing failures.

### Video - [The importance of complete test coverage](https://www.cloudskillsboost.google/course_templates/1105/video/492178)

* [YouTube: The importance of complete test coverage](https://www.youtube.com/watch?v=5-EZ8mWOeTo)

Let’s review the rationale behind the criticality of having a complete test coverage during the development lifecycle of your agent. Good test coverage enables us to detect issues and bugs at an early stage. By rigorously testing different aspects of our virtual agent, we can identify and resolve problems before they impact users. This proactive approach saves time and resources in the long run. Comprehensive test coverage directly contributes to a quality user experience. It ensures that the agent behaves as expected in various scenarios, leading to more accurate and helpful interactions with users. Each test case can validate specific functionalities of your virtual agent. Good test coverage means that all features, from basic to complex are tested and confirmed to work correctly; ensuring the virtual agent’s functionality aligns with your design goals. As you update or enhance your virtual agent, there's a risk of new changes impacting existing functionalities. Good test coverage helps prevent these regressions, ensuring that updates do not inadvertently break what was already working. As seen earlier in the training, in Dialogflow CX, the conversation is managed through flows so comprehensive testing can confirm that their logic is accurate. It also ensures that the conversation progresses smoothly and logically based on user inputs. Good test coverage includes testing for edge cases as well – unusual or rare scenarios that may not be immediately obvious. Identifying and handling these cases ensures that the virtual agent is robust and can handle a wide range of user interactions. Finally, thorough testing helps improve the accuracy of our Natural Language Processing, or NLP, model. By testing with diverse phrases, we can train our model to understand and respond to user inputs more effectively. As developers and testers, when you introduce new features to your Dialogflow CX agent, your top priority is to validate the new functionality and performance. This is where Test Cases come into play. Dialogflow CX allows you to create, modify, and organize your Test Cases, forming a comprehensive suite that covers various aspects of your virtual agent interactions. But how do we measure the effectiveness of your test cases once built? This is where the Test Coverage functionality comes into the picture. Test Coverage in Dialogflow CX is a functionality that helps you understand the extent to which your test cases validate the functionality of your agent. It's a measure of how thoroughly your test cases assess the transitions, intents and route groups aspects of the virtual agent behavior and responses. By analyzing your Test Coverage, you can identify gaps in your testing process and add new test cases where necessary. This ensures that your virtual agent is rigorously tested and ready to handle real-world interactions effectively. To increase the test coverage just add new test cases or modify existing ones to ensure that all transitions, intents, and route groups are thoroughly tested. Make sure to regularly review and update test cases to align with any changes or updates to your virtual agent design.

### Quiz - [Understanding Dialogflow CX testing features Quiz](https://www.cloudskillsboost.google/course_templates/1105/quizzes/492179)

## Create a test plan

This module explores the key steps required to create an end-to-end testing plan and the unique considerations required when testing the performance of a Virtual Agent cross-channel (Chat vs Voice)

### Video - [Create a test plan](https://www.cloudskillsboost.google/course_templates/1105/video/492180)

* [YouTube: Create a test plan](https://www.youtube.com/watch?v=He1ZZMc0xec)

Now that you are aware of the different types of testing, and the key functionalities within Dialogflow CX that support them, let’s learn how to create a test plan for your virtual agent. This section aims to provide you with some tactical guidance to create an end-to-end virtual agent testing plan, as well as unique voice and chat capabilities for additional testing considerations. Let’s dive in! There are 8 main steps for the creation of a robust end-to-end agent testing plan: First begin with the creation of a test Scope, which defines which aspects of the Virtual Agent will be tested such as response accuracy, user redirection, or fallback handling. Next you can start creating test cases covering all possible user interactions, including edge cases. Remember to use your CUJs to inform which test cases to create. The third step is the setup of Tools and Environments to ensure that your lower tier environments are as close to production as possible. When creating an Execution Plan, this defines the schedule for executing test cases and recording results. Once you have an execution plan you can then define Performance Metrics, like response time, accuracy, and user satisfaction to measure the effectiveness of the conversational flow. Then, collecting data allows you to identify how the virtual agent handles queries, redirection, and fallback scenarios. The next major step is Analysis and Reporting, which can be divided into three sub-steps: Step 1 is Analyze the data, identify areas of improvement, and compile a report. Step 2 involves Feedback Implementation, and Step 3 implements changes based on test results and retests if it’s necessary. Lastly, a final review is required to ensure all objectives are met prior to moving to the next environment.

### Video - [Exploratory testing while developing](https://www.cloudskillsboost.google/course_templates/1105/video/492181)

* [YouTube: Exploratory testing while developing](https://www.youtube.com/watch?v=-yxSZmUdb9I)

Let’s now investigate the specificities in testing for voice agents in Dialogflow CX. First, when you encounter issues in transcriptions from Speech-to-Text, or STT, first check the alignment between the transcription and the original transcript. Sometimes utterances can be mistranscribed. When this occurs, try to reproduce the issue. If you can’t reproduce the issue, because sometimes mistranscribed utterances don’t create issues for intent matching, then it is not an STT issue. When you suspect your agent may have issues related to NLU detection, the test console can be leveraged to test the utterance that is causing the agent to fail. Prior to testing, you will want to confirm that the webhooks are disabled to ensure that any potential webhook errors are not interfering with the desired intents. If the utterance invokes the expected intent, then the issue may be found in the transition logic. If the wrong intent is invoked, you know that you will have to update the training phrases between the invoked intent and the expected intent until there is no confusion between the competing intents. Often, the configuration for the specific Dialogflow CX environment may be the cause of certain telephony issues. You should ensure that the configuration of the affected environment is identical to the environments that are not producing the issue before continuing debugging efforts. Some metrics are commonly used to assess the performance of Voice agents. It’s helpful to keep the following metrics in mind during testing: First is Misroute, which is the count of callers whole ended up in the wrong place. First call resolution is the number of calls that are resolved on the first call or contact. Average handling time measures the average length of the call before a resolution is reached. Customer satisfaction measures the voice agent’s customer satisfaction (or CSAT) score. Number of turns measures how many exchanges it takes to accomplish the user's task. And lastly User churn is the rate at which users disengage from the conversation. In other words, ending the session before a resolution is reached.

### Video - [Creating a test case in Simulator](https://www.cloudskillsboost.google/course_templates/1105/video/492182)

* [YouTube: Creating a test case in Simulator](https://www.youtube.com/watch?v=tMqSBCo7ONM)

Let’s now investigate the nuances with regard to testing chat agents in Dialogflow CX. The first consideration when addressing chat agent textual nuances is to try and test understanding variations in phrasing, slang, misspellings, and abbreviations that users might naturally employ. If the virtual agent is integrated with a visual UI, make sure to test the coordination between text responses, buttons, images, or other visual elements. Unlike voice, chat often allows for delays between user messages and agent responses. Asynchronous potential, tests how the agent handles timeouts and resumes conversations gracefully.

### Quiz - [Create a test plan Quiz](https://www.cloudskillsboost.google/course_templates/1105/quizzes/492183)

## Deployment Lifecycle

This module explores what the deployment lifecycle of a virtual agent looks like and how to choose the best flow versions and environments for your development needs

### Video - [Using and applying versions to your flows](https://www.cloudskillsboost.google/course_templates/1105/video/492184)

* [YouTube: Using and applying versions to your flows](https://www.youtube.com/watch?v=BWxxkxIu44A)

The next section deals with what the deployment lifecycle of a virtual agent looks like. The objectives of this section is to: Unpack versions in your deployment strategy that guarantee high-quality bot deliveries and successful deployments. Explore how to utilize environments in your deployment strategy. Discuss how to choose the best deployment for you team. Investigate how to move virtual agents and test cases in different environments. And how production experimentation can be used in production to allow sectioning of traffic. Let‚Äôs begin by discussing how and when to use the Versioning feature for flows. What is a flow version? When you edit a flow, you are always editing a draft. At any point, you can save that draft flow as a flow version. This way, multiple versions can be created for a single flow. Why do we use them? To create an immutable snapshot of your flow data and the associated agent data like intents, entities, webhooks, pages, route groups, etc. This allows you to test updates to existing flows without editing them. Before you create a new version: Always establish with the team a naming convention since the name should reflect the changes made, features additions or, like in the example, it can reflect the version number and the date. Add a description of the changes made like what was added, what was removed and if needed, what was not included in this version. This may be new features, fixes to specific issues or bugs, or updates. After a new version is created by clicking the "Save" button, the version will appear on the list with the others versions and the status will be "In Progress" while it's being created. Status will change to "Ready" when the version is completely created, the NLU model retrained and available for testing or to be used in different environments. To work on an specific flow version on the Draft environment you can load it by following the next few steps: First, hover over a version row. Then, click the three dots on the right side of the row. Next, click the Load to draft option. You'll have the choice to replace agent-level data when loading. If you choose to overwrite it, any updates made after the version you're loading will be erased. If you want to keep them, simply don't select that option. To test different versions in the test simulator, check the option "Test agent with specific flow versions" and choose the version of the selected flow you want to test. Even when multiple users are working on the same flow each of them can test different flow versions in isolation without impacting each other. This feature is very important to enable cross team collaboration. The versions features unlock the following functionalities: Listing the current environments for the agent. Creating new environments. The selection of flow versions and environment configuration. Viewing of the edit history of an environment. Copying an environment resource name, including the environment id. Deleting an environment. Specifying environments for session calls. And finally it allows the tester to specify flow versions for session calls. Regarding best practices for versions, begin by always using flow versions for production traffic. A draft flow is updated every time a developer makes a change. Remember that it is very easy to break a draft flow unintentionally, especially if multiple people have editing access to the agent. Also, a recently edited draft flow may be inconsistent with the trained model, because training may have a delay or require manual execution. Draft flows should be tested before promoting them to production versions, and you can roll-back to a previous version if any problems are discovered. Next, use flow versions to save progress. Once the development of a new feature or the fix of a bug is completed, tested and working as intended, create a version to save your progress. Then, you can continue experimenting or developing without the risk of breaking what you have already fixed. If you require multiple developers working on the same flow, at the same time, ensure that you all coordinate when creating a new version. That way, everyone is aware of which features and changes are fully developed in that version and which ones are halfway. And finally, consider possible agent errors while changing versions. If you change a flow version in your production environment while sessions are active, it may cause agent errors for some active sessions. These errors may happen if the previous and new versions are inconsistent with each other in a way that disrupts the session state. Due to this, you should plan to change versions during down-time or off-peak hours. Note that Dialogflow CX just allows you to store up to 20 versions per flow. If you need to create more than that for your flow you will first need to delete the oldest version, to create a new one.

### Video - [Using environments to version your entire Conversational Agent](https://www.cloudskillsboost.google/course_templates/1105/video/492185)

* [YouTube: Using environments to version your entire Conversational Agent](https://www.youtube.com/watch?v=i-YpGSYgpDY)

Next, let’s explore how to use "environments" to version your entire virtual agent. Let’s start with where the Environments feature is located in your Dialogflow CX agent console. Navigate to the Manage tab on the left side panel and find Environments under the Testing and Deployment section. There you can view the existing environments and their current version. You can also create new environments from this view by clicking on the Create button. When creating a new environment you will need to select flow versions and provide a name and description for it. To view editing history of an environment, you need to click the Lookup environment history button near the right side of an environment in the list. To copy an environment resource name that includes the environment ID, you should click the Copy environment name to clipboard button near the right side of an environment in the list. And lastly to delete an environment, you should click the remove environment button near the right side of an environment in the list. Base the name of the environment on its purpose, such as “development”, “testing”, or “production”. In that way testing or QA can run the test cases on flows that are not constantly changing or being updated because of development. Development can happen on another environment than testing to prevent breaking the logic where the test cases will run. You should also have a production environment, so that all your completed and tested flows’ versions remain in order to be released without being affected by development, testing or QA changes. Once your environments are created, you can start testing in the test simulator by checking the "Test agent environment" option. There are three different dropdowns from which to select the environment you want to test on, as well as the flow and the page. Make sure the intended flow version is set on that environment.isindeed The key functionalities unlocked with the environments feature, include: Listing the current versions of a flow. , Providing information about each flow. , Creating new flow versions. , Loading specific flow versions as the draft flow. , and Deleting specific flow versions. Remember to use the compare versions tool to view a side-by-side comparison between flow versions or the draft version.

### Video - [Choosing the best deployment for your team](https://www.cloudskillsboost.google/course_templates/1105/video/492186)

* [YouTube: Choosing the best deployment for your team](https://www.youtube.com/watch?v=zWwDXUfcZ5s)

Now let’s investigate how to choose the best deployment for your team. Consider these guidelines for assessing the best deployment model for a development team: Begin by assessing your agent's flow complexity. Simple flows usually have only one purpose whereas more complex flows might have multiple purposes. A more complex flow is more likely to have multiple developers working in parallel, making it harder to isolate changes. Factor in the size of the team. Smaller teams might prefer simpler deployment methods, while larger teams can leverage custom solutions to allow for more independent development or custom CI/CD pipelines. And lastly evaluate the Google Cloud Platform, or GCP, project requirements. Complex GCP project requirements like user roles or division of development and production projects may require a custom solution. The suggested GCP setup consists of four projects that represent the various stages of development. The Lab project is where the development of the virtual agent occurs by the delivery team. The Dev project is where the customer can assess the quality of the agent. Non-prod is the project where integration work and customer User Acceptance testing or UAT occurs. And finally, prod is where the production version of the agent is served to live traffic.

### Quiz - [Deployment lifecycle Quiz](https://www.cloudskillsboost.google/course_templates/1105/quizzes/492187)

## Deploying CI/CD

This module explores some additional considerations for the set up of a continuous improvement and continuous development (CI/CD) pipeline for Virtual Agent development. It also provides details on how to best leverage the change history tool, define an incidence management strategy, and account for key metrics for managing capacity and quota. 

### Video - [Checking recent changes using the change history tool](https://www.cloudskillsboost.google/course_templates/1105/video/492188)

* [YouTube: Checking recent changes using the change history tool](https://www.youtube.com/watch?v=hfikzuT2_WY)

Finally, let’s discuss the deployment process for continuous improvement, or CI, and continuous development, or CD. Specifically, here we’ll learn about checking recent changes using the “change history” tool, how to best leverage CI or CD in a Dialogflow CX implementation, and the CI/CD and DevOps Automation Framework. First, let’s cover how to check for recent changes using the “change history” tool. On the Dialogflow CX console, go to the "Change history" section. Click on "Filter" to get a dropdown with the following options: Display name, Action involving options like create, update, delete, or restore, Resource type including agent, backup, entity type, flow, intent, or page, Editor, and Timestamp. Select your filter criteria. You will be able to view the following before and after filtering: With Display name, view the name of the flows and the pages. With Resource type, view the page or flow. View actions like create, update, delete, or restore. View the editor’s email address. And view the timestamp including the date and time.

### Video - [Using continuous testing and continuous deployment](https://www.cloudskillsboost.google/course_templates/1105/video/492189)

* [YouTube: Using continuous testing and continuous deployment](https://www.youtube.com/watch?v=CD5oCmeUvTY)

Let’s now discover how to use continuous testing and continuous deployment. The continuous tests feature automatically runs a set of test cases configured for an environment to verify the intended behavior of the flow versions in that environment. To prevent a bad version from becoming live in the environment, you can also use continuous deployment to run the same set of verification tests before deploying a flow version to the environment. For more information on these practices, please refer to the Additional resources document. To select the test cases you want to include in the “evaluation set” for continuous testing, follow these steps: Open the Dialogflow CX Console. Choose your GCP project. Select your agent. Click the Manage tab. Click CI/CD. Select an environment from the “Environment name” dropdown menu. Click the “Continuous tests” tab. The Test Cases tab opens by default. In the “Select test cases” dropdown, select a predefined test case to add to the continuous testing set. And in the Settings tab, the Continuous test toggle should be on by default. To add continuous deployment, click the Continuous deployment toggle. To view the results of daily continuous tests for an environment, click the Continuous tests tab, then Results. To view whether continuous testing is enabled for an environment, click environments under the Manage tab. For each environment, the continuous testing status is under the Continuous Test column.

### Video - [CI/CD and DevOps Automation Framework](https://www.cloudskillsboost.google/course_templates/1105/video/492190)

* [YouTube: CI/CD and DevOps Automation Framework](https://www.youtube.com/watch?v=vuQAZs6UkSI)

Let’s now move on to CI/CD and devOps automation framework. Automation helps you standardize your builds, tests, and deployments by eliminating human-induced errors for repeated processes like code updates. As you automate your deployment, you need checks and guards to ensure that your deployments are applied safely and that you have a mechanism in place to restore previous deployments as needed without significantly affecting your user's experience. In light of this: Whenever new code is committed to the repository, have the commit automatically invoke the build and test pipeline. Automate unit and integration testing. And automate your deployment so that changes deploy after your build meets specific testing criteria. Monitoring is the process of collecting, analyzing, and using information to track applications and infrastructure in order to guide business decisions post deployment. Monitoring is key in your implementation lifecycle because it gives you insight into your systems and your work. Monitoring enables service owners to: Make informed decisions when changes to the service affect performance. Apply a scientific approach to incident response. Measure your service's alignment with business goals. Analyze long-term trends. Compare your experiments over time. Define alerting on critical metrics. Build relevant real-time dashboards. And perform retrospective analysis. Also, keep in mind the importance of a well-defined escalation process, which is key to reducing the effort and time that it takes to identify and address any issues in your systems. This also covers issues that require support for Google Cloud products or for other cloud producers or third-party services. What you see on the screen is an example of an escalation process that helps define how and when to escalate issues internally, create support cases, document the architecture, and work with teams through best practices, such as during an outage. Throughout this process it is important that anyone needing support has the necessary permissions. The process also includes steps to set up monitoring, alerting and logging, and creating templates. Moreover it includes documenting the escalation process with actions and succession planning to train new team members in how to deal with support. Lastly, keep in mind that analyzing historical data related to your virtual agent volume can help you plan for the future with regard to the storage and bandwidth capacity required to service spikes or dips in expected traffic. Some important metrics to consider to plan for this are: Average and peak utilization, Spikes in usage patterns, Seasonal spikes based on business requirements, such as holiday periods for retailers, And how much over-provisioning is needed to prepare for peak events and rapidly handle potential traffic spikes. Congratulations on completing this training! As we have seen throughout this training, having a testing and deployment strategy can strengthen the program confidence in signing off on a new delivery. By anchoring your testing strategy in customer user journeys, you are can create focused test cases that will handle all likely planned scenarios. Remember that testing is integrated in the development lifecycle through Unit Tests, Integration tests, End to End tests, and A/B tests. When it comes to deployment, choose a strategy that will enable your team to make versioned updates to the virtual agent and provide them with the ability to quickly and reliably run regression tests between them. This will ensure the best experience for your customer and their end users.

### Quiz - [Deploying continuous integration and continuous delivery (CI/CD) Quiz](https://www.cloudskillsboost.google/course_templates/1105/quizzes/492191)

### Lab - [Conversational AI | Testing and Logging in Conversational Agents](https://www.cloudskillsboost.google/course_templates/1105/labs/492192)

In this lab, you'll explore testing and logging tools available for developing your agent in Conversational Agents.

* [ ] [Conversational AI | Testing and Logging in Conversational Agents](../labs/Conversational-AI-|-Testing-and-Logging-in-Conversational-Agents.md)

## Additional Resources

This module includes the list of additional resources that complement the course learning

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/1105/documents/492193)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
