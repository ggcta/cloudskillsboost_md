---
id: 1099
name: 'Advanced Performance Measurement'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1099
date_published: 2024-12-02
topics:
  - Dashboard
  - SQL
---

# [Advanced Performance Measurement](https://www.cloudskillsboost.google/course_templates/1099)

**Description:**

In this course, you will learn about advanced methods and tools to monitor the performance of your virtual agents in Dialogflow CX

**Objectives:**

* Differentiate between transcripts and display a clear understanding of the customer journey.
* Illustrate an understanding of Dialogflow CX log data and conversational basics.
* Identify how to transform Dialogflow CX data into information.
* Learn how to analyze the best ways to answer ad-hoc questions using SQL.
* Utilize Looker Studio to create custom dashboards.
* Understand the process of collaborating with Dialogflow CX developers on reporting.
* Determine which tools you can use to help achieve feedback loop analysis.

## Revisiting our Customer Journey

This module explores the importance of analyzing customer transcripts and how specific metrics can be used to help better understand the customer journey.

### Video - [Analyse the differences between 3 transcripts](https://www.cloudskillsboost.google/course_templates/1099/video/515387)

* [YouTube: Analyse the differences between 3 transcripts](https://www.youtube.com/watch?v=CvOhgw8y3K8)

SPEAKER: Welcome to Advanced Performance Measurement. This course will give you an advanced understanding of monitoring and performance measurement techniques. In the basic feedback loop course, a prerequisite to this training, we established the importance of understanding your data, analyzing it, identifying root causes, then enhancing and monitoring your conversational agent. Here, we'll explore advanced techniques to analyze agent actions at a more detailed level. As you can see on the agenda, we'll cover revisiting the customer journey. This involves revisiting the different touchpoints users have with your agent, and understanding how your agent performs at each stage. By analyzing user interactions at each touchpoint, we can pinpoint areas for improvement. Next, we will discuss Dialogflow CX log data, which captures detailed information about every user interaction with your agent. By analyzing these logs, we can gain insights into user behavior, identify common issues, and track agent performance over time. However, raw Dialogflow CX data is not very meaningful on its own. That's why we will explore transforming Dialogflow CX data into information next. We must transform it into actionable insights by cleaning, organizing, and visualizing the data. This will allow us to identify trends, patterns, and areas for improvement. Moving on, we will discuss answering ad hoc questions using SQL. SQL is a powerful query language that allows us to extract specific information from our Dialogflow CX data. Furthermore, we will go over creating custom dashboards in Looker Studio. Looker Studio is a data visualization tool that allows us to create custom dashboards to track key agent performance metrics. These dashboards can provide at-a-glance insights into agent performance and help us identify areas for improvement. The next topic we will explore is collaborating with Dialogflow CX developers on reporting. And lastly, we will talk about other tools to consider. By leveraging these advanced techniques, we can gain a deeper understanding of how your agent is performing and identify opportunities for improvement. This will help you to build a better, more effective conversational agent that meets the needs of your users. Let's revisit the customer journey covered in the basic performance measurement, and try to understand what the gaps in the basic analysis are that we need to bridge. Our first objective is to analyze the differences between multiple transcripts to help illustrate this point and why a more critical approach to the data is necessary. Next, we'll discuss the metrics that can help us better understand the customer journey. We'll explore metrics that tell us how well the agent is actually helping customers achieve their goals. Let's start by analyzing the chat transcripts in the following slides. Try to determine what happened in the conversation. Try to categorize them as optimization or 10x opportunity. So what is the difference between these conversations? Well, the conversations differ, but all relate to making a payment for internet services. But it's clear that transcripts can't help differentiate between what's expected and what's not. While reading transcripts is very important, it's not the only tool we can use. As we move beyond analyzing individual conversations, we face a challenge. How do we make sense of large data sets of conversations to identify patterns and areas for improvement? Looking at raw transcripts of multiple conversations becomes overwhelming and loses context quickly. This is where metrics and dimensions come in. They act as data shortcuts, summarizing key aspects of each conversation into a format that can be easily analyzed and aggregated. Consider items like the number of turns taken, intents triggered, or the date that allows us to easily aggregate conversations. But even with these tools, pinpointing the root causes of issues can still be tricky. Take the three conversations on the slide, for example. Both conversations B and C triggered the same intent, indicating that the user's goals were similar. Yet, conversation C ended up being escalated while conversation B did not. Why? Not thinking about the process of making a payment makes reviewing conversations ineffective. So let's review our payment journey. This is a flow diagram for making a payment. As you can see, it involves multiple steps to collect the required information from the user, and has several decision points that can lead to live agent escalation or end the session. Keep this flow in mind and pay attention to key decision points as we walk through the following slides to analyze the three conversations. Conversation A is an example of a no attempt. A user wanted to do something and then steered away. They are likely to call back. Now you could treat this as an incremental opportunity by offering an expedited payment when the user calls back to say something like, hey, I realized you called about making a payment. Did you want to pick it up from there? It might offer a better customer experience, but it might not offer any incremental containment. You could also view this as a 10x opportunity if there is a business need for getting customers to pay their bills. For instance, they might be offered a 10% discount if they pay the balance on the same day. This really depends on the type of customers and any challenges faced by the business. Conversation B has account characteristics that make it ineligible for the designed experience, and it was likely sent to a legacy payment system outside of Dialogflow. Now if we add a volume metric to determine how often this happens, for instance, 1 million conversations per month, it would more likely be a 10x opportunity because we could make changes in our payment flow to support these types of conversations. Conversation C has a hard failure that escalated the conversation, but the origin of that failure was a webhook issue which only happens 1,000 times per month. So this might only be an incremental opportunity due to the volume of cases. Reaching surefire conclusions hinges on reliable and comprehensive data. Without it, optimizing processes or identifying breakthrough opportunities becomes a guessing game. Open-ended questions like, what's the 10x opportunity, help explore possibilities. When reviewing our conversations, we might have been biased towards fixing conversation C. Conversation C was a clear escalation, but when we added volume, it only occurred for a few conversations. Conversation A, where a user does not want to make a payment, could be an interesting use case for incentivizing users to make a payment. Lastly, conversation B is a net new opportunity to replace another system, but a business case would need to be built on how to deliver more value.

### Video - [Metrics for better understanding the customer journey](https://www.cloudskillsboost.google/course_templates/1099/video/515388)

* [YouTube: Metrics for better understanding the customer journey](https://www.youtube.com/watch?v=KDK7_TvdnAA)

SPEAKER: Next, let's look at what metrics would be needed to better understand the customer journey. But what are the key events to track? First are attempts at self-service. This is when the intent of the user is captured as they attempt to complete a specific self-service, and they are able to enter the experience. Second is completing self-service. This is when the user has completed the action or has received some information to complete the service elsewhere. Last is event error reason. This is for logging any error messages encountered during the process. The benefits of event tracking can be summarized in these four points. Identify friction points. This is the first and most crucial step. You need to be able to pinpoint the specific places in your conversation where users are having difficulty. Another benefit is to improve virtual agent response. Once you know where the friction points are, you can start to improve the way your Virtual Agent responds in those situations. This might involve providing more clarification or guidance or rephrasing the question in a way that is easier to understand. A third benefit of event tracking is that it can help reduce payment abandonment. If users get stuck during the payment process, you're likely losing out on payments. By identifying and resolving these issues, you can make the payment process smoother and more efficient. And lastly, it assists in measuring assistance effectiveness. Ultimately, we want to provide the best experience. Knowing how often a user is able to complete self-service and hang up is likely aligned with what the customer was expecting. Let's refer back to the payment-flow diagram and explore how event tracking applies to the experience of making a payment. In this case, pay attention to individual pieces of information that can be collected as the customer progresses through the flow. At a minimum, we would want to know when an intent is collected, when the user gets into the service flow, and when they successfully complete it. We can see these marked with a yellow box. By tracking these metrics, you can gain a deeper understanding of how well your agent is capturing user intent, how often do users attempt to resolve their issues independently through self-service options, and how successful those attempts are? Let's break down each of these metrics to explore why they provide valuable insights. Intent collection rate measures the percentage of conversations where the agent correctly identifies the user's intent. A high intent collection rate indicates that your agent effectively understands what users are trying to achieve. The self-service attempt rate tells you how often users try to resolve their issues through self-service options, such as FAQs or virtual assistance. A high self-service attempt rate suggests that users are confident in your agent's ability to handle their requests independently or if they are even eligible for the experience. The self-service success rate measures the percentage of self-service attempts that successfully resolve the user's issue without requiring human intervention. A high self-service success rate indicates that your self-service options are effective and reduce the need for agent support. By monitoring these metrics over time, you can identify areas for improvement in your conversational agent. While tracking successful and failed payments is important, there's a wealth of insights to be gained by monitoring other key events throughout the payment process. These additional events can expose friction points and bottlenecks that might otherwise go unnoticed. Let's zoom in on a few examples. First is the selected payment method. This event captures which payment method the user chooses, be it a credit card, digital wallet, or something else. Analyzing this data can reveal user preferences or even varying success rates. Another metric is the abandoned payment attempts. This event flags instances where users drop off before completing the payment. Identifying where in the process this happens can expose roadblocks, like confusing forms or technical glitches. By tracking these additional events, you can gain a deeper understanding of how users interact with your payment system.

### Quiz - [Revisiting the customer journey Quiz](https://www.cloudskillsboost.google/course_templates/1099/quizzes/515389)

## Understanding DFCX log data

This module explores how to leverage Dialogflow log data for the purpose of advanced performance monitoring

### Video - [Enabling Cloud logging data for Conversational Agents](https://www.cloudskillsboost.google/course_templates/1099/video/515390)

* [YouTube: Enabling Cloud logging data for Conversational Agents](https://www.youtube.com/watch?v=rIioLYR_Gwo)

SPEAKER: Dialogflow generates exhaustive logs of each and every state a bot transitions through during a session. In this section, we will focus on how to enable and use this log data for the purpose of advanced performance measurement. We will first review the steps to enable Dialogflow CX logs. Next, we will discuss Dialogflow CX conversational basics and how to make sense of the logs. First up, enabling Cloud Logging in Dialogflow CX. Dialogflow API requests and responses can be logged to Cloud Logging for your agent. This will allow you to view the details of each API call within Cloud Logging. But logging is not enabled by default, so you'll need to enable Cloud Logging in the Agent Settings. This will allow Dialogflow CX to log all interactions into the Cloud Logging part of GCP, making it available for later analysis. It's possible to log all interaction data with Dialogflow, which is useful for a number of reasons. For instance, by recording a user's interactions as part of their transaction history, Dialogflow provides valuable insights that can help you identify areas for improvement and optimize your agents for better engagement. One key use of logging data is to analyze conversation flow. You can understand how users navigate through your dialogue, where they might encounter roadblocks, and where you might need to refine the conversation paths for smoother interactions. Furthermore, the log data can reveal areas where your agent has difficulty understanding user intent. By analyzing misinterpretations and failed responses, you can pinpoint and address weaknesses in your training data or NLP models. This could involve adding more training examples, clarifying prompts, or fine tuning your language models. Lastly, path analysis allows you to see the most frequent paths customers take in conversations.

### Video - [Conversation structure in Conversational Agents](https://www.cloudskillsboost.google/course_templates/1099/video/515391)

* [YouTube: Conversation structure in Conversational Agents](https://www.youtube.com/watch?v=4AvTcz-rOqk)

SPEAKER: To under the logs, we will first need to understand foundational Dialogflow CX conversational basics. Let's first go over a few conversation concepts. A conversation is any interaction between a user and an agent at a point in time. This can encompass various interactions, from simple greetings and inquiries to complex, multi-turn dialogues involving information exchange, task completion, and problem solving. The next unit of a conversation is a turn. This is a pairing of one user utterance-- for example, How much is due?-- with one agent response. Agent responses can include many sentences string together from different pages. For instance, turn 3 of the conversation on the screen includes two sentences. Now that we have covered conversation basics, we can explore Dialogflow CX-specific language. The first is flows. Flows are used to capture complex conversations by defining specific topics and the associated conversational paths. Every agent starts with a default start flow, and then subsequent flows are designed around different use cases like authentication or stating a bill balance. Within a flow are individual pages. Pages capture information, questions, and response messages. Not all pages have user input. Pages with user input are called "input pages," and pages without user input are called "routing pages." Input pages typically ask a question to the user, and then it subsequently captures the information. Routing pages typically contain different business logic to navigate the conversation to a successful outcome. Take note that there are many intermediate routing pages that do not appear in the chat transcript presented on the screen. As the session progresses, the conversation flows from the steering page, where the agent greets the customer, to the authentication page, where it collects account information. And then it moves to the billing balance page to provide information on the bill due. But in order for this to work, there might be additional intermediate steps which are not obvious from the transcript. For example, from steering to authentication, there might be steps to check if the user is authenticated, what plan they're using, and so on. All of this might occur before asking for more information from the customer. Similarly, there might be intermediate steps between the authentication and billing balance pages to check if the provided account information is valid, or if the account is active, and so on. In the world of conversational agents, intents are all about understanding what a user wants to achieve in a given interaction. An intent, in essence, captures the user's objective. Now, there are different types of intents, and understanding these types can further enhance your conversational agent's capabilities. The primary intent, also known as the head intent, represents the main goal of the interaction. In our previous example, making a payment was the head intent. But sometimes users might express additional intents alongside the head intent. These are called "supplemental" or "contextual" intents, providing further details or clarification. For example, a user saying "yes" or "no" to a question. In this example, billing inquire balance is a head intent and is the primary reason for the user to call. It is trained on similar phrases to what the user said. Then, when the user is asked if they want to make a payment, the answer is "no." This response is captured as "Confirmation - No," which is a contextual intent. Understanding these foundational Dialogflow CX concepts is essential for making sense of generated log data.

### Video - [Understanding Cloud logging data](https://www.cloudskillsboost.google/course_templates/1099/video/515392)

* [YouTube: Understanding Cloud logging data](https://www.youtube.com/watch?v=8JC5256LYYk)

SPEAKER: Next, we will dive into the specifics of the log entries. We can view the Dialogflow logs in the Logs Explorer in the GCP console. You will first need to filter the logs to just Dialogflow CX. You can do this by filtering by the log name. Next, you will want to filter for specific conversations by using the Session ID label as a filter. Session ID is commonly called Conversation ID, as well. Lastly, you will want to filter for the final message for each turn in the session by adding a response-type filter. A conversation contains a log entry for each turn. Let's inspect a single log entry. Each log entry has a unique insert ID. The timestamp indicates when the turn started. The log name corresponds to the Google API generating the log and differentiates Dialogflow CX logs from others, and the labels contain information like the session ID, which is the conversation ID used to associate all the turns. Now let's look deeper into the JSON payload. This is the JSON serialized data of the response provided by the Dialogflow API at every turn. It is critical for the bot-performance measurement. The queryResult field contains detailed information on all the state transitions the bot goes through in that turn. It also has a response type. By filtering for the word "final," it gets the log entry that will appear in the BQ export. And lastly, it has a response ID, which is the unique ID for each turn. Next, we look at the query result field inside the JSON payload. The transcript field contains the user utterance, whereas the language field corresponds to the language the bot is configured for. The Match field contains information on intent matching or other events. It provides match confidence score, match type, and display name of the corresponding intent match. Additional fields include the Parameter field, which contains information about all the session parameters set at the end of a given turn. Response messages contain all the responses given by the bot in that turn. There could be multiple responses because of transitioning through multiple intermediate steps. Lastly, DiagnosticInfo contains detailed information about the step-by-step transition of session state, including information about webhooks or intents triggered, pages and flows, and so on. The diagnosticInfo field is used for debugging bot behavior or even setting custom metrics. Some important fields are the Execution Sequence array, which contains step-by-step details of what happened in the turn, including pages, flows, and intents. The first step contains information related to the source flow, and page. This is the user's utterance in response to what the agent had asked in the previous turn. The last step corresponds to the final or target state of the bot at the end of that turn. The logs contain much information, so it's best to look at them programmatically and focus on small sections at a time.

### Quiz - [Understanding DFCX log data Quiz](https://www.cloudskillsboost.google/course_templates/1099/quizzes/515393)

## Transforming DFCX data into information

This module explores how to enable Big Query to analyze logs and transform them into valuable insights.

### Video - [Enabling BigQuery export](https://www.cloudskillsboost.google/course_templates/1099/video/515394)

* [YouTube: Enabling BigQuery export](https://www.youtube.com/watch?v=N8h7bF_d05M)

SPEAKER: So now that we have an overview of some core Dialogflow CX concepts and learned how to read the logs, we will cover how to transform that data into information for analysis. In this section, we will briefly discuss how to enable the BQ Export and how to transform that data. We will not cover each field in detail, but you can review the resource provided in the additional resource document for supplemental information. First, to analyze the data in SQL, we need to export the data from Dialogflow CX to BigQuery. Fortunately, there's an automated function for this. The BQ Export feature can help automate conversation data analysis for insights and improvement. It sends session logs such as user interactions, responses, and details. It also sends conversation metadata like intents, pages, and flows to BigQuery. To use the BQ Export functionality, you must create a specific table on the same project. This is so the Dialogflow can start populating the table. On the right, you can see the current schema that is required for Dialogflow CX to start populating the data. You can manually create the table or use the query. For more details about this, refer to the additional resources document. After creating the BigQuery table with the specific schema, it's time to enable the interaction logging functionality on Dialogflow CX. This is needed to point the formatted logging to the recently created table. To enable and point on Dialogflow CX, you need to enable Interaction Logging and select the project, data set, and table accordingly. Now that everything is set up, all near real-time logging information will be populated into BigQuery. Take note that request and response packages are incredibly modular to fit all possibilities that can occur in Dialogflow. Therefore, most of the information is in JSON-like format. In the following section, you will learn how to transform this data into information that is easy to query.

### Video - [Transforming BigQuery data into information](https://www.cloudskillsboost.google/course_templates/1099/video/515395)

* [YouTube: Transforming BigQuery data into information](https://www.youtube.com/watch?v=7a9pidG98ZU)

SPEAKER: So far, we have learned how to export the Dialogflow CX logs data into BigQuery. Next, let's explore how to transform this data to extract useful insights for performance monitoring and improvement. Before we go about transforming the raw logs data, let's explore why we need to transform BigQuery data. Turn level information like turn position, request time, and so on is still available even without flattening the table. Further insights can be extracted by breaking down data within turns, often involving multiple state transitions. This is because Dialogflow CX is a state machine. It could be beneficial to break down this data into transition levels for deeper analysis. The export includes request and response JSONs for each Dialogflow API call, which we'll explore in detail to uncover valuable information. The detect intent response JSON would be one of the most crucial fields to unravel and extract key pieces of information for performance tracking and monitoring. To get the most usability out of the BQ export, we recommend transforming the data of the most-used columns for your analysis. For voice conversations, Dialogflow CX designers will typically allow input from either a voice command or as DTMF digits. For example, pressing your keypad numbers. You will see values filled for either user utterance or optional DTMF digits. For chat conversations, users will have more options for interacting, for example, pressing a button, and so on. So it's important to consult your Dialogflow designer about the ways in which users may interact with the bot. For voice conversations, Dialogflow CX designers will typically type the agent response and either the Agent Says field or a Conditional Response field. In the SQL, you will see the individual responses from an agent concatenated together. For chat conversations, depending on the implementation, there may be some variance in how a message is rendered or typed out to a user. It is important to consult your Dialogflow CX designers to understand how they implemented it, as this varies by deployment. For each turn, there is a source page. This is what the user responds to in the conversation. After what the user said is processed, it will follow the rules on the source page and subsequent pages until it gets to another input page to ask a question, it's best to use the execution sequence for this information. Within each turn, multiple webhooks can be called. This dedicated array to webhooks contains the most pertinent information, like which page was triggered from, the amount of time it takes to return data, and which session parameters were updated. Depending on how the Dialogflow CX designers decided to track agent escalation, either or neither of these approaches could correctly correspond to true agent escalation. You must consult with your Dialogflow CX designers on how they end conversations when the user hangs up or when the request gets escalated. Remember, these are tracked at the turn level.

### Quiz - [Transforming DFCX data into information Quiz](https://www.cloudskillsboost.google/course_templates/1099/quizzes/515396)

## Answering ad-hoc questions using SQL

This module explores the most common SQL patterns to support the successful fulfillment of recurring user queries.

### Video - [Key metrics and how to measure them](https://www.cloudskillsboost.google/course_templates/1099/video/515397)

* [YouTube: Key metrics and how to measure them](https://www.youtube.com/watch?v=nSFM9o0HA_g)

SPEAKER: Up to this point, we've reviewed how to read the data and covered the basics of data transformations for analysis. Now we are ready to explore some common SQL patterns for questions users may ask. In this section, we will review key metrics and how to measure them in SQL. Let's first look at some of the key metrics that will help us drive actionable insights from the BigQuery table. To gauge the success of your conversational agents, focus on three core areas-- interaction experience, agent design, and back-end readiness. It's important to note that the specific metrics you track will depend on your unique goals for the conversational agent. Interaction experience metrics measure how well users are able to interact with a conversational agent. Agent design metrics measure the quality of the conversational agent itself. This includes metrics such as no-match and no-input rates. Back-end readiness metrics measure the ability of the conversational agent's back-end systems to support the agent's operation. This includes metrics such as webhook failures and latency. These business metrics showcase the success of the virtual agent and can be customized based on the business use case, and some of the metrics may have additional components in their definitions, depending on the business and customer. However, tracking these metrics lets you understand the bot's overall performance and identify areas of improvement or even new use cases. Let's explore some examples of these three types of metrics. First is interaction experience metrics, which enable us to look beyond mere conversation volume and delve deeper into metrics like containment rate, head intent detection, escalation rate, and repeat rate. Analyzing these metrics across various contexts-- for example, steering flow versus other flows-- provides valuable insights into user behavior and the overall effectiveness of the conversational AI system. The agent escalation rate focuses on how often a conversation is escalated to a human agent. This is done by counting the number of conversations marked as escalated and dividing it by all conversations. This metric can be broken down by any Dialogflow CX design component like flows, intents, or pages. It is also important to view this by date, as well as to track the impact of changes made to the agent. Some challenges that arise with tracking the agent escalation rate are-- how are you tracking abandoned calls versus hang ups? And are there legacy systems that can still escalate the call after it leaves Dialogflow? It's important to work with your Dialogflow designers to explore these scenarios. While the escalation rate provides an immediate indication if a call is contained or not, call centers typically measure their ultimate success by 72-hour containment. You may also hear this called "first call resolution." Therefore, another essential interaction experience metric is the 72 containment rate. This metric essentially tries to capture when a user calls in, they only call in once, and they are not escalated to an agent in any subsequent conversation within 72 hours. Contained session refers to the sessions wherein the bot could respond to the user queries without needing to refer the user to a human agent and where there was no repeat contact in the same channel or use case within a given time period. Containment is a very important signal for determining the effectiveness of the bot in serving the actual customer needs. This is because, from a call center perspective, every contained session contributes to cost saving since it prevents human agent effort. As a metric, it is better to track the containment rate than the absolute number. Some challenges that arise with tracking the containment rate are-- what if a user calls multiple times but never escalates? And how is a user tracked across different conversations or different technology platforms? Another simple metric is looking at the volume of different data dimensions like head intent and date. This can be useful to pick up seasonal changes or identify the number of conversations impacted by a bug. While volume is simple to calculate, there is naturally a lot of variation. So how do you track when it's lower or higher than expected? Next, let's discuss agent design metrics that relate to the performance of the bot and its ability to function optimally, and how it affects the user experience in a session. Bot builders can use these to debug design issues and track improvements after any change. Age and design metrics include metrics such as the no-match rate and no-input rate. The no-match rate is when the NLU fails to identify any match for the user utterance. No-match messages are messages where the Dialogflow bot fails to detect any intent, resulting in a no-match fallback. It is calculated by dividing the no-match messages by the unique messages. The sample query demonstrates how to track this metric by a source page. Similarly, this can also be broken down by flows or final pages. Use the frequency of different no-match types to identify the intents that are in scope and understand new desired user paths. The no-input rate is the rate at which the user utterance does not respond to an agent. It is calculated by dividing the no-input turns by the total number of turns. This rate can be broken down by flows and pages and can be used to indicate if the question by the agent is confusing or does not have a clear explanation for what the user needs to say. Use the frequency of different no-input types to simplify complicated or lengthy steps and provide clear instructions. The last category of metrics is back-end readiness, which typically focuses on webhook performance. But other performance metrics can also be considered. We typically find these metrics related to an SLA and are heavily monitored in dedicated systems. The most important metric is to track failures of webhooks. You can do this by observing the status. And any time the status is not OK, it is likely escalated directly to an agent. You want to guarantee fast and responsive webhooks for a smooth customer experience. The statuses of webhook will generally provide a summary of what caused the failure, such as time-out messages. For each webhook, inspect how often the webhook does not complete successfully, as these are typically directly linked to direct escalations. Some of the use cases for NLU improvements include a time series plot for the no-match rate that helps determine the effect of NLU improvements made over time. For example, the plot line showing low or no-match rates after the fix confirms your enhancement. Similarly, identifying the top N or bottom N pages with high no-match rates helps detect NLU bottlenecks. We can enhance performance by adding training phrases to intents within such pages and observing the no-match rates after the enhancement. The most crucial analysis for optimization is by tracking the no-match metrics within steering flows, since head intent detection occurs at the steering layer. Likewise, we can also analyze verbatim no-match phrases to determine if new intents or intent routes are needed. Tracking overall hand-off trends helps to assess bot stability and identify external influencing factors-- for example, new self-service features or sporadic issues related to the industry. Hand-off rates can be analyzed at different levels to indicate where issues occur. First, you can look at it over time. This will be key to monitoring to see if performance as a whole is getting better or worse by day, week, or month. Second, you can analyze it by intents. This is useful to determine which intents have the most escalation. Any feature developed to reduce escalation by an intent is a 10x opportunity. Lastly, you can look at it by page to determine friction points for the customer. Looking at your hand-off rate in different contexts can provide different clues about performance as a whole.

### Quiz - [Answering ad-hoc questions using SQL Quiz](https://www.cloudskillsboost.google/course_templates/1099/quizzes/515398)

## Creating custom dashboards in Looker Studio

This module explores how to build self-serve reusable analytics dashboards in Looker.

### Video - [Agent transitions](https://www.cloudskillsboost.google/course_templates/1099/video/515399)

* [YouTube: Agent transitions](https://www.youtube.com/watch?v=NKhxbbru8pY)

SPEAKER: While you can write custom queries for each request, it's important to build reusable dashboards that the other teams can use to self-service their own analytics. In this section, we will discuss some ideas for dashboards you can create to help monitor or review agent performance. Specifically, we will look into agent transitions, intent launch, and webhook performance. Agent transitions is the most flexible dashboard for reviewing customer and agent responses in bulk. In the context of a conversational agent, a transition refers to the specific exchange between a user and a conversational agent, encompassing three key elements. One, the agent's previous turn. This refers to the agent's most recent utterance or statement within the conversation. Two, the user's current input. This encompasses the user's latest response, whether it be a textual query, a voice command, or any other form of interaction. And three, the agent's subsequent turn. This represents the agent's response to the user's current input, marking the next step in the conversation flow. Understanding transitions is crucial for evaluating and optimizing conversational agents. By analyzing these individual exchanges, we can gain valuable insights into how effectively the agent understands user intent. Are users consistently providing clear and unambiguous prompts that the agent can accurately interpret? The agent's ability to handle context. Can the agent maintain a coherent conversation thread by leveraging the information gleaned from previous exchanges? The naturalness and fluidity of the conversation. Do transitions feel smooth and intuitive, or are there abrupt shifts or awkward pauses that disrupt the flow? Ultimately, focusing on transitions allows us to fine tune our conversational agents for more effective and satisfying user interactions. The dashboard is laid out in the following order. First, the pre-transition phase. That is what the agent asked. Then the transition, or what the user says. And then the post-transition, or where the agent goes. In the top half of the dashboard, this is arranged by looking at pages and the matched intent in the middle. On the bottom half of the dashboard, you can view the most common agent responses and user utterances. You can utilize filters to understand, for instance, which pages most often lead to end session, or what user utterances triggered that specific behavior. What's nice about this dashboard is you can use it for many different use cases. For example, you can also use it to investigate no matches to find out what users are saying. Furthermore, you can perform a quick clustering analysis with the conversation counts to see the frequency.

### Video - [Intent launch](https://www.cloudskillsboost.google/course_templates/1099/video/515400)

* [YouTube: Intent launch](https://www.youtube.com/watch?v=abGZRGRUo9Y)

SPEAKER: The next dashboard, Intent Launch, is focused on monitoring different intents based on some summary metrics. The Intent Launch dashboard is a tool that can help you gain valuable insights into the performance of your conversational agents. As you can see, the dashboard is divided into three key sections. First, is a high-level summary of performance metrics. This section provides a quick overview of how your conversational agent performs overall compared to the previous period. The metrics displayed here may include volume, the self-service attempt rate, the self-service success rate, and the escalation rate. By comparing these metrics to the previous period, you can identify areas where your agent performance may be improving or declining. The next section focuses on the same metrics, but by head intents. This allows you to quickly identify which intents are having the most impact on your user's experience. Lastly, you can drill into specific intents to understand the performance metrics over time. This section allows you to delve deeper into the performance of individual intents. By clicking on an intent, you can see a detailed timeline of its performance metrics over time. This can help you identify trends and pinpoint specific areas for improvement. By using the Intent Launch dashboard, you can gain a comprehensive understanding of how your conversational agents are performing and identify areas for improvement. By regularly monitoring and analyzing your intent performance data, you can make data-driven decisions to optimize your conversational agents and improve the user experience. Lastly, you can drill into specific intents to understand the performance metrics over time. This section allows you to delve deeper into the performance of individual intents. By clicking on an intent, you can see a detailed timeline of its performance metrics over time. This can help you identify trends and pinpoint specific areas for improvement. By using the Intent Launch dashboard, you can gain a comprehensive understanding of how your conversational agents are performing and identify areas for improvement. By regularly monitoring and analyzing your intent performance data, you can make data-driven decisions to optimize your conversational agents and improve the user experience. The remaining position of the dashboard allows the data to be split among different dimensions to look for reasons why performance may be up or down. This part provides different slices of dimensions that might be valuable, like product collected, biller type, event, et cetera.

### Video - [Webhook performance](https://www.cloudskillsboost.google/course_templates/1099/video/515401)

* [YouTube: Webhook performance](https://www.youtube.com/watch?v=NcUQ_Twly2g)

SPEAKER: The last important dashboard is a webhook performance dashboard for tracking the gathered or sent information. In this section, we'll dive into webhook performance metrics crucial for ensuring your conversational agents function smoothly. Webhooks are real-time communication channels that enable data exchange between your agents and other applications. By monitoring key performance indicators, you can identify bottlenecks and optimize webhook delivery for a seamless user experience. The top position focuses on the general sense of your webhook's health by looking at trigger count and failure rates. The trigger count measures how often the webhook is activated, indicating overall agent activity. And the failure rate tracks the percentage of webhook deliveries that encounter errors. A high failure rate suggests potential issues with a receiving application or the webhook configuration. Moving deeper, percentiles paint a clearer picture of delivery speed. By analyzing metrics like P50, P90, and P99, you can understand how long it typically takes for webhooks to reach their destination and identify any outliers causing delays. Ideally, you want these percentiles to remain stable and within acceptable thresholds. After looking at overall trends, you can drill into specific webhooks for various performance metrics, and you use the box plots to compare the latency variance. The next section focuses on analyzing those webhook failures. Operational failures occur when the webhook itself doesn't function as expected. This could be due to a number of reasons such as network errors, server errors, or authentication failures. Functional failures occur when the webhook is delivered successfully but does not produce the desired outcome. This could be due to a number of reasons, such as incorrect payload, logic errors, or external dependencies. The last section focuses on what session parameters were updated by a webhook. This can be helpful for seeing how common a specific value is.

### Quiz - [Creating custom dashboards in Looker Studio Quiz](https://www.cloudskillsboost.google/course_templates/1099/quizzes/515402)

## Collaborating with DFCX developers on reporting

This module explores common session parameters that can best enable reporting during development in Dialogflow CX

### Video - [Collaborating with Conversational Agents developers on reporting](https://www.cloudskillsboost.google/course_templates/1099/video/515403)

* [YouTube: Collaborating with Conversational Agents developers on reporting](https://www.youtube.com/watch?v=f0b0M7rRjqM)

SPEAKER: As we worked through this course, there were multiple mentions of needing to work with Dialogflow CX developers to get the most value out of reporting. In this section, we will discuss some common developer practices that can make reporting more impactful. We will focus on common enhancements in the Dialogflow CX design that can make reporting more impactful, such as different session parameters. Specifically, we will discuss the event session parameter, the head intent parameter, and the version number session parameter. The first is an event session parameter that allows for a high-level disposition of what happened to the call. As we reviewed different metrics, one area that is still not completely solved is why the user did not do the self-service, or what happened during steering for those unidentified head intents? Ideally, we would like to see a disposition reason for why the caller escalated and what page triggered it. We recommend expanding these base events as the conversational agent is developed. The more events you mark when the user takes a negative path, the easier it gets to identify the volume of these occurrences. We categorized our values into four groups, agent request-related, max attempts, business rules, and webhook failures. Let's go over these categories in more detail. The first category is when a user specifically asks to be transferred to an agent. The agent requested event is triggered when a user requests to speak to a live agent, indicating that the bot was unable to resolve their query or they required additional assistance. The next category, max attempts, encompasses several events that occur when the bot reaches a predefined threshold of attempts within a specific area of the conversation flow. These include-- first is maxed intent collection. This event is triggered if the bot attempts to gather the user's intent more than a set number of times within a specific section of the flow. This could indicate that the bot's intent recognition model is struggling in that context. Next is maxed ID collection. This event occurs if the bot repeatedly tries to gather the user's account ID or other identifying information within a limited number of attempts. This might suggest that the bot's prompts for this information are unclear or ineffective. There is also a maxed no match event, which is triggered if the user's utterances result in more than a predefined number of no-match situations within a specific Dialogflow CX page. And the last event is maxed no input, which occurs if the user fails to respond to the bot's prompts or questions beyond a set number of attempts within a page. This could mean that the bot's prompts are overly complex or that the user is encountering technical difficulties. The category of business rules relates to where the customer is not eligible to perform self-service. The event caller not eligible is triggered if the bot's business rules determine that the user is not eligible for self-service support through the conversational agent. And the event forced to agent occurs if the bot's flow logic dictates that the user should be transferred to a live agent. Last is the webhook failures category, which focuses on how to differentiate between different types of webhook problems. First is webhook operational failure. This event is triggered if there is a technical failure with a webhook integration used by the bot, forcing the conversation to be escalated to a live agent. And second is webhook functional failure. This event occurs if the bot receives an unsuccessful response from a webhook integration due to an issue with the data or processing within the webhook itself. As discussed earlier, there are two different types of intents, head intents and supplemental intents. A common problem is knowing the difference between a head intent and supplemental intents. To resolve this issue, we encourage having the developers mark the head intent in the session parameter. Assigning a parameter with a specific value not only enhances the bot's design by facilitating the identification of appropriate self-service options, but also streamlines the correlation between turns and corresponding head intents. Additionally, it enables the tagging of confirmation responses, yes or no, to specific head intents. This will also allow more complex agent questions, like do you want to make a payment, or a confirmation message. Yes actually means make payment. The last optimization to consider is adding a Dialogflow CX version number. A common problem is that when a bug is discovered, it's identifiable by the date it started to occur, but it's also corresponding to a version of the deployed bot. To resolve this issue, we encourage having the developers mark the version number of the bot as the default start flow to allow for better bug resolution. By adding a version number, you can better correlate when a specific bug started to occur after a new agent was deployed. It allows you to track which new features were released in each version in your changelog.

### Quiz - [Collaborating with DFCX developers on reporting Quiz](https://www.cloudskillsboost.google/course_templates/1099/quizzes/515404)

## Other Tools to Consider Leveraging

This module explores other common steps and tools to optimize the Feedback loop process in Dialogflow CX

### Video - [Tools to help achieve feedback loop analysis](https://www.cloudskillsboost.google/course_templates/1099/video/515405)

* [YouTube: Tools to help achieve feedback loop analysis](https://www.youtube.com/watch?v=ZWJan7ROivI)

SPEAKER: Now that we have a detailed understanding of various performance metrics and how to effectively use them to drive performance improvements, let's take a look at some of the other tools that will be helpful in executing advanced performance analysis. We will discuss other tools for you to consider in your feedback loop analysis. The main tool we'll be considering is Colab, which is a Jupyter Notebook. Let's go over the steps to optimize feedback loop analysis for a conversational agent. Step 1 is to integrate Dialogflow CX with BigQuery for data analysis. Next, utilize Colab Notebooks to read the agent metadata using SCRAPI and export agent-level information to understand the design. Lastly, create Looker Studio dashboards on the BigQuery tables for visualization and design insights. Colab is a product from Google Research. Colab allows anybody to write and execute arbitrary Python code through the browser, and is especially well-suited to machine-learning, data analysis, and education. More technically, Colab is a hosted Jupyter Notebook service that requires no setup to use. It provides free access to computing resources, including GPUs. You can use Colab to experiment with Python. The Python Dialogflow CX Scripting API, or DFCX SCRAPI, is a high level API that extends the official Google Python client for Dialogflow CX SCRAPI. It makes using Dialogflow CX easier, more friendly, and more Pythonic for bot builders, developers, and maintainers. With DFCX SCRAPI, you can perform many bot-building and maintenance actions at scale, including but not limited to, creating, updating, deleting, getting, and listing for all CX resource types. It allows you to convert commonly accessed CX resources to Panda's DataFrames. You can also have fully automated conversations with a CX agent. Moreover, you can extract validation information and change history information. It also allows you to search across all flows, pages, and routes to find a specific parameter or utterance. And it even enables you to quickly move CX resources between agents using copy util functions. Additionally, you can build the fundamental protobuf objects the CX uses for each resource, and much, much more. With the powerful combination of the tools covered in this section, you can significantly extend your optimization capabilities. You can use the technologies presented in this module to build many additional things that are not currently available in Dialogflow. And that brings us to the end of this course on advanced performance measurement. As we come to a close, let's revisit our end goal, which is to develop a successful virtual agent. You can use different categories of metrics to understand how the agent is performing today and recommend improvements to the Dialogflow CX developers. There are a variety of different tools you can use, like BigQuery, Looker Studio, and Colab, to accomplish that goal. We hope that this course empowered you to delve into the finer details of Dialogflow, allowing you to uncover interesting user sentiments and create great user experiences.

### Quiz - [Other tools to leverage Quiz](https://www.cloudskillsboost.google/course_templates/1099/quizzes/515406)

## Additional Resources

This module includes the list of additional resources that complement the course learning

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/1099/documents/515407)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
