---
id: 50
name: 'Essential Google Cloud Infrastructure: Foundation'
type: Course
url: https://www.cloudskillsboost.google/course_templates/50
date: 2025-04-03
datePublished: 2025-01-29
topics: []
---

# [Essential Google Cloud Infrastructure: Foundation](https://www.cloudskillsboost.google/course_templates/50)

**Description:**

This accelerated on-demand course introduces participants to the comprehensive and flexible infrastructure and platform services provided by Google Cloud with a focus on Compute Engine. Through a combination of video lectures, demos, and hands-on labs, participants explore and deploy solution elements, including infrastructure components such as networks, virtual machines and applications services. You will learn how to use the Google Cloud through the console and Cloud Shell. You'll also learn about the role of a cloud architect, approaches to infrastructure design, and virtual networking configuration with Virtual Private Cloud (VPC), Projects, Networks, Subnetworks, IP addresses, Routes, and Firewall rules.

**Objectives:**

- Interact with the Google Cloud console and Cloud Shell
- Deploy solutions using Google Cloud Marketplace
- Implement VPC networks and firewall rules
- Create and customize VM instances using Compute Engine

## Introduction

Introduction to the course.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/50/video/523056)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=G1Onqg8Oybs)

Hello. I'm Philipp Maier. I'm Mylene Biddle, we're both Course Developers, at Google Cloud and we want to welcome you to Architecting with Compute Engine, a series of three courses. Before we start using all of the different services that Google Cloud Platform, or GCP offers, let's talk about what GCP is. When you look at Google Cloud, you'll see that it's actually part of a much larger ecosystem. This ecosystem consists of open-source software, providers, partners, developers, third-party software, and other Cloud providers. Google is actually a very strong supporter of open-source software. That's right. Now, Google Cloud consists of Chrome, Google devices, Google Maps, Gmail, Google Analytics, G Suite, Google Search, and the Google Cloud Platform. GCP itself is a computing solution platform that really encompasses three core features: infrastructure, platform, and software. This map represents GCP's global infrastructure. As of this recording, GCP's well-provisioned global network connects over 60 zones to over 130 points of presence through a global network of fiber optic cables. And Google is continuously investing in this network, with new regions, points of presence, and subsea cable investments. On top of this infrastructure, GCP uses state of the art software-defined, networking and distributed systems of technologies to host and deliver your services around the world. These technologies are represented by a suite of Cloud-based products and services that is continuously expanding. Now, it's important to understand that there is usually more than one solution for a task or application in GCP. To better understand this, let's look at a solution continuum. Google Cloud Platform spans from infrastructure as a service, or IaaS, to software as a service, or SaaS. You really can build applications on GCP for the web or mobile that are global, auto-scaling, and assistive, and that provide services where the infrastructure is completely invisible to the user. It is not just that Google has opened the infrastructure that powers applications like Search, Gmail, Google Maps, and G Suite. Google has opened all of the services that make these products possible and packaged them for your use. Alternative solutions are possible. For example, you could start up your own VM in Google Compute Engine, install open-source MySQL on it and run it just like a MySQL database on your own computer in a data center. Or you could use the Cloud SQL service, which provides a MySQL instance and handles operational work like backups and security patching for you using the same services Google does to automate backups and patches. You could even move to a NoSQL database that is auto-scaling and serverless so that growth no longer requires adding server instances or possibly changing the design to handle the new capacity. This series of courses focuses on the infrastructure. An IT infrastructure is like a city infrastructure. The infrastructure is the basic underlying framework of fundamental facilities and systems, such as transport, communications, power, water, fuel, and other essential services. The people in the city are like users, and the cars and bikes, and buildings in the city are like applications. Everything that goes into creating and supporting those applications for the users is the infrastructure. The purpose of this course is to explore as efficiently and clearly as possible the infrastructure services provided by GCP. You should become familiar enough with the infrastructure services that you will know what services do and how to use them. We won't go into very deep dive case studies on specific vertical applications. But you'll know enough to put all the building blocks together to build your own solution. Now, GCP offers a range of compute services. The service that might be most familiar to newcomers is Compute Engine, which lets you run virtual machines on-demand in the Cloud. It's Google Cloud's infrastructure as a service solution. It provides maximum flexibility for people who prefer to managed server instances themselves. Google Kubernetes Engine lets you run containerized applications on a cloud environment that Google manages for you under your administrative control. Think of containerization as a way to package code that's designed to be highly portable and to use resources very efficiently. And think of Kubernetes as a way to orchestrate code in containers. App Engine is GCP's fully managed platform as a service framework. That means it's a way to run code in the cloud without having to worry about infrastructure. You just focus on your code and let Google deal with all the provisioning and resource management. You can learn a lot more about App Engine in the "Developing Applications with Google Cloud Platform" course series. Cloud Functions is a completely serverless execution environment or functions as a service. It executes your code in response to events, whether those events occur once a day or many times per second. Google scales resources as required, but you only pay for the service while your code runs. The "Developing Applications with Google Cloud" course series also discusses Cloud Functions. Cloud Run, a managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It is built on Knative, an open API and runtime environment built on Kubernetes that gives you freedom to move your workloads across different environments and platforms. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges you only for the resources you use calculated down to the nearest 100 milliseconds, so you'll never pay for your over-provisioned resources. In this series of courses, In this series of courses, Compute Engine will be our main focus. The Architecting with Google Compute Engine courses are part of the Cloud Infrastructure learning path. This path is designed for IT professionals who are responsible for implementing, deploying, migrating, and maintaining applications in the cloud. The prerequisite for these courses is the Google Cloud Platform Fundamentals: Core Infrastructure course, which you can find in the link section for this video. The Architecting with Google Compute Engine series consists of three courses. Essential Cloud Infrastructure: Foundation is the first course of the Architecting with Compute Engine series. In that course, we start by introducing you to GCP and how to interact with the GCP Console and Cloud Shell. Next, we'll get into virtual networks and you will create VPC networks and other networking objects. Then we'll take a deep dive into virtual machines, and you will create virtual machines using Compute Engine. Essential Cloud Infrastructure: Core Services is the second course of this series. In that course, we start by talking about Cloud IAM and you will administer Identity and Access Management for resources. Next, we'll cover the different data storage services in GCP, and you will implement some of those services. Then we'll go over resource management, where you will manage and examine billing of GCP resources. Lastly, we'll talk about resource monitoring and you will monitor GCP resources using Stackdriver services. Elastic Cloud Infrastructure: Scaling, and Automation, is the last course of the series. In that course, we start by going over the different options to interconnect networks to enable you to connect your infrastructure to GCP. Next, we'll go over GCP is load balancing and auto-scaling services. Would you will get to explore directly. Then we'll cover infrastructure automation services like Terraform so that you can automate the development of GCP infrastructure services. Lastly, we'll talk about other managed services that you might want to leverage in GCP. Now, our goal for you is to remember and understand the different GCP services and features, and also be able to apply your knowledge, analyze requirements, evaluate different options, and create your own services. That's why these courses include interactive hands-on maps through the Qwiklabs platform. Qwiklabs provisions you with a Google account and credentials, so you can access the GCP console for each lab at no cost.

### Document - [Welcome to Essential Cloud Infrastructure: Foundation](https://www.cloudskillsboost.google/course_templates/50/documents/523057)

## Interacting with Google Cloud

Overview of Google Cloud

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/50/video/523058)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=iL_Iel-Vryc)

In this module, we will provide you with an introduction to GCP by building on what you learned about the GCP infrastructure from the course introduction. This module is focused on how to interact with GCP. In the labs of this module, you will explore both the GCP's graphical user interface and it's command-line interface. You will also deploy a solution from the GCP marketplace without having to manually configure the software, Virtual Machine instances, storage, or network settings. To complete your learning experience, I will provide a quick demo of Projects. Let's get started.

### Video - [Using Google Cloud](https://www.cloudskillsboost.google/course_templates/50/video/523059)

- [YouTube: Using Google Cloud](https://www.youtube.com/watch?v=n26gjF7k3HU)

There are four ways you can interact with Google Cloud, and we'll talk about each in turn. There's the Google Cloud console, Cloud Shell and the Google Cloud CLI, the APIs, and the Cloud Mobile App. The Google Cloud console provides a web-based, graphical user interface that you access through console.cloud.google.com. For example, you can view your virtual machines and their details, as shown on the top. If you prefer to work in a terminal window, the Google Cloud CLI provides the gcloud command-line tool. For example, you can list your virtual machines and their details as shown on the bottom with the "gcloud compute instances list" command. Google Cloud also provides Cloud Shell, which is a browser-based, interactive shell environment for Google Cloud that you can access from the Google Cloud console. Cloud Shell is a temporary virtual machine with 5 GB of persistent disk storage that has the Google Cloud CLI pre-installed. Throughout this course, you will apply what you learn in different labs. These labs will have instructions to use the Google Cloud console, such as, "On the Navigation menu, click Compute Engine > VM instances." Let me dissect these instructions. First, within the console you will click on the icon with the three horizontal lines, which is the Navigation menu, as shown on the left. This opens a menu, as shown on the right. All of the major products and services are listed on this menu. Then, within the menu, hover over "Compute Engine" to open a submenu. Finally, click on "VM instances" on the submenu. You will get more comfortable with these instructions and the console as you work on labs. Now, labs will also use command-line instructions. You will enter these instructions either in Cloud Shell or an SSH terminal by simply copying and pasting them. In some cases, you will have to modify these commands, for example, when choosing a globally unique name for a Cloud Storage bucket. In addition to the Google Cloud CLI, you can also use client libraries that enable you to easily create and manage resources. Google Cloud client libraries expose APIs for two main purposes: App APIs provide access to services, and they are optimized for supported languages, such as Node.js or Python. Admin APIs offer functionality for resource management. For example, you can use admin APIs if you want to build your own automated tools. The Cloud Mobile App is another way to interact with Google Cloud. It allows you to manage Google Cloud services from your Android or iOS device. For example, you can start, stop, and SSH into Compute Engine instances and see logs from each instance. You can also set up customizable graphs showing key metrics such as CPU usage, network usage, requests per second, and server errors. The app even offers alerts and incident management and allows you to get up-to-date billing information for your projects and get billing alerts for projects that are going over budget. You can download the Cloud Mobile App from Google Play or from the App Store.

### Video - [Lab Intro: Working with the Google Cloud Console and Cloud Shell](https://www.cloudskillsboost.google/course_templates/50/video/523060)

- [YouTube: Lab Intro: Working with the Google Cloud Console and Cloud Shell](https://www.youtube.com/watch?v=54x8MTrNo-8)

Slides are great for explaining concepts, but let's apply what we just talked about. In this first lab, you'll explore the GCP peer interface. That is the entry point of the graphical user interface that's called the GCP console. Within the GCP console, you will create a storage bucket in Cloud Storage which is Google's unified object storage. Then you'll repeat the same task using Cloud shell, which is the command line interface in GCP. I encourage you to develop familiarity with both the GCP console, and Cloud shell, and to become comfortable moving back and forth between them.

### Lab - [Working with the Google Cloud Console and Cloud Shell](https://www.cloudskillsboost.google/course_templates/50/labs/523061)

In this lab, you will become familiar with the Google Cloud web-based interface including Console, the  GUI (graphical user interface) environment, and Cloud Shell, the CLI (command line interface) environment.

- [ ] [Working with the Google Cloud Console and Cloud Shell](../labs/Working-with-the-Google-Cloud-Console-and-Cloud-Shell.md)

### Video - [Lab Review: Working with the Google Cloud Console and Cloud Shell](https://www.cloudskillsboost.google/course_templates/50/video/523062)

- [YouTube: Lab Review: Working with the Google Cloud Console and Cloud Shell](https://www.youtube.com/watch?v=o0rm0MXcQzo)

In this lab, you created a Cloud Storage bucket using both the GCP Console and Cloud Shell within GCP. The GCP Console can do things Cloud Shell can't and vice versa. For example, the GCP Console can keep track of the context of your configuration activities. It can use the Cloud API to determine from the current system state, what options are valid, and it can perform repetitive or more leveraged activities on your behalf. Cloud Shell in contrast offers detailed and precise control, and through its commands, a way to script and automate activities. However, don't think of the constant Cloud Shell as alternatives, think about it as one extremely flexible and powerful interface. You can stick around for our lab walkthrough. But remember, that GCP is user interface can change, so your environment might look slightly different. So here we are in the GCP Console and the first thing we're going to do is create a bucket using the GCP Console. So to do that, I'm going to use the navigation menu which is the icon up here in the top left corner, and I'm currently scroll down to Storage which is here, and click on Browser. What we want to do is create a bucket, so I'm going to click the Create bucket. The first thing we need to do is define a name, and now this name needs to be a globally unique name. So you could for example use your Qwiklab's project ID here, so that's what I'll do, copy that and paste it in there. The instructions just say to create, you could also choose a change the default storage class is currently set to multi-regional. We'll talk more about that in a later module. You can control the access to the objects and there are even some advanced settings around encryption. So I'm just going to go ahead and click Create. You can see that this now has created a bucket and here we see the bucket ID or the name. So now we're going to access Cloud Shell. Then what we're going to do this we're going to click this button up here on the right corner, says Activate Cloud Shell, and then it will prompt you to start clutches, so we'll click that as well. You can see that's coming up here. You could actually expand this and open this in a new tab, or you could realigned this to get a little bit more real estate in here. So we created a bucket using the GCP Console, now we're going to repeat the same using Cloud Shell. So I'm going to go ahead and copy the command from the lab instructions and paste it in here. Another command has the bucket name here in brackets and we want to change that. So this again has to be a globally unique name. So what we could do is we could again grab the ID of our project and maybe just add something to it. We could just add -shell to say that this is the one that we created from Cloud Shell. So the command is gsutil, these are the commands for Cloud Storage and mb is the make bucket command. You'll see that it has created that here, and we can see if we navigate in the GCP Console back to buckets, that we now have two buckets in here. So we're able to create both of those. So there are other Cloud Shell features that we can explore here. So while we're in Cloud Shell, we can click these three dots over here and get some more options. One of which is we can upload a file, and if I click that, I'm just present it with my browser, and I could for example, select this text file and click Open. We see that it's being uploaded and now that has finished, and then I can use the ls command to list that file. So here's that file. There's also a read me already in there. Then we could copy that now, that file to the bucket that we have. So there's a command for that also in the lab instructions. So again, we're working with Cloud Storage. So gsutil is going to be the command and CP to copy. We're going to give the name of the file, so MyFile.txt, and then we want to get to that Cloud Storage bucket. So we could choose either of the two buckets we've created. Why don't we choose the one we create a from Cloud Shell, paste it in there, and then it's telling us that it's copying over the files. If we now go into that, we can see that now that file is in there. The file doesn't contain anything, so that's why it is that size. Then we could also go ahead and close Cloud Shell, and do some other activities. Task 5 of the lab goes into creating a persistent state in Cloud Shell. So you could open Cloud Shell and we could list for example, all the variable regions with the G loud command that's listed in there, G Cloud compute regions list, and from these regions we can now select a region and store that in an environment variable. So let's take the command from the lab instructions and for class region equals, and let's say for example I pick the US Central 1 region, could paste that in there, store it, and then I could verify that with the echo command, just running that and it's not telling me that that is stored in there. The other thing we could do is we could expand this a little bit, we could also create a folder in here with the MK direct command, and now we could create a configuration file, and then we can append the environment variable that we just created to to that file. Then we could add another one for example, we could also store our project ID. So I can put that in there, grab my project ID, copy that, and store that in the environment variable, and then I run the command from the lab instructions to also append the value of the project ID to my environment variable and the configuration file. Then I can just verify all of that and make sure that that's been stored. So this gives us a method to create environment variables and easily recreate them as Cloud Shell is cycled. However, you will still need to remember to issue this source command each time Cloud Shell is opened. So let's modify the.profilefile so that the source command is issued automatically anytime a terminal Cloud Shell is opened. So we're going to close and reopen Cloud Shell. So let me do that, close it and then reopen it, and then I'm going to paste the echo command again. We see that it's not outputting anything, so that command is coming out down. So let's modify that.profilefile using nano, and at the end of that file, let's go all the way to the bottom. We'll go into paste in sourceinfraclassconfig, and then we're going to save that file to profile, and then exit. Then let's verify that we are able to get that environment variable, that is project ID. So that's currently not in there, that is because I haven't restarted it, propagates run when I restart, sorry for that. So let me close it, let me reopen it, and then let's verify. There we go. So now we can see that expected value and that's because we edit the d.profilefile. That's it. So we've leveraged in this lab, the GCP Console, we created a Storage bucket, we also created a Storage bucket using Cloud Shell, and then we looked into some features run Cloud Shell in terms of uploading files, than copying those files to the Storage bucket, and even at the end configuring the profile and setting some environment variables. That's the end of the lab.

### Video - [Lab Intro: Infrastructure Preview](https://www.cloudskillsboost.google/course_templates/50/video/523063)

- [YouTube: Lab Intro: Infrastructure Preview](https://www.youtube.com/watch?v=OHqI-dggoPs)

In this lab, you're going to experience the power of GCP Automation by setting up a complete Jenkins Continuous Integration environment using the GCP marketplace. You will then verify that you can manage the service from the Jenkins UI and administer the service from the VM host through SSH. Now, you could accomplish a very similar result through manual configuration in a couple of hours or even days, but in this lab you will see it set up in only a few minutes.

### Lab - [Infrastructure Preview](https://www.cloudskillsboost.google/course_templates/50/labs/523064)

In this lab you will use Marketplace to examine some of the powerful infrastructure features available in GCP. Many of the services that are used automatically in this lab will be explored in detail later in the class.

- [ ] [Infrastructure Preview](../labs/Infrastructure-Preview.md)

### Video - [Lab Review: Infrastructure Preview](https://www.cloudskillsboost.google/course_templates/50/video/523065)

- [YouTube: Lab Review: Infrastructure Preview](https://www.youtube.com/watch?v=HVc4x5twvag)

In this lab, you are able to launch a complete continuous integration solution in a few minutes. You demonstrated that you had user access through the Jenkins UI and that you had administrative access control over Jenkins by using SSH to connect to the VM, where the service is hosted, and by stopping and then restarting the services. Many of the activities that occurred in that lab were nearly transparent, and they use resources and methods that you learn about in the rest of this course. Examples of this include; the acquisition and configuration of a network IP address, the provisioning of a virtual machine instance along with the installation of software on that machine, and the passing of default state information from the environment during the setup process. You can stick around for a lab walk-through, but remember that user interface can change. So you're environment might look slightly different. So here I am in the GCP Console, and the first thing I want to do is navigate to the Marketplace. So up here, I've already clicked on the Navigation menu and Marketplace is pretty much on top. So I'm going click on that. Now, I want to search for Jenkins. Specifically, the one that's certified by Bitnami. So I can just directly paste that in the search address here. Here we go. This is the one I'm looking for. So I'm going to click on that. Now, I can read all about this. There's an overview. It doesn't mean that function Compute Engine, uses a single virtual machine, when it was last updated. It talks about all the packages, the operating system. If I scroll down, I can learn more about the pricing. There's obviously, pricing associate with the VM instance itself. It does not have a usage fee. If it did, that would be displayed here, and you'd be billed for all of that together. There's a standard discharge, and then there's the sustained use discount, which we'll learn more about in a later module. So once I'm happy with all that and I've read through, I can go ahead and click on "Launch on Compute Engine". Now, it's going to present me with an interface here, where I could change the name, the zone, the Machine tab, a lot of other settings that are very similar to configuring a virtual machine. I can again, see all the Software, Terms of Service, the cost one more time. Once I'm ready to go, I can click "I accept the Terms of Services", and click "Deploy". So now, I'm actually in a different interface. This is Deployment Manager, we'll learn about this later in the course series, but the interesting thing now, is I can see the setup process. So there is an actual file here that has all the configuration in a ginger file. There is a VM that's being created. There are two firewall rules that are created. TCP for port 80 and 443. So that's HTTP and HTTPS. I can wait for this machine to now come up. There's also some software configuration. I can again, learn about all the software that is installed here. I can click on the VM instance to get more information about it. We can see the VMs instance is up, the firewalls are up. So the last thing that's happening here is the software is being configured. I can even learn more about that software. Here, I already clicked on that. So these are again, all the different versions that we can get to and engage. Once this is running, this table up here will be populated, all currently pending because this is still being initialized. Here, we can see that the instance is now ready. So there are a couple different things we could do. We have an admin user, as well as a password. So we can copy that. We could click on "Visit The Site", and this is going to open that in a new tab, that's navigating us to the external IP address. It's going to load, let's see it's the starting. It's part of the service itself, it's still getting ready to work. So you can see that the software in the background on the instance is installed, but it also needs to launch. So that itself can take some time too, and now it's up and running. I can put my username in and I can put the password in. I can click "Sign In". Here, I should be asked to customize Jenkins. There'll be some suggested plug-ins that I can install. Once I've done that, I can restart the instance. Deployment Manager and the G Suite Marketplace, will also give you some time some suggests next steps. For example, this password up here, it's just temporary. So we could go change that. The other thing we could do is we could assign a static external IP address so that when you visit the site, it's always going to be the same IP address, and that really helps if you have a DNS setup for this instance. If I go back here, I can click that I want to install the suggested plug-ins. It's going to do that. It's going to tell me where that instance is. I can save and finish, and I can go start using it. They should again now restart service. So here we are. So I can explore this a little bit. I could manage Jenkins itself. There are lots of different actions that I could perform here. I could also now, further administer the service if I go back to the Console. I'm looking at this deployment here. I'm looking Jenkins-1, I could actually SSH now to this instance. So let me click that button. That's going to establish now, an SSH session to the service. I can then actually shut down all the services by copying the command that's in the lab instructions. So let me just paste that in here and run that. If we go back to the Jenkins UI and refresh that page, we'll see that it's gone. That is expected because I have gone ahead and I have restarted that service. So what I can do now, is I stop them, I can now restart it by running the Restart command in here. So let's grab that and paste that in here. Now, the service should come back up. We might have to refresh the page a couple times for that to happen. So let's just wait a couple seconds, refresh it and see if that's service comes back up. My tab name has changed to Start in Jenkins. So it looks like that service is already coming back up right now. We can see that the service is getting ready right now. So at this point, we've completed all the task. I could now go back to the SSH session and exit out of here. Here, we see that Jenkins is back up and running. That's the end of the lab.

### Video - [Demo: Projects](https://www.cloudskillsboost.google/course_templates/50/video/523066)

- [YouTube: Demo: Projects](https://www.youtube.com/watch?v=0Dz0HUw7hcQ)

Let's explore projects which are the key organizer of infrastructure resources and relate these resources to billing accounts. Resources can only be created and consumed within projects in a way that projects isolate related resources from one another. I will demonstrate how to create and delete projects, and switch contexts between projects. Some of these actions cannot be performed in the Qwiklabs environment due to security restrictions. Therefore, I'm going to demonstrate them in my environment. So here I am in the GCP console. You can actually see this is a trial account and you can also create a trial account yourself if you would like to follow along with this. Essentially, what I'm going to do first is go ahead and create a project. So I'm going to click up on my product name up here, and there's this icon up here to create a new project, so let me go click that. Now, the one thing I want to do is, I want to define a project name. So let me just say my new project. You can see that it automatically creates the project ID and project ID is going to be unique versus my name is really not so unique. So let me click "Create" on that. It is now telling me here that is going to create that project. I can follow along with that here in the notification pane. One thing to notice is, when you create a new project, that some of the services that you're going to use may not be initially available. So here, I now have my new project. I could now switch projects. So if I go to my home, for example, I see here the project itself, I could go to the project settings, I could shut that down, or I could switch to a different one. So let me actually change up here to this new project that I created. You go in there and let's follow the process for shutting that down. So I'm going to click on "Shutdown", I wants to make sure that I really want to do that. It's telling me a little bit about what's going to happen when I do this. Specifically, all building in traffic serving will stop, but the shutdown is actually scheduled. So it will take 30 days, and this is in case that you want up maybe undo this. So I need to just retype my project ID, and I can actually copy and paste it in here, and I can click ''Shutdown'', and it should now give me a notification. So here it's telling me when exactly it's going to shut this down, and I can click ''OK'' on that. So now, this is being scheduled for shutdown. So now, I can go back and obviously want to grab in project, it's automatically put me in the sight. Alternatively, if I go home, you'll see that I also have an option up here. It's telling me, hey, you really need to select a project. So lots of different ways to go about. So I could click on that and select a project. Now, I want to show in a second how we can also move switched projects on screen Cloud Shell. So let's actually go ahead and create another project. Let's just call this My Second Project. We can create that as well. They'll start in the background for us. So what I want to do now, as I said I want to go to Cloud Shell. So if I go up here on the right corner, it's Activate Cloud Shell. I'll just click on that. It doesn't ask me if you want to start Cloud Shell because I've already been using Cloud Shell with this user. So it's also telling me that I haven't used my Cloud Shell in awhile, so it has to unarchive my disk and that's going to take a little bit of time. But once that's up, we can actually go use gcloud config list command and we can paste it in, and it's going to give us more information about the configuration that we currently have. That will include the project that we currently have selected. We can actually see the project right here. This is the project I'm working on right now. So if I paste in, I automatically copy that when I clicked on it. So I want to instead type in here gcloud config list. So here, we get some more information. I can also use the grep command in here to directly got my project and there we see this is the project that we're currently using. I could actually now even changed the focus of my GCP console to this new project. You'll see if I run this command again, my focus of Cloud Shell is still focused on this other project that I had. So one thing we could do now, is we could store the project ID maybe in an environment variable and then we could maybe set it so we could swap back and forth. So let me get the project ID, it's right here. I'm going to maybe just store that in an environment variable. Let's just call that my project ID1. So let me grab the project ID, copy that, paste it in there. So now, I have that stored and now I could use the gcloud config set project to define an action to change the project ID. Now, you can see that I have that other project ID listed here. So I can actually see that, and I could also now use it the same gcloud config list command and grep the project, and you'll see that now I'm working with different project. That's how easy it is to create and delete projects, and switch contexts between projects.

### Quiz - [Quiz: Interacting with Google Cloud](https://www.cloudskillsboost.google/course_templates/50/quizzes/523067)

#### Quiz 1.

> [!important]
> **What is the difference between the Google Cloud Console and Cloud Shell?**
>
> - [ ] Cloud Shell is a command-line tool, while the Cloud Console is a graphical user interface
> - [ ] There is no difference as these tools are 100% identical.
> - [ ] The Cloud Console is a command-line tool, while Cloud Shell is a graphical user interface
> - [ ] Cloud Shell is a locally installed tool, while the Cloud Console is a temporary virtual machine.

#### Quiz 2.

> [!important]
> **Which of the following does not allow you to interact with Google Cloud?**
>
> - [ ] REST-based API
> - [ ] Google Cloud Console
> - [ ] Cloud Explorer
> - [ ] Cloud Shell

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/50/video/523068)

- [YouTube: Module Review](https://www.youtube.com/watch?v=h5pJ4L38tj4)

In this module, we looked at how to use GCP ,which you got to experience firsthand in two short labs. I also gave a demonstration of how to use projects, which are the key organizer of infrastructure resources. Now that you can interact with GCP, it's time to explore two of the foundational components of GCP's infrastructure, virtual networks and virtual machines. So, what are you waiting for? Move on to the next module to learn more.

## Virtual Networks

Create VPC networks and other networking objects

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/50/video/523069)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=8sCchuVVPUA)

In this module, we will be covering virtual networks. GCP uses a software defined network, that is built on a global fiber infrastructure. This infrastructure makes GCP one of the world's largest and fastest networks. Thinking about resources as services instead of as hardware, will help you understand the options that are available, and their behavior. In this module, we start by introducing Virtual Private Cloud or VPC, which is Google's managed networking functionality, for Euro Cloud Platform resources. Then, we dissect networking into its fundamental components. Which are projects, networks, subnetworks, IP addresses, routes, and firewall rules, along with network pricing. Next, you will explore Google Cloud's network structure in a lab, by creating networks of many different varieties, and exploring the network relationships between them. After that, we will look at common network designs. This map represents Google Cloud. On a high level, Google Cloud consists of regions, which are the icons in blue, points of presence or PoPs, which are the dots in blue, a global private network, which is represented by the blue lines, and services. A region is a specific geographical location where you can run your resources. This map shows several regions that are currently operating, as well as future regions. Regions indicated with blue icons have three zones. Iowa is an exception, where the region called US-Central1 has four zones: US-Central1-A, US-Central1-B, US-Central1-C, and US-Central1-F. For up-to-date information on regions and zones, please refer to the documentation in the slides. The PoPs, are where Google's network is connected to the rest of the internet. Google Cloud can bring its traffic closer to its peers, because it operates an extensive global network of interconnection points. This reduces costs and provides users with a better experience. The network connects regions and PoPs, and is composed of a global network of fiber optic cables with several submarine cable investments. For more information about Google's networking infrastructure, please refer to these slides. Let's start by talking about GCPs network, and specifically Virtual Private Cloud or VPC.

### Video - [Virtual Private Cloud](https://www.cloudskillsboost.google/course_templates/50/video/523070)

- [YouTube: Virtual Private Cloud](https://www.youtube.com/watch?v=KTDHvLFvwrw)

With GCP, you can provision your GCP resources, connect them to each other, and isolate them from each other in a Virtual Private Cloud. You can also define fine-grained network and policies within GCP and between GCP and On-premises or other public Clouds. Essentially, VPC is a comprehensive set of Google managed networking objects, which we will explore in detail throughout this module. Let me give you a high-level overview of these objects. Projects are going to encompass every single service that you use including networks. Networks come in three different flavors; default, auto mode, and custom mode. Subnetworks allow you to divide or segregate your environment. Regions in zones represents Google's datacenters and they provide continuous Data Protection and high availability. VPC provides IP addresses for internal and external use along with granular IP address range selections. As for virtual machines, in this module, we will focus on configuring VM instances from a networking perspective. We'll also go over routes and firewall rules.

### Video - [Projects, networks, and subnetworks](https://www.cloudskillsboost.google/course_templates/50/video/523071)

- [YouTube: Projects, networks, and subnetworks](https://www.youtube.com/watch?v=EbaXu92LYvU)

Let's start exploring the VPC objects by looking at projects, networks, and subnetworks. Projects are the key organizer of infrastructure resources in Google Cloud. A project associates objects and services with billing. Now, it's unique that projects actually contain entire networks. The default quota for each project is 15 networks, but you can simply request additional quota using the Google Cloud console. These networks can be shared with other projects, or they can be peered with networks in other projects, both of which we will cover later in the Architecting with Google Compute Engine course series. These networks do not have IP ranges but are simply a construct of all of the individual IP addresses and services within that network. Google Cloud's networks are global, spanning all available regions across the world that I showed earlier. So, you can have one network that literally exists anywhere in the world—Asia, Europe, Americas—all simultaneously. Inside a network, you can segregate your resources with regional subnetworks. I just mentioned that there are different types of networks: default, auto, and custom. Let's explore these types of networks in more detail. Every project is provided with a default VPC network with preset subnets and firewall rules. Specifically, a subnet is allocated for each region with non-overlapping CIDR blocks and firewall rules that allow ingress traffic for ICMP, RDP, and SSH traffic from anywhere, as well as ingress traffic from within the default network for all protocols and ports. In an auto mode network, one subnet from each region is automatically created within it. The default network is actually an auto mode network. These automatically created subnets use a set of predefined IP ranges with a /20 mask that can be expanded to /16. All of these subnets fit within the 10.128.0.0/9 CIDR block. Therefore, as new Google Cloud regions become available, new subnets in those regions are automatically added to auto mode networks using an IP range from that block. A custom mode network does not automatically create subnets. This type of network provides you with complete control over its subnets and IP ranges. You decide which subnets to create, in regions you choose, and using IP ranges you specify. These IP ranges cannot overlap between subnets of the same network. Now, you can convert an auto mode network to a custom mode network to take advantage of the control that custom mode networks provide. However, this conversion is one way, meaning that custom mode networks cannot be changed to auto mode networks. So, carefully review the considerations for auto mode networks to help you decide which type of network meets your needs. Google Cloud now supports IPv6 in a custom VPC network mode, for example you can configure IPv6 addressing on 'dual-stack' VM instances running both IPv4 and IPv6. You can learn a lot more about IPv6 in the "Networking in Google Cloud" course. On this slide, we have an example of a project that contains 5 networks. All of these networks span multiple regions across the world, as you can see on the right. Each network contains separate virtual machines: A, B, C, and D. Because VMs A and B are in the same network, network 1, they can communicate using their internal IP addresses, even though they are in different regions. Essentially, your virtual machines, even if they exist in different locations across the world, take advantage of Google's global fiber network. Those virtual machines appear as though they're sitting in the same rack when it comes to a network configuration protocol. VMs C and D, however, are not in the same network. Therefore, by default, these VMs must communicate using their external IP addresses, even though they are in the same region. The traffic between VMs C and D isn't actually touching the public internet, but is going through the Google Edge routers. This has different billing and security ramifications that we will explore later. Because VM instances within a VPC network can communicate privately on a global scale, a single VPN can securely connect your on-premises network to your Google Cloud network, as shown in this diagram. Even though the two VM instances are in separate regions (us-west1 and us-east1), they leverage Google's private network to communicate between each other and to an on-premises network through a VPN gateway. This reduces cost and network management complexity. I mentioned that subnetworks work on a regional scale. Because a region contains several zones, subnetworks can cross zones. This slide has a region, region 1, with two zones, zones A and B. Subnetworks can extend across these zones within the same region, such as, subnet-1. The subnet is simply an IP address range, and you can use IP addresses within that range. Notice that the first and second addresses in the range, .0 and .1, are reserved for the network and the subnet's gateway, respectively. This makes the first and second available addresses .2 and .3, which are assigned to the VM instances. The other reserved addresses in every subnet are the second-to-last address in the range and the last address, which is reserved as the "broadcast" address. To summarize, every subnet has four reserved IP addresses in its primary IP range. Now, even though the two virtual machines in this example are in different zones, they still communicate with each other using the same subnet IP address. This means that a single firewall rule can be applied to both VMs, even though they are in different zones. Speaking of IP addresses of a subnet, Google Cloud VPCs let you increase the IP address space of any subnets without any workload shutdown or downtime. This diagram illustrates a network with subnets that have different subnet masks, allowing for more instances in some subnets than others. This gives you flexibility and growth options to meet your needs, but there are some things to remember: The new subnet must not overlap with other subnets in the same VPC network in any region. Each IP range for all subnets in a VPC network must be a unique valid CIDR block. Also, the new subnet IP address ranges are regional internal IP addresses and have to fall within valid IP ranges. Subnet ranges cannot match, be narrower, or be broader than a restricted range. Subnet ranges cannot span a valid RFC range and a privately used public IP address range. Subnet ranges cannot span multiple RFC ranges. The new network range must be larger than the original, which means the prefix length value must be a smaller number. In other words, you cannot undo an expansion. Now, auto mode subnets start with a /20 IP range. They can be expanded to a /16 IP range, but no larger. Alternatively, you can convert the auto mode subnetwork to a custom mode subnetwork to increase the IP range further. Also, avoid creating large subnets. Overly large subnets are more likely to cause CIDR range collisions when using Multiple Network Interfaces and VPC Network Peering, or when configuring a VPN or other connections to an on-premises network. Therefore, do not scale your subnet beyond what you actually need.

### Video - [Demo: Expand a Subnet](https://www.cloudskillsboost.google/course_templates/50/video/523072)

- [YouTube: Demo: Expand a Subnet](https://www.youtube.com/watch?v=-PmauCDjzyA)

Let me show you how to expand a custom subnet within GCP. I've already created a custom subnet with a slash 29 mask. A slash 29 mask provides you with eight addresses. But of those, four are reserved by GCP, which leaves you with another four for your VM instances. Let's try to create another VM instance in this subnet. So here we are on the GCP console, and I have my four instances, and if I go into the network interface details here, you can see that these are part of a network and I have a subnet here, and if I drilled further into that you can see that I currently have a slash 29. So let's go back and try to create that other instance. Just going to click on Create Instance. I don't need a very large machine, I'm okay with the micro, and let's hit Create. Ideally, we should be getting an error now about the fact that the IP space should have been exhausted, so we're just going to wait for that. You can also follow this along in the notification pane up here and see that it is trying to create that right now. So we're going to wait for that and see if we get an error here in a second. Once we have that, we're going to go ahead and expand the subnet. So here we can see that the instance creation has failed, I can hover over this and it's just telling me that the IP space of that subnet has been exhausted just as expected. We actually have a "Retry" button here as well as a notification pane. We're going to try to use that in a second once we expand the subnet to recreate that instance. Now, what's important is to note that all of these four instances are currently running. So we're not going to take any of these town during the subnet expansion. Now to expand the subnet, I could go to VPC networks through the navigation menu, or I can go back by clicking on nic0 here directly through the network interface details. So the subnet, this is what I want to change, so let me click the "Edit" button, and lets expand this all the way to a slash 23, and this is going to allow a lot of instances, actually over 500 instances. We're going to wait for this to update, and then we're going to head back and try to recreate that instance. So we can also follow this process along right here. It's still saving, so we're going to just hang on tight here. It should just take a couple seconds. All right. We see it's complete. Now, I still have that "Retry" button here to recreate that instance. So let me actually click that, and I can head back to Compute Engine to see if that is going to succeed. So here we are, instance five, it's being staged and will soon begin running. Let's see if this works out. We can see that already has now an internal IP address allocated now that we've expanded the subnet itself, and if I refresh this we can see that the instance is now created. That's how easy it is to expand a subnet in GCP without any workload shutdown or downtime

### Video - [IP addresses](https://www.cloudskillsboost.google/course_templates/50/video/523073)

- [YouTube: IP addresses](https://www.youtube.com/watch?v=H1wYg3GPvIo)

Person: Now that we've covered Google Cloud Networks at a high level, let's go deeper by exploring the IP addresses. In Google Cloud, each virtual machine can have two IP addresses assigned. One of them is an internal IP address, which is going to be assigned via DHCP internally. Every VM that starts up and any service that depends on virtual machines gets an internal IP address. An example of such services are App Engine and Google Kubernetes Engine, which are explored in other courses. When you create a VM in Google Cloud, its symbolic name is registered with an internal DNS service that translates the name to an internal IP address. The DNS is scoped to the network, so it can translate web URLs and VM names of hosts in the same network, but it can't translate host names from VMs in a different network. The other IP address is the external IP address, but this one is optional. You can assign an external IP address if your device or machine is externally facing. That external IP address can be assigned from a pool, making it ephemeral, or it can be assigned from a reserved external IP address, making it static. If you reserve static external IP address and do not assign it to a resource, such as a VM instance or a forwarding rule, you are charged at a higher rate than for static and ephemeral external IP addresses that are in use. You can use your own publicly routable IP address prefixes as Google Cloud external IP addresses and advertise them on the Internet. In order to be eligible, you must own and bring a /24 block or larger.

### Video - [Demo: Internal and external IP](https://www.cloudskillsboost.google/course_templates/50/video/523074)

- [YouTube: Demo: Internal and external IP](https://www.youtube.com/watch?v=UwLrs2VkfQA)

I just mentioned that VMs can have internal and external IP addresses. Let's explore this in the GCP Console. So here I am on the Compute Engine page. What I'm going to do is just create a VM and walk through the process of choosing your internal and external IP address. So let me click Create. I can leave the name. You have obviously a selection of regions and zones you can choose, but I want to focus on the IP addresses. So let me go down to this option, expand management security networking sole tenancy. Let's focus on networking. Here at the network interface, I'm going to click the pencil icon. I could choose between two different networks. So if I had different networks, I could choose between them. That's not the case here. Then I have the primary or internal IP and external IP. So if we look at those options, you can see that I can use an ephemeral address either the one that's created automatically or I could custom select one. So within the range that I have here I could just type IP address. I could also reserve a static internal IP address. This is great if you want to keep that IP address for a longer time and we have similar options with the external IP address. But one of the big differences is that you can also just select none. So as I mentioned your instances don't need to have an external IP address. So let's just leave this as ephemeral. By the way, with the slash 20 here we have a lot of space in this IP range over 4,000 addresses. So we could definitely have that many instances. There are also limits of how many instances you can have per network. As of this recording is actually 15,000. So do keep that in mind you might have a very large IP range but that doesn't mean that you actually can create that many instances. That's a quota. There may also be actual limitations on physical hardware that's even available within a specific region or zone. So let me go ahead and create this instance. We're going to keep an eye on the internal and as well as the external IP address. Once the instance is created. Then we're also going to stop and start the instance to see if any of the IP address has changed. So here we can see the internal IP address. So that is definitely within that space that we just looked at. The external IP address obviously is within Google strange here and we could have reserved that, but this is an fMRL one. So let's actually test this out. I'm going to select the instance. I'm going to stop it. So it's telling me that it doesn't move in 90 seconds that might be forced. So if you had any shutdown scripts in here you want to make sure that they can actually complete within 90 seconds. So let's run through that. Remember this external IP address that we currently have here as well as the internal IP address. So this is going to take it's time now. We can also click Refresh to keep an eye on this. But this will take about 90 seconds, and that's just to give your shutdown script enough time to perform any task to gracefully shut down this instance. So here we are, we can see the instance is stopped, the external IP address is gone. So now we're just going to startup that instance again. It's going to tell us it we're going to be build while it's running, that's fine. You can see that the internal IP address remained the same wildest instance stopped. So that has actually stayed for the time being. Now, while this instance spins up which we can by the way monitor the progress over here, we should see that we should be getting a new external IP address now because that was an ephemeral address. So here we can see the instance has started back up and we can see that the external IP address has changed. This demonstrates that every VM needs an internal IP address but external IP addresses are optional and by default, there are ephemeral.

### Video - [Mapping IP addresses](https://www.cloudskillsboost.google/course_templates/50/video/523075)

- [YouTube: Mapping IP addresses](https://www.youtube.com/watch?v=msHZKpYBnDQ)

Regardless of whether you use an ephemeral or static IP address, the external address is unknown to the OS of the VM. The external IP address is mapped to the VM's internal address transparently by VPC. I am illustrating this here by running ifconfig within a VM in Google Cloud, which only returns the internal IP address. Let's explore this further by looking at DNS resolution for both internal and external addresses. Let's start with internal addresses. Google Cloud has two types of internal DNS names, Zonal and Global (project wide) DNS. In general, Google strongly recommends using zonal DNS because it offers higher reliability guarantees by isolating failures in the DNS registration to individual zones. Each instance has a hostname that can be resolved to an internal IP address. This hostname is the same as the instance name. There is also an internal fully qualified domain name, or FQDN, for an instance that uses the format shown on the slide. If you delete and recreate an instance, the internal IP address can change. This change can disrupt connections from other Compute Engine resources, which must obtain the new IP address before they can connect again. However, the DNS name always points to a specific instance, no matter what the internal IP address is. Each instance has a metadata server that also acts as a DNS resolver for that instance. The metadata server handles all DNS queries for local network resources and routes all other queries to Google's public DNS servers for public name resolution. I previously mentioned that an instance is not aware of any external IP address assigned to it. Instead, the network stores a lookup table that matches external IP addresses with the internal IP addresses of the relevant instances. Now, let's look at external addresses. Instances with external IP addresses can allow connections from hosts outside of the project. Users can do so directly using the external IP address. Public DNS records pointing to instances are not published automatically; however, admins can publish these using existing DNS servers. Domain name servers can be hosted on Google Cloud, using Cloud DNS. This is a managed service that is definitely worth considering, so let's explore it in more detail. Cloud DNS is a scalable, reliable, and managed authoritative Domain Name System, or DNS, service running on the same infrastructure as Google. Cloud DNS translates requests for domain names like google.com into IP addresses. Cloud DNS uses Google's global network of Anycast name servers to serve your DNS zones from redundant locations around the world, providing lower latency and high availability for your users. High availability is very important because if you can't look up a domain name, the internet might as well be down. That's why Google Cloud offers a 100% uptime Service Level Agreement, or SLA, for domains configured in Cloud DNS. Cloud DNS lets you create and update millions of DNS records without the burden of managing your own DNS servers and software. Instead, you use a simple user interface, command-line interface, or API. Another networking feature of Google Cloud is Alias IP Ranges. Alias IP Ranges let you assign a range of internal IP addresses as an alias to a virtual machine's network interface. This is useful if you have multiple services running on a VM, and you want to assign a different IP address to each service. In essence, you can configure multiple IP addresses, representing containers or applications hosted in a VM, without having to define a separate network interface. You just draw the alias IP range from the local subnet's primary or secondary CIDR ranges. This diagram provides a basic illustration of primary and secondary CIDR ranges and VM alias IP ranges.

### Document - [IP addresses for default domains](https://www.cloudskillsboost.google/course_templates/50/documents/523076)

### Video - [Routes and firewall rules](https://www.cloudskillsboost.google/course_templates/50/video/523077)

- [YouTube: Routes and firewall rules](https://www.youtube.com/watch?v=3TYssEfEFlw)

So far you've learned about projects, networks, subnetworks, and IP addresses. Let's use what you learned to understand how GCP routes traffic. By default, every network has routes that let instances in a network send traffic directly to each other, even across subnets. In addition, every network has a default route that directs packets to destinations that are outside the network. Although these routes cover most of your normal routing needs, you can also create special routes that overwrite these routes. Just creating a route does not ensure that your packet will be received by the specified next top. Firewall rules must also allow the packet. The default network has pre-configured firewall rules that allow all instances in the network to talk with each other. Manually created networks do not have such rules, so you must create them, as you will experience in the first lab. Routes match packets by destination IP addresses. However, no traffic will flow without also matching a firewall rule. A route is created when a network is created, enabling traffic delivery from "anywhere". Also, a route is created when a subnet is created. This is what enables VMs on the same network to communicate. This slide shows a simplified routing table, but let's look at this in more detail. Each route in the Routes collection may apply to one or more instances. A route applies to an instance if the network and instance tags match. If the network matches and there are no instance tags specified, the route applies to all instances in that network. Compute Engine then uses the Routes collection to create individual read-only routing tables for each instance. This diagram shows a massively scalable virtual router at the core of each network. Every virtual machine instance in the network is directly connected to this router, and all packets leaving a virtual machine instance are first handled at this layer before they are forwarded to their next hop. The virtual network router selects the next hop for a packet by consulting the routing table for that instance. GCP firewall rules protect you virtual machine instances from unapproved connections, both inbound and outbound, known as ingress and egress, respectively. Essentially, every VPC network functions as a distributed firewall. Although firewall rules are applied to the network as a whole, connections are allowed or denied at the instance level. You can think of the firewall as existing not only between your instances and other networks, but between individual instances within the same network. GCP firewall rules are stateful. This means that if a connection is allowed between a source and a target or a target at a destination, all subsequent traffic in either direction will be allowed. In other words, firewall rules allow bidirectional communication once a session is established. Also, if for some reason, all firewall rules in a network are deleted, there is still an implied "Deny all" ingress rule and an implied "Allow all" egress rule for the network. You can express your desired firewall configuration as a set of firewall rules. Conceptually, a firewall rule is composed of the following parameters: The direction of the rule. Inbound connections are matched against ingress rules only, and outbound connections are matched against egress rules only. The source of the connection for ingress packets, or the destination of the connection for egress packets. The protocol and port of the connection, where any rule can be restricted to apply to specific protocols only or specific combinations of protocols and ports only. The action of the rule, which is to allow or deny packets that match the direction, protocol, port, and source or destination of the rule. The priority of the rule, which governs the order in which rules are evaluated. The first matching rule is applied. The rule assignment. By default, all rules are assigned to all instances, but you can assign certain rules to certain instances only. For more information on firewall rule components, please refer to the links section of this video. Let's look at some GCP firewall use cases for both egress and ingress. Egress firewall rules control outgoing connections originated inside your GCP network. Egress allow rules allow outbound connections that match specific protocol, ports, and IP addresses. Egress deny rules prevent instances from initiating connections that match non-permitted port, protocol, and IP range combinations. For egress firewall rules, destinations to which a rule applies may be specified using IP CIDR ranges. Specifically, you can use the destination ranges to protect from undesired connections initiated by a VM instance towards an external host, as shown on the left. You can also use destination ranges to prevent undesired connections from internal VM instances to specific GCP CIDR ranges. This is illustrated in the middle, where a VM in a specific subnet is shown attempting to connect inappropriately to another VM within the same network. Ingress firewall rules protect against incoming connections to the instance from any source. Ingress allow rules allow specific protocol, ports, and IP ranges to connect in. The firewall prevents instances from receiving connections on non-permitted ports and protocols. Rules can be restricted to only affect particular sources. Source CIDR ranges can be used to protect an instance from undesired connections coming either from external networks or from GCP IP ranges. This diagram illustrates a VM receiving a connection from an external address, and another VM receiving a connection from a VM within the same network. You can control ingress connections from a VM instance by constructing inbound connection conditions using source CIDR ranges, protocols, or ports.

### Video - [Pricing](https://www.cloudskillsboost.google/course_templates/50/video/523078)

- [YouTube: Pricing](https://www.youtube.com/watch?v=uYMxIjUPqbo)

Before you apply what you just learned, let's talk about network pricing. It is important that you understand the circumstances in which you are built for GCP's network. This table, is from the Compute Engine documentation and it lists the price of each traffic type. First of all, egress or traffic coming into GCP's network is not charged, unless there is a resource, such as a load balancer that is processing egress traffic. Responses to request account as egress and are charged. The rest of this table, lists egress or traffic leaving a virtual machine. Egress traffic to the same zone, is not charged as long as that egress is through the internal IP address of an instance. Also egress traffic to Google products like YouTube, maps, drive, or traffic to a different GCP service within the same region, is not charged for. However, there is a charge for egress between zones in the same region, egress within a zone, if the traffic is through the external IP address of an instance, and egress between regions. As for the difference in egress traffic to the same zone, Compute Engine cannot determine the zone of a virtual machine through the external IP address. Therefore, this traffic is treated like egress between zones in the same region. Also there are some exceptions and pricing can always change. So refer to the documentation in the links section of these slides. Now, you are charged for static and ephemeral external IP addresses. This table, represents the external IP pricing for us-central1 as of this recording. You can see that if you reserve a static external IP address and do not assign it to a resource, such as a VM instance or a forwarding rule, you are charged at a higher rate and for static and ephemeral external IP addresses that are in use. Also external IP addresses on preemptible VMs, have a lower charge than for standard VM instances. Remember, pricing can always change, so please refer to the documentation link in the slides. Also I recommend using the GCP pricing calculator to estimate the cost of a collection of resources, because each GCP service has its own pricing model. The pricing calculator is a web-based tool, that you use to specify the expected consumption of certain services and resources, and it then provides you with an estimated cost. For example, you can specify a specific instance type, in a specific region along with 100 gigabytes of monthly egress traffic to Americas and EMEA. The pricing calculator then returns the total estimated cost. You can adjust the currency and time frame to meet your needs, and when you finish, you can e-mail the estimate or save it to a specific URL for future reference. To use the pricing calculator today, refer to the link in the slides.

### Video - [Lab Intro: VPC Networking](https://www.cloudskillsboost.google/course_templates/50/video/523079)

- [YouTube: Lab Intro: VPC Networking](https://www.youtube.com/watch?v=EtZBPjb-cD4)

Let's apply some of the network features we just discussed in a lab. In this lab, you create an auto mode VPC network with firewall rules and to VM instances. Then you convert the auto mode network to a custom mode network and create other custom mode networks as shown in this network diagram. You also explore the connectivity across networks.

### Lab - [VPC Networking](https://www.cloudskillsboost.google/course_templates/50/labs/523080)

In this lab, you create several VPC networks and VM instances and test connectivity across networks.

- [ ] [VPC Networking](../labs/VPC-Networking.md)

### Video - [Lab Review: VPC Networking](https://www.cloudskillsboost.google/course_templates/50/video/523081)

- [YouTube: Lab Review: VPC Networking](https://www.youtube.com/watch?v=UjlYPCDJM30)

In this lab, you explore the default network, and determine that you cannot create VM instances without a VPC network. So you created a new auto mode VPC network, with subnets, roots, firewall rules, and two VM instances, and tested connectivity for those VM instances. Because auto mode networks aren't recommended for production, you converted the auto mode network to a custom mode network. Next, you create two more custom mode VPC networks with firewall rules and VM instances using the GCP console, and the GCloud command line. Then you test the connectivity across VPC networks, which worked when you pinged external IP addresses, but not when you pinged internal IP addresses. VPC networks are, by default, isolated private networking domains. Therefore, no internal IP address communication is allowed between networks, unless you set up mechanisms such as VPC peering, or a VPN connection. You can stay for a lab walk through. But remember, that GCP's user interface can change, so your environment might look slightly different. All right. So here I am in the GCP console. The first thing I'm going to do is I'm just going to explore the default network. So if I, on the left-hand, side click on the navigation menu, and scroll down to VPC network, we will see that this project has a default network. Every project has a default network. That is unless you have an organizational policy that prevents this default network from being created. But essentially, all the different projects that used through Qwiklabs will always have this. So in here, we can see we have a different subnet in each of the different regions. All of these are private IP addresses. I can also go to the routes, and these are established automatically with the networks. So we can see routes between subnets, as well as to the default route, to the Internet. We can even look at the firewall rules. The default network comes with some preset firewall rules to allow ICMP traffic from anywhere. RDP traffic, as well as SSH. Then also, all protocols imports within the network. So this is the range of the network. So we also allow all traffic from within the network itself. So let's go ahead and let's actually delete these firewall rules. I can just check them all right here and delete them. Let's just assume that we want to get rid of everything that's been created for us, and just create our own network instead. So I'm going to go ahead and delete these. I can look at the status up here. We can see that all four are being deleted. It'll update as each as being deleted. Once that is done which is now, I can head to the network, select the default network, and we're also just going to delete that entire network. Once we delete this network, we should see that there should be no routes without a network because there's no use case for them. So let's just wait for the network to be deleted and then we'll verify that. So we can, again, see the progress bar up here, that's deleting, you can also hit refresh, and this should just take a couple seconds. You can see that as I'm refreshing, some of the subnets are disappearing. It's actually just deleting them all the subnets first, and then it's getting rid of the network as a whole, because the network is really nothing else than just a combination of subnets. So all these subnets have to be deleted. There we go. They're all gone now. Now, it's just the network itself that is remaining. If I go to routes, we should see that all the routes already gone, because without the subnets, there's really no need for the routes. If I go back to the network, we should see that any moment now the network itself also disappears. There we go. All right. So without a VPC network now, we shouldn't be able to create any VM instances, containers, or app engine application. Let's actually verify that. I'm going to go to the navigation menu, go to compute engine, and let's just try to create an instance, just going to click create. I'm going to leave everything as its default. If I go actually under networking, we should see that it's going to complain here. If I click on networking, that actually doesn't have a local network available. But let's just click create and see what happens, and it does indeed give us an error, and point out the fact that this tab has an issue. So we clearly cannot create an instance, because again, these instances live in networks, and without a network, we can't create it. So let's hit cancel, and what we're going to do now is we're going to create our own auto mode network. So I'm going to head back to VPC networks. You can pin, by the way, the services. So I'm just going to pin VPC network, compute engine, because we're going to be going back and forth between these. Then within VPC network, we're just now going to create our own network. I can give it a name. I'm going to use the same name that I have in lab instructions, which is My Network. Now I have the option of creating a custom or an automatic. Let's start off by creating an automatic network. So that's going to preset all the distance subnets for us in all the different regions that are available. You can scroll through those and see those all in here. They have a preset to side arrange. You can expand that side arrange later. But again, as an auto network, you don't define the actual IP address range. There are also firewall rules that are available. What's interesting here is you see that there's a deny-all ingress and allow all ingress firewall rule. So these are here by default, and they're actually. You can't even uncheck them. So these are actually with all networks that you create, and you can see that this has the highest party integer, which really means it's a lowest priority. So by default, all ingress traffic is denied, and all ingress traffic is allowed. Unless we create other firewalls to see differently. So if I check all these boxes, we're now allowing ingress traffic for these IP ranges, and these protocols imports. So let's go ahead and click create, and we're going to wait for that network to be created. Then we're going to look at the IP addresses for two of the different regions, and we're going to create instances in those regions, and verify that it's taking those IP addresses. So here, you can see the subnets already all populated here. I can monitor the progress also up here, but this is really done any second now. I'm actually going to start heading over to compute engine, and to create our instances. So let's click create. I'm going to give it a name mynet-us-vm. This is going to be in your central one, specifically, the Zone C. I don't really need a big machine. We're just doing some testing here. So let me just create a micro that reduces the cost a little bit, and I'm going to now click create. Then we're going to repeat. I can close this panel over here. The same workflow and create an instance in Europe. So I'm going to grab the name from the lab instructions for that. I'm going to select the Europe West One region, specifically the Zone 1C. Again, a micro machine which is just a shared-core, and click create for that as well. We can see the US Central 1C machine is already up. We also see the internal IP address that has been provided. Again, there are some reserved IP addresses. The dot zero is reserved as well as the dot one. So in both of these ranges, the dot two is the first available address. Now, we can verify that these are part of the right subnet, if I click on nic0, I go to the network interface details. Here, we can see it's part of the sub-network. Now the sub-network, in this case, has the same name as the network because this is an auto network. Here, we can see that it's part of this range. So 1012800/20. Let's verify that, and that is correct. We are in there with a dot two, and let's verify that the other should be now a 10132.00/20. So again, click on nic0, go to the sub-network, and we can see that's true. You can also see here that this address is reserved for the gateway. All right. So that way, the dot two was really the first usable address within that range. So now, these are on the same network. So let's verify some connectivity between those. I'm going to grab the internal IP address of mynet-eu-vm, just copy that, and then we're going to SSH too this other instance. So again, these instances are in two separate regions but in the same network. So we should be able to ping these addresses now. So if I ping three times using the internal address, then we can see that this works. This works because we have that allow internal firewall rule that we selected earlier. I can actually repeat the same by using the name of the instance. You can see that it's taking that name. It's actually has, here, the fully qualified domain name, and it's just using the IP address for that. So VPC networks have an internal DNS service that allows you to address instances by that DNS names, instead of their internal IP addresses. That's very useful because, well, this internal IP address could change, right? But the name is not going to change. So it's always good to be aware of that, that you can use the fully qualified domain name to ping those. All right. Now we can try this whole thing the other way round. Let me exit this instance, grab the internal IP address of the instance in the US, and SSH to the instance in Europe. We're also going to ping the internal IP address here. We can see that works. We could even now try to ping the external IP address. So that's 34, in my case, 671818, and that works as well. The reason that I'm able to ping the external is because I have firewall rule that allows ICMP externally. I can verify those again. If I click on the network interface details, here I can see all of the firewall rules, and what filters they have, and what protocols, and ports. So this all works fine, and let's assume that this workflow has worked for us but now we have decided that we want to convert the auto mode network that we have to a custom mode network. So let's go ahead and do that. We're going to go to "VPC networks", and we're going to click on "my network", and then we're going to click on "edit", and we're going to change this subnet creation mode from auto to custom, and hit "save". So now we can navigate back. You can see that this is in progress up here. The mode still says "auto". We could have also flipped that here. Let's wait for that to be refreshed, and now we can see that this subnetwork is now a custom subnet. So let's say that this has worked so far, and now we've realized that we need a couple more networks. There's a network diagram in the lab that has two other networks, as well as some instances and everything. So let's go ahead and create those. So now we're going to go to "create a VPC network". We're going to create the management network, and rather than starting with automatic and converting, we're just going to start with the custom net. For that we have to define each of these subnets. The minimum information we need to provide is a name, the region, so let's select "us-central1", and then the IP address range, and then can click "done". Now I can add, if I wanted to, another subnet. But the other thing that's very interesting about this is, I'm creating this right now through the GCP console but you can also create networks, as well as subnets, from the command line using Cloud Shell. If I click down here on command line, I'm actually provided with the commands to do that. The first one just creates the network itself. You don't have to use the project flag in here. So we could just say G Cloud compute, networks create, the name of the network, and the fact that this subnet is a custom mode. Similarly then, we create these subnets which is "networks subnets create" the name of the subnets, add the subnet itself, the name of the network, the region, and the range. So again, that's the sort of minimal information. Let's just click close and "create". We'll create the other one from the command line. So it's creating that network, and in parallel I can go and now activate Cloud Shell by clicking up here in the right corner. Yes, I want to start using Cloud Shell. I'm just going to make this a little bit bigger, and once this is up, we're going to use those commands that we just saw to create first a network, and this is going to be the privatenet, which is also of the mode custom. Once we have that, we're going to create two subnets within that network. So you can see in the console that the other network was created. Privatenet, is being created right now here, and once that is ready we can add the two subnets to that. So there we go. There is the subnet. It's also telling us this is a new network. You don't have any firewall rules, here are some commands if you want to create some firewall rules. We'll do that in a second. Let's just create these subnets in here. So first we're going to create one in the US, and then we're also going to create one in Europe. If you wanted to speed this up you could actually launch another Cloud Shell session. Now that the network is up, you could create these subnets in parallel. But we're just going to wait for this to complete and then we'll paste that command in there. You can monitor all of this in a console. If we click "refresh," there we see it. It's also completed. It just returns, I've done exactly what you told me. Let's create the other one. It didn't copy the command correctly. There we go. This is now in Europe, specifically Europe-west1. Refresh. You see that's already being created there. So we can definitely display all of those in the GCP console. If you click the button over here in Cloud Shell you can actually open this in a new window. This actually opens it in a new tab, that way you preserve your real estate. You can keep focusing on the console, as well as focusing on Cloud Shell. So let me actually create some real estate by just clearing this, and then paste the command to list all the networks with just G Cloud compute networks list. So we can see them, three networks. They're all Custom. We can dig deeper into this by also listing the subnetworks, and using the "sort-by" command to sort these by network. So now we'll see my network has a lot of subnets because it used to be in auto mode. Then, I mentioned that we want subnet and for permanet. We've two subnets. So now we're going to create some firewall rules. So let's click on "Firewall rules" up here. You can see the ones that are already there. Click create "Firewall rule". We'll repeat the same process we did earlier. We'll first create this using the console, and then we'll repeat the firewalls for a different network using Cloud Shell. So let me give it a name. Let's make sure I select the right network that the firewall rule applies to. Let's just do all instances. For the IP ranges select all addresses. I'm allowing, in this case, ICMP, SSH, and RDP. So let me define ICMP, and then 22 for SSH, and 3389 for RDP, and now down here I can click on "command line", and we can see this as one long command. Again, you don't need to define the project flags as gcloud compute, firewalls create. The name of the rule, the fact it's an ingress party, that is also actually default. We could leave that out. Importantly is the name of the network. Action allows default too, and then the rules as well as source ranges. So let's create that in the console, and we'll grab the command from the lab instructions to do the same for either network. So here you can see. We paste that in, and that should now create the other firewall rule for us. We can monitor the firewall rules in the console, as well as in Cloud Shell. So we'll run a command to list all the firewall rules in a second. So they're all created. If we list them, we can see them all here. If I refresh this we can also see them right here. So now it's time to create some more Instances, and then explore the connectivity. So let's head back to compute engine. I'm going to create instances in these new networks I created. So let me click "create instance". I'm actually going to close Cloud Shell for now. Let me just make it smaller. We're going to provide a name, and "US- Central1-c". Small machine is very fine. Now importantly I need to expand this option down here to select the right network. We've three options right now, and it has actually pre-selected that network. That's because from an order, it's listed up top. That is correct. So let's click "done" and there's again a command. There's a lot of information here that we don't need. You'll see that in a second when we run our command, like the BootDisk. We're selecting a lot of standard options. So let's just hit "create", and let's pull the command from the lab that creates the same in a different network. That's "gcloud compute instance create", the name of the instance, the zone, the machine type, the subnet. That is the bare minimum that we need to provide. So let's run that. You can see the other instance is already created. I can refresh this. See that the other Instances are already coming up too, and once Cloud Shell is updated we can list all the instances. Let's do that here. Can sort them by zone, or we could sort them by network. So we can see in one zone here we have an instance, and then in another zone we have three instances. Keep in mind these Instances are in different networks, and we can display that if we go to columns and check "Network", you can see that these Instances, with exception of the "mynet" these are on the same IVPC network, the others are indifferent. That's going to now go into the connectivity that we're going to explore. We're going to try to again ping IP addresses, both external and internal, and see what works. So let me grab the Management USVM external IP address, and we're going to SSH to the "mynet-us-vm". They are in the same zone, but they're in different networks. So let's see if we can ping the external IP address, and then we'll try the internal. So external works. That's because we set up the firewall rules for that. I can also do the same for privatenet. I can plug that IP address in which is 35.188.20.220 That works as well. So you can ping those, even though they are in different networks. Now from an internal perspective I should only be able to ping mynet-uvm which we actually tried earlier already. So let me just hop on the other ones. I'm going to try 10.130.0.2, and we can see that's not leading to anything. We should be getting a 100 percent packet loss, and then we'll try the same from the other one. So 172.16.0.2 and we can see that again isn't working either. So even though this Instance is in the same zone as these other instances I'm trying to ping, the fact that they are in a different network does not allow me to ping on the internal IP, unless we set up other mechanisms such as VPC peering or a VPN. That's the end of the lab.

### Video - [Common network designs](https://www.cloudskillsboost.google/course_templates/50/video/523082)

- [YouTube: Common network designs](https://www.youtube.com/watch?v=28tXeWMXEgg)

Let's use what we have learned so far and look at common network designs. Now, common is a fairly relative term, while I could spend all day talking about network designs, I have picked a handful of designs that best relate to this module. Let's start by looking at availability. If your application needs increased availability, you can place two virtual machines into multiple zones, but within the same subnet work as shown on this slide. Using a single sub-network allows you to to create a file a rule against the sub-network, in this case, 10.2.0.0/16. Therefore, by allocating VMs on a single subnet to separate zones, you get improved availability without additional security complexity. A regional managed instance group contains instances from multiple zones across the same region, which provides increased availability. Next, let's look at globalization. In the previous design we placed resources in different zones in a single region, which provides isolation for many types of infrastructure, hardware and software failures. Putting resources in different regions as shown on this slide provides an even higher degree of failure independence. This allows you to design robust systems with resources spread across different failure domains. When using a global load balancer like the HTTP load balancer, you can route traffic to the region that is closest to the user. This can result in better latency for users and lower network traffic costs for your project. We'll explore both managed instance groups and load balancers later in this course series. Now, as a general security best practice, I recommend only assigning internal IP addresses to your VM instances whenever possible. Cloud NAT is Google's managed network address translation service. It lets you provision your application instances without public IP addresses, while also allowing them to access the internet in a controlled and efficient manner. This means your private instances can access the internet for updates, patching, configuration management, and more. In this diagram Cloud NAT enables two private instances to access an update server on the Internet, which is referred to as outbound NAT. However, Cloud NAT does not Implement inbound NAT. In other words, hosts outside your VPC network cannot directly access any of the private instances behind the cloud NAT gateway. This helps you keep your VPC networks isolated and secure. Similarly, you should enable private Google access to allow VM instances that only have internal IP addresses to reach the external IP addresses of Google APIs and services. For example, if your private VM instance needs to access a cloud storage bucket, you need to enable private Google access. You enable private Google access on a subnet by subnet basis. As you can see in this diagram, subnet A has private Google access enabled and subnet B has it disabled. This allows VMA one to access Google APIs and services, even though it has no external IP address. Private Google access has no effect on instances that have external IP addresses, that's why VMs A2 and B2 can access Google APIs and services. The only VM that can't access those APIs and services is VM B1. This VM has no public IP address and it is in a subnet where Google private access is disabled.

### Video - [Lab Intro: Implement Private Google Access and Cloud NAT](https://www.cloudskillsboost.google/course_templates/50/video/523083)

- [YouTube: Lab Intro: Implement Private Google Access and Cloud NAT](https://www.youtube.com/watch?v=9OgvlhCLCvY)

Let's apply what we just covered. In this lab, you implement Private Google Access and Cloud NAT for a VM instance that doesn't have an external IP address. Then you verify access to public IP addresses of Google APIs and services and other connections to the Internet.

### Lab - [Implement Private Google Access and Cloud NAT](https://www.cloudskillsboost.google/course_templates/50/labs/523084)

In this lab, you configure Private Google Access and Cloud NAT for a VM instance that doesn't have an external IP address. Then, you verify access to public IP addresses of Google APIs and services and other connections to the internet.

- [ ] [Implement Private Google Access and Cloud NAT](../labs/Implement-Private-Google-Access-and-Cloud-NAT.md)

### Video - [Lab Review: Implement Private Google Access and Cloud NAT](https://www.cloudskillsboost.google/course_templates/50/video/523085)

- [YouTube: Lab Review: Implement Private Google Access and Cloud NAT](https://www.youtube.com/watch?v=RLRZ5OEO26Q)

In this lab, you created an instance with no external IP address and access it using Cloud IAP. You then enable Private Google Access and configured a NAT gateway and verified that vm-internal can access Google APIs and services and other public IP addresses. VM instances without external IP addresses are isolated from external networks. Using Cloud NAT, these instances can access the Internet for updates and patches, and in some cases for bootstrapping. As a managed service, Cloud NAT provides high availability without user management and intervention. Let me walk you through the lab. Now, remember that the GCP user interface can change. So your environment might look slightly different. So the first thing I'm going to do is create the VM instance. After that, we are also going to have to create a VPC network and some firewall rules. So let me go to navigation menu, scroll down to VPC networks. We're going to create a network and call it privatenet. So I'm going to name it privatenet, keep this subnet creation mode as custom. We're just going to create one subnet in here. We're going to call it privatenet-us. Let's place this in the us-central1 region, as given to us in the instructions. Here we go, and we even have an IP address range for that. Now, we are going to enable Private Google Access later. So you want to keep that off for now. I turned it on by accident. So you can see the effect of it being off. So let me click "Done" and click "Create". Now, I'm going to wait for this network to be created and once it's up and running, we're going to add a firewall rule because we want to allow SSH to the instance that we're going to put on this network. So I can see the network here. A firewall rule is created for networks, so I had to wait for that to be ready. So let me go to firewall rules, create firewall rule, give it a name. Specify that the network is privatenet. Let's just do all instances and sort by IP ranges. Now, rather than just saying, "Hey, you can SSH this instance from anywhere," we are actually going to give it a very specific range. This is because we're using Cloud IAP. So we're going to use a Cloud IAP tunnel, and because of that, we can limit the site range. Now, this is for an SSH connection. So I want to enable TCP port 22, and then click "Create". While this is creating, I can go ahead and create my Compute Engine instance. So let's go to "Compute Engine", click "Create". We're going to give it the name vm-internal. Now, we need to make sure we choose a region for which we've created a subnet. So us-central1, so us-central1-c. I can keep them machine type as my standard, n1-standard-1, 1virtualCPU, and I'm going to scroll down. The important thing is I need to select the actual VPC networks. Let's go to networking. Networking again, we're going to edit the network interfaces. I want to select the privatenet network. It only has one subnet, and I'm going to set the external IP address to none. Click "Done" and click "Create". So this is a way to create a private instance. Let me close this. That has no external IP address. Now, when the instance comes up, you will see that we won't be able to directly SSH to it because it doesn't have an external IP address. So if we use this, this wouldn't work on us, so instead what we're going to use is, we're going to do an IAP tunnel. For that, we're going to open Cloud Shell. So let me go click "Activate Cloud Shell", and that popped up in a new window. That can certainly happen sometimes. Looks like there's some A, B testing going on here. So here I have Cloud Shell, doesn't look like it has the correct project set. So let's actually do that. I'm going to set the project and then just grab the project ID from here, and set this up for the correct project, and there we can see that now. So it's setup, and now what I'm going to do is, I'm going to run the command to SSH from here. I'm going to specify this is through IAP, and then I want to confirm. For passphrase, we're just going to hit "Enter" and then "Enter" again. Once this is complete, we should now see that the command prompt has changed to vm-internal. So we're now in vm-internal, it doesn't have an external IP address, but let's confirm that we can't just ping the World Wide Web. This ping command isn't working because vm-internal does not have an external IP address. So we can wait for this to complete and it's failing. Again, when instances don't have external IP addresses, they can only be reached by other instances on the network, either through a managed VPN gateway or Cloud IAP tunnel, and Cloud IAP enables contexts where access to VMs through SSH and RDP without a bastion host. That would be the other idea or option. We could create a bastion host, but that would still have an external IAP. Then we're just using the bastion host to then connect to this. Instead, we can just use Cloud Shell and IAP. So this isn't working. So what we're going to now is we're going to look into Private Google Access. So currently, VM instance with no external IP address can use Private Google Access to reach external IP addresses of Google APIs and services. But by default, this is disabled. We saw that earlier, we left it as disabled. So let's test the effect of this being disabled. I'm going to go to the navigation menu, and we're going to create a cloud storage bucket. So let's go to "Storage". I'm going to click "Create Bucket". Now, the most difficult piece is you need to have a unique bucket name. You could do that by grabbing the ID of a project. Click Continue, you can leave this as Multi-region, we can leave everything else by default, and just click Create. The important thing is you're going to have to remember that bucket names, so here's the bucket. So I'm going to do now, is I'm going to go back to Cloud Shell. Importantly, I'm still in my VM Instance here. So I want to change that, so let me exit out of here. So now I'm back in Cloud Shell and then I'm going to run a command to copy an image from a public bucket to my bucket, but I need to specify what my bucket is. So I can take the name of the bucket and add that here to copy this image, so that worked. We can go in here and refresh to verify that we now have an image in here. You can actually click on this image and this just shows you how Private Google Access is implemented pending if it's on or off for a network. We're going to explore that a little bit more. So now what we're going to do is, we're going to now try to copy this image, first from Cloud Shell. Well, Cloud Shell has an external IP address, so that is going to work, run that. I need to actually click Enter. Obviously, I didn't specify my bucket, that is on me. So I need to change my bucket, so typical error that you might see. Let me grab the name of the bucket, placed it in there. Let's try that again, okay, that works. We even use Cloud Shell to move this image anyway, so we're able to access Cloud Storage currently through Cloud Shell. Let's go back to our VM internal [inaudible]. So we use the same command use earlier to SSH through a IAP tunnel. Here, I can see the command prompt changed. Now, I'm just going to copy the same command here to copy this image, so I don't have to change the bucket name a couple times, and we're going to run that. We should see that this does not work, because currently VM internal can only send traffic within the VPC networks because again, Private Google Access is disabled. So with two options, we can wait for this to fail and give us an error or we can use Control C to just stop the request. So let's actually just stopped this. What we do now is I'm going to able Private Google Access. So let's go back to the Cloud Console, the Navigation menu and I'm going to navigate to my VPC network, specifically privatenet. Private Google Access is enabled at the subnet level. So I'm going to go directly to the subnet, click the Edit icon, scroll down and able Private Google Access or set it to on, click Save. I'm going to wait for this to update and then I'm going to come back to my instance, my SSH lessons through Cloud Shell and just try to run the command again. So it looks like it's all set, you can also see that here. Going back to my SSH window, run that command again, and now it works. So that's how easy it is to enable Private Google Access. So now in this last task of the lab, we're going to configure a Cloud NAT gateway. Now although our Instance here, VM internal can now access certain Google APIs and services without an external IP address, the instance cannot access the Internet for updates and patches. So for that, we're going to configure our Cloud NAT gateway, but again, we're going to try this behavior first without the NAT gateway and then we're going to enable it. So what we're going to do is I'm going to exit here to just get to my Cloud Shell Instance. There we go, you can see the command prompt changed to Cloud Shell. I'm just going to run sudo apt-get update, and that should obviously work for my Cloud Shell instance because it has an external IP address. So we can see it's getting all these packages and that is working just fine. So now that's complete, we're going to use the SSH command again using the IP tunnel to get to VM internal, there we can see this change. Now we're going to run the same command here. You might say, "Well, hold on." It's actually able to get some of these packages. Yes, that's because we've enabled Private Google Access, so it's able to get those within Google. Once it's trying to get something else here, it's failing. So we can just stop that, this is not going to happen. Now we're going to go ahead and configure Cloud NAT gateway and then try to run that command again. So let's go to the Cloud Console and under the Navigation menu, we're going to go to Network services and Cloud NAT. We're going to go click Get started, just give this a name called nat-config. It's just a name that we have in the lab instructions. You really want to follow these lab instructions because any of our labs that are scored, we'll use names that we're defining in the lab instructions. So important distance to be on privatenet, Region is us-central1. For Cloud Router, we currently don't have one, so we're going to go create one. This is super simple, you just give it a name and click Create. Now, there's also a NAT mapping section and this allows you to choose the subnets to map to the NAT gateway, so you could manually assign static IP addresses that should be used when performing that. But in this case, we're not going to go that and get that fancy, we're just going to click Create. We're going to wait now for the gateway status to change to running. So we can see that the status changed to running, it actually only took a couple seconds. Now, even though this is running, it may actually take up to three minutes for the NAT configuration to propagate all the way to the VM. So you want to wait at least a minute before trying to access the Internet again. What I mean by that is in our SSH session that we currently still have to VM internal, we're going to run the command again. I want to make sure it works this time. So I could actually just try it right now and see if it's ready or not. If I do, you see it's still failing at the step. So let me hit Control C and let's get a couple more minutes and then try to run the command again. So we've waited a couple minutes, let's try to run the command one more time and now we can see that's working. It's getting all the packages and with that we can confirm that Cloud NAT decline a gateway is not working. Now, couple of things to remember, the Cloud NAT gateway implements outbound net, but not inbound net. In other words, what that means is that hosts outside of your VPC network, can only respond to connections initiated by your instances. They cannot initiate their own. So new connections to your instances via the net, so keep that in mind. The other thing is in this lab we used IAP, and IAP uses your existing project roles and permissions when you connect to VM instances. So by default, instance owners, which your instance owner since you created this instance. They're the only ones that have the IAP secure tunnel user role. If you want to allow other users to connect to access using VM, using IP tunneling, you need to grant them those roles. You can actually do that directly through the Navigation menu and go to Cloud IP, and just give people those roles. That's the end of the lab.

### Quiz - [Quiz: Virtual Networks](https://www.cloudskillsboost.google/course_templates/50/quizzes/523086)

#### Quiz 1.

> [!important]
> **What are the three types of networks offered in Google Cloud?**
>
> - [ ] Gigabit network, 10 gigabit network, and 100 gigabit network
> - [ ] Default network, auto network, and custom network.
> - [ ] IPv4 unicast network, IPv4 multicast network, IPv6 network
> - [ ] Zonal, regional, and global

#### Quiz 2.

> [!important]
> **In Google Cloud, what is the minimum number of IP addresses that a VM instance needs?**
>
> - [ ] One: Only an internal IP address
> - [ ] Three: One internal, one external and one alias IP address
> - [ ] Two: One internal and one external IP address

#### Quiz 3.

> [!important]
> **What is one benefit of applying firewall rules by tag rather than by address?**
>
> - [ ] Tags help organizations track firewall billing.
> - [ ] Tags in network traffic help with network sniffing.
> - [ ] When a VM is created with a matching tag, the firewall rules apply irrespective of the IP address it is assigned.
> - [ ] Tags on firewall rules control which ephemeral IP addresses VMs will receive.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/50/video/523087)

- [YouTube: Module Review](https://www.youtube.com/watch?v=1lqmGhTghtI)

In this module, I gave you an overview of Google's Virtual Private Cloud. We looked at the different objects within VPC like projects, networks, IP addresses, routes, and firewall rules. I also provided a brief overview of how you're network design choices can affect billing. Then you apply the different concepts that we covered in a thorough lab. Next, we looked at common network designs and you got to implement private Google axis and Cloudnet in a lab. Now, that you have a solid understanding of how GCP has implemented networking, let's move on to learn more about other services. Next up, is Compute Engine which offers scalable, high-performance Virtual Machines.

## Virtual Machines

Create virtual machines using Compute Engine

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/50/video/523088)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=EEj4pUZ8xC0)

In this module, we cover virtual machine instances, or VMs. VMs are the most common infrastructure component and in GCP they're provided by Compute Engine. A VM is similar but not identical to a hardware computer. VMs consists of a virtual CPU, some amount of memory, disk storage, and an IP address. Compute Engine is GCP's service to create VMs. It is very flexible and offers many options including some that can't exist in physical hardware. For example, a micro VM shares a CPU with other virtual machines, so you can get a VM with less capacity at a lower cost. Another example of a function that can't exist in hardware is that some VMs offer burst capability, meaning that the virtual CPU will run above its rated capacity for a brief period, using the available shared physical CPU. The main VM options are CPU, memory, discs, and networking. Now, this is going to be a very robust module; there's a lot of detail to cover here with how virtual machines work on GCP. First, we'll start with the basics of Compute Engine, followed by a quick little lab to get you more familiar with creating virtual machines. Then, we'll look at the different CPU and memory options that enable you to create different configurations. Next, we'll look at images and the different disk options available with Compute Engine. After that, we will discuss very common Compute Engine actions that you might encounter in your day-to-day job. This will be followed by an in-depth lab that explores many of the features and services covered in this module. Let's get started with an overview of Compute Engine.

### Video - [Compute Engine](https://www.cloudskillsboost.google/course_templates/50/video/523089)

- [YouTube: Compute Engine](https://www.youtube.com/watch?v=ITH1idwhYBQ)

As mentioned in the introduction to the course, there is a spectrum of different options in Google Cloud for compute and processing. We will focus on the traditional virtual machine instances. Now the difference is, Compute Engine gives you the utmost in flexibility: run whatever language you want—it's your virtual machine. This is purely an infrastructure as a service or IaaS model. You have a VM and an operating system, and you can choose how to manage it and how to handle aspects, such as autoscaling, where you'll configure the rules about adding more virtual machines in specific situations. Autoscaling will be covered later in the course. The primary work case of Compute Engine is any generic workload, especially an enterprise application that was designed to run on a server infrastructure. This makes Compute Engine very portable and easy to run in the cloud. Other services, like Google Kubernetes Engine, which consists of containerized workloads, may not be as easily transferable as what you're used to from on-premises. So what is Compute Engine? At its heart, it's physical servers that you're used to, running inside the Google Cloud environment, with a number of different configurations. Both predefined and custom machine types allow you to choose how much memory and how much CPU you want. You choose the type of disk you want, whether you want to use persistent disks backed up by standard hard drives or solid-state drives, local SSDs, Cloud Storage, or a mix. You can even configure the networking interfaces and run a combination of Linux and Windows machines. We will discuss these options in more detail later in the module. Several different features will be covered throughout this module, such as machine rightsizing, startup and shutdown scripts, metadata, availability policies, OS patch management, and pricing and usage discounts. It is important to mention that hardware manufacturers have run up against limitations, and CPUs, which are central processing units, and GPUs, which are graphics processing units, can no longer scale to adequately reach the rapid demand for ML. To help overcome this challenge, in 2016 Google introduced the Tensor Processing Unit, or TPU. TPUs are Google's custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads. TPUs act as domain-specific hardware, as opposed to general-purpose hardware with CPUs and GPUs. This allows for higher efficiency by tailoring architecture to meet the computation needs in a domain, such as the matrix multiplication in machine learning. TPUs are generally faster than current GPUs and CPUs for AI applications and machine learning. They are also significantly more energy-efficient. Cloud TPUs have been integrated across Google products, making this state-of-the-art hardware and supercomputing technology available to Google Cloud customers. TPUs are mostly recommended for models that train for long durations and for large models with large effective batch sizes. Refer to the documentation for more details. Let's start by looking at the compute options. Compute Engine provides several different machine types that we'll discuss later in this module. If those machines don't meet your needs, you can also customize your own machine. Your choice of CPU will affect your network throughput. Specifically, your network will scale at 2 gigabits per second for each CPU core, except for instances with 2 or 4 CPUs which receive up to 10 gigabits per second of bandwidth. There is a theoretical maximum throughput of 200 gigabits per second for an instance with 176 vCPU, when you choose an C3 machine series. When you're migrating from an on-premises setup, you're used to physical cores, which have hyperthreading. On Compute Engine, each virtual CPU (or vCPU) is implemented as a single hardware hyper-thread on one of the available CPU Platforms. For an up-to-date list of all the available CPU platforms, refer to the CPU platforms documentation link in the course resources. After you pick your compute options, you want to choose your disk. You have three options: Standard, SSD, or local SSD. So basically, do you want the standard spinning hard disk drives (HDDs), or flash memory solid-state drives (SSDs)? Both of these options provide the same amount of capacity in terms of disk size when choosing a persistent disk. Therefore, the question really is about performance versus cost, because there's a different pricing structure. Basically, SSDs are designed to give you a higher number of IOPS per dollar versus standard disks, which will give you a higher amount of capacity for your dollar. Local SSDs have higher throughput and lower latency than SSD persistent disks, because they are attached to the physical hardware. However, the data that you store on local SSDs persists only until you stop or delete the instance. Typically, a local SSD is used as a swap disk, just like you would do if you want to create a ramdisk, but if you need more capacity, you can store those on a local SSD. Standard and non-local SSD disks can be sized up to 257 TB for each instance. The performance of these disks scales with each GB of space allocated. As for networking, we have already seen networking features applied to Compute Engine in the previous module's lab. We looked at the different types of networks and created firewall rules using IP addresses and network tags. You'll also notice that you can do regional HTTPS load balancing and network load balancing. This doesn't require any pre-warming because a load balancer isn't a hardware device that needs to analyze your traffic. A load balancer is essentially a set of traffic engineering rules that are coming into the Google network, and VPC is applying your rules destined to your IP address subnet range. We'll learn more about load balancers in a later course of the Architecting with Google Compute Engine series.

### Video - [Demo: Create a VM](https://www.cloudskillsboost.google/course_templates/50/video/523090)

- [YouTube: Demo: Create a VM](https://www.youtube.com/watch?v=7dLQD1L8Sfc)

Let me give you a quick walk through of the VM instance creation process, and point out CPU, storage, and network options in the GCP Console. So here I am already on the Compute Engine instance page. You can get to here by going to the navigation menu and then clicking on "Compute Engine". As we use this in the course a lot, you might actually want to pin this sometimes so that you can get to it more easily. Then within there I have gone into VM instances. So I just want to again show you some of the options that are available when creating instance. To get started I'm going to click on "Create". The first thing I want to choose is a name, so you have that right up here. Then maybe more importantly is actually where you want the instance to be located. So you have an option of all the different available regions. It has the name of the regions as well as the closest city as to where that region is located. Then within the regions you have different zones that you can choose from. You also see on the right-hand side that there is a cost associated with the current configuration, net cost is going to change as we change the configuration. So for example, if I instead of creating an instance in US Central one, I create one maybe in Europe West One. You will see that the cost is slightly adjusted. So I can try that a couple different ways by choosing different locations. You should see that the cost changes depending on the region that we choose. Now it goes further if I then choose the machine type. There are different types, we'll go into all of those. But if I go in here, the different types explain to me what they provide. This standard n1 standard one provides one virtual CPU with 3.75 gigabytes of memory. If I change to a machine with more CPU and more memory, we'll see that the cost is adjusted. You can also go into details here. It actually spells it out for you that there's a cost for the CPU and memory, but there's also costs for the persistent disk. We haven't configured that yet, but this is the default value and if we configure that, it's going to ingest a cost. There's also sustained use discount would go into that as well, but essentially all of that is what ultimately gets you to this total monthly cost. It's also broken down in an hourly cost here, and we'll talk more about pricing later within the module. So again I can choose different machine types, maybe we want a larger machine type that's going to be more expensive. Maybe I just need a shared core. So something really, a micro machine or a small machine and that can really drive the cost down a lot. So let me go back to the default. The other thing to think about in terms of your region and zone is not just the cost, but really you want to create your instances that are close to your users. Maybe you want to have them spread out across different regions for habilability. You might be having restrictions for data locality meaning that your data has to be in a specific region. So these are all the different things that you want to consider when choosing the region and zone. Now if I scroll further down, one of the next pix options is the boot disk. So we can see here that currently it has a 10 gigabytes standard persistent disk. I can change that. I can change the image itself, but I can also change the boot disk type. Now the boot disk needs to be a persistent disk. We have the standard persistent disk. Think of an HDD. And we have the SSD. You can see that we can define the size here, and you can see that both of them have the same exact maximum size. So if I make this larger, let's say 1,000, then we're going to see that the cost now is adjusted to that disk size. So it can go back, that's very large. Maybe I'm just okay with 10 gigabytes as the boot disk. You can also add more disks. So if I scroll down and go to management security disk networking, I can go to disks here. So here I can choose the type of encryption for the disk. I have Google managed key, Customer managed key, Customer supplied key. Then I can add more disks. So if I add a new disk here then under type I could also choose a local SSD. Disks come in predefined sizes, depending on how many you choose your performance as you can see down here, is going to get adjusted. There is a limit. So at some point the more disk you choose, you're going to hit a limit into your performance, and same if I choose an SSD disk and change the size here, you'll see that also there's a limit but it's also adjusted if I scale as you are changing the IOPS as well as the sustained throughput limit. Now another important thing is obviously networking. So if I click on here, you want to choose the network interface. We already went into this a little bit in the previous module in terms of near choosing your primary internal IP, choosing if you want an external IP or not. So those are all of the different options that you can get there. Now what's really cool is this is all using the GCP Console, but down the road you might say well, I want to create these instances quickly and I want to use a command line. Well, this user interface gives you the command line options. So it's spelling out exactly all the different options you have chosen, how you would recreate that using GCloud. So this can help you get started using the command line and make you more comfortable using that command line. So let me just go ahead and create this instance. Once we create it we have these different columns that are listed here, there are more columns that we can choose from. For example, when you created it, what the machine type is, what network this is a part off, if you had labels or other things, so lots of different options you can list here. So for example, I can just hear when the machine was created, the type as well as the network it is a part off. That's how easy it is to configure the location, CPU, memory, storage, and network interface for a VM instance using the GCP Console. Let's get back to the slides to go over VM axis and lifecycle.

### Video - [VM access and lifecycle](https://www.cloudskillsboost.google/course_templates/50/video/523091)

- [YouTube: VM access and lifecycle](https://www.youtube.com/watch?v=gPaQTu-rMnU)

For accessing a VM, the creator of an instance has full root privileges on that instance. On a Linux instance, the creator has SSH capability and can use the Google Cloud console to grant SSH capability to other users. On a Windows instance, the creator can use the console to generate a username and password. After that, anyone who knows the username and password can connect to the instance using a Remote Desktop Protocol, or RDP, client. We listed the required firewall rules for both SSH and RDP here, but you don't need to define these if you are using the default network that we covered in the previous module. The lifecycle of a VM is represented by different statuses. We will cover this lifecycle on a high level, but we recommend returning to this diagram as a reference. When you define all the properties of an instance and click Create, the instance enters the provisioning state. Here the resources such as CPU, memory, and disks are being reserved for the instance, but the instance itself isn't running yet. Next, the instance moves to the staging state where resources have been acquired and the instance is prepared for launch. Specifically, in this state, Compute Engine is adding IP addresses, booting up the system image, and booting up the system. After the instance starts running, it will go through pre-configured startup scripts and enable SSH or RDP access. Now, you can do several things while your instance is running. For example, you can live migrate your virtual machine to another host in the same zone instead of requiring your instance to be rebooted. This allows Google Cloud to perform maintenance that is integral to keeping the infrastructure protected and reliable, without interrupting any of your VMs. While your instance is running, you can also move your VM to a different zone, take a snapshot of the VM's persistent disk, export the system image, or reconfigure metadata. We will explore some of these tasks in later labs. Some actions require you to stop your virtual machine; for example, if you want to upgrade your machine by adding more CPU. When the instance enters this state, it will go through pre-configured shutdown scripts and end in the terminated state. From this state, you can choose to either restart the instance, which would bring it back to its provisioning state, or delete it. You also have the option to reset a VM, which is similar to pressing the reset button on your computer. This action wipes the memory contents of the machine and resets the virtual machine to its initial state. The instance remains in the running state through the reset. The VM may also enter a repairing state. Repairing occurs when the VM encounters an internal error or the underlying machine is unavailable due to maintenance. During this time, the VM is unusable. You are not billed when a VM is in repair. VMs are not covered by the Service level agreement (SLA) while they are in repair. If repair succeeds, the VM returns to one of the above states. Finally, when you suspend the VM, it enters in the suspending state, before being suspended. You can then resume the VM or delete it. There are different ways you can change a VM state from running. Some methods involve the Google Cloud console and the gcloud command, while others are performed from the OS, such as for reboot and shutdown. It's important to know that if you are rebooting, stopping, or even deleting an instance, the shutdown process will take about 90 sec. For a preemptible VM, if the instance does not stop after 30 seconds, Compute Engine sends an ACPI G3 Mechanical Off signal to the operating system. Remember that when writing shutdown scripts for preemptible VMs. As I mentioned previously, Compute Engine can live migrate your virtual machine to another host due to a maintenance event to prevent your applications from experiencing disruptions. A VM's availability policy determines how the instance behaves in such an event. The default maintenance behavior for instances is to live migrate, but you can change the behavior to terminate your instance during maintenance events instead. If your VM is terminated due to a crash or other maintenance event, your instance automatically restarts by default, but this can also be changed. These availability policies can be configured both during the instance creation and while an instance is running by configuring the Automatic restart and On host maintenance options. For more information on live migration, refer to the documentation. OS updates are a part of managing an infrastructure. Let's see how we can manage the updates to a fleet of Windows VMs. When you provision a premium image, there is a cost associated with the image. This cost includes both the usage of the OS but also the patch management of the OS. Using Google Cloud, we can easily manage the patching of your OSes. Managing patches effectively is a great way to keep your infrastructure up-to-date and reduce the risk of security vulnerabilities. But without the right tools, patching can be daunting and labor intensive. Use OS patch management to apply operating system patches across a set of Compute Engine VM instances. Long-running VMs require periodic system updates to protect against defects and vulnerabilities. The OS patch management service has two main components: Patch compliance reporting, which provides insights on the patch status of your VM instances across Windows and Linux distributions. Along with the insights, you can also view recommendations for your VM instances. Patch deployment, which automates the operating system and software patch update process. A patch deployment schedules patch jobs. A patch job runs across VM instances and applies patches. There are several tasks that can be performed with patch management. You can: … Create patch approvals. You can select what patches to apply to your system from the full set of updates available for the specific operating system. Set up flexible scheduling. You can choose when to run patch updates (one-time and recurring schedules). Apply advanced patch configuration settings. You can customize your patches by adding configurations such as pre and post patching scripts. And you can manage these patch jobs or updates from a centralized location. When a VM is terminated, you do not pay for memory and CPU resources. However, you are charged for any attached disks and reserved IP addresses. In the terminated state, you can perform any of the actions listed here, such as changing the machine type, but you cannot change the image of a stopped VM. Also, not all of the actions listed here require you to stop a virtual machine. For example, VM availability policies can be changed while the VM is running, as discussed previously.

### Video - [Lab Intro: Creating Virtual Machines](https://www.cloudskillsboost.google/course_templates/50/video/523092)

- [YouTube: Lab Intro: Creating Virtual Machines](https://www.youtube.com/watch?v=jPWqVFk_Ces)

Let's take some of the Compute Engine constants we just discussed and apply them in a lab. In this lab, you explore virtual machine instance options by creating several standard VMs and a custom VM. You also connect to those VMs using both SSH for Linux machines and RDP for Windows machines.

### Lab - [Creating Virtual Machines](https://www.cloudskillsboost.google/course_templates/50/labs/523093)

In this lab you will create several Virtual Machine Instances and explore the options available.

- [ ] [Creating Virtual Machines](../labs/Creating-Virtual-Machines.md)

### Video - [Lab Review: Creating Virtual Machines](https://www.cloudskillsboost.google/course_templates/50/video/523094)

- [YouTube: Lab Review: Creating Virtual Machines](https://www.youtube.com/watch?v=gTZ_zCrUWkc)

In this lab, you created several Virtual Machine instances of different types with different characteristics. Specifically, you created a small utility VM for administration purposes, a windows VM, and accustomed Linux VM. You also acts as both the Windows and Linux VM and deleted all your creative VMs. In general, start with a smaller VM when you're prototyping solutions to keep the cost down. When you're ready for production, trade up to larger VMs based on capacity. If you building and redundancy for availability, remember to allocate excess capacity to meet performance requirements. Finally, consider using custom VMs when your applications requirements fit between the features of the standard types. You can stay for a lab walk through but remember that GCP user interface can change, so your environment might look slightly different. So in the GCP console, I'm going to navigate to Compute engine and then VM instances, and in here we're going to click "Create". Now, we can define a name there's a small question mark here and if you hover over it can tell you a little bit more about some of the restrictions you have in regards to creating a name, choosing a name that is, and I'm just going to call this my utilityVM. We're going to go over some of the options that actually went over a little bit in the demo, but we obviously can choose region and zones. So let's change the zone to what the lab is instructing, which is 1-C, and then for the machine type we have a lot of different options to choose from. We can see that the cost changes if I scale up to a machine with four virtual CPUs versus a machine that's just maybe a micro, which is a shared core machine. So the cost can change quite drastically. So let's just leave all the remaining settings and click "Create", and once the machine is up and running, we're going to explore the different VM details that we have. So we're going to go into the VM Instances page, and look at things like the CPU platform, the availability policies and so on. So let me do that, let me click on "Utility VM" because it's now in a running state. I'm going to look for a CPU platform, you can see that right here and if I click "Edit", you'll see that I actually am unable to modify that. So that's because I can't do that while the instances is running. There are other things I could do, I could change the firewall rules, I can add network tags. So certain things are available to change while and instances is running. In some cases, you have to stop the instance to change some of the properties. In other cases, you cannot actually even change it unless you delete it. One of those is for example the network interfaces, if you had multiple network interfaces, you'd have to recreate your instance. The good thing is you could keep your boot disk and just reattach that boot disk later on. Now, I can also go and look at the availability policies, just scroll down to some what the enhanced maintenance is. By default, it's set to migrate the VM instance, and that's recommended but you could set this to terminate the instance. It's also going to automatically restart that instance so you could configure that as well. All right, so this is just a little bit exploring the different options, I'm going to go click "Cancel". What we're going to now is explore some of the VM logs. So I'm looking at the detail page here. We want to get a little bit more information about the monitoring options that are available. We can click "Monitoring" here, and we'll get more information about the CPU. This instances barely runs, we don't have much data yet. We get information about the network bytes and packets, disk I/O. We can also, if we go back to details, look at stackdriver logging. So this is now a different user interface and here we now have individual logs that we can explore. We can view options here, we could expand all of these and dig into all of these different logs that are in here and even within there, expand each of the logs to get more information. So this uses stackdriver logging, we'll cover this feature a little bit but more in a later course in the course series if you're interested to learn more about both the logging piece that we just looked at as well as the monitoring. So let's go to Test 2, we're now going to create a windows virtual machine. So I'm going to go back through the navigation menu Compute engine to VM instances, and I'm not going to create a another instance. So I'm going to define a name, and this is just going to call it Windows VM, and we're going to choose a different region and zone this time. Why don't we put this into Europe-West2, and specifically to zone 2A. Let's pick a larger machine. Let's pick one that has two virtual CPUs and 7.5 gigabytes of memory. We can even go ahead now and changed the boot disk because by default, this would be a Linux machine, so if we want to change this because we want to create a windows machine. Specifically, the lab is instructing me to look for the Windows Server 2016 Datacenter Core image. It's first scroll down, I can see that image right here, can change the boot disk. Maybe I want some higher IOPS, I can choose an SSD, and I can even make this larger and click "Select". All of that again is going to affect obviously the cost. I have the cost of the machine, I have the cost of the disc, but the new thing I have now also is the image, I've chosen zupimages which means there is a cost associated with using that image, but it's built all together for you. So you can see that cost broken up right here. Now, the other thing we're going to do is we're going to allow a specific traffic, HTTP and HTTPS traffic. This just creates a network tag for us and then creates filer roles on the network tag so that we can enable traffic on those ports for the TCP protocol. So let's hit "Create" and create this instance. One thing we'll notice when the instance comes up is that under the connect column [inaudible] now seeing an SSH button which is we would have for a Linux machine. We should now see a RDP, which is for the Remote Desktop Protocol. So that's how you would access a Windows machine. Now, the important thing is there you obviously want to configure your username and password so that only authorized users access that machine. So here you can see the RDP button now. What we're going to do now is we're going to click onto the machine and set the Windows password. You can actually also do this by clicking "Down here" set windows password there as well. So actually, let's just do it that way. So you have a username here. It's taking the username that I have for my lab account. So this is the username right now. So I can set that and then it's going to provide me with a password. So there we go. So I can now copy that password and if I use an RDP connection, I can then get into that. This is a little bit outside of the scope for this lab, but if you want to and have an RDP client, you can actually install one through Chrome, through an extension. You could access that instance that way and then configure it and do anything else you wanted to in this Windows Virtual Machine. So let me go ahead and close that, and I'm going to move on to a Task 3, now which is to create a custom Virtual Machine. So I'm going to go back to Create Instance, and to find a name, let's just call it my custom-VM. I'll follow the lab instructions here for setting the region and zone which is US-West1-B. Now, rather than choosing a specific machine type, I can go in here and just select Custom as the machine type and then define the exact numbers of cores memory. So let's say, my specification I want six virtual CPU, and you can see how the scales by the way, there are only certain options. You can choose it goes all the way to 96. So let me choose six here. It's going to scale that memory automatically for us. It gives us a range now depending on that CPU, there's an option to extend the memory so you could get more than 39 and see all the way to 624. This is a separate option, we'll talk more about this in the slides. So let me choose 32, and rather than scrolling here I could also just type the value in and that's also going to adjust the cost now. Sometimes, it's important to note that your custom machine may be between two machine types are actually already provided. A custom machine is generally going to be slightly more expensive. So if you have a standard machine that's very close to the custom machine, it's definitely something you would want to consider. Once the machine runs more than 24 hours, you'll get right sides recommendations. So It'll tell you if the machine is too small or too large and make recommendations based on that. So let's go ahead and create that. Once it's up and running, we're going to SSH to the machine. We're going to run some commands on that machine, and that's actually going to wrap up the Lab for us. Now, with any new project, you get this column here on the right-hand side to help you get started because we're using Qwiklabs generated projects, they're always going to be new products. So you'll see this throughout the training. You can certainly leveraged this if you want but I'm going to collapse that. So VM is up and running, let me SSH to it. Then we're going to run the free command to see information about any unused and used memory and swap space. So let me type free. So we can see that here and that lines up with the memory selections that we made in the machine. I can also see I get more information or details about the RAM installed. So here we get more information about that as well. I can verify the number of processors. So that should have been six, and yep, and prox is sixth, great. We can see details about the CPU itself. So here we get information about the architecture, the byte order, which model exactly, so you can get all this information about any VM that you create. You can also get more information about this in the documentation depending on which region and zone you choose. You'll have different architectures and different models available to choose from. So that's all we wanted to show you here with this lab. We went ahead and created that Virtual Machine, the Utility VM, we created a Windows VM, and then we created a custom Virtual Machine and verified that whatever custom settings we applied were actually used to create the machine by running commands within that machine.

### Video - [Compute options](https://www.cloudskillsboost.google/course_templates/50/video/523095)

- [YouTube: Compute options](https://www.youtube.com/watch?v=LjozZwHn4Fo)

Now that you have completed the lab, let's dive deeper into the compute options that are available to you in Google Cloud, by focusing on CPU and memory. You have three options for creating and configuring a VM. You can use the Cloud Console as you did in the previous lab, the Cloud Shell command line, or the RESTful API. If you'd like to automate and process very complex configurations, you might want to programmatically configure these through the RESTful API by defining all the different options for your environment. If you plan on using the command line or RESTful API, I recommend that you first configure the instance through the Google Cloud console and then ask Compute Engine for the equivalent REST request or command line, as shown in the demo earlier. This way you avoid any typos and get dropdown lists of all the available CPU and memory options. Speaking of CPU and memory options, let's look at the different machine types that are currently available. When you create a VM, you select a machine type from a machine family that determines the resources available to that VM. There are several machine families you can choose from and each machine family is further organized into machine series and predefined machine types within each series. A machine family is a curated set of processor and hardware configurations optimized for specific workloads. When you create a VM instance, you choose a predefined or custom machine type from your preferred machine family. Alternatively, you can create custom machine types. These let you specify the number of vCPUs and the amount of memory for your instance. There are four Compute Engine machine families. General-purpose, Compute-optimized, Memory-optimized, and Accelerator-optimized. Let's look at each in more detail. The general-purpose machine family has the best price-performance with the most flexible vCPU to memory ratios, and provides features that target most standard and cloud-native workloads. The E2 machine series is suited for day-to-day computing at a lower cost, especially where there are no application dependencies on a specific CPU architecture. E2 VMs provide a variety of compute resources for the lowest price on Compute Engine, especially when paired with committed-use discounts. You simply pick the amount of vCPU and memory you want, and Google provisions it for you. The Standard E2 VMs have between 2 to 32 vCPUs with a ratio of 0.5 GB to 8 GB of memory per vCPU. They are a great choice for web servers, small to medium databases, development and test environments, and many applications that don't have strict performance requirements. They offer a compatible performance baseline with the current N1 VMs for those of you who have been using them. The E2 machine series also contains shared-core machine types that use context-switching to share a physical core between vCPUs for multitasking. Different shared-core machine types sustain different amounts of time on a physical core. In general, shared-core machine types can be more cost-effective for running small, non-resource intensive applications than standard, high-memory, or high-CPU machine types. Shared-core E2 machine types have 0.25 to 1 vCPUs with 0.5 GB to 8 GB of memory. The N2 and N2D are the next generation following the N1 VMs, offering a significant performance jump. N2 and N2D are the most flexible VM types and provide a balance between price and performance across a wide range of VM shapes, including enterprise applications, medium-to-large databases, and many web and app-serving workloads. Committed use and sustained use discounts are supported. N2 VMs support the latest second generation scalable processor from Intel with up to 128 vCPUs and 0.5 to 8 GB of memory per vCPU. Cascade Lake is the default processor for machine types with up to 80 vCPUs. For larger machine types Ice Lake is the default processor for specific regions and zones. N2D are AMD-based general purpose VMs. They leverage the latest EPYC Milan and EPYC Rome processors, and provide up to 224 vCPUs per node. Tau T2D and Tau T2A VMs are optimized for cost-effective performance of demanding scale-out workloads. T2D VMs are built on the latest 3rd Gen AMD EPYCTM processors and offer full x86 compatibility. They are suited to scale-out workloads including web servers, containerized microservices, media transcoding, and large-scale Java applications. T2D VMs come in predefined VM shapes, with up to 60 vCPUs per VM and 4 GB of memory per vCPU. Tau T2A machine series is the first machine series in Google Cloud to run on Arm processors. The Tau T2A machine series runs on a 64 core Ampere Altra processor with an Arm instruction set and an all-core frequency of 3 GHz. If you have containerized workloads, Tau VMs are supported by Google Kubernetes Engine to help optimize price-performance. You can add T2D nodes to your GKE clusters by specifying the T2D machine type in your GKE node pools. The compute-optimized machine family has the highest performance per core on Compute Engine and is optimized for compute-intensive workloads. C2 VMs are the best fit VM type for compute-intensive workloads, including AAA gaming, electronic design automation, and high-performance computing across simulations, genomic analysis, or media transcoding. They might also be applications that have very expensive per core licensing and thus would benefit from higher per core performance. Powered by high-frequency Intel-scalable processors, Cascade Lake, C2 machine types offer up to 3.8 Ghz sustained all-core turbo and provide full transparency into the architecture of the underlying server platforms, enabling advanced performance tuning. The C2 series comes in different machine types ranging from 4 to 60 vCPUs, and offers up to 240 GB of memory. You can also attach up to 3 TB of local storage to these VMs for applications that require higher storage performance. The C2D machine series provides the largest VM sizes and are best-suited for high-performance computing. The C2D series also has the largest available last-level cache per core. The C2D machine series come in different machine types ranging from 2 to 112 vCPUs, and offer 4 GB of memory per vCPU core. You can also attach up to 3 TB of local storage to these machine types for applications that require higher storage performance. C2D VMs are available on the third generation AMD EPYC Milan platform. The H3 series offer 88 cores and 352 GB of DDR5 memory and are available on the Intel Sapphire Rapids CPU platform and Google's custom Intel Infrastructure Processing Unit (IPU). The memory-optimized machine family provides the most compute and memory resources of any Compute Engine machine family offering. They are ideal for workloads that require higher memory-to-vCPU ratios than the high-memory machine types in the general-purpose machine family. The M1 machine series has up to 4 TB of memory, while the M2 machine series has up to 12 TB of memory. These machine series are well-suited for large in-memory databases such as SAP HANA, as well as in-memory data analytics workloads. Both the M1 and M2 machine series offer the lowest cost per GB of memory on Compute Engine, making them a great choice for workloads that utilize higher memory configurations with low compute resource requirements. Additionally, they offer up to 30% sustained use discounts and are also eligible for committed use discounts, bringing additional savings of greater than 60% for three-year commitments. M3 VMs offer up to 128 vCPUs, with up to 30.5 GB of memory per vCPU, and are available on the Intel Ice Lake CPU platform. These machines are well-suited for memory intensive applications, such as genomic modeling and electronic design automation and high performance computing. The accelerator-optimized machine family is ideal for massively parallelized Compute Unified Device Architecture (CUDA) compute workloads, such as machine learning and high-performance computing. This family is the optimal choice for workloads that require GPUs. The A2 series has 12 to 96 vCPUs, and up to 1360 GB of memory. Each A2 machine type has a fixed number (up to 16) of NVIDIA's Ampere A100 GPUs attached. An A100 GPU provides 40 GB of GPU memory—ideal for large language models, databases, and high-performance computing. G2 VMs offer 4 to 96 vCPUs, up to 432 GB of memory, and are available on the Intel Cascade Lake CPU platform. These machines are well-suited for CUDA-enabled ML training and inference, video transcoding, remote visualization workstation. Additional information, including the latest specs for currently available VM machine types, can be found in the machine types documentation. If none of the predefined machine types match your needs, you can independently specify the number of vCPUs and the amount of memory for your instance. Custom machine types are ideal for the following scenarios: When you have workloads that are not a good fit for the predefined machine types that are available to you. Or when you have workloads that require more processing power or more memory, but don't need all of the upgrades that are provided by the next larger predefined machine type. It costs slightly more to use a custom machine type than an equivalent predefined machine type, and there are still some limitations in the amount of memory and vCPUs you can select: Only machine types with 1 vCPU or an even number of vCPUs can be created. Memory must be between 1 GB and 8 GB per vCPU. The total memory of the instance must be a multiple of 256 MB. Selected custom machine types can allow up to 8 GB of memory per vCPU. However, this might not be enough memory for your workload. At an additional cost, you can get more memory per vCPU beyond the 8 GB limit. This is referred to as extended memory, and you can learn more about this in the link provided in the module PDF located in Course Resources. The first thing you want to consider when choosing a region and zone is the geographical location in which you want to run your resources. This map shows the current and planned Google Cloud regions and the number of zones. For up-to-date information on the available regions and zones, see the documentation linked for this video. Each zone supports a combination of Ivy Bridge, Sandy Bridge, Haswell, Broadwell, and Skylake platforms. When you create an instance in the zone, your instance will use the default processor supported in that zone. For example, if you create an instance in the us-central1-a zone, your instance will use the Sandy Bridge processor.

### Video - [Compute pricing](https://www.cloudskillsboost.google/course_templates/50/video/523096)

- [YouTube: Compute pricing](https://www.youtube.com/watch?v=uNy8wfme34E)

Google Cloud offers a variety of different options to keep the prices low for Compute Engine resources. All vCPUs, GPUs, and GB of memory are charged a minimum of 1 minute. For example, if you run your virtual machine for 30 seconds, you will be billed for 1 minute of usage. After 1 minute, instances are charged in 1-second increments. Compute Engine uses a resource-based pricing model, where each vCPU and each GB of memory on Compute Engine is billed separately rather than as a part of a single machine type. You still create instances using predefined machine types, but your bill reports them as individual vCPUs and memory used. There are several discounts available but the discount types cannot be combined. Resource-based pricing allows Compute Engine to apply sustained use discounts to all of your predefined machine types usage in a region collectively rather than to individual machine types. If your workload is stable and predictable, you can purchase a specific amount of vCPUs and memory for a discount off of normal prices in return for committing to a usage term of 1 year or 3 years. The discount is up to 57% for most machine types or custom machine types. The discount is up to 70% for memory-optimized machine types. Preemptible and Spot VMs are instances that you can create and run at a much lower price than normal instances. For both types of VM, Compute Engine might terminate (or preempt) these instances if it requires to access those resources for other tasks. Both preemptive VMs and Spot VMs are excess Compute Engine capacity so their availability varies with usage. Importantly, preemptible VMs can only run for up to 24 hours at a time, but Spot VMs do not have a maximum runtime. The ability to customize the amount of memory and CPU through custom machine types allows for further pricing customization. Speaking of sizing your machine, Compute Engine provides VM sizing recommendations to help you optimize the resource used of your virtual machine instances. When you create a new instance, recommendations for the new instance will appear 24 hours after the instance has been created. Compute Engine also has Free Usage Limits. Sustained use discounts are automatic discounts that you get for running specific Compute Engine resources (vCPUs, memory, and GPU devices) for a significant portion of the billing month. For example, when you run one of these resources for more than 25% of a month, Compute Engine automatically gives you a discount for every incremental minute you use for that instance. The discount increases with usage, and you can get up to 30% net discount for instances that run the entire month. The tables shown on this slide describes the discount you get at each usage level of a VM instance. To take advantage of the full 30% discount, create your VM instances on the first day of the month, because discounts reset at the beginning of each month. The graph on this slide demonstrates how your effective discount increases with use. For example, if you use a virtual machine for 50% of the month, you can an effective discount of 10%. If you use it for 75% of the month, you get an effective discount of 20%. If you use it for 100% of the month, you get an effective discount of 30%. You can also use the Google Cloud Pricing Calculator to estimate your sustained use discount for any arbitrary workload. Compute Engine calculates sustained use discounts based on vCPU and memory usage across each region and separately for each of the following categories: Predefined machine types, and Custom machine types. Let's go through an example where you have two instances that are in the same region but have different machine types and run at different times of the month. Compute Engine breaks down the number of vCPUs and amount of memory used across all instances that use predefined machine types and combines the resources to qualify for the largest sustained usage discounts possible. As shown on this slide, you run the following two instances in the us-central1 region during a month: For the first half of the month, you run an n1-standard-4 instance with 4 vCPUs and 15 GB of memory. For the second half of the month, you run a larger n1-standard-16 instance with 16vCPUs and 60 GB of memory. In this scenario, Compute Engine reorganizes these machine types into individual vCPUs and memory resources and combines their usage to create the following resources, as shown on the bottom: 4 vCPUs and 15 GB of memory for a full month. And then 12 vCPUs and 45 GB of memory for half of the month.

### Video - [Special compute configurations](https://www.cloudskillsboost.google/course_templates/50/video/523097)

- [YouTube: Special compute configurations](https://www.youtube.com/watch?v=-bboz9sc0WA)

As I mentioned earlier, a preemptible VM is an instance that you can create and run at a much lower cost than normal instances. See whether you can make your application function completely on preemptible VMs, because a 60 to 91% discount is a significant investment in your application. Now, just to reiterate, these VMs might be preempted at any time, and there is no charge if that happens within the first minute. Also, preemptible VMs are only going to live for up to 24 hours, and you only get a 30-second notification before the machine is preempted. It's also worth noting that there are no live migrations nor automatic restarts in preemptible VMs, but something that we might want to highlight is that you can actually create monitoring and load balancers that can start up new preemptible VMs in case of a failure. In other words, there are external ways to keep restarting preemptible VMs if you need to. One major use case for preemptible VMs is running batch processing jobs. If some of those instances terminate during processing, the job slows but it does not completely stop. Therefore, preemptible instances complete your batch processing tasks without placing additional workload on your existing instances, and without requiring you to pay full price for additional normal instances. Spot VMs are the latest version of preemptible VMs. Spot VMs are virtual machine (VM) instances with the spot provisioning model. New and existing preemptible VMs continue to be supported, and preemptible VMs use the same pricing model as Spot VMs. However, spot VMs provide new features that preemptible VMs do not support. For example, preemptible VMs can only run for up to 24 hours at a time, but Spot VMs do not have a maximum runtime. Like preemptible VMs, Compute Engine might preempt Spot VMs if it needs to reclaim those resources for other tasks. The probability that Compute Engine stops Spot VMs for a system event is generally low, but might vary from a day to day and from zone to zone depending on current conditions. Spot VMs are finite Compute Engine resources, so they might not always be available. Like preemptible VMs, it's worth noting that Spot VMs can't live-migrate to become standard VMs while they are running or be set to automatically restart when there is a maintenance event. There are many best practices which can help you get the most of using Spot VMs. For example, resources for Spot VMs come out of excess and backup Google Cloud capacity. Capacity for spot VMs is often easier to get for smaller machine types, meaning machine types with less resources like vCPU and memory. If you have workloads that require physical isolation from other workloads or virtual machines in order to meet compliance requirements, you want to consider sole-tenant nodes. A sole-tenant node is a physical Compute Engine server that is dedicated to hosting VM instances only for your specific project. Use sole-tenant nodes to keep your instances physically separated from instances in other projects, or to group your instances together on the same host hardware, for example if you have a payment processing workload that needs to be isolated to meet compliance requirements. The diagram on the left shows a normal host with multiple VM instances from multiple customers. A sole-tenant node is shown on the right and it also has multiple VM instances, but they all belong to the same project. You can also fill the node with multiple smaller VM instances of varying sizes, including custom machine types and instances with extended memory. Also, if you have existing operating system licenses, you can bring them to Compute Engine using sole-tenant nodes while minimizing the physical core usage with the in-place restart feature. To learn how to create nodes and place your instances on those nodes, see the link section of this video. Another compute option is to create a shielded VM. Shielded VMs offer verifiable integrity to your VM instances, so you can be confident that your instances haven't been compromised by boot or kernel-level malware or rootkits. Shielded VMS is the first offering in the Shielded Cloud Initiative. The Shielded Cloud Initiative is meant to provide an even more secure foundation for all of Google Cloud by providing verifiable integrity and offering features, like vTPM shielding or sealing, that help prevent data exfiltration. In order to use the shielded VM features, you need to select a shielded image. We'll learn more about images in the next section. Confidential VMs are a breakthrough technology that allows you to encrypt data in use, while it's been processed. Google Cloud's approach to encrypt data in use is simple, easy-to-use deployment without making any code changes to their applications or having to compromise performance. You can collaborate with anyone, all while preserving the confidentiality of your data. Confidential Virtual Machine (Confidential VM) is a type of N2D Compute Engine VM instance running on hosts based on the second generation of AMD Epyc processors, code-named "Rome". Using AMD Secure Encrypted Virtualization (SEV), Confidential VM features built-in optimization of both performance and security for enterprise-class high memory workloads, as well as inline memory encryption that doesn't introduce significant performance penalties on those workloads. The AMD Rome processor family is specifically optimized for compute-heavy workloads, with high memory capacity, high throughput, and support for parallel workloads. In addition, AMD SEV provides for Confidential Computing support. With the confidential execution environments provided by Confidential VM and AMD SEV, Google Cloud keeps customers' sensitive code and other data encrypted in memory during processing. Google does not have access to the encryption keys. You can select the Confidential VM service when creating a new VM using the Google Cloud Console, the Compute Engine API, or the gcloud command-line tool.

### Video - [Images](https://www.cloudskillsboost.google/course_templates/50/video/523098)

- [YouTube: Images](https://www.youtube.com/watch?v=nTFpJLHfXJo)

Next, let's focus on images. When creating a virtual machine, you can choose the boot disk image. This image includes the boot loader, the operating system, the file system structure, any pre-configured software, and any other customizations. You can select either a public or custom image. As you saw in the previous lab, you can choose from both Linux and Windows images. Some of these images are premium images, as indicated in parentheses with a p. These images will have per-second charges after a 1-minute minimum, with the exception of SQL Server images, which are charged per minute after a 10-minute minimum. Premium image prices vary with the machine type. However, these prices are global and do not vary by region or zone. You can also use custom images. For example, you can create and use a custom image by pre-installing software that's been authorized for your particular organization. You also have the option of importing images from your own premises or workstation, or from another cloud provider. This is a no-cost service that is as simple as installing an agent, and I highly recommend that you look at it. You can also share custom images with anybody in your project or among other projects, too. A machine image is a Compute Engine resource that stores all the configuration, metadata, permissions, and data from one or more disks required to create a virtual machine (VM) instance. You can use a machine image in many system maintenance scenarios, such as creation, backup and recovery, and instance cloning. Machine images are the most ideal resources for disk backups as well as instance cloning and replication.

### Video - [Disk options](https://www.cloudskillsboost.google/course_templates/50/video/523099)

- [YouTube: Disk options](https://www.youtube.com/watch?v=q-FIYArz1qo)

At this point you've chosen an operating system, but that operating system is going to be included as part of some kind of disk. So let's look at the disk options. Every single VM comes with a single root persistent disk, because you're choosing a base image to have that loaded on. This image is bootable in that you can attach it to a VM and boot from it, and it is durable in that it can survive if the VM terminates. To have a boot disk survive a VM deletion, you need to disable the "Delete boot disk when instance is deleted" option in the instance's properties. As I discussed earlier, there are different types of disks. Let's explore these in more detail. The first disk that we create is what we call a persistent disk. That means it's going to be attached to the VM through the network interface. Even though it's persistent, it's not physically attached to the machine. This separation of disk and compute allows the disk to survive if the VM terminates. You can also perform snapshots of these disks, which are incremental backups that we'll discuss later. The choice between HDD and SSD disks comes down to cost and performance. To learn more about disk performance and how it scales with disk size, refer to the link in the course resources for this module. Another cool feature of persistent disks is that you can dynamically resize them, even while they are running and attached to a VM. You can also attach a disk in read-only mode to multiple VMs. This allows you to share static data between multiple instances, which is cheaper than replicating your data to unique disks for individual instances. Zonal persistent disks offer efficient, reliable block storage. Regional persistent disks provide active-active disk replication across two zones in the same region. Regional persistent disks deliver durable storage that is synchronously replicated across zones and are a great option for high-performance databases and enterprise applications that also require high availability. When you configure a zonal or regional persistent disk, you can select one of the following disk types. Standard persistent disks are backed by standard hard disk drives and are suitable for large data processing workloads that primarily use sequential I/Os. Performance SSD persistent disks are backed by solid-state drives and are suitable for enterprise applications and high-performance databases that require lower latency and more IOPS than standard persistent disks provide. Balanced persistent disks are also backed by solid-state drives. They are an alternative to SSD persistent disks that balance performance and cost. These disks have the same maximum IOPS as SSD persistent disks and lower IOPS per gigabyte. For most VM shapes, except very large ones, this disk type offers performance levels suitable for most general-purpose applications at a price point between that of standard and performance persistent disks. Extreme persistent disks are zonal persistent disks also backed by solid-state drives. Extreme persistent disks are designed for high-end database workloads, providing consistently high performance for both random access workloads and bulk throughput. Unlike other disk types, you can provision your desired IOPS. By default, Compute Engine encrypts all data at rest. Google Cloud handles and manages this encryption for you without any additional actions on your part. However, if you wanted to control and manage this encryption yourself, you can either use Cloud Key Management Service to create and manage key encryption keys (which is known as customer-managed encryption keys) or create and manage your own key encryption keys (known as customer-supplied encryption keys). Now, local SSDs are different from persistent disks in that they are physically attached to the virtual machine. Therefore, these disks are ephemeral but provide very high IOPS. For up-to-date numbers I recommend referring to the documentation in the course resources for this module. Currently, local SSDs are 375 gigabytes in size and you can attach up to to 24 local SSD partitions for a total of 9 terabytes per instance. Data on these disks will survive a reset but not a VM stop or terminate, because these disks can't be reattached to a different VM. You also have the option of using a RAM disk. You can simply use tmpfs if you want to store data in memory. This will be the fastest type of performance available if you need small data structures. I recommend a high-memory virtual machine if you need to take advantage of such features, along with a persistent disk to back up the RAM disk data. In summary, you have several different disk options. Persistent disks can be rebooted and snapshotted, but local SSDs and RAM disks are ephemeral. I recommend choosing a persistent HDD disk when you don't need performance but just need capacity. If you have high performance needs, start looking at the SSD options. The persistent disks offer data redundancy because the data on each persistent disk is distributed across several physical disks. Local SSDs provide even higher performance, but without the data redundancy. Finally, RAM disks are very volatile but they provide the highest performance. Now, just as there is a limit on how many Local SSDs you can attach to a VM, there is also a limit on how many persistent disks you can attach to a VM. As illustrated in this table, this limit depends on the machine type. For the Shared-core machine type, you can attach up to 16 disks. For the Standard, High Memory, High-CPU, Memory-optimized, and Compute-optimized machine types, you can attach up to 128 disks. So you can create massive amounts of capacity for a single host. Now remember that little nuance when I told you about how throughput is limited by the number of cores that you have? That throughput also shares the same bandwidth with Disk IO. So if you plan on having a large amount of Disk IO throughput, it will also compete with any network egress or ingress throughput. So remember that, especially if you will be increasing the number of drives attached to a virtual machine. There are many differences between a physical hard disk in a computer and a persistent disk, which is essentially a virtual networked device. First of all, if you remember with normal computer hardware disks, you have to partition them. Essentially, you have a drive and you're carving up a section for the operating system to get its own capacity. If you want to grow it, you have to repartition, and if you want to make changes you might even have to reformat. If you want redundancy, you might create a redundant disk array, and if you want encryption, you need to encrypt files before writing them to the disk. With cloud persistent disks, things are very different because all that management is handled for you on the backend. You can simply grow disks and resize the file system because disks are virtual networked devices. Redundancy and snapshot services are built in and disks are automatically encrypted. You can even use your own keys, and that will ensure that no party can get to the data except you.

### Video - [Common Compute Engine actions](https://www.cloudskillsboost.google/course_templates/50/video/523100)

- [YouTube: Common Compute Engine actions](https://www.youtube.com/watch?v=1iwt9lzrl4E)

Now that we have covered all the different compute, image, and disk options, let's look at some common actions that you can perform with Compute Engine. Every VM instance stores its metadata on a metadata server. The metadata server is particularly useful in combination with startup and shutdown scripts, because you can use the metadata server to programmatically get unique information about an instance, without additional authorization. For example, you can write a startup script that gets the metadata key/value pair for an instance's external IP address and use that IP address in your script to set up a database. Because the default metadata keys are the same on every instance, you can reuse your script without having to update it for each instance. This helps you create less brittle code for your applications. Storing and retrieving instance metadata is a very common Compute Engine action. We recommend storing the startup and shutdown scripts in Cloud Storage, as you will explore in the upcoming lab of this module. Another common action is to move an instance to a new zone. For example, you might do so for geographical reasons or because a zone is being deprecated. You can move a VM even if one of the following scenarios applies: The VM instance is in a TERMINATED state. The VM instance is a Shielded VM that uses UEFI firmware. If you move your instance within the same region, you can automate the move by using the gcloud compute instances move command. To move your VM, you must shut down the VM, move it to the destination zone or region, and then restart it. After you move your VM, update any references that you have to the original resource, such as any target VMs or target pools that point to the earlier VM. During the move, some server-generated properties of your VM and disks change. If you move your instance to a different region, you need to manually do so by following the process outlined here. This involves making a snapshot of all persistent disks and creating new disks in the destination zone from that snapshot. Next, you create the new VM in the destination zone and attach the new persistent disks, assign a static IP, and update any references to the VM. Finally, you delete the original VM, its disks, and the snapshot. Speaking of snapshots, let's take a closer look at these. Snapshots have many use cases. For example, they can be used to backup critical data into a durable storage solution to meet application, availability, and recovery requirements. These snapshots are stored in Cloud Storage, which is covered later. Snapshots can also be used to migrate data between zones. We just discussed this when going over the manual process of moving an instance between two regions, but this can also be used to simply transfer data from one zone to another. For example, you might want to minimize latency by migrating data to a drive that can be locally attached in the zone where it is used. Which brings me to another snapshot use case of transferring data to a different disk type. For example, if you want to improve disk performance, you could use a snapshot to transfer data from a standard HDD persistent disk to a SSD persistent disk. Now that we've covered some of the snapshot use cases, let's explore the concept of a disk snapshot. First of all, this slide is titled persistent disk snapshots because snapshots are available only to persistent disks and not to local SSDs. Snapshots are different from public images and custom images, which are used primarily to create instances or configure instance templates, in that snapshots are useful for periodic backup of the data on your persistent disks. Snapshots are incremental and automatically compressed, so you can create regular snapshots on a persistent disk faster and at a much lower cost than if you regularly created a full image of the disk. You can create a snapshot schedule to regularly and automatically back up your zonal and regional persistent disks. As we saw with the previous examples, snapshots can be restored to a new persistent disk, allowing for a move to a new zone. To create a persistent disk snapshot, refer to the module PDF under Course Resources. Another common Compute Engine action is to resize your persistent disk. The added benefit of increasing storage capacity is to improve I/O performance. This can be achieved while the disk is attached to a running VM without having to create a snapshot. Now, while you can grow disks in size, you can never shrink them, so keep this in mind.

### Video - [Lab Intro: Working with Virtual Machines](https://www.cloudskillsboost.google/course_templates/50/video/523101)

- [YouTube: Lab Intro: Working with Virtual Machines](https://www.youtube.com/watch?v=xfZee9KqgVI)

Let's get started with the second lab of this module. In this lab, you will be setting up an application server. Now, this example happens to be a gaming application, but it applies to many other use cases. You will configure the VM and also add capacity for a production gaming system, and you will build the infrastructure that you need for production activities. These include backups and graceful shutdown and restart services.

### Lab - [Working with Virtual Machines](https://www.cloudskillsboost.google/course_templates/50/labs/523102)

In this lab you will create a typical application VM and set it up with many of the necessary features. In this case a gaming system - a Minecraft server.


- [ ] [Working with Virtual Machines](../labs/Working-with-Virtual-Machines.md)

### Video - [Lab Review: Working with Virtual Machines](https://www.cloudskillsboost.google/course_templates/50/video/523103)

- [YouTube: Lab Review: Working with Virtual Machines](https://www.youtube.com/watch?v=7c_3m5t53dA)

In this lab, you created a customized Virtual Machine instance by installing base software which was a headless Java runtime environment and application software specifically a Minecraft Game Server. You customize the VM by preparing and attaching a high-speed SSD and you've reserved a static external IP address so that the address will remain consistent. Using that IP address, you then verify the availability of the gaming server online. Next, you set up a backup system to backup the service data to a Cloud Storage bucket, and then you tested that backup system. You then automated backups using cron. Finally, you set up maintenance scripts using metadata for graceful startup and shutdown off the server. Many of these techniques including these script automation can be adapted to administration of production servers in any application. You can stay for a lab walk-through. But remember that GCPs user interface can change. So your environment might look slightly different. So here I am in the VM Instances page. Let's go ahead and create our instance. We're going to use the same properties that are provided to us, properties and values in the lab. So I'm going to call this the mc server for our Minecraft Server. We're going to place this in the US Central one, a zone. We're going to modify the access scopes for this. So I'm going to set axis for each API. I'm going to modify for storage that besides just read only, I want read write. This is going to allow the VM instance to write to the Cloud Storage bucket that we're going to create later on. Now we're also going to modify the disk of this instance. So let's expand the option down here, and under disks we're going to add a new disk. We're going to call this the Minecraft disk, and we're going to make that an SSD persistent disk. It's going to be blank so no source. Fifty gigabytes is more than enough for what we're trying to do. I'm going to leave the encryption as Google managed key. So let me click done and this is going to create that disk and automatic attach it to the VM. Now under networking, we're also going to add a network tag. This is going to then allow us to locate specific firewall rules, we call that Minecraft Server. On the network interface I'm going to click on the pencil icon here to edit. We are leaving the internal IP as is but for the external IP, we're actually going to create an IP address which means that we are reserving a static IP address. This is going to make sure that these IP address is not ephemeral and doesn't change. So I'll just give it a name and I click reserve and then we're going to click done once that is reserved and from there we're going to create this instance. It's done and then create. Now once the instance is up and running we're going to have to prepare the data disks. So we're not going to create a directory, format and mount the disk. I don't need this tab over here so I can close that. We're going to wait for the instance there it is. So the SSH to the instance. I'm going to start by creating a director that serves as the amount point for the data off the disk. For that I'm just going to use the command that's provided in the lab and then we're going to format the disk itself. So we're just going to wait for that SSH connection to be established. This is allowed because the default network has a default firewall rule for SSH. So let me go ahead and run that, and then we're going to format the disk. Great. Now we're going to mount it, and this is not going to display any outputs, so don't be surprised about that. There's a checkpoint in the lab. So you can check your progress, worked for me. So I'm going to move on to task three and now install and run the application and the micro server itself runs on top of the Java virtual machine. So we do require the Java Runtime Environment, or JRE, to run. But we don't need the user interface. So we're just going to install actually headless version and that's going to reduce a lot of the resource usage on that machine which will ensure that the Minecraft Server has enough room to expand its own resource usage if needed. So let me go ahead and start by updating the repository. Then I'm going to install that headless JRE, and after that I'm going to navigate to the directory where we mounted that persistent disk. Into that we're then going to download the Minecraft jar file. So we navigate into that under command. You can see it's downloading and the lab manual also provides information on the download page itself. So you can read more about where this comes from. There are also lots of instructions actually in there on how to set this up on a Linux machine. So if you wanted to customize this, I definitely recommend referring to that link. So let's go ahead and initialize the Minecraft Server. Run that command and it's telling us that this is not going to run unless we agree to the end-user licensing agreement. So we need to do that now. Let me just check my progress. Make sure that the JRE installation and Minecraft server installation worked out and I got a green check in my lab. So let's look at the files that were created to identify where this license agreement is, and there it is. We can see it right there. So let me use nano to edit that now. All we really have to do is we have to change this last line, instead of saying false, we just have to agree to it by setting this to true. So let me change that and then we're going to click Control O to write that to that filename hit Enter and then Control X to come back out. So we're not going to try to restart the Minecraft Server yet. We're going to use a different technique in a second. What we're going to do next is we're going to create a virtual terminal screen to start that server, and to do that we're going to install screen. So let's grab that command from the lab instructions. It seems like it was already actually installed. Then we're going to go ahead and start that now using the screen command. So let's run that and this might take a while now but it's going to establish the whole environment for us. So we can see here it's preparing the level world. It's loading some recipe. So these are all now very specific commands in regards to the gaming application that we're installing here and we're going to wait for this to complete before detaching from this and moving on. So we can see that the spawn area here has been completed. We could not detached from this, but one thing I want to point out that we're going to have to do next is when this whole thing started it told us which port it is going to do that for. So the port is right here and we're going to have to create a firewall rule in a second to actually allow client traffic to that port. So we can now detach from this. So we're going to just use Control A and Control D. To get out of here. There is a command if you wanted to reattach to the terminal, we're not going to do that. So I'm just going to exit out of here and we're now going to allow Cloud traffic. So for that, we need to create a firewall rule and we're going to use the network tag that we created which we can display by going to Columns and then Network tags. We can see that Minecraft Server was the network tag. So let's do that. I'm going to navigate to VPC network and specifically Firewall rules. I'm going to give a new firewall rule the name of Minecraft rule. It's going to be on the default network. Could this be the only network we have right now? For specified target tags, we're now going to define Minecraft Server so only apply to the instances that have that tag. So let me define the IP ranges as from anywhere. Now specifically for the protocol that's TCP, and then that port was 25565. Then I'm going to go ahead and click Create. Once it's up and running we're going to verify availability of the server. So I can already start navigating back and I'll monitor the process up here in the notification pane. I'm going back to Compute Engine and we have the external IP address here. We're now going to use a couple of different ways to verify that this is running. Note that we can't click on it because we didn't enable HTTP, that would have been TCP for port 80. In the lab instructions, we have listed a website and we also currently have a Chrome extension there, have that Chrome extension actually right up here. So let's try that. I'm going to go to Options, change the IP address that is in here. Save that, and then we're going to try to verify. I can change this through my Minecraft Server, save those changes and then we're going to keep an eye on here to see if this is coming up. Alternative also we could use any of the websites that are listed in here. Since these are third-party tools sometimes they don't work. So that's definitely something to keep in mind. I think that's actually what's going on right now. With this extension it doesn't seem to want to display this to us right now. But if I check the box in the lab instructions itself, it's telling me that everything is tracked correctly. So we've done all the work. It's just that sometimes, again, these third-party tools that we're using to test the status may not always work. There is another one that I can try really fast. We could grab the external IP address and copy it in there. Get the service status that way. It is telling us that it does have it, so it is up and running currently has no players in it, and it tells us the exact version that we're running. So clearly it is working for this page, just not for the Chrome extension right now. All right. So then let's move on. What we're going to do now is, these services up and running but now we want to actually scheduled some regular backups, have some maintenance around the server so that we plan for the long term. So what I can do now is I can SSH back into the server. Since I allowed for read write access to Cloud Storage, I can actually directly create a bucket now through my server here similarly as you would from Cloud Shell. So the first thing I'm going to do is I'm just going to define my own bucket name, and store it in an environment variable. So here we go, export your bucket name. You want to use something that's globally unique. So one thing we could do is we could take our project ID. You take that right here, and you go back to that server and paste it in there. Whenever you create a an environment variable, you want to run the echo command to make sure that you created it correctly. Here we can see that worked. Now I can use the gsutil command specifically MB for make bucket for Google Cloud Storage, and then use that unique part that I just entered and just append Minecraft backup so that I also know what this is. So this becomes a little bit more readable. Great. So there it is. I could also know verify by the way that it is created in my project. I could go to the navigation menu, and if we go to storage we'll be able to see our bucket right here. We could have also just created this way. But this way we now have everything stored that is the variable in here, and then going forward we can do all of the backup right through the VM. So let's go ahead, and create a backup script. I'm just going to navigate to the home directory that we have within Minecraft, and we're going to just create a new script using nano. I'm going to paste the script that we already have in here which has the screen command, and then talks about the backups. So let me paste this in there, and then we're going to press Control O, and then enter to save and control X to come back. So this script saves the current state of the server's world and pauses a service odyssey functionality. Then it's going to backup the service world data directory and place its content in a Timestamp directory in the Cloud Storage bucket. After the script, I've finished his backup the data it resumes odyssey saving on the Minecraft Server. Now we got to make sure that this is actually executable. So let's run the following command, and now we can go and test this. So let's actually run the backup script so there we can see that we are copying some files. Let's verify that. So I'm going to now navigate into my Cloud Storage bucket that I already have here. If I open that, we can now see a folder in there, and I could dig further into there to get more information about the world. So clearly we can see that the backup is working for us. We can also now schedule the backup to run in and more automated fashion. So I'm going to go back to my SSH session, run the pseudo crontab command. Now we want to choose nano in this case, it does tell us it's easiest but you do have other options available if those are more comfortable. At the bottom, we're now going to define how often this runs. This is going to tell it to run the backup every four hours. There's documentation that you can look into and how to define this, but in this case, that's more than enough for what we're trying to achieve. So let's save that file and get back out. This is going to create a lot of backups. I mean about 300 a month. So maybe you want to look into regular deleting those Cloud Storage does offer Object Lifecycle Management features that let you set the time to live for objects and even archive older objects to a different Storage class. You'll learn more about that in the next course of this series when we talk about Cloud Storage. I'm just going to go ahead and check my progress. In my lab looks like everything worked. The last thing we're going to do is now perform some maintenance. So specifically when we shut down and restart that certain actions happen. So let me run the pseudo screen command, and then I'm going to go and actually stop this instance. So I'm going to go navigate to Compute Engine, click on the server so select it and click stop. It's going to ask us if we sure we want to do that, and yes we're going to stop. Then later if we want to start it back up we can do that. This is also going to log us out of our SSH session obviously. So let's wait for this to stop, and then we're going to automate the server maintenance with some startup and shutdown scripts. So the instance has stopped, am going to click on it now to edit some of the custom metadata. So let me click Edit, and we're going to scroll down to the metadata. Here we go. What we're going to define now is a startup script as well as the shutdown script. We're going to point those to files that we have in Cloud Storage that are publicly available. So the key is going to be startup script URL, and then the value is going to be the location of the file. I can make them bigger to make sure that formatting that correctly. I'll add another item, and we'll do the same for the shutdown script. You can actually navigate yourself to these files if you want to, and you could read more about what exactly happens in these startup and shutdown script. So now I can click Save, and I could restart the service. I did in the meantime while the service was shutting down. I went back to the status page, and you can see that the status as currently says could not resolve so clearly the server is shut down. Now when we restart this, once all the startup script is done running, we can go back and we can verify that this service is indeed now accessible again. Just keep in mind that that might take a while for the actual instance to startup which it is now, and then for the startup script to actually finish.

### Quiz - [Quiz: Virtual Machines](https://www.cloudskillsboost.google/course_templates/50/quizzes/523104)

#### Quiz 1.

> [!important]
> **Which statement is true of persistent disks?**
>
> - [ ] Persistent disks are always HDDs (magnetic spinning disks).
> - [ ] Once created, a persistent disk cannot be resized.
> - [ ] Persistent disks are encrypted by default.
> - [ ] Persistent disks are physical hardware devices connected directly to VMs.

#### Quiz 2.

> [!important]
> **What are sustained use discounts?**
>
> - [ ] Automatic discounts that you get for running specific Compute Engine resources for a significant portion of the billing month
> - [ ] Per-second billing that starts after a 1 minute minimum
> - [ ] Discounts you receive by using preemptible VM instances
> - [ ] Purchase commitments for specific resources you know you will use

#### Quiz 3.

> [!important]
> **Which statement is true of Virtual Machine Instances in Compute Engine?**
>
> - [ ] In Compute Engine, a VM is a networked service that simulates the features of a computer.
> - [ ] A VM in Compute Engine always maps to a single hardware computer in a rack.
> - [ ] Compute Engine uses VMware to create Virtual Machine Instances.
> - [ ] All Compute Engine VMs are single tenancy and do not share CPU hardware.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/50/video/523105)

- [YouTube: Module Review](https://www.youtube.com/watch?v=ABJumQbrFZo)

In this module, we cover the different Compute image and disk options within Compute Engine, along with some common actions. The two labs provided you with real-world applications of most of the topics covered in this course. Remember there are many Compute options to choose from. If a predefined machine type does not meet your needs, you can also customize your own VM and you can even create a sole-tenant node. You can also install different public and custom images on the boot disk of you instances, and you can attach more disks if needed.

### Video - [Course Review](https://www.cloudskillsboost.google/course_templates/50/video/523106)

- [YouTube: Course Review](https://www.youtube.com/watch?v=mDWvxWO9LFA)

Thank you for taking the Essential Cloud Infrastructure Foundation course. I hope you have a better understanding of how to architect with Compute Engine, and I also hope that the demos and labs made you feel more comfortable with using the different GCP services that we covered. Next, I recommend enrolling in the Essential Cloud Infrastructure Core Services course of the architecting with Google Compute Engine series. In that course, we start by talking about Cloud IAM, and you will administer identity and access management for resources. Next, we'll cover the different data storage services in GCP, and you will implement some of those services. Then will go into resource management where you will manage and examine billing data of GCP resources. Lastly, we'll talk about resource monitoring, and you will monitor GCP resources using Stackdriver services. Enjoy that course.

### Document - [Next Course: Essential Cloud Infrastructure: Core Services](https://www.cloudskillsboost.google/course_templates/50/documents/523107)

## Course Resources

PDF links to all modules

### Document - [Course Resources](https://www.cloudskillsboost.google/course_templates/50/documents/523108)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
