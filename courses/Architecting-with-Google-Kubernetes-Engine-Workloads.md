---
id: 34
name: 'Architecting with Google Kubernetes Engine: Workloads'
type: Course
url: https://www.cloudskillsboost.google/course_templates/34
date_published: 2024-12-24
topics:
  - Deployment
  - GKE
  - Compute Instances
---

# [Architecting with Google Kubernetes Engine: Workloads](https://www.cloudskillsboost.google/course_templates/34)

**Description:**

In "Architecting with Google Kubernetes Engine- Workloads", you'll embark on a comprehensive journey into cloud-native application development. Throughout the learning experience, you'll explore Kubernetes operations, deployment management, GKE networking, and persistent storage. This is the first course of the Architecting with Google Kubernetes Engine series. After completing this course, enroll in the Architecting with Google Kubernetes Engine- Production course.

**Objectives:**

* Create and manage workloads in Google Kubernetes Engine.
* Explain how pod networking works in Google Kubernetes Engine.
* Define and work with different Kubernetes storage abstractions.

## Workloads Course Introduction

In this introduction, you'll explore the course goals and preview each section.

### Video - [Course introduction](https://www.cloudskillsboost.google/course_templates/34/video/518699)

* [YouTube: Course introduction](https://www.youtube.com/watch?v=fBNDnqL_4vs)

Imagine effortless scaling, reliability, and streamlined development for cloud-native applications. That's the power of architecting with Google Kubernetes Engine—or GKE. But powerful tools like GKE manage a vast array of interactions and configurations. How can you harness its power? That’s where this course, “Architecting with Google Kubernetes Engine” comes in. Getting the most out of Kubernetes means understanding its architecture and best practices. We’re here to introduce you to the tools to design robust, scalable, and secure applications using GKE. Through a combination of videos, quizzes, and hands-on labs, you’ll get comfortable with GKE architecture and practice building your own containerized applications. You’ll begin with an introduction to Workloads, Deployments, and Jobs, then move on to learn about GKE networking, followed by persistent data and storage. You’ll then explore access control and security, logging and monitoring, and Google Cloud managed storage services. Finally, before wrapping up, you’ll learn how to use CI/CD with GKE. This course was designed for developers, DevOps engineers, and cloud architects with basic to intermediate knowledge of GKE. Oh, and if you haven’t already done so, you’ll need to complete the Getting Started with GKE course before starting this one. Okay, let’s begin.

## Workloads: Deployments and Jobs

In this section of the course, you'll learn to manage Kubernetes Deployments, Jobs, and CronJobs to deliver applications and automate tasks. You'll also explore cluster scaling techniques to optimize performance and resource utilization.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/34/video/518700)

* [YouTube: Introduction](https://www.youtube.com/watch?v=-r4tLLpl78g)

Google Kubernetes Engine, or GKE, uses containerized applications–which are packaged units with all dependencies included–for efficient deployment and management. In both GKE and Kubernetes, containers are called workloads. The concept of workloads in GKE streamlines the management of both applications and batch jobs. This becomes even more efficient with the help of Kubernetes' declarative approach. Because Kubernetes is declarative, you define your cluster's desired state, and GKE's orchestration engine will then automatically manage the cluster to match that state. So, in the first section of this course titled, “Workloads: Deployments and Jobs,” you’ll learn to define what a Deployment is, and identify methods to create and use them; configure, manage and update Deployments; define Jobs and CronJobs in GKE and explore relevant use cases; create and run Jobs; scale clusters manually and automatically; and configure node and Pod affinity.

### Video - [Creating Deployments](https://www.cloudskillsboost.google/course_templates/34/video/518701)

* [YouTube: Creating Deployments](https://www.youtube.com/watch?v=lENgg-Rxys0)

Let’s begin with Deployments, the core method for rolling out containerized applications in GKE. But before defining what a deployment is, it’s important to understand what Pods are. Pods are the smallest deployable unit in Kubernetes. They consist of one or more containers that are tightly coupled and share resources like storage and networking. Pods are ephemeral, which means that they can be created and destroyed as needed. A Deployment is a Kubernetes resource that describes the desired state of Pods. Deployments are managed by a built-in controller. A controller is a continuously running process that monitors the state of an object–or set of objects–running on the cluster, and takes action to ensure that the desired state matches the observed state. Now that we have defined some crucial terminology, let’s describe the deployment process, at a high level. The desired state of a Pod, including characteristics and details about how they should run and handle lifecycle events, is described in the Deployment specification file. After this file is submitted to the Kubernetes control plane, a Deployment controller is created. The controller is responsible for converting the desired state into the current state, and maintaining that desired state over time. During this process, the Deployment also creates and configures a ReplicaSet. The ReplicaSet controller instantiates and maintains a replica version of the Pods specified in the Deployment. So, what details are included within a Deployment? A Deployment object file is written in a YAML format and typically identifies the API version; the kind, which in this case is a Deployment; the name of the Deployment; the number of Pod replicas; a Pod template, which defines the metadata and specifications of each of the Pods in the ReplicaSet; a container image; and a specific port to expose and accept traffic for the container. Deployments exist in three distinct lifecycle states: processing, complete, and failed. The processing state indicates what task is being performed. For example the Deployment may be creating a new ReplicaSet, or scaling a ReplicaSet up or down. The complete state indicates that all new replicas are available and updated to the most current version. It also confirms that any old replicas are not running. And finally, the failed state occurs when the creation of a new ReplicaSet could not be completed. For example, if Kubernetes was unable to surface images for the new Pods, or the user who launched the operation lacks permissions, the failed state will occur. Because Deployments declare the state of Pods, they can be used for Pod management tasks such as updating, rolling back to previous revisions, and scaling or autoscaling. Deployments are designed for stateless applications. A stateless application does not store data or state in a cluster or persistent storage, and can be scaled up or down as needed without impacting the application's functionality. Examples of stateless applications include API servers that provide access to data, and websites that don’t contain dynamic content. So, how do you create a Deployment? The first option is to create a YAML file and use the kubectl apply command to describe the desired state of the Deployment object. This is a declarative method, because you specify the desired state, and then the Kubernetes API server creates the necessary objects to achieve that state. The second option is to use the kubectl create deployment command, which specifies the parameters inline. This is an imperative method because you explicitly create the objects that you want to exist. If you use the kubectl create deployment command, you need to: specify the image and tag to run in the container; how many replicas to launch; which port to expose; which API to use; and whether to save this configuration for future use. Finally, you can create a Deployment through the Google Cloud console. The GKE Workloads menu includes an option to create a Deployment. With this method you can include details such as a container image, environment variables, and initialization commands. The Google Cloud console also gives the option to display the Deployment specifications in YAML format.

### Video - [Inspecting Deployments](https://www.cloudskillsboost.google/course_templates/34/video/518702)

* [YouTube: Inspecting Deployments](https://www.youtube.com/watch?v=rUN1uIzw3ro)

Let’s explore how to inspect the state of a Deployment. Two useful commands to inspect the state of a Deployment are the “kubectl get” and “describe” commands. The “kubectl get deployment” command displays the ready, up-to-date, and available status of all the ReplicaSets within a Deployment, along with their ages. The “ready” field displays how many replicas of the application are available to your users. The “up-to-date” field displays the number of replicas that are fully up to date as per the current Deployment specification. The “available” field displays the number of replicas available to the users. And the “age" field displays the time the replicas have been available to the users. The “kubectl get deployment” command can also be used to output the Deployment configuration in a YAML format. This could be useful if you originally created a Deployment by using the “kubectl run” command, but then decided to make it a permanent, managed part of your infrastructure. By editing that YAML file to remove the unique details of the Deployment you created it from, you can add it to your repository of YAML files for future Deployments. Another option to inspect the state of a Deployment is through the “kubectl describe” command. Information includes the Deployment’s current state, its desired state, its replicas, its selector, and its events. And finally, a third option to inspect a Deployment is through the Google Cloud console. Displayed in the Google Cloud console is the Deployment name, the date the Deployment was created, the desired state, the current state, the number of replicas the Deployment is trying to maintain, the selector being used to match Pods, and a list of events that have occurred for the Deployment. The details here can be exported to a YAML file for future use.

### Video - [Updating Deployments](https://www.cloudskillsboost.google/course_templates/34/video/518703)

* [YouTube: Updating Deployments](https://www.youtube.com/watch?v=oVex_q-WUkQ)

Let’s say you recently updated a container image with new and exciting features. You want your Pods to benefit from the improvements, so you need to update your Pod’s specification with the new version of the image. This is an example of a deployment update. Deployment updates are common, and they do not affect the availability of other applications and services in the cluster. There are a few different ways to update a Deployment. The first option is to use the “kubectl apply command” with an updated Deployment specification YAML file. This command lets you update Deployment specifications, such as the number of replicas, outside the Pod template. The next option is the “kubectl set command,” which lets you make changes to Pod template specifications, like the image, resources, or selector values. Next is the “kubectl edit command,” which lets you make changes directly in the specification file. Vim, which is an open-source, screen-based text editor, can be used to open the file. After you save your changes, kubectl will automatically apply the updates. And finally, you can also update Deployments by using the Google Cloud console. Now, imagine updating your app without anyone noticing. That’s the magic of a rolling update, also known as a “ramped strategy.” When a Deployment is updated, a new set of Pods are launched in a new ReplicaSet. Then, after the new Pods start running smoothly, the old Pods in the outdated ReplicaSet are gracefully retired. GKE updates the Pods in a Deployment one at a time, meaning there’s always at least one Pod running the old version of your application and there won’t be any downtime or disruptions. So, how is a rolling update configured? There are two primary parameters used to control the speed of rolling updates: “maxSurge” and “maxUnavailable.” “maxSurge” specifies the maximum number of extra Pods that can be simultaneously running on the new version. “maxUnavailable,” in contrast, specifies the maximum number of Pods that can be unavailable at the same time. The maximum values can either be absolutes or percentages. For example, if you set maxSurge to 1 and maxUnavailable to 0, Kubernetes will update one Pod at a time, without any downtime. To explain this further, let’s work through an example of a rolling update. A Deployment has a desired number of Pods set to 10, with the maxUnavailable parameter set to 10% and the maxSurge parameter set to 5. The old ReplicaSet has 10 Pods. The Deployment will begin by creating 5 new Pods in a new ReplicaSet based on the maxSurge parameter. When those new Pods are ready, the total number of Pods will change to 15. Since the maxUnavailable parameter is set to 10%, the minimum number of Pods that can run, regardless of whether they are from the old or new ReplicaSet, is 10 minus 10%. This equals at least a minimum of 9 Pods. Therefore 6 of the 15 Pods can be removed from the old ReplicaSet, leaving a minimum of 9: 5 in the new ReplicaSet and 4 in the old ReplicaSet. Next, an additional 5 Pods will be launched on the new ReplicaSet, totalling 10 Pods in the new ReplicaSet and 14 in all ReplicaSets. Finally, the remaining 4 Pods in the old ReplicaSet will be deleted. The old ReplicaSet will be retained for rollback even though it’s empty, and this will leave 10 Pods in the new ReplicaSet. Cluster resources, like CPU and memory, will change as rolling updates occur. To control these resources, you can use requests and limits. Requests and limits can be set manually, or you can have a Pod set them for you. Requests determine the minimum amount of CPU and memory that a container will be allocated on a node. The scheduler will only assign a container to a node if that node has enough available resources to meet the container's requests. If you specify a CPU or memory value that is larger than the available resources on your nodes, your pod will never be scheduled. Limits can set the upper boundary of how much CPU and memory a container can use on a node. This prevents one container from consuming excessive resources and affecting other containers or critical node processes. Please note that the limit cannot be lower than the request. Requests and limits must be for each individual container in the Pod. Pods, however, are scheduled as a group, so you still need to calculate the total resource requests and limits by adding up the values from each individual container within the Pod. To manage the requests and limits as they come into a cluster, Kubernetes uses the kube-scheduler to decide which node to place the Pod on. If the optimal node cannot be found for the request, it is sent back to the kube-scheduler to try again. Once the optimal node is found, the kubelet is used to enforce resource limits and ensure that the running container is not allowed to use more of that resource than the limits set. The kubelet is also responsible for reserving request amounts for containers. CPU resources are measured in millicores. For example a container that needs two full cores to run would be measured as 2000m (millicores). A container that needs one quarter of a core, would be measured as 250m.

### Video - [Other Deployment strategies](https://www.cloudskillsboost.google/course_templates/34/video/518704)

* [YouTube: Other Deployment strategies](https://www.youtube.com/watch?v=P4khxyn98Jo)

GKE supports multiple deployment strategies to give you the flexibility and control over how you introduce changes and updates to your applications. Let’s briefly explore three primary deployment options: recreate deployment, blue/green deployment, and canary test. First, you can use the recreate strategy. Unlike a rolling strategy, where both old and new Pods are running simultaneously, the recreate strategy is to delete the old Pods before creating new ones. The advantage of this strategy is that once the new Pods are available, all users will gain access to the updated deployment at the same time. The disadvantage is that because all Pods won’t be instantly available, users will experience disruptions while new Pods are being created. Another option is to use the blue/green deployment strategy. This strategy is to create a completely new Deployment with a newer version of the application. Blue refers to the old version, and green refers to the new version. When the Pods are ready, the traffic can be switched from the old blue version to the new green version. Then, Pods in the blue deployment version will be deleted. The advantage of this strategy is that the rollouts are instantaneous, and the newer versions can be tested internally before releasing them to all users. The disadvantage is that resource usage is doubled during the Deployment process, resulting in cost increases, capacity constraints, and inefficiencies. Finally, the canary update strategy is to gradually move traffic to the new version of your application. The main advantage of this strategy is that it minimizes excess resource usage during the update. Also, because the rollout is gradual, issues can be identified before they affect all instances in the application. The disadvantages are that canary deployments are often slower and may require tools like Anthos Service Mesh to accurately move the traffic to the new version. Now, what if there’s an issue with a deployment? You’ll likely need to undo, or “roll back” the update by using the kubectl rollout undo command. The rollout undo command will revert the Deployment to the previous version or to a different version that you specify. Using the kubectl rollout history command, you can inspect the rollout history. By default, the previous 10 ReplicaSets are retained. You can change the default by specifying a revision history limit in the Deployment specification. The kubectl rollout undo and kubectl rollout history commands can also be run from Cloud Shell in the Google Cloud console. You can view a revision list before making any changes. Typically, an automatic rollout is triggered when a deployment is edited, so frequent updates with small fixes will lead to a large amount of rollouts. In these situations, it can be difficult to pinpoint which specific rollout is causing a problem. You can use the kubectl rollout pause command to halt the rollout so that you can troubleshoot. Please note that any new changes to the deployment will only be implemented after the rollout is resumed by using the kubectl rollout resume command. Then all new changes will be released as one new version. To monitor the status of a rollout, you can use the kubectl rollout status command. Finally, you can use the kubectl delete command to delete a rollout, or you can delete it by using the Google Cloud console. Kubernetes will delete all resources managed by the Deployment, including any running Pods.

### Video - [Jobs and Cronjobs](https://www.cloudskillsboost.google/course_templates/34/video/518705)

* [YouTube: Jobs and Cronjobs](https://www.youtube.com/watch?v=GApPoaJELXs)

Like a Deployment, a Job is a Kubernetes resource. Jobs create one or more Pods to execute a specific task, and then automatically terminate the pods once the task is finished. At its simplest, a Job creates a single Pod, and tracks the task to completion within that Pod. Jobs are also useful if a Pod fails. Unlike Kubernetes controllers, a Job manages a task to completion, rather than to a desired state. Let’s explore this concept with an example–like when a user uploads a video file to a web server for transcoding. In this instance, transcoding would be the task. To begin, the web server would create a Job manifest in a YAML file for the task, and then a Job would be created on the cluster. From there, the Job controller would schedule a Pod on a node to complete the task, and then monitor the Pod. If a node fails, the Pod is lost. The Job controller, which monitors the Pod, would be aware that the task has not been completed. As a result, the Job controller would then reschedule the Pod to run on a different node, and continue to monitor the Pod until the task is completed. After the task is complete, the Job controller would remove the finished Job and any Pods associated with it. It’s at this point that the video would be successfully transcoded! There are two main types of Jobs: non-parallel and parallel. Non-parallel Jobs are the simplest type, used to run a single task one time and ensure its completion. Examples include image processing and data migration. Kubernetes creates one Pod to execute the task, and the Job is considered finished when the Pod exits successfully, or when the required number of completions is reached. If the Pod fails, it will be recreated. Alternatively, parallel Jobs schedule multiple Pods to work on the Job concurrently, but with a predefined limit on the number of completions. This means that each Pod works independently on its assigned task and the Job finishes when the specified number of tasks are completed successfully. Parallel Jobs are useful for situations where tasks need to be completed more than once, like bulk image resizing or parallel scientific computations. Other types of parallel Jobs exist such as work queues and indexed completion jobs, but these are beyond the scope of this course. Let's explore another example, this time how a parallel Job can be used to calculate pi to 2000 decimal places. In a manifest file, the specific type of a Job object is identified through the value of its "Kind" property. This property acts as a label that tells the Kubernetes system what type of batch process the Job represents and how it should be handled. Details about how the job should perform its tasks are specified in the Job spec. Nested within the job spec is a Pod template, which acts as a blueprint for the Pods that the Job will create. Also, this is where its restartPolicy is defined. The restartPolicy can either be set to Never or onFailure. If the restartPolicy is set to Never, it means that any container failure within a Pod will cause the entire Pod to fail permanently. In response, the Job controller will automatically create a new Pod to continue the task. If set to OnFailure, the Pod remains on the node, but the Container restarts. And then there is a backOffLimit field that controls how many times the Job controller should attempt to restart failed Pods before considering the Job itself as failed. Setting spec.parallelism greater than 1 signals to the Job controller that this is a parallel job and that it should create and run multiple Pods concurrently to execute the Job's tasks. To run multiple Pods concurrently for a Job, you need to identify both the completions and parallelism values. The Job controller will initially create as many Pods as specified by parallelism. It will then keep launching new Pods to replace any that finish, up until the total successful completions reach the completions count. Like other Kubernetes objects, Jobs can be inspected by using the kubectl ‘describe’ command. You can use the kubectl get command and label selector to filter Pods. Job details can also be viewed from the Google Cloud console. You can delete a Job by using a kubectl delete command. When you delete a Job, the Job’s Pods will also be deleted, but you can retain the Pods by setting the cascade flag to false. Alternatively, you can delete jobs directly in the Google Cloud console. Let’s finish this lesson by exploring Cronjobs. For scheduling Jobs, the Cron format offers a powerful and flexible way to specify precise dates and times for repeated execution. Cron uses a straightforward text format, which is like a mini-language with specific rules: you write a string with different parts separated by spaces, and each part controls a different aspect of when and what to execute. You can use an asterisk to represent an entire range of possible values, for example, each minute or each hour. Cron lets you schedule your jobs precisely by specifying individual times or ranges. You can do this by listing specific values separated by commas (like 1, 3, 7) or using a hyphen to define a range (like 1-5). To make a value repeat at regular intervals, add a slash followed by the interval number. This works for both asterisks (to repeat every interval) and ranges (to repeat the entire range at the specified interval).

### Video - [Cluster scaling](https://www.cloudskillsboost.google/course_templates/34/video/518706)

* [YouTube: Cluster scaling](https://www.youtube.com/watch?v=BaSAtVSZaZY)

Let’s explore cluster scaling in Google Kubernetes Engine. GKE can either be used in Standard mode or Autopilot mode, but cluster scaling is only available in Standard mode. Unlike Autopilot mode, which automatically scales your cluster based on demand, Standard mode hands you the reins for manual scaling. As your applications' needs fluctuate, the resources they require will change too. The good news is that you can easily scale your cluster up or down to match those changing demands, right from the Google Cloud console or CloudShell. In GKE, a cluster contains one or more node pools. A node pool groups nodes that have the same configuration type within a cluster. The size of a node pool is set by specifying a minimum and maximum number of nodes, the maximum being 1000. Node pools use a NodeConfig specification. Each node in the pool has a Kubernetes node label that has the node pool's name as its value. When a container cluster is created, the number and type of nodes specified becomes the default node pool. Then additional custom node pools of different sizes and types can be added to the cluster. And while individual node pools within a cluster can be scaled down to 0, the cluster itself can not be entirely shut down. This is because a cluster must have at least one node to run system Pods. When a cluster is scaled down, nodes are treated the same regardless of whether or not they are running Pods. As stated previously, a cluster can be resized manually or automatically. You can resize a cluster in Standard mode by manually entering the resize gcloud command, or by using the Google Cloud console. The resize command will remove instances at random, and any running Pods will terminate gracefully. And what if you want to automatically scale clusters? GKE’s cluster autoscaler, which is a feature available on your Standard cluster node pools, automatically resizes a cluster based on the resource demands of your workload. The cluster autoscaler is disabled by default. When enabled, GKE will automatically add new nodes to a cluster whenever it detects that your Pods lack sufficient resources to operate as intended. GKE will also delete underutilized nodes if their Pods can run on other nodes. Now let’s say there is a scenario where all of your node pools are low on resource capacity. To fix this, one or more Pods will need to be terminated or additional nodes need to be added. The Pod will enter a holding pattern as it awaits additional resource capacity. During this period, the scheduler, which has the job of filtering any nodes that don't meet a Pod's specific scheduling needs, sets the schedulable Pod condition to false and marks it as Unschedulable. If a cluster doesn’t need to scale up, the cluster autoscaler will check for disposable nodes every 10 seconds. A node is considered disposable if all of the following conditions are true: total CPU and memory is less than 50% of a node’s allocatable capacity, all pods running on the node can be moved to other nodes, the cluster does not have scale-down disabled. The cluster autoscaler will then continue to monitor and if a node is unneeded for more than 10 minutes, it will be terminated The cluster autoscaler can handle up to 15,000 nodes, with each node supporting a maximum of 256 pods. However, there's a cluster-wide limit of 200,000 pods, regardless of your node setup. Be aware that standard Google Cloud limits for Compute Engine instances still apply. So if you haven't increased your default quota, new VMs won't start and disruptions may occur. Let’s wrap up by exploring some common gcloud commands used for autoscaling. You can add the --enable-autoscaling flag to a clusters create command to create a new cluster with autoscaling enabled. You can also add --enable-autoscaling flag to a node-pools create command to create a new node pool with autoscaling enabled. You can enable or disable autoscaling on existing node pools by adding a --enable-autoscaling, or --no-enable-autoscaling flag to a cluster update command. These actions can be executed from the Google Cloud console too. With zonal clusters, all resources (nodes and the control plane) are created in the same zone by default. If secondary zones are enabled, all node pools will be duplicated in the secondary zone, similar to how pools are duplicated for regional clusters.

### Video - [Controlling Pod placement with labels and affinity rules](https://www.cloudskillsboost.google/course_templates/34/video/518707)

* [YouTube: Controlling Pod placement with labels and affinity rules](https://www.youtube.com/watch?v=ajjdJlWukk8)

Pod placement in GKE refers to the process of controlling where your application's Pods are deployed across the cluster's nodes. This process plays a crucial role in optimizing performance, ensuring availability, and managing resource allocation within your GKE environment. In GKE Standard mode, you can control and manage Pod placement by using labels, affinity rules, taints, and tolerations. Let’s begin by exploring labels and affinity rules. To identify the appropriate location to place a Pod, GKE uses a Pod’s specifications–like resource requests and limits. Then, the scheduler automatically spreads Pods across nodes within their capacity limits. Upon startup, nodes automatically receive zone-specific labels from the kubelet. This ensures accurate tracking even when nodes span across compute zones. But what if you want certain application types to run on a specific node? In GKE Standard mode, you can use the nodeSelector field within a Pod's configuration to specify its preferred node labels. Think of it as a checklist the Pod uses to find a compatible home. Node labels can be automatically assigned, or can be manually added to identify nodes with specific characteristics. This creates a system of matching between Pod preferences and node capabilities. If your Pods need more CPU or memory than the general-purpose class allows, or if they require a specific CPU type, you can use the nodeSelector field in a Pod's specification to request the "Balanced" compute class. This will ensure Pods are placed on nodes with the resources they need. Autopilot can do this for you, just be sure to add the cloud.google.com/compute-class label in the nodeSelector rule for the Pods to be placed on a specific compute class. This can also be accomplished with node affinity rules. If a node's capabilities match the Pod’s preferences, then the node is considered a suitable host for that Pod. As an example, let’s say a Pod needs a node from the "ssd" node pool to run. In this case, if a node doesn't have an “ssd” label, the Pod will stay in waiting mode. However, please note that future label changes won’t impact Pods that are already running. Like nodeSelectors, affinity rules can also be used to influence Pod placement. Node affinity and anti-affinity are label-based Pod placement approaches, but with more flexible and expressive rules. Affinity rules can be used to ensure that two or more pods are placed together on the same node, and anti-affinity rules can be used to spread Pods across different nodes. Unlike nodeSlectors, which only schedule Pods if they meet requirements, node affinity lets you define rules as preferences instead of requirements. Affinity and anti-affinity rules can be expressed by using two specific keywords: 'requiredDuringSchedulingIgnoredDuringExecution' and ‘preferredDuringSchedulingIgnoredDuringExecution’. ‘RequiredDuringScheduling’ enforces a strict requirement that must be met when scheduling Pods, whereas ‘PreferredDuringScheduling’ indicates a flexible preference that isn’t mandatory. You can set a weight for this preference, where 1 is the weakest and 100 is the strongest. The stronger the weight, the more the scheduler will try to favor nodes that match your preference. ‘IgnoredDuringExecution’ indicates that changes to labels will not affect Pods that are already running. Now let’s say there is a scenario where the NodeSelectorTerms field is used and the node must meet all matchExpression requirements. This is an example of affinity, where a single matchExpression–accelerator-type–is specified. Logically, the matchExpression values are joined using the boolean AND. From there, the In operator would indicate that only one of the listed values must match the preferences. Other operators, like ‘NotIn’, can be used to configure a node anti-affinity rule. Let’s explore another example, this time one that indicates a flexible preference. You might recall that you can set a weight for a preference. So if the weight has been set to 90, it signals a strong preference. After the scheduler evaluates each available node, it will assign the Pod to the node that earns the highest overall score, and it will take this preference and its weight into account. The node affinity rules in this example are set to express a strong preference for nodes that are in the n1-highmem-4 or n1highnem-8 node pools. It’s recommended that node pool names indicate the type of compute instances that will be used to create the nodes. All nodes within a node pool share identical configurations, so a descriptive name accurately reflects their hardware specifications. Also, GKE automatically assigns labels to nodes based on their node pool names. This empowers you to easily create preferences for specific hardware types using those labels during Pod scheduling. Affinity and anti-affinity rules aren't limited to individual nodes—they can be applied at broader levels of infrastructure organization. You can achieve this by using topologyKeys, which allow you to define preferences based on broader topology domains like zones and regions. For example, you can use a topologyKey to indicate that you prefer not to schedule a Pod to a webserver zone if other Pods are already running in that zone. And if you need more granular control, inter-pod affinity and anti-affinity features can be used, because they consider the labels of other Pods already running on nodes.

### Video - [Controlling Pod placement with taints and tolerations](https://www.cloudskillsboost.google/course_templates/34/video/518708)

* [YouTube: Controlling Pod placement with taints and tolerations](https://www.youtube.com/watch?v=0j8o4329_Nk)

In addition to affinity and anti-affinity rules, taints and tolerations can be used to control Pod placement. A taint is a special label applied to a node that acts as a restriction. It indicates that the node is not suitable for running certain Pods unless those Pods specifically tolerate the taint. A toleration, therefore, is a specification within a Pod's configuration that allows it to be scheduled onto a node that has a matching taint. Unlike nodeSelector, affinity, and anti-affinity rules, taints are configured on nodes instead of Pods, and they affect all Pods in the cluster. You can apply a taint on a node by using the kubectl taint command. A taint consists of three elements: A key, which gives a descriptive name that represents the taint's purpose; a value, which provides optional information that further clarifies the taint; and an effect to prevent or discourage Pods from being scheduled on tainted nodes. When a node has a taint, it notifies certain types of Pods that they are not welcome on a node. A toleration on a Pod acts like an exception, and it allows it to override the taint and run on the node anyway. A toleration field consists of four components: a key, value, effect, and operator. A Pod can only override a taint if its toleration has the same key, effect, and passes the value check using its operator. If a toleration's operator is "Equal", its value must match the taint's value exactly to be effective. If a toleration's operator is "Exists", it only needs to have the same key and effect as the taint, regardless of value. And if multiple taints are applied to a node, new Pods will be prevented from landing on the node, and running Pods will be evicted. There are three effect settings that can be applied to taints and tolerations: NoSchedule prevents new Pods from being scheduled on the tainted node, unless the toleration effect is also set to NoSchedule. PreferNoSchedule encourages–but does not strictly prohibit–Pod scheduling, which means that the Pod might still be scheduled. NoExecute evicts existing Pods and prevents new ones from landing on the tainted node, unless the Pods have a toleration set to NoExecute. Constraining Pods to particular nodes adds additional complexity to cluster management. GKE helps abstract some of that complexity by using node pools to place Pods on the correct hardware. This is because node pools have the same hardware, and NodeSelectors can be used to direct Pods to the correct node pool. Let’s demonstrate this concept through an example of a GKE cluster that has two node pools, one named “High CPU” and the other named “High Memory.” GKE automatically labels the nodes in each node pool with the specified name, so that NodeSelectors can direct Pods to the appropriate nodes. Imagine that a fleet of frontend web servers requires a moderate increase of vCPUs relative to memory. In this case, the nodeSelectors identify nodes with the “High CPU” name and GKE will direct those Pods to the “High CPU” node pool.

### Video - [Getting software into your cluster](https://www.cloudskillsboost.google/course_templates/34/video/518709)

* [YouTube: Getting software into your cluster](https://www.youtube.com/watch?v=dv2SmDzsbY4)

Google Cloud tools like Cloud Build, Artifact Registry, and Helm help get software into your cluster. But you are responsible for defining deployment patterns and services for reliable and efficient operation. Cloud Build is a serverless tool that helps you build, test, and deploy software across various environments and programming languages. Artifact Registry provides a single location to store, manage, and secure build artifacts, like container images. GKE can fetch these images from the registry and then run them in Pods. Helm is an open-sourced, package manager that simplifies application management. Helm provides traditional software installation and management functionalities similar to what apt-get and yum provide for Linux. Developers can use Helm to organize Kubernetes objects into packages called “charts.” Charts manage the deployment of complex applications, and can be versioned, shared, and published. And they also manage the installation of required dependencies. Helm and the API server work together to install, upgrade, query, and remove Kubernetes resources. Helm makes open-source software deployment easier and reduces the risk of error, but the tool still needs to be managed. Google Cloud Marketplace offers development stacks, solutions, and services to help manage and accelerate development. And thanks to kublectl commands and Helm charts, installation is automated.

### Lab - [Creating Google Kubernetes Engine Deployments](https://www.cloudskillsboost.google/course_templates/34/labs/518710)

Architecting with Google Kubernetes Engine: Creating Kubernetes Engine Deployments

* [ ] [Creating Google Kubernetes Engine Deployments](../labs/Creating-Google-Kubernetes-Engine-Deployments.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/34/quizzes/518711)

#### Quiz 1.

> [!important]
> **You are configuring a Job to convert a sample of a large number of video files to a different format. Which parameter should you configure to stop the process once a sufficient quantity is reached?**
>
> * [ ] completions=4
> * [ ] replicas=4
> * [ ] parallelism=4
> * [ ] backofflimit=4

#### Quiz 2.

> [!important]
> **How do you configure a Kubernetes Job so that Pods are retained after completion?**
>
> * [ ] Configure the backofflimit parameter with a non-zero value.
> * [ ] Configure the cascade flag for the Job with a value of false.
> * [ ] Set an activeDeadlineSeconds value high enough to allow you to access the logs.
> * [ ] Set a startingDeadlineSeconds value high enough to allow you to access the logs.

#### Quiz 3.

> [!important]
> **You have autoscaling enabled on your cluster. What conditions are required for the autoscaler to decide to delete a node?**
>
> * [ ] If a node is underutilized and there are no Pods currently running on the Node.
> * [ ] If a node is underutilized and running Pods can be run on other Nodes.
> * [ ] If the overall cluster is underutilized, a randomly selected node is deleted.
> * [ ] If the overall cluster is underutilized, the least busy node is deleted.

#### Quiz 4.

> [!important]
> **Inter-pod affinity rules are specified at the zone level, not at the individual Node level. To apply this override, which additional parameter must be configured in the Pod manifest YAML?**
>
> * [ ] matchLabels: failure-domain.beta.kubernetes.io/zone
> * [ ] label: failure-domain.beta.kubernetes.io/zone
> * [ ] zone: failure-domain.beta.kubernetes.io/zone
> * [ ] topologyKey: failure-domain.beta.kubernetes.io/zone

#### Quiz 5.

> [!important]
> **After a Deployment has been created and its component Pods are running, which component is responsible for ensuring that a replacement Pod is launched whenever a Pod fails or is evicted?**
>
> * [ ] Deployment
> * [ ] ReplicaSet
> * [ ] StatefulSet
> * [ ] DaemonSet

#### Quiz 6.

> [!important]
> **You are resolving a range of issues with a Deployment and need to make a large number of changes. Which command can you execute to group these changes into a single rollout, thus avoiding pushing out a large number of rollouts?**
>
> * [ ] kubectl stop deployment
> * [ ] kubectl delete deployment
> * [ ] kubectl rollout resume deployment
> * [ ] kubectl rollout pause deployment

#### Quiz 7.

> [!important]
> **Which of the following commands will display the desired, current, up-to-date, and available status of all the ReplicaSets within a Deployment?**
>
> * [ ] kubectl describe
> * [ ] kubectl get
> * [ ] Google Cloud Console
> * [ ] kubectl logs

## Google Kubernetes Engine Networking

In this section of the course, you'll gain a comprehensive understanding of Kubernetes networking. You'll delve into how pods and clusters communicate, create services to expose your applications to the network, and configure load balancers for external access. You'll also examine container-native load balancing and master the configuration of Google Kubernetes Engine Networking for optimal performance and security.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/34/video/518712)

* [YouTube: Introduction](https://www.youtube.com/watch?v=VWoSDCvx33c)

When you design an application, it’s important to understand what other types of applications it’ll talk to and where they can be located. This means it’s also important to consider how to expose them to the outside world for consumption, and how to balance incoming traffic to cope with varying workload demands. In this section of the course titled, “Google Kubernetes Engine Networking”, you’ll explore Kubernetes networking, including Pod, and cluster networking; create services to expose to applications running within Pods; configure load balancers to expose services to external clients; explore container-native load balancing in GKE; explain the purpose of network policies in GKE; and configure Google Kubernetes Engine Networking.

### Video - [Pod networking](https://www.cloudskillsboost.google/course_templates/34/video/518713)

* [YouTube: Pod networking](https://www.youtube.com/watch?v=1ayxQP1Jswo)

The Kubernetes networking model relies heavily on IP addresses, because services, Pods, containers, and nodes all communicate using IP addresses and ports. And Kubernetes provides different types of load balancing to direct traffic to the correct Pods. Let’s review some of the basics of Pod networking. A Pod is a group of containers that shares storage and networking. This is based on the “IP-per-pod” model of Kubernetes, where each Pod is assigned a single IP address, and the containers within a Pod share the same network namespace, including that IP address. In Kubernetes, namespaces provide a mechanism for isolating groups of resources within a single cluster. For example, let’s say a legacy application uses nginx as a reverse-proxy for client access. If the nginx container runs on TCP port 80, and the legacy application runs on TCP port 8000, the two containers appear as though they’re installed on the same machine because both containers share the same networking namespace. The nginx container will contact the legacy application by establishing a connection to “localhost” on TCP port 8000. This works well for a single Pod, but workloads can run in multiple Pods and can be composed of many different applications that need to talk to each other. So, how do Pods talk to other Pods? Each Pod has a unique IP address, just like a host on the network. On a node, Pods are connected to each other through the node's root network namespace, which ensures that Pods can find and reach each other on that virtual machine. The root network namespace is connected to the Node's primary NIC, network interface card. Using the node’s VM NIC, the root network namespace is able to forward traffic out of the node. This means that the IP addresses on the Pods must be routable on the network that the node is connected to. In GKE, nodes get their Pod IP addresses from address ranges assigned to the Virtual Private Cloud, or VPC, being used. VPCs are logically isolated networks that provide connectivity for resources deployed within Google Cloud, such as Kubernetes Clusters, and Compute Engine instances. A VPC can be composed of many different IP subnets in regions around the world. When you deploy a GKE cluster, you can select a VPC along with a region or zone. By default, a VPC has an IP subnet pre-allocated for each Google Cloud region in the world. The IP addresses in this subnet are then allocated to the compute instances that you deploy in the region. Then there are GKE cluster nodes, which are compute instances that GKE customizes and manages for you. These machines are assigned IP addresses from the VPC subnet that they reside in. Google Cloud offers something called an alias IP, which can configure additional secondary IP addresses or IP ranges on Compute Engine VM instances. VPC-Native GKE clusters automatically create an alias IP range to reserve approximately 4,000 IP addresses for cluster-wide services that you might want to create later. VPC-Native GKE clusters also create a separate alias IP range for your Pods. You’ll recall that each Pod must have a unique address, so this address space will be large. By default the address range uses a /14 block, which contains over 250,000 IP addresses. Google Cloud quota limits will prevent you from running 250,000 Pods in a single cluster, but the large IP address range allows GKE to divide the IP space among the nodes. Using this large Pod IP range, GKE allocates a much smaller /24 block to each node, which contains about 250 IP addresses. This allows for 1000 nodes, with over running 100 pods each, by default. This functionality allows you to configure both the number of nodes you expect to use and a maximum number of Pods per node. The nodes allocate a unique IP address from their assigned range to each Pod as it starts. Because the Pods' IP addresses are a part of the alias IP, GKE automatically configures your VPC to recognize this range of IP addresses as an authorized secondary subnet of IP addresses. This ensures the Pod’s traffic is permitted to pass the anti-spoofing filters on the network. Each node maintains a separate IP address space for its Pods, which means the nodes don’t need to perform network address translation on the Pod IP addresses. Pods can directly connect to each other by using their native IP addresses. Pod IP addresses are natively routable within the cluster's VPC network and other VPC networks connected to it by VPC Network Peering. The traffic from clusters is routed or peered inside Google Cloud but becomes NAT translated at the node IP address if it has to exit Google Cloud.

### Video - [Kubernetes Services](https://www.cloudskillsboost.google/course_templates/34/video/518714)

* [YouTube: Kubernetes Services](https://www.youtube.com/watch?v=mZsBFkvZF3o)

So, what is a Service in Kubernetes? A Service is a logical abstraction that defines a set of Pods and a single IP address for accessing them. A Service is how the outside world accesses the cluster. Think of it like a GKE doorman or bouncer, keeping out unwanted visitors! Services are used to provide a stable IP address and name for a Pod, because these can change frequently. With Services, these details can remain the same through updates, upgrades, scalability changes, and Pod failures. This stability allows applications to connect to each other and be found by external clients, even when Pods are updated or replaced. Pods have a different lifecycle compared to virtual machines. While VMs are typically designed to be durable and persist through application updates and upgrades, Pods are usually terminated and replaced with newer Pods after application updates. Pod IP addresses are ephemeral, which means that they’re temporary, and as a result of the new Pod deployment, the updated version of the application gets a new IP address. Also, if a Pod deployment is rescheduled for any reason, then the Pod gets assigned a new IP address. Please note, if a Pod's IP address changes unexpectedly, it can cause significant Service disruptions. All of this is to say that locating an application running in your cluster by IP address can be difficult, so this is where a Service comes in. The dynamic collection of IP addresses created by a Kubernetes Service is called an endpoint. Endpoints belong to Pods that match the Service’s label selector. When a Service is created, it’s issued a static virtual IP address from the pool of IP addresses that the cluster reserves for Services. Unlike the Pod’s IP address, the Virtual IP is durable. It’s published to all the nodes in the cluster and it doesn't change, even if all of the Pods behind it change. In GKE, this range is automatically managed, and by default contains over 4,000 addresses per cluster. There are multiple ways to search for and locate a Service in GKE, including searching by environment variables, DNS, or by Service type. Let’s explore what each means, starting with environment variables. When a new Pod starts running on a node, kubelet adds a set of environment variables for each active service in the same namespace as the Pod. This allows the Pod to access the Service by using the environment variables. However, this isn’t the most robust mechanism for discovery, as changes made to a Service after Pods have been started will not be visible to the Pods that are already running. Let’s explore an example of the environment variables for a Service named “demo”, where several environment variables have been defined to hold commonly used values such as the host IP, port address, and tcp port. A better practice to locate a Service in GKE is by using a DNS server. DNS comes pre-installed in Google Kubernetes Engine, and the DNS server watches the API server to identify when new Services get created. When a new Service is created, kube-dns, which is a lightweight DNS server, automatically creates a set of DNS records. This allows all the Pods in the cluster to resolve Kubernetes Service names automatically. By default, a client Pod’s DNS search list will include the Pod’s own namespace and the cluster’s default domain. The final way to find a Service in GKE is by changing the service type, which we’ll explore next.

### Video - [Service type and load balancers](https://www.cloudskillsboost.google/course_templates/34/video/518715)

* [YouTube: Service type and load balancers](https://www.youtube.com/watch?v=ock9mziYCLw)

In addition to using environmental variables and DNS to find a Service in GKE, you can also find a Service by changing the Service type. There are three principal types of Services: ClusterIP, NodePort, and LoadBalancer. A Kubernetes ClusterIP Service has a static IP address and operates as a traffic distributor within the cluster. ClusterIP Services aren’t accessible by resources outside the cluster. Other Pods will use the ClusterIP as their destination IP address when they communicate with the Service. To create a ClusterIP Service, you need to start by creating a Service object by defining its kind in a YAML file. If a Service type isn’t specified during the creation of the Service, ClusterIP will be the default Service type. Next, a label selector can be used to choose the Pods that will run the target application. In this case, the Pods with a label of “app: Backend” are selected and included as endpoints for this Service. From there, you’ll need to specify the port that the target containers are using, for example, TCP port 6000. This means that this Service will receive traffic on port 3306 and then remap it to 6000 as it delivers it to the target Pods. By creating, loading, or applying this manifest, ClusterIP Service named “my-service” will be created. When the Service is created, the cluster control plane, which is responsible for managing the cluster's nodes and workloads, assigns a virtual IP address —also known as ClusterIP— from a reserved pool of Alias IP addresses in the cluster’s virtual private cloud. This IP address won’t change throughout the lifespan of the Service. The cluster control plane selects Pods to include in the Service’s endpoints based on the label selector on the Service and the labels on the Pods. Alright, so we’ve established that the ClusterIP Service is useful for internal communication within a cluster, but what about the external communication? This is where we look at the NodePort Service. The NodePort Service is built on top of a ClusterIP Service. This means that when you create a NodePort Service, a ClusterIP Service is automatically created in the process to distribute the inbound traffic internally across the set of Pods. So, let’s say that there’s a Service that can be reached from outside of the cluster by using the IP address of any node and the corresponding NodePort number. For this to work, we’ll say that traffic through this port would need to be directed to a Service on Port 80 and further directed to one of the Pods on port 9376. NodePort Service can be useful to expose a Service through an external load balancer that you set up and manage yourself. Using this approach, you would have to deal with node management, making sure there are no port collisions. And finally, there is the LoadBalancer Service, which also builds on the ClusterIP Service and can be used to expose a Service to resources outside the cluster. With GKE, the LoadBalancer Service is implemented using Google Cloud’s passthrough Network Load Balancer. When a LoadBalancer Service is created, GKE automatically provisions an external passthrough Network Load Balancer for inbound access to the Services from outside the cluster. Client traffic will be directed to the external IP address of the Google Cloud network load balancer, and the load balancer will forward the traffic on to the nodes for this Service. The nodes will forward that traffic to the internal LoadBalancer Service, and the LoadBalancer Service will then forward the request to one of the Pods. To create a LoadBalancer Service, you need to start by specifying the type: LoadBalancer. Google Cloud will then assign a static load balancer IP address that is accessible from outside your project. When you specify kind: Service with type: LoadBalancer in the resource manifest, GKE creates a LoadBalancer Service. GKE makes appropriate Google Cloud API calls to create either an external passthrough Network Load Balancer or an internal passthrough Network Load Balancer. GKE creates an internal passthrough Network Load Balancer when you add the networking.gke.io/load-balancer-type: Internal annotation. Otherwise, GKE creates an external passthrough Network Load Balancer.

### Video - [Ingress](https://www.cloudskillsboost.google/course_templates/34/video/518716)

* [YouTube: Ingress](https://www.youtube.com/watch?v=GcrkQacNIHI)

One of the most powerful tools to direct traffic into your clusters is the Ingress resource. The Ingress resource operates one layer higher than Services. You can think of it as a Service for Services. That being said, Ingress is not a Service, or even a type of Service. It’s a collection of rules that direct external inbound connections to a set of Services within the cluster. With GKE, Kubernetes Ingress resources are implemented using Cloud Load Balancing. When an Ingress resource is created in the cluster, GKE creates an Application Load Balancer and configures it to route traffic to the application. Ingress can deliver traffic to either NodePort Services or LoadBalancer Services. Within a simple Ingress resource, the Ingress controller creates an Application Load Balancer using the Ingress object specification. In the object, a backend Service can be selected by specifying the Service name “demo” and the Service port “80.” This configuration tells the Application Load Balancer to route all client traffic to the Service named “demo” on port 80. Let’s shift to a different example where there is an Ingress manifest and inside the specifications are rules. Currently, GKE only supports HTTP rules, and each rule takes the same name as the host. The host can be further filtered based on the path, and a path will have a Service backend that defines the Service’s name and port. Ingress supports multiple host names for the same IP address, for example demo.example.com and lab.user.com. The traffic will be redirected from the Application Load Balancer, based on the host names, to their respective backend Services. For example, the load balancer will route traffic for demo.example.com to the Service named demo1 on port 80. This example considers rules based on the URL path. Under Spec, a path defined as /demopath will be directed to the backend Service named demo1. Similarly, /labpath will be directed to its backend Service demo2. So what happens to the traffic that doesn’t match any of these host-based or path-based rules? The traffic with no matching rules is simply sent to the default backend, which is a service that acts as the destination for traffic that doesn't match any of the specified paths in an Ingress manifest. A default backend can be specified by providing a backend field in your Ingress manifest. If a default backend isn’t specified, GKE will supply one that replies with error code 404. Ingress can be updated with a kubectl edit command. When the Ingress resource has been updated, the API server will tell the Ingress controller to reconfigure the Application Load Balancer according to the changes that have been made. Ingress can also be updated by using the kubectl replace command, which replaces the Ingress object manifest file entirely. Ingress for an external Application Load Balancer natively supports many Google Cloud services. Using Identity-Aware Proxy provides granular access control at the application level. With this, authenticated users can have HTTPS access to the applications within a cluster without any VPN setup. Google Cloud Armor provides built-in protection against distributed denial of service (DDOS) and web attacks for clusters using an Application Load Balancer. Security rules can be set up to allow list or deny list IP addresses or ranges. Predefined rules can also be used to defend against cross-site scripting and SQL injection application-aware attacks. Security rules can be customized to mitigate multivector attacks and restrict access using geolocation. Cloud CDN allows an application’s content to be brought closer to its users. It does this by using more than 100 Edge points of presence. These settings can be configured using BackendConfig. BackendConfig is a custom resource used by the Ingress controller to define configuration for all of these Services. Ingress gains many security features from the underlying Google Cloud resources it relies on. For example, Ingress provides TLS termination support at the loadbalancer at the edge of the network. This allows the load balancer to create another connection to the destination. Although this second connection is unsecured by default, it can be secured. This allows SSL certificates to be managed in one place. Ingress can serve multiple SSL certificates. Ingress also supports the HTTP/2 standard in addition to HTTP/1.0 and HTTP/1.1. So, if a microservices-based application is being developed, it’s necessary to ensure that each microservice’s communication with all the others uses a high-performance, low-overhead remote procedure call system. gRPC is a common way to solve this, and it needs HTTP/2. So gRPC can be used along with HTTP/2 to create performant, low-latency, scalable microservices within the cluster. Finally, with multi-cluster and multi-region support, a single standard Ingress resource can be used to load balance traffic globally to multiple clusters across multiple regions. This also supports location-based load balancing, called geo-balancing across multiple regions, which improves the availability of the cluster.

### Video - [Container-native load balancing](https://www.cloudskillsboost.google/course_templates/34/video/518717)

* [YouTube: Container-native load balancing](https://www.youtube.com/watch?v=OWLQ7v0YnIw)

With the combination of the Application Load Balancer and the Ingress object, it's possible to encounter something called the double-hop dilemma. The double-hop dilemma describes when traffic makes an unnecessary second hop between VMs running containers in a GKE cluster. Let’s explore a scenario where traffic is distributed without a container-native load balancer. The responsibility of a regular Application Load Balancer is to distribute traffic to all nodes of an instance group, regardless of whether the traffic was intended for the Pods within that node. By default, a load balancer can route traffic to any node within an instance group. When the client sends traffic, it’s directed through the Network Load Balancer. The Network Load Balancer chooses a random node in the cluster and forwards the traffic to it. In this example, there are three possible Nodes to choose from. Node 1 is chosen. Next, to keep the Pod use as even as possible, the initial node will use kube-proxy to select a Pod at random to handle the incoming traffic. The selected Pod might be on this node or on another node in the cluster. For this example, let’s say that Node 1 chooses Pod 5, which isn’t on this node. This means that Node 1 will forward the traffic to Pod 5 on Node 3. Pod 5 then directs its responses back through Node 1, which is when the double-hop happens. Node 1 then forwards the traffic back to the Network Load Balancer, which sends it back to the client. This method has two levels of load balancing, one by the load balancer, and the other by kube-proxy. This results in multiple network hops. The response traffic also follows the same path. As the name double-hop dilemma indicates, this method is not optimal for load balancing. This process only keeps the Pod use even at the expense of increased latency and extra network traffic, which is not ideal. When using traditional Kubernetes networking, you’ll need to decide what’s more important: having the lowest possible latency or the most even cluster load balancing. If low latency is the most important, then the LoadBalancer Service can be configured to force the kube-proxy to choose a Pod local to the node that received the client traffic. To do this, set the externalTrafficPolicy field to “Local” in the Service manifest. This choice eliminates the double-hop to another node, as the kube-proxy will always choose a Pod on the receiving node. When packets are forwarded from node to node, the source client IP address is preserved and directly visible to the destination Pod. Although this preserves the source IP address, it introduces a risk of creating an imbalance in cluster load. Alternatively, if getting the most even cluster load balancing is more important, then the container-native load balancing configuration might be a better choice. With this option, the powerful Google Cloud Application Load Balancer is still used, however, the load balancer will direct the traffic to the Pods directly instead of to the nodes. This method requires GKE clusters to operate in VPC-native mode, and it relies on a data model called network endpoint groups. Network endpoint groups represent IP-to-port pairs, which means that Pods can simply be just another endpoint within the group, equal in standing to compute instance VMs. Every connection is made directly between the load balancer and the intended Pod. For example, traffic intended for Pod 3 will be routed directly from the load balancer to the IP address of Pod 3 using a network endpoint group. So, what’s the best choice? The answer to that question depends on the application. Both configurations can be profiled and the one that provides the best overall application performance can be selected. However, the “Local” external-traffic policy may cause other issues as it imposes constraints on the mechanisms that balance Pod traffic internally. It may also cause issues externally as the Application Load Balancer forwards traffic via nodes with no awareness of the state of the Pods themselves. Note, there are many benefits to using container-native load balancing and Network Endpoint Groups. Let’s explore a few. One benefit is that Pods can be specified directly as endpoints for Google Cloud load balancers. This means that the traffic will be directed to the intended Pod, eliminating extra network hops. Another benefit is that load balancer features, such as traffic shaping and advanced algorithms, are supported. The load balancer can accurately distribute the traffic, since it has a direct connection to the Pod. The next benefit is that container-native load balancing allows direct visibility to the Pods and more accurate health checks. Since the source IP is preserved, the roundtrip time it takes traffic to travel from the client to the load balancer can be measured, which can be useful when troubleshooting issues. This visibility can be easily extended using Google Cloud Observability tools. And then there is the fact that there are fewer network hops in the path, which optimizes the data path. This improves latency and throughput, providing a better network performance overall. And finally, it’s worth noting that there is support for Google Cloud networking services, like Google Cloud Armor, Cloud CDN, and Identity-Aware Proxy.

### Video - [Network Policies](https://www.cloudskillsboost.google/course_templates/34/video/518718)

* [YouTube: Network Policies](https://www.youtube.com/watch?v=ZU5umxTWMrs)

By default, all Pods can communicate with one another. However, what if access to certain Pods must be restricted? The solution is to implement a network policy. A network policy is a set of firewall rules applied at the Pod level that restrict access to other Pods and Services inside the cluster. For example, in a multi-layered application, network policies can be used to restrict access at each stack level. A web layer could only be accessed from a certain Service, and an application layer beneath this could only be accessed from the web layer. This effectively promotes defense in depth. So, why should you consider using network policies? Imagine an attacker compromised one of your Pods by exploiting a security vulnerability in it. Just by gaining access, an attacker would have the power to probe outwards from the compromised Pod to all other Pods in your cluster. And they might even find other vulnerabilities. As a result, it would be wise to limit the attack surface of your cluster. While network policies can be complex to manage, GKE Dataplane V2 simplifies the process with built-in policy enforcement, eliminating extra overhead. Autopilot clusters in GKE come with Dataplane V2 enabled by default, simplifying network policy management. For standard clusters, you can manually enable Dataplane V2 and take advantage of its streamlined network policies, provided your cluster meets the minimum requirements. GKE Dataplane V2 leverages eBPF, a powerful technology that operates within the Linux kernel. As packets arrive at a GKE node, GKE Dataplane V2 decides how to route and process the packets. Crucially, Dataplane V2 taps into Kubernetes-specific metadata embedded in these packets. This enables efficient packet processing and provides detailed, annotated logs for comprehensive network visibility. After you enable network policy enforcement for your cluster, you need to define the actual network policy. Enabling network policy enforcement consumes additional resources in nodes. This means that you might need to increase your cluster’s size to keep your scheduled workloads running. Network Policies are written in YAML files and have the kind NetworkPolicy. The podSelector lets you select a set of Pods based on labels. If “podSelector” isn’t provided, or is empty, the networking policy will be applied to all Pods in the namespace. policyTypes indicates whether ingress, egress, or both traffic restrictions will be applied. If policyTypes is left empty, a default ingress policy will apply automatically, and an egress policy won’t be specified. In the Ingress section of the policy, there are two main sections: “from” and “ports.” The “from” section can be from three sources: ipBlock, namespaceSelector, or podSelector. The “ports” section states what ports ingress will be accepted from. The combination of these two elements defines where traffic is allowed to enter the network. In the Egress section of the policy, there are two main sections: “to” and “ports.” In the example, traffic destined for network 10.0.0.0/24 on TCP port 5978 will be permitted to egress from the demo-app Pods. Remember, applying network policies does nothing if you haven’t enabled network policy on your cluster. You can use the “gcloud container clusters update” command to disable a network policy. In the Google Cloud console, disabling a network policy is a two-step process. You first disable the network policy for nodes, and then you disable the network policy for the control plane. If no network policies exist, all ingress and egress traffic will be allowed between the Pods in the namespace regardless of whether network policy enforcement is enabled or disabled. There are default policies that can be invoked if no other rules match. The Default-deny policy for ingress, or Default-deny policy for Egress block all incoming or outgoing traffic respectively. An “allow-all” policy exists for both ingress and egress. This allows all traffic in that direction. Please note the difference in wording between this and the “default-deny” keyword. For the default policies, a policyType: Ingress or policyType: Egress is required depending on the direction of the traffic.

### Lab - [Configuring Google Kubernetes Engine (GKE) Networking](https://www.cloudskillsboost.google/course_templates/34/labs/518719)

Architecting with Google Kubernetes Engine: Configuring Kubernetes Engine Networking

* [ ] [Configuring Google Kubernetes Engine (GKE) Networking](../labs/Configuring-Google-Kubernetes-Engine-(GKE)-Networking.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/34/quizzes/518720)

#### Quiz 1.

> [!important]
> **You need to apply a network policy to five Pods to block ingress traffic from other Pods. Each Pod has a label of app:demo-app. In your network policy manifest, you have specified the label app:demo-app in spec.podSelector. The policy is configured and listed when you view Network Policies on your cluster. However, it's not having the intended effect. Pods can still be pinged from other Pods within the cluster. What is the cause of this and what action can you take to correct it?**
>
> * [ ] A network policy must be applied to all Pods in the cluster in order to block ingress traffic. In the network policy manifest, do not define any value for spec.podSelector.
> * [ ] You have not enabled network policies on the cluster. Enable network policies and recreate all of the nodes.
> * [ ] You need to create a matching Google Cloud firewall rule for the network policy. In the Cloud Console or Cloud Shell, create a firewall rule that matches the Network Policy.
> * [ ] Network policies are only applied when a Pod is started. Stop and restart all of the Pods in your application to activate the network policy.

#### Quiz 2.

> [!important]
> **You are designing a GKE solution. One of your requirements is that network traffic load balancing should be directed to Pods instead of balanced across nodes. How can you enable this for your environment?**
>
> * [ ] Set the externalTrafficPolicy field to local in the YAML manifest for your external services.
> * [ ] Configure or migrate your cluster to VPC-Native Mode and deploy a container-native load balancer.
> * [ ] Configure affinity and anti-affinity rules that ensure your application's Pods are distributed across nodes.
> * [ ] Configure all external access for your application using Ingress resources rather than services.

#### Quiz 3.

> [!important]
> **You have updated your application and deployed a new Pod. How can you ensure consistent network access to the Pod throughout its lifecycle?**
>
> * [ ] Add metadata annotations to the Pod manifest that define a persistent DNS name.
> * [ ] Register the fully qualified domain name of the application's Pod in DNS.
> * [ ] Deploy a Kubernetes Service with a selector that locates the application's Pods.
> * [ ] Add the fully qualified domain name of the application's Pod to your local hostfile.

#### Quiz 4.

> [!important]
> **Your Pod has been rescheduled, and the original IP address that was assigned to the Pod is no longer accessible. What is the reason for this?**
>
> * [ ] The new Pod IP address is blocked by a firewall.
> * [ ] The new Pod has received a different IP address.
> * [ ] The Pod IP range for the cluster is exhausted.
> * [ ] The old Pod IP address is blocked by a firewall.

#### Quiz 5.

> [!important]
> **During testing, you cannot find the Google Cloud load balancer that should have been configured for your application. You check the manifest and notice that the application's frontend service type is ClusterIP. How can you correct this?
**
>
> * [ ] Define spec.type as NodePort in the YAML manifest for the service and redeploy the application.
> * [ ] Define spec.type as LoadBalancer in the YAML manifest for the service and redeploy the application.
> * [ ] Make sure the cluster has been configured in VPC native mode as Alias IP is required for Google Cloud load balancers.
> * [ ] Manually configure the Google Cloud load balancer and configure it to direct traffic to the GKE Cluster Nodes.

## Persistent Data and Storage

In this section of the course, you'll unlock the secrets of Kubernetes storage. You'll learn how to define and work with Kubernetes storage abstractions, ensuring your applications have access to persistent data. You'll also discover how to manage sets of Pods efficiently using StatefulSets, decouple configuration from Pods using ConfigMaps, and securely manage sensitive access and authentication data. Finally, you'll practice configuring persistent storage for Google Kubernetes Engine, ensuring your data remains safe and accessible.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/34/video/518721)

* [YouTube: Introduction](https://www.youtube.com/watch?v=iXz2S5s2ziA)

As Kubernetes Pods are ephemeral by design, the way GKE handles data storage is a little different than the way Compute Engine instances do. As a result, it’s important to ensure the application’s data remains present even if a Pod is updated or replaced. For this to happen, StatefulSets can be used. StatefulSets are a Kubernetes controller that manages the lifecycle of Pods that require persistent storage. ConfigMaps are a Kubernetes API object that stores configuration data. They can be used during application deployment to decouple configuration artifacts from container definitions. To keep sensitive information from accidental exposure Kubernetes Secrets can be used. In this section of the course titled, “Persistent Data and Storage,” you’ll explore and practice working with Kubernetes storage abstractions; identify ephemeral and durable Volumes; see how to run and maintain sets of Pods by using StatefulSets; examine how ConfigMaps decouple configuration from Pods; and manage and store sensitive access and authentication data with Secrets.

### Video - [Volumes](https://www.cloudskillsboost.google/course_templates/34/video/518722)

* [YouTube: Volumes](https://www.youtube.com/watch?v=1VUJ5wgBW_k)

In GKE, a storage abstraction is a layer of software that allows you to interact with storage in a consistent way, regardless of who the underlying storage provider is. This abstraction layer is important because it helps simplify the process of provisioning and managing storage, it provides a consistent interface for accessing storage, and it lets you use different storage providers so you have more flexibility in choosing the right storage for your needs. The standard storage abstractions that Kubernetes provides are Volumes and PersistentVolumes. Let’s examine the difference between the two and how they can be used to store and share information between Pods. We’ll begin with Volumes. Kubernetes uses objects to represent the resources it manages, and this applies to both storage and Pods. A Volume is a directory accessible to all containers in a Pod. Some Volumes are ephemeral, which means they last only as long as the Pod to which they are attached. However, some Volumes are persistent, which means that they can outlive a Pod. Volumes are attached to Pods, not containers. This is important to understand because if a Pod isn't mapped to a node, then the Volume won’t be either. And then there are PersistentVolumes, or PVs, which are cluster resources that exist independently of Pods, and are used to manage durable storage in a cluster. In GKE, a PersistentVolume is typically–as the name states–backed by a persistent disk. This means that any disk or data represented by a PersistentVolume will continue to exist even as a cluster changes or if Pods are deleted and recreated. PersistentVolume resources can be provisioned dynamically through PersistentVolumeClaims, or they can be explicitly created by a cluster administrator.

### Video - [Ephemeral Volumes](https://www.cloudskillsboost.google/course_templates/34/video/518723)

* [YouTube: Ephemeral Volumes](https://www.youtube.com/watch?v=6Fyu3eO5ilA)

Kubernetes offers a variety of Volume types, both ephemeral and durable. Let’s start by looking at ephemeral Volumes, including emptyDir, DownwardAPI, ConfigMap, and Secret. emptyDir is the most basic type of ephemeral Volume. It creates an empty directory within the Pod's filesystem to read and write from, and will exist as long as that Pod is running on that node. emptyDir is commonly used for storing temporary files or data that doesn't need to persist beyond the Pod's lifetime. Examples might include scratch space, such as for a disk-based merge sort, checkpointing a long computation for recovery from crashes, or holding files that a content-manager container fetches while a web server container serves the data. When a Pod is removed from a node for any reason, the data in the emptyDir is permanently deleted. However, if a container crashes, that event will not cause a Pod to be removed from a node. This means the data in the emptyDir volume will remain safe. Next is the DownwardAPI Volume type, which can be used to make data from the Downward API available to applications. This data can include Pod labels, annotations, secrets, and node information, which makes it useful for configuring applications based on their deployment context. It’s a way for containers to learn about their Pod environment. Then there are ConfigMap Volumes. Similar to DownwardAPI, ConfigMap Volumes can be used to inject configuration data into the Pod's environment. However, ConfigMap data is more structured, is stored as key-value pairs, and can be shared across multiple Pods. The data stored in a ConfigMap object can be referenced in a volume, as if it were a tree of files and directories. Applications can then consume the data. Finally, there is the Secret Volume type, which is specifically designed for storing sensitive data, such as passwords, tokens, or API keys. Secrets are unencrypted, but Google encrypts the data at rest and ensures secure access within the Pod. Secret Volumes are backed by underlying data store (etcd), so the Secrets are never written to non-volatile storage. Secrets aren’t secret just because of the way they are configured. Differentiating between ConfigMaps and Secrets provide a way to manage non-sensitive and sensitive Pod configuration data differently. Some Volume types, like Secrets and ConfigMaps, are coupled to the life of the Pod and are deleted when the Pod ceases to exist. But it’s important to note that although the Secret and ConfigMap Volumes that attach to individual Pods are ephemeral, the objects are not. At a fundamental level, ConfigMap, Secret, and DownwardAPI allow for different kinds of Kubernetes data into a Pod.

### Video - [Durable Volumes](https://www.cloudskillsboost.google/course_templates/34/video/518724)

* [YouTube: Durable Volumes](https://www.youtube.com/watch?v=dHJiZrrm3_Y)

Along with ephemeral Volume types, Kubernetes offers a durable Volume type: PersistentVolume. The PersistentVolume abstraction has two components: PersistentVolume, or PV, and PersistentVolumeClaim, or PVC. Let’s define both, starting with PersistentVolume. PersistentVolume is a type of storage that can be used to store data that needs to be kept even if the Pod or node that the data is stored on is restarted or fails. This is in contrast to ephemeral storage, which is only available while the Pod or node is running. PersistentVolumes are typically used to store data that is important to the application, such as databases, file systems, and configuration files. That brings us to a PersistentVolumeClaim, or PVC, which represents a request for storage resources. It’s a way for users to request storage that can be used by Pods to store data that needs to persist across Pod restarts or node failures. A PVC specifies the desired characteristics of the storage, such as the size, access mode, and storage class. When a Pod requests a PVC, the Kubernetes controller matches the PVC to an available PersistentVolume that meets the requirements of the PVC. Once a PersistentVolume is matched to a PVC, the Pod can mount the PersistentVolume and access the storage. Application developers can claim and use provisioned storage using PersistentVolumeClaims without creating and maintaining storage volumes directly. This allows for the separation of roles, as it’s the job of administrators to make persistent volumes available, and the job of developers to use those volumes in applications. Now let’s shift focus and explore how to create a PersistentVolume manifest. The first task is to specify the StorageClassName, which is a resource used to implement PersistentVolumes. When you define a PVC in a Pod, the PVC uses the StorageClassName. This means that the PV StorageClassName must match the PVC StorageClassName if you want the claim to be successful. The next step is to decide if you want to use the Compute Engine Standard Persistent Disk type. If so, GKE has a default StorageClass name ‘standard’ that can be used. The persistent disk must be created first, or you will encounter an error. With GKE clusters, a PVC with no defined StorageClass will use this default StorageClass, and it will provide storage by using a standard Persistent Disk. From there, specify volume capacity, which is the storage capacity required for your PersistentVolume. However, if you want to use an SSD persistent disk, create a new StorageClass, and give it an appropriate name, such as “SSD”. In the parameters section, define the type as pd-ssd. Remember, A PVC that uses this new StorageClass named SSD will only use a PV that also has a StorageClass named SSD. And when a new storage class is created, it can be viewed in the Cloud Console. Now it’s important to note that Kubernetes StorageClass and Google Cloud Storage classes are not the same thing. Google Cloud Storage provides object storage for the web, whereas Kubernetes StorageClasses are choices for how PersistentVolumes are backed. Then there are AccessModes. AccessModes in PersistentVolumes define how Pods can mount and access the storage provided by the PV. They determine the level of access and the number of Pods that can simultaneously mount the PV. There are different types of AccessModes. With ReadWriteOnce, the PV can be mounted read-write by a single Node in the cluster. This means that any Pod running on that Node can access the PV for reading and writing. For most applications, persistent disks are mounted as ReadWriteOnce. With ReadOnlyMany, the PV can be mounted read-only by multiple nodes simultaneously. This means that Pods running on these nodes can only read data from the PV, not write to it. And with ReadWriteMany, the PV can be mounted read-write by multiple nodes simultaneously. Pods running on these nodes can both read and write data to the PV. However, Google Cloud persistent disks do not support ReadWriteMany. Persistent Volumes can also be created from a YAML manifest by using a kubectl apply command. For example, let’s say there is a PV named pd-volume that will have 100 GB of storage allocated and will allow ReadWriteOnce access. It will use a standard Persistent Disk based on the storageClassName, and it will be maintained by cluster administrators. Remember, PersistentVolumes can’t be added to Pod specifications directly. Instead, PersistentVolumeClaims must be used. In order for this PersistentVolumeClaim to claim the PersistentVolume, their storageClassNames and accessModes must match. Also, the amount of storage requested in a PersistentVolumeClaim must be within a PersistentVolume’s storage capacity. Otherwise, the claim will fail. Now let’s explore what happens when a PersistentVolumeClaim is added to a Pod. When this Pod is started, GKE will look for a matching PV with the same storageClassName, accessModes, and sufficient capacity. The specific cloud implementation doesn’t really matter, because the specific storage that is used to deliver this storage class is controlled by the cluster administrators, not the application developers. But what if application developers claim more storage than has already been allocated to PersistentVolumes? If there isn’t an existing PersistentVolume to satisfy the PersistentVolumeClaim, Kubernetes will try to provision a new one dynamically. By default, Kubernetes will try to dynamically provision a PersistentVolume if the PersistentVolumeClaim’s storageClassName is defined and an appropriate PV does not already exist. If a matching PersistentVolume already exists, Kubernetes will bind it to the claim. If storageClassName is omitted, the PersistentVolumeClaim will use the default StorageClass, which, in GKE, is named “standard.” Dynamic provisioning will only work in this case if it is enabled on the cluster. GKE manages all of this. The application owner does not have to provision the underlying storage, and does not need to embed the details of the underlying storage into the Pod manifest. Deleting the PersistentVolumeClaim will also delete the provisioned PersistentVolume. So, if you want to retain the PersistentVolume, set its persistentVolumeReclaimPolicy to Retain in the YAML file. PersistentVolumeClaims should be deleted when their underlying PersistentVolume is no longer required.

### Video - [StatefulSets](https://www.cloudskillsboost.google/course_templates/34/video/518725)

* [YouTube: StatefulSets](https://www.youtube.com/watch?v=_50FLyzpmZQ)

Although PersistentVolumes provide durable storage for a Pod, they can also be used for other controllers like Deployments and StatefulSets. A Deployment is a Pod template, typically used for stateless applications, that runs and maintains a set of identical Pods, commonly known as replicas. These replicas need to attach and reattach to PersistentVolumes dynamically, which might cause a deadlock. StatefulSets help resolve this problem. This means that whenever an application needs to maintain state in PersistentVolumes, it should be managed with a StatefulSet. Useful for stateful applications, StatefulSets run and maintain a set of Pods. A StatefulSet object defines a desired state, and its controller achieves it. This allows StatefulSets to maintain a persistent identity for each Pod. Each Pod in a StatefulSet has an ordinal index with a relevant Pod name, a stable hostname, and stably identified persistent storage that is linked to the ordinal index. An ordinal index is a unique sequential number that is assigned to each Pod in the StatefulSet. This number defines the Pod’s position in the set’s sequence of Pods. Deployment, scaling, and updates are ordered by using the ordinal index of the Pods within a StatefulSet. For example, if a StatefulSet named ‘demo’ launches 3 replicas, it will launch Pods named demo-0, demo-1, and demo-2 sequentially. This means that all of its predecessors must be running and ready before an action is taken on a newer Pod. For example, if demo-0 is not running and ready, demo-1 will not be launched. If demo-0 fails after demo-1 is Running and Ready, but before the creation of demo-2, demo-2 will not be launched until demo-0 is relaunched and becomes Running and Ready. Scaling and rolling updates happen in reverse order, which means demo-2 would be changed before demo-1. To launch Pods in parallel without waiting for the Pods to maintain Running and Ready state, ensure that the PodManagementPolicy is Parallel. As mentioned, StatefulSets are useful for stateful applications. With stable storage, StatefulSets use a unique PersistentVolumeClaim for each Pod. In order for each Pod to maintain its own individual state, it must have reliable long-term storage to which no other Pod writes. These PersistentVolumeClaims use ReadWriteOnce access mode for applications. When it comes to controlling networking, StatefulSets require a service. For example, if load-balancing and a single service IP are not needed, a headless service can be created by specifying “None” for the cluster IP in the Service definition. To ensure a specific service gets used for a StatefulSet, add that service to the serviceName field. A label selector is required for the Service, and this must match the template’s labels defined in the template section of the StatefulSet definition. The container details must also be defined, including the image, containerPort for the Service, and Volume mounts. And most importantly, VolumeClaimTemplates must be specified under the template section. The VolumeClaimTemplate must be named, and the spec needs to be the same as the PersistentVolumeClaim that is required by the Pods in this StatefulSet.

### Video - [ConfigMaps](https://www.cloudskillsboost.google/course_templates/34/video/518726)

* [YouTube: ConfigMaps](https://www.youtube.com/watch?v=yCSf0h3YAto)

A ConfigMap in Kubernetes is an API object that stores configuration data as key-value pairs. It provides a mechanism to decouple application configuration from Pods, which means a Pod’s specifications are stored and maintained in one place and act as a single source of truth. This prevents configuration drift. ConfigMap can be used to store configuration files, command-line arguments, environment variables, port numbers, and other configuration artifacts and make them available inside containers. This makes applications more portable and manageable without requiring them to be Kubernetes-aware. Let’s explore an example where a ConfigMap named demo is created using literal values. You can add multiple key-value pairs, such as lab.difficulty=easy, and lab.resolution=high, and even view details of configMaps using kubectl or the Google Cloud console. Another way to create a ConfigMap is by using the from-file syntax. These files contain multiple key-values. Multiple files can be added to a ConfigMap, and it’s recommended that you check these files into a source code control system to maintain their versioning and history. Names can be specified for keys. As an alternative to using the source filenames, key names can be added. This syntax is very similar to the from-file syntax, but here an additional key value is inserted to rename the key used. Let’s say we have an example where the key value Color is added for the file called color.properties, and then the key value Graphics is added for the file called ui.properties. The contents of ConfigMaps can be verified using kubectl or the Google Cloud console. ConfigMaps can also be created from a manifest. The data is the same as in the previous examples. The kubectl apply command can be added to the manifest file and it will create the ConfigMap. Pods refer to ConfigMaps in three ways: as a container environment variable, in Pod commands, or by creating a Volume. For example, let’s say we have a single ConfigMap that is used in the Pod as a container environment variable. Within an env field in the YAML file, a container environment variable can be named as VARIABLE_DEMO. The values are retrieved using configMapKeyRef. Multiple variables can be added from the same or different ConfigMaps. After the container environmental variables are defined, they can be used inside Pod manifest commands. A dollar sign $ and an opening parenthesis ( is put in front of the environment variable’s name, and a closing parenthesis ) after it. This allows configuration artifacts to be decoupled from image content to keep containerized applications portable. As a result, the kubelet has no way to reach into the Pod and modify these values later. ConfigMap data can also be added into an ephemeral Volume. For example, let’s say a Volume named config-volume is created in the Volumes section, with a ConfigMap named demo. The result is that a ConfigMap Volume is created for this Pod. This means that all the data from the ConfigMap is stored in this ConfigMap Volume as files, and then this Volume is mounted to the container by using the mountPath directory. When a ConfigMap Volume is already mounted and the source ConfigMap is changed, the projected keys are eventually updated.

### Video - [Secrets](https://www.cloudskillsboost.google/course_templates/34/video/518727)

* [YouTube: Secrets](https://www.youtube.com/watch?v=UENBKMeXwHE)

Similar to ConfigMaps, Secrets pass information to Pods. However, the difference is that Kubernetes applications use Secrets to store sensitive information like passwords, tokens, and SSH keys. This lets users manage sensitive information in their own control plane. Secrets also help to ensure Kubernetes doesn't accidentally output this data to logs. That being said, if an application is managing high-value assets or requires stringent regulatory requirements, key management systems like Cloud Key Management Service, also referred to as Cloud KMS, should be used for full secret management. There are three types of Secrets: generic, TLS, and Docker-Registry. Generic Secrets are used for creating Secrets from files, directories, or literal values. TLS Secrets are designed to securely store transport layer security (TLS) certificates and their associated private keys. These are the digital credentials used to establish encrypted communication between different components in a cluster. And Docker-Registry Secrets are designed to store the credentials needed to authenticate with private Docker registries. However, in GKE, Artifact Registry integrates with Cloud Identity and Access Management, so this Secret type is not needed. Let’s explore the generic Secret type in some more detail. Just like ConfigMaps, generic Secrets are stored in key-value pairs. However, in Secrets, values are base-64–encoded strings. Base-64 encoding is a way of representing binary data–like images, audio, or code–using text characters. Now it’s important to note that base-64 encoding is not a form of encryption. If encryption is required, please use an encryption key management system like Cloud KMS. Any encoded strings produced can then be used in the Secret manifest. But how exactly do you create a Secret? The answer is by using the kubectl create secret command. One option is to create a Secret using files, which means populating the Secret object with data sourced directly from files on your local system. A second option is to create a Secret using literal values, which means defining the secret data directly within the Secret manifest file itself. And a third option is to create a Secret using naming keys, which means using the default key name as the file name. For all three options, the syntax is similar. The differences come from the Secrets entry details. There are separate control planes for Configmaps and Secrets. Control planes provide a mechanism to create a secure storage that can be used to store and protect Secrets. Let’s say a Secret Volume named storagesecrets is created and refers to a Secret named demo-secret. This Volume is mounted to the container with read-only access. This Volume can be used by multiple containers within the Pod. Which allows for better latency, because Pods do not need to wait to gain access to the secret. A password key for the Secret will not be displayed. If a password key is required, it must be listed under the items field. Just like ConfigMaps, kubelet periodically syncs with Secrets to keep a Secret Volume updated. If a Secret that is already attached as a Volume has changed, the keys and values will eventually be updated.

### Lab - [Configuring Persistent Storage for Google Kubernetes Engine](https://www.cloudskillsboost.google/course_templates/34/labs/518728)

Architecting with Google Kubernetes Engine: Configuring Persistent Storage for Kubernetes Engine

* [ ] [Configuring Persistent Storage for Google Kubernetes Engine](../labs/Configuring-Persistent-Storage-for-Google-Kubernetes-Engine.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/34/quizzes/518729)

#### Quiz 1.

> [!important]
> **A StatefulSet consists of four Pods that are named Demo-0, Demo-1, Demo-2 and Demo-3. The StatefulSet originally had only two replica Pods but this number was recently increased to four. An update is being rolled out to The StatefulSet is currently being updated with a rolling strategy. Which Pod in the StatefulSet will be updated last?**
>
> * [ ] Demo-1
> * [ ] Demo-3
> * [ ] Demo-0
> * [ ] Demo-2

#### Quiz 2.

> [!important]
> **You need to store image registry credentials to allow Pods to pull images from a private repository. What type of Kubernetes Secret should you create?**
>
> * [ ] A TLS Secret.
> * [ ] A Docker-Registry Secret.
> * [ ] A generic Secret.
> * [ ] A JSON credential file.

#### Quiz 3.

> [!important]
> **You have created a ConfigMap and want to make the data available to your application. Where should you configure the directory path parameters in the Pod manifest to allow your application to access the data as files?**
>
> * [ ] spec.containers.name
> * [ ] spec.containers.volumes
> * [ ] spec.containers.volumeMounts
> * [ ] spec.containers.env

#### Quiz 4.

> [!important]
> **A GKE application might need persistent storage. The app owner creates a PersistentVolumeClaim (PVC) with a StorageClassName labeled "standard." What type of storage will likely be used for the volume?**
>
> * [ ] Google Persistent Disk.
> * [ ] Local volume on the node.
> * [ ] NFS Storage.
> * [ ] Memory Backed.

## Workloads Course Summary

The course closes with a summary of the key points covered in each section.

### Document - [Workloads Course summary](https://www.cloudskillsboost.google/course_templates/34/documents/518730)

## Course Resources

Student PDF links to all modules

### Document - [Course Resources](https://www.cloudskillsboost.google/course_templates/34/documents/518731)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
