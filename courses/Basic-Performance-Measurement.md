---
id: 1121
name: 'Basic Performance Measurement'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1121
date_published: 2024-07-16
topics:

---

# [Basic Performance Measurement](https://www.cloudskillsboost.google/course_templates/1121)

**Description:**

This course explores the fundamentals of the feedback loop process for Virtual Agent development and introduces the native capabilities within Dialogflow CX that support it

**Objectives:**

* Learn the feedback loop process
* Understand a typical customer journey
* Learn how to evaluate the metrics that need measurement along the customer journey
* Identify the root cause to a specific problem and rank its optimization opportunity against other issues identified in live traffic.

## Feedback loop

This module introduces the main phases of the feedback loop process and the optimization opportunities it unlocks in Virtual Agent development

### Video - [What is the feedback loop?](https://www.cloudskillsboost.google/course_templates/1121/video/494984)

* [YouTube: What is the feedback loop?](https://www.youtube.com/watch?v=UZtIqZfln_k)



### Video - [What type of optimizations are we looking for?](https://www.cloudskillsboost.google/course_templates/1121/video/494985)

* [YouTube: What type of optimizations are we looking for?](https://www.youtube.com/watch?v=0sO-CUnPsTg)

Next, we’ll discuss what type of optimizations we’re looking for. Optimizations can come in many different forms. Natural language understanding is the ability of the virtual assistant or chatbot to comprehend the user's input. This involves tasks such as: Identifying the user's intent, extracting relevant information from the input, and generating a response that is both informative and engaging. Conversational flow refers to the way the virtual assistant or chatbot interacts with the user. This includes aspects such as the structure of the conversation, the use of natural language, and the overall tone of the interaction. How the agent responds to the user as different words or directions can improve the likelihood of how a user will respond. Changes on a customer journey by making more data or automated processes available to the agent for the user to participate in Fallback handling refers to the way the virtual assistant or chatbot responds when it is unable to understand the user's input or when the user requests something that is beyond its capabilities. This can involve providing the user with alternative options, suggesting ways to rephrase the request, or escalating the issue to a human agent. By optimizing these various aspects of performance, conversational agents can be developed to better meet user needs and provide a more satisfying experience. Optimizations can fall into two major buckets: incremental opportunities and ten-ex opportunities. Incremental opportunities are small improvements that can be made to the current experience. These might involve fixing minor defects or making small optimizations. Ten-ex opportunities, on the other hand, are more ambitious goals that could lead to a significant improvement in the performance of the conversational agent. These might involve reimagining the design of the agent or the way it interacts with users.

### Quiz - [Feedback Loop Quiz](https://www.cloudskillsboost.google/course_templates/1121/quizzes/494986)

## Understand phase

This module explores the Understand Phase of the Feedback loop process, by reviewing the typical end-to-end experience of a customer interacting with a conversational agent, the tools available in Dialogflow CX to review it and the best practices to analyse it for continuous improvement

### Video - [What does a typical customer journey look like in a bot?](https://www.cloudskillsboost.google/course_templates/1121/video/494987)

* [YouTube: What does a typical customer journey look like in a bot?](https://www.youtube.com/watch?v=LX4ZAfY_3YM)

We’ll now explore each phase of the Feedback Loop process starting with the Understand phase, which is focused on grounding yourself in the type of data you have available. At the end of this section you should be able to answer the following questions: What does a typical customer journey look like in a bot? What tools can we use? How do we review conversations? And what are the limitations of this step? FIrst, let’s answer the question of “what does a typical customer journey look like in a bot?” In this section, we are exploring the end-to-end experience of a customer interacting with conversational agents, a crucial part of the Feedback Loop process. This process is instrumental in reviewing and enhancing the performance of conversational agents. We start with the Customer Pain Point. This is where a specific problem faced by the customer, related to their account, is identified. Understanding this pain point is key to improving the agent's ability to provide relevant assistance. Moving on to Self-service Online Tools, these are the first line of defense. The customer attempts to resolve their issue through resources available on the search or account page. The effectiveness of these tools can significantly reduce the need for live assistance. When the issue is not resolved online, the customer progresses to Dialing Customer Service. This step is about the customer reaching out for direct support, initiating the contact that could lead to either resolution or further escalation. Steering is next, where we determine the customer's intent for the call. Proper steering ensures that the customer is routed to the right resource, improving efficiency and customer satisfaction. The Self-Service Experience during the call is another touchpoint that can either resolve the issue or indicate the need for escalation. This stage reflects how well the automated system understands and responds to customer queries. Finally, we may reach Escalation or a Hangup. If self-service options are exhausted, the customer's issue is escalated to a human agent. The ease of this transition and the subsequent handling of the call are critical metrics for performance measurement. This journey can also end with a resolution or, in some cases, the customer may choose to Hang up. Each step of this journey provides valuable data points that can be analyzed to enhance the conversational agent's performance. Remember, the goal is to create a seamless and efficient experience for the customer, reducing the need for escalation and improving overall satisfaction.

### Video - [What tools can we use?](https://www.cloudskillsboost.google/course_templates/1121/video/494988)

* [YouTube: What tools can we use?](https://www.youtube.com/watch?v=q8GH3r_0QCo)

Next, let’s answer the question of “What tools can we use to understand the customer journey?” As we continue refining our conversational AI agents, it’s important to be able to track their performance and identify areas for improvement. This is where the conversation history tool comes in. This valuable resource allows you to browse and analyze actual production conversations between your agents and end users. By examining these conversations, you can gain insights into how your agents are performing, identify areas where they may be struggling, and ultimately determine ways to improve their responses. Here in the Conversation History Overview, you’ll have access to essential details regarding your previous agent interactions. You can track down information about the date and time of the conversation, how long it lasted, on which channel the conversation took place, and even the language used. At the top, you can use filters for searching entire conversations for time they took place, if they contain specific user utterances, or even intents triggered. There are a variety of filters allowing you to target a subset of conversations. This will be useful for finding a segment of calls to review. In the conversations that return, you can see the following information: “The Unique identifier” can be used to to locate and retrieve specific conversations for much closer examination. “How long the conversation” was is the time the conversation started through to the last user utterance. Turns is how many changes the user and agent went back and forth. The higher the number, the longer the conversation. Channel indicates how the user and agent interacted. Language is the language the agent was initially set to when the conversation started. And finally, Start time is what time the conversation started. Within a singular conversation within conversation history, you can see the following information: Agent Response: This is what the agent said to the user. User Utterance: This is what the user said to the agent. Intent Detected: This is the intent that the agent detected in the user's utterance, in this example pay my iPhone bill triggered the intent named “payment make payment” Entity Detected: This is the entity that the agent detected in the user's utterance, in this example, iPhone - which is a provided device - was detected Page the User is Responding To: This is the page that the user was on when they interacted with the agent. Flow the User is in: This is the flow that the user was in when they interacted with the agent. And finally, there’s Escalation Indicator: This indicates whether the conversation was escalated to a human agent. By reviewing this information, you can gain valuable insights into how users are interacting with your agent. Conversations are a rich source of data that can be used to understand user behavior and improve conversational AI systems. Conversation history tools offer valuable insights for improving conversational AI systems. Developers can analyze user utterances, intent recognition, and agent responses. Through this analysis, they can: understand how users interact with the system, identify common user intents and potential confusion points, and evaluate the effectiveness of the agent's responses. This information can be used to refine intents, improve response quality, and ultimately enhance the overall user experience. In terms of looking for different areas of improvement, for example, you can start to understand: Common misunderstandings - examples could be misunderstood words or phrases in how they were transcribed, not recognizing the correct intent, or even not understanding the entities mentioned by the customer, like their specific phone model. You can also explore areas where the agent is providing unhelpful responses - examples could be if users are frequently dissatisfied with the agent's responses or the question the agent is asking is not clear to the customer And you can also identify topics that the agent is not able to handle - for example, a common user utterance like wanting to escalate to an agent is not being recognized on a specific page By analyzing conversation history data, you can identify areas for improvement and make your conversational agent more helpful and effective for your users. Extracted information can be used to personalize conversations, identify customer pain points, and improve agent performance. Conversation history analysis provides valuable insights for enhancing user experience, product development, and marketing strategies. By analyzing these conversations, businesses can: build detailed user personas, refine chatbots for more natural interactions, and address customer pain points to improve overall satisfaction. This data can ultimately lead to improved customer relationships and more effective business strategies. Ethical considerations when extracting information from conversations include data privacy, transparency, and user control. Ethical handling of conversation history requires robust data privacy and security measures. Users deserve transparency regarding data collection and usage, with clear control over their information. This includes the ability to opt out of data collection altogether, empowering users to manage their digital footprint.

### Video - [How do we review conversations?](https://www.cloudskillsboost.google/course_templates/1121/video/494989)

* [YouTube: How do we review conversations?](https://www.youtube.com/watch?v=fKlZ2Z_dd1U)

Next, let’s review how we review conversations. There are two main ways to review conversation history: manually and targeted. Manual review involves human agents listening to or reading through recorded conversations and taking notes. This can be a time-consuming process, but it can also provide valuable qualitative insights that might be missed by automated methods. Targeted review involves using more filters to review a set of conversations that are more related to one another, like having a common user utterance or a detected intent. The best approach for reviewing conversation history will depend on the size and complexity of your conversational agent, as well as your available resources. But regardless of the method you choose, it's important to make conversation history review a regular part of your feedback loop so that you can continuously improve your agent's performance. Before you start diving into your transcripts, it's important to set a specific goal for your review process. This will help you stay focused and ensure that you're collecting the feedback you need to improve your conversational agent. Why set a goal? Setting a goal helps you get the most out of your transcript review and ensures that the outcome aligns with your team's needs. Without a goal, you might end up just collecting generic feedback that doesn't address your agent's specific weaknesses. Common goals for reviewing transcripts include: Effectiveness of intent gathering: Are user’s utterances getting tagged to the correct intent? Bot's understanding of entities: Can your agent correctly identify and interpret the key elements of a user's request, such as names, dates, and locations? Appropriateness of agent responses: Is your agent responding to users in a way that is natural, helpful, and consistent with your brand voice? Once you've set a goal, you can tailor your transcript review process to collect the specific feedback you need. For example, if your goal is to improve your agent's intent gathering, you might pay close attention to instances where users have difficulty expressing their requests or where the agent misinterprets their intent. By setting a clear goal and focusing your review process, you can get the most out of your feedback and make continuous improvements to your conversational agent. Once you have a goal in mind, you can create a template for gathering information from the transcripts. This template should include fields for things like the user's query, the agent's response, and whether the conversation was successful. Different reviewers will evaluate responses differently. A template helps to set guidelines for how to rate and a consistent format for easier aggregation. Create a Google Sheet for gathering the information needed from the transcripts. Ensure proper definitions are provided to ensure the gathering process is consistent from many different reviewers. After creating a template, you can pull a random sample of conversations to review. It's important to review a random sample to avoid bias. If you only review the conversations that were reported as being problematic, you'll get a skewed view of the agent's performance. You can do this by setting filters on the conversation history for the subset of conversations needed for the goal and sort by ID (note that this is randomly generated). You must also make sure that you continue to review conversations. The more time you invest in establishing a clear goal and a robust template, the more you can ensure a more usable outcome. Remember to check-in on reviewers to see if any issues occurred that were not anticipated when setting up the template. When reviewing the conversations, pay attention to things like: How well the agent understands the user's query. Is the agent able to correctly identify the user's intent? The quality of the agent's responses. Are the responses clear, concise, and helpful? Whether the agent is able to complete tasks. Is the agent able to help the user achieve their goal? As you review the conversations, document any high-level observations you make. You can do this by aggregating responses from all reviewers and creating a common disposition of what the behavior was. These observations can be used to identify trends and patterns that can help you improve the agent. Catalog the behavior for prioritization and discussion with the team. Once you've reviewed a sample of conversations, you can aggregate the results and share them with your team. This will help everyone on the team understand how the agent is performing and identify areas where it can be improved. What are the limitations of this step? Manually reviewing transcripts allows for the most in-depth analysis of individual conversations. You can identify specific strengths and weaknesses, understand user intent, and diagnose the root cause of problems. However, this approach is also time-consuming and not scalable. As the number of conversations grows, it becomes increasingly difficult to manually review them all. Automated trend detection can help you overcome these limitations. By analyzing large datasets of conversations, you can identify patterns and trends that might be difficult to spot through manual review. This can help you prioritize your efforts and focus on the most pressing issues. However, trend detection may not provide the same level of detail as manual review, and it may miss some important issues. The best approach will be a blend of your specific needs and resources. If you have a limited number of conversations, manual review may be the best option. But if you have a large number of conversations, or if you need to be able to identify trends quickly, automated trend detection may be a better choice.

### Quiz - [Understand Phase Quiz](https://www.cloudskillsboost.google/course_templates/1121/quizzes/494990)

## Analyze phase

This module explores the Analysis Phase of the Feedback loop process, by reviewing the key metrics worth capturing along the customer journey, and how they can support the identification of patterns and trends for Virtual Agent performance optimization 

### Video - [What metrics can be captured along the customer journey?](https://www.cloudskillsboost.google/course_templates/1121/video/494991)

* [YouTube: What metrics can be captured along the customer journey?](https://www.youtube.com/watch?v=Co-hOFQ9ZB4)

The next phase is the Analyze phase, which is focused on looking for patterns and trends in the data available from Dialogflow. At the end of this section you should be able to answer the following questions: What metrics can be captured along the customer journey? What tools can we use? How do we analyze large traffic? What are the limitations of this step? Next, let’s take a few moments to explore the metrics that can be captured along the customer journey. If we revisit the customer journey, presented earlier in the presentation, we can start to overlay different methods of measuring each step. For our presentation, we are more focused on DFCX interactions which generally start with Dialing customer service. The key metric to look at here would be incoming volume. Volume can vary by day of the week and even hourly given the behaviors of the customer base. Within the steering position of the conversation, we may be more interested in looking at No Match or Not Input rates on specific pages to determine if we’re obtaining the customer’s initial intent. As we dive deeper into performing self-service, like making a payment, more metrics may come into play. This includes how the business may want to measure success or failure. For example, in the make payment experience, we would want to track how often payments are successfully processed by the webhook to ensure customers are able to submit their payments. Additionally, because organizations are typically larger, different teams monitor different things. A team responsible for monitoring may care about webhook uptime whereas a V-P might care more about the escalation rate. It’s important to remember this as each stakeholder, while caring about different metrics, works towards the goals that support their respective business cases and ensure a consistent customer experience. Classifying metrics can be helpful for a few reasons: First, it can help us to better understand the different aspects of our conversational agents that we need to measure. Second, it can help us to identify the metrics that are most important to us, given our specific goals. Finally, it can help us to track our progress over time and identify areas where we need to improve. Three classifications for metrics can be interaction experience, agent design, and backend readiness. Interaction experience metrics measure how well users are able to interact with the conversational agent Agent design metrics measure the quality of the conversational agent itself. This includes metrics such as NO MATCH and NO INPUT rates. Backend readiness metrics measure the ability of the conversational agent's backend systems to support the agent's operation. This includes metrics such as webhook failures and latency. Interaction experience metrics help you understand how well users are interacting with your conversational agent. These metrics are linked to strategic objectives in the business, which means they can help you track how well your agent is meeting your business goals. Examples include volume by channel, intent, handle time, number of turns, escalation rate, and abandonment rate. It’s important to observe these over time as well, to account for seasonality. Agent design metrics measure the bot's performance, assess its ability to function optimally, and indicate where improvements can be made. Examples include NO MATCH rate and traffic volume of specific pages. These metrics are likely to change over time when new features are released, so these can be helpful to understand. Backend readiness metrics relate to the performance of supporting systems that allow an agent to create a positive experience. These are normally tied to Service-level agreements (or SLAs) and more advanced monitoring systems. Examples can include webhook failures and latency.

### Video - [What tools can we use?](https://www.cloudskillsboost.google/course_templates/1121/video/494992)

* [YouTube: What tools can we use?](https://www.youtube.com/watch?v=HGB9oZWDqw8)

What tools can we use to observe these metrics? First we’ll discuss tracking of conversation outcomes like succeeded and escalated. Dialogflow CX provides valuable insights into: user behavior, intent escalation, troubleshooting, and webhook errors, This enables you to improve your conversational AI system. You’ll also be able to see common places of confusion and errors which are an easy way to increase satisfaction from customers. The tools that we currently have are: Overall numbers, Intent escalation, Troubleshooting, And webhook errors. It is common to slice and dice these metrics by different dimensions to see if there are any detectable patterns. One of the common ways to drill into data is tracking conversation outcomes like escalation by their intents. Conversational AI struggles with understanding user intent can lead to costly escalations. You can mitigate these issues and boost customer satisfaction by: Refining intent definitions, improving NLP, offering clearer prompts, leveraging context, and confirming intent. This table provides a snapshot of the intents that have resulted in the most human escalations. It helps you identify the intents where the system is struggling to accurately understand user intent. By prioritizing these intents for improvement, you can effectively reduce the overall number of escalations. The time series chart visualizes the escalation trend for a specific intent over time. This allows you to track changes in the escalation rate and identify potential contributing factors. By analyzing the trend, you can determine whether the escalation rate is increasing, decreasing, or remaining stable. If you have labeled certain intents as head intents, you can filter the view to display only those intents. This is useful for focusing on the most critical intents that drive your conversational AI application. By prioritizing head intents, you can ensure that the system is performing well in the areas that matter most to your users. Overall, this view provides valuable insights into intent escalation patterns. This enables you to identify areas for improvement and enhance the effectiveness of your conversational AI system. By addressing the intents with the highest escalations, you can proactively reduce user frustration and improve overall customer satisfaction. Sessions is the total number of sessions where the intent was detected. This metric indicates the overall usage of the intent across all conversations. A high number of sessions suggests that the intent is frequently used. A a low number may indicate that the intent is not well-defined or relevant to user queries. Escalation rate is the percentage of sessions where the intent detection led to a request for human escalation. This metric measures the effectiveness of the system in handling user queries related to this intent. A high escalation rate indicates that the system is struggling to handle the intent accurately. This leads to user frustration and the need for human intervention. Turns is the number of conversational turns where the intent was detected. Each conversation may have multiple matches for one intent, reflecting the intent being detected at different points in the conversation flow. A high number of turns suggests that the intent is relevant to multiple conversation topics or that users are repeatedly expressing their intent in different ways. Head Intent Escalated is the percentage of escalation requests for sessions in which the intent was the final head intent. A head intent is the intent that ultimately drives the conversation flow. This metric measures the impact of the intent on overall conversation outcomes. A high percentage of head intent escalations indicates that the intent is critical for resolving user queries and that errors in handling this intent can significantly impact user satisfaction. By analyzing these metrics together, you can gain a comprehensive understanding of the intent's performance and identify areas for improvement. Take the example of a high escalation rate for a frequently used intent (or high sessions). This suggests a priority for refining the intent definition, improving natural language processing (or NLP) capabilities, or providing clearer prompts. Conversely, you might encounter a high head intent escalation rate for an intent with a low overall usage. This indicates a need to reevaluate the intent's relevance or revise the conversation flow to reduce the reliance on that intent. Effectively managing intent escalations is crucial for enhancing the user experience and overall performance of a conversational AI system. By analyzing these metrics and implementing targeted improvements, you can: reduce user frustration, improve customer satisfaction, and optimize the system's ability to handle user queries effectively.

### Video - [Troubleshooting](https://www.cloudskillsboost.google/course_templates/1121/video/494993)

* [YouTube: Troubleshooting](https://www.youtube.com/watch?v=gMeV01i92P4)

Another common way to analyze problems is looking for different troubleshooting areas that are typically related to the agent design. These can include what intents are in scope on a page, no matches, or missing responses. The troubleshooting section in conversational AI tools provides various views to assist in debugging common issues with conversational flows and pages. This section serves as a valuable resource for identifying and resolving problems promptly This ensures the seamless operation of your conversational interfaces. The Intent Name column displays the name of the intent that would have matched if it were included in the scope of the current context. This information helps identify potential gaps in the conversational flow and areas where intents need to be added or refined. The Turns column indicates the number of conversational turns where the matching intent should have been triggered. This metric provides insights into the frequency at which this issue occurs and helps prioritize troubleshooting efforts. By analyzing the data in this view, you can identify potential causes for missing intent matches and implement corrective actions. Several factors can hinder intent matching, including: incomplete routing configurations, overly restrictive intent definitions, limitations in natural language processing, and insufficient use of contextual information. The No Match Rate metric represents the percentage of total conversation turns that resulted in no intent matches for the page. A high no-match rate indicates that the system is frequently failing to identify the user's intent when they interact with this page. This can lead to user frustration and confusion, as the system may provide irrelevant or unhelpful responses. The No Match Turns metric represents the total number of conversation turns that resulted in no intent matches for the page. It provides a quantitative measure of the frequency of no-match occurrences, allowing you to prioritize troubleshooting efforts. By analyzing these metrics, you can identify potential issues with the page's intent configuration, intent definitions, or NLP capabilities. Several factors can contribute to a page's high no-match rate, including: Missing or overly restrictive intent definitions, Limited NLP capabilities, And insufficient use of contextual cues. Addressing these issues can enhance intent matching accuracy, leading to a more seamless and effective conversational AI experience. The Empty Responses metric represents the number of interactions on the page where the system failed to provide any agent response. This can occur due to various reasons, such as missing or incomplete flow definitions, errors in routing configuration, or limitations in the system's capabilities. A high number of empty responses indicates that the system is not handling user interactions effectively on this page. This can lead to user frustration and impede their ability to complete their tasks or receive the desired information. By analyzing this metric, you can identify areas in the conversational flow where agent responses are missing. You can then take corrective actions to ensure that users receive appropriate and helpful responses. A page with many empty responses in a conversational AI system could be due to missing flow definitions, routing issues, or limitations in the system's capabilities. Addressing these problems can improve the user experience by making the AI's responses more complete and effective. The Page Loop Sessions metric represents the number of sessions where the system produced the exact same agent response at least three times on the same page. This indicates that the system is likely stuck in a loop. It’s repeating the same response without progressing the conversation or providing the user with the desired information. A high number of page loop sessions suggests that there may be issues with the flow logic or the intent configuration associated with the page. This can lead to user frustration and dissatisfaction. They may be unable to break out of the loop and receive meaningful assistance. By analyzing this metric, you can identify potential loop-causing patterns in the conversation flow. You can then take corrective actions to ensure that the system progresses appropriately. Errors in the page's flow logic, conflicting intent definitions, or missing exit paths can all cause a high number of page loop sessions. Addressing these issues can prevent the system from getting stuck in loops and ensure a smooth user experience.

### Video - [Webhook errors](https://www.cloudskillsboost.google/course_templates/1121/video/494994)

* [YouTube: Webhook errors](https://www.youtube.com/watch?v=j22HCfO_z0k)

We can look into issues in retrieving external data via webhooks. Webhook errors occur when a conversational AI system fails to communicate with external systems through webhooks. These errors can disrupt data exchange, hinder workflow automation, and lead to user frustration. Common causes include: URL (or Uniform Resource Locator) misconfiguration, network issues, payload mismatches, and authorization problems. Server Errors from backend / integration layers Webhook timeouts To address webhook errors, ensure that you have: accurate URLs, stable network connections, valid payload structures, and correct authorization. You should also: Monitor external systems, analyze error logs, enable retries, and consider fallback URLs for additional resilience. Let‚Äôs review some of the available metrics. The name or tag of the webhook identifies the specific webhook being analyzed. Calls represents the total number of times the webhook has been invoked. Failure rate indicates the percentage of webhook invocations that failed, excluding timeouts. Timeout rate measures the proportion of all webhook invocations that exceeded the specified timeout limit. Average latency (measures in milliseconds) calculates the average time it takes for a successful webhook invocation to complete, excluding timeouts and errors.

### Video - [Flow analysis](https://www.cloudskillsboost.google/course_templates/1121/video/494995)

* [YouTube: Flow analysis](https://www.youtube.com/watch?v=fp1akgeANes)

Lastly, let’s look into common user paths. It offers insights into agent performance, traffic patterns, and potential issues. This enables developers to optimize agent design and enhance user experiences. Flow analysis, tables, and graphs provide valuable insights into conversation performance. You can use them to visualize conversation patterns, metrics, and trends. Correlating these insights with design documents helps validate conversational flow, assess intent recognition, and evaluate outcomes. You can use flow analysis to view conversations that are going off path. You can also read the transcripts to understand the context about the conversation and understand why customers are on that path. Next, let’s find out how to analyze large traffic. To gain valuable insights from performance data you need to delve deeper than just the numbers. Craft targeted questions based on key metrics like traffic patterns, escalation rates, and live agent handoffs. This approach sheds light on trends and potential issues. By tracking traffic patterns, you can identify peak usage times and resource allocation needs. Monitoring escalation rates reveals areas requiring improvement in self-service options or agent training. Analyzing live agent handoffs exposes bottlenecks in the support flow. By asking the right questions and actively investigating these metrics, you unlock valuable performance insights for targeted improvement. This analysis aims to improve virtual assistant performance by examining: no-match intents, webhook performance, incorrect intents, and end-user escalations. By understanding how users interact with the assistant and where it struggles, the team can identify areas for improvement and enhance the overall experience. This could involve refining the intent detection system, optimizing webhook interactions, or providing better support for complex requests. Ultimately, the goal is to create a more natural and effective virtual assistant that can better meet user needs. By analyzing performance data, you can gain insights into how users are interacting with your agent, and identify areas where it's performing well or poorly. This information can then be used to make targeted improvements to the agent's responses, dialog flows, and overall performance. Here's a breakdown of the steps involved. First, review top flows and intents for volume. This helps you identify which parts of your agent are getting the most use. This can be helpful in prioritizing areas for improvement. To ensure your solving the largest problems, ground your familiarity with the naming standards of the intents and flows. Retrieve the following common documentation from the development team: Business Requirements Document, Logical Design, Physical Design, Naming Standards, And Webhook Specifications. Determine an expected performance range. For each metric, you need to set an expected performance range. This will help you determine when the agent's performance is outside of acceptable parameters. Once you've identified the top flows and intents, you can review specific metrics associated with them. Some common metrics to track include the following: The Intent Escalation Rate should be less than 40% (but this is highly variable depending on what self-service intents are established). The Page Escalation Rate should be less than 20% for non-end pages. A No Match Rate should be less than 20%. No Responses / Loops is ideally None. Webhook Failure Rate must be less than 1%. And Webhook Latency should be less than 500 milliseconds. Finally, document high-level observations. As you analyze the data, make note of any key observations you make. This will help you identify trends and patterns that can be used to improve the agent. By following this process, you can gain valuable insights into how your conversational agent is performing and identify areas for improvement. This will help you ensure that your agent is providing a positive and effective experience for your users. Next, let’s find out what the limitations of this step are. Manual reading of conversations provides a deep understanding of individual interactions. However, it can be time-consuming and subjective. Trends analysis can identify broad trends and be automated. However, it may miss individual conversations and be difficult to interpret. Both methods can be used to effectively understand and improve conversational agents.

### Quiz - [Analyze Phase Quiz](https://www.cloudskillsboost.google/course_templates/1121/quizzes/494996)

## Identify phase

This module explores the Identify Phase of the Feedback loop process, by introducing the key steps to spot areas of improvement in the conversational journey of the Virtual Agent and by explaining how to best rank optimization opportunities to aid their prioritization in the development process

### Video - [Identify phase](https://www.cloudskillsboost.google/course_templates/1121/video/494997)

* [YouTube: Identify phase](https://www.youtube.com/watch?v=TgFbvsTgsPc)

Next, let’s review the identify phase. At the end of this section you should be able to answer the following questions: How do you identify a root cause to a specific problem? What process should be followed? And how should optimizations be ranked? Let’s begin with the question of “How do you avoid solving for symptoms and identify root causes?” We will cover this more in depth later in the course, but in order to appropriately synthesize the data from the previous steps, we need to adhere to the following steps: Create a problem statement, Document the Current State, Determine a Root Cause, Develop New Solutions, Test and Implement, And finally, Monitor and measure outcomes. Next, let’s find out what process should be followed. In the last two steps of the feedback loop, we've understood and analyzed the performance of our conversational agent. Now, it's time to get to the root of the problem and identify the issues that are causing its performance to dip. This slide outlines a process for identifying and ranking root causes of these performance issues. By following these steps, we can pinpoint the exact problems that need to be addressed to improve the agent's effectiveness. Step 1 is to Consolidate All Problems Documented First, we need to gather all the documented problems related to the conversational agent. This includes issues identified by analysts during the understand and analyze steps, as well as any feedback received from stakeholders like call center managers. Step 2 is to Create a Unique Problem Statement for Each Issue For each documented problem, we need to craft a clear and concise problem statement. This statement should capture the essence of the issue, using the 5Ws (who, what, when, where, and why) to provide context and ensure everyone understands the pain point. Step 3 is to Document the Current State Document the current state of the process or system that the conversational agent is interacting with. This could involve mapping out the user journey, outlining the technical architecture, or detailing the business requirements. Understanding the current state helps us to identify where the problem might be originating. Step 4 involves determining a root cause problem. This step involves pinpointing the underlying reason why performance issues are occurring, not just the immediate symptoms. It's like peeling back the layers of an onion to get to the core issue. Addressing the root cause is crucial for lasting improvement. Fixing symptoms might provide temporary relief, but the underlying problem could persist and resurface later. The final step, step 5, is to Rank the Root Causes. You must rank the identified root causes from all of our observations based on their severity and impact. This can be done using a matrix that considers factors like the difficulty of fixing the issue and the size of the impact it has on production traffic. By following these steps, we can effectively identify and rank the root causes of performance issues in our conversational agent. This will allow us to prioritize our efforts and focus on fixing the problems that have the biggest impact. Remember, identifying and addressing root causes is crucial for improving the performance of your conversational agent. By following this process, you can ensure that you're making the most of your feedback loop and taking the right steps to improve the agent's effectiveness. Next, let’s find out how to rank the optimizations. Prioritize areas with high traffic or that are critical for user journeys. Even small improvements can significantly impact user satisfaction and completion rates. For high traffic areas, analyze user feedback and conversation logs to identify pain points and refine NLU models, conversation flows, or UI elements. Optimize top-of-the-funnel interactions to ensure clarity, engagement, and effective user guidance from the very beginning. By focusing on these key areas, you can dramatically improve the overall user experience. To improve user experience, quickly address areas where the AI can't understand user queries. Analyze "no-match" situations to find missing intents and rephrase existing ones to capture these common phrases. This ensures all relevant questions are understood, leading to smoother interactions. Look beyond busy areas for improvement opportunities in your virtual assistant. Analyze escalations to live agents to fix issues like confusing responses or looping. Identify drop-off points and understand why users abandon conversations. Improve the assistant's communication style for better engagement and clarity. Address ambiguous references to provide clear and context-aware interactions. These efforts can significantly enhance the overall user experience. Continuously improving your virtual assistant or chatbot can lead to better user experiences and overall performance. This is achieved by focusing on key areas that have a big impact, like natural language processing and understanding user intent. By systematically addressing weaknesses in these areas, you can refine your chatbot and make it more helpful and responsive.

### Quiz - [Identify Phase Quiz](https://www.cloudskillsboost.google/course_templates/1121/quizzes/494998)

## Enhance phase

This module explores the Enhance Phase of the Feedback loop process, by highlighting the critical relationship between Natural Language Understanding quality, user experience, and integration and the importance of testing for continuous improvement

### Video - [How to enhance the virtual agent](https://www.cloudskillsboost.google/course_templates/1121/video/494999)

* [YouTube: How to enhance the virtual agent](https://www.youtube.com/watch?v=5iCKGwr1LoU)

The next phase is to enhance the conversational agent. This will require working with the team responsible for developing the agent. At the end of this section you should be able to answer the following questions: How do we enhance the virtual agent? And what process do we follow? Let’s begin by exploring how to enhance the virtual agent. Optimizing agent performance requires constant attention to Natural Language Understanding quality, user experience, and integration. Ongoing testing and refinement guarantee exceptional user interactions, and a well-functioning agent ultimately contributes to business success. Enhancing virtual agents requires focusing on four key areas: The first is Natural Language Understanding (or NLU) quality, Then user experience, integration with existing systems, and continuous testing and improvement. Ensuring NLU accurately interprets user intent is crucial, as is designing a user-friendly interface for smooth interaction. Integrating the virtual agent with other systems expands its capabilities, while continuous testing and improvement refine its performance over time. By diligently attending to these areas, virtual agents can provide a consistently valuable and satisfying user experience. To accurately recognize user intent, it's crucial to have a well-tuned NLU system. You can improve its quality by: expanding its knowledge of common queries, guiding user requests to the right intents, learning from real-world examples, and enhancing its ability to extract entities from text. To make AI assistants perform better, prioritize user experience. This means: removing confusing language from their responses, ensuring they stay consistent, and constantly gathering feedback to refine their interactions. By focusing on how users feel when interacting with AI, we can significantly improve their overall performance. For agents to be truly effective, they need to be tightly linked to backend systems. This means: ensuring their responses match the backend data and logic, Confirming that errors are handled smoothly with clear explanations, and checking that the user experience is consistent across all channels. Regularly testing a system is vital to detect and fix performance problems. A strong testing process allows you to: recreate past issues, assess agent behavior in diverse situations, and constantly track and enhance agent performance. This ensures optimal system operation.

### Video - [What process to follow](https://www.cloudskillsboost.google/course_templates/1121/video/495000)

* [YouTube: What process to follow](https://www.youtube.com/watch?v=IqwoyFtQD9w)

Next, let’s review what process to follow. To recap, this is the high level framework for how to tackle any problem. In the subsequent slides we will cover each step of this process in more detail. When we started this process, we were preparing the steps to determine a root cause from the identify phase. Step 1 is to create a problem statement. First, we need to clearly understand the problem we're trying to solve. This means pinpointing the specific pain point faced by your business or customers, quantifying its impact, and understanding its historical context. Remember, a well-defined problem statement lays the foundation for a successful improvement process. We can use the problems identified in previous phases to inform each problem statement. Step 2 is to document the Current State. Next, we'll document the current state of your conversational agents' performance. This involves verifying how performance is currently measured and assessed, along with establishing a baseline and identifying relevant metrics. Data is our guide here, as it ensures we base our improvement efforts on concrete evidence. Step 3 is to Determine a Root Cause. By using a technique like root cause analysis, we can delve deeper to uncover the underlying reason behind the identified problem. Tools like the 5 Whys can be helpful in this stage. While finding the absolute root cause is desirable, remember that identifying a manageable root cause that allows for actionable solutions is key. When we transition into the enhance phase, we need to determine what solutions will work best to address the issues by making changes in the conversational agent. This will also require robust testing. This stage begins with Step 4, which is to Develop New Solutions Armed with the root cause, we'll brainstorm and evaluate different solutions. This might involve carefully weighing potential solutions, selecting the most promising one, and then developing a concrete plan for putting it into action. Remember, feasibility and measurability are crucial considerations here. It then ends with Step 5: Test and Implement Next, we'll rigorously test and implement the solution to gauge its effectiveness. A/B testing or other comparative methods can help us assess performance before and after the solution is put in place. Lastly is the monitor phase. We’ll explore this phase in more depth later. For now, let’s explore it at a high level. This stage involves the final step, which is to monitor and measure outcomes. We need to implement Continuous monitoring, as this helps us identify any unintended consequences and opportunities for further improvement. By systematically following these steps, basic performance measurement becomes a powerful tool for pinpointing and resolving issues with your conversational agents. Ultimately, this leads to enhanced performance and a more satisfying customer experience. Now that we've identified the root cause of the performance issue with your conversational agent, let's explore some potential solutions and how to implement them effectively. This is where the "Develop and Implement New Solutions" stage of our Feedback Loop comes in. So, why we develop and implement new solutions? One reason is to address the root cause. By pinpointing the underlying problem, we can develop targeted solutions that have a lasting impact on performance. Another reason is to test and iterate. It's crucial to remember that the first solution we try may not be the perfect one. This stage allows us to test different options, gather data, and refine our approach based on the results. So, how we develop and implement new solutions? First, we brainstorm with the team and get everyone involved in generating ideas. This could involve your development team, customer support specialists, and even subject matter experts. The goal is to come up with a comprehensive list of potential solutions. The next step is prioritization. Not all solutions are created equal. We need to prioritize them based on two key factors: impact and difficulty. Ideally, we want to choose the option that will deliver the most significant improvement for the least amount of effort. Next is piloting and testing. Before rolling out any new solution to all your users, it's essential to test it in a controlled environment first. This could involve a small group of pilot users or a simulated testing platform. Then we engage in evaluating and iterating. Once we have some data from the pilot test, we can evaluate the effectiveness of the solution. Did it address the root cause? Are there any unintended consequences? Based on the results, we may need to refine the solution or even go back to the drawing board and come up with a new one. Then the final step is deployment and communication. If the pilot test is successful, the next step is to deploy the solution to all your users. This should be accompanied by clear communication to your stakeholders and users about what changes have been made and why. Remember, developing and implementing new solutions is an iterative process. Be prepared to test, learn, and adapt as you go. By following these steps, you can ensure that you're making well-informed decisions that will ultimately improve the performance of your conversational agent.

### Quiz - [Enhance Phase Quiz](https://www.cloudskillsboost.google/course_templates/1121/quizzes/495001)

## Monitor phase

This module explores the Monitor Phase of the Feedback loop process, by delving into the common methods for detecting performance issues in conversational agents (thresholds, standard deviations, and N-daily), and the best ways to deal with performance anomalies

### Video - [Monitor phase](https://www.cloudskillsboost.google/course_templates/1121/video/495002)

* [YouTube: Monitor phase](https://www.youtube.com/watch?v=yOeKgwcR0KE)

Finally, let’s review the Monitor Phase. At the end of this section you should be able to answer the following questions: How does continuous monitoring work? What is the process? And how do I deal with failures? Let’s begin by asking how continuous monitoring works? Let’s explore three common methods for detecting performance issues in your conversational agents: thresholds, standard deviations, and N-daily. These methods help us identify anomalies in performance that might indicate areas for improvement. The first method is thresholds. Here, we define absolute upper and lower bounds for what constitutes acceptable performance. If the agent's performance falls outside these bounds, we know something is wrong. This is a simple and straightforward approach, but it can be inflexible. If we set the thresholds too high, we might miss subtle performance issues. And if we set them too low, we might be bombarded with false alarms. A more nuanced approach is to use standard deviations. Here, we analyze the agent's performance over time and calculate the standard deviation. This tells us how much variability there is in the agent's performance. If a new data point falls more than a certain number of standard deviations from the mean, we can flag it as a potential issue. This is a more adaptive approach than using fixed thresholds, but it can be computationally expensive. The third method is N-daily. This method compares the most recent data point with the data point from N days ago. If the change is greater than a certain threshold, we can flag it as a potential issue. This is a simple and efficient way to detect sudden spikes or slumps in performance. However, it's not as good at detecting gradual drifts in performance. These are just three common methods for detecting performance issues in conversational agents. The best method for you will depend on your specific needs and goals. It's often a good idea to use a combination of methods to get a more complete picture of your agent's performance. Next, let’s review the process. As we implement new solutions for our conversational agents, it's crucial to monitor and measure their outcomes. This helps us understand if they're truly resolving the root cause of the issues we're trying to address. There are two main reasons why this is so important. First, as we roll out new solutions, or as customer behaviors change, the performance of the conversational agent can change over time. What works well today might not work well tomorrow. Second, even if the new solution seems to be working initially, we need to make sure it's actually having the desired impact. We need to be able to measure whether it's truly resolving the problem we're trying to solve. So, how do we go about monitoring and measuring the outcomes of our conversational agents? Here are four key steps. First, we establish what we'll measure: Before we implement any new solutions, we need to define what metrics we'll use to track their success. This could include things like customer satisfaction, task completion rates, or call deflection rates. Then we identify how often we'll measure. We need to determine how frequently we'll track these metrics. This could be daily, weekly, or monthly, depending on the specific goals we're trying to achieve. Next, we establish a review process. Once we have our metrics and measurement cadence in place, we need to establish a process for reviewing the results. This could involve holding regular meetings to discuss the data and identify any areas for improvement. Finally, we determine actions for changes. We need to decide what actions we'll take if we see any spikes or dips in our control measurements. This could involve rolling back a new solution, making adjustments to the existing solution, or even developing entirely new solutions. By following these steps, we can ensure that we're effectively monitoring and measuring the outcomes of our conversational agents. This will help us to identify areas for improvement and make sure that our agents are always delivering the best possible experience for our customers. When looking at changes over a longer period of time, it’s important to remember that successful conversational agents are not static. Some important things to look for include: Changes in user behavior: If users start using the agent in new or unexpected ways, the performance on the originally envisioned tasks might decrease. You also need to look for a drift in the training data: The data used to train the agent might become outdated or no longer reflect real-world usage, leading to performance decline. And finally, observe any technical issues: Bugs or technical problems with the agent's infrastructure could also impact performance. By continuously monitoring for changes, you can proactively identify and address any potential performance issues before they significantly impact user experience. Remember, a successful conversational agent is not a static creation; it's a constantly evolving entity that needs to adapt to changes in user behavior, language, and the surrounding environment. By embracing a culture of continuous monitoring and improvement, you can ensure your agents keep delivering exceptional user experiences. Next, let’s find out how to deal with failures. “Hope is not a strategy” is an unofficial Site Reliability Engineering motto at Google. It is a truth universally acknowledged that systems do not run themselves. How, then, should a system (particularly a complex computing system that operates at a large scale) be run? Conversational AI systems, including webhooks and APIs, are prone to failures. To ensure system resilience, proactively identify and address potential risks by mapping out failure scenarios and developing mitigation strategies. Analyze metrics like failure rate to evaluate the effectiveness of mitigation strategies and continuously improve the system. It’s crucial to be prepared for failures in any system, including conversational AI systems. Webhooks and APIs, as integral components of these systems, are also susceptible to errors and disruptions. So proactively identifying and addressing potential risks is essential for maintaining system resilience and ensuring a seamless user experience. This involves identifying common failure scenarios, such as network outages, payload formatting errors, and authentication issues. For each potential risk, a corresponding mitigation strategy should be in place, such as: implementing retries, utilizing fallback URLs, and employing robust authentication mechanisms. The analytics tool plays a vital role in assessing the impact of failures and evaluating the effectiveness of mitigation strategies. By analyzing metrics such as failure rate, timeout rate, and average latency, you can gain insights into the frequency and severity of failures and how your mitigation efforts are performing. This data-driven approach enables continuous improvement of your webhook and API services, ensuring they meet the demands of your conversational AI system. In summary, proactively preparing for failures and leveraging analytics to improve mitigation strategies are essential practices for maintaining the reliability and performance of conversational AI systems. You can ensure that your system is resilient to disruptions and delivers a consistent, positive user experience by: Identifying risks, Implementing effective mitigation measures, and continuously evaluating the impact of failures, To review everything we’ve discussed today, let’s return to our original image of the Feedback loop process. Ultimately our goal to have a way to improve agent performance that is repeatable and guarantees results to increased customer satisfaction which can lead to better sales and brand reputation. By continuously monitoring, analyzing, and improving your conversational agent, you can ensure that it's always delivering the best possible experience for your users. Take what you've learned here and start implementing it in your own Feedback Loop for your conversational agents. Google Cloud can provide you with the tools and resources you need to get started, including CCAI Academy, which offers a wealth of information and training on conversational AI best practices.

### Quiz - [Monitor Phase Quiz](https://www.cloudskillsboost.google/course_templates/1121/quizzes/495003)

## Additional Resources

This module includes the list of additional resources focused on Dialogflow CX Analytics and Dialogflow CX Conversation History that complement the course learning

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/1121/documents/495004)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
