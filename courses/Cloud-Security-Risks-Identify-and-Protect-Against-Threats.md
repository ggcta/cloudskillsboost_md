---
id: 968
name: 'Cloud Security Risks: Identify and Protect Against Threats'
type: Course
url: https://www.cloudskillsboost.google/course_templates/968
date_published: 2024-05-22
topics:
  - IAM
  - Cloud Computing
---

# [Cloud Security Risks: Identify and Protect Against Threats](https://www.cloudskillsboost.google/course_templates/968)

**Description:**

This is the third of five courses in the Google Cloud Cybersecurity Certificate. In this course, you'll explore the principles of identity management and access control within a cloud environment, covering key elements like AAA (Authentication, Authorization, and Auditing), credential handling, and certificate management. You'll also explore essential topics in threat and vulnerability management, cloud-native principles, and data protection measures. Upon completing this course, you will have acquired the skills and knowledge necessary to secure cloud-based resources and safeguard sensitive organizational information. Additionally, you'll continue to engage with career resources and hone your interview techniques, preparing you for the next step in your professional journey.

**Objectives:**

* Describe fundamental identity and access management concepts specific to the cloud.
* Identify key components of AAA (Authentication, Authorization, Auditing) for strengthened cloud security.
* Explain capabilities and best practices for efficient asset and resource management in the cloud.
* Explore common security vulnerabilities and evaluate their impact and risk mitigation strategies.

## Access control and identity management

In this module, you'll delve into the crucial aspects of identity management and access control in cloud computing, learning about security principles such as least-privilege and separation-of-duties, and the crucial elements of AAA: authentication, authorization, and auditing. You'll explore various access control methods, credential management, and certificate handling, key to securing cloud applications.

### Video - [Introduction to Course 3](https://www.cloudskillsboost.google/course_templates/968/video/483882)

* [YouTube: Introduction to Course 3](https://www.youtube.com/watch?v=UVLmV6snm2c)

Hi there. It’s great to have you here. In this course, we’ll explore the essential aspects of securing digital assets in the cloud. And, by the end of the course, you'll have a well-rounded understanding of the tools and techniques needed to protect your organization's data and infrastructure. Before we dive in, let me introduce myself and tell you a bit about how I got into the interesting world of cloud security. I'm Manny and I’m a Security Engineer. Like some of you, I started in a different field before finding my way to cloud security. I started my career in IT. I worked as a help desk technician for a few years and then I moved into a systems administration role. I enjoyed working in IT, but I was always fascinated by security. My unexpected journey took me to exploring vulnerabilities and challenges in the cloud space. I looked into Google Cloud projects to understand the various resources I could deploy, like compute instances, and how to configure firewalls for a home network lab. I realized that cloud security was a rapidly growing field, and I wanted to be a part of it. From my experiences and learnings, I'm now an Offensive Security Engineer at Google, where I simulate real-world attacks against Alphabet and keep our data, systems, and users safe. Venturing into cloud security can be an exciting challenge offering great opportunities. As we journey through this course, you'll gain key knowledge and insights, shining a fresh light on how to defend our digital infrastructure. Now, let's dive into the content and build skills essential for safeguarding digital assets in the cloud. First, you'll explore cloud security fundamentals, including identity management, access controls, trust boundaries, and the differences between zero trust and traditional perimeter security. Next, you'll learn about cloud-native architecture and data protection, secure configuration, threat and vulnerability management, and data protection techniques like encryption and data masking. Then, you'll learn about cloud-based operations and continuity, including business continuity planning, disaster recovery, and infrastructure as code. Finally, you’ll be introduced to automation, continuous integration and continuous delivery, service accounts, and secure management of credentials in cloud-native environments. Now let’s examine each topic in more detail. In the Cloud Security Fundamentals section, you'll learn more about the essential concepts and practices related to securing cloud environments. Your primary area of focus will be identity management principles and effective access controls in cloud environments. This includes how you’ll learn to effectively manage user access. Next, you’ll examine the concept of trust boundaries in cloud security. This will enable you to better understand and manage the flow of sensitive information within your organization. You’ll then compare two widely-used security models: zero trust and traditional perimeter security. By comparing these two models, you'll discover the knowledge and insights needed to determine the best security approach for your organization. In the Cloud-Native Architecture and Data Protection section of this course, you'll review best practices for secure configuration, and the effective management of threats and vulnerabilities in the cloud. This knowledge is essential for ensuring the security and integrity of your cloud-based applications and infrastructure. During this section, you'll learn the details of cloud-native architecture and design considerations, focusing on key components like containers, serverless computing, and orchestrators. You'll also examine various data protection and privacy techniques that are crucial for safeguarding sensitive information and upholding privacy standards in the cloud. These techniques include: encryption, data masking, the use of certificates, and the implementation of robust governance models. In the Cloud-Based Operations and Continuity section, you’ll first focus on business continuity planning within the world of cloud computing. You’ll inspect the steps organizations take to develop a robust plan that enables recovery from any potential disruptions that may occur. You’ll also explore various key concepts and technologies that are crucial for efficient cloud-based operations. These include Infrastructure as Code, or IaC, Automation, and Continuous Integration and Continuous Delivery, or CICD. Along with these concepts, this section will also cover the usage of Service Accounts within cloud-native environments, and the topic of Secure Credential Handling. Throughout this section, you’ll learn the importance of automating processes and securely managing credentials. After completing this material, you’ll be on your way to becoming an effective and knowledgeable cloud security expert who is capable of addressing challenges and protecting an organization's cloud assets. Join me as we continue the exciting journey to expand your cloud security skills.

### Document - [Course 3 overview](https://www.cloudskillsboost.google/course_templates/968/documents/483883)

### Video - [Manny: A day in the life of a red team security engineer](https://www.cloudskillsboost.google/course_templates/968/video/483884)

* [YouTube: Manny: A day in the life of a red team security engineer](https://www.youtube.com/watch?v=WGO3VE7ERIk)

Hi, I'm Manny. I’m a Security Engineer on Google's Red Team. My job is to simulate real-world attacks against Alphabet and keep our users safe. I make it harder to hack Google by hacking Google. I started out as a Civil Engineer, but I was always fascinated by the idea of protecting people's data and systems from attackers. I learned most of my hacking knowledge through free online resources like YouTube, books, and other helpful hands-on learning material. I also tinkered with virtual machines and looked at foundational security certificates, technical books, and programming to learn more about the field. I did extensive sandbox testing on different platforms. I was able to explore security topics in depth. I connected with friends already working in the field to get a better understanding while researching the various paths into cybersecurity. Additionally, Google's Cybersecurity Professional Certificate gave me a comprehensive foundation into cybersecurity and helped build my security mindset. I started my career as a parking garage attendant in college. I then worked in IT at a help desk, where I learned a lot of my initial computer skills. After graduating, I realized that I enjoyed troubleshooting and tearing computers apart. I pursued IT full time and, from there, I became eager to learn more about the various disciplines within cybersecurity and how to protect organizations from cyberattacks. I've gained a lot of transferable skills along the way, from communicating effectively with stakeholders to leading projects, to working with colleagues and friends. And involvement with community service has given me a variety of skills to work with when collaborating with Security Engineers and tackling some of the hardest challenges each day. I accepted an amazing opportunity with Google’s Information Technology Residency Program where I honed in on my computer, programming, and networking skills. I participated in a security rotation program and realized that I enjoyed the work, from defending devices and networks against malicious activity to analyzing logs and deep diving into issues. I realized that cybersecurity was a great career and so I followed my passion. I continued to learn independently so that I could transfer into the Security Org. Eventually, I made it, and I’m still here today.

### Document - [Helpful resources and tips](https://www.cloudskillsboost.google/course_templates/968/documents/483885)

### Document - [Lab technical tips](https://www.cloudskillsboost.google/course_templates/968/documents/483886)

### Document - [Explore your course 3 scenario: Cymbal Bank](https://www.cloudskillsboost.google/course_templates/968/documents/483887)

### Video - [Welcome to module 1](https://www.cloudskillsboost.google/course_templates/968/video/483888)

* [YouTube: Welcome to module 1](https://www.youtube.com/watch?v=R4i1cuawQPY)

It often helps to think about the big picture. In this course, we’ll provide you with a comprehensive overview of the latest cloud security concepts, tools, and best practices. We’ll cover three main topics that are crucial for understanding the cloud environment. First, we’ll discuss identity management and access control in a cloud environment. We’ll also review related concepts, including single sign-on, or SSO, multifactor authentication, or MFA, and identity and access management, or IAM, services. Second, we’ll explore the different types of access controls employed in a cloud environment, including role-based access control, or RBAC, and attribute-based access control, or ABAC. And third, we’ll provide you with an overview of trust boundaries, best practices for configuring and managing defense, and the zero trust security model. By the end of this course, you'll have an understanding of cloud cybersecurity concepts and be able to apply these concepts to scenarios you might really encounter in the workplace. Let’s get started.

### Video - [Core principles of identity management](https://www.cloudskillsboost.google/course_templates/968/video/483889)

* [YouTube: Core principles of identity management](https://www.youtube.com/watch?v=Ty_zDOZyRdM)

Good security starts by knowing who has access to what. In this video, we'll be discussing the core principles of identity management in a cloud environment, and how it aligns with least privilege and separation of duties. We'll also explore how these concepts differ from traditional on-premises environments. Then, we’ll dive into key topics like role-based access control, single sign-on, multifactor authentication, and identity and access management services. Let's get started. Identity management is a core component for access control in cloud environments. Combining identity and network controls strengthens the defense of cloud environments. One of the key differences between cloud-based and on-premises identity management is the scale and variety of providers and platforms available in the cloud ecosystem. Organizations are no longer limited to in-house infrastructure and can now choose from a multitude of service providers, technologies, unique tools, and interfaces. This diversity, while offering flexibility and choice, also requires a unified approach to managing user identities and access control. Centralized and consistent identity management is essential for several reasons. Let’s discuss five of them. Reason one, you streamline access control when you have multiple platforms and services in play and centralize the process of granting, modifying, and revoking access to resources —as opposed to managing these resources in separate environments. Reason two, you enhance security when you minimize vulnerable systems by consolidating access controls and preventing potential breaches. Reason three, you improve compliance when you fulfill requirements to meet compliance rather than when you just focus on implementing controls on data access and user authentication. Reason four, you simplify administration when you provide a single interface for managing users, groups, and permissions across many systems in your environments. Reason five, you save operational and resource costs when you centralize multiple identity management systems into a single identity platform. Now, let's go over some key topics in identity management for cloud environments. Role-based access control, or RBAC, is a method of controlling access to resources based on the roles assigned to users. This helps ensure users only have permissions and access to the resources necessary for their job. Single sign-on, or SSO, is a technology that combines several different logins into one. It simplifies the authentication process by allowing users to access multiple applications using a single set of credentials. This minimizes the risk of password fatigue, which is when users feel overwhelmed by the number of different login credentials they need to manage. Multifactor authentication, or MFA, is a security measure that requires a user to verify their identity in two or more ways to access a system or network. MFA adds an extra layer of security by requiring users to prove their identity using multiple methods, like a password and a fingerprint scan. Identity and access management services, or IAM, is a collection of processes and technologies that help organizations manage digital identities in their environment. These services enable you to create, manage, and delete users and groups, and assign and enforce access policies. And that's a wrap. Understanding key concepts of identity management in the cloud, like RBAC, SSO, MFA, and IAM services, and how to apply them to cloud-based resources will increase your effectiveness as a cloud security professional. By implementing these principles, you'll be well on your way to securing your cloud environment.

### Document - [Uses for identity management measures](https://www.cloudskillsboost.google/course_templates/968/documents/483890)

### Video - [Authentication, authorization and auditing (AAA)](https://www.cloudskillsboost.google/course_templates/968/video/483891)

* [YouTube: Authentication, authorization and auditing (AAA)](https://www.youtube.com/watch?v=mUd6l74zJ-0)

In our ever-evolving digital world, security is essential to protect sensitive data and prevent unauthorized access. With the growing threat of cyberattacks, it's essential to have robust security measures in place to safeguard against potential breaches. Fortunately, the Triple A framework provides a reliable and effective way to help ensure security in computer systems. So, what are the three essential components of Triple A? Authentication, authorization, and auditing, or Triple A for short, describes a security framework that is used to verify the identity of users or groups in systems and grant them access based on their privileges. Let's examine each component. First is authentication. Authentication is the process of verifying who someone is. A common example is using a password to authenticate a user on a server. But, relying solely on passwords is insufficient, leading to the development of multifactor authentication, or MFA. MFA is a security measure that requires a user to verify their identity in two or more ways to access a system or network. Importantly, MFA prohibits the use of identical factors for authentication, adding an extra layer of security that helps protect against unauthorized access. Examples of MFA include combining a password with a fingerprint scan or a one-time code sent to a user's phone. This approach significantly reduces the risk of unauthorized access, as malicious actors would need multiple forms of authentication to breach the system. Next, we have authorization. Authorization is the concept of granting access to specific resources in a system. One form of authorization is role-based access control, or RBAC. RBAC is a method of controlling access to resources based on the roles assigned to users. RBAC streamlines the process for administrators, allowing them to manage permissions collectively rather than individually. Understanding the differences between authentication and authorization, their respective roles in identity management and access control, and their importance in maintaining system security is crucial. By implementing strong authentication and authorization processes, organizations can ensure that only authenticated and authorized users access their systems, and that those users can access only the resources they need to perform their job functions. Finally, there’s auditing. Auditing is the process of recording and reviewing system activity to ensure compliance with security policies and identifying potential security breaches. The primary purpose of auditing is to ensure that any deviations from the expected behavior of a resource are identified and addressed. Auditing also helps identify potential security threats and vulnerabilities. There are several types of audits that can be conducted, including compliance audits, security audits, and operational audits. Compliance audits ensure that the organization is complying with the relevant laws and regulations. Security audits assess the security posture of the information system. Operational audits evaluate the effectiveness and efficiency of the system's operations. An audit is only effective when its objectives are clearly defined from the beginning. These objectives should be specific, measurable, achievable, relevant, and time-bound. These are called SMART goals. Audit objectives should align with the organization's overall goals and objectives. By establishing clear audit objectives, auditors can focus their efforts on areas that are most critical to the organization's security. Simultaneously, auditors need to choose suitable tools and techniques that also align with the audit objectives and scope to help streamline the audit. The Triple A framework —authentication, authorization, and auditing— provides a comprehensive and effective approach to ensure the confidentiality, integrity, and availability of cloud resources. By implementing this framework, organizations can safeguard their data and systems against cyber threats and breaches. With this in mind, use SMART goals to audit your organization’s compliance, security, and operational systems to identify potential security threats and vulnerabilities. Remember, prevention is always better than response.

### Video - [Credential handling and service accounts](https://www.cloudskillsboost.google/course_templates/968/video/483892)

* [YouTube: Credential handling and service accounts](https://www.youtube.com/watch?v=5bgHnqm2gSw)

You may come across security practices in your everyday life. For example, businesses lock their doors when they close for the day and often keep security cameras on-premises. You can compare these business security practices with a cloud security team’s security principles to keep their organization’s services and accounts safe. In this video, we'll discuss important topics related to security risks, and best practices to help keep systems and data secure and protected. Specific security principles we’ll cover include credential management, secrets management, non-interactive access, and security protocols to protect your systems from security risks. As a cloud security professional, credential management is a good place to start. The risks associated with unauthorized access to data are primarily because of inadequate credential management practices. If your organization’s users create weak passwords, share their credentials, or fail to change credentials regularly, threat actors can easily gain access to your systems. To avoid such risks, it's important to require strong and unique passwords for all user accounts, and require users to regularly change passwords. Users can also use a password manager to securely store and manage their credentials. Another important concept is secret management. Secrets are sensitive information like Application Programming Interface, or API keys, passwords, and certificates that are used to authenticate and authorize access to systems. Your cloud security team will want to manage your organization’s secrets to prevent threat actors from compromising sensitive information, which may lead to negative consequences. Best practices for managing secrets include using a centralized secret management tool, strong encryption methods to protect them, and regular rotation of API keys, passwords, and certificates. This way, unauthorized users are unable to use secrets to exploit services or gather data. Non-interactive service accounts are another security risk that your security team needs to address. These accounts are created for automated processes and services and are often overlooked. If not properly managed, they can provide an easy entry point for malicious actors to infiltrate your systems. To mitigate this risk, it's important to regularly review and audit these accounts, rotate keys associated with service accounts regularly, and limit their privileges. Now that we’ve covered some primary security risks, let’s dive into some important security protocols that can help protect your systems. mTLS, OAuth, and OpenID are three common protocols that provide secure communication between different systems and services. Let's explore each of these protocols and understand their role in securing user communication and network access. mTLS, or mutual Transport Layer Security, is a protocol that provides mutual authentication and encryption between servers. It's a variation of the Transport Layer Security, or TLS protocol, that secures communication between a client and a server. With mTLS, both the client and the server authenticate each other, ensuring that the communication is secure and private. This helps prevent potential breaches or attacks, and ensures that only authorized parties can access the information being exchanged. Open Authorization, or OAuth, is a method that allows users to grant applications access to their information on other sites or systems without the need to share their passwords. For example, if you want to allow an application to access your Google Drive account, you can use OAuth to securely authorize the application to access your account without sharing your credentials. This ensures that your credentials will not be compromised and you have control over the access given to the application. Finally, OpenID is a protocol that is used for single sign-on functionality, allowing users to authenticate once and access multiple services. The process involves the user authenticating with an identity provider in exchange for an identifier. This identifier can then be used on another site or service, without the user having to authenticate on that site or service for a second time. This simplifies the login process for users and reduces the risk of password fatigue and credential reuse. It's critical to implement best practices for credential management, secrets management, non-interactive access, and security protocols to protect your systems from security risks. By following these practices, you can ensure that your systems and data are secure and protected, allowing the appropriate people to reach the information they need.

### Lab - [Create a role in Google Cloud IAM](https://www.cloudskillsboost.google/course_templates/968/labs/483893)

Create a custom role and grant access to a user.

* [ ] [Create a role in Google Cloud IAM](../labs/Create-a-role-in-Google-Cloud-IAM.md)

### Quiz - [Test your knowledge: Access management](https://www.cloudskillsboost.google/course_templates/968/quizzes/483894)

### Video - [Access controls in the cloud](https://www.cloudskillsboost.google/course_templates/968/video/483895)

* [YouTube: Access controls in the cloud](https://www.youtube.com/watch?v=xXAYK-9vDJg)

Imagine you’re a receptionist working at the front desk of an apartment complex. It’s your responsibility to decide who you let in. The concept of access controls in security works in a similar way. In this video, we’ll discuss the different types of access controls and how they can be implemented to help keep your cloud resources secure. Let's get started. Access control is a crucial element of cybersecurity because it ensures that only authorized individuals can access your data and resources. There are three main types of access controls: discretionary, mandatory, and role-based access controls. First is discretionary access control, or DAC, a security model where the owner of the data or resource has the discretion to grant or revoke access to other users. For example, in Google Drive, it’s possible to share only view access with an individual to one specific file, while simultaneously providing edit access to a different file. Both files can reside within the same directory. These varying levels of access —from partial to full— from the owner of the files, are discretionary. In a cloud environment, DAC can be implemented using access control lists, or ACLs. ACLs specify the permissions for each user or group. Next is mandatory access control, or MAC, which is a strict security model in which access is granted based on predefined security policies. For example, a private company’s information is classified into three categories: confidential, internal, and public. The company controls the distribution of crucial information under the confidential classification by granting access on a need-to-know basis. By assigning individual permissions based on their required access levels, the company successfully secures its proprietary information. In a cloud environment, MAC can be implemented using security labels or tags which are assigned to both data and users. Access is granted only if the user's security label matches the data's label. Now, let’s discuss role-based access control, or RBAC. RBAC is a method of controlling access to resources based on the roles assigned to users. In a cloud environment, RBAC can be implemented by assigning users to different roles and managing permissions at the role level. In some cases, it may be necessary to implement role hierarchies to simplify the management of permissions. Role hierarchies allow higher-level roles, such as a user with edit access to a Google Drive directory, to inherit permissions from lower-level roles, such as a user with view access to a Google Drive directory, ensuring that users with more responsibility have the necessary access to perform their duties. In other words, a user that can edit, can also view documents in a directory, though a user with view access will not be able to edit documents in a directory. Finally, attribute-based access control, or ABAC is a security model where access is granted based on attributes like user, resource, and environment. In a cloud environment, ABAC works by creating policies that define the conditions under which access is granted or denied. These policies can be based on a variety of attributes, like the user's job title, the sensitivity level of the data, or even the current day and time. Implementing ABAC in a cloud environment typically involves setting up a policy decision point, or PDP, and a policy enforcement point, or PEP. The PDP evaluates policies and makes access decisions, while the PEP enforces those decisions by granting or denying access to the resources. As a cloud security professional, you’ll need to apply a multifaceted approach to effectively implement access controls in the cloud and secure your data. One key component to implementing access controls in the cloud is identity and access management, or IAM. Remember IAM allows you to manage users, groups, roles, and permissions across your cloud environment. Along with IAM, organizations must also consider policies and resource hierarchies. These are crucial for structuring and enforcing access controls. Now that you know the different access control types, let's discuss some best practices for implementing access controls in cloud environments. The top three best practices include one, apply the principle of least privilege, giving users only the permissions they need to perform their tasks. Two, separate duties to minimize the risk of unauthorized access or actions. Three, regularly audit access controls in your cloud environment to ensure access permissions are up to date. Then review and update to and add or revoke access when necessary. By implementing these access controls you can ensure the security of your data in the cloud. Remember, as the receptionist of the cloud, you only want to let approved guests and visitors into the building.

### Document - [Cloud access control in action](https://www.cloudskillsboost.google/course_templates/968/documents/483896)

### Quiz - [Test your knowledge: Types of access controls](https://www.cloudskillsboost.google/course_templates/968/quizzes/483897)

### Video - [Perimeter protection](https://www.cloudskillsboost.google/course_templates/968/video/483898)

* [YouTube: Perimeter protection](https://www.youtube.com/watch?v=yo7B9Y_xQWs)

Setting boundaries is important. In this video, you’ll learn the different types of perimeter protection, explore various types of defenses, and learn about the importance of trust boundaries. Finally, you’ll learn best practices for configuring and managing these controls. This is all very exciting stuff. Let's get into it. With the increasing adoption of cloud environments, perimeter protection has become more essential than ever. In a cloud environment, perimeter protection refers to the security measures implemented at the edge of a network or system to defend against unauthorized access and cyber threats. As a cloud security professional, there are several types of perimeter defenses that can be implemented to protect a cloud environment. Let's explore some of them. Identity and context-based access is increasingly becoming the first line of defense because it offers a robust security solution compared to traditional methods. Firewalls typically are only for internal networks and can help prevent lateral movement within the system. Intrusion detection systems, or IDSs, monitor network traffic, check for signs of suspicious activity, and alert administrators when detected. Moving along the same lines as the intrusion detection system (IDS), we come across a similar yet distinct system, the intrusion detection and prevention system (IDPS). Intrusion detection and prevention systems, or IDPSs, monitor network traffic for signs of malicious activity and either alert the administrator, or actively block the threat, or both. Virtual private networks, or VPNs, create a secure, encrypted tunnel for data to travel between the cloud environment and the user, ensuring that data remains confidential and safe from prying eyes. Finally, access control lists, or ACLs, restrict access to resources in the cloud, ensuring that only authorized users have access to specific data or services. Trust boundaries are a crucial aspect of perimeter protection because they define the points in a network where the level of trust changes, like transitioning from a trusted internal network to an untrusted external network. For example, you can use systems like firewalls and routers to separate the public internet from the internal corporate network at both physical and logical levels. This action establishes and maintains a clear boundary between trusted and untrusted environments, providing a vital layer of security for the organization. An office building is like a trusted network, with each floor representing different sectors. Inside this network, doors and walls represent firewalls. Think of the outer walls of the office building as the shift from a trusted network, moving into untrusted territory, like the public internet. Now, imagine some offices in the building. These offices are trusted spaces, with extra security measures like locked doors. These spaces are like a company's private, secure zones that contain critical information. Other rooms, like the lobby or an outdoor patio, would be untrusted spaces. They're more open, like public areas in a company's network, but still have basic security. The key point is that trusted or untrusted isn't about the room itself, but about the level of protection and the sensitivity of the information within. Regular checks, like office security inspections, help keep both real and digital environments secure. Without clearly defined trust boundaries, your network is at risk of unauthorized access, data breaches, and other security issues. Establishing trust boundaries can help you better manage access to sensitive data and identify potential vulnerabilities. Now that we understand the importance of perimeter protection and trust boundaries, let's discuss some best practices for configuring and managing these controls to ensure your cloud environment is secure. These best practices can be categorized into three primary areas: improve cloud network security, monitor and control access, and maintain security policies and training. Here’s how you can improve cloud network security. Implement network architecture that always requires user authentication. Segment networks, use microperimeters, and apply context-aware access controls and define trust boundaries between network zones based on risk and required access controls. To monitor and control access, follow these best practices. Regularly review and update firewall rules. Deploy cloud-based IDS IPS solutions for protection, and implement secure remote access solutions, and enforce strong authentication methods. Lastly, you can maintain security policies and training in these ways: audit and update IAM policies for least privileges, train employees on cloud security best practices, and enforce strong password policies and multifactor authentication, or MFA. By understanding the concepts of perimeter protection, trust boundaries, and implementing cloud security best practices, you can significantly improve the security of your cloud environment. Remember, the best defense is a good defense.

### Document - [Trust boundaries](https://www.cloudskillsboost.google/course_templates/968/documents/483899)

### Document - [Guide to firewall rules](https://www.cloudskillsboost.google/course_templates/968/documents/483900)

### Lab - [Access a firewall and create a rule](https://www.cloudskillsboost.google/course_templates/968/labs/483901)

Manage firewall rules.

* [ ] [Access a firewall and create a rule](../labs/Access-a-firewall-and-create-a-rule.md)

### Video - [Common attack vectors](https://www.cloudskillsboost.google/course_templates/968/video/483902)

* [YouTube: Common attack vectors](https://www.youtube.com/watch?v=DYxVGcuNEHs)

Think about what it’s like for someone just learning how to play a new game. There are rules or guidelines to follow and strategies to win. Just as in learning a new game, knowing how malicious actors operate allows you —as a cloud security professional— to stop them more easily. In this video, we'll discuss attack vectors and their significance for network and system security. We'll explore various attack vectors, understanding their impact on security, and discuss common examples along with effective countermeasures. Let’s first explain attack vectors. Attack vectors are pathways attackers use to penetrate security defenses. Now, let’s learn the guidelines of using attack vectors. One, gain insight into attack vectors that are vital for network and system security. Two, once you gain insight into attack vectors, pinpoint potential vulnerabilities, and effectively defend against targeted attacks. Next, we’ll discuss strategies using countermeasures to help you overcome common attack vectors that impact perimeter and boundary protections. Malicious actors may use social engineering tactics that exploit human mistakes to gain access to sensitive data or valuables. Examples of social engineering are phishing, vishing, smishing, and spear phishing attacks. These types of social engineering attacks involve the use of digital communications to trick people into revealing sensitive data or deploying malicious software. To counter this, you can implement user training and email security measures. Next, password attacks are a widespread threat that usually involve brute force techniques in which attackers use trial and error to reveal private information. To significantly improve security, strengthen your password policies and incorporate multifactor authentication. Cybercriminals might also exploit vulnerable software by taking advantage of known vulnerabilities. Remember, a vulnerability is a weakness that can be exploited by a threat. To counteract this, it's crucial to regularly patch and update software, and use vulnerability scanning tools. Then, there's malware, or software designed to harm devices or networks, which infiltrate and compromise systems. To combat malware, use advanced malware protection solutions and regularly update your software. Lastly, distributed denial of service, or DDoS attacks, are a type of denial of service attack that uses multiple devices or servers located in different locations to flood the target network with unwanted traffic. This can cause service outages and potential unauthorized access. To protect your company from these attacks, deploy DDoS mitigation services and monitor network traffic. It’s important to know the threat actors you might encounter and how they think. You’ll first need to understand guidelines of using common attack vectors. Next, you can utilize strategies to implement robust perimeter and boundary protection measures. Then, you’re in position to keep your network and systems secure.

### Quiz - [Test your knowledge: Perimeter protection](https://www.cloudskillsboost.google/course_templates/968/quizzes/483903)

### Video - [Zero trust](https://www.cloudskillsboost.google/course_templates/968/video/483904)

* [YouTube: Zero trust](https://www.youtube.com/watch?v=EMKmyUq7f8A)

User authentication —or verification— is similar to receiving a phone call and the phone displays the caller’s name and number. Verification allows organizations to help ensure that an individual accessing any asset or piece of data is who they claim to be. In this video, we'll explore the zero trust security model and how it differs from traditional models. We’ll also discuss its core principles and the network security mechanisms involved. Traditional access control models, like role-based access control, or RBAC, and discretionary access control, or DAC, were designed with the assumption that everything within the network perimeter could be trusted. But, this assumption can no longer be made because organizations have embraced remote workforces, cloud-based infrastructure, and Internet of Things, or IoT devices. In these new environments, the perimeter is no longer a clearly defined boundary, creating opportunities that can be exploited by malicious actors. In recent years, the adoption of the zero trust security model has been one of the most significant shifts, offering a more robust and adaptable approach to security in today's complex, distributed IT environments. Zero trust is based on the principle of "never trust, always verify." This means that every user, device, or system must be authenticated and authorized before accessing any resources or data. This approach assumes that any user or device, whether inside or outside the network perimeter, could be compromised. So, every access request must be validated and authorized on a case-by-case basis. By shifting the focus from perimeter security to securing individual resources, zero trust offers a granular approach to access control. Granular access control allows for more defined control over access, as permissions can be granted or restricted based on specific conditions. Let's discuss the core principles of zero trust. The first principle is verify explicitly. This means that every access request must be authenticated and authorized before access is granted to any resource. The second principle is apply least-privilege access. This means that users, devices, and systems should only be granted the minimum access necessary to perform their tasks. The third principle is assume breach. Organizations embracing zero trust should operate under the assumption that a breach has already happened or will happen, and design their security measures accordingly. Now, let's discuss how organizations can implement zero trust policies using key access control mechanisms. These mechanisms include: identity and access management, or IAM, multifactor authentication, or MFA, microsegmentation, and network access control, or NAC. We’ve already defined IAM and MFA. Now, let’s focus on microsegmentation and NAC. Microsegmentation is a security technique that divides a network into smaller, isolated segments to limit unauthorized access and reduce the potential attack surface. Note that an attack surface includes all the potential vulnerabilities that a threat actor could exploit. This approach ensures that even if an attacker gains access to one segment, they can’t easily move within the network. Network access control, or NAC, is a security solution that enforces policy-based access control to network resources, ensuring that only authorized devices and users can access the network. NAC's core function doesn't just stop at policy enforcement; it extends to identification, monitoring, and control of devices and users on the network. By employing these mechanisms, organizations can strengthen their security posture and minimize the potential attack surface. Another critical aspect of zero trust is the use of context-aware access controls. Context-aware access controls are decisions about granting or denying access to resources, based on the user's identity and contextual information. Resources can include the user's location, device, and system access patterns. By incorporating this additional data, zero trust can make more informed access control decisions, reducing the risk of unauthorized access. As a cloud security professional, it's essential to understand how cloud access security brokers, or CASBs, and secure access service edge, or SASE, platforms can assist organizations in implementing zero trust policies, or ZTP. CASBs act as intermediaries between cloud service users and cloud service providers, enabling organizations to enforce security policies and maintain visibility over cloud-based activities. By incorporating ZTP, CASBs can restrict access to sensitive data based on user identity, device, and context, ensuring that trust is not automatically granted. SASE platforms, on the other hand, combine network and security functions into a single, cloud-based service. By integrating ZTP with SASE, organizations can apply adaptive security policies based on real-time context, like user identity, device, location, and application. This approach eliminates the traditional trust boundaries and ensures that access is continuously evaluated and granted only when necessary. Both CASBs and SASE platforms play an important role helping organizations implement ZTP by providing granular access controls, continuous monitoring, and adaptive security policies based on context. To wrap up, the zero trust security model represents a shift in the approach to access control, better securing today's complex, distributed IT environments. In a future role as a cloud security professional, you’ll be well prepared to help organizations stay ahead of emerging threats, and protect their critical assets in an increasingly interconnected world.

### Document - [Zero trust policies and complementary controls](https://www.cloudskillsboost.google/course_templates/968/documents/483905)

### Quiz - [Test your knowledge: Zero trust](https://www.cloudskillsboost.google/course_templates/968/quizzes/483906)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/968/video/483907)

* [YouTube: Wrap-up](https://www.youtube.com/watch?v=0TvteohO_OM)

Congratulations, you made it. During this section, you gained essential knowledge and practical abilities to maintain an organization's assets securely within cloud environments. We dove into the intricacies of identity management and access control, gaining insights into vital concepts like single sign-on, or SSO, multifactor authentication, or MFA, and identity and access management, or IAM services. You also learned about various access control mechanisms, the significance of trust boundaries, and the foundational principles of the zero trust security model. You began this course by exploring the core principles of identity management and access control within a cloud environment, focusing on SSO, MFA, and IAM services offered by cloud providers. Second, you explored the different types of access controls employed in a cloud environment, including role-based access control, or RBAC, and attribute-based access control, or ABAC. Finally, you learned about trust boundaries, identified best practices for configuring and managing defense, and how the zero trust security model employs mechanisms like IAM, MFA, microsegmentation, network access control, and encryption to strengthen security posture. Great job on the progress you've made so far. You’re doing awesome.

### Document - [Glossary terms from module 1](https://www.cloudskillsboost.google/course_templates/968/documents/483908)

### Quiz - [Module 1 challenge](https://www.cloudskillsboost.google/course_templates/968/quizzes/483909)

## Threat and vulnerability management

In this module, you'll immerse yourself in the world of threat and vulnerability management, learning to identify threats and vulnerabilities, assess risks, and implement effective mitigation strategies. You'll discover the ins and outs of asset management, including best practices and tools for efficient resource management, and delve into the crucial role of secure configuration in cloud security, with practical applications in the cloud environment.

### Video - [Welcome to module 2](https://www.cloudskillsboost.google/course_templates/968/video/483910)

* [YouTube: Welcome to module 2](https://www.youtube.com/watch?v=XPwoWfvaEqs)

Welcome. I’m so glad you’re here. We have a lot of fascinating topics to cover. This section will cover three main topics: effective threat management, asset management in cloud security, and vulnerability remediation and posture management. The first topic you'll start by exploring is the significance of effective threat management in protecting an organization's assets, reputation, and operational efficiency. Then, you'll dive into the elements of a successful threat management strategy, including risk identification, assessment, prevention, detection, and response. Next, you'll uncover how to address challenges in executing threat management programs, like limited resources and evolving threats, by emphasizing continuous employee training, proactive risk management, and collaboration with industry experts to stay informed on trends and best practices. In the second topic, you’ll discuss best practices for asset management in cloud security and dive into the crucial aspects of managing and optimizing your organization's resources. Our focus will be on ensuring you're well-versed in the most effective strategies for tracking, maintaining, and optimizing your assets. You’ll also find out about various tools and technologies for streamlining asset management. You'll then learn valuable tips for maintaining accurate and up-to-date records of your assets, and emphasize the significance of data integrity and consistency. This knowledge is crucial for making informed decisions and minimizing the risk of expensive errors. The third and final topic covers vulnerability remediation and posture management. You'll inspect secure configuration and posture management in cloud environments, and give you the knowledge to protect your organization's data and resources from potential threats. You’ll also dive into best practices for vulnerability remediation and posture management, so that you can effectively and efficiently identify, assess, and tackle potential risks. Finally, you’ll review information about emerging trends and technologies in threat management to keep you updated on the latest advancements and innovations, so you can enhance your organization's security posture. By the end of this section, you'll have a comprehensive understanding of threat management, asset management, and vulnerability remediation and posture management in a cloud environment. This will have you well on your way to navigating the cloud security environment with ease.

### Video - [Introduction to threat management](https://www.cloudskillsboost.google/course_templates/968/video/483911)

* [YouTube: Introduction to threat management](https://www.youtube.com/watch?v=lkaCXen-ueA)

As technology continues to advance, cyber threats are becoming more sophisticated and widespread. It's no longer if an organization will be targeted, but when. That's why having an effective threat management strategy is crucial for protecting your organization's digital assets. In this video, we'll discuss the essentials of establishing a strong threat management strategy. A threat management strategy is a comprehensive plan that addresses the various types of cyber threats an organization may face. This strategy should include preventative measures, incident response procedures, and regular audits. The key components of an effective threat management strategy are prevention, detection, response, and recovery. Prevention involves implementing security controls to prevent cyber attacks from occurring in the first place. This includes firewalls, endpoint protection software, and access controls. Detection involves monitoring systems and networks for any signs of suspicious activity. This includes intrusion detection systems, log analysis, and security information and event management, or SIEM, systems. Response involves quickly identifying and containing any cyber attacks that occur. This could include incident response plans, communication protocols, and emergency response teams. And finally, recovery involves restoring systems and data to their pre-attack state. This could include backups and disaster recovery plans. As a cloud security professional, implementing an effective threat management strategy is not without its challenges. One of the biggest challenges is the constantly evolving nature of cyber threats. As soon as you think you’ve handled one threat, another emerges. Another challenge is the lack of resources and expertise in the field of threat management. A lot of organizations struggle to keep up with the latest threats and don't have the staff or resources to effectively manage them. Finally, there is the challenge of balancing security with productivity. Organizations need to ensure their threat management strategies don't hinder their employees' ability to do their jobs. So, what can organizations do to overcome these challenges and implement an effective threat management strategy? The first step is to stay informed about the latest threats and trends in cloud security. This can include training, attending conferences, and staying up to date on the latest industry news. The second step is to invest in the right tools and technologies to help you manage threats more effectively. This could include next-generation firewalls (NGFWs), intrusion detection systems, and threat intelligence platforms. Ultimately, organizations must encourage and reinforce cybersecurity awareness among their employees, especially because people represent the most susceptible element in security systems. For step three, organizations can support their employees when they create a culture of awareness through regular training and maintaining clear communication about the importance of security measures. Organizations rely heavily on IT systems to operate efficiently. But, without proper maintenance and regular audits, systems can be vulnerable to cyberattacks and downtime. Regular audits can help identify vulnerabilities in your system that may have gone unnoticed. By being proactive, you can address these vulnerabilities before malicious actors can exploit the system. Downtime can be a severe issue for any business. It can lead to lost revenue, decreased productivity, and even damage the business’ reputation. Regular audits can help increase system uptime —when the system is working— by identifying potential issues before they cause downtime —when the system isn’t working. Audits provide valuable insight into the health of your cloud resources. By understanding your audit results you can make informed decisions on where to invest resources to ensure your systems are working at peak performance. In summary, regular audits are critical to protect organizations from cyberattacks, increase system uptime, and uncover vulnerabilities that may have gone unnoticed. In this video you explored the fundamental components of a threat management strategy, uncovered how to address potential challenges, and learned the importance of regular audits for enhancing cyber defenses. By taking these insights into account, you’ll become better prepared to actively protect your organization from the rising regularity of sophisticated cyber threats.

### Document - [Threat and vulnerability management assessments](https://www.cloudskillsboost.google/course_templates/968/documents/483912)

### Video - [Eyre: Secure cloud assets](https://www.cloudskillsboost.google/course_templates/968/video/483913)

* [YouTube: Eyre: Secure cloud assets](https://www.youtube.com/watch?v=uXiv17Y6oEQ)

My name is Erye. I am the lead security engineer for Google's Threat Analysis Group. Basically, we try to protect Google's users, as well as Google itself, and we do this by having these three different missions, namely tracking financially motivated threat actors, tracking disinformation, as well as like tracking various government backed threat actors. My favorite thing about my job is that we are basically protecting the more vulnerable population of the internet. This includes like activists, dissidents, journalists, as well as like other political entities. How did I get started? Somewhere in between my junior and senior year of undergrad, I ended up joining a cybersecurity competition. I hadn't been around other people that were very much interested in security before, so going to the cyber camp, I was like, "Oh, this is awesome," and I loved like the vibe and everyone was so friendly and sharing tips and all that. I was actually enjoying myself, so I was like, "Yes, this is for me." A lot of companies have information and data in the cloud, and it's very important to protect these assets. Having knowledge of how to secure these assets in the cloud is very important as a skill, especially when you're trying to work in cybersecurity as everyone is moving all of their their data into the cloud. If you're just starting out in your cybersecurity career, my advice would be to go seek out mentors. Aside from trying to learn everything you can, trying to watch videos, trying to take time to read books, I would also reach out to like a few people that you admire. A lot of the people in cybersecurity are very helpful. You can go to like a lot of conferences or you can join various different local communities, as well as, like, basically just going to, like, your local cybersecurity meetups. There is a lot of people that have helped me out and pointed me to like the correct resources in order for me to further my skills as well as further my career. Working in cybersecurity, I think, is very much like an adventure. Sometimes you don't know what you're going to get, right? Sometimes it's going to be awesome, and it's going to be amazing. Sometimes, like, it's a very difficult adventure, but at the end of the day, you actually end up learning something.

### Quiz - [Test your knowledge: Importance of threat and vulnerability management](https://www.cloudskillsboost.google/course_templates/968/quizzes/483914)

### Video - [Asset and resource management](https://www.cloudskillsboost.google/course_templates/968/video/483915)

* [YouTube: Asset and resource management](https://www.youtube.com/watch?v=FyI0AS5TnTo)

Organizing a closet is one of those chores we might put off. And the longer you wait, and the more you pile on to the existing pile, the more work you have to do later. When you access a well-organized closet, it’ll take you less time to get what you need, and so you’ll be more efficient. Maybe you’ll even save some money because you won’t have to buy something you couldn't find in a messy closet. Proper asset and resource management organization —similar to a closet— saves time, increases efficiency, and can help cut costs. In this video, you'll learn more about the role of asset management in maintaining a secure and well-organized digital environment. We’ll cover best practices for managing your assets and tools and technologies that can help streamline these processes. You'll also learn tips for maintaining accurate and up-to-date records to ensure a smooth and efficient asset management process. Ready? Let’s get started. You may be wondering, what is asset management? Asset management is the process of tracking assets and the risks that affect them. Effective asset management is key for maintaining a secure environment because it helps organizations identify and track critical assets, detect and respond to security incidents more effectively, and mitigate risks associated with unauthorized access, data breaches, and other cyber threats. In the context of cybersecurity, asset management involves identifying all devices, software, and data within the organization; classifying assets based on their sensitivity and criticality; tracking assets throughout their lifecycle, from procurement to disposal; and ensuring proper security controls are in place for each asset. Let’s examine some best practices for asset management. First, it’s important to develop an asset inventory and create a comprehensive, centralized inventory of all digital assets, including devices, software, and data. This inventory should include information like asset type, location, owner, and any associated security controls. Next, categorize assets based on risk. Assess the risk associated with each asset and categorize them accordingly. This will help prioritize security efforts and allocate resources more effectively. It’s important to establish ownership and accountability. Assign clear ownership and accountability for each asset to ensure that they are properly managed and protected. Then, you can make sure to implement access controls. Establish access controls to limit who can access specific assets and ensure that only authorized personnel have access to sensitive data. Finally, monitor and audit assets regularly to identify any potential security risks, unauthorized access, or changes to the asset. There are a variety of tools and technologies that can help streamline the asset management process. The first tool we'll discuss is IT asset management, or ITAM software, a centralized application designed to manage and track the lifecycle of your organization's IT assets. ITAM software offers a comprehensive inventory of software and hardware within a business environment, including details like purchase dates, prices, license information, and serial numbers. ITAM software also streamlines the process of tracking, monitoring, and managing your IT assets. Next is the configuration management database, or CMDB, a repository designed to store data about your entire IT environment, including its relationships between hardware and software assets. This provides a holistic view of your IT infrastructure, allowing for better understanding and management. Then, there are network scanners and discovery tools, which can help identify and catalog all devices connected to your network, providing visibility into your organization's assets. Last, but not least, are vulnerability scanners. Vulnerability scanners can help identify potential security risks and vulnerabilities in your organization's assets, allowing you to prioritize remediation efforts. To ensure a smooth and efficient asset management process, it's essential to maintain accurate and up-to-date records. Here are four useful tips to help you achieve this. One, establish a centralized asset repository. Create a single, centralized location for storing all asset-related information to make it easier to manage and access. Two, automate asset discovery and updates. Utilize tools and technologies that can automatically discover and update asset information, reducing the risk of human error and ensuring that your records are always current. Three, implement a regular review process. Schedule regular reviews and audits of your asset records to identify and correct any discrepancies or outdated information. Finally, train your staff. Provide training and resources to your staff on the importance of asset management and clearly define their role in maintaining accurate records. Effective asset management is an important aspect in a robust cybersecurity strategy. By following best practices, utilizing the right tools and technologies, and maintaining accurate and up-to-date records, you can ensure that your organization's assets are well-protected and secure.

### Video - [Steps for asset management](https://www.cloudskillsboost.google/course_templates/968/video/483916)

* [YouTube: Steps for asset management](https://www.youtube.com/watch?v=FzexrtzgctI)

In this video, we'll discuss how a comprehensive asset management system provides transparency and continuous monitoring for cloud resource usage. You'll also learn the steps for effective asset management, including identifying assets, categorizing them based on risk levels, and maintaining accurate digital records. It’s important to call out our discussion in this course should not be considered legal advice. As a cloud security professional, the first step in effective asset management is to identify all the assets within your cloud environment. When the cloud security team identifies assets, it helps them understand the scope of their cloud environment and prioritize resources based on their criticality. Assets include servers, like virtual machines, which run your applications and services within your cloud environment; databases, which hold your organization's data and are important components of your cloud infrastructure; applications, including various software programs and tools, web applications, mobile apps, and application programming interfaces, or APIs; other resources, like load balancers, storage buckets, and network components, are also part of your cloud environment. Once you identify assets, you’ll continue to the second step in effective asset management: risk tiering. Risk tiering is a process that enables organizations to identify and categorize their assets based on their importance and potential impact. There are three tiers. You’ll devote a different amount of time, energy, and resources to each tier. For high-risk assets, you’ll provide the most attention and protection to assets like sensitive data and mission-critical applications. For medium-risk assets, you’ll provide substantial attention and protection to assets like internal documentation and finances. For low-risk assets, you’ll provide a small amount of attention and protection to assets like public information and non-copyrighted materials. Risk tiering helps organizations allocate resources effectively and focus on the most critical assets. Let’s examine an example of risk tiering in a healthcare organization. High-risk assets include electronic health records, or EHR, which contain sensitive patient information, proprietary research data, and mission-critical applications like hospital management systems. These assets require the highest level of protection, like access control and regular security audits. Medium-risk assets include internal communication systems, employee records, and billing information. While not as critical as high-risk assets, medium-risk assets still require a solid level of protection to prevent unauthorized access and data breaches. Low-risk assets include general information about the organization, like marketing materials, public-facing websites, and non-sensitive internal documents. Even though low-risk assets need protection, they can be allocated fewer resources and are a lower priority compared to high and medium-risk assets. By risk tiering their assets, the healthcare organization can allocate resources more effectively. The organization can then focus on protecting the most critical assets that have the highest potential impact on the organization should the assets be compromised. After you perform risk tiering, it’s time for the third step in effective asset management: maintain effective oversight. As a cloud security professional, it's a must to have an accurate and up-to-date inventory of all your cloud assets. This inventory should include information like asset type, location, ownership, and risk level. Regularly updating your inventory helps ensure you have a clear understanding of your cloud environment at all times. Implementing a comprehensive asset management system involves monitoring and tracking changes to your assets. This includes monitoring asset configurations, access controls, and vulnerabilities. By continuously monitoring your assets, you can quickly detect and respond to potential security issues. The process you follow to monitor and track assets is called asset lifecycle management, which helps organizations optimize their cloud resource usage and reduce potential security risks. Asset lifecycle management encompasses planning –which entails defining procurement requirements and handling capital expenditure– and addressing all aspects of the asset lifecycle. From acquisition and deployment to maintenance and disposal, planning ensures a seamless process. Let’s check out an example of lifecycle management. A software company recently migrated its applications and services to a cloud environment. To ensure effective asset management, the company follows these steps. Acquisition: the company selects the appropriate cloud resources, like virtual machines, storage, and databases, based on their specific requirements. Deployment: the company deploys these resources in a well-organized way to ensure seamless integration with their existing infrastructure. Maintenance: the company regularly monitors the performance of its cloud resources and updates assets as needed to maintain optimal performance. They also ensure that security patches and updates are applied to reduce any potential vulnerabilities. Disposal: when a cloud resource is no longer needed, the company securely disposes of it by following the proper decommissioning process. This includes deleting any sensitive data and ensuring that the resource is no longer accessible, reducing the risk of unauthorized access. By following these steps, the software company can effectively manage the lifecycle of its cloud assets, optimizing resource usage and minimizing security risks. In this video, you learned that implementing a comprehensive asset management system is vital for maintaining transparency and oversight over your cloud resource usage. By identifying and risk tiering assets, maintaining an accurate inventory, monitoring changes, and managing the asset lifecycle, you can ensure the security and efficiency of your cloud environment. The more you can effectively establish the security of a cloud environment, the more confidence you —and your organization— will have in the security of your assets.

### Document - [On-the-job asset management applications](https://www.cloudskillsboost.google/course_templates/968/documents/483917)

### Quiz - [Test your knowledge: Asset management](https://www.cloudskillsboost.google/course_templates/968/quizzes/483918)

### Video - [Vulnerability remediation and posture management](https://www.cloudskillsboost.google/course_templates/968/video/483919)

* [YouTube: Vulnerability remediation and posture management](https://www.youtube.com/watch?v=amrxRGZQiOM)

As a cloud security professional, sometimes you can correct or even mitigate problems when you take a step back and try to view the whole picture, and identify what the end result is supposed to be. In this video, we'll explore vulnerability remediation and posture management in a cloud environment, review secure configuration, and discuss the importance of these concepts in maintaining a secure cloud environment. Let's first examine vulnerability remediation. Vulnerability remediation is the process of identifying, assessing, and resolving security vulnerabilities in your cloud environment. Vulnerability remediation is a process that typically involves six steps. One: identify vulnerabilities through automated scans, manual testing, or third-party reports. Two: assess the severity and impact of these vulnerabilities on infrastructure and operations. Three: prioritize vulnerabilities based on their severity, potential impact, and ease of exploitation. Four: implement patches, updates, or configuration changes to address these vulnerabilities. Five: verify the remediation efforts to ensure that you’ve effectively resolved the vulnerabilities. And six: document the details of the vulnerabilities, your remediation efforts, and any lessons learned for future reference. This comprehensive approach helps ensure a secure and resilient cloud environment for your organization. Now we’ll explore posture management, or the continuous process of monitoring, assessing, and maintaining the security stance of an organization's cloud resources. During this process, your cloud security team ensures that the cloud infrastructure is configured and operated securely, adhering to best practices and compliance requirements. Posture management typically follows six steps. One: monitor the cloud environment. In this step, you’ll detect potential security threats, misconfigurations, and compliance violations. Two: evaluate the security posture. In this step, you’ll assess the robustness of the security posture by comparing the current state to established security standards and best practices. Three: report. In this step, you’ll create regular reports and dashboards that highlight areas of concern and improvement. Four: remediate. In this step, you’ll address security issues through vulnerability remediation, configuration changes, and other corrective actions to ensure a secure environment. Five: implement and enforce security policies. In this step, you’ll ensure that cloud resources are consistently configured and operated securely. Six: regularly review and update security policies, procedures, and controls. In this final step, you’ll keep pace with evolving threats, technologies, and business requirements, fostering continuous improvement in cloud security. Now, let's review the definition of secure configuration. Secure configuration refers to the practice of setting up your cloud resources with the proper security settings and configurations to minimize potential risks. This includes following best practices, like limiting access to sensitive data and regularly updating software and applications. So, why are secure configuration and posture management important in a cloud environment? Well, they help prevent unauthorized access, maintain data integrity, and protect your cloud resources from potential threats. In conclusion, vulnerability remediation and posture management are crucial aspects of maintaining a secure cloud environment. By adopting secure configuration practices, you can effectively prioritize remediation efforts and ensure the safety of your data and applications. Once you know where your organization might be vulnerable, you can take additional steps to properly secure those assets.

### Video - [Vulnerability remediation and posture management in software development](https://www.cloudskillsboost.google/course_templates/968/video/483920)

* [YouTube: Vulnerability remediation and posture management in software development](https://www.youtube.com/watch?v=HQ20S72_kWI)

Identifying weaknesses is just as important as developing security infrastructure. In this video, we'll discuss vulnerability remediation and posture management, as they relate to software development. As a cloud security expert, it's important for you to understand and apply these concepts to ensure the safety of your software and systems. Let’s start off by reviewing vulnerability remediation and posture management. Recall that vulnerability remediation is the process of identifying, assessing, and resolving security vulnerabilities in your cloud environment. This can include system patching, package rebuilding, or other external mitigations. Now let’s return to posture management, the continuous process of monitoring, assessing, and maintaining the security stance of an organization's cloud resources. This process helps organizations stay ahead of potential threats and minimize risks. In the world of software development, it’s important to ensure the security and integrity of applications. One key aspect of this process is vulnerability remediation and management, which helps prevent security breaches, and protects data while maintaining the integrity of applications. Imagine a software development team is working on a web application that handles sensitive user data, like personal information and financial transactions. During the development process, the team discovers a vulnerability in the application's authentication system, which could potentially allow unauthorized users to gain access to sensitive data. The software development team will complete the following steps to address vulnerabilities in an application. First, they need to identify the vulnerability. To do this, the team must understand the vulnerability and how potential attackers could exploit it. Tasks may involve reviewing code, consulting documentation, and researching known security issues related to the active technologies the organization uses. The team can conduct an impact analysis on vulnerabilities to complete these tasks. This analysis will help the team address potential consequences and set how they prioritize the urgency of issues. After that, the team needs to develop a remediation plan. Once the vulnerability has been identified, the team needs to develop a plan to address the issue. This may involve updating software libraries, modifying code, or implementing new security measures. Next, they need to implement the remediation. The team must implement the changes outlined in their remediation plan to ensure that the vulnerability is effectively addressed, and that the application is no longer susceptible to the identified security risk. Then the team needs to test the solution. After implementing the remediation, the team should thoroughly test the application to ensure that they’ve resolved the vulnerability, and that they didn’t introduce any new issues as a result of the changes. And finally, they need to monitor and maintain. The team should continuously monitor the application for any new vulnerabilities, and be prepared to address them as they arise, to ensure the ongoing security of the application. By effectively addressing vulnerabilities through remediation, the software development team can prevent security breaches, protect user data, and maintain the integrity of their application. This will also ensure they provide a safer and more reliable experience for their users. There are several types of remediations. Let’s explore four of them. Hotfixes are quick, temporary fixes for critical vulnerabilities that require immediate attention. Hotfixes are unscheduled because of urgent and time-sensitive needs. Also, hotfixes are usually added into patches at a later stage, which helps to make the system stronger over time. Security patches are updates that address specific security vulnerabilities. Unlike hotfixes, they’re usually scheduled and systematically planned. A key feature of security patches is their ability to simultaneously fix multiple security issues, enhancing the overall security of the system. Updating dependencies involves updating or replacing third-party libraries and frameworks to more secure versions. Feature patches are updates that not only fix vulnerabilities, but also introduce new features or improvements. Follow these five best practices for vulnerability management and a strong security posture. First, implement a vulnerability management program that includes regular scanning, assessment, and remediation processes. Second, prioritize vulnerabilities based on severity, exploitability, and potential impact. Third, quickly apply patches and updates to minimize exposure to threats. Fourth, monitor and analyze logs and alerts for signs of potential security incidents. And fifth, continuously improve your security posture through training, awareness, and process improvements. Now that you have a better understanding of vulnerability remediation and posture management, you can begin applying these concepts to cloud security-related scenarios.

### Document - [Posture management tools and techniques](https://www.cloudskillsboost.google/course_templates/968/documents/483921)

### Video - [IT automation tools for posture management](https://www.cloudskillsboost.google/course_templates/968/video/483922)

* [YouTube: IT automation tools for posture management](https://www.youtube.com/watch?v=kEzsJ5bHrRo)

Automation is becoming increasingly common in technology. It can help improve the efficiency of systems and strategies. In this video, we'll explore automation tools for posture management and vulnerability remediation. We'll also discuss best practices for validating security remediations to ensure compatibility and minimize potential disruptions. The features and benefits of IT automation tools include flexibility, efficiency, and consistency. These tools are flexible because they support various platforms and environments. These tools are efficient because they allow for faster deployment of updates and patches. And these tools ensure consistent configurations across systems. IT automation tools streamline the process of resource provisioning and configuration management, and improve the detection and remediation of vulnerabilities. These tools play an important role in how you, as a cloud security professional, can manage complex IT infrastructure and ensure security compliance. Let's compare two popular IT automation tools: Ansible, a fast and simple configuration management tool; and Puppet, a more powerful but complex configuration engine. First up are the key features of Ansible. Ansible is an open-source IT automation tool that helps organizations manage their infrastructure and applications more efficiently. Ansible has agentless architecture and does not require any additional software to be installed on the managed nodes. It uses yet another markup language, or YAML, for defining automation tasks. YAML makes it easier to create and maintain automation scripts. Ansible’s modular design allows for the creation and sharing of reusable components, which can be used to automate tasks across multiple systems. With Ansible, you can also integrate with security tools, like vulnerability scanners and intrusion detection systems, to automate security tasks and enhance overall security posture. Next, here are the key features of Puppet. Puppet is another popular IT automation tool that helps organizations manage their infrastructure and ensure security compliance. Puppet uses a declarative language to specify the desired infrastructure configuration. This approach allows it to manage the implementation details itself. Puppet is designed for scalability. Puppet is suitable for organizations of all sizes because you can match the software to meet the needs of your organization, from small businesses to large enterprises. Puppet also provides comprehensive reporting and auditing capabilities, enabling organizations to track changes, demonstrate compliance, and identify potential security issues. And, created with extensibility in mind, Puppet includes a wide range of modules and plugins. This enables organizations to extend its functionality and tailor it to meet their needs. Along with IT automation tools, businesses also need to have robust update and patch management tools in place. Update and patch management tools are essential for businesses to mitigate security risks. They ensure timely installation of updates and patches and address vulnerabilities, decreasing the possibility of cyber attacks. Organizations are recommended to have proper patch management processes in place to ensure the security of their systems and data. These tools help maintain compliance with various industry regulations and standards. Also, regular updates can improve system performance and stability by fixing errors and optimizing code. Finally, the automation of update and patch deployment can reduce the manual effort required from IT teams, allowing them to focus on other critical tasks. Before deploying security remediations, it’s important to validate them to ensure compatibility and minimize potential disruptions. Some best practices for validating security remediations include: canary testing, rolling deployments, and blue-green testing. Let’s quickly review each best practice. Canary testing involves deploying a security update to a small percentage of the environment first, monitoring its impact, and then rolling it out to the rest of the environment if no issues are detected. This approach helps minimize the risk of widespread disruptions caused by a problematic update. Rolling deployments involve gradually deploying a security update across the environment in stages. This approach allows you to monitor the impact of the update on each stage, and address any issues before moving on to the next stage. Blue-green testing involves creating two identical environments, blue and green, and deploying a security update to one of them while the other remains unchanged. Once the update is tested and verified in the updated environment, traffic is gradually shifted from the unchanged environment to the updated one. This approach helps minimize downtime and allows for easy rollback, when appropriate. The key to helping an organization maintain a secure and well-managed cloud environment is three-fold. First, use IT automation tools like Ansible and Puppet. Second, implement robust update and patch management solutions. And third, follow best practices for validating security remediations. The more you know about these tools and concepts, the more prepared you’ll be to navigate modern cloud security environments.

### Document - [IT automation tools for security configuration management](https://www.cloudskillsboost.google/course_templates/968/documents/483923)

### Document - [Guide to web application security scanning](https://www.cloudskillsboost.google/course_templates/968/documents/483924)

### Lab - [Identify vulnerabilities and remediation techniques](https://www.cloudskillsboost.google/course_templates/968/labs/483925)

Use vulnerability scans to identify security issues and remediate them.

* [ ] [Identify vulnerabilities and remediation techniques](../labs/Identify-vulnerabilities-and-remediation-techniques.md)

### Quiz - [Test your knowledge: Vulnerability remediation and posture management](https://www.cloudskillsboost.google/course_templates/968/quizzes/483926)

### Video - [Patching and rehydration](https://www.cloudskillsboost.google/course_templates/968/video/483927)

* [YouTube: Patching and rehydration](https://www.youtube.com/watch?v=mkBgyC_PtrY)

Consider the role of a vehicle mechanic responsible for maintaining and enhancing the performance of cars. Just as a car may seem to be operating smoothly, it’s vital to conduct regular inspections and make necessary improvements to keep it in top condition. Systems, much like cars, also require periodic checkups and updates to maximize their effectiveness. In this video, we'll explore two key system maintenance methods: patching and rehydration. Let’s get started. First up is patching. Patching is the process of installing updates to software to address vulnerabilities, improve stability, or add new features. In the context of system maintenance, patching plays an important role in keeping systems and applications secure and up to date reducing the risk of cyberattacks. As a cloud security professional, you’ll follow these mechanics of patching steps. Step 1: develop a cloud-based inventory and baseline. The first step is creating a baseline of all the resources in your environment. This baseline is a reference point for tracking any changes. Be sure to maintain and update this list regularly. Step 2: identify vulnerabilities and available patches for the system. Next, identify possible vulnerabilities on resources in your environment. Tools like vulnerability scanners can help you in this process. After identifying these vulnerabilities, find patches that are compatible with the system. Once you have appropriately identified the vulnerabilities and their corresponding patches, it's crucial to examine how these patches will affect the system. Step 3: assess the patch's impact on the system. Patches may affect the performance of the system. A system evaluation should consider effects on stability, software compatibility, and performance of the patch. This evaluation should also consider the risks associated with leaving the vulnerability unpatched. Step 4: download and test the patch in a non-production environment. Download the patch from a trustworthy source. Before deploying it in the production environment, test the patch in a non-production environment that closely resembles the existing system. This ensures the patch operates as intended without causing any additional problems. But we’re not done yet. Step 5: schedule the downtime or plan a service restart. Installing patches may require system downtime. To minimize the impact on operations, schedule this during off-peak hours. Also, make sure to communicate the estimated downtime to stakeholders. Step 6: back up data and system configurations. Next, back up all the essential data and system configuration files. Doing so will enable you to restore the system to its previous state if the patch causes unexpected issues. With all important data and system configurations securely backed up, you can start to apply the patches. Step 7: apply the patch to the system. Now you can apply the patch to the system. This step includes the actual installation of the updates. Step 8: verify the patch's effectiveness and stability. The final step of the patching process is to confirm whether the patch has successfully fixed the intended vulnerabilities and monitor system performance to make sure no new issues have emerged. Now let’s discuss rehydration. Rehydration is a cloud-native process where new servers are created with the latest updates and patches, allowing for the workload to be transferred from old servers, and for the outdated servers to be decommissioned or destroyed. Unlike conventional patching, rehydration emphasizes continual replacement with up-to-date instances. Let’s review each rehydration step in detail. Step 1: inventory of all cloud-based resources. In the first step of rehydration, establish a system baseline that includes all of your cloud resources as a point of reference. This will assist you in tracking system or configuration changes. Step 2: identify vulnerabilities and available upgrades. The second step is to identify any vulnerabilities in your resources from the baseline. This includes known bugs, outdated software, or poorly configured settings. Step 3: run updated and old instances to compare. Once you’ve created new resources with updates, run them alongside the existing ones. This helps identify and fix issues or compatibility problems before they replace the production resources. Step 4: slowly redirect traffic from old to new instances. Next, slowly redirect traffic from the old resources to the new, patched resources to minimize downtime. Doing this gradually ensures that the new resources can handle the workload, and that no data is lost in the transition. Step 5: decommission or destroy old instances. Finally, after all the traffic has been successfully redirected to the new resources, decommission or destroy the old instances. This final step ensures that only updated systems are in the production environment. In this video, we explored the concepts of patching and rehydration system maintenance methods. You gained a better understanding of the mechanics, benefits, and limitations of both methods. Remember, choosing the right approach depends on your system architecture, environment, and organization's requirements. Be the vehicle mechanic your security system needs. Stay up to date with the latest security updates and regularly refine your process to ensure your systems remain secure and perform optimally.

### Video - [Compare and contrast: Patching and rehydration](https://www.cloudskillsboost.google/course_templates/968/video/483928)

* [YouTube: Compare and contrast: Patching and rehydration](https://www.youtube.com/watch?v=tvs0TJHHPiA)

Hi there. Let's discuss the differences between the patching and rehydration maintenance methods. We'll also cover when to use each approach, and the advantages and disadvantages of both. Let's dive in. Patching is a common method of updating software or systems to fix vulnerabilities, errors, or improve performance. It involves applying a small piece of code, called a patch, to an existing software or system. Rehydration is a more recent approach to updating systems and applications in the cloud. Rehydration involves creating new servers with the latest updates patches, and then decommissioning or destroying the outdated servers. This method is different from patching, which introduces minor updates to the current system without needing a complete replacement. Now, let's discuss when you, as a cloud security professional, might use patching, and when you might use rehydration. Patching is an effective method for making targeted updates, particularly necessary for old systems or monolithic applications. It is especially useful when downtime is unavoidable, ensuring system performance and integrity. Cloud-native rehydration is a process that minimizes downtime and ensures consistency for serverless and containerized applications. It allows easy system updates and is great for maintaining uniform configurations and security settings, making it efficient for large-scale changes. Both methods help keep operations running smoothly, but they have their own unique advantages and disadvantages. Let’s start with patching. There are several advantages of patching. Patching includes targeted updates to specific components and doesn’t require the entire system to be rebuilt or reconfigured. Also, patching has compatibility with legacy systems. But, there are also some disadvantages. Patching can lead to increased complexity and potential for human error. It can also have incomplete updates, leaving systems vulnerable. Cumulative patches can also lead to performance issues. Now let’s shift our focus to rehydration. The main advantages of rehydration are speed, consistency, immutable infrastructure, and simplified management, reduced downtime, and reduced human error. Let’s discuss these differences in more detail. First is speed. Rehydration is faster, as it replaces components rather than updating existing systems. Second is consistency. This method ensures that all instances of a cloud resource run the same software version, eliminating issues caused by patching. Third is immutable infrastructure. This approach promotes unmodified components after deployment, reducing risks. Fourth is simplified management. Rehydration can be automated for easy cloud resource management and large-scale updates. Fifth is reduced downtime. Rehydration minimizes downtime as it creates new instances with the latest updates. The last is reduced human error. The automation aspect of rehydration decreases the potential for human error. We’ve listed the advantages of rehydration. Now let's review some of its disadvantages. The integration of rehydration with legacy systems, managed by the organization or underlying system, may present a compatibility issue. On the other hand, the organization's IT professional or team bears the responsibility of mitigating potential data loss risks by ensuring proper data management practices are in place. Next, what’s a good example of how to apply rehydration? Use rehydration with systems running on Google's Kubernetes Engine, or GKE. This is a good example of taking advantage of immutability's benefits for containerized applications. By creating new containers with updated dependencies, a stable environment is established. This process minimizes downtime, enabling seamless traffic switching and easy rollbacks. Maintaining an immutable infrastructure using disposable components enhances reliability, ensuring Kubernetes application stability with smooth management. Patching and cloud-native rehydration are two distinct approaches to updating systems and applications. Understanding the advantages and disadvantages of each will help you choose the right approach for your specific use case, ensuring optimal system performance and security.

### Document - [Rehydration keeps systems up-to-date](https://www.cloudskillsboost.google/course_templates/968/documents/483929)

### Quiz - [Test your knowledge: Patching and rehydration for system maintenance](https://www.cloudskillsboost.google/course_templates/968/quizzes/483930)

### Video - [Trends in vulnerability and threat management](https://www.cloudskillsboost.google/course_templates/968/video/483931)

* [YouTube: Trends in vulnerability and threat management](https://www.youtube.com/watch?v=sRWYlz9dcv4)

We can’t predict the future, but we can prepare for it. In this video, we'll discuss current and future trends in vulnerability and threat management. We'll also explore emerging trends and technologies like machine learning, artificial intelligence, and the Internet of Things. We’ll also discuss cloud security posture management and cloud attack surface management solutions. Let’s dive in. Machine learning, or ML, and artificial intelligence, or AI, have had a significant impact on cloud security. These technologies have revolutionized the cybersecurity landscape by enabling the analysis of massive data sets, pattern recognition, and proactive threat prediction and prevention. ML, a subset of AI, uses algorithms to learn from data, allowing computers to make decisions and predictions without explicit programming. In cloud security, ML can analyze vast amounts of data, detect anomalies, and automate threat detection and response. AI, a broader concept encompassing ML and other technologies, creates systems capable of learning, reasoning, and problem-solving. In cloud security, AI enhances ML capabilities, improves threat intelligence, and enables the development of sophisticated security tools. The synergy of ML and AI actively drives significant advancements in cloud security, including real-time threat detection, advanced analytics, and automation of routine tasks. These advancements enable continuous adaptation to emerging threats, ensuring secure and well-protected cloud environments. Next, let's discuss the Internet of Things, or IoT. IoT refers to the interconnection of everyday objects and devices that enables them to collect, exchange, and analyze data through the internet. IoT devices like smart home systems, wearable devices, and industrial sensors rely on cloud-based computing infrastructure for data storage, processing, and analysis. This seamless integration allows for real-time monitoring, automation, and remote control of various processes, ultimately improving user efficiency and convenience. The rapid growth of IoT devices brings exciting possibilities, but it also presents new cybersecurity challenges. To safeguard user privacy and ensure data integrity, it's crucial to secure both the cloud infrastructure and the connected devices in this ever-evolving digital landscape. Lastly, let's discuss cloud security posture management, or CSPM, and cloud attack surface management, or CASM, solutions. These tools help organizations maintain visibility and control over their cloud environments, ensuring adherence to security best practices and threat identification. CSPM tools continuously monitor and assess cloud environments, and identify misconfigurations, vulnerabilities, and compliance violations. Key features of a cloud security posture management (CSPM) solution include automated assessment, compliance monitoring, policy enforcement, and real-time alerting and reporting. CASM solutions focus on identifying, monitoring, and minimizing an organization's cloud attack surface. Key features of a cloud attack surface management (CASM) solution include asset discovery, vulnerability assessment, threat intelligence integration, and risk prioritization. Integrating CSPM and CASM solutions into an organization's cloud security strategy ensures a comprehensive approach to their security posture and proactive threat management. Some of the most significant trends in cloud security include the following: zero trust architecture, container security, edge computing security, security as code, automated incident response. Each of these trends presents unique challenges and opportunities for organizations. For example, AI and ML can help automate threat detection and response, but may also introduce new attack vectors. Similarly, zero trust architecture can enhance access controls and monitoring, but it requires a significant shift in cloud security and infrastructure management. As a cloud data professional, you’ll evaluate technologies and approaches to protect your organization’s digital assets and mitigate security challenges. Here are five key steps to help solve cloud security challenges. Step one: assess the effectiveness of various technologies. Assess the performance and capabilities of security technologies and methods, and choose solutions with a proven track record of success in managing similar challenges. Step two: determine suitability for unique cloud environments. Each organization needs to determine if they need a unique cloud environment, potentially requiring customized security solutions. Assess whether a specific technology aligns with your organization's unique needs. Step three: consider scalability for growth and changing needs. As your organization expands, so will your cloud security requirements. Ensure that the technologies you select can scale to support future growth, and adapt to emerging challenges. Step four: evaluate compatibility with existing systems and processes. When choosing a security solution, evaluate its compatibility with your organization's current systems and processes. A solution that integrates with your existing infrastructure will be easier to implement and manage. And step five: examine ease of integration for smooth implementation. Examine how easy —or complex— it will be to integrate selected technologies into your organization's cloud environment. A straightforward solution will reduce disruption to your operations and expedite your organization’s ability to benefit from cloud security. The future of cloud security will be shaped by emerging trends and technologies. By staying up-to-date on advancements and implementing the appropriate security measures, organizations can protect their cloud environments and ensure the safety of their data. Now that’s what I call future-proof.

### Document - [Trends in security: Artificial Intelligence (AI), machine learning (ML), and Internet of things (IoT)](https://www.cloudskillsboost.google/course_templates/968/documents/483932)

### Video - [Reports and assessments for threat and vulnerability management](https://www.cloudskillsboost.google/course_templates/968/video/483933)

* [YouTube: Reports and assessments for threat and vulnerability management](https://www.youtube.com/watch?v=bJT44glUPtc)

In this video, we’ll cover several important threat and vulnerability topics: the significance of regular reporting and assessments, analyzing potential business risks, evaluating the role of reporting and assessments, and some emerging trends and technologies in cloud security. Let’s get started. Regular reporting and assessments are important to maintaining a strong security posture. They help organizations identify vulnerabilities, detect security incidents, and monitor the effectiveness of security controls. By conducting these evaluations, organizations can proactively address potential threats, and reduce the likelihood of cyberattacks. Also, regular assessments help organizations stay compliant with industry regulations and demonstrate their commitment to security best practices. For example, imagine the ABC Company, a financial services provider, understands the importance of a strong security posture. To achieve an improved security posture, they’ve implemented a comprehensive security assessment and reporting strategy. First, the ABC Company conducts regular vulnerability scans of their network infrastructure, web applications, and critical systems to identify potential weaknesses. Second, to ensure timely incident detection and response, the ABC Company utilizes advanced security monitoring tools, and has a trained incident response team ready to quickly assess and mitigate threats. Third, periodic internal and external security control audits are conducted to assess the effectiveness of security controls and ensure compliance with industry regulations. Fourth, the ABC Company also provides ongoing employee security training to keep employees aware of the latest threats and best practices. Finally, the security team regularly generates monthly and quarterly security reports, sharing the results with senior management to drive continuous improvement. By implementing this security assessment and reporting strategy, the ABC Company proactively addresses potential threats, maintains compliance, and fosters trust among users and partners. By following this approach, the ABC Company has successfully implemented an effective program. An effective program involves the following steps. First, identify assets, like devices, applications, and data. Then, assess vulnerabilities and potential threats to these assets. Next, prioritize remediation efforts based on risk and impact. Then, implement fixes, patches, or other security measures. And finally, monitor progress and continuously adjust strategies as needed. By following these steps, organizations can effectively manage and mitigate risks associated with cyber threats. Failing to conduct regular reporting and assessments can lead to severe business risks. These include financial losses because of data breaches, reputational damage as clients lose trust, and legal penalties for noncompliance with industry regulations. Organizations that don’t conduct ongoing security evaluations leave themselves vulnerable to cyber attacks and potential breaches. This can result in long-term consequences and hinder a company's growth and success. Emerging trends and technologies in cloud security offer a lot of advantages. Some benefits of these advancements include stronger data encryption, more efficient threat detection, streamlined security management, automation, increased visibility, and improved detection and response capabilities. But there are also potential disadvantages to consider, like increased complexity, and reliance on third-party vendors. These trends usually require continuous learning and adaptation to maintain a secure cloud environment, and stay ahead of evolving threats. When organizations adopt new technologies, they must ensure that their security teams have the knowledge and expertise to manage these tools effectively. So, it's essential to carefully evaluate these factors when adopting new security solutions. Balancing the advantages and disadvantages of emerging trends and technologies can help organizations make informed decisions, and optimize their cloud security strategy. In this video, we explored the importance of regular reporting and assessments, and the potential business risks of not conducting them. We also discussed the role they play in threat and vulnerability management, and the advantages and disadvantages of emerging trends and technologies. By understanding and implementing these concepts, you can strengthen your organization's cloud security posture and protect against cyber threats.

### Quiz - [Test your knowledge: Trends in threat management](https://www.cloudskillsboost.google/course_templates/968/quizzes/483934)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/968/video/483935)

* [YouTube: Wrap-up](https://www.youtube.com/watch?v=90ceFxJvMYU)

Great to see you. Let’s recap what you’ve been learning. So far, you’ve gained an understanding of threat management, asset management, vulnerability remediation, and posture management in a cloud environment. We started by exploring the importance of threat management in protecting an organization's assets, reputation, and operational efficiency. Then, you learned the components of an effective threat management strategy, including risk identification, assessment, prevention, detection, and response. We also explored challenges faced by organizations when implementing threat management programs, and gave you some helpful tips for overcoming these challenges. Next, you learned about the aspects of maintaining and optimizing your organization's resources, and best practices for asset management. You were also introduced to tools and technologies for asset management, and learned some tips for maintaining accurate records of your assets. Then you learned about the aspects of maintaining a secure infrastructure in today's digital landscape. We also explored the importance of secure configuration and posture management in a cloud environment, and discussed best practices for vulnerability remediation and posture management. Finally, we examined emerging trends and technologies in threat management to keep you informed about the latest advancements and innovations. Great job on the progress you've made so far. Keep the momentum going.

### Document - [Glossary terms from module 2](https://www.cloudskillsboost.google/course_templates/968/documents/483936)

### Quiz - [Module 2 challenge](https://www.cloudskillsboost.google/course_templates/968/quizzes/483937)

## Cloud Native Principles of Ephemerality and Immutability

In this module, you'll explore cloud-native principles of ephemerality and immutability, focusing on proper credential handling that aligns with these concepts. You'll discover the automation of infrastructure provisioning and its impact on security, including key areas such as containers, orchestration, and Infrastructure-as-Code (IaC).

### Video - [Welcome to module 3](https://www.cloudskillsboost.google/course_templates/968/video/483938)

* [YouTube: Welcome to module 3](https://www.youtube.com/watch?v=a2sQqWOfnFg)

Hi there. I can’t wait to join you on your journey through cloud-native concepts. We’ll start with an overview of cloud-native design and architecture, including designing for automation. You’ll also learn about cloud-native security models. Then, we’ll explore time-to-live, or TTL, policies. You’ll also learn about the benefits of ephemerality and immutability in cloud security. Next, we’ll explore automation, specifically infrastructure automation. You’ll also learn about infrastructure as code and policy as code. Then, you’ll learn about Terraform and cloud security automation. Next, you’ll learn about containers and their critical elements. You’ll also learn the difference between containers and virtual machines. Finally, we’ll explore container security. You’ll learn about container drift and how to prevent it, and explore orchestration. You’ll also learn about serverless functions. As soon as you’re ready, meet me in the next video.

### Video - [Cloud-native design and architecture principles](https://www.cloudskillsboost.google/course_templates/968/video/483939)

* [YouTube: Cloud-native design and architecture principles](https://www.youtube.com/watch?v=pc4DucpJFno)

Let’s start your journey into cloud-native design. As a cloud security professional, understanding how cloud resources are deployed is imperative to securing the network. In this video, you’ll learn more about what role automation has in deploying cloud-native infrastructure and how understanding cloud-native design contributes to overall network availability. You’ll also learn about managed services, and examine some powerful defense strategies. Cloud-native design is the method of creating and deploying applications and services that are optimized for cloud environments, ensuring scalability, reliability, and security. This method is key to understanding how to automate resources, and most importantly, how to keep your cloud environment secure. Designing cloud infrastructure with automation is very important, and it comes with some really great benefits. Automation will provision assets, with limited manual input. With just a few simple commands, you can choose the number of resources you want to deploy on demand. Imagine you are a cloud security professional working on a cloud site with millions of visitors. You need to deploy enough resources for all of them. Resources cost money, so you want to reduce those resources when you have less traffic. So how do you make sure you’re deploying enough resources for the visitors while not overusing resources? Use autoscaling. Autoscaling is a process where automation sets up resources for you and reduces unused resources. Automation provides a consistent platform across all resources so issues and misconfigurations can be fixed in a centralized location. As a cloud security professional, you'll come across automation often in your career. Sometimes you may need to use services from a third party or managed services. So, what is a managed service? A managed service is a service, application, or ecosystem managed by a third party. Managed services can reduce costs and reduce administrative tasks. Managed services can do things like monitor your network, protect against threats, and maintain compliance, so they can help you with some of your security duties. Now that you know more about automation and managed services, it’s time to discuss defense. One of the ways to protect your assets is to apply authentication between each component. You can think about authentication like getting past a guard in a restricted area. Before gaining access, proper identification must be shown. Authentication is one part of your protection program. You should also use rate limiting. Rate limiting is a method that prevents an operation’s frequency from exceeding a set limit or value. This will help you avoid overusing services or overloading them beyond use. Consider this example of rate limiting. A user is logging into an app on their phone. They enter the wrong password and the app gives them a warning: only two more attempts. They put in another password, but it’s also wrong. The app gives them another warning: only one attempt left. The user tries again, but it’s the wrong password. Now the app blocks them from accessing their account. This is an example of rate limiting based on the number of attempts. The user was limited to three attempts, so the app won’t allow them to continue after they exceed those attempts. Here’s another example of rate limiting where a user meets a data limit. A user is streaming video on their mobile device, which uses a lot of high-speed data. The user gets a message from their service provider warning them that they’ve used up 18 gigabytes of their 20 gigabyte high-speed data allotment. After a short time, the user gets another message. They’ve used up all of their high speed data allotment and their data speed has been downgraded. Along with rate limiting, time-to-live, or TTL, policies let you remove access from assets that don’t need to be accessed anymore. Imagine you have private data that needs to be available externally for a limited time. Once the time to live expires, the data can no longer be accessed. This will help secure your cloud, leaving less data available for possible attacks. When implemented correctly, cloud-native design uses automation to provision assets and employs authentication, rate limiting, and time to live to defend data.

### Video - [Cloud-native architecture for security](https://www.cloudskillsboost.google/course_templates/968/video/483940)

* [YouTube: Cloud-native architecture for security](https://www.youtube.com/watch?v=8DDCGqvB0Xw)

Cloud environments contend with a unique set of threats, but the good news is there are effective ways to deal with them. In this video, you’ll learn how to keep your assets safe in the cloud. With everything in a cloud environment on the internet instead of an on-premises server, the cloud needs a specific type of security. One of the pillars of cloud security is the shared responsibility model: an agreement between the customer and the cloud service provider, also called the CSP. The shared responsibility model is the implicit and explicit agreement between the customer and the cloud service provider regarding the shared accountability for security controls. The shared responsibility model states that a cloud provider monitors and responds to security threats related to the cloud and its underlying infrastructure. Meanwhile, end users, including individuals and businesses, are responsible for protecting the data and other assets they store in any cloud environment. As a cloud security professional, it’s important to consider how to maintain your end of the shared responsibility agreement. You may need to follow regulatory obligations, develop a risk management plan, apply the security standards of your organization and your customers, and follow vendors’ security requirements. It’s important to understand the different responsibilities that you, your organization, and your CSP have in securing cloud assets. The shared security model can help you determine these responsibilities. In a cloud model, servers and hardware are typically housed in a data center —outside of your control— so the provider takes care of the physical hardware. As a cloud security professional, you’re responsible for setting the security controls for your organization. For example, imagine a construction company builds a brand new gate and fence. The gate is strong, and no one can break it down. Sounds pretty secure, right? Well, maybe not. After the gate is built, they put an old, rusty lock on it. Because the construction company used an ineffective security measure to secure the gate, a malicious actor who really wants to get in can. This is an example of misconfiguration. Misconfiguration is like using a security control on your cloud assets that can be easily compromised. That’s why it’s your responsibility to keep up with changes, and always configure your security controls for the latest threats. Now that you know you’re responsible for setting security controls, you also need to determine who gets access to what in the cloud system. This is known as multilevel security. One aspect of multilevel security is access control. Access controls are security controls that manage access, authorization, and accountability of information. Other aspects of multilevel security include using data sensitivity and classification together with IAM. To do this, you need to determine how sensitive your data is and assign access based on its classification. You’ve taken quite a journey through the security concepts in this video. Now you know how to secure data in cloud-native architecture.

### Document - [TTL policies and expiration](https://www.cloudskillsboost.google/course_templates/968/documents/483941)

### Video - [Introduction to ephemerality and immutability](https://www.cloudskillsboost.google/course_templates/968/video/483942)

* [YouTube: Introduction to ephemerality and immutability](https://www.youtube.com/watch?v=dw0KFOLKAig)

Nothing lasts forever, right? In this video, you’ll learn about ephemerality and immutability. These are two important concepts in the cloud, especially in cloud security. You’ll review ephemerality and immutability and how they keep the cloud secure. You’ll also explore the distributable, immutable, and ephemeral triad, also known as the DIE triad. So what is ephemerality? Ephemerality is the concept that things only exist for a short amount of time. For example, an access token is a piece of code that contains important information like the user’s ID, their permissions, and groups they belong to. Temporary access tokens expire shortly after they’re created. They give users enough time to provide the information needed to gain access, then they expire. So, when you apply the concept of ephemerality to infrastructure, it refers to resources that are created as the need arises, then made unavailable when no longer needed. Think of it this way: ephemerality is like having a paper file of confidential data that you read and then lock in a secure vault so no one else can read the file. Similarly, in cloud security, once your task is complete, ephemeral data is locked and is no longer available. Imagine an organization has highly confidential data that will only be needed for a short time. If the organization saves the data permanently there’s still a chance the data can be breached. However, if the data is ephemeral, once it’s used and the expiration time is up, the data is made unavailable. Even the most skilled attacker can’t access data once it becomes unavailable. Immutability is the concept of being unable to change an object after it is created and assigned a value. An object that’s immutable can't be changed after it’s created. So if your infrastructure is immutable, you can’t modify it. If something needs to be changed, you replace it with a different version. Immutability has benefits in cloud security. One benefit is that immutable assets can’t be changed, so they can’t introduce new vulnerabilities. For example, a web application has code that can’t be modified by users. This prevents the introduction of malicious code in the app. Because cloud servers are in remote data centers managed by cloud service providers, they need to be configured differently. Cloud security professionals need to rely on cloud-specific solutions. The DIE triad can help you do just that. DIE stands for distributed, immutable, and ephemeral. Distributed refers to the system resources that are distributed and not dependent on a single zone. Remember, a zone is the collective number of data centers in an area. Immutable means your infrastructure can be disposed of and replaced rather than modified. This is helpful because you can more easily identify and reverse changes by reverting to a previous version that’s been saved in an immutable state. Ephemeral means your resources are finite and can be disposed of when they are no longer being used. Attackers can be persistent, but they can’t access information that’s not available. In this video, you learned how ephemerality can help keep your data protected. You also learned about immutability and how you use it can protect your resources from being modified by keeping them in an unchangeable state. This knowledge will strengthen your effectiveness as a cloud security professional.

### Quiz - [Test your knowledge: Ephemerality and immutability](https://www.cloudskillsboost.google/course_templates/968/quizzes/483943)

### Video - [Automation in cloud security](https://www.cloudskillsboost.google/course_templates/968/video/483944)

* [YouTube: Automation in cloud security](https://www.youtube.com/watch?v=axkNpzhFPIo)

As a cloud security professional, you can use automation to streamline tasks and responsibilities. In this video, we’ll discuss some of the benefits of automation, how it applies to infrastructure, and how it can help you in your career. Before we dive into the automation of infrastructure, it’s helpful to understand the overall concept of automation. Automation is the use of technology to reduce human and manual effort to perform common and repetitive tasks. Automation also reduces the likelihood of human error. Cloud architecture is pretty complex, and there’s a lot you need to do to keep it secure. Automation can support you in this process. One common security task is monitoring your system for threats. You could do this yourself, but using automation to monitor the system, you can free up time to focus on other tasks. Once automation is set up, it completes tasks efficiently without deviation. For example, if you set automation to monitor your systems, it’ll complete this task within the parameters you’ve provided. Now that you know the benefits of using automation, let’s explore how it works. You can use code and automation tools to program your system to perform repetitive tasks and monitor the system for security. You can even apply and distribute patches when vulnerabilities occur. Now let’s explore Infrastructure as Code, or IaC, which is the practice of automating and managing infrastructure using reusable scripts. To set up IaC, you will need to use tools like Terraform to provision infrastructure. Terraform is an automation tool. It lets you build, change, and version cloud resources efficiently and safely. With Terraform, if you specify the cloud platform and the resources needed, Terraform will build the assets for you. IaC can provide you with the automation power to do fantastic things like creating cloud resources, reviewing changes, testing changes, and auditing. Now, you’ll explore how to use automation to deploy containers. A container can be built in Terraform and is a software package that holds only the components necessary to execute a particular application. There are several platforms available for deploying these containers. One of the most popular is Kubernetes. Kubernetes has the ability to manipulate containers and automate the deployment of resources throughout their lifecycle. Software goes through a lifecycle, or a series of stages and automated steps in a continuous-integration, continuous-development, or CI/CD, pipeline. Since this is a continuous and iterative process, automating this process with Kubernetes allows the system to do the work for you. Automation can also help with configuration, scanning for vulnerabilities, reporting, enforcing policy, patching software, and resolving threats by taking the proper steps to solve them. Automation is a key factor in efficient cloud security operations and a resource you can rely on. Learning how to use it efficiently will improve your competence as a cloud security professional.

### Video - [Infrastructure as code, policy as code, and DevSecOps](https://www.cloudskillsboost.google/course_templates/968/video/483945)

* [YouTube: Infrastructure as code, policy as code, and DevSecOps](https://www.youtube.com/watch?v=JJbHnS7uMh8)

Infrastructure is the foundation of many services. Working with cloud infrastructure doesn’t always mean working with physical hardware. You can use code to build infrastructure and to define and manage policy. Let’s explore infrastructure and policy as code, then explore how DevSecOps keeps security involved in operations throughout the coding process. Infrastructure as a Service, or IaaS, and other cloud infrastructure hosting platforms, have made infrastructure as code possible. So how does infrastructure as code work? Users create files with infrastructure specifications, meaning they code the infrastructure they want to use. It’s like turning code into components. Infrastructure as code is made available by an application programming interface, also known as API. API is a library function or system access point with well-defined syntax and code that communicates with other applications and third-parties. Infrastructure as code is also compatible with many different coding languages. There are two approaches to infrastructure as code. First is the declarative approach in which you use an infrastructure as code tool to configure the desired state of the system, including necessary resources and any properties those resources should have. This also makes it simpler to manage taking down the infrastructure. The declarative approach focuses on what you need done. Second is the imperative approach, which defines the specific commands you need to achieve the desired configuration. After configuration, these commands need to be executed in the correct order. The imperative approach focuses on how you need something done. As a cloud security professional you can use tools to help automate and manage your infrastructure components like networks, cloud-managed services, firewalls, applications, and other components. But, automation can be used for so much more. Let’s explore another way you can use automation to help with policy management. Policy as code is the use of code to define, manage, and automate policies, rules, and conditions using a high-level programming language. Policy refers to the rules, conditions, or instructions that govern processes or operations in IT. Cloud professionals build policy into infrastructure to help manage policy. So how does security fit into the coding process? There’s a growing movement known as DevSecOps that’s working to integrate security into the coding process. Development Operations, or DevOps, consists of practices designed to increase the frequency of delivery and reduce lead time. DevOps’ goal is to make sure the operations team is collaborating with the development team from the very beginning of a project. When security is incorporated, it’s called Development Security Operations or DevSecOps. An effective DevSecOps model will engage security throughout the operations and development process. With the DevSecOps model, policy as code and other security measures are incorporated throughout the operations and development process. Overall, this process promotes teamwork and collaboration. The increased teamwork helps identify vulnerabilities early, providing faster delivery. DevSecOps employs automation tools to automate several security processes. Some of these processes include: Continuous integration and continuous deployment, or CI/CD, the version control system, continuous testing, continuous monitoring, containerization, orchestration, and configuration management and deployment. In this video, you learned about infrastructure as code, policy as code. Additionally with DevSecOps, you learned how to save time and resources by incorporating security into infrastructure, promoting collaboration, and using automation. You’ll encounter these concepts in your career as a cloud security professional.

### Document - [Automation to improve cloud security efficiency](https://www.cloudskillsboost.google/course_templates/968/documents/483946)

### Document - [AI and automation in security](https://www.cloudskillsboost.google/course_templates/968/documents/483947)

### Document - [Infrastructure as code and cloud-native security](https://www.cloudskillsboost.google/course_templates/968/documents/483948)

### Video - [Benefits of policy as code](https://www.cloudskillsboost.google/course_templates/968/video/483949)

* [YouTube: Benefits of policy as code](https://www.youtube.com/watch?v=vIahnWdryXA)

Policy is one of the most important factors of security, but maintaining policy is complex, takes time, and people make mistakes. A mistake in enforcing policy can cause major problems. What if you could save time while reducing the potential for human error? Well, you can using policy as code. Cloud security engineers enforce policies like the creation of virtual machines, limiting access to resources, and more. While these can be done manually, there are benefits to using policy as code. These benefits include: implementing updates at a large scale, providing fast feedback about compliance, preventing mistakes, and avoiding violations and penalties. Let’s start with implementing updates at a large scale. Updates to policies across many resources can be automated using policy as code. Imagine security cloud engineers are setting policy for an e-commerce retailer. They need to apply the same policy changes to many resources at one time. They use policy as code to automate updates to all of their resources, reducing the manual process which could take several hours or even days. Another benefit to using policy as code is fast feedback about compliance. Going back to the example, the retailer’s security team decides to automate policy enforcement for compliance purposes. Doing so provides the team faster feedback about compliance than if employed using a manual approach. Whether policies are automated or updated manually, humans make mistakes. These mistakes can cause big, expensive problems especially when done on a large scale. Suppose the retailer’s security team automates the creation of virtual machines. They accidentally provision hundreds of large-size machines, which isn’t appropriate for their use case. This costly mistake uses up a lot of resources. Thoughtful implementation of policy as code can prevent mistakes. Another benefit to consider when using policy as code is avoiding violations and penalties in order to comply with government and industry standards. Security teams can code the controls from the standards into the policy to ensure compliance. As a cloud security professional, it’s important to remember that policy that works right now may not always work. Policies may need to change. Technology changes too, and sometimes policy causes new problems with new technology. It’s a good idea to keep track of all the different versions of your policies. That way, you can go back to an earlier version of your policy configuration if there are problems. Now you know about the benefits of policy as code. As a cloud security professional, you can use these benefits to your advantage in order to maintain security standards and compliance.

### Video - [Terraform for IaC management](https://www.cloudskillsboost.google/course_templates/968/video/483950)

* [YouTube: Terraform for IaC management](https://www.youtube.com/watch?v=iYtLdyCurdM)

In your cloud security career, implementing thoughtful infrastructure design will be critical. You can use infrastructure as code, or IaC tools, to help automate and manage your infrastructure components like networks, cloud-managed services, firewalls, applications, and other components. IaC tools give you a way to define resources using configuration files. Within these configuration files, you can define parameters, options, settings, and preferences for these resources. Terraform is a popular IaC tool that enables users to provision cloud resources by communicating with an application programming interface, or API. Terraform creates and manages resources through that code. Some of the benefits of using Terraform include automation, reproducibility, and consistency. Terraform can automate the process of building and managing your infrastructure, which can save you a lot of time and effort. Terraform can also help you to create reproducible infrastructure deployments. In other words, you can easily create the same infrastructure environment on different cloud platforms or on-premises. Additionally, Terraform ensures that your infrastructure is consistent across different environments, improving the overall reliability and security of your infrastructure. Terraform is a declarative tool, which means it defines what the desired state of the system is, including which resources you need, and what properties those resources should have. There are three steps to the core Terraform workflow. The first step is to write the infrastructure as code. This involves defining your desired infrastructure configuration using Terraform. Consider this scenario: As a cloud security professional, a developer team asks for your help in the building and deploying a virtual machine. They provide requirements and you use Terraform to write instructions to build the virtual machine. In this case you use a Google Compute F1 micro virtual machine. First, you designate your resource, then name your instance. Then, put in the machine type. So, now you have an IaC that is ready to preview. The second step is to run the Terraform plan command. In this step, you preview your configuration before you apply it. The focus of this step is to ensure that the feedback loop aligns to the design of your infrastructure and is error free. Please be aware that this step may be iterative. When building infrastructure, the development team should work together with the security team to build in policy-as-code using Terraform. You can use these policies to ensure the Terraform plan is compliant with past practices and security rules. For example, Terraform uses a tool called Sentinel to enforce policy on Terraform configurations. Sentinel uses a high-level, purpose built programming language to build policies. Benefits of policy as code include sandboxing, codification with comments for quick learning about policies, and version control. The Sentinel step happens between the planning and applying steps in the Terraform workflow. The third step is to run the Terraform apply command to provision the infrastructure. The focus of this step is for Terraform to communicate with the cloud provider to initiate the provisioning process. This involves the creation, modification, or deletion of resources based on the configuration in the IaC. It also determines the relationships between resources. It can use provider plugins to modify, create, and destroy resources on infrastructure providers. You can also use a single workflow to manage your infrastructure, but it will acknowledge the uniqueness of your provider. In this video, you learned that you can save yourself a lot of work by building and automating your infrastructure. So remember, tools like Terraform can help!

### Document - [Terraform and cloud security](https://www.cloudskillsboost.google/course_templates/968/documents/483951)

### Document - [Guide to automating deployment with Terraform](https://www.cloudskillsboost.google/course_templates/968/documents/483952)

### Lab - [Change firewall rules using Terraform and Cloud Shell](https://www.cloudskillsboost.google/course_templates/968/labs/483953)

Change firewall rules using Terraform.

* [ ] [Change firewall rules using Terraform and Cloud Shell](../labs/Change-firewall-rules-using-Terraform-and-Cloud-Shell.md)

### Quiz - [Test your knowledge: Automation in cloud infrastructure](https://www.cloudskillsboost.google/course_templates/968/quizzes/483954)

### Video - [Containers vs. virtual machines](https://www.cloudskillsboost.google/course_templates/968/video/483955)

* [YouTube: Containers vs. virtual machines](https://www.youtube.com/watch?v=rE7GkFccSQ8)

In this video, you’ll learn how virtual machines and containers use virtualization differently. As a cloud security professional, you’ll need to be familiar with the security implications and best practices of these technologies. Let’s get started! A virtual machine is a full operating system running on a host computer. It can run applications and use its own operating system independent of the host computer. It even includes the hardware layer, just like the hardware you’d find in a physical computer. This layer holds components like the CPU, memory, and disk drives. It also uses a hypervisor. The hypervisor is software that serves as an abstraction layer that sits between the physical computer and the virtual machine, allowing the creation and management of virtualized computing environments. The hypervisor allows one host computer to share its memory, processing, and other resources to support multiple virtual machines. Containers let you run applications online, making them more efficient, and faster. A container is a software package that holds only the components necessary to execute a particular application. When an application goes into a container, it’s just like putting something into a box. And after adding an application, you can then add libraries and other dependencies to the container, just like adding more things to the box. Imagine you need to build a container for a specific Linux distribution. Instead of searching for every required component of that distribution, you can use the search term of the distribution you want. When you query a container repository, that repository will find the specific distribution and all dependencies and libraries needed for the Linux distribution to run and place them in the container. Both containers and virtual machines let you use isolated environments for running software services. But, it’s important to remember, there are differences between them. A virtual machine has its own guest operating system, like Linux, running on top of a host operating system. A benefit of using a virtual machine is that the hypervisor is allowed access to utilize the host's hardware. Containers hold only the dependencies they need, but they don’t have access to their own hardware. A benefit to containers is that they’re lightweight, meaning they use a lot less memory than virtual machines. You can use a deployment platform, like Docker, to deploy your container once the image is created from building the application and all dependencies that were retrieved from the container repository. You can also use Docker to create, deploy, and manage your containers. Docker has its own command-line interface, and builds the containers based on your needs. You can use this interface to interact with Docker in a terminal. Your commands are then sent to a program running in the background called the Docker Daemon. The Docker Daemon manages the containers, and the images those containers are created from. When working in the cloud, you’ll frequently have to use virtualization. Remember, virtual machines are complete virtual systems with hardware access. Containers hold only what you need to run your application but they don’t have hardware-level access. Knowing these differences will help you decide which technology to use depending on the situation.

### Document - [A brief guide to containers](https://www.cloudskillsboost.google/course_templates/968/documents/483956)

### Document - [Containers’ importance in the cloud](https://www.cloudskillsboost.google/course_templates/968/documents/483957)

### Video - [Container benefits and considerations](https://www.cloudskillsboost.google/course_templates/968/video/483958)

* [YouTube: Container benefits and considerations](https://www.youtube.com/watch?v=Z2WV1YLKVxA)

As a cloud security professional, it’s important to know about containers. Chances are, you’ll be working with them often. In this video, you’ll learn about how containers can benefit you, and what to be aware of when you use them. One critical element of containers is that they share the kernels of their hosts instead of having their own kernels. The kernel is the component of an operating system that manages processes and memory. For example, when you turn on a computer, the first component that starts is the kernel. Since they’re not connected directly to kernels, containers can be moved easily between systems. They’re platform-independent, and can be run on any systems that can support them. Remember, containers share the host’s kernel, so if the host is vulnerable to an exploit, an infected container can attack the kernel’s vulnerability. This means that the host kernel needs to be up-to-date, and all the libraries in the containers need to be patched. You’ll also need to be extra careful when configuring your container permissions. If you give them more privilege than they need, like having them run as root users, you can expose them to attack. Now that we’ve covered containers and container security, let’s discuss container orchestration. Container orchestration allows you to deploy, scale, and manage your applications using automation. Kubernetes is a popular container orchestration platform that you can use in a cloud environment. Imagine Kubernetes as the conductor of a grand orchestra. Kubernetes, like a band or orchestra instructor at a school or university, directs the performance. It issues commands and ensures harmony with the ensemble of containers, just as an instructor guides students. Containers are like the musicians in an orchestra or students in a music class. Each container represents a specific instrument, each with unique roles within the performance. Kubernetes groups containers into pods, which are like the classrooms or music halls within the school where the orchestra practices. They are shared spaces that house multiple containers, similar to classrooms accommodating students with various roles. These pods enable containers to work closely together, sharing resources like power and the local network, much like students sharing facilities within the same room. Nodes are the physical or virtual machines where the performance takes place, akin to the school buildings hosting multiple classrooms. Just as a school building may have many classrooms, a node can have many pods running within it, providing the stage for the orchestra -containers- to perform. A layered file system is another important element of containers. When you make a new container, there’s one new container layer that new commands and files can be written to. So, any new commands, such as run or copy, that you give to the running container are written to the container layer. Each layer of a container is built on another layer to form an image. After the layers are put together, you get a final image. Remember, an image includes all the files and dependencies you need for your containerized app. Container runtime is software that is responsible for running and managing containers. The container runtime provides an interface that interacts with the host operating system to create, start, stop, and manage containers. Thinking back to our orchestra example, it’s like a conductor cuing a musician to play their instrument at the right time. Now, let’s explore how a container runtime works to put containers together. First, you’ll need an image that includes all the software and dependencies you need for your container. When you use the run command, the platform puts together the container and deploys it. Because containers make up an important part of cloud computing, you’ll likely use them a lot in your role as a cloud security professional. Now that you know more about the elements that set containers apart from other cloud technologies, you can use them to save time and resources.

### Quiz - [Test your knowledge: Containers explained](https://www.cloudskillsboost.google/course_templates/968/quizzes/483959)

### Video - [Techniques to secure containers](https://www.cloudskillsboost.google/course_templates/968/video/483960)

* [YouTube: Techniques to secure containers](https://www.youtube.com/watch?v=yEhVSuf3U-g)

Containers are convenient and make sharing CPU, storage, memory, and network resources easy and portable. An important part of being a cloud security professional is securing containers. In this video, we’ll explore ways to keep containers secure so that you’re prepared for a role in cloud security. It’s important to know the rules when it comes to securing containers. First, don’t put anything in your container that you don’t need. This can give attackers a larger surface to breach. If you find tools you don’t need, remove them. If you leave them there, attackers might use them. Second, use verified or signed images. Third, scan each image you use to make sure there are no vulnerabilities or misconfigurations. Pro tip: be aware that images that may have been verified as secure originally can have new vulnerabilities later. A container deployment platform lets you build containers and deploy them. They may include command-line interfaces, or CLIs, that allow you to input commands to deploy the containers. A container deployment platform is a software solution that allows you to manage containerized applications. It provides capabilities like automation, orchestration, governance, security, customization, and enterprise support for container architectures. Now, let’s explore deployment and security with automation using a container orchestration system. There are a few container orchestration systems available to cloud security professionals. For example, Kubernetes is one of the more popular orchestration systems you might come across in your career. Kubernetes is a platform for automating deployment, scaling, and managing containers. If a container goes down, Kubernetes will start a new one using automation. This makes the process easier because it’s done for you automatically. Kubernetes also includes built-in commands to deploy applications, scale up and down for changing needs, monitor applications, and roll out changes. The Kubernetes platform handles a lot of the work that goes into application management, runs health checks against services, and replaces stalled containers when they’re unhealthy. This means that containers are not updated or patched, but replaced by new versions. This allows you to roll the container back to a previous version if vulnerabilities are discovered in new code. Even after completely replacing containers, there are still vulnerabilities you need to be aware of. Let’s explore some of them together. Kubernetes is a useful deployment system, but there are some vulnerabilities that can happen at different stages. When containers are being built, all the code needs to be trusted, and checked. Code from any untrusted registries might contain backdoors which can allow potential attackers in. When building containers, you also want to avoid too much clutter. Remember to search for any unnecessary libraries and dependencies. To keep these artifacts from becoming compromised, delete them as soon as you find them. There are some considerations you’ll need to be aware of during deployment. First, never grant unnecessary privileges. Only grant what is necessary for the task to shrink the attack surface. Next, avoid problems by using namespaces to group services for your application, and to isolate applications in your container clusters. Then, use policies to divide the network into segments. Finally, use role-based access controls, or RBAC to prevent unauthorized access. Vulnerability scanning is another process that can help you find any vulnerabilities in your container images. You can manually scan your container software images, or you can use automation. When you use automation, it will automatically scan each container you deploy to make sure there are no vulnerabilities. Security threats can also happen in containers. As a cloud security professional, you’ll need to be able to detect and respond to these threats. Using container runtime security can help you detect abnormal behavior, and stop a threat by isolating a container on a different network. It can pause or restart a container if it finds potential problems. It can also enforce run-time policies. Now that you know more about securing containers, you can keep them safe from attackers!

### Video - [Container drift](https://www.cloudskillsboost.google/course_templates/968/video/483961)

* [YouTube: Container drift](https://www.youtube.com/watch?v=snki1PNWYRE)

When your container has a container layer, it can write to that layer and make changes. This can be convenient, but it can also introduce dangerous container drift. In this video, we’ll examine container drift and how to deal with it. Let’s investigate! A container layer is a writable space in a container. But, this space poses security risks. The more the container writes to this space, the less it has in common with the original image it was created from. When the container writes to the container layer, the container changes. That’s container drift. One way to find vulnerabilities is by scanning for them. If containers can’t be changed or updated, scanning images would prevent vulnerabilities from being introduced at runtime. When a container has a writable container layer that can be changed, scanning the image won’t help much. So, while having a writable layer can give the container some flexibility, it can also introduce vulnerabilities. Vulnerable images don’t pose an active threat by themselves. But, they can introduce vulnerabilities to a live environment. Imagine the following scenario: A developer team needs to deploy a resource quickly, so they use an image from an outdated container library. The image had been previously scanned and was free from vulnerabilities and malware. So, they dispatch the container with the added image. When they dispatch the image, they discover new vulnerabilities in the image. Now, there’s a vulnerability in the live environment. Even a popular, commonly-used container library can contain thousands of images with malware. This malware might include cryptocurrency miners, back doors, website redirectors, and DNS hijackers. These consume resources and increase operational costs. They also give attackers opportunities to gain information and hijack your website. Container images are layered. The container layer is a writable layer that allows changes to be made. As the container writes to this space, it becomes different from the image it originated from. To prevent the problems associated with container drift, you need to prevent the drift itself. Drift prevention stops files from running if they’re not part of the original image. You can use drift prevention to keep your container immutable, meaning no new executables can be added. Code needs to be updated to keep up with changes to technology. When these changes happen, you can build a new image and deploy it. But, make sure to keep the old image available in case you need to go back, so you can redeploy it. When you’re building containers, you should also keep a list of their components. These components include the software built into the container. A software bill of materials, or SBOM, is a machine-readable list of each piece of software and its components involved in the supply chain. An SBOM will provide information on applications and components running in your container, and software information for all third party libraries you’re using. An SBOM can also track software and its components. It can track where they come from, and what security vulnerabilities they contain. And, it enables protectors to stop vulnerabilities from being exploited. SBOMs usually follow the Software Package Data Exchange, or SPDX standard. SBOM documents may contain: package information, file information, snippet information, any other licensing information detected, Information on the relationships between SPDX elements, annotation information, and review information. Container drift can compromise your containers’ security, but you can prevent it. You can use the knowledge you learned in this video to keep containers secure throughout your cloud security career.

### Document - [Security in containers](https://www.cloudskillsboost.google/course_templates/968/documents/483962)

### Document - [Serverless functions and security](https://www.cloudskillsboost.google/course_templates/968/documents/483963)

### Video - [Container orchestration](https://www.cloudskillsboost.google/course_templates/968/video/483964)

* [YouTube: Container orchestration](https://www.youtube.com/watch?v=JNW8a3RLE8I)

There are many tasks cloud security professionals need to perform on a regular basis. That’s a big responsibility. So, how do they manage them all? Well, orchestration can help. Orchestration is useful because it manages a full set of automated tasks for you. For example, it manages deployment, scaling, load balancing, networking, provisioning, lifecycle management, and container availability. And, orchestration itself can also be automated. To use orchestration, use declarative programming. With declarative programming, you can write a configuration file focused on the desired outcome of an automation instead of detailing the steps needed to complete a task. Then, you can choose a platform or tool to find the appropriate host for your containers. Orchestration tools choose a host based on the requirements or predetermined constraints in the configuration files. These tools also schedule container deployments. Container orchestration tools help automate a variety of operational tasks. For example, these tools scale containers and load balance, and allocate resources between containers. Additionally, orchestration tools monitor the health and performance of the application. There are several orchestration platforms available. For example, the Kubernetes engine is a popular platform that enables you to configure infrastructure to run your containerized apps. This infrastructure includes networking, hardware, scaling, and security. It also manages underlying components, like nodes, pods, and container clusters for you. Nodes are machines that can be either physical or virtual, and include one or more containerized applications which are known as pods. A pod uses shared resources and a specification on how to run containers. Pods are grouped by container clusters. A pod uses shared resources and a specification on how to run containers. Pods are grouped by container clusters. Container clusters are dynamic systems that manage and place containers, grouped in pods. Container clusters run on nodes, along with all the interconnections and communication channels. Once an image is deployed to the Kubernetes engine, it goes into a cluster, through a load balancer, and into several nodes containing pods. Clusters can be set up for a single zone, or for a region. If the clusters are set up for a region, they’re replicated across multiple zones. Setting up a regional cluster can help minimize disruptions, but it can also use a lot of resources. So, it’s important to use horizontal autoscaling. You can use horizontal autoscaling to increase or decrease the number of pods based on utilization. And, you can use virtual autoscaling to scale your clusters for both pods and nodes. Orchestration can automate policies, and keep errors from happening. Imagine a container is about to be deployed, but it has vulnerabilities that violate a policy. If a violation of the policy is found during orchestration, deployment will be stopped. This can help you avoid human error, and ensure the container is safely deployed. Role-based Access Control, or RBAC is a type of identity and access management, or IAM, that is central to Kubernetes operations. It involves a template or permission set that determines who can execute what, and where it can be executed. With RBAC, you can create and grant roles for any object or object type within a cluster. A role defines access to only resources within a single namespace, while a cluster role defines access to the entire cluster’s resources. RBAC can help keep your clusters safe, but there are some things to avoid. Because the cluster-admin role grants unlimited access, never grant cluster-admin access to any users or teams that don’t need high-level access. Also, avoid aggregating or combining privileges. Combining them may change the access they grant. It’s also important to avoid creating roles that are not used. Orchestration does a lot for you. To be safe, always be sure to carefully assign roles, and only give access to those who need it. Now that you know how it can help you with cloud security, you can use it to manage your automated tasks. Orchestration can also help you prevent security risks.

### Document - [Activity: Analyze the security of a container](https://www.cloudskillsboost.google/course_templates/968/documents/483965)

### Quiz - [Activity Quiz: Analyze the security of a container](https://www.cloudskillsboost.google/course_templates/968/quizzes/483966)

### Document - [Activity Exemplar: Analyze the security of a container](https://www.cloudskillsboost.google/course_templates/968/documents/483967)

### Quiz - [Test your knowledge: Orchestrators and security of containers](https://www.cloudskillsboost.google/course_templates/968/quizzes/483968)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/968/video/483969)

* [YouTube: Wrap-up](https://www.youtube.com/watch?v=JXECamqxYus)

Congratulations! You’ve arrived at the end of this section on cloud security risks. You’ve learned a lot about cloud-native concepts and security along the way- well done! On your journey, you learned about cloud-native design considerations. We started you off with an overview of cloud native design and architecture, then introduced you to cloud-native security models. Next, you explored time-to-live, or TTL policies. Then, you learned about the concepts and benefits of ephemerality and immutability in cloud security. Next we discussed infrastructure automation, infrastructure-as-code, and policy as code. Then, you explored Terraform and cloud security automation. After that, we introduced you to containers and their critical elements, and discussed the difference between containers and virtual machines. Finally, we explored container security. You learned about container drift and how to prevent it, and explored orchestration. Congratulations on finishing this section of this course. You’re doing an amazing job. Meet you again soon!

### Document - [Glossary terms from module 3](https://www.cloudskillsboost.google/course_templates/968/documents/483970)

### Quiz - [Module 3 challenge](https://www.cloudskillsboost.google/course_templates/968/quizzes/483971)

## Data Protection and Privacy

In this module, you'll gain a comprehensive understanding of data protection and privacy in the cloud. The module will guide you through the essentials of data loss prevention, data discovery, data processing, data treatment, and data classification, empowering you to understand and implement effective data governance and access strategies. Additionally, you'll explore the nuances of data retention levels and how compliance controls are applied, equipping you to explain and navigate governance in cloud security.

### Video - [Welcome to module 4](https://www.cloudskillsboost.google/course_templates/968/video/483972)

* [YouTube: Welcome to module 4](https://www.youtube.com/watch?v=UizVmq5TQ0g)

Hey there! Are you ready to learn about the fascinating world of data protection and privacy? Let’s get started! We’ll start by exploring the three states of data and how to use encryption for each state. Next you’ll learn about data classification, data governance, and sovereignty. Then we’ll discuss data discovery, retention policy, and business continuity planning. First, we’ll provide you with an introduction to the three states of data. You'll also learn about data encryption at rest, in transit, and in use. Then, you’ll learn about data classification and tagging. Next, you’ll learn about data governance, data sovereignty, data discovery and data retention. Then, we’ll discuss business continuity planning. You’ll learn about the process of creating a business continuity plan that’s best for your organization. Plus, we’ll go through a possible scenario for using a business continuity plan. There are lots of cool things for you to explore. As soon as you’re ready, let’s meet in the next video.

### Video - [Introduction to the three states of data](https://www.cloudskillsboost.google/course_templates/968/video/483973)

* [YouTube: Introduction to the three states of data](https://www.youtube.com/watch?v=N1RR6NWP-iU)

Companies collect large amounts of highly confidential data, like customer data, financial data, and business data. As a cloud security professional, you’ll work to ensure data is secure. In this video, you’ll learn about the three states of data. Let’s get started! The first state of data is data at rest. This is data that is not being accessed or actively moving from device to device or network to network. Data at rest is stored on servers, databases, mobile devices, physical drives, or in the cloud. At rest data includes intellectual property, employee and customer personal information, financial documents, contracts, and other types of data, like healthcare data. The next state of data is data in transit. The next state of data is data in transit. This is data that is moving between two places, like between a cloud provider and your site, or between two services. For example, using a mobile device to access a banking app moves data between two places: your mobile device and the host of the banking app. The third state of data is data in use. Data in use is data that’s being accessed by one or more users. Data in use includes, but is not limited to data that is being processed, erased, read, or accessed. For example, after you have accessed the banking app, accessing your transaction history on a banking website or app is an example of data in use. If you then decide to transfer money between accounts, the data then returns to an in transit state, while it moves between your device and the banking app’s host. The data returns to being in use as you review additional account details. Now that you know about the three states of data protection, you’ll be better prepared for your career as a cloud security professional.

### Document - [Data encryption](https://www.cloudskillsboost.google/course_templates/968/documents/483974)

### Video - [Data encryption at rest, in transit, and in use](https://www.cloudskillsboost.google/course_templates/968/video/483975)

* [YouTube: Data encryption at rest, in transit, and in use](https://www.youtube.com/watch?v=dVLbRyTr-mo)

Data encryption protects your data at rest, in use, and in transit data. But how does at rest encryption work? In this video, we’ll explore encryption techniques for the three states of data. Remember, data at rest is any data that is not currently being accessed. Data at rest needs to be encrypted to be secure. Data encryption is the process of converting data from a readable format to an encoded format. For example, a readable format might be a plain text email. And, an encoded format might be scrambled text and symbols that don’t make any logical sense to a reader. The Advanced Encryption Standard, or AES is the world standard encryption tool that helps protect data at rest. AES is a tool that converts data to unintelligible cybertext and back into its original form with the proper key. This means that AES encrypts information while at rest, and this information can only be unencrypted with a key. Encryption helps keep data protected with data encryption keys, or DEKs. Even if an attacker gets access to the data storage, they won’t be able to read any of it without the appropriate keys. Another way to protect data at rest is to break it into chunks for storage. Each of these chunks get their own DEKs. Each time a chunk is updated, it gets a new encryption key. Because there are a variety of keys that change regularly, the risk of an encryption key compromise is reduced or eliminated. It’s not just data at rest that needs to be protected. Data in transit also needs to be protected by encryption. You can use Transport Layer Security or TLS to encrypt data in transit to keep it secure. TLS is a security protocol that encrypts data transmitted between two communicating applications. TLS is often used to encrypt data sent over the Internet to ensure that unauthorized users are unable to read what has been transmitted. This is critical when transmitting private and sensitive information, such as passwords, credit card numbers, and personal correspondence. For example, say you’re using a mobile banking app. If you transfer money to another account or pay a bill with the banking app, TLS will help to ensure that your sensitive information -like your account number- is transmitted securely over the internet when you make the transfer. Encrypted data is useless to any attackers, because they’re not able to read it. Lastly, you will need to encrypt data in use. Confidential computing is one way to do this. Confidential computing is the protection of data in use with hardware-based Trusted Execution Environment, or TEE. To encrypt data, a TEE offers a secure and isolated environment that prevents unauthorized access, or modification of applications and data, while they are in use. A TEE is secured through embedded encryption keys and only authorized code can utilize these keys. Now that you know how different types of data encryption protect data at rest, in transit, and in use, you will be able to help secure different states of data as a cloud security professional.

### Document - [Asymmetric versus symmetric encryption](https://www.cloudskillsboost.google/course_templates/968/documents/483976)

### Quiz - [Test your knowledge: Cloud data protection and privacy techniques](https://www.cloudskillsboost.google/course_templates/968/quizzes/483977)

### Video - [Data classification and tagging](https://www.cloudskillsboost.google/course_templates/968/video/483978)

* [YouTube: Data classification and tagging](https://www.youtube.com/watch?v=8y74Ejqwv2U)

The last thing anybody wants to learn is that their personal information has been leaked or stolen. In your cloud security career, you or your team will be responsible for keeping sensitive personal information away from attackers. In this video, you'll learn about data classification, tags, and how categorizing data helps keep assets secure. Data classification is the process of analyzing data to determine its sensitivity and value. Data classification enables organizations to categorize their data, which helps security analysts efficiently find and secure assets. Organizations use sensitivity levels, data type, and value as metrics to organize the data. Let’s explore the different sensitivity levels of data: low, medium, and high. Lower sensitivity levels represent less of a risk for organizations. This makes low sensitivity data less valuable. For example, imagine you create an open-access, public blog that has lots of readers! Publishing a blog for the public to read is an example of sharing public information. And public information is the least sensitive type of data you can have. Medium sensitivity level data is more sensitive, and should be inaccessible to the public. Here’s an example. At a marketing company, an associate sends an internal memo with a little information about the company, along with meeting notes on an advertising plan outline. There’s no personal information, like social security numbers or medical documentation, and there are no company secrets. But, the information is still sensitive, and should not be shared with the public. High sensitivity data includes information like personal financial data, legal documentation, and company secrets. High sensitivity data is very valuable to organizations. For example, a database with all employees’ personal and employment data is highly sensitive. If an attacker gets that information, the employees’ identities will be in danger. Stolen or leaked sensitive data can lead to major consequences, like fines for violating privacy laws, or damage to the organization’s reputation. Cloud security professionals use tags as a consistent way to document and classify data. Tags are custom metadata fields you can attach to a data entry to provide context to people authorized to access the data. Metadata can include information like who is responsible for the data entry, and whether it contains highly sensitive data like personally identifiable information, or PII. Keep in mind, tags or labels are essential for data protection. They help your security controls find the data that needs to be protected. So, to protect your data, you need both the tags and the security controls. Let’s review an example of a customized PII tag. The information in this tag is presented as a label, followed by two metadata pairs. The first line of code is the label for the tag and it has the letters PII. The next line of code is the first metadata pair; it verifies that the value is true. This means there is personally identifiable information in this asset. The last line of code has the second metadata pair and it identifies the type of PII as a social security number, or SSN. Both of these pairs together, create a field in the tag. This PII tag example has a boolean field, abbreviated BOOL. That means there can only be two possible results, in this case: true or false. The asset with this tag has highly sensitive information, and the tag indicates what type of data it is. The tag classifies this information as high sensitivity, making it easier for security controls to locate. When you make your tags, you can use tag templates, or reusable structures to rapidly create new tags. You can make a template by putting together metadata values in fields. You can then grant or deny access to a template with identity and access management roles, or IAM roles. Now that you know about data classification, and tagging assets to classify and protect them, you can use them to protect data so you can keep the data from being leaked or stolen.

### Document - [Protection of personally identifiable information (PII)](https://www.cloudskillsboost.google/course_templates/968/documents/483979)

### Document - [Cryptographic keys for data protection](https://www.cloudskillsboost.google/course_templates/968/documents/483980)

### Lab - [Create symmetric and asymmetric keys](https://www.cloudskillsboost.google/course_templates/968/labs/483981)

Key management - create symmetric and asymmetric keys

* [ ] [Create symmetric and asymmetric keys](../labs/Create-symmetric-and-asymmetric-keys.md)

### Quiz - [Test your knowledge: Techniques for protection of personal data](https://www.cloudskillsboost.google/course_templates/968/quizzes/483982)

### Video - [Data governance for security and data quality](https://www.cloudskillsboost.google/course_templates/968/video/483983)

* [YouTube: Data governance for security and data quality](https://www.youtube.com/watch?v=EQCFbn-yo14)

Data governance enables cloud security teams to ensure data is private, accurate, secure, available, and usable. In this video, we’ll explore how you can use data governance to maintain data quality and keep data secure. Data governance is a set of processes that ensures that data assets are managed throughout an organization. These processes are everything an organization does to ensure data is private, accurate, secure, available, and usable. The processes people follow, the actions they must take, and the technology supporting them throughout the data life cycle are all included in data governance. Data stewards are an important part of data governance. Data stewards are subject matter experts who are responsible for collecting and managing data, and preserving the quality of the data. Data governance can be used to help maintain data quality. Usually, data quality is judged on six dimensions: accuracy, completeness, consistency, timeliness, validity, and uniqueness. Let’s examine each of these dimensions. Data is accurate when it can be confirmed with a verifiable source. For example, an accurate email address for a team member ensures they can be reached. Data is complete when there is enough data to deliver meaningful decisions and inferences. For instance, when completing an online form, some fields may be required while others are not. The data is complete when all the required fields have information. Data is consistent when data stored and used at multiple instances matches the values across various records. Suppose a business has more than one customer database. If a specific customer's name is spelled differently in each database, the data is not consistent. If the name is spelled the same way in each database, the data is consistent. Data timeliness means that the data is available when the user needs it. For example, if an auditor is conducting a review, the audit report data must be ready when the auditor needs to access it. Data is valid when value attributes are aligned with specific requirements. For instance, if a person is purchasing a product online, they need to input their credit card number. The number is valid if the correct number of characters, of a verified credit card number, are in the credit card number field in the payment form. And data is unique when there are minimal overlaps or duplicates. Consider a salon customer who uses an online form to make two appointments for the same service, at the same time, on the same day. If that customer record is duplicated in the system, then the record is not unique. Data quality means your data is complete, accurate, available when you need it, and fits with the organization’s requirements. Having formal governance processes in place to maintain the quality of data will help to ensure that your organization’s data maintains a high quality. For example, say you’re working on a data governance framework. You’ll want to make sure to create processes that address the accuracy of data, and ensure that your data is available when you need it. Establishing clear policies and procedures for data governance helps to protect and secure data. The policies and procedures in your governance plan or framework should cover data access, use, storage, and disposal, and should align with industry standards and regulatory requirements. For example, policies and procedures should prevent threat actors and other unauthorized users from accessing certain confidential data. There must also be policies and procedures in place, so that authorized users are able to gain access to the data. When you collect, keep, and use data in secure, and effective ways, you’re practicing data governance. Data governance can help you and your organization maintain high quality secure data that meets regulation and policy requirements. Now that you know more about how you can use data governance, you can apply those skills to maintain high quality standards and keep your data secure!

### Video - [Data sovereignty and data governance](https://www.cloudskillsboost.google/course_templates/968/video/483984)

* [YouTube: Data sovereignty and data governance](https://www.youtube.com/watch?v=KLEUtHKbtsM)

Cloud data can be stored in different physical locations around the world. As a cloud security professional, you’ll need to know where data is stored to ensure that it complies with required regulations and policies. In this video, you’ll learn about data sovereignty, data residency, and data localization. You’ll also explore some ways that data governance can help to ensure data sovereignty. Physical servers that store cloud data are kept in buildings called data centers. Data centers are located in many different countries. These countries have their own laws and regulations for data storage. This is where data sovereignty applies. Data sovereignty means data stored in a physical location has to follow the regulations of that geographic location. A country enforces its laws on the data systems, controllers, and processors by exercising data sovereignty. Data sovereignty also extends across borders and even has international implications. A country may refuse another country to exert control over its sovereign data. To comply with data sovereignty, data professionals need to know what data they have and where their data resides. Data residency refers to the physical or geographic location of an organization's data or information. Once data is moved, stored, or processed within a particular geographic location, it is subject to the laws, customs, and expectations of that specific location. Sometimes there’s a need to keep sensitive data within the borders of a particular country to enhance data security. This is called data localization. Data localization is the requirement that all data generated within a country's borders remain within those borders. Consider how this impacts the way an organization conducts business. An organization must research —and comply with— the data sovereignty laws of all geographic locations where they intend to operate. This ensures that the organization is in compliance. This is so important, because data localization helps countries safeguard the data of their citizens and organizations in their territory. Data governance helps organizations comply with and reinforces data sovereignty. These work in tandem. Remember, data governance is a set of processes that ensures that data assets are managed throughout an organization. Having data governance processes and procedures that address sovereignty risks, such as data that’s subject to the regulations of multiple governments, will help organizations remain compliant with relevant laws and regulations. An organization can take several actions to facilitate data governance. For example, an organization might conduct a data audit as part of their data governance. In the audit, they could identify where the data is stored, processed, and transmitted, and also the relevant data protection laws and regulations for those locations. This will help identify any potential data sovereignty risks and ensure that the organization is in compliance. The organization could also research and select cloud providers with data residency options that align with the organization’s data sovereignty requirements. They might also map out the regulations from each country that stores or might store data to find similarities and differences between the sovereignty requirements for each country. The organization could even create an organizational policy to use as a preventative guardrail. As a reminder, guardrails are the policies, procedures, and processes to manage and monitor an organization’s regulatory, legal, risk, environmental, and operational requirements. It’s often helpful for organizations to check with their cloud provider to figure out if there’s a policy the organization can use. For example, the cloud service can help the organization identify a resource’s location property, and also where it’s maintained and deployed. When an organization uses location constraints in their policy, they can limit where their resources are deployed and maintained to make sure that they meet sovereignty requirements. As a cloud security professional, you’ll need to work with data sovereignty, data localization, and data residency to keep your organization compliant with data regulations. Having strong data governance will help you to meet your sovereignty requirements.

### Document - [Data sovereignty challenges and strategies](https://www.cloudskillsboost.google/course_templates/968/documents/483985)

### Video - [Data discovery to support data governance](https://www.cloudskillsboost.google/course_templates/968/video/483986)

* [YouTube: Data discovery to support data governance](https://www.youtube.com/watch?v=rmLJ0cvhmVs)

In this video, we’ll explore data discovery. We’ll consider how data discovery relates to data governance and data sovereignty, and how data discovery works to keep sensitive data from being moved out of protective spaces. Let’s get started! Data discovery is the process of searching, identifying, and analyzing large amounts of data within an organization to uncover hidden patterns, relationships, and insights. Data discovery can identify risks and inform better decision making. For organizations, data discovery supports both data governance and data sovereignty. During data discovery, organizations can flag data that is subject to governmental sovereignty laws and regulations. Then, they can use data governance to ensure they’re in compliance with those regulations. Data transparency enables organizations to remain in compliance with data sovereignty laws and regulations. Organizations use data discovery to check for the dimensions of data quality, including the accuracy, completeness, consistency, timeliness, validity, and uniqueness of data. Each of these dimensions factor into how organizations use data for different purposes, but completeness, timeliness, and accuracy especially affect the ways organizations ensure transparency. Transparency is one benefit of data discovery. In data discovery, an organization’s data may come from many different data sources and be stored by cloud service providers. The organization needs to keep track of this data and know how the data is collected, where it is collected from, and where it’s stored. Data transparency ensures that data is available and searchable. For security, discovery of sensitive data is particularly important. Sensitive data discovery is finding and identifying all confidential and regulated information kept by an organization. This includes any data that’s regulated by law, like confidential payment data and PHI. It also includes finding data related to intellectual property. For sensitive data, it's important to know what kind of data it is and where it’s located to ensure that it’s properly protected. As a cloud security professional, there are tools that you can use to find and protect sensitive data. One tool that we’ll explore is a data loss prevention engine. At various stages of the data lifecycle, you can use a data loss prevention engine -also known as a DLP engine- to filter and search for sensitive data. You can also use DLP to inspect, mask, or remove sensitive data once it’s discovered. Doing so may lower the risks inherent to certain data types. The DLP can also be utilized to complement encryption for data at rest, manage access control, and other security measures. You can also examine data closely with DLP. After, you might decide to apply security controls based on the data’s sensitivity, or move the data to a less restricted area. If you move data, during a cloud migration for example, you can use data discovery to validate whether the data is being moved into places that are properly secured. Data discovery also checks to make sure there are no mistakes in data classification and placement. This prevents regulated data from being moved into an environment without proper controls in place. Now that you know about data discovery -and how data discovery supports data governance and data sovereignty- you have the info you need to identify and protect sensitive data wherever it is.

### Video - [Data retention policies](https://www.cloudskillsboost.google/course_templates/968/video/483987)

* [YouTube: Data retention policies](https://www.youtube.com/watch?v=1HWtEFatCJ4)

Everything needs somewhere to go, whether it’s forks and spoons in a kitchen drawer, or data in the cloud. And to find a spoon, you know which drawer it’s stored in. Similarly, security professionals need access to well-organized data, and they need to know where to find it. That’s where storage becomes useful! In this video, you’ll learn about how data is stored, and how a data retention policy can help you manage your data and follow regulations. Let’s get started! Data retention is the process of storing data, including how long it needs to be stored. In cloud storage, the length of time that data is stored is determined by an organization’s needs. The time period for stored data is called a retention period. A data retention period is the length of time an organization keeps information. It’s also a key element of an organization’s data retention policy. A data retention policy helps determine: What data needs to be stored, and where the data should be stored. Because it includes an expiration period, a retention policy can also help decide how long the data can be stored. These considerations need to follow compliance regulations, which are often outlined in an organization's compliance policy. And, having a data retention policy will help ensure that data is compliant and safe. As a security professional, you may be responsible for working on data retention policies. As part of this process, you’ll need to conduct research before you start creating the policy. While conducting research, you’ll need to make compliance considerations, like: industry or regional compliance standards for data, and regulations for the location of the data. Take notes while you research. Then, consult the legal staff members on your team for insights on your research findings. While you’re preparing to build your data retention policy, consider the following questions: What are your organization’s needs for specific data? What are your organization’s policies for data retention periods? Do organizational needs change over time? Do different data types need different policies? A data retention policy should be flexible, and be based on an organization’s needs. Keep your organization’s compliance policy in mind as you build your data retention policy. Try to ensure that both policies work together, that they’re not contradictory, and that they’re clear to members of the organization. For example, if your organization has a compliance policy that states user data is only retained for a short time, make sure your retention policy reflects that. As you create policy, here’s a pro tip: Policy creation is also affected by what type of resource is used to store data, and the applications you allow to access the data. You’re responsible for keeping your applications and hardware secure. Vendors may not provide regular security updates, so it’s important to understand the limitations and capabilities of applications and hardware when creating your policies. Organizations also need to consider risk when creating data retention policies. When data is available, there’s a potential risk of a data breach. This means attackers can steal your data while it’s available, including sensitive data. To defend against this risk, it’s useful to delete or replace data over time. Let’s review how risk consideration works. Retention policy is defined by time. First, you add a retention policy to a bucket. In cloud storage, a bucket is a virtual container that holds objects. Then, you can define the age of the objects that can be deleted or replaced by specifying the retention period. It’s important to know that you can only delete or replace those objects once they reach the age you define. If you try to delete the objects before that time, you’ll get an error. You can set your retention policy in units of days. You can also permanently set a retention policy by locking it. This means the policy is locked in and stays for the life of the bucket. To delete the bucket, you’ll need to wait until every object has completed its retention period, since the objects’ age must be greater than the retention period duration. Understanding how a data retention policy keeps data compliant and safe allows you to create and implement a plan for your organization.

### Quiz - [Test your knowledge: Data sovereignty and data governance](https://www.cloudskillsboost.google/course_templates/968/quizzes/483988)

### Document - [Plan for business continuity](https://www.cloudskillsboost.google/course_templates/968/documents/483989)

### Video - [Create a business continuity plan](https://www.cloudskillsboost.google/course_templates/968/video/483990)

* [YouTube: Create a business continuity plan](https://www.youtube.com/watch?v=GOikjIoDQ_I)

Imagine a software company is using cloud resources to host their applications when a large storm strikes a data center, taking the company’s resources offline. It’s important for the business -and its customers- to get the cloud environment back up and running quickly. That’s where a business continuity plan comes in. A business continuity plan, also called a BCP, is a document that outlines the procedures to sustain business operations during and after a significant disruption. BCPs can minimize damage from disruptions like natural disasters or cyber attacks. A disaster recovery plan, or DRP, is an important part of each business’ continuity plan. A DRP is a plan that allows an organization’s security team to outline the steps needed to minimize the impact of a security incident. While BCPs focus on keeping a business going during a disruption, DRPs focus on recovering assets and data after the disruption. Let’s explore some impacts that business continuity planning can help prevent or mitigate. Business continuity planning can decrease the financial impact of a disruption, since it helps an organization get back to normal quickly. It can also mitigate brand impact and reputation loss. The sooner an organization recovers from a disruption, the less it affects their customers. And that keeps customers happy with the organization and likely to keep using its services. Let’s explore the steps to develop a business continuity plan. Departments throughout an organization usually collaborate to create a BCP, with each department planning for their own operations. Leadership helps to keep the plan in line with the company’s goals. The first step in building a business continuity plan is to determine the most critical apps and data for business function. You can organize these resources into tiers based on importance. For example, let’s say you’re helping create the BCP for your employer, a financial institution. If your online banking app goes down, your customers won’t be able to access their accounts from their phones -this means the banking app is a high priority. By contrast, let’s say the financial institution stores archived employee newsletters in a cloud storage bucket. If the storage bucket goes offline, your company can still continue its operations and your customers won’t be affected: It’s a lower priority. After ranking the most important apps and data, the second step is to identify acceptable amounts of downtime for each one. You can do this by assigning recovery time objectives. A recovery time objective, or RTO, is the target time allowed for the recovery of a service in the event of a disaster. The services most critical to your organization’s operations should have the shortest RTOs. You should also assign recovery point objectives to your applications. A recovery point objective, or RPO, is the maximum acceptable length of time during which data might be lost from an application due to a major incident. The longer an application is offline, the more data will be lost, so you should assign the shortest RPOs to the most critical data. The third step in developing a business continuity plan is to conduct a risk assessment to identify what risks could affect your resources. Next up, the fourth step is to document your disaster recovery plan. This plan should include solutions to recover your operations as soon as possible, including steps to restore backup data. It should clearly define each team member’s role in the event of a disruption. You should include contact information for your cloud service provider, backup providers, and key personnel who will be involved in restoring operations. After developing a BCP, the fifth step is to communicate the plan with others on the security team and throughout the organization, so everyone is familiar with how their role fits within the overall strategy. Finally, the sixth step is to regularly test and update both your BCP and your DRP. Testing should include simulation drills of different scenarios with the security team and throughout the organization. You’ll want your plan to cover any type of event, from small malfunctions to complete shutdowns. Business continuity planning is an ongoing job. It’s important to update business continuity plans regularly to make sure they keep up with any changes to the business or its IT infrastructure. I hope you had fun learning to create an effective business continuity plan! You’re ready to help your organization keep operations going, no matter what!

### Video - [Business continuity scenario](https://www.cloudskillsboost.google/course_templates/968/video/483991)

* [YouTube: Business continuity scenario](https://www.youtube.com/watch?v=Po6Fs6TT-_0)

Imagine you’re attempting to access your organization’s cloud data storage. Suddenly, a message appears telling you that sensitive customer data has been encrypted and you need to pay a large sum of money to unlock it. You realize the data storage has been infected with ransomware. This ransomware attack could cause a major disruption for your organization if you don’t handle it properly. Fortunately, you have a business continuity plan. Let’s explore how business continuity planning can help the company solve this problem. After you alert colleagues to the ransomware attack, your team can begin carrying out the business continuity plan. Your plan identifies the most important data and services to your organization’s operation. Your business plan also lists acceptable amounts of downtime for each of those resources. Because the ransomware attack affected sensitive customer data, your plan only allows for one hour of downtime. Your risk assessment identified that a ransomware attack could affect your data storage, so you have a disaster recovery plan in place. The disaster recovery plan portion of your BCP contains information about each team member’s role. According to your plan, your colleague is responsible for communicating with other departments to make sure everyone knows what’s going on. Another colleague is responsible for shutting down cloud resources affected by the ransomware. Meanwhile, you are responsible for getting the data back online. Your disaster recovery plan also includes recovery solutions for restoring apps and data. Because the data affected by the ransomware has a short acceptable downtime, your team has worked with your cloud service provider to back it up to another site. Your plan contains cloud service provider contact information, or CSP contact information. You’re able to reach out to your CSP and access the backed-up data, which hasn’t been affected by the ransomware. Since your team has been regularly updating your BCP and training for all types of disruptions -including a ransomware attack- everything goes smoothly and the data is only unavailable for a few minutes. Disruptions happen, and they can require significant effort from you and your team. But, with a proper business continuity plan in place, you and your team can minimize disruptions efficiently, effectively, and with confidence!

### Quiz - [Test your knowledge: Business continuity in cloud computing](https://www.cloudskillsboost.google/course_templates/968/quizzes/483992)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/968/video/483993)

* [YouTube: Wrap-up](https://www.youtube.com/watch?v=q-JKWJ8m46w)

Congratulations! You’ve completed your journey through data protection and privacy. Well done! You learned about encryption, data classification, and data governance. And, you explored the steps to building a business continuity plan. You started with the basics of data protection and privacy. Then, you learned about data encryption at rest, in transit, and in use. Next, you learned the importance of data classification and tagging to data security. You also explored data discovery, and how it supports both data governance and data sovereignty. You learned how to keep data storage compliant and secure by creating a retention policy. Then, you learned about business continuity planning, including disaster recovery planning. And finally, you explored how using a business continuity plan can help your organization deal with an emergency. Congratulations on getting here, you’re doing a phenomenal job. Keep going. You got this!

### Video - [Lauren: What makes candidates stand out](https://www.cloudskillsboost.google/course_templates/968/video/483994)

* [YouTube: Lauren: What makes candidates stand out](https://www.youtube.com/watch?v=LNyMwfAWhd0)

Confidence is very important, and something that one of my first managers ever told me, it's not what you know, it's how you say it. And it's something that I continue to tell myself day after day. My name is Lauren, and I'm a cloud sourcing lead. What that means is my team is responsible for reaching out to candidates for our open roles within the cloud organization. The three top things that a candidate should do when updating or creating their resume is first tailor your resume to the job description. You wanna make sure you're pointing out keywords and key indicators that match your experience to ensure that the recruiters can see pretty quickly the experience that you have that's relevant to the job description. Secondly, do not exaggerate or do not lie on your resume. That probably goes without saying, but anything on your resume is technically fair game. It can be asked in an interview, so you wanna make sure that you're being as accurate as possible. My third tip would be, keep it concise, keep it short. We definitely wanna make sure that we can get all of your experience, but you can cut things out. That's okay too, if it's not relevant to the job. What makes a resume stand out for a recruiter or a hiring manager is an organized resume. You wanna make sure you have clear headers, it's in chronological order, and you have short and concise bullet points. Some ways that candidates can demonstrate their technical skills in interviews is to talk out their thought process in the answer to the questions they are asked. The process of how you get there is just as important as getting to the correct answer. Some common interview question types are problem-solving-based questions, experience-based questions and also leadership-based questions. The best way to answer a hypothetical question is really just talking through your thought process. You're gonna get a scenario, something that you may or may not have experienced before. And your interviewer just wants to see your thought process, how you're connecting the dots as you're talking through your answer. The best way to answer a behavioral-based interview question is really leaning on your own personal experience. This could be a personal experience from a previous job, from school, from a class, and talking through that experience and that thought process. It's very important to ask follow-up questions throughout the interview. First and foremost, you wanna make sure you're understanding the question that's being asked. You have a very limited amount of time with your interviewer and you wanna make sure that you're spending it as productively as possible. I think some candidates worry that it's bad to ask questions. I love when candidates ask me questions. I wanna make sure that my candidate knows what I'm asking so we can get to the best solution. Workplace skills are absolutely important during the interview process. These are skills that can't be taught. Are you kind? Are you empathic? How are you showing up? Do you have that eagerness to learn or to thrive in ambiguity? We really wanna see candidates that have that self-drive and that self-determination. A candidate can best position those workplace skills just on how they're simply answering their questions. Especially in those behavioral questions, lean on those examples that show your grit, that show maybe a struggle that you had and how you overcame it. I'm not looking specifically for something that happened perfectly the right way. I'm looking for someone that maybe had to overcome some obstacles to get to the right solution. Candidates hold a lotta power in the interview process. We want you to be successful. We want reasons to say yes. So please show up, be your authentic self, be confident, and show us all the amazing skills that you could bring to the company.

### Video - [Patrick and Brenda: Interview role play](https://www.cloudskillsboost.google/course_templates/968/video/483995)

* [YouTube: Patrick and Brenda: Interview role play](https://www.youtube.com/watch?v=eoHBH-4VXJU)

Hi, I'm Patrick. Hi, I'm Brenda. Let's step into an interview in progress. In this interview, questions will focus on cloud cybersecurity topics relating to identifying and detecting threats. We hope this will give you some tips for your next interview. Hi, Brenda. What interests you about roles in cloud cybersecurity? I would say what interests me the most about jobs in cloud cybersecurity is the fact that you're continuously learning. As a whole with security and with cloud specifically, there's always new features, new services that you have to learn. And as security practitioners, you always want to essentially learn and know how to protect them. And so as a whole, I'm attracted to the fact that you're continuously learning. Let's say you're working on a project to help a new company implement access control processes. What are some cloud access control processes you might recommend? So some of the cloud access processes that I would recommend in this instance are, for example, perimeter protection and also trust boundaries. And so with the first, with perimeter protection, this is essentially your security measures that you wanna take into place or you wanna put in place to essentially prevent unauthorized access. And so some of these might be like firewalls, virtual private network, intrusion detection systems or intrusion prevention systems. With the second, with trust boundaries, that's essentially trying to separate or separate your trusted and untrusted environments. And so this can be done with something like firewalls or routers to essentially set that logical and also physical separation. What are some examples of trust boundaries that you could delineate and kind of isolate based on? So essentially a very simple example would be trying to separate the public internet from your corporate intranet. And that can be one instance where again, you would have more of a physical and also logical separation. What impact did you have on the project and what impact did that project have on the company? The impact that I had on the project was essentially to act as one of the security gates to make sure that there were proper security requirements put in place and practices before we actually went through with full migration. As far as impact for the overall organization and company, this was a big move for us and the fact that we had to have so many teams coordinate together was essentially all for the goal of getting to a more modern kind of cloud architecture. Imagine you're asked to explain the key components of a threat and vulnerability management strategy to a business leader with not a lot of technical experience. What components would you choose and what would you say about each one? So there are four components that I would highlight to explain a effective threat vulnerability management strategy. The first being prevention, the second being detection, the third being response, and the fourth being recovery. So with the first component, this is the part where you essentially wanna prevent any kind of unauthorized access to begin with. You wanna prevent that cyber attack. And so here you wanna make use of endpoint protection software, virtual private networks, et cetera. Then we move on to the second component where this is all about detection, and this is going to be monitoring efforts where you essentially wanna set up things like event management systems, and you wanna make sure you have proper log analysis. And so here is where we wanna detect the malicious activity and also alert on it. Once we alert on that, then we move on to the third component, which is going to be response. And so with response, this is where we want to not only identify the threats but also contain it. And so we'll make use of incident response teams and plans as well as other emergency response teams. Moving on to the fourth and final component, this is going to be recovery, and so here we'll want to make use of any kind of disaster recovery plans or teams and get back to a state before the attack. And that will be not only for your systems, but also potentially for data. Could you gimme an example of the impact that each of those components has? With the first one where we essentially want to prevent, the impact as a whole here is to reduce your attack surface. And so as a whole, you want to have security measures so that attackers just have less avenues to actually get into your environments. For the second component, for the overall detection, again here the goal is to monitor. And so the benefit and the overall impact is that you actually have the proper logging to actually detect whatever malicious activity comes your way to your environment. For the third component, that's going to be again, the response part. And so you wanna make sure you have the proper teams and plans set in place so that the overall company will benefit by having actions readily available. For the fourth and final component of recovery, again, this goes back to what plans do you have already in place because a company can't really have too much downtime. You need to make sure you have essentially steps in order to get back to the pre attack state and again, get your shop up and running. Thanks, Brenda, do you have any questions for me? Yes, I do. So what would you say are some of the toughest challenges that you see in environments relating to access control? When we think about access control, there's a really big difference between what is mandatory and what is discretionary. And so being able to assert that a control is actually mandatory and that there aren't ways to bypass it or back doors or ways that you know, teams, customers, and malicious actors can route around those controls is probably the biggest challenge at scale. That makes sense. I can see how something like Terraform would help enforce this. Thank you so much. Thanks for watching. In this scenario, Brenda demonstrated how to explain impact when answering questions. In interviews, be sure to share the beneficial effects of any process, concept, or tool that you discuss. Whether it's deploying a concept to identify a particular threat or deploying a tool to protect against a particular threat, explaining the potential impact will help you stand out to an interviewer. Keep watching for more interview tips.

### Document - [Interview tip: Explain impact](https://www.cloudskillsboost.google/course_templates/968/documents/483996)

### Document - [Glossary terms from module 4](https://www.cloudskillsboost.google/course_templates/968/documents/483997)

### Quiz - [Module 4 challenge](https://www.cloudskillsboost.google/course_templates/968/quizzes/483998)

### Video - [Course wrap-up](https://www.cloudskillsboost.google/course_templates/968/video/483999)

* [YouTube: Course wrap-up](https://www.youtube.com/watch?v=1WcNuSI31c4)

Great work finishing this course! I love being in a field where I can use my skills to protect users, data, and systems daily. I work behind the scenes to identify and mitigate threats, and I'm always learning about the new and novel tools and techniques attackers use to gain access to information. If you're interested in a career in cybersecurity, I encourage you to pursue it. It's a challenging and rewarding field, and we need more talented people like you to help make the world a safer place. Together, we can make a difference! Let’s go through all the things you learned in this course. You started by learning about identity management principles and the implementation of effective access controls in cloud environments. You also examined the concept of trust boundaries in cloud security. Then, you compared two widely-used security models: traditional perimeter security and zero trust. Next, you explored best practices for managing threats and vulnerabilities in the cloud, including asset management, vulnerability remediation, and posture management. And you learned about the cloud-native concepts of rehydration and reinstantiation. You also explored the key cloud concepts of ephemerality and immutability. You learned how infrastructure as code (IaC) and policy as code (PaC) let you build security into your cloud environment. You learned all about containers, including container orchestration and security. Then, you learned about data protection and privacy, including data sovereignty and data governance. Finally, you learned how business continuity planning can prepare you to deal with disruptions to your organization’s cloud resources. You now know all about the essential aspects of securing digital assets in the cloud. And, you have a well-rounded understanding of the tools and techniques needed to protect your organization's data and infrastructure. Great work throughout this course. Congratulations on this milestone!

### Document - [Course 3 resources and citations](https://www.cloudskillsboost.google/course_templates/968/documents/484000)

### Document - [Glossary terms from course 3](https://www.cloudskillsboost.google/course_templates/968/documents/484001)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
