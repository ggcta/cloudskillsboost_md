---
id: 54
name: 'Modernizing Data Lakes and Data Warehouses with Google Cloud'
datePublished: 2025-01-17
topics: []
type: Course
url: https://www.cloudskillsboost.google/course_templates/54
---

# [Modernizing Data Lakes and Data Warehouses with Google Cloud](https://www.cloudskillsboost.google/course_templates/54)

**Description:**

The two key components of any data pipeline are data lakes and warehouses. This course highlights use-cases for each type of storage and dives into the available data lake and warehouse solutions on Google Cloud in technical detail. Also, this course describes the role of a data engineer, the benefits of a successful data pipeline to business operations, and examines why data engineering should be done in a cloud environment.

This is the first course of the Data Engineering on Google Cloud series. After completing this course, enroll in the Building Batch Data Pipelines on Google Cloud course.

**Objectives:**

- Differentiate between data lakes and data warehouses.
- Explore use-cases for each type of storage and the available data lake and warehouse solutions on Google Cloud.
- Discuss the role of a data engineer and the benefits of a successful data pipeline to business operations.
- Examine why data engineering should be done in a cloud environment.

## Introduction

This module introduces the Data Engineering on Google Cloud source series and this Modernizing Data Lakes and Data Warehouses with Google Cloud course.

### Video - [Course series introduction](https://www.cloudskillsboost.google/course_templates/54/video/521373)

- [YouTube: Course series introduction](https://www.youtube.com/watch?v=XqjFHLo1VEk)

Damon: Hello, and welcome to the Data Engineering on Google Cloud course series. I'm Damon, and I'm a technical curriculum developer at Google. Together with my fellow instructors, we look forward to showing you how to design data processing systems, build end to end data pipelines, analyze data, and implement machine learning. In addition to video lectures, you will also complete a series of hands-on labs. As part of the data engineering learning path, we will first discuss the differences between data lakes and data warehouses, the two key components of any data pipeline. This course highlights use cases for each type of storage, and dives into the available data lake and data warehouse solutions on Google Cloud in technical detail. Also, this course describes the role of a data engineer, the benefits of a successful data pipeline to business operations, and examines why data engineering should be done in a Cloud environment. Data pipelines typically fall under one of the extract load, extract load transform, or extract transform load paradigms. So the next course, building batch data pipelines, describes which paradigm should be used and when for batch data. Furthermore, it covers several technologies on Google Cloud for data transformation, including BigQuery, executing spark on data proc, pipeline graphs and data fusion, and serverless data processing with Dataflow. Processing streaming data is becoming increasingly popular as streaming enables organizations to get real time metrics on operations. So the third course covers how to build streaming data pipelines on Google Cloud. Pub Sub is the primary product for handling incoming streaming data. The course also covers how to apply aggregations and transformations to streaming data using Dataflow, and how to store or process records in BigQuery, or Big Table for analysis. Incorporating machine learning into data pipelines increases the ability of organizations to extract insights from their data. The final course covers several ways for machine learning to be included in data pipelines on Google Cloud depending on the level of customization required. For little to no customization, the course covers Auto ML. For more tailored machine learning capabilities, the course introduces notebooks and BigQuery machine learning. Also, the final course covers how to productionize machine learning solutions using Kubeflow.

### Video - [Course introduction](https://www.cloudskillsboost.google/course_templates/54/video/521374)

- [YouTube: Course introduction](https://www.youtube.com/watch?v=f_4LPqS6aZA)

Damon: Welcome to modernizing data lakes and data warehouses with Google Cloud, the first course of the data engineering learning path. We'll start off by describing the role of a data engineer. We'll talk about a data engineer's clients and what the benefits of a successful data pipeline are for your organization. Also, we will explain why data engineering should be done in a Cloud environment. We'll concentrate on data lakes and data warehouses in this course, these are the two key components of any data pipeline. We'll describe the differences between data lakes and data warehouses, and highlight use cases for each type of storage. Also, we'll go into the available data lake and data warehouse solutions on Google Cloud in some technical detail. Finally, you'll get hands-on experience with data lakes and data warehouses by using quick labs.

## Introduction to Data Engineering

This module discusses the role of data engineering and motivates the claim why data engineering should be done in the Cloud

### Video - [Module introduction](https://www.cloudskillsboost.google/course_templates/54/video/521375)

- [YouTube: Module introduction](https://www.youtube.com/watch?v=AkTPAzLsTNI)

>> In this introduction to data engineering module, we'll describe the role of a data engineer and motivate the claim why data engineering should be done in the Cloud. A data engineer is someone who builds data pipelines. And so we'll start by looking at what this means, what kinds of pipelines a data engineer builds and their purpose. We'll look at the challenges associated with the practice of data engineering, and how many of those challenges are easier to address when you build your data pipelines in the Cloud. Next, we'll introduce you to BigQuery, Google Cloud's petabyte scale serverless data warehouse. Having defined what data lakes and data warehouses are, we'll then discuss these in more detail. Data engineers may be responsible for both the backend transactional database systems that support a company's applications, and the data warehouses that support their analytic workloads. In this lesson, we'll explore the differences between databases and data warehouses, and the Google Cloud's solutions for each of these workloads. Since a data warehouse also serves other teams, it's crucial to learn how to partner effectively with them. As part of being an effective partner, your engineering team will be asked to set up data access policies and overall governance of how data is to be used and not used by your users. We'll discuss how to provide access to the data warehouse while keeping to data governance best practices. We'll also discuss productionizing the whole operation and automating and monitoring as much of it as possible. Finally, we'll look at a case study of how a Google Cloud customer solved a specific business problem before you complete a hands-on lab where you will use BigQuery to analyze data.

### Video - [The role of a data engineer](https://www.cloudskillsboost.google/course_templates/54/video/521376)

- [YouTube: The role of a data engineer](https://www.youtube.com/watch?v=FDsjPLtGEuk)

person: Let's start by exploring the role of a data engineer in a little more detail. What does a data engineer do? A data engineer builds data pipelines. Why does the data engineer build data pipelines? Because they want to get their data into a place, such as a dashboard or report or machine learning model, from where the business can make data-driven decisions. The data has to be in a usable condition so that someone can use this data to make decisions. Many times, the raw data is, by itself, not very useful. One term you will hear a lot when you do data engineering is the concept of a data lake. A data lake brings together data from across the enterprise into a single location. So you might get the data from a relational database or from a spreadsheet and store the raw data in a data lake. One option for this single location to store the raw data is to store it in a Cloud Storage bucket. What are the key considerations when deciding between data lake options? What do you think? There are some considerations that you need to keep in mind as you build a data lake. Does your data lake handle all the types of data you have? Can it all fit into a Cloud Storage bucket? If you have an RDBMS, you might need to put the data in Cloud SQL, a managed database, rather than Cloud Storage. Can it elastically scale to meet the demand? As your data collected increases, will you run out of disk space? This is more a problem with on-premises systems than with Cloud. Does it support high-throughput ingestion? What is the network bandwidth? Do you have edge points of presence? Is there fine-grained access control to objects? Do users need to seek within a file, or is it enough to get a file as a whole? Cloud Storage is blob storage, so you might need to think about the granularity of what you store. Can other tools connect easily? How do they access the store? Don't lose sight of the fact that the purpose of a data lake is to make data accessible for analytics. We mentioned our first Google Cloud product, the Cloud Storage bucket, which is a good option for staging all of your raw data in one place before building transformation pipelines into your data warehouse. Why choose Google Cloud Storage? Commonly, businesses use Cloud Storage as a backup and archival utility for their businesses. Because of Google's many data center locations and high network availability, storing data in a Cloud Storage bucket is durable and performant. For a data engineer, you will often use a Cloud Storage bucket as part of your data lake to store many different raw data files, such as CSV, JSON, or Avro. You could then load or query them directly from BigQuery as a data warehouse. Later in the course, you'll create Cloud Shell buckets using the Cloud console and command line, like you see here. Other Google Cloud products and services can easily query and integrate with your bucket once you've got it set up and loaded with data. Speaking of loading data, what if your raw data needs additional processing? You may need to extract the data from its original location, transform it, and then load it in. One option is to carry out data processing. This is often done using Dataproc or Dataflow. We'll discuss using these products to carry out batch pipelines later in this course. But what if batch pipelines are not enough? What if you need real-time analytics on data that arrives continuously and endlessly? In that case, you might receive the data in Pub/Sub, transform it using Dataflow, and stream it into BigQuery. We'll discuss streaming pipelines later in this course.

### Video - [Data engineering challenges](https://www.cloudskillsboost.google/course_templates/54/video/521377)

- [YouTube: Data engineering challenges](https://www.youtube.com/watch?v=7L-AkuQvycA)

>> Let's look at some of the challenges that a data engineer faces. As a data engineer, you'll usually encounter a few problems when building data pipelines. You might find it difficult to access the data that you need, you might find that the data, even after you access it, doesn't have the quality that's required by the analytics or machine learning model. You plan to build a model, and even if the data quality exists, you might find that the transformations require computational resources that might not be available to you. And finally, you might run into challenges around query performance, and being able to run all of the queries and all of the transformations that you need with the computational resources that you have. Let's take the first challenge of consolidating disparate datasets, data formats and managing access at scale. For example, you want to compute the customer acquisition cost, how much does it cost in terms of marketing and promotions and discounts to acquire a customer? That data might be scattered across a variety of marketing products and customer relationship management software. And finding a tool that can analyze all of this data might be difficult, because it might come from different organizations, different tools, and different schemas, and maybe some of that data is not even structured. So in order to find something as essential to your business as how much getting a new customer costs so that you can figure out what kind of discounts to offer to keep them from turning, you can't have your data exist in silos. So what makes data access so difficult? Primarily, this is because data in many businesses is siloed by departments, and each department creates its own transactional systems to support its own business processes. So for example, you might have operational systems that correspond to store systems, have a different operational system maintained by your product warehouses that manages your inventory, and have a marketing department that manages all the promotions given that you need to do an analytic query on, such as, give me all the in store promotions for recent orders and their inventory levels. You need to know how to combine data from the stores, from the promotions, and from the inventory levels. And because these are all stored in separate systems, some of which have restricted access, building an analytic system that uses all three of these datasets to answer an ad hoc query like this can be very difficult. The second challenge is that cleaning, formatting, and getting the data ready for insights requires you to build ETL pipelines. ETL pipelines are usually necessary to ensure data accuracy and quality. The cleaned and transformed data are typically stored not in a data lake but in a data warehouse. A data warehouse is a consolidated place to store the data and all the data are easily joinable and queryable. Unlike a data lake where the data is in the raw format, in the data warehouse, the data is stored in a way that makes it efficient to query. Because data becomes useful only after you clean it up, you should assume that any raw data that you collect from source systems need to be cleaned and transformed. And if you are transforming it, you might as well transform it into a format that makes it efficient to query. In other words, ETL the data and store it in a data warehouse. Let's say you're a retailer, and you have to consolidate data from multiple source systems. Think about what the use case is. Suppose the use case is to get the best performing in store promotions in France, you need to get the data from the stores and you have to get the data from the promotions. But perhaps the stored data is missing information. Maybe some of the transactions are in cash. And for those, perhaps there is no information on who the customer is. Or some transactions might be spread over multiple receipts, and you might need to combine those transactions because they come from the same customer. Or perhaps the timestamps of the products are stored in local time, whereas you have to spread across the globe. And so before you do anything, you need to convert everything into UTC. Similarly, the promotions may not be stored in the transaction database at all. They might be just a text file that somebody loads on their web page and has a list of codes that are used by the web application to apply discounts. It can be extremely difficult to do a query like finding the best performing in store promotions because the data has so many problems. Whenever you have data like this, you need to get the raw data and transform it into a form with which you can actually carry out the necessary analysis. It is obviously best if you can do this sort of cleanup and consolidation just once and store the resulting data to make further analysis easy. That's the point of a data warehouse. If you need to do so much consolidation and cleanup, a common problem that arises is where to carry out this compute. The availability of computation resources can be a challenge. If you're on an on-premises system, data engineers will need to manage server and cluster capacity and make sure that enough capacity exists to carry out the ETL jobs. The problem is that the compute needed by these ETL jobs is not constant over time. Very often it varies week to week, and depending on factors like holidays and promotional sales. This means that when traffic is low, you're wasting money, and when traffic is high, your jobs are taking way too long. Once your data is in your data warehouse, you need to optimize the queries your users are running to make the most efficient use of your compute resources. If you're managing an on-premise data analytics cluster, you will be responsible for choosing a query engine and installing the query engine software and keeping it up to date as well as provisioning any more servers for additional capacity. Isn't there a better way to manage server overhead so we can focus on insights?

### Video - [Introduction to BigQuery](https://www.cloudskillsboost.google/course_templates/54/video/521378)

- [YouTube: Introduction to BigQuery](https://www.youtube.com/watch?v=J1neqizXri0)

There is a much better way to manage server overhead so we can focus on insights. It is to use a serverless data warehouse. BigQuery is Google Cloud’s petabyte-scale serverless data warehouse. You don’t have to manage clusters. Just focus on insights. The BigQuery service replaces the typical hardware setup for a traditional data warehouse. That is, it serves as a collective home for all analytical data in an organization. Datasets are collections of tables that can be divided along business lines or a given analytical domain. Each dataset is tied to a Google Cloud project. A data lake might contain files in Cloud Storage or Google Drive or even transactional data from Bigtable. BigQuery can define a schema and issue queries directly on external data as federated data sources. Database tables and views function the same way in BigQuery as they do in a traditional data warehouse, allowing BigQuery to support queries written in a standard SQL dialect which is ANSI: 2011 compliant. Identity and Access Management is used to grant permission to perform specific actions in BigQuery. This replaces the SQL GRANT and REVOKE statements that are used to manage access permissions in traditional SQL databases. A key consideration behind agility is being able to do more with less, and it’s important to make sure that you're not doing things that don’t add value. If you do work that is common across multiple industries, it's probably not something that your business wants to pay for. The cloud lets you, the data engineer, spend less time managing hardware and more time doing things that are much more customized and specific to the business. You don’t have to be concerned about provisioning and reliability and utilization improvements in performance or tuning on the cloud, so you can spend all your time thinking about how to get better insights from your data. You don't need to provision resources before using BigQuery, unlike many RDBMS systems. BigQuery allocates storage and query resources dynamically based on your usage patterns. Storage resources are allocated as you consume them and deallocated as you remove data or drop tables. Query resources are allocated according to query type and complexity. Each query uses some number of slots, which are units of computation that comprise a certain amount of CPU and RAM.

### Video - [Data lakes and data warehouses](https://www.cloudskillsboost.google/course_templates/54/video/521379)

- [YouTube: Data lakes and data warehouses](https://www.youtube.com/watch?v=knqB_U0yJQ0)

We've defined what a data lake is and what a data warehouse is. Let's look at these in a bit more detail. Recall that we emphasized that the data has to be in a usable condition so that someone can use this data to make decisions. Many times, the raw data is by itself not very useful. We said that raw data gets replicated and stored in a data lake. In order to make that data usable, you will use Extract Transform Load or ETL pipelines to make the data usable and store this more usable data in a data warehouse. Let's consider what are the key considerations when deciding between data warehouse options? We need to ask ourselves these questions. The data warehouse is going to definitely serve as a sink, you're going to store data in it. But will it be fed by a batch pipeline or by a streaming pipeline? Does the warehouse need to be accurate up to the minute? Or is it enough to load data into it once a day or once a week? Will the data warehouse scale to meet my needs? Many cluster based data warehouses will set per cluster concurrent query limits. Will those query limits cause a problem? Will the cluster size be large enough to store and traverse your data? How is the data organized, cataloged and access controlled? Will you be able to share access to the data to all your stakeholders? What happens if they want to query the data? Who will pay for the querying? Is the warehouse designed for performance? Again, carefully consider concurrent query performance and whether that performance comes out of the box, or whether you need to go around creating indexes and tuning the data warehouse. Finally, what level of maintenance is required by your engineering team? Traditional data warehouses are hard to manage and operate. They were designed for a batch paradigm of data analytics and for operational reporting needs. The data in the data warehouse was meant to be used by only a few management folks for reporting purposes. BigQuery is a modern data warehouse that changes the conventional mode of data warehousing. Here, we can see some of the key comparisons between a traditional data warehouse and BigQuery. BigQuery provides mechanisms for automated data transfer, empowers business applications using technology that teams already know and use, so everyone has access to data insights. You can create read-only shared data sources that both internal and external users can query, and make query results accessible for anyone through user-friendly tools such as Looker, Google Sheets, Tableau, or Google Data Studio. BigQuery lays the foundation for AI, it's possible to train TensorFlow and Google Cloud Machine Learning models directly with datasets stored in BigQuery, and BigQuery ML can be used to build and train machine learning models with simple SQL. Another extended capability is BigQuery GIS, which allows organizations to analyze geographic data in BigQuery essential to many critical business decisions that revolve around location data. BigQuery also allows organizations to analyze business events real time as they unfold by automatically ingesting data and making it immediately available to query in their data warehouse. This is supported by the ability of BigQuery to ingest up to 100,000 rows of data per second and for petabytes of data to be queried at lightning fast speeds. Due to Google's fully managed serverless infrastructure and globally available network, BigQuery eliminates the work associated with provisioning and maintaining a traditional data warehousing infrastructure. BigQuery also simplifies data operations through the use of identity and access management to control user access to resources, creating roles and groups and assigning permissions for running jobs and queries in a project and also providing automatic data backup and replications. Even though we talked about getting data into BigQuery by running ETL pipelines, there is another option. That is to treat BigQuery as a query engine, and allow it to query the data in place. For example, you can use BigQuery to directly query database data in Cloud SQL, that is, managed relational databases like Postgres SQL and MySQL. You can also use and MySQL. You can also use BigQuery to directly query files on Cloud Storage as long as these files are in formats like CSV or Parquet. The real power comes when you can leave your data in place and still join it against other data in the data warehouse. Let's take a look.

### Video - [Transactional databases versus data warehouses](https://www.cloudskillsboost.google/course_templates/54/video/521380)

- [YouTube: Transactional databases versus data warehouses](https://www.youtube.com/watch?v=2EAOL1Y2XRI)

Data engineers may be responsible for both the backend transactional database systems that support your company's applications AND the data warehouses that support your analytic workloads. In this lesson, you'll explore the differences between databases and data warehouses and the Google Cloud solutions for each workload. If you have SQL Server MySQL or PostgresSQL as your relational database, you can migrate it to Cloud SQL, which is Google Cloud's fully managed relational database solution. Cloud SQL delivers high performance and scalability with up to 64 terabytes of storage capacity, 60,000 IOPS and 624 gigabytes of RAM per instance. You can take advantage of storage auto-scale to handle growing database needs with zero downtime. One question you might get asked is: "Why not simply use Cloud SQL for reporting workflows? You can run SQL directly on the database, right?" This is a great question and will be answered in greater detail in the "Building a Data Warehouse" module. Google Cloud has several options for RDBMS's, including Spanner and Cloud SQL. When considering Cloud SQL, be aware that Cloud SQL is optimized to be a database for transactions or writes, and BigQuery is a data warehouse optimized for reporting workloads (mostly reads). The fundamental architecture of these data storage options is quite different. Cloud SQL databases are RECORD-based storage, meaning the entire record must be opened on disk. Even if you just selected a single column in your query. BigQuery is COLUMN based storage, which as you might guess, allows for really wide reporting schemas, since you can simply read individual columns out from disk. This isn't to say RDBMS's aren't as performant as Data Warehouses. They serve two different purposes. RDBMS helps your business manage new transactions. Take this point of sale terminal at a storefront. Each order and product is likely written out as new records in a relational database somewhere. This database may store all of the orders received from their website. All of the products listed in the catalog or the number of items in their inventory. This is so that when an existing order is changed, it can be quickly updated in the database. Transactional systems allow for a single row in a database table to be modified in a consistent way. They also are built on certain relational database principles like referential integrity to guard against cases like a customer ordering a product that doesn't exist in the product table. So where does all this raw data end up In our data lake and data warehouse discussion? What's the complete picture? Here it is. Our operational systems, like our relational databases that store online orders, inventory and promotions, are our raw data sources on the left. Note that this isn't exhaustive, you could have other source systems that are manual like CSV files or spreadsheets too. These upstream data sources get gathered together in a single consolidated location in our Data Lake, which is designed for durability and high availability. Once in the data lake, the data often needs to be processed via transformations that then output the data into our data warehouse, where it is ready for use by downstream teams. Here are three quick examples of other teams that often build pipelines on our data warehouse. An ML team may build a pipeline to get features for their models. An engineering team may be using our data as part of their data warehouse. And a BI team may want to build dashboards using some of our data. So who works on these teams and how do they partner with our data engineering team?

### Video - [Partner effectively with other data teams](https://www.cloudskillsboost.google/course_templates/54/video/521381)

- [YouTube: Partner effectively with other data teams](https://www.youtube.com/watch?v=s8UA2ui2q04)

Since a data warehouse also serves other teams, it is crucial to learn how to partner effectively with them. Remember that once you’ve got data where it can be useful and it’s in a usable condition we need to add new value to the data through analytics and machine learning. What teams might rely on our data? There are many data teams that rely on your data warehouse and partnerships with data engineering to build and maintain new data pipelines. The three most common clients are: The machine learning engineer, The data or BI analyst, and Other data engineers. Let’s examine how each of these roles interacts with your new data warehouse and how data engineers can best partner with them. As you’ll see in our course on machine learning, an ML team’s models rely on having lots of high quality input data to create, train, test, evaluate, and serve their models. They will often partner with data engineering teams to build pipelines and datasets for use in their models. Two common questions you may get asked are … “How long does it take for a transaction to make it from raw data all the way into the data warehouse”? They’re asking this because any data that they train their models on must also be available at prediction-time as well. If there is a long delay in collecting and aggregating the raw data it will impact the ML team’s ability to create useful models. A second question that you will definitely get asked is how difficult it would be to add more columns or rows of data into certain datasets. Again, the ML team relies on teasing out relationships between the columns of data and having a rich history to train models on. You will earn the trust of your partner ML teams by making your datasets easily discoverable, documented, and available to ML teams to experiment on quickly. A unique feature of BigQuery is that you can create high-performing machine learning models directly in BigQuery using just SQL by using Bigquery ML. Here is the actual model code to CREATE a model, EVALUATE it, and then MAKE predictions. You’ll see this again in our lectures on machine learning later on. Other critical stakeholders are your business intelligence and data analyst teams that rely on good clean data to query for insights and build dashboards. These teams need datasets that have clearly defined schema definitions, the ability to quickly preview rows, and the performance to scale to many concurrent dashboard users. One of the Google Cloud products that helps manage the performance of dashboards is BigQuery BI Engine. BI Engine is a fast, in-memory analysis service that is built directly into BigQuery and available to speed up your business intelligence applications. Historically, BI teams would have to build, manage, and optimize their own BI servers and OLAP cubes to support reporting applications. Now, with BI Engine, you can get sub-second query response time on your BigQuery datasets without having to create your own cubes. BI Engine is built on top of the same BigQuery storage and compute architecture and servers as a fast in-memory intelligent caching service that maintains state. One last group of stakeholders are other data engineers that rely on the uptime and performance of your data warehouse and pipelines for their downstream data lakes and data warehouses. They will often ask “How can you ensure that the data pipeline we depend on will always be available when we need it?” Or, “We are noticing high demand for certain really popular datasets. How can you monitor and scale the health of your entire data ecosystem?” One popular way is to use the built-in Cloud Monitoring of all resources on Google Cloud. Since Google Cloud Storage and BigQuery are resources, you can set up alerts and notifications for metrics like “Statement Scanned Bytes” or “Query Count” so you can better track usage and performance. Here are two other reasons why Cloud Monitoring is used. One Is for tracking spending of all the different resources used and two is for what the billing trends are for your team or organization. And lastly, you can use the Cloud Audit Logs to view actual query job information to see granular level details about which queries were executed and by whom. This is useful if you have sensitive datasets that you need to monitor closely. A topic we will discuss more next.

### Video - [Manage data access and governance](https://www.cloudskillsboost.google/course_templates/54/video/521382)

- [YouTube: Manage data access and governance](https://www.youtube.com/watch?v=3kW8m_nbSt0)

>> As part of being an effective partner, your engineering team will be asked to set up data access policies and overall governance of how data is to be used and not used by your users. This is what we mean when we say a data engineer must manage the data. This includes critical topics such as privacy and security. What are some key considerations when managing certain datasets? Clearly communicating a data governance model for who should and should not have access. How is personally identifiable information like phone numbers or email addresses handled? And even more basic tasks like how will our end users discover the different datasets we have for analysis? One solution for data governance is the Cloud data catalog and the data loss prevention API. The data catalog makes all the metadata about your datasets available to search for your users. You group datasets together with tags, flag certain columns as sensitive et cetera. Why is this useful? If you have many different datasets with many different tables, which different users have different access levels to? The data catalog provides a single unified user experience for discovering those datasets quickly. No more hunting for specific table names and SQL first. Often used in conjunction with data catalog is the Cloud Data Loss Prevention API, or DLP API, which helps you better understand and manage sensitive data. It provides fast, scalable classification and reduction for sensitive data elements like credit card numbers, names, social security numbers, US and selected international identifier numbers, phone numbers and Google Cloud credentials.

### Video - [Demo: Finding PII in your dataset with the DLP API](https://www.cloudskillsboost.google/course_templates/54/video/521383)

- [YouTube: Demo: Finding PII in your dataset with the DLP API](https://www.youtube.com/watch?v=NROXkWE9lGE)

>> In this demo, we're going to use the Cloud Data Loss Prevention API to redact personally identifiable information like email addresses, phone numbers, that sort of thing, PII data, at scale. Again, find all these demos under the data engineering demos course folder in our public repository. This one is pretty short, we're just going to be using the web tool, and you can invoke the API and experiment with that yourself. So navigating to the web demo, I'm just going to copy that link. So imagine in BigQuery, you've got like email addresses or something like that inside of your data. How do we actually proactively identify it? So this demo will actually take you to a page that has just a tax code file inside of here. Inside of the file, it actually explains a little bit about the DLP API itself. You can have this, it identifies this, it just looks through here, this unstructured data, and it structures it and basically says, "Hey, I'm reading through here, yeah, it's all good. Hey, somebody put a phone number inside of their comments, I don't want to have this go to anybody." I'm going to actually flag that as very high likelihood that it's this type, it is a phone number, here is the string, here is where it is. And you can inform the model whether or not that's a good result or a poor result. So what we can do is we can hide that welcome text, and we can paste in our own examples where we know this is a personally identifiable information, and we can see if it catches it. Boom, immediately. Credit card number, very high. US driver's license, very high. Email address, part of the email address, you also have the domain and where the data is as well. So you imagine this is a trivial demo. This is just four lines here, but imagine your inherited dataset that is terabytes. How can you automatically and at scale, run through, identify it, and then also use the different components of the API to run hash functions or obfuscation functions to redact that data so that PII doesn't leak out there? All right, that's DLP, the API.

### Video - [Build production-ready pipelines](https://www.cloudskillsboost.google/course_templates/54/video/521384)

- [YouTube: Build production-ready pipelines](https://www.youtube.com/watch?v=Lk0lO_zShRY)

Person: Once your data lakes and data warehouses are set up and your governance policy is in place, it's time to productionalize the whole operation and automate and monitor as much of it as we can. That's what we mean when we say productionalize the data process. It has to be an end-to-end and scalable data processing system. Your data engineering team is responsible for the health of the plumbing-- that is, the pipelines-- and ensuring that the data is available and up to date for analytic and ML workloads. Common questions that you should ask at this phase are: "How can we ensure pipeline health and data cleanliness?" "How do we productionalize these pipelines to minimize maintenance and maximize uptime?" "How do we respond and adapt to changing schemas and business needs?" And, "Are we using the latest data engineering tools and best practices?" One common workflow orchestration tool used by enterprises is Apache Airflow. Google Cloud has a fully-managed version of Airflow called "Cloud Composer." Cloud Composer helps your data engineering team orchestrate all the pieces to the data engineering puzzle that we have discussed so far and even more that you haven't come across yet. For example, when a new CSV file gets dropped into Cloud Storage, you can automatically have that trigger an event that kicks off a data processing workflow and puts that data directly into your data warehouse. The power of this tool comes from the fact that Google Cloud big data products and services have API endpoints that you can call. A Cloud Composer job can then run every night or every hour and kick off your entire pipeline from raw data to the data lake and into the data warehouse for you. We'll discuss workflow orchestration in greater detail in later modules, and you'll do a lab on Cloud Composer as well.

### Video - [Google Cloud customer case study](https://www.cloudskillsboost.google/course_templates/54/video/521385)

- [YouTube: Google Cloud customer case study](https://www.youtube.com/watch?v=-VBB9RiqrHw)

Person: We have looked at a lot of different aspects of what a data engineer has to do. Let's look at a case study of how a Google Cloud customer solves a specific business problem. That will help tie all these different aspects together. Twitter has large amounts of data and they also have high-powered sales teams and marketing teams which, for a long time, did not have access to the data and couldn't use that data for carrying out the analysis that they wanted to be able to do. Much of the data was stored on Hadoop clusters that were completely overtaxed. So Twitter replicated some of that data from HDFS onto Cloud Storage, loaded it into BigQuery, and provided BigQuery to the rest of the organization. These were some of the most frequently requested data sets within Twitter, and they discovered that with ready access to the data, many people who were not data analysts are now analyzing data and making better decisions as a result. For more information, a link to the blog post is available in the PDF version of this content under "course resources."

### Video - [Recap](https://www.cloudskillsboost.google/course_templates/54/video/521386)

- [YouTube: Recap](https://www.youtube.com/watch?v=E2-JnCfQHIY)

Let’s summarize the major topics we covered so far in this introduction. Recall that your data sources are your upstream systems like RDBMS’ and other raw data that comes from your business in different formats. Data lakes -- your consolidated location for raw data that is durable and highly available. In this example our data lake is Cloud Storage. And data warehouses which are the end result of preprocessing the raw data in your data lake and getting it ready for analytic and ML workloads. You’ll notice a lot of other Google Cloud product icons here like batch and streaming data into your lake and running ML on your data. We’ll cover those topics in detail later in this course. A useful cheatsheet to bookmark as a reference is the “Google Cloud Products in 4 words or less” which is actively maintained on GitHub by our Google Developer Relations team. It’s also a great way to stay on top of new products and services that come out just by following the GitHub commits!

### Video - [Lab Intro: Using BigQuery to do Analysis](https://www.cloudskillsboost.google/course_templates/54/video/521387)

- [YouTube: Lab Intro: Using BigQuery to do Analysis](https://www.youtube.com/watch?v=dwuI6se2k2g)

>> Now it's time to practice analyzing data with BigQuery in your lab. In this lab, you will execute interactive queries in the BigQuery console and then combine and run analytics on multiple datasets.

### Lab - [Using BigQuery to do Analysis](https://www.cloudskillsboost.google/course_templates/54/labs/521388)

In this lab, you analyze 2 different public datasets, run queries on them, separately and then combined, to derive interesting insights.

- [ ] [Using BigQuery to do Analysis](../labs/Using-BigQuery-to-do-Analysis.md)

### Quiz - [Quiz: Introduction to Data Engineering](https://www.cloudskillsboost.google/course_templates/54/quizzes/521389)

#### Quiz 1.

> [!important]
> **There are a number of common challenges encountered by data engineers. Which of the following approaches best address the challenge of data accuracy and quality?**
>
> - [ ] Consolidate disparate datasets and data formats into a data lake.
> - [ ] Manage server and cluster capacity and make sure that enough capacity exists to carry out the ETL jobs
> - [ ] Build ETL pipelines to clean and transform data that is then stored in a data warehouse.
> - [ ] Optimize queries for performance

#### Quiz 2.

> [!important]
> **Which of the following statements are true? (Choose TWO)**
>
> - [ ] BigQuery is a row-based storage
> - [ ] Cloud SQL is optimized for high-read data
> - [ ] BigQuery is optimized for high-read data
> - [ ] Cloud SQL is optimized for high-throughput writes

## Building a Data Lake

In this module, we describe what data lake is and how to use Cloud Storage as your data lake on Google Cloud.

### Video - [Module Introduction](https://www.cloudskillsboost.google/course_templates/54/video/521390)

- [YouTube: Module Introduction](https://www.youtube.com/watch?v=3Yz5ziF9G3A)

>> Welcome to the module on building a data lake. We'll start by revisiting what data lakes are, and discuss your data storage and options for extracting, transforming, and loading your data into Google Cloud. Then we'll do a deep dive into why Google Cloud Storage is a popular choice to serve as a data lake. Securing your data lake running on Cloud Storage is of paramount importance. We'll discuss the key security features that you need to know as a data engineer to control access to your objects. Cloud Storage isn't your only choice when it comes to storing data in a data lake on Google Cloud. So we'll look at the storing of different data types. Finally, we'll look at Cloud SQL, the default choice for OLTP or Online Transaction Processing Workloads on Google Cloud. You'll also do a hands-on lab where you'll practice creating a data lake for your relational data with Cloud SQL.

### Video - [Introduction to data lakes](https://www.cloudskillsboost.google/course_templates/54/video/521391)

- [YouTube: Introduction to data lakes](https://www.youtube.com/watch?v=FM1OWtxdlXM)

Let’s start with a discussion about what data lakes are and where they fit in as a critical component of your overall data engineering ecosystem. What is a data lake after all? It’s a fairly broad term, but it generally describes a place where you can securely store various types of data of all scales, for processing and analytics. Data lakes are typically used to drive data analytics, data science and ML workloads, or batch and streaming data pipelines. Data lakes will accept all types of data. Finally, data lakes are portable, on-premise or in the cloud. Here’s where Data Lakes fit into the overall data engineering ecosystem for your team. You have to start with some originating system or systems that are the source of all your data -- those are your data sources. Then, as a data engineer, you need to build reliable ways of retrieving and storing that data -- those are your data sinks. The first line of defense in an enterprise data environment is your data lake. Again it’s the central “give me whatever data at whatever volume, variety of formats, and velocity you got” I can take it. We’ll cover the key considerations and options for building a data lake in this module. Once your data is off the source systems and inside your environment -- generally considerable cleanup and processing is required to transform that data into a useful format for the business. It will then end up in your Data Warehouse. That’s our focus for the next module. What actually performs the cleanup and processing of data? Those are your data pipelines! They are responsible for doing the transformations and processing on your data at scale and bring your entire system to life with fresh newly processed data for analysis. An additional abstraction layer above your pipelines is what I will call your entire workflow. You will often need to coordinate efforts between many different components at a regular or event-driven cadence. While your data pipeline may process data from your lake to your warehouse, your orchestration workflow may be the one responsible for kicking off that data pipeline in the first place when it noticed that there was new raw data available from a source. Before we move into what cloud products can fit what roles, I want to leave you with an analogy that helps disambiguate these components. Picture yourself in the world of civil engineering for a moment. You’re tasked with building an amazing skyscraper in a downtown city. Before you break ground, you need to ensure you have all the raw materials that you’re going to need to achieve your end objective. Sure -- some materials could be sourced later in the project but let’s keep this example simple. The act of bringing the steel, the concrete, the water, the wood, the sand, the glass from wherever source elsewhere in the city onto your construction site is analogous to data coming from source systems and into your lake. Great! Now you have all these raw materials but you can’t use them as-is to build your building. You need to cut the wood and metal, measure and format the glass before it is suited for the purpose of building the building. The end-result, the cut glass, shaped metal, that is the formatted data that is stored in your data warehouse. It’s ready to be used to directly add value to your business -- which in our analogy is building the building. How did you transform these raw materials into useful pieces? On a construction site that’s the job of the worker. As you’ll see later when we talk about Data Pipelines, the individual unit behind the scenes is literally called a worker (which is just a Virtual Machine) and it takes some small piece of data and transforms it for you. What about the building itself? That’s whatever end-goal or goals you have for this engineering project. In the data engineering world, the shiny new building could be a brand new analytical insight that wasn’t possible before, or a machine learning model, or whatever else you want to achieve now that you have the cleaned data available. The last piece of the analogy is the orchestration layer. On a construction site you have a manager or a supervisor that directs when work is to be done and any dependencies. They could say “once the new metal gets here, send it to this area of the site for cutting and shaping, and then alert this other team that it’s available for building”. In the data engineering world that’s your orchestration layer or overall workflow. So you might say “Everytime a new piece of CSV data drops into this Cloud Storage bucket I want you to automatically pass it to our data pipeline for processing. AND, once it’s done processing, I want you -- the pipeline -- to stream it into the data warehouse. AND, once it’s in the data warehouse, I will notify the machine learning model that new cleaned training data is available for training and direct it to start training a new model version.” Can you see the graph of actions building? What if one step fails? What if you want to run that every day? You’re beginning to see the need for an orchestrator which, in our solutioning, will be Apache Airflow running on Cloud Composer later. Let’s bring back one example solution architecture diagram that you saw earlier in the course. The data lake here is Cloud Storage buckets right in the center of the diagram. It’s your consolidated location for raw data that is durable and highly available. In this example our Data Lake is a Cloud Storage but that doesn’t mean Cloud Storage is your only option for Data Lakes. Cloud Storage is one of a few good options to serve as a Data Lake. In other examples we will look at, BigQuery may be your Data Lake AND your Data Warehouse and you don’t use Cloud Storage buckets at all. This is why it’s so important to understand what you want to do first and then find which solutions best meet your needs. Regardless of which cloud tools and technologies you use -- your data lake generally serves as that single consolidated place for all your raw data. Think of it as a durable staging area. The data may end up in many other places like a transformation pipeline that cleans it up, moves it to the warehouse, and then it’s read by a machine learning model BUT it all starts with getting that data into your Lake first. Let’s do a quick overview on some of the core Google Cloud Big Data products that you need to know as a data engineer and will practice later in your labs. Here is a list of big data and ML products organized by where you would likely find them in a typical data processing workload from storing the data on the left, to ingesting it into your cloud-native tools for analysis, training machine learning models, and serving up insights. In this Data Lake module, we will focus on two of the foundational Storage products which will make up your Data Lake: Cloud Storage and Cloud SQL for your relational data. Later in the course, you will practice with Bigtable as well when you do high-throughput streaming pipelines. You may be surprised to not see BigQuery in the storage column. Generally BigQuery is used as a Data Warehouse -- what’s the core difference between a Data Lake and Data Warehouse then? A data lake is essentially the place where you capture every aspect of your business operations. Because you want to capture every aspect you tend to store the data in its natural raw format - - the format in which it is produced by your application so you may have a log file and the log files stored as is.. . in a data lake. You can basically store anything that you want and because you want to store it all…. .you tend to store these things as object blobs or files. The advantage of the data lake’s flexibility as a central collection point is also the problem. With a data lake, the data format is very much driven by the application that writes the data and it is in whatever format it is. The advantage of a data lake is that whenever the application gets upgraded it can start writing the new data immediately because it's just a capture of whatever raw data exists. So how do you take this flexible and large amount of raw data and do something useful with it? Enter the Data Warehouse. On the other hand a data warehouse is much more thoughtful. You might load the data into a data warehouse only after you have a schema defined and the use case identified. You might take the raw data that exists in a data lake, and transform it, organize it, process it, clean it up and then store it in a data warehouse. Why are you getting the data warehouse? Maybe because the data in the data warehouse is used to generate charts, reports, dashboards, and so on. The idea is that because the schema is consistent and shared across all of the applications, someone can go ahead and analyze the data and derive insights from it much faster. So a data warehouse tends to be structured and semi-structured data that is organized and placed in a format that makes it conducive for querying and analysis.

### Video - [Data storage and ETL options on Google Cloud](https://www.cloudskillsboost.google/course_templates/54/video/521392)

- [YouTube: Data storage and ETL options on Google Cloud](https://www.youtube.com/watch?v=NTGAldng-pw)

Next, let’s discuss your data storage and Extract, Transform, and Load options on Google Cloud. Your options on Google Cloud for building a Data Lake are your storage solutions you saw earlier. You’ve got Cloud Storage as a great catch-all, Cloud SQL and Spanner for Relational Data, and Firestore and Bigtable for NoSQL data. Choosing which option or options to use depends heavily on your use case and what you’re trying to build. In this lesson, we will focus on Cloud Storage and Cloud SQL, but you will see the NoSQL options like Bigtable later on in the course when we talk about very high-throughput streaming. So how do you decide on which path to take for your lake? The final destination for where your data lands on the cloud and the paths that you take to get your data to the cloud depends on where your data is now. How big your data is- this is the Volume component of the 3 Vs of Big Data. And ultimately where does it have to go? In architecture diagrams the ending point for the data is called a data sink. A common data sink after a data lake is your data warehouse. Don’t forget a critical thing to consider is how much processing and transformation your data needs before it is useful to your business. Now you may ask, do I complete the processing before I load it into my Data Lake or afterward before it gets shipped off somewhere else? Let’s talk about these patterns. The method that you use to load the data into the cloud depends on how much transformation is needed from the raw data that you have, to the final format you want it in. In this lesson, we will look at some of the considerations for the final format that you want it in. The simplest case might be that you have data in a format that is readily ingested by the cloud product that you want to store it in. Let's say for example you have your data in Avro format and you want to store the data in BigQuery because your use case fits what BigQuery is good at. Then what you do is simply E-L or Extract and Load. BigQuery will directly load Avro files. E-L refers to when data can be imported "as is" into a system. Examples include importing data from a database, where the source and the target have the same schema. One of the features that makes BigQuery unique is that -- as you saw before with the federated query example -- you may end up not even loading the data into BigQuery and still can query off of it. Avro, ORC, and Parquet files are all now supported for federated querying. The T in E-L-T is TRANSFORM. That’s when the data loaded into the cloud product isn’t in the final format you want it in. You may want to clean it up. Or maybe you want to transform the data in some way, for example if data needs to be corrected. In other words, you would extract from your on-premise system, load into the cloud product, and then do the transformation. That's an extract load and transform, or E-L-T. You tend to do this when the amount of transformation that's needed is not very high and the transformation will not greatly reduce the amount of data that you have. E-L-T allows raw data to be loaded directly into the target and transformed there. For example, in BigQuery, you could use SQL to transform the data and write a new table. The third option is extract, transform, and load - or E-T-L. This is the case when you want to extract the data, apply a bunch of processing to it, and then load it into the cloud product. This is usually what you pick when this transformation is essential or if this transformation greatly reduces the data size, so by transforming the data before loading it into the cloud you might be able to greatly reduce the network bandwidth that you need. If you have your data in some binary proprietary format, and you need to convert it before loading, then you need E-T-L as well. E-T-L is a data integration process in which transformation takes place in an intermediate service before it is loaded into the target. For example, the data might be transformed in a data pipeline like Dataflow before being loaded into BigQuery.

### Video - [Build a data lake using Cloud Storage](https://www.cloudskillsboost.google/course_templates/54/video/521393)

- [YouTube: Build a data lake using Cloud Storage](https://www.youtube.com/watch?v=kW5LUTXVza0)

Person: Cloud Storage is the essential storage service for working with data, especially unstructured data in the cloud. Let's do a deep dive into why Google Cloud Storage is a popular choice to serve as a data lake. Data in Cloud Storage persists beyond the lifetime of VMs or clusters, i.e. it is persistent. It is also relatively inexpensive compared to the cost of compute. So, for example, you might find it more advantageous to cache the results of previous computations in Cloud Storage. Or if you don't need an application running all the time, you might find it helpful to save the state of your application into Cloud Storage and shut down the machine it is running on when you don't need it. Cloud Storage is an object store, so it just stores and retrieves binary objects without regard to what data is contained in the objects. However, to some extent, it also provides file system compatibility and can make objects look like and work as if they were files so you can copy files in and out of it. Data stored in Cloud Storage will basically stay there forever. In other words, it is durable, but is available instantly. It is strongly consistent. You can share data globally, but it is encrypted and completely controlled and private if you want it to be. It is a global service, and you can reach the data from anywhere. In other words, it offers global availability. But the data can also be kept in a single geographic location if you need that. Data is served up with moderate latency and high throughput. As a data engineer, you need to understand how Cloud Storage accomplishes these apparently contradictory qualities, and when and how to employ them in your solutions. A lot of Cloud Storage's amazing properties have to do with the fact that it is an object store, and other features are built on top of that base. The two main entities in Cloud Storage are buckets and objects. Buckets are containers for objects, and objects exist inside of buckets and not apart from them. So buckets are containers for data. Buckets are identified in a single global unique namespace. So that means once a name is given to a bucket, it cannot be used by anyone else unless and until that bucket is deleted and the name is released. Having a global namespace for buckets simplifies locating any particular bucket. When a bucket is created, it is associated with a particular region or with multiple regions. Choosing a region close to where the data will be processed will reduce latency. And if you are processing the data using cloud services within the region, it will save you on network egress charges. When an object is stored, Cloud Storage replicates the object. It monitors the replicas, and if one of them is lost or corrupted, it replaces it with a fresh copy. This is how Cloud Storage gets many 9s of durability. For a multi-region bucket, the objects are replicated across regions. And for a single-region bucket, the objects are replicated across zones. In any case, when the object is retrieved, it is served up from the closest replica to the requester, and that is how low latency occurs. Multiple requesters could be retrieving the objects at the same time from different replicas, and that is how high throughput is achieved. Finally, the objects are stored with metadata. Metadata is information about the object. Additional Cloud Storage features use the metadata for purposes such as access control, compression, encryption and life cycle management. For example, Cloud Storage knows when an object was stored and it can be set to automatically delete after a period of time. This feature uses the object metadata to determine when to delete the object. You may have a variety of storage requirements for a multitude of use cases. Cloud Storage offers different classes to cater for these requirements, and these are based on how often data is accessed. Standard storage is best for data that is frequently accessed, also referred to as hot data, and/or stored for only brief periods of time. When used in a region, colocating your resources maximizes the performance for data-intensive computations and can reduce network charges. When used in a dual region, you still get optimized performance when accessing Google Cloud products that are located in one of the associated regions, but you also get the improved availability that comes from storing data in geographically separate locations. When used in a multi-region, standard storage is appropriate for storing data that is accessed around the world, such as serving website content, streaming videos, executing interactive workloads or serving data supporting mobile and gaming applications. Nearline storage is a low-cost, highly durable storage service for storing infrequently accessed data. Nearline storage is a better choice than standard storage in scenarios where slightly lower availability, a 30-day minimum storage duration and costs for data access are acceptable trade-offs for lowered at-rest storage costs. Nearline storage is ideal for data you plan to read or modify on average once per month or less. Nearline storage is appropriate for data backup, long-tail multimedia content and data archiving. Coldline storage is a very low-cost, highly durable storage service for storing infrequently accessed data. Coldline storage is a better choice than standard storage or nearline storage in scenarios where slightly lower availability, a 90-day minimum storage duration and higher costs for data access are acceptable trade-offs for lowered at-rest storage costs. Coldline storage is ideal for data you plan to read or modify at most once a quarter. Archive storage is the lowest-cost, highly durable storage service for data archiving, online backup and disaster recovery. Archive storage has higher costs for data access and operations as well as a 365-day minimum storage duration. Archive storage is the best choice for data that you plan to access less than once a year. For example, cold data storage such as data stored for legal or regulatory reasons, and disaster recovery. Cloud Storage is unique in a number of ways. It has a single API, millisecond data access latency, and 11 9s durability across all storage classes. Cloud Storage also offers object life cycle management, which uses policies to automatically move data to lower-cost storage classes as it is accessed less frequently throughout its life. Cloud Storage uses the bucket name and object name to simulate a file system. This is how it works. The bucket name is the first term in the URI. A forward slash is appended to it, and then it is concatenated with the object name. The object name allows the forward slash character as a valid character in the name. The very long object name with forward slash characters in it looks like a file system path, even though it is just a single name. In the example shown, the bucket name is declass. The object name is de/modules/02/script.sh. The forward slashes are just characters in the name. If this path were in a file system, it would appear as a set of nested directories beginning with declass. Now for all practical purposes, it works like a file system, but there are some differences. For example, imagine that you wanted to move all the files in the 02 directory to the 03 directory inside the modules directory. In a file system, you would have actual directory structures, and you would simply modify the file system metadata so that the entire move is atomic. But in an object store simulating a file system, you would have to search through all the objects contained in the bucket for names that had 02 in the right position in the name. Then you would have to edit each object name and rename them using 03. This would produce apparently the same result, moving the files between directories. However, instead of working with a dozen files in a directory, the system had to search over possibly thousands of objects in the bucket to locate the ones with the right names and change each of them. So the performance characteristics are different. It might take longer to move a dozen objects from directory 02 to directory 03 depending on how many other objects are stored in the bucket. During the move, there will be list inconsistency, with some files in the old directory and some in the new directory. A best practice is to avoid the use of sensitive information as part of bucket names because bucket names are in a global namespace. The data in the buckets can be kept private if you need it to be. Cloud Storage can be accessed using a file access method. That allows you, for example, to use a copy command from a local file directly to Cloud Storage. Use the tool [Indistinct] to do this. Cloud Storage can also be accessed over the web. The site, storage.cloud.google.com, uses TLS HTTPS to transport your data, which protects credentials as well as data in transit. Cloud Storage has many object management features. For example, you can set a retention policy on all objects in the bucket. For example, the objects should expire after 30 days. You can also use versioning so that multiple versions of an object are tracked and available if necessary. You might even set up life cycle management to automatically move objects that haven't been accessed in 30 days to nearline, and after 90 days to coldline.

### Video - [Secure Cloud Storage](https://www.cloudskillsboost.google/course_templates/54/video/521394)

- [YouTube: Secure Cloud Storage](https://www.youtube.com/watch?v=Ck3TYW3shV0)

Person: Securing your data lake running on Cloud Storage is of paramount importance. We'll discuss the key security features you need to know as a data engineer to control access to your objects. Cloud Storage implements two completely separate but overlapping methods of controlling access to objects, IAM policy and access control lists. IAM is standard across the Google Cloud. It is set at the bucket level and applies uniform access rules to all objects within a bucket. Access control lists can be applied at the bucket level or on individual objects, so it provides more fine-grained access control. The IAM controls are as you would expect. IAM provides project roles and bucket roles, including bucket reader, bucket writer and bucket owner. The ability to create or change access control lists is an IAM bucket role. And the ability to create and delete buckets and to set IAM policy is a project level role. Custom roles are available. Project level viewer, editor and owner roles make users members of special internal groups that give them access by being members of bucket roles. See the online documentation for details. When you create a bucket, you are offered the option of disabling access lists and only using IAM. Access lists are currently enabled by default. This choice used to be immutable, but now you can disable access lists even if they were in-force previously. As an example, you might give some bob@example.com reader access to a bucket through IAM, and also give them write access to a specific file in that bucket through access control lists. You can give such permissions to service accounts associated with individual applications as well. All data in Google Cloud is encrypted at rest and in transit. There is no way to turn this encryption off. The encryption is done by Google using encryption keys that we manage, Google-managed encryption keys, or GMEK. We use two levels of encryption. First, the data is encrypted using a data encryption key. Then the data encryption key itself is encrypted using a key encryption key, or KEK. The KEKs are automatically rotated on a schedule, and the current KEK stored in Cloud KMS, Cloud Key Management Service. You don't have to do anything. This is automatic behavior. If you want to manage the KEK yourself, you can. Instead of Google managing the encryption key, you can control the creation and existence of the KEK that is used. This is called customer-managed encryption keys, or CMEK. You can avoid Cloud KMS completely and supply your own encryption and rotation mechanism. This is called CSEK, or customer-supplied encryption keys. Which data encryption option you use depends on business, legal and regulatory requirements. Please talk to your company's legal counsel. The fourth encryption option is client-side encryption. Client-side encryption simply means that you have encrypted the data before it is uploaded and have to decrypt the data yourself before it is used. Cloud Storage still performs GMEK, CMEK or CSEK encryption on the object. It has no knowledge of the extra layer of encryption you may have added. Cloud Storage supports logging of data access, and these logs are immutable. In addition to Cloud audit logs and Cloud Storage access logs, there are various holds and locks that you can place on the data itself. For audit purposes, you can place a hold on an object, and all operations that could change or delete the object are suspended until the hold is released. You can also lock a bucket, and no changes or deletions can occur until the lock is released. Finally, there is the lock retention policy previously discussed, and it continues to remain in effect and prevent deletion, whether a bucket lock or object hold are in-force or not. Data locking is different from encryption. Where encryption prevents someone from understanding the data, locking prevents them from modifying the data. There are a whole host of special use cases supported by Cloud Storage. For example, decompressive coding. By default, the data you upload is the same data you get back from Cloud Storage. This includes gzip archives, which usually are returned as gzip archives. However, if you tag an object properly in metadata, you can cause Cloud Storage to decompress the file as it is being served. Benefits of the smaller compressed file are faster upload and lower storage costs compared with the uncompressed files. You can set up a bucket to be requester pays on access. Normally, if data is accessed from a different region, you will have to pay network egress charges, but you can make the requester pay so that you pay only for data storage. You can create a signed URL to anonymously share an object in Cloud Storage, and even have the URL expire after a period of time. It is possible to upload an object in pieces and create a composite object without having to concatenate the pieces after upload. There are a lot of useful features in Cloud Storage, but we have to move on.

### Video - [Store all sorts of data types](https://www.cloudskillsboost.google/course_templates/54/video/521395)

- [YouTube: Store all sorts of data types](https://www.youtube.com/watch?v=RSTwDTk4ikQ)

As highlighted before, Cloud Storage isn’t your only choice when it comes to storing data on Google Cloud. You don’t want to use Cloud Storage for transactional workloads. Even though the latency of Cloud Storage is low, it is not low enough to support high-frequency writes. For transactional workloads, use Cloud SQL or Firestore depending on whether you want to use SQL or No-SQL. You also don’t want to use Cloud Storage for analytics on structured data. If you do that, you will spend a significant amount of compute parsing data -- it is better to use Bigtable or BigQuery for analytics workloads on structured data, depending on the latency required. We keep talking about transactional versus analytics workloads. What exactly do we mean? Transactional workloads are ones where you require fast inserts and updates. You want to maintain a snapshot, a current state of the system. The tradeoff is that queries tend to be relatively simple and tend to affect only a few records. For example, in a banking system, depositing your salary to your account is a transaction. It updates the balance field. The bank is doing online transaction processing or O-L-T-P. An analytics workload, on the other hand, tends to read the entire dataset and is often used for planning or decision support. The data might come from a transaction processing system, but it is often consolidated from many O-L-T-P systems. For example, a bank regulator might require us to provide a report of every customer who transferred more than $10,000 to an overseas account. They might ask the bank to include customers who try to transfer the $10,000 in smaller chunks over the period of a week. A report like this will require scanning through a significantly large dataset and require a complex query that involves aggregating over moving time windows. This is an example of online analytical processing or an O-LAP workload. The reason we treat these use cases differently is that transactional systems are write heavy. These tend to be operational systems. For example, a retailer’s catalog data will require updating every time the retailer adds a new item or changes the price. The inventory data will need to be updated every time the retailer sells an item. This is because the catalog and inventory systems have to maintain an up-to-the-moment snapshot of the business. Analytical systems can be periodically populated from the operational systems. We could use this once a day to generate a report of items in our catalog whose sales are increasing, but whose inventory levels are low. Such a report will have to read a bunch of data, but not have to write much. O-LAP systems are read-focused. Recall what we said. Analytical systems can be periodically populated from the operational systems. Data engineers build the pipelines to populate the O-LAP system from the OLTP system. One simple way might be to export the database as a file and load it into the data warehouse. This is what we called E-L. On Google Cloud, the data warehouse tends to be BigQuery. There is a limit to the size of the data that you can directly load to BigQuery. This is because your network might be a bottleneck. Rather than load the data directly into BigQuery, it can be much more convenient to first load it to Cloud Storage and load from Cloud Storage to BigQuery. Loading from Cloud Storage will also be faster because of the high throughput it offers. Getting back to the discussion on transactional workloads, you have a few options for relational databases. The default choice here is Cloud SQL, but if you require a globally distributed database, then use Spanner. You’d want a globally distributed database if your database will see updates from applications running in different geographic regions. The True Time capability of Spanner is very appealing for this kind of use case. Another reason you might choose Spanner is if your database is too big to fit into a single Cloud SQL instance. If our database is many gigabytes, you need a distributed database. The scalability of Spanner is very appealing for this use case. Other than that, you’d use Cloud SQL because it is more cost-effective. For analytics workloads, the default choice is BigQuery. However, if you require high-throughput inserts, more than millions of rows per second, or if you require low latency, on the order of milliseconds, use Bigtable. Other than that, you’d use BigQuery because it is more cost-effective.

### Video - [Cloud SQL as a relational data lake](https://www.cloudskillsboost.google/course_templates/54/video/521396)

- [YouTube: Cloud SQL as a relational data lake](https://www.youtube.com/watch?v=BGNdp0Ye7nc)

Cloud SQL, we said, is the default choice for OLTP (or online transaction processing) workloads on Google Cloud. Let’s take a quick look. Cloud SQL is an easy-to-use service that delivers fully managed relational databases. Cloud SQL lets you hand off to Google the mundane, but necessary and often time-consuming tasks—like applying patches and updates, managing backups, and configuring replications—so you can put your focus on building great applications. Cloud SQL is our managed service for third party RDBMSs. It supports MySQL, PostgreSQL, and Microsoft SQL Server and additional RDBMSs will be added over time. What this means is that we provide a compute engine instance that has MySQL already installed. We’ll manage the instance on your behalf. We’ll do backups, we'll do security updates, and update the minor versions of the software so that you don't have to worry about that. In other words, Google Cloud manages the MySQL database to the point where you can treat it as a service. We even do DBA-like things. You can tell us to add a failover replica for your database. We’ll manage it for you, and you’ll have a 99.95% availability SLA. Another benefit of Cloud SQL instances is that they are accessible by other Google Cloud services and even external services. You can use Cloud SQL with App Engine using standard drivers like Connector/J for Java or MySQLdb for Python. You can authorize Compute Engine instances to access Cloud SQL instances and configure the Cloud SQL instance to be in the same zone as your virtual machine. Cloud SQL also supports other applications and tools that you might be used to, like SQL Workbench, Toad and other external applications using standard MySQL drivers. One of the advantages of Google managing your database is that you get the benefits of Google security. Cloud SQL customer data is encrypted when on Google's internal networks and when stored in database tables, temporary files, and backups. Every Cloud SQL instance includes a network firewall, allowing you to control network access to your database instance by granting access. Cloud SQL is easy to use: it doesn't require any software installation or maintenance. And Google manages the backups. Cloud SQL takes care of securely storing your backed-up data and makes it easy for you to restore from a backup and perform a point-in-time recovery to a specific state of an instance. Cloud SQL retains up to 7 backups for each instance, which is included in the cost of your instance. You can vertically scale Cloud SQL -- just increase your machine size. Scale up to 64 processor cores and more than 100 GB of RAM. Horizontally, you can quickly scale out with read replicas. Google Cloud SQL supports three read replica scenarios: Cloud SQL instances replicating from a Cloud SQL primary instance Cloud SQL instances replicating from an external primary instance External MySQL instances replicating from a Cloud SQL primary instance If you need horizontal read-write scaling, consider Spanner. For the special case of failover, Cloud SQL supports this. Cloud SQL instances can be configured with a failover replica in a different zone in the same region. Then, Cloud SQL data is replicated across zones within a region for durability. In the unlikely event of a data center outage, a Cloud SQL instance will automatically become available in another zone. All changes made to data on the primary are replicated to failover. If the primary instance’s zone has an outage, Cloud SQL automatically fails over to the replica. If the primary has issues not caused by a zone outage, failover doesn’t occur. You can, however, initiate failover manually. There are a few caveats: Note that the failover replica is charged as a separate instance. When a zonal outage occurs and your primary fails over to your failover replica, any existing connections to the instance are closed. However, your application can reconnect using the same connection string or IP address; you do not need to update your application after a failover. After the failover, the replica becomes the primary, and Cloud SQL automatically creates a new failover replica in another zone. If you located your Cloud SQL instance to be near other resources, such as a Compute Engine instance, you can relocate your Cloud SQL instance back to its original zone when the zone becomes available. Otherwise, there is no need to relocate your instance after a failover. You can use the failover replica as a read replica to offload read operations from the primary. We keep saying that Cloud SQL is fully managed. We have also used the word serverless to describe BigQuery, for example. What’s the difference? As a fully managed service, Cloud SQL provides you access much like you have with an on-premises installation. For instance, you can directly connect to the Cloud SQL instance through the gcloud client and perform tasks directly through SQL. However, Google helps you manage the instance by automating backups, setting up failover instances, and so on. Serverless is the next step up. You can treat a serverless product as just an API that you are calling. You pay for using the product, but don’t have to manage any servers. BigQuery is serverless. So are Pub/Sub for asynchronous messaging and Dataflow for parallel data processing. You can think of Cloud Storage as being serverless as well. Sure, Cloud Storage uses disks, but you never actually interact with the hardware. One of the unique things about Google Cloud is that you can build a data processing pipeline of well-designed components all of which are fully serverless. Dataproc, on the other hand, is fully managed. It helps you run Spark and Hadoop workloads without having to worry about setup. Given the choice between doing a brand-new project on BigQuery or Dataflow (which are serverless), and Dataproc (which is fully managed), which one should you choose? All other things being equal, choose the serverless product.

### Video - [Lab Intro: Loading Taxi Data into Google Cloud SQL](https://www.cloudskillsboost.google/course_templates/54/video/521397)

- [YouTube: Lab Intro: Loading Taxi Data into Google Cloud SQL](https://www.youtube.com/watch?v=EJs10v-vS_o)

Person: In this next lab, you practice creating a data lake and bringing in all of your relational data from outside the cloud into a Google Cloud SQL-hosted environment. Specifically, you'll first create a Google Cloud SQL instance which can hold multiple databases. After the instance is up, you'll create a new Cloud SQL database, and then import some text data into Cloud SQL. Lastly, you'll check that data for integrity. Good luck.

### Lab - [Loading Taxi Data into Google Cloud SQL 2.5](https://www.cloudskillsboost.google/course_templates/54/labs/521398)

In this lab you will import data from CSV text files into Cloud SQL and then carry out some basic data analysis using simple queries.

- [ ] [Loading Taxi Data into Google Cloud SQL 2.5](../labs/Loading-Taxi-Data-into-Google-Cloud-SQL-2.5.md)

### Quiz - [Quiz: Building a Data Lake](https://www.cloudskillsboost.google/course_templates/54/quizzes/521399)

#### Quiz 1.

> [!important]
> **Which of the following statements on Cloud Storage are true?**
>
> - [ ] Cloud Storage allows you to set retention policies on all objects in a bucket
> - [ ] Cloud Storage simulates a file system
> - [ ] Data in Cloud Storage is not encrypted
> - [ ] Cloud Storage implements both IAM policy and Access Control Lists

#### Quiz 2.

> [!important]
> **Which statement best describes a data lake?**
>
> - [ ] Storage optimized for high-throughput writes.
> - [ ] Storage for current/historical data intended for reporting.
> - [ ] Data storage intended for analytics.
> - [ ] The place where you capture every aspect of your business operations. Data is stored in its natural, raw format.

## Building a Data Warehouse

In this module, we talk about BigQuery as a data warehousing option on Google Cloud

### Video - [Module Introduction](https://www.cloudskillsboost.google/course_templates/54/video/521400)

- [YouTube: Module Introduction](https://www.youtube.com/watch?v=gAUQ5u1xj9M)

Person: Hello and welcome to the Building a Data Warehouse module. This is the third module in the course, Modernizing Data Lakes and Data Warehouses with Google Cloud. We'll start by describing what makes a modern data warehouse. We'll also talk about what distinguishes a data lake from an enterprise data warehouse. Then we're going to introduce BigQuery, a data warehouse solution on Google Cloud. Once you're familiar with the basics of BigQuery, we'll talk about how BigQuery organizes your data and then how to load new data into BigQuery. You'll also have the opportunity to load data into BigQuery through a hands-on lab. Finally, we'll dive into the world of data warehouse schemas. We'll talk about efficient data warehouse schema design and take a closer look at BigQuery support for nested and repeated fields and why this is such a popular schema design for enterprises. You'll get some experience working with JSON and array data in BigQuery through a hands-on lab. We'll end by discussing how you can optimize the tables in your data warehouse with partitioning and clustering.

### Video - [The modern data warehouse](https://www.cloudskillsboost.google/course_templates/54/video/521401)

- [YouTube: The modern data warehouse](https://www.youtube.com/watch?v=mxsRC_p8NZc)

>> Let's start by describing what makes a modern data warehouse. An enterprise data warehouse should consolidate data from many sources. If you recall from the previous module, a data lake does something very similar. The key difference between the two is the word consolidate. A data warehouse imposes a schema. A data lake is just raw data, but an enterprise data warehouse brings the data together and makes it available for querying and data processing. To use a data warehouse, an analyst needs to know the schema of the data. However, unlike for a data lake, the analyst doesn't have to write code to read and parse the data. Another reason to consolidate all your data besides standardizing the format and making it available for querying is making sure the query results are meaningful. You want to make sure the data is clean, accurate and consistent. The purpose of a data warehouse is not to store data. That's the purpose of a data lake. If you have raw data that you want to keep around but not necessarily query, don't bother with cleaning and streamlining it, leave it in a data lake. All data in a data warehouse should be available for querying. It's important to ensure that those queries are quick. You don't want people waiting hours or days for results. We described an enterprise data warehouse and how it's different from a data lake. What makes a data warehouse modern? Businesses' data requirements continue to grow, you want to make sure the data warehouse can deal with datasets that don't fit into memory. Typically, this is gigabytes to terabytes of data, but occasionally can be petabytes. You don't want separate warehouses for different datasets. Instead, you want a single data warehouse that can scale from gigabytes to petabytes of data. Second, you want the data warehouse to be serverless and fully no-ops. You don't want to be limited to clusters that you need to maintain, or indexes that you need to fine-tune. Removing these responsibilities will allow data analysts to carry out ad hoc queries faster, which is important because you want the data warehouse to increase the speed at which your business makes decisions. Next, your data warehouse is not productive if it allows you to do queries but doesn't support rich visualization and reporting. Ideally, your data warehouse can seamlessly plug into whichever visualization or reporting tool your business is most familiar with. Similarly, because the data warehouse requires clean and consistent data, you will often have to build data pipelines to bring data into the warehouse. The modern data warehouse should be able to integrate with an ecosystem of processing tools for building ETL pipelines. Your data pipeline should be capable of constantly refreshing data in the warehouse in order to keep it up to date. You need to be able to stream data into the warehouse and not rely on batch updates. Also, predictive analytics is becoming increasingly important for data analysts. As a result, a modern data warehouse has to support machine learning without moving the data out of the warehouse. Last but not least, in a modern data warehouse, it should be possible to impose enterprise grade security like data exfiltration constraints. It should also be possible to share data and queries with collaborators.

### Video - [Introduction to BigQuery](https://www.cloudskillsboost.google/course_templates/54/video/521402)

- [YouTube: Introduction to BigQuery](https://www.youtube.com/watch?v=4yFbXK6KfM0)

In this lesson, we’re going to introduce BigQuery, a data warehouse solution on Google Cloud. BigQuery has many capabilities that make it an ideal data warehouse. When we talked about a modern data warehouse, we talked about having the warehouse be able to scale from gigabytes to petabytes seamlessly. We talked about being able to do ad hoc queries and no-ops. BigQuery cost-effectively handles large, petabyte-scale datasets for storage and querying. In fact it’s similar to the cost of Cloud Storage. This enables you to store your data without having to worry about archiving off older data to save on storage. Unlike traditional data warehouses, BigQuery has features like GIS and machine learning built in. It also provides capabilities to stream data in, so you can analyze your data in near real time. Because it’s part of Google Cloud, you get all of the security benefits the cloud provides while also being able to share datasets and queries. BigQuery supports standard SQL queries and is compatible with ANSI SQL 2011. BigQuery is a serverless fully-managed service, which means that the BigQuery engineering team takes care of updates and maintenance for you. Upgrades don’t require downtime or hinder system performance. For example, data aging and expiration can be a cumbersome operation in traditional data warehouses. In BigQuery, you just supply a table expiration flag at the time of table creation or update a table to add this feature. The table will automatically expire when it reaches that age or duration. Many traditional systems require resource-intensive vacuum processes to run at various intervals to reshuffle and sort data blocks and recover space. BigQuery has no equivalent of the vacuum process, because the storage engine continuously manages and optimizes how data is stored and replicated. Also, because BigQuery doesn't use indexes on tables, you don't need to rebuild these. The bottom line is that you can free up real work hours by not having to worry about common database management tasks. BigQuery is implemented in two parts: a storage engine … … and an analytic engine as illustrated. The separation of compute and storage is a common theme in Google Cloud and works effectively because of Google’s petabit network called Jupiter. Jupiter allows blazing fast communication between compute and storage. BigQuery data is physically stored on Google's distributed file system, called Colossus, which ensures durability by using erasure encoding to store redundant chunks of the data on multiple physical disks. Moreover, the data is replicated to multiple data centers. You don't need to provision resources before using BigQuery, unlike many RDBMS systems. BigQuery allocates storage and query resources dynamically based on your usage patterns. Storage resources are allocated as you consume them and deallocated as you remove data or drop tables. Query resources are allocated according to query type and complexity. Each query uses some number of slots, which are units of computation that comprise a certain amount of CPU and RAM. So what makes BigQuery fast? BigQuery tables are column-oriented, compared to traditional RDBMS tables which are row-oriented. Row-oriented tables are efficient for making updates to data contained in fields. For OLTP systems, row-oriented tables are necessary because OLTP systems have frequent updates. Analytics is slow on row-oriented tables because queries have to read all the fields in a row and, depending on the kind of indexing or key, queries may have to read extra rows and fields to find the information that is requested. BigQuery, however, is an O-LAP system. It’s meant for analytics. BigQuery tables are immutable and are optimized for reading and appending data. BigQuery tables are not optimized for updating. BigQuery leverages the fact that most queries involve few columns, so it only reads the columns required for the query. BigQuery is very efficient in this sense and is the reason tables are column-oriented. Every table has a schema. You can enter the schema manually through the Cloud Console, or by supplying a JSON file. BigQuery is implemented using a microservice architecture, so there are no virtual machines to configure and maintain. Under the hood, analytics throughput is measured in BigQuery slots. A BigQuery slot is a unit of computational capacity required to execute SQL queries. BigQuery automatically calculates how many slots are required at each stage in a query, depending on size and complexity. A BigQuery slot is a combination of CPU, memory, and networking resources. It also includes a number of supporting technologies and sub-services. Note that each slot doesn’t necessarily have the same specification during query execution. Some slots may have more memory than others, or more CPU or more I/O. On the right hand side you can see how multiple slots work together under the hood to execute a query. You can imagine a slot operates here the same way as a worker node in a cluster for distributed processing of big data. When executing a query, BigQuery may split it up into one or multiple stages that contain different tasks to do. Tasks are assigned to workers to perform the work in parallel. In this example, we have 2 stages. In the first one, the workers will pick up a subset of the data they need to work on from BigQuery’s storage, then they apply a specific filter to their data (coming from the WHERE clause) and do a partial count of the data that is left (which is specified as COUNT() in the SELECT clause). Each worker will send its intermediate result to stage 2, where one worker will create the final result set by summing up all the counts it received by the previous workers. This result set is then presented to us in the UI. We will look at this again in our module around BigQuery performance. If a single, simple query is submitted that needs fewer slots than are available, the query will generally execute faster.

### Video - [Demo: Querying TB of data in seconds](https://www.cloudskillsboost.google/course_templates/54/video/521403)

- [YouTube: Demo: Querying TB of data in seconds](https://www.youtube.com/watch?v=Cr59-XPgKCw)

>> Welcome to the Big Data demo using BigQuery on Google Cloud Platform. Here, we're going to show the serverless scaling features of BigQuery, how it scales up automatically behind the scenes without your intervention to query large datasets. We're going to talk about 10 billion rows of Wikipedia data. So the first things first, we're going to follow the demo scripts, all these demos in the code that I'm going to be running, everything I'm going to be running is inside of our demos folder in our public repository. So first up, we're actually going to copy the query on the clipboard, and navigate to- no, I don't want to search for it- navigate to BigQuery. Inside the Google Cloud Platform, I already have BigQuery open, but if you needed to navigate to it, navigation, I have it pinned up here. It's kind of like starring it, but if you scroll all the way down, under big data, you have BigQuery, and to promote things, you don't have to continuously scroll and search, I just pin them. So I'm often using AI platform notebooks for machine learning work, composer for data engineering work, BigQuery for data analysis work. Once you're inside of BigQuery, I'm going to paste in our query in the query editor window. And you notice where you're getting datasets from, where your data actually lives is under resources. And one of the very popular public datasets that's available is Wikipedia. So inside of- it's one of many, so you can actually get airline data for flights, Reddit data, geographic data, and Wikipedia benchmark for very, very large datasets. If you were given a script, one of my favorite hotkeys that you can choose is you can actually hold down, it's- on a Mac, its command key. On a Windows, I think it's Ctrl, and that'll actually highlight- or I think it's the Windows key- that'll highlight all the tables inside of your query, and it turns them into buttons. So if you clicked on this, you automatically get back to the schema. So it's a great way to iterate between what are the columns, and what are the details and the preview of the data versus the query results as well. So again, that's just that cool hotkey, all the shortcuts that I mentioned are available, if you open up that modal, you will get the shortcuts there as well. So 10 billion rows, is this really 10 billion rows? The fastest way we can find that out is in the details. It is just about a gigabyte. Yeah, there we go. 10 billions, 600 and million rows of Wikipedia data. What type of data are we talking about? What are we actually going to be querying here? The schema is not too wide. It is the year, month and day, the Wikimedia project, and the language that it's in and the title of the Wikipedia page and how many views it has. And it's just a lot of rows. So what are we going to do? What type of operation are we going to do? Well, you can see our query, when we're going to run it, it's going to go through 10 billion rows, which is about 415 gigabytes of data. Let's see how fast it does that. It's going to return not only just columns, but it's going to do a calculation. So it's basically to say, give me the language that Wikipedia page was written in, give me the title of that page, give me the total number of views, where somewhere in the title of any of these articles the name Google was featured and it has to be a capital G. Okay, SQL is case sensitive, I'll show you how to ignore that in just a second with a function. And of course, anytime you're doing aggregations, you need to group by, and we want to have the pages that have Google somewhere in the title, the top pages of by view count first, which I'm assuming is just going to be a page called Google. But let's go ahead and run this. How long does it take to process 400 gigabytes? And we're running. And again, you're not a DBA, you're not managing the indexes or anything like that, you just have your SQL query. It's not even our dataset, we're just using somebody else's dataset. And you can see how long it took for- when I recorded this video, we got 10 seconds, 415 gigabytes processing, and here's your inside. So it reached out and it found out 10 billion rows, 10 billion pages of Wikipedia data that's stored here, looked into- a like is a rather expensive operation, it's got to not only look at the columns, it's going to look into that string value, find the word Google will somewhere appears anywhere within there. The wildcard character percentage sign is any characters before, any characters after, and sum up those total views, and it did that pretty quickly. So in total, there are 214,000 pages with Google somewhere in the name, the most popular pages is the English page for Google, the Spanish page for Google shortly after that, and then Google Earth, Google Maps, and then Chrome as well. Now, of course, if you wanted to make this not case sensitive, one of the things that you could do is you could do, say, I wanted to wrap the title and everything is going to be uppercase, and then you would have to do this as well. So you just match like for like. So if you're doing wildcard operators using Like, it's a good idea to use upper or lower, or if you're experienced with regex, you can do that as well. So that is 10 billion. And you can see what the really cool thing behind the scenes is on the execution details, you can see how it actually did this. So it took you, the human, while you're watching it, 10 seconds, you're just watching it. Behind the scenes, it took all of the computers, if you were to do it serially, linearly, stack all the computers, all the work that they did, it would be two hours and 38 minutes for one computer to do it, essentially. But that's the beauty of distributed parallel processing that happened behind the scenes. These, you don't have to care about how many virtual machines were spun up to do this work, but in aggregate, they did about three- almost three hours of work automatically, and they shared a lot of data in between themselves as well. And you can see the process of going from those 10 billion records, all the way down after the aggregations to outputting the result that you see there. All right, that's cool. 10 billion. Let's see if we can do 100 billion. So let's see, if we have a dataset, I think it's literally just adding another zero, why not? Why not go bigger, right? And again, if you want to get back to that dataset, I'm going to hotkey it. We have more information here? Yeah, we do, we got the title. I think it's largely the same schema, details. Okay, cool. We got a real big dataset, we got six terabytes, water records, same principle, expensive operation, we're going to go into every single field. How long do you think it's going to process to take to go through 100 billion records, open up every single title, and then see whether or not somewhere in that title is a string of the letters Google. Once it's got that result, it has to take that and all of its friends of the other 100 billion, or those that match, and then sum them all together. So the virtual machines have to communicate with each other when they're doing aggregations, that's where that shuffling step comes into play. Let's see how much data it's going to process. So less than a minute, just over 30 seconds, it went through 4.1 terabytes of data, and it gave us the result there. And you can see almost a full day of computing, if you're going to be doing that just on a single machine. And you don't even- it doesn't even tell you how many machines were there behind the scenes. So that slot time is a phenomenally interesting metric that just shows you the scale. You waited 31 seconds, behind the scenes, you don't even have to manage them, we're using 24 hours, essentially of compute, boom, just like that. And when you don't need any more, obviously, you're not paying for those machines, you're just paying for the bytes of data that were processed. All right, that's the demo of BigQuery at scale.

### Video - [Get started with BigQuery](https://www.cloudskillsboost.google/course_templates/54/video/521404)

- [YouTube: Get started with BigQuery](https://www.youtube.com/watch?v=K_Z3XXXieZ0)

Now that you’re familiar with the basics of BigQuery, it’s time to talk about how BigQuery organizes your data. BigQuery organizes data tables into units called datasets. These datasets are scoped to your Google Cloud project. When you reference a table from the command line in SQL queries or in code, you refer to it by using the construct: project.dataset.table. What are some reasons to structure your information into projects, datasets, and tables? These multiple scopes —project, dataset, and table— can help you structure your information logically. You can use multiple datasets to separate tables pertaining to different analytical domains, and you can use project-level scoping to isolate datasets from each other according to your business needs. Also, as we will discuss later, you can align projects to billing and use datasets for access control. You store data in separate tables based on logical schema considerations. In the queries we saw earlier, we wrote the query in SQL and selected Run on the UI. What this did was to submit a QueryJob to the BigQuery service. The BigQuery query service is separate from the BigQuery storage service. However, they are designed to collaborate and be used together. In this case, we were querying native tables in the bigquery-public-data project. Querying native tables is the most common case, and is the most performant way to use BigQuery. BigQuery is most efficient when working with data contained in its own storage service. The storage service and the query service work together to internally organize the data to make queries efficient over huge datasets of terabytes and petabytes in size. The query service can also run query jobs on data contained in other locations, such as tables in CSV files hosted in Cloud Storage. So you can query data in external tables or from external sources without loading it into BigQuery. These are called federated queries. In either case, the query service puts the results into a temporary table and the user interface pulls and displays the data in the temporary table. This temporary table is stored for 24 hours, so if you run the exact same query again, and if the results would not be different, then BigQuery will simply return a pointer to the cached results. Queries that can be served from the cache do not incur any charges. It is also possible to request that the query job write to a destination table. In that case, you get to control when the table is deleted. Because the destination table is permanent, and not temporary, you will get charged for the storage of the results. To calculate pricing, you can use BigQuery's query validator in combination with the pricing calculator for estimates. The query validator provides an estimate of the size of data that will be processed during a query. You can plug this into the calculator to find an estimate of how much running the query will cost. You can separate cost of storage and cost of queries. By separating projects A and B, it’s possible to share data without giving access to run jobs. In this diagram, Users 1 and 2 have access to run jobs and access the datasets in their own respective Projects. If they run a query, that job is billed to their own project. What if User 1 needs the ability to access Dataset D in Project B? The person who owns Project B can allow User 1 to query Project B Dataset D and the charges will go to Project A when executed from Project A. The public dataset project owner granted all authenticated users access to use their data. The special setting allAuthenticatedUsers makes a dataset public. Authenticated users must use BigQuery within their own project and have access to run BigQuery jobs so that they can query the Public Dataset. The billing for the query goes to their project, even though the query is using public or shared data. In summary, the cost of a query is always assigned to the active project from where the query is executed. The active project for a user is displayed at the top of the Cloud console or set by an environmental variable in the Cloud Shell or client tools. The project is what the billing is associated with. For example, if you queried a table that belongs to the 'bigquery-public-data' project, the storage costs are billed to that data project. To run a query, you need to be logged in to the Cloud console. You will run a query in your own Google Cloud project and the query charges are billed to your project, not to the public data project. In order to run a query in a project, you need Identity Access Management permission to submit a job. Remember that running a query means that you must be able to submit a query job to the service. Access control is through IAM and is at the dataset, table/view, or column level. In order to query data in a table or view, you need at least read permissions on the table or view. Like Cloud Storage, BigQuery datasets can be regional or multi-regional. Regional datasets are replicated across multiple zones in the region. As with Cloud Storage, BigQuery storage encrypts data at rest and over the wire using Google-managed encryption keys. It’s also possible to use customer-managed encryption keys. Authentication is through IAM, so it’s possible to use Gmail addresses or Google Workspace accounts for this task. Access control is through IAM roles and involves giving permissions. We discussed two of those in read access and the ability to submit query jobs. However, many other permissions are possible. Remember that access control is at the level of datasets, tables, views, or columns. When you provide access to a dataset, either read or write, you provide access to all the tables in that dataset. Logs in BigQuery are immutable and are available to be exported to Cloud Operations. Admin activities and system events are all logged. An example of a system event is table expiration. If, when creating a table, you configure it to expire in 30 days, at the end of 30 days a system event will be generated and logged. You will also get immutable logs of every access that happens to a dataset under your project. BigQuery provides predefined roles for controlling access to resources. You can also create custom IAM roles consisting of your defined set of permissions, and then assign those roles to users or groups. You can assign a role to a Google email address or to a Google Workspace Group. An important aspect of operating a data warehouse is allowing shared but controlled access against the same data to different groups of users. For example, finance, HR, and marketing departments all access the same tables, but their levels of access differ. Traditional data warehousing tools make this possible by enforcing row-level security. You can achieve the same results in BigQuery with access control to datasets, tables, views, or columns, or by defining authorized views and row-level permissions. Sharing access to datasets is easy. Traditionally, onboarding new data analysts involved significant lead time. To enable analysts to run simple queries, you had to show them where data sources resided and set up ODBC connections and tools and access rights. Using Google Cloud, you can greatly accelerate an analyst's time to productivity. To onboard an analyst on Google Cloud, you grant access to relevant project(s), introduce them to the Cloud console and BigQuery web UI, and share some queries to help them get acquainted with the data. The Cloud console provides a centralized view of all assets in your Google Cloud environment. The most relevant asset to data analysts might be Cloud Storage buckets, where they can collaborate on files. The BigQuery web UI presents the list of datasets that the analyst has access to. Analysts can perform tasks in the Cloud console according to the role you grant them, such as viewing metadata, previewing data, executing, and saving and sharing queries. When you provide read access to a dataset to a user, every table in that dataset is readable by that user. What if you want more fine-grained control? In addition to access controls at the table or column level, you can use views. In this example, we are creating a view in Dataset B, and the view is a subset of the table data in Dataset A. Now, by providing users with access to Dataset B, we are creating an authorized view that is only a subset of the original data. Note that you cannot export data from a view, and dataset B has to be in the same region or multi-region as dataset A. A view is a SQL query that looks like and has properties similar to a table. You can query a view just like you query a table. BigQuery supports materialized views as well. These are views that are persisted so that the table does not need to be queried every time the view is used. BigQuery will keep the materialized view refreshed and up to date with the contents of the source table. Giving view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view allows you to share query results with particular users and groups without giving them access to the underlying source data. With column-level security you can achieve a similar use case, so that you can define when to show the content of the column, or when to hide or obfuscate it. You will define a Policy Tag in BigQuery and assign the corresponding users/groups to it. These users will now be able to see the column’s content. You can also define data masking rules and assign them to the Policy Tag and users/groups, so that the content is obfuscated / nullified / or transformed using your own custom logic. If a user/group isn’t included into the Policy Tag’s definition, they won’t be able to query the column at all. In BigQuery, row-level security involves the creation of row-level access policies on a target BigQuery table. This policy then acts as a filter to hide or display certain rows of data, depending on whether a user or group is in an allowed list. An authorized user, with the IAM roles BigQuery Admin or BigQuery DataOwner, can create row-level access policies on a BigQuery table. When you create a row-level access policy, you specify the table by name, and which users or groups (called the grantee-list) should have access to certain row data. The policy includes the data on which you wish to filter, called the filter_expression. The filter_expression functions like a WHERE clause in a typical query. In this example, users in the group:apac can only see partners from the APAC region. In BigQuery, materialized views periodically cache the results of a query for increased performance and efficiency. BigQuery leverages precomputed results from materialized views and whenever possible reads only delta changes from the base table to compute up-to-date results. Materialized views can be queried directly or can be used by the BigQuery optimizer to process queries to the base tables. Queries that use materialized views are generally faster and consume fewer resources than queries that retrieve the same data only from the base table. Materialized views can significantly improve the performance of workloads that have the characteristic of common and repeated queries.

### Video - [Load data into BigQuery](https://www.cloudskillsboost.google/course_templates/54/video/521405)

- [YouTube: Load data into BigQuery](https://www.youtube.com/watch?v=7qQg0zrVAkc)

Next, we’ll talk about how to load new data into BigQuery. Recall from an earlier module that the method you use to load data depends on how much transformation is needed. E-L, or Extract and Load, is used when data is imported as-is where the source and target have the same schema. E-L-T, or Extract, Load, Transform, is used when raw data will be loaded directly into the target and transformed there. E-T-L, or Extract, Transform, Load, is used when transformation occurs in an intermediate service before it is loaded into the target. You might say that the simplest case is E-L. If the data is usable in its original form, there’s no need for transformation. Just load it. You can batch load data into BigQuery. In addition to CSV, you can also use data files with delimiters other than commas by using the field_delimiter flag. BigQuery supports loading gzip compressed files. However, loading compressed files isn't as fast as loading uncompressed files. For time-sensitive scenarios or scenarios in which transferring uncompressed files to Cloud Storage is bandwidth- or time-constrained, conduct a quick loading test to see which alternative works best. Because load jobs are asynchronous, you don't need to maintain a client connection while the job is being executed. More importantly, load jobs don't affect your other BigQuery resources. A load job creates a destination table if one doesn't already exist. BigQuery determines the data schema as follows: If your data is in Avro format, which is self-describing, BigQuery can determine the schema directly. If the data is in JSON or CSV format, BigQuery can auto-detect the schema, but manual verification is recommended. You can specify a schema explicitly by passing the schema as an argument to the load job. Ongoing load jobs can append to the same table using the same procedure as the initial load, but do not require the schema to be passed with each job. If your CSV files always contain a header row that should be ignored after the initial load and table creation, you can use the skip_leading_rows flag to ignore the row. For details, see the documentation on BigQuery load flags. BigQuery sets daily limits on the number and size of load jobs that you can perform per project and per table. In addition, BigQuery sets limits on the sizes of individual load files and records. You can launch load jobs through the BigQuery web UI. To automate the process, you can set up Cloud Functions to listen to a Cloud Storage event that is associated with new files arriving in a given bucket and launch a BigQuery load job. BigQuery can import data stored in the JSON file format, as long as it is newline delimited. It can also import files in Avro, Parquet, and ORC format. The most common import is with CSV files, which are the bridge between BigQuery and spreadsheets. BigQuery can also directly import Firestore and Datastore export files. Another way that BigQuery can import data is through the API. Basically, any place where you can get code to run, can theoretically insert data into BigQuery tables. You could use the API from a Compute Engine instance, a container on Kubernetes, App Engine, or from Cloud Functions. However, you would have to recreate the data processing foundation in these cases. In practice, the API is mainly used from either Dataproc or Dataflow. The BigQuery Data Transfer Service provides connectors and pre-built BigQuery load jobs that perform the transformations necessary to load report data from various services directly into BigQuery. Cloud Storage can be useful in the E-L process. You can transfer files to Cloud Storage in the schema that is native to the existing on-premises data storage and then load those files into BigQuery. BigQuery is a managed service, so you don't have the overhead of operating, maintaining or securing the system. A typical Data Warehouse system requires a lot of code for coordination and interfacing. You can get BigQuery Data Transfer Service running without coding. The core of BigQuery Data Transfer Service is scheduled and automatic transfers of data from wherever it is located (in your data center, on other clouds, in SaaS services) in to BigQuery. Transfering the data is only the first part of building a data warehouse. If you were assembling your own system, you would need to stage the data so that it can be cleaned (data quality), and transformed (E-L-T, extract, load, transform), and processed (put into its final and stable form). A common issue with Data Warehouse systems is late arriving data. For example, a cash register closes late and does not report its daily receipts during the scheduled transfer period. To complete the data, you would need to detect that not all of the data was received, and then request the missing data to fill in the gap. This is called "data backfill" and it is one of the automatic processes provided by BigQuery Data Transfer Service. "Backfilling data" means adding missing past data to make a dataset complete with no gaps and to keep all analytic processes working as expected. Use the data transfer service for repeated, periodic, scheduled imports of data directly from Software as a Service systems into tables in BigQuery. The BigQuery Data Transfer Service provides connectors, transformation templates, and the scheduling. The connectors establish secure communications with the source service and collect standard data, exports, and reports. This information is transformed within BigQuery. The transformations can be quite complicated, resulting in from 25 to 60 tables. And the transfer can be scheduled to repeat as frequently as once a day. The BigQuery Data Transfer Service can also be used to efficiently move data between regions. Notice that you don't need Cloud Storage buckets. BigQuery Data Transfer Service runs BigQuery jobs that transform reports from SaaS sources into BigQuery Tables and Views. Google offers several connectors, including Campaign Manager, Cloud Storage, Amazon S3, Google Ad Manager, Google Ads, Google Play transfers, YouTube channel, YouTube content owner, Teradata migration, and over 100 other connectors through partners. It is a common practice to automate execution of queries based on a schedule or event and cache the results for later consumption. You can schedule queries to run on a recurring basis. Scheduled queries must be written in standard SQL, which can include Data Definition Language and Data Manipulation Language statements. The query string and destination table can be parameterized, allowing you to organize query results by date and time. By maintaining a complete 7-day history of changes against your tables, BigQuery allows you to query a point-in-time snapshot of your data. You can easily revert changes without having to request a recovery from backups. This slide shows how to do a SELECT query to query the table as of 24 hours ago. Because this is a SELECT query, you can do more than just restore a table. You can join against some other table or correct the value of individual columns. You can also do this using the BigQuery command-line tool as shown in the second snippet. Here, we’re restoring data as of 120 seconds ago. You can recover a deleted table only if another table with the same ID in the dataset has not been created. In particular, this means you cannot recover a deleted table if it is being streamed to. Chances are that the streaming pipeline would have already created an empty table and started pushing rows into it. Also be careful using “CREATE OR REPLACE TABLE” because this makes the table irrecoverable. Keep in mind if your data transformations are simple enough, you may be able to do them with just SQL. If Cloud Storage is part of your workflow, you can load files from Cloud Storage into staging tables in BigQuery first, and then transform the data into the ideal schema for BigQuery by using BigQuery SQL commands. BigQuery supports standard DML statements such as insert, update, delete, and merge. There are no limits on D-M-L statements. However you should not treat BigQuery as an O-L-T-P system. The underlying infrastructure is not structured to perform optimally as an O-L-T-P. There are other more appropriate products on Google Cloud for such workloads. BigQuery also supports DDL statements like CREATE OR REPLACE TABLE. In the example on this slide, the replace statement is used to transform a string of genres into an ARRAY. We’ll cover ARRAYs in greater detail later in the course. Lastly, what if your transformations went beyond what functions were currently available in BigQuery? Well, you can create your own! BigQuery supports user-defined functions, or UDF. A UDF enables you to create a function using another SQL expression or an external programming language. JavaScript is currently the only external language supported. We strongly suggest you use Standard SQL though, because BigQuery can optimize the execution of SQL much better than it can for JavaScript. UDFs allow you to extend the built-in SQL functions. UDFs take a list of values, which can be ARRAYs or STRUCTs, and return a single value, which can also be an ARRAY or STRUCT. UDFs written in JavaScript can include external resources, such as encryption or other libraries. Previously, UDFs were temporary functions only. This meant you could only use them for the current query or command-line session. When you create a UDF, BigQuery persists it and stores it as an object in your database. What this means is you can share your UDFs with other team members or even publically if you wanted to. The BigQuery team has a public GitHub repo for common User Defined Functions at the link you see here.

### Video - [Lab Intro: Loading Data into BigQuery](https://www.cloudskillsboost.google/course_templates/54/video/521406)

- [YouTube: Lab Intro: Loading Data into BigQuery](https://www.youtube.com/watch?v=IpWE73spVDw)

>> Now it's time to get some hands-on experience with a lab. In this lab, you're going to practice loading data into BigQuery. The primary objective of this lab is to load data into BigQuery using both the command line interface and the Cloud Console. You'll also get experience loading several datasets into BigQuery and using the data description, language or DDL.

### Lab - [Loading data into BigQuery](https://www.cloudskillsboost.google/course_templates/54/labs/521407)

This lab focuses on how to ingest data into tables inside of BigQuery.

- [ ] [Loading data into BigQuery](../labs/Loading-data-into-BigQuery.md)

### Video - [Explore schemas](https://www.cloudskillsboost.google/course_templates/54/video/521408)

- [YouTube: Explore schemas](https://www.youtube.com/watch?v=b10CAx4X9n4)

>> Now let's dive into the world of Data Warehouse schemas. Designing efficient schemas that scale is a core job responsibility of any data engineering team. BigQuery hosts many public datasets and schemas for you to explore on popular topics like daily weather readings, taxi cab logs, health data, and more. Let's explore some of these public dataset schemas using SQL.

### Video - [Demo: Exploring Schemas](https://www.cloudskillsboost.google/course_templates/54/video/521409)

- [YouTube: Demo: Exploring Schemas](https://www.youtube.com/watch?v=qEIEFcm4l0g)

>> Welcome back to another BigQuery demo. In this one, it's a little bit of a meta demo, meaning we're going to be using a lot of BigQuery metadata that's stored inside of BigQuery native storage, and some unique functions like information schema, and tables to explore the metadata of a dataset that's given to you. Why is this important? Well, as a data engineer, you're often given a dataset or tables within a dataset or multiple datasets as part of a GCP project, and you'd have a very short amount of time to figure out a lot of key information about those tables. How many tables are there? How many columns are there? Are any of those columns partitioned or clustered columns? What's the data size? When were the tables last updated? And yes, you can go in and click on the UI and individual places inside of BigQuery to find that information, but it wouldn't be amazing to use SQL to query information about the metadata that already exists inside of BigQuery. So we're going to do that with a couple quick queries here, but first, we need a dataset. As usual, we're going to be going to the BigQuery public datasets and getting some information there. So I'm going to copy the first query inside of there. And inside of BigQuery, we're going to paste the query. Now, if you notice, I have my project BigQuery public data, and then I have a table. I'm just using- there's many datasets inside of BigQuery public data. I'm using the baseball dataset, which has multiple tables. If you notice, I'm using this suffix underscore underscore tables, which is interesting. So if you hold down the Command key on your Macs, if you have a Mac, or use a shortcut there for the Windows, you'll see that for tables, if I'm just bringing this up, it has some interesting metadata. So it has your project, your dataset, your table ID and it has some short but useful information about when the table was created, when it was last modified or updated, how many rows it has, you don't need to do this select count star, what's the size of the table in bytes, and whether or not it's a table or a view. I think table is one, view is two. Now, I already have the- the whole point of this query is because this is unreadable to me, it's just milliseconds from the dawn of time or Unix epoch or something like that. And the size in bytes, I can't do the conversion in my head. So that's literally all the rest of my query does here for you, is it just as a little bit of rounding from bytes to gigabytes, from the millisecond timestamp to the actual readable timestamp. So actually running that query will just translate those results into something that's a little bit more readable. So we have three tables inside of our baseball dataset, we have games wide, games post wide, and some baseball schedules. You can see the largest table here is about two gigabytes, and it has 761,000 rows. I can infer without much analysis that there are over almost 1,700,000 baseball games that we can have analysis on, immediately right off the bat, no select star queries or anything like that. All right, so that's just information about it, and again, you can just replace baseball with say, New York. And then boom, you can get all the information about the New York data tables that are in there, a lot more tables. And you can see that there is a Yellow Taxi Cab trips, 311 service requests for New Yorkers, a lot of motor vehicle collisions. So New York has a lot of tables in that dataset for BigQuery public data. And you can see the row count in there as well, large, very large datasets. But literally, you're just plugging in your project, your dataset, and you get a lot of metadata about the tables. So it's comparing information across tables very easily. So now if we're going to drill in to the columns of data, how many columns of data are present in one of those tables? Again, I'm just going to be using baseball, for example, but you can replace that with one that's interesting to you. Here, we're using information schema dot columns. Executing that, let's get all that data. All right, here we go. So we have the table, which is baseball, that's the dataset it belongs to, the table name is games wide. Don't forget, we also have a couple more tables in this dataset, we're not doing a Where clause filter. So we have 306 columns in total. So no kidding, that's a very wide table. But hopefully, you'll be able to see games wide, there should be two other tables, games post wide, and there was another one there as well. So if you wanted to do a filter for just a single table, you could see, let's just do a quick Where clause filter, where the actual table name is just this one singular table, games wide. And then we'll see just simply by row count, this table has 145 columns in it. Wow, that is a wide table. No kidding. And then the ordering position of the columns, whether or not it's allowed to be No, the data type, is it a generative column? Is it hidden? Here's really interesting, as a data engineer, is this a- is there a partitioning on that table itself? And if there is partitioning, is there clustering as well, which is super interesting. So that's literally what this next query is going to do for us, is just a very simple Where clause basically to say, "Hey, for performance reasons, one of the easiest things that you can do is just have a partitioned and clustered columns in very large datasets if your use case warrants it." Let's see if there are any in this particular dataset. So I'm going to run this. And it's going to look again filtering on the partition column. And then no. So it's very easy, look through all of the different tables within that baseball dataset, and then you got nothing. Let's see if the New York one has any in it as well. No, no partitioned, no clustered columns there. So again, some immediate insight that you can give back is, "Hey, let's explore the benefit of partitioned or clustered columns." All right, the last query that I'm going to show you here is, you can basically get metadata across the different datasets just by doing a simple union all. I've chosen about 10 or 15 interesting ones from the BigQuery public datasets. And I want to see, we looked at the first table, three tables inside of the baseball dataset, I want to basically list all the tables from all these different datasets. And I want to order it by the dataset that has the table with the most rows. So let's see which table out of all of these has the most row, just pasting in that query. I think it'll probably be those taxi transactions. So let's see. Getting all that metadata together- Oh, I was wrong. Wow. So we have- and again, you can change this with the timestamp seconds query that I had a little bit earlier. It's actually the GitHub repository, their files inside of GitHub, and the row count- Wow, can I buy a comma? - is 2.3- is that billion? Yeah, 2.3 billion. And you can see the size that are on disk there as well. So it's GitHub, GitHub, GitHub. Wikipedia is up there. It's only number two, though. And then here's my New York taxi cabs there as well. And again, this is just unioning together, unioning again inside of SQLs mashing together rows vertically, whereas joins is joining together columns horizontally, and it just mashes them together here as well. Lastly, I'll show you a bonus query, I'm not going to run it for you, though, is if you wanted to, you want to view all the datasets within your given GCP project, actually, a lot, I will run this for you because it looks pretty cool. I'm going to copy this. And I want to see, let's see if- data to insights, this is a public- at least the tables within here are public. Let's see if we can- So all I'm doing is basically saying, if somebody just gives you the project name, just the project name, you want to query all the datasets they're within. It looks like yeah, we don't have- it's permissions denied. So we can use our own project. I have a couple of tables inside of here, so I'm copying the project name. Hopefully, I'm admin for my own project. And essentially, what you're going to see is I've just a bunch of example datasets from previous demos. And you'll see from, for example, you'll have all of the different datasets returned, and then within there, all the tables and views inside of them. And then let's see which ones have been modified most recently, last modified time descending. So let's go ahead and run that. Again, you would just replace your project name inside of there as well. So the use case is somebody gives you a GCP account, and you're like, "Well, I really want to just look at what came up inside of here." All right, cool. So we have, it looks like I was doing some machine learning stuff on movie recommendations data most recently. Yep. And I actually get the dataset description, and it's been around for 39 days last modified in there as well. So you can get a general sense for all of the- and this is- I think this is only ones that actually have a description for it- all of the different dataset values and table values within a given project, assuming you have access to query it. And that's what information schema dot schemata does, and you can get schemata options. A little bit more advanced but that's also there for you as well. A really interesting use case is all the way at the bottom, let me get this on screen. All the way at the bottom, this link right here will show you, hey, if I wanted to recreate- check this out- if I weren't to recreate all the tables in my dataset, so for example, if I wanted to just have something that says, hey, I have a staging environment or a production environment, I need to recreate all these data tables, or at least the schema for them to populate them with data. And then doing the Create or replace statements, if I have 100 different datasets here, it's awful. Is there a way to programmatically create that? And there is. So you can actually say, the pre-canned query that's given to you actually uses a Create function, and it respects partitioning and it actually does all of this stuff for you. And it can catch together those Create or Replace table names and the list of the columns. And if it has partitioning and clustering and options like a description, it will all do that for you. So for example, the output looks like this, you want to get a single dot SQL file that will recreate all of your projects and all of your datasets. Boom, here you go. There's the table one, my dataset population by zip. Boom, there's the table two, this is the GitHub commits or something like that as well. Why is this useful? Well, it's really a good idea to track your schema changes over time, like if your column definitions change. So generally what I would do on a production project is generate this, check this into version control somewhere, and anytime there's changes to my schema, I would regenerate this and then check it into version control so I can see how my schema has evolved over time. A little bit more advanced use case but that is out there for you as well.

### Video - [Schema design](https://www.cloudskillsboost.google/course_templates/54/video/521410)

- [YouTube: Schema design](https://www.youtube.com/watch?v=eBJ0za5lTH0)

>> Next we will talk about efficient data warehouse schema design. Take a look at the original data table here and the normalized data tables which contain the same data. The data in the original table is organized visually as you might have used merge cells or columns in a spreadsheet. But if you had to write an algorithm to process the data, how might you approach it? Access could be by rows, by columns, by rows then columns, and the different approaches would perform differently based on the query. Also, your method might not be parallelizable. The original data can be interpreted and stored in many ways in a database. normalizing the data means turning it into a relational system. This stores the data efficiently and makes query processing a clear and direct task. Normalizing increases the orderliness of the data, it is useful for saving space. Many people with database experience will recognize this procedure. Normalizing data usually happens when a schema is designed for a database. Denormalizing is the strategy of allowing duplicate field values for a column in a table in the data to gain processing performance. Data is repeated rather than being relational. Flattened data takes more storage, but the flattened non-relational organization makes queries more efficient because they can be processed in parallel using columnar processing. Specifically, denormalizing data enables BigQuery to more efficiently distribute processing among slots resulting in more parallel processing and better query performance. You would usually denormalize data before loading it into BigQuery. However, there are cases where denormalizing data is bad for performance. Specifically, if you have to group by a column with a one to many relationship. In the example shown, order ID is such a column. In this example, to group the data, it must be shuffled. That often happens by transferring the data over a network between servers or systems. shuffling is slow. Fortunately, BigQuery supports a method to improve the situation. BigQuery supports columns with nested and repeated data. In this example, a denormalized flattened table is compared with one that has been denormalized and the schema takes advantage of nested and repeated fields. Order ID is a repeated field. Because this is declared in advance, BigQuery can store and process the data respecting some of the original organization in the data. Specifically, all order details for each order are co-located, which makes retrieval of the whole order more efficient. For this reason, nested and repeated fields are useful for working with data that originates in relational databases. Nested columns can be understood as a form of repeated field. It preserves the relational qualities of the original data and schema while enabling columnar and parallel processing of the repeated nested fields. It is the best alternative for data that already has a relational pattern to it. Turning the relation into a nested or repeated field improves BigQuery Performance. Nested and repeated fields help BigQuery work with data source in relational databases. Look for nested and repeated fields whenever BigQuery is used in a hybrid solution in conjunction with traditional databases.

### Video - [Nested and repeated fields](https://www.cloudskillsboost.google/course_templates/54/video/521411)

- [YouTube: Nested and repeated fields](https://www.youtube.com/watch?v=8hw2_YqokCI)

>> Let's take a closer look at BigQuery's support for nested and repeated fields and why this is such a popular schema design for enterprises. I'll illustrate by using an example from a real business running on Google Cloud. GoJek is a company in Indonesia that is well known for its ride booking service. And they process over 13 petabytes of data on BigQuery per month from queries to support business decisions. What kind of decisions? For GoJek, they track whenever a new customer places an order, like hail a ride with their mobile app, that order is stored in an orders table. Each order has a single pickup location and drop off destination. For a single order, you could have one or many events like ride ordered, ride confirmed, drive on route, drop off complete, et cetera. As a data engineer, how would you efficiently store these different pieces of data in your data warehouse? Keep in mind, you need to support a large user base querying petabytes per month. Well, as you saw earlier, we could store one fact in one place with the normalization route, which is typical for relational systems. Or we could go the fully denormalized route and just store all levels of granularity in a single big table, where you would have one order ID like 123 repeated in a row for each event that happens on that order. Faster for querying, sure, but what are the drawbacks? For relational schemas, normalized schemas, often the most intensive computational workloads are joins across very large tables. Remember, RDBMSs are record based, so they have to open each record entirely, and pull out the join key from each table where a match exists. And that's assuming you know all the tables that need to be joined together. Imagine for each new piece of information about an order like promotion codes, or user information, and you could be talking about 10 plus table join. The alternative has different drawbacks. Pre-joining all your tables into one massive table makes reading data faster, but you now have to be really careful if you have data at different levels of granularity. In our example, each row would be at the level of granularity of a specific event like driver confirmed for a given order. What does that mean for an order ID like 123? It is duplicated for each event in that order. Imagine if you're looking to join higher level information like the revenue per order, and you now have to be exceedingly careful with aggregations to not double or triple count your duplicate order IDs. See the problem? One common solution in enterprise data warehouse schemas is to take advantage of nested and repeated data fields. You can have one row for each order, and repeated values within that one row for data that is at a more granular level. For example, you could simply have an array of timestamps as your events. Let's see an example to illustrate this point. Here you see it clearly, shown here on screen are just four rows for four unique order IDs. Notice all that gray space in between the rows. That's because the event status and event time is at a deeper level of granularity. That means there are multiple repeated values for these events per each order. An array is a perfect data type to handle this repeated value and keep all the benefits of storing that data in a single row. I mentioned the fields event dot status and event dot time. If this is one giant table, what is a dot doing in those column names? There are no other table aliases we've joined on. What's up with those fields. Event, pickup, and destination are what are called struct or structured data type fields in SQL. This isn't BigQuery specific. Structs are standard SQL data types and BigQuery just supports them really well. Structs you can think of as pre-joined tables within a table. So instead of having a separate table for event and pickup and destination, you simply nest them within your main table. So let's recap. You can go deep into a single field and have it be more granular than the rest by using an array data type like you see here for status and time. And you can have really wide schemas by using structs which allow you to have multiple fields of the same or different data types within them, much like a separate table would. The major benefit of structs is that the data is conceptually pre-joined already. So it's much faster to query. People often ask, with really wide schemas like 100 columns, how is it still fast to query? Remember that BigQuery is column based storage, not record based when storing data out on disk. If you did just account order underscore ID here to get your total orders, BigQuery wouldn't even care that you have 99 other columns, some of which are more granular with array data types, it wouldn't even look at them. That gives you the best of both worlds if you're an analyst, lots of data all in one place, and no issues with multiple granularity pitfalls when doing aggregations. Now it's your turn to practice reading one of these schemas that has nested and repeated fields, take a moment and spot those structs. As a hint, you can look at the field name to see any field with a dot in the name or you can look at the data type for any field values of the type record, which means struct. Did you get them all? Here are the four strokes in this dataset you saw earlier: events, pickups, destination, and duration. Duration is a new one, but we can simply keep adding more dimensions to our dataset by adding more structs. Remember, structs let you build really wide and informative schemas. Now it's time to go deep. Find the array data types in this schema. As a hint, look at the mode and find the repeated values. Got them? In this schema, the repeated value is the event struct, which means here we have an array of event structs with each having a status and time possibly. A critical point I like to make here is that struct and array data types in SQL can be absolutely independent of each other. You can have a regular column in SQL be an array column that has nothing to do with any struct. Likewise, you can have a struct that has zero array field types in its columns. The benefit of using them together is that arrays allow a given field to go deep into granularity, and structs allow you to organize all those useful fields into logical containers instead of separate tables. So here's the cheat sheet. Structs are a type of record when looking at a schema, and arrays are of mode repeated. Arrays can be of any single type, like an array of floats, or an array of strings, et cetera. Arrays can be part of a regular field or be part of a nested field nestled inside of a struct, a single table can have zero to many structs. And lastly, the real mind bending point is that a struct can have other structs nested inside of it as you will soon see in your upcoming Lab, which uses the real Google Analytics schema. We've been talking a lot about nested and repeated fields. So you're probably wondering what to do with your existing star schema, snowflake and third normal form data. The great news is that BigQuery also works well with those schema types. Use arrays and structs when your data naturally arrives in that format, and you'll benefit immediately from optimal performance. For the other schema types, bring them directly to BigQuery and you'll likely be pleased with the performance.

### Video - [Demo: Nested and repeated fields](https://www.cloudskillsboost.google/course_templates/54/video/521412)

- [YouTube: Demo: Nested and repeated fields](https://www.youtube.com/watch?v=_whHSXutbuI)

>> In this demo, we're going to be querying a cool dataset. This is a Bitcoin dataset in BigQuery. And what makes it cool besides the subject matter is it actually has nested and repeated columns. So if this is the first time that you've worked with structs or arrays, This demo will be super useful for you. You get all these demos in our public repository under the data engineering course within demos. So let's follow along. So the first thing that we want to do is access the datasets. This first query is going to give us where the dataset comes from. So inside of GCP, I'm going to paste the query there. One of my favorite things is one of the shortcuts that you can do. I'm on a Mac. So if I just hold the Command key down, I can highlight all the tables that are in the query. So if you're just given a query from a demo, you can see we've got a Bitcoin public dataset as a project, the dataset is Bitcoin blockchain. The table name, we have two, the blocks for inside of the blockchain, you can't have a blockchain without blocks, and you've got the transactions as well. So once these are highlighted, if you click on the table name, it will actually take you to the schema, which is super useful. So again, dataset, and then two different tables. Clicking on blocks, we can take a look at how much data is in here just by looking at the metadata in BigQuery. Schema, those are the different columns that we have available, a lot more on that in just a second. Let's see how big this dataset is. It is half a terabyte, 540,000 rows of blocks that are here. And as you see in a minute, you can have multiple transactions as part of a Bitcoin block that's solved by Bitcoin miners. You can actually see that when you preview the data. If you haven't seen nested and repeated fields before, this may look a little surprising. So let's see, I'll zoom out just for a little bit. And you can see one row ID and it's you got the block ID, it's just an identifier for the individual block, which is a record on this shared distributed ledger, that is blockchain for the specific Bitcoin network. And if you scroll over, you see a lot of gray area here, what's going on with that? Where's row number two? Well, row number two doesn't show up because you have different levels of granularity inside of the same data table. And this is because you have nested data and repeated columns in there as well. So you can see, for a given block, a block can have more than one transaction. So this block ending in A3876 has multiple transactions as it's scrolled to the right. It has as part of this, miners have confirmed that this Bitcoin transaction, either sending or receiving Bitcoins, is in there as well and ending in 89- ending in 1995. A few more as well. So it's kind of like if you've used merged cells inside of Excel or Google Spreadsheets, similar concept. This is a nested field because it has a prefix transactions dot transaction ID, that means it's a struct. And it's a repeated field because you can have more than one value, in repeated, when you hear that word repeated field, immediately think of the data type arrays. So how do you query this stuff? Right? So we've got our query here, we're going to get- give us the block ID. You saw that one, there's multiple sequence numbers that could be a part of the block one, the most- the latest, the maximum. And also, as I mentioned before, you can have multiple transactions, Bitcoin transactions that are part of a single block on the register. So we got blocks, and as you saw here, the first query, we're not going to use this nested column, we're going to pretend like the transaction struct, transactions dot whatever, transactions dot inputs, transaction dot transaction ID doesn't exist, we're actually going to just join on a separate table, a transactions table that also has the same information. And this is more typical if you've used relational schemas before and you've got just another table that you just do a join against using a common key- in this particular case, it's block ID- to get the count of those transactions. I'm going to show you yes, this will absolutely work, and we're going to query it, as the demo is going to tell you a little bit. You want to look at that slot time consumed, that somewhat kind of compute power distributing parallels behind the scenes, and the byte shuffled, which is those VMs behind the scenes talking to each other. So it processed about 48 gigabytes, did it in about 12 seconds. On your execution details right here, you can see, if you are going to process this in linear 101 machine, it could take two hours, if not more. And we had to shuffle around 18 gigabytes of data in order for all those VMs to talk to each other and get that data in there. And largely you can see as you scroll down, most of that work was done in the join. This is the query kind of execution plan, you can drill open and get a little bit of the pseudocode of how it actually runs behind the scenes and processes that. So showing the- now that you know that inside of that table, you don't need to have the transactions as a join, it's already part of the table in a wide schema as part of blocks, you also have this transactions struct, a struct is just a container. For other fields, you can think of it as a table that's already been pre-joined. To further illustrate that point, going back to the schema, you can scroll down, you can see anything of type record is a struct, S-T-R-U-C-T. And that is not unique to BigQuery, that's a generic SQL, BigQuery that supports it. And you can see transactions that struct has a bunch of different nested columns as part of it, and some of those columns happen to be repeated. What that means is structs can have other structs as their children, so transactions dot inputs dot something else. And those children, those column fields can also have the- they can be repeated, meaning that their column type is in array of strings, in array of integers, you're going to see what that means in the later half of this demo. So let's paste this, if you remember, that was shuffling a lot of query bytes. Now, instead of doing the join, you notice we only have one table here, and it processed in zero seconds because I ran this before, it was cached. Any time you're doing a performance demo instead of more query settings, you can disable cache results as well. Wouldn't that be great, right? So let's see, it took about 12 seconds for the first query to run, it's processing I think about like 40 gigabytes or something like that. So we're going to see what the performance improvement is. So it's a little bit faster, it's seven seconds, and about half the data that's being processed. And you can notice that a dramatic shift in the number of bytes shuffled because all the data is in one place. BigQuery supports what we call de-normalization, having super wide schemas in a single table, because BigQuery is column based. And you can see, we're not shuffling gigabytes of data anymore. Like, you know, I can't remember what the last number was, it was 40 gigabytes or something like that. But the amount here that we're shuffling is just megabytes, 88 megabytes, very trivial amount. And as well as the slot time consumed, this isn't two hours, it's just nine minutes. So it's much, much, much faster to do the query instead of doing the join to have this separate nested field via the struct inside of there as well. So that's how you query. For those following along at home, if you're wondering like, from this table as B comma here, the comma actually is the exact same thing as saying a comma is an implicit cross join. Inside of the world of structs, this is a correlated cross join. So the analogy is, you have multiple tables. Conceptually, you can think of these as tables, multiple child tables as part of a parent table. That's how I like to think of it. So your structs, these record values, or kind of like multiple tables within the same table. And you can- in order to access their columns, you have to do what's called a correlated cross join. So you can write it out, cross join, generally, folks just exclude that. And when you need to unpack these struct values, you can just do the comma into them. And that's how you get the values out of the struct. So it's similar to joining, not joining any other tables, there's no- you join command inside of the referring to another table. Okay, part two, that's explaining structs, and you can see that's much faster and processes a lot less resources. But that just covers the nested part, the repeated part is what's super interesting to me, because you have different levels of granularity. One block, as you saw previously had multiple transactions inside of it. Let's find something interesting. So you're going to learn a little bit about Bitcoin here, but don't worry, it'll apply to any of your datasets even if you're not working in cryptocurrency. So what we want to do is, inside of this dataset- I'm going to pull up in this query- inside of this dataset is a total amount of Bitcoins that are trading hands as part of the distributed ledger here. So I'm going to open up the blocks and let's preview the table. And the actual output column, how much is done in this transaction, let me zoom out a little bit. So that's the block ID all the way on the right. And again, this is a public dataset, is what's called output Satoshis. Satoshi in the Bitcoin world is, I think, if you multiply it by this number here, it's the smallest atomic unit, so you can't get below one Satoshi. If you multiply it by this number here, which is like 1E8 I think, you get actually- you can convert this, it's a direct correlation to an actual value in Bitcoin. So you'll- generally, you always see huge dollar, but it's kind of like cents versus dollars if you're in the US. So we want to get the block ID, we want to get convert this timestamp numerical value into a timestamp that we can read. We want to get the transaction ID and we want to get the value, the original Satoshi value and then the actual BTC or Bitcoin value. Same thing as before, we want to get the highest amount of bitcoins in a given transaction, the top 10. So you see it's the same thing as before. We have a struct that has transactions and we also have a struct that has inputs, that's fine. I don't think we actually need that because we're not actually doing anything on the inputs. But when you run this, you're going to get the extremely popular error that says "can't access some field with a type array." Inside of there is a integer, repeated value. So what does that actually mean? So you get that error, and you did your correlated across joins that we talked before with the comma, and you're previewing things. And you're going like, what- I just literally- it's literally called outputs dot outputs. So that's literally the column value here, what's going on. I already did my unpacking of transactions, beta transactions as T. So it's T dot outputs. Outputs, great, absolutely. What's not working? Well, you see that there's multiple for a given transaction. So where's the transaction? Transaction ID is right here. So this one, follow it visually as we scroll to the right. There's multiple possible output values for Satoshis, for a given transaction ID. So you have data that's on two different levels of granularity. So the first thing that we have to do is we have to un-nest or unpack this array of values and get them on individual rows. So row one, row two, row three, because right now they're all nested up in an array as part of one row. How do you do the unpacking of arrays? Well, that's with the un-nest command. So what that actually looks like is we want to take the- let's find the array field. Let me just get to us here. And we want to un-nest the- the actual column name, it's a nested struct within transactions, much like there was transactions dot inputs or transactions dot outputs, we've already crossed joined transactions as T. So I don't need to type in full transactions dot outputs. You can use this, this one's already been aliased. So we can use transactions T dot outputs. So we're going to break apart that repeated struct. How do you break it apart? You use unnest, as give it a creative alias, I put some notes in the demo about making sure that you have got correct aliases. And knowing that this is inside of our dataset, it's blocks dot transactions dot output. Anytime you're talking dots, you're using a lot of structs inside of your dataset, which helps you with the performance, and then we can literally, instead of saying the outputs there, we can just say, all right, after you've broken everything apart, this is the final result in order to unpack those arrays and get them on a single row value. So when you actually query this, you're not going to see these gray spaces anymore, everything's going to be on the same level of granularity. And then hopefully, you'll get the top 10 transactions for Bitcoin value, the timestamp and then the block that they belong to. So let's take a look while this runs. Running, running, running, running, here we go. So this block had a transaction that was the largest. How large are we talking? So that huge number of Satoshis which I just can't convert in my head, that's what we did in the query, is 500,000 Bitcoin, it's a lot of Bitcoin, and on 2011. So you can see this is how you can access those array values automatically using the unnest breaking them apart. So last thing we want to do is say, all right, you mentioned previously that you can have more than one transaction for a given block. So how Bitcoin works is you have blocks that are solved by mathematical hashing by these large compute power to mine and kind of solve these blocks. And as part of those blocks getting solved, there are unconfirmed transactions, people sending and receiving cryptocurrency that get recorded into the shared distributed ledger that is a block. So one block can have many transactions associated with it. And to prove that, the last part of the demo has literally just- this is the previous query that we just ran- is just taking that block that has that really large Bitcoin transaction, and just saying, "All right, well, what other transactions are part of that block? And can we find that?" It's going to be one- if you remember that number, that five with a lot of zeros on it, hopefully, we'll be able to find that within this. All I did was, I found this block ID, and I just want to filter out the entire blocks table and just pull that up. And this should have the block. You can see again, one row has many different field values in it. That's where you get the nested because the structs and repeated, because these are repeated data type values of the arrays, multiple different transactions in here, scrolling all over to the right, let's try to find that. Boom, there it is. That's a super large value, but it wasn't the only transaction that appeared inside of that block. You can see a bunch of other transactions that are in there as well. There's a lot of interesting web resources if you literally just Googled Bitcoin, or whatever cryptocurrency network, and the block ID, there's a lot of websites that literally will build web interfaces that show you a little bit more information about the distributed ledger and all the transactions for that. But for our purposes, it's just a great example to show you the differences between a highly normalized schema- normalized means many different tables that you have to join together- versus denormalized, which is where you can have a lot of what looks like conceptually, tables join within tables, like transactions, transactions dot inputs has other columns, transactions dot outputs has other columns there as well. So don't be afraid if you see really, really long scroll bar schemas inside of BigQuery. That is a performance best practice.

### Video - [Design the optimal schema for BigQuery](https://www.cloudskillsboost.google/course_templates/54/video/521413)

- [YouTube: Design the optimal schema for BigQuery](https://www.youtube.com/watch?v=ernjvTTGAwk)

>> Let's recap some of the ways to design the schema of tables to improve query performance and lower query costs. It's much more efficient to define your schema to use nested repeated fields instead of joins. Suppose you have orders and purchase items for each order. In a traditional relational database system, you'd have two tables, one table for purchase items, and another for orders with a foreign key to connect the two tables. In BigQuery, it's much more efficient if you store each order in a row and have a nested repeated column called purchase_item. Arrays are a native type in BigQuery. Learn to think in terms of arrays. When you have dimension tables that are smaller than 10 gigabytes, keep them normalized. The exception to this is if the table rarely goes through update and delete operations. If you cannot define your schema in terms of nested repeated fields, you have to make a decision on whether to keep the data in two tables or denormalize the tables into one big flattened table. As a datasets tables increase in size, the performance impact of a join increases. At some point, it can be better to denormalize your data. The crossover point is around 10 gigabytes. If your tables are less than 10 gigabytes, keep the tables separate and do a join.

### Video - [Lab Intro: Working with JSON and Array data in BigQuery](https://www.cloudskillsboost.google/course_templates/54/video/521414)

- [YouTube: Lab Intro: Working with JSON and Array data in BigQuery](https://www.youtube.com/watch?v=2VbxU67SZlo)

>> In the next lab, you'll get some experience working with JSON and array data in BigQuery. The objectives of this lab are to load semi-structured JSON data into BigQuery and to learn how to create and query arrays and structs. You will also query nested and repeated fields.

### Lab - [Working with JSON and Array data in BigQuery 2.5](https://www.cloudskillsboost.google/course_templates/54/labs/521415)

In this lab you will work with semi-structured data (ingesting JSON, Array data types) inside of BigQuery. You will practice loading, querying, troubleshooting, and unnesting various semi-structured datasets.

- [ ] [Working with JSON and Array data in BigQuery 2.5](../labs/Working-with-JSON-and-Array-data-in-BigQuery-2.5.md)

### Video - [Optimize with partitioning and clustering](https://www.cloudskillsboost.google/course_templates/54/video/521416)

- [YouTube: Optimize with partitioning and clustering](https://www.youtube.com/watch?v=XvRgxJvk6Wc)

>> Next up is optimizing with partitioning and clustering. In a table partitions by a date or a timestamp column, each partition contains a single day of data. When the data is stored, BigQuery ensures that all the data in a block belongs to a single partition. A partition's table maintains these properties across all operations that modify it, query jobs, data manipulation language, DML statements, Data Definition Language, DDL statements, load jobs and copy jobs. This requires BigQuery to maintain more metadata than a non-partitioned table. As the number of partitions increases, the amount of metadata overhead increases. One of the ways you can optimize the tables in your data warehouse is to reduce the cost and amount of data read by partitioning your tables. For example, assume we have partitioned this table by the event date column. BigQuery will then change its internal storage so the dates are stored in separate shards. Now, when you run a query with a WHERE clause that looks for dates between 01-03 and 01-04, BigQuery will have to read only two fifths of the full dataset. This can lead to dramatic cost and time savings. You enable partitioning during the table creation process. This slide shows how to migrate an existing table to an ingestion time partitioned table. Using a destination table, it will cost you one table scan. As new records are added to the table, they will be put into the right partition. BigQuery creates new date based partitions automatically with no need for additional maintenance. In addition, you can specify an expiration time for data in the partitions. Partitioning can be set by ingestion time on a timestamp, date or date time column, or based on a range of an integer column. Here, we are partitioning customer_ID in the range zero to 100 in increments of 10. Although more metadata must be maintained, by ensuring that data is partitioned globally, BigQuery can more accurately estimate the bytes processed by a query before you run it. This cost calculation provides an upper bound on the final cost of the query. The good practice is to require that queries always include the partition filter, make sure that the partition field is isolated on the left side, because that's the only way BigQuery can quickly discard unnecessary partitions. An example of this in practice can be found in the blog Optimizing BigQuery, Cluster Your Tables. A link to the blog is available in the course resources. Clustering can improve the performance of certain types of queries, such as queries that use Filter clauses, and those that aggregate data. When data is written to a clustered table by a query or a load job, BigQuery sorts the data using the values in the clustering columns. These values are used to organize the data into multiple blocks in BigQuery storage. When you submit a query containing a clause that filters data based on the clustering columns, BigQuery uses the sorted blocks to eliminate scans of unnecessary data. Similarly, when you submit a query that aggregates data based on the values and the clustering columns, performance is improved, because the sorted blocks co-locate rows with similar values. In this example, the table is partitioned by event date, and clustered by user ID. Now, because the query looks for partitions in a specific range, only two of the five partitions are considered. Because the query looks for user ID in a specific range, BigQuery can jump to the row range and read only those rows for each of the columns needed. You set up clustering at table creation time. Here, we are creating the table partitioning by event date and clustering by user ID. We are also telling BigQuery to expire partitions that are more than three days old. The columns you specify in the cluster are used to co-locate related data. When you cluster a table using multiple columns, the order of columns you specify is important. The order of the specified columns determines the sort order of the data. Over time, as more and more operations modify a table, the degree to which the data is sorted begins to weaken, and the table becomes only partially sorted. In a partially sorted table, queries that use the clustering columns may need to scan more blocks compared to a table that is fully sorted. You can re-cluster the data in the entire table by running a select asterisk query that selects from and overwrites the table. But guess what, you don't need to do that anymore. The great news is that BigQuery now periodically does auto re-clustering for you. So you don't need to worry about your clusters getting out of date as you get new data. Automatic re-clustering is absolutely free and automatically happens in the background. You don't need to do anything additional to enable this. Partitioning provides a way to obtain accurate cost estimates for queries and guarantees improved cost and performance. Clustering provides additional cost and performance benefits in addition to the partitioning benefits. BigQuery supports clustering for both partitioned and non-partitioned tables. When you use clustering and partitioning together, the data can be partitioned by a date, date time or timestamp column, and then clustered on a different set of columns. In this case, data in each partition is clustered based on the values of the clustering columns. Partitioning provides a way to obtain accurate cost estimates for queries. Keep in mind, if you don't have partitioned columns, and you want the benefits of clustering, you can create a fake underscore date column of type date and have all the values be null.

### Video - [Lab Intro: Partitioned Tables in BigQuery](https://www.cloudskillsboost.google/course_templates/54/video/521417)

- [YouTube: Lab Intro: Partitioned Tables in BigQuery](https://www.youtube.com/watch?v=_d6DTowYsT0)

In this lab, you practice creating date-partitioned tables in BigQuery. Specifically, you query a partitioned dataset, and then you'll create dataset partitions to improve the query performance and reduce the overall cost.

### Lab - [Partitioned Tables in Google BigQuery](https://www.cloudskillsboost.google/course_templates/54/labs/521418)

This lab focuses on how to query partitioned datasets and how to create your own dataset partitions to improve query performance, which reduces cost.

- [ ] [Partitioned Tables in Google BigQuery](../labs/Partitioned-Tables-in-Google-BigQuery.md)

### Video - [Review](https://www.cloudskillsboost.google/course_templates/54/video/521419)

- [YouTube: Review](https://www.youtube.com/watch?v=ii3N63RXjOc)

>> I started by describing what makes a modern data warehouse and what distinguishes a data lake from an enterprise data warehouse. You were then introduced to BigQuery, a scalable data warehouse solution on Google Cloud. You don't need to provision resources before using BigQuery unlike with many relational database systems. BigQuery allocates storage and query resources dynamically based on your usage patterns. BigQuery enables you to structure your information into datasets, projects, and tables. You can use multiple datasets to separate tables pertaining to different analytical domains. And you can use project level scoping to isolate datasets from each other according to your business needs. Also, you can align projects to billing and use datasets for access control. BigQuery allows you to batch load source data into a BigQuery table in a single batch operation. For example, the data source could be a CSV file, an external database, or a set of log files. BigQuery Data Transfer Service enables you to run batch transfers on a schedule. Streaming allows you to continually send smaller batches of data in real time, so the data is available for querying as it arrives. You can also use SQL to generate data and store the results in BigQuery. Also, some third party applications and services provide connectors that can ingest data into BigQuery. The table schema provides structure to the data. Remember that every table has a schema which you can enter manually or provide a JSON file with the structure. Those table schemas can also have array data types, which makes them repeated and or struct data types, which makes them nested. This type of denormalization will often give you a performance boost because it avoids intensive joins. You can also setup table partitioning and clustering to reduce the amount of data scanned and speed up your queries.

### Quiz - [Building a Data Warehouse](https://www.cloudskillsboost.google/course_templates/54/quizzes/521420)

#### Quiz 1.

> [!important]
> **True or False: ARRAYS can be a standalone field type or part of a STRUCTS field in BigQuery?**
>
> - [ ] False
> - [ ] True

#### Quiz 2.

> [!important]
> **Which of the following statements on BigQuery is incorrect?**
>
> - [ ] The number of slots allotted to a query is independent of query complexity
> - [ ] Data is run length-encoded and dictionary-encoded
> - [ ] A BigQuery slot is a combination of CPU, memory, and networking resources
> - [ ] Data on BigQuery is physically stored in a redundant way separate from the compute cluster

## Summary

A summary of the key learning points

### Video - [Course Summary](https://www.cloudskillsboost.google/course_templates/54/video/521421)

- [YouTube: Course Summary](https://www.youtube.com/watch?v=w2Rz_KjhSEw)

>> Let's review some key concepts we covered in this course on data lakes and data warehouses. The primary role of a data engineer is to build data pipelines. The ultimate purpose of a data pipeline is to enable stakeholders in an organization to use data to make faster and better decisions. While the role of a data engineer is not new, being able to build data pipelines entirely in the Cloud is relatively new. We argue that doing data engineering in the cloud is advantageous because you can separate compute from storage, and you don't have to worry about managing infrastructure and even software. This allows you to spend more time on what matters, getting insights from data. We introduced data lakes and data warehouses and discussed the key differences between the two. At a high level, a data lake is a place to store unprocessed data, while a data warehouse is a place to store transformed data that you ultimately want to use for analytics, machine learning and dashboards. Next, we discussed Cloud storage as the data lake solution on Google Cloud in some technical depth. We also presented other Google Cloud solutions for low latency requirements, transactional workloads, and structured data. We introduced BigQuery as the data warehouse solution on Google Cloud. We discussed partitioning and clustering in BigQuery as techniques for improving query performance. Also, we talked about EL, ELT, and ETL, and how these relate to data lakes and warehouses. Finally, we presented some reference architectures on Google Cloud for streaming and batch data pipelines. The hope is that these reference architectures serve as a starting point for your data pipeline. Congratulations on completing modernizing data lakes and data warehouses with Google Cloud. Building batch data pipelines on Google Cloud is the second course of the data engineering on Google Cloud core series. We hope to see you there.

## Course Resources

Links to PDF versions of each module 

### Document - [Modernizing Data Lakes and Data Warehouses with Google Cloud](https://www.cloudskillsboost.google/course_templates/54/documents/521422)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
