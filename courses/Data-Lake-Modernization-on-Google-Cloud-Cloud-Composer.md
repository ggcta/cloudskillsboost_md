---
id: 484
name: 'Data Lake Modernization on Google Cloud: Cloud Composer'
type: Course
url: https://www.cloudskillsboost.google/course_templates/484
date_published: 2023-03-10
topics:

---

# [Data Lake Modernization on Google Cloud: Cloud Composer](https://www.cloudskillsboost.google/course_templates/484)

**Description:**

Welcome to Cloud Composer, where we discuss how to orchestrate data lake workflows with Cloud Composer.

**Objectives:**

* Create code to process data in Cloud Composer
* Outline the foundational concepts that underpin the orchestration of workflows with Cloud Composer and Apache Airflow
* Describe how you can debug with Cloud Composer
* Orchestrate the movement of data from Microsoft SQL to BigQuery

## Cloud Composer

Welcome to Cloud Composer, where we discuss how to orchestrate data lake workflows with Cloud Composer.

### Document - [Course Introduction](https://www.cloudskillsboost.google/course_templates/484/documents/368589)

### Video - [Demo: Cloud Composer Code Walkthrough](https://www.cloudskillsboost.google/course_templates/484/video/368590)

* [YouTube: Demo: Cloud Composer Code Walkthrough](https://www.youtube.com/watch?v=spbeTMrdvdg)

One of the best ways I found to learn Apache airflow is by looking at the actual code and trying out a demo example yourself. So let's dive right into a short demo. In this example, we're going be using Cloud Composer, which is going to be fully managed Apache Airflow on Google Cloud platform. And this is what we're going to be doing when we're given it an input dataset file and just some random CSV, some upstream system could have created it multiple different systems. All that we know is we have some data that's in common and limited format. It's going to pop into our Google Cloud storage bucket, which is a great staging area for our data like content, and then we want to do something with it. In this particular example, it just can be a rather simple trigger that's going to take that CSV file, process it through a Dataflow pipeline, and then drop it into BigQuery. So let's take a look at the different pieces. Well, we have a source data that's an input data set. As you see here, we have a storage a GCS bucket is an excellent place to store as many CSV files as you want just to put them there. And then immediately the cool part is we're using a cloud function that's going to trigger as soon as the event happens that, hey, there's a new object here in our GCS bucket. The cloud function says, Hey, guys, there is a new object here, the GCS bucket who wants to know and then we're actually going to trigger our Cloud Composer, our Apache Airflow Dag that's going to run in workflows. Your air, for instance, is going to say, hey, you have something new in there. I'll take it, and then I'll tell all of my friends to do something with it. The friends in this particular scenario, it's just a very simple couple of lines of Python code that's going to take the CSV, convert it to BigQuery rows and write it out to BigQuery as a sync. So overall, that's what we're trying to do. This code is in a public repository. I'll make sure to share the link with you all as well so you can follow along. But let's go back to the top of the repository and look at two key files. Arguably, I would say the read me as the most important, but the two key code files that we're going to be looking at are the Python file for your Apache airflow. And you going to be familiar with each of the different sections of that as we compose are directed acyclic graph or DAG. And then the second piece, just for this particular example is the Dataflow python code that does the actual processing and MapReduce and all that kind of good stuff there. Okay. So first and foremost, this simple load dag.py. It might look complex if you've never seen airflow code before, but let's walk through each of the pieces and then in subsequent videos we'll do a review of these core concepts. So airflow code to create that directed acyclic graph or nodes in a graph is basically just a set of python steps or operators. So I'm going to scroll down and we're going to see what this actually does. This one is very well documented, which is nice and it's got a nice purpose up here. We need some utility functions if we're going to be recording things like, Hey, this CSV was loaded on this date time like dt we're going to import date time and import logging. So this just allows us to do some really cool things attaching to those event hooks that we're going to bring in. Speaking of hooks, we want to know when things are moving around in GCS and then also do things as well so airflow can reach out and talk to GCS and say, Hey, I know in your staging data like buckets and raw data just came in, I'm going to process it and then if you give me permission, I want to move that CSV from staging to a separate subfolder or even a different bucket within GCS. Basically saying, I've processed this CSV so I could do pretty much anything that you want. That's why it's the beauty of an orchestration engine. And to do that, we import this this hook into GCS also. It's not just GCS we're going to be using to do the actual processing of that. I'm going to be using a Dataflow operator as well too. And of course there are some great BigQuery functions that you can reach out and grab here as well, which we'll be covering in just a second. So the good news is these the library of available functions, not just GCP. If you're doing multi-cloud in including AWS or Azure operators here as well, you can do that as well. You just add another line of Python here. Interesting part about airflow, as you can see right here, is when do you want it to actually start? So you can have two different ways of scheduling airflow every day at a scheduled interval or whatever cron interval that you want or you can actually have it be triggered. And the cool part is really the triggering here. So we're not going to have it run on its own. We're going to have the Cloud Function or some other method of actually kicking this off, which is going to be pretty cool. But here's the first mistake that I made is if you can kick off the airflow, start time based on a date in the past. So we're going to say, all right, well, we're going to have it start as of yesterday. So airflow as soon as it comes online in your cloud composer refinement is going to say, oh, shoot, yesterday, great, let's make myself available or we can kick it off sometime in the future. The mistake that I've made in the past is if you set this to say a year in the past and you have it run daily as your interval, because you basically is going to say like, ok, cool, like I wish I had this up and running last year, but now I'm going to put it in there just so it's available for running Airflow will say, wow, I'm a year behind and running these I'm going to take every day from that 365 past days and run a dag event, you know, every single day as well, too. So don't make the mistake of hard go hard coding a date in the past here that's why we had this yesterday function built in here, too. So that's a great piece of code to copy you. All right. And then we just have some variables that we say were a success. Tag means that. Okay, we're going to tag this with success. If it all goes well, if there's an issue that happens with Dataflow or something else, we're going to flag those nodes in our pipeline with a failure as well. And inside of the airflow, you are going to see everything is color coded. So we are very easy to see what's in it. Success state or a failed state? Great. We're still doing a little bit of meta work here, just setting up all of our different parameters and environments. For here, we want to have the user specify the GCS completion bucket. Where is it that they actually want those raw data files to be ultimately living when they're processed? And you can see if I just duplicate this and then I just go back in the browser, you'll see GCS completion bucket. Let me just search GCS completion, there's a bunch of different airflow variables that the user is going to specify or that could be defaulted by the administrator that allow you to parameter ise your Apache airflow instance, which is amazing. So here we have it, output GCS bucket and then you just define where you're actually going to be providing that same thing with a project id. We don't want to hardcoded any of these because these are going to be different for when you're running these as well. And this is actually where it gets consumed, which is the models.variable.get and then it'll get the actual value of that too. Okay. So this is the part where we're invoking Dataflow. So we have a separate Python file, which is the Dataflow file which says, hey, look in the Dags folder, which is ultimately where all of this airflow code and it's any dependent code if we set it up this way, is going to live. It's a big folder in GCS called Dags, as you're going to see. And in there we're going to be looking for process delimited files or process_delimited.py that actual file that you see here is under Dataflow and is process_delimited.py. So it's looking for this exact file by name and you're going to see that now that we've defined this variable here, we'll go through this file in just a second. Don't worry about the Dataflow file specifically, but we're going to see where this actually gets invoked. This Dataflow a variable gets invoked later on in the code when we're actually ready to do that processing. But we're not there yet. All right. Additional variables that we're going to set when we want to start yesterday. Are there any emails that we want to get and send out these emails? Sure. We can do that through these different email hooks there as well. If you're doing email through Cloud Composer, you might need to set up like a simple mailing service through Google Cloud or SendGrid or something like that there as well. The amount of retries on failure. You can play around with this maybe zero, maybe one, two or three. The project id here is again, those models.varibles.get. But I get that you specify as an admin or change as a user, a power user was those settings that you said at the beginning there too? Excellent. Once you have these default arguments for the DAG, it's time to actually start creating these functions. For the DAG to get some power. We haven't created the architecture of what those nodes look like yet. That's still yet to come. We're already halfway through this file, which is great. Okay, so now we have a function called defined function. Move to completion bucket, take a target bucket and then push a bucket where it's going to be going. As you might expect, this is going to move an object to a location in GCS. So after we're done processing it, after Dataflow is done, processing it, move it from our data lake staging bucket to our data lake process bucket or within a subfolder within the same bucket. How have you want to set it up? That's completely up to your architecture. But you can see we're going to establish the connections out to GCS here. So there is a function called Google Cloud Storage Hook that allows composer to have that orchestration power over GCS. And as part of these different dictionaries, these are the configuration files that Apache airflow provides via a Python dictionary. These are keyword arguments inside of the dag_run. Pull the bucket, pull the name, and then can pull that the date timestamp for when it actually gets done. And then log when you're actually doing this work, when you're moving files from one bucket to another or within the same bucket as well too, it's just a helper function. So you're going to see move to completion bucket called a little bit later in this code. We still have not gotten to where we actually create the directed acyclic graph for the DAG yet. That comes next. So we have helper functions finally. Now we get not that the code wasn't important before, but this is where we actually get to the meat of creating your architecture. So starting on line one of seven as you as you see here with models. DAG give the name of your dag an ID. This is what's going to show up in Apache airflow when you go to click on it, logging, debugging. So you want to give that a descriptive name. Here we're going to say GCS, Google Cloud Storage to BigQuery triggered by our cloud function in this case. And then look at you, just give it a description which is great schedule interval being none as you see with the notes up here indicates that we don't want it to run by itself. It's going to be triggered by a cloud cloud function and then pass in our default or even some not making this code look incredibly verbose. We're just passing that variable with models. DAG pass and all those arguments as a DAG here is the arguments for the actual Dataflow job that's going to run. So take a look. These job arguments as you're going to see when we actually kick off the Dataflow task, they're going to be passed in as the options for that Dataflow job. What is Dataflow Dataflow in this particular case? As you're going to see in the Dataflow file, I need an input. Where am I getting my data from? What's the source? Where do you want this stuff? What's my sink? This is all core data lake concepts. In this particular case, input bucket is parameterized output is in this particular case it's BigQuery. And then the field name is what are the field names to create that BigQuery schema there as well in this particular case, the field names. So let me go back to the read me. The field names are it's a very simple BigQuery public dataset file. So the field names are I think it's pulling from USA names, which is the names from the birth of kids over the last hundred years. And this is the schema that you see here where we actually generate a few of these columns and values on the fly to like when that dataset is loaded, that's particularly useful so you can see how recent your data is. Okay, so these are the arguments that the Dataflow job is going to consume. No Dataflow work has happened yet up to this point. The things that do the actual work in Apache airflow are called operators. Well, actually operators specify how the work gets done and the actual execution does the work. But if there's one core part of this to know, the operators which actually specify tasks and marshal of those tasks. So here is where we get to the really fun part of my opinion. We've got three different operators. Well, two different operators, but we've got three different tasks that we want to do. So we got the main Dataflow task which is here. So Dataflow task is defined as well. You know that Dataflow python operator that we specified before I actually want to invoke. So we didn't specify this before, but this is pre canned or it's available for you automatically because we invoked the library, we imported this. So thanks to all these amazing libraries that are available, we can just call on Dataflow resources. Dataflow is like, All right, I'm here. I'm ready to operate and run some tests. What you want me to do? Well, Dataflow operator. or Dataflow Python operator. The first task I want you to do is we're going to give it this name. We define this. You could call it whatever you want, but this is what's going to show up as the actual node. Now we're actually creating these nodes. We're going to run tasks and it's going to show up in that little bubble. You're going to process delimited files and push them and Dataflows like, Great, where is my actual Dataflow file I'm going to be doing? We're going to say, Don't worry about that. Your Python file is in this Dataflow file variable, which will be some GCS location which will kick back over to not this file within the Dataflow folder Dataflow here to actually reference this. We'll go through this in brief in just a second. Dataflow was like, okay, great. Do you have any arguments for the job and BigQuery schema, that sort of thing? We're like, absolutely, we do. And we just created this little dictionary that you have right here and it's going to process that. Okay, Dataflow's like, Cool, I'm ready to run. And apparently I'm not going to be running at a scheduled interval. So I'm just going to wait for a composer to tell me when to run and composer, it's like, Great, I'm waiting for a GCS cloud function to run. So we're all just waiting for those events to kick off and then what we're going to do in addition to that, and you can just stop there and have that run. But the cool part is we're going to fork this condition by basically saying process delimited and push. Two things can happen. Happy path, sad path. Happy path. Is it right? It's BigQuery, everything's fine. And now you have a CSV file that's moved from your staging area of your data lake to your processed area, raw data like and we got some the CSV is now ingested into BigQuery as a sync, which is great. If that didn't happen, then there's a side path. So let's talk about this happy sad path. Success move task. This is the name of the other, you know, time where you see these task ideas. This is what's going to show up in the bubble of that dag that you're going to see success, move to completion. Great. This is triggered on these predefined trigger rule. Everything up until this point is is a success. So that's the only time this is going to happen, which is great success. Then I want to call, assuming everything's been done completely check this out. Call the python function that we created earlier called move to completion bucket. So if you fail to process some of that data that's in staging, I don't want you automatically to continue and just push that staging CSV raw data file into completion because you failed, right? So don't do that until it's a success. Great. So the Python failure on failure, everything that's failed, it looks like we're still calling this Python cobble move the completion bucket to but oh, here we go. A failure tag. We actually are we tag it with failure too, so you can go back in and see which files have actually failed, which is cool. Now you might ask, how do you actually create the branching aspect in that directed acyclic graph? It's super easy. All the way at the bottom. You can see these these little python operators right here and it basically allows you to find to define the structure so you're going to say first and that's why we call this directed first, I want you to do this first, run the Dataflow task and then run success, move task and then look second oh not really second because we're still on first Dataflow task. Also this is how you do branching by the way, also failure, moving, task and that's how you get one node splitting into two. If you had a really, really complex, crazy, beautiful artwork, Apache airflow, you can imagine all the different dependencies that you have down here. Just make sure I add a ton of comments in your code so it makes sense for the person downstream from you, no pun intended that this could be maintaining that code. All right. That is a walk through of a lot of key concepts. Let's review what you've learned so far and then we'll jump into actually executing this code. All right. So, you know that the Apache airflow takes a Python file to create that dag. The dag itself, the logic is a little bit further down, but to power it oftentimes you'll be relying on those external libraries. In this particular case is a Google Cloud storage Dataflow, generic python and triggering rules for success and failure and some BigQuery stuff later on. Scheduling can be done inside of the DAG file itself. We're going to tell this to be live as of yesterday and we've got some tags here and then you can just specify all of the set up that you want that your dag to know when it runs. In this particular case, it's we're dealing heavily with GCS to create a bunch of variables to say what's our GCS bucket, what's our Dataflow file? What, what are the arguments that we're passing into the Apache airflow jobs? What are the arguments that we're passing into the Dataflow job after the airflow job has been kicked off? That's all of this. And then we create our own helper function here in Python just to move things around in GCS and we invoke that at our leisure later on. And the biggest part of the code that you really, really need to get familiar with is the actual creation of the DAG, giving it all these IDs, the idea of the DAG ID as a whole, and the idea of the individual tasks which are invoked by these operators. There's many, many, many, many different operators. If you just did a Google search of airflow operators, you'll be taken to the amazing documentation and you'll just see on the left hand side there's so many different operators across cloud providers. If you had data coming from all of their that you could just do it in invoke on thanks to us this is check out all of the different things that you can do. Hive to MySQL look at this like all of this stuff it's awesome. S3 check operator is super cool if you want it. Bonus points. Take a look at the ways that you could actually before you moved it around. You can assert that certain conditions are true before kicking things off the check operator is really, really cool to do that too, but a little bit beyond the scope of what we want to do here. The last thing I promised if we go through is just very quickly the dime tour of Dataflow inside of this DAG for Dataflow. So inside of Dataflow this is the file. It's actually kicked off. Where is it kicked off? Dataflow file. Where is that file located. Well it's a parameterized operating system path.join of where this dags folder is. Somewhere within there. There better be a top level folder named Dataflow and there better be a file name itself named, processed_delimited. And that's exactly what we have here. Dataflow process, delimited data. And here is just if you work with Dataflow before, it's just essentially a great data processing at scale tool that is built on the invokes. It's a managed, completely managed service that where you run pipelines in Apache beam beam being batch or streaming. And then here we just specify that we're going to be using a RowTransformer, which basically is going to say, well, the data that's coming into Dataflow is not in a form that BigQuery can accept yet we're going to transform or parse it into something that BigQuery is going to like and going to tell BigQuery. This is what you can expect as far as a schema. So way down the way down here, you see just a bunch of Python splitting that we're doing. We're splitting it based on the CSV, all good Python stuff that you can see here. And then we're also going to add additional fields that aren't present in the BigQuery public dataset as cool metadata that we want to include. Like what was the file name of the CSV where this data came from? When did we load it? Super cool. And this is when we actually run this. What do you actually want to do? Well, as part of the run, we're going to add all of our different arguments like where are you going to stuff it inside of BigQuery, what's the delimiter? the delimiter Or in our particular case is a common delimiter. What are the fields that we're pushing this into the schema? And then what is the date time? When are we actually loading this thing? Now the actual beam pipeline code is but you know, two lines. There's a lot of comments here, but it's really just take a dictionary of records that's coming in the door from the CSV file. It's been ingested into Dataflow, mapped them to rows, parse them into rows, and then once we have them in rows, BigQuery is like, cool, I can totally take rows and then just run write them to to BigQuery. Here you can specify how you want it to write. It's BigQuery in this particular case you can say, well, if there is anything in there before I could do right truncate or write append to basically say like add on or append to the rows or remove everything that was in there before and then truncate that. You can completely edit this up to you as well too, and you can set your level of logging that you want to see inside of Dataflow. Cool. I know it looks like a lot of code, but honestly once you understand it is super high level all of the pieces that are involved, which is why I was I would argue the most important file in here is the read me of going through here. You can see how putting the pieces together and using Cloud Composer as an orchestration engine for data lakes is an incredibley valuable tool. The hard part is the fact that it is so flexible that you have to have a use case or an example in mind before you can start moving around architecture pieces so highly. Encourage you. Try this demo yourself. We'll provide a link to the code, modify it, change it, try to pull in from S3, try to pull and move from different GCS buckets. See what works for you all right, good luck.

### Video - [Core Concepts](https://www.cloudskillsboost.google/course_templates/484/video/368591)

* [YouTube: Core Concepts](https://www.youtube.com/watch?v=dIMBwzuHUF0)

Now that you've seen a little bit of an overview of cloud composer and Apache airflow. Let's review those core foundational concepts that you got to know before we start building those code. All right. The first thing is something that's really hard to say five times fast. It's the DAG or the Directed Acyclic Graph. This was the biggest barrier to entry before I started building these things. But once you know what it is, you're like, Oh, okay. So a DAG is essentially the graph means it's going to have nodes or like circles and lines which are called edges. It has nodes and edges we call a graph, which is something you're going to have to get used to. Dataflow has graphs, Apache airflow has graphs, Tenserflow has graphs. It's called "Directed" and "Acyclic" because it can't loop back on itself. That means it's "Acyclic" or can't be a circle. And it's "Directed" meaning this step happen, it's ordered, this step happens first and then this step. So pull data out of GCS. Step one, process it in dataflow. Step two, dump it into BigQuery. Step three. Now Naturally, you can have like two step twos and you can do some really crazy things. So it's not just perfectly one, two, three, four, five, but we'll cover that later. All right. So the DAG's what do they actually look like? Well, as you can see here, everything is just going to be in a Python script file. Everything. Your entire DAG code. Now, it might invoke other things that live in other code files. But when the DAG starts, when and what resources you're calling, what's happening? What's the order in which those circles and those nodes are happening inside of your graph? All that's defined inside of a Python script file. So what actually is in there? Well, the DAG itself is just an amorphous concept until it's actually executed. So when it's run, it's called an individual DAG run. So if I have a pipeline called GCS to BigQuery, that's great. Until I actually press play or execute or run this thing. They're not going to have multiple instances. If you imagine you're building a pipeline, right? Maybe you have seven days in the week and you've got running it every single day. You'd have seven DAG runs for a given DAG I.D. So you created a DAG I.D. The DAG run has an actual execution date associated with it? Now, it's a logical date. And it just essentially allows the all of the different things that the DAG is doing to be associated, especially the logs, in case anything goes wrong with a particular run. Now, the good thing about a particular run is you can click on it. You'll get a visual representation in the Apache Airflow UI of all of the steps that ran or didn't ran, haven't run. And you can look for any errors and debug that way as well too. It's really cool to watch the code actually execute in Apache Airflow has these really nice colored indicators that flip from hasn't started in process completed successfully completed failure as well too. So inspecting DAG runs is where you're going to spend most of your time after you've already written your DAG. If you haven't written your DAG yet. Well, that's where we're going to talk to you about tasks and operators. Tasks are the actual units of work within a given DAG. They represent a task. ID like give me data out of GCS. All hyphenated represents a node or that little circle inside of that graph. This is what actually gets the work done. So a task could be like a bash operator or a print this date and print a date as you see in the example code here. The great thing about Composer and honestly this is the most critical thing to know is that Composer comes in Apache Airflow ships with a bunch of predefined or pre can't operators that allow you to do a variety of amazing different tasks that are out there. If you just Google Apache Airflow operators, you'll see you can pull data from AWS S3 or Azure or run arbitrary SQL commands so things down the line won't execute unless they pass some sort of a cert statement or something like that that you have. So if the tasks are the individual units of work within DAG, the operators are the holistic item that says what it actually is going to be doing and operators are pretty easy to read. Let's take a look at an example DAG. As you see, we specify a DAG. Let's call it my DAG. We're gonna tell it to start on a start date of 2016 and January 1st. The danger there is if today is 2020, the DAG, if you set it to run, will backfill all of those dates as well. We'll talk more about that in the Best Practices section and then I want to specify the order in which my tasks execute. So task one says dummy operator could be pulled from GCS or pulled from BigQuery. Is task one task two use that same operator, but do this task instead. So the actual set up of the task one, then task two is that last operator. There you see task one. Task two where you can define those dependencies. So task two isn't going to run. It's dependent on task one to execute. Now the cool thing is you can actually specify additional logic. If it executes unsuccessfully, then do this. Like if you're converting data, raw data to processed data and there's an error, you could actually say in case of failure, write all those records or write a log to my log entry file. Send an email to my data engineer and basically flagged them as well too. so you can actually trigger based on certain rules that are preset like pass or fail for a given airflow node. This is what the task lifecycle actually looks like. So it starts with no status and then it's scheduled, meaning it's waiting to have work that's done and then it's queued when a scheduler sent the task to the individual executer to run. And then as you might expect, it's running and then hopefully it'll go into success or failed or if it's been skipped, you can manually skip over it or if it's been rescheduled as well or up for retry, which is the the yellow icon there too, in case something failed. And this is like a second retry. All of these different parameters, including the number of retries on the event of failure are again all in that single DAG script file that Python file that you're going to see a demo of shortly. As I mentioned, the how the actual work that gets done and the really great part about what makes Apache Airflow because it's open source extensible are the Apache Airflow operators. This is your amazing library of things that you can do. So the key thing that you need to learn here is that there is three major types of operators. You've got operators that perform an action basically saying, Hey, do this like dataflow. So, you know, take in some default arguments that run and process this data and actually do it within sight of the airflow task or they form or orchestrate some other service within Google Cloud or not to perform that task. The second step are transfers that can move data from one system to another. So it's like these little migration operators as well too, which is pretty cool. And then sensors, which are a type of operator that'll keep running, it's kind of a wild loop until a certain criterion is met, which is pretty cool. So those three operators and you get familiar with all the different flavors of operators that are out there, and the really cool part is, since it's open source, you're going to inspect the code that makes the operators themselves tick to get a better understanding of how they're using just normal rest APIs for these other services that are out there. As we show this so many different types of operators, extremely common ones that you have are running bash commands, bash operator running arbitrary Python commands. Python Operator sending an email email operator invoking HTTP request an HTTP operator. Lastly, the cool thing is that there are operator relationships with these, as you see with a bit shift operators between how you actually want things to run. So you can see that these operators are traditionally you can say like, hey, I want to say this operator is going to happen upstream from me or downstream. But in the latest release of Airflow, you can use these bit shift operators. That's the the greater than greater than or the less than less than. And what this is, is it allows you to determine the shape or the architecture of that beautiful piece of art that we call it. So if you're a visual person, this is how in code you're going to specify the dependency between all of your operators, and it means what order you want your task to run. I know you might be thinking, how do I set a task to kick off two different things? As you see at the bottom there, you can either set, set the same parent or the upstream operator with two different lines of code like op1, going to op2 and then op1 going to op3. That's how you get that branch or you can use that list right here as you can see.

### Video - [Debugging](https://www.cloudskillsboost.google/course_templates/484/video/368592)

* [YouTube: Debugging](https://www.youtube.com/watch?v=_LHm15g-vmc)

All right. So by now you've probably had some experience working with Composer uploading the DAG's and you realized, Hey, something's gone wrong. Not everything is green, and I'm going on that sad path where some windows are failing or something strange is happening. What can you do? Well, let's talk about how you can debug a Composer first and foremost. What are the things that you can do first? Well, with any of these pipelines, my two best recommendations are 1. your code for your Airflow. DAG is just that Python script file that we talked about that you built that you can keep in version control. So that's the first thing you should do is you should always have your DAG code somewhere on GitHub or Cloud Storage repositories or Bitbucket, you name it. Make sure that code is version control. So in case you always need to revert back to something later, you can do that because once you have version controlled, essentially and we'll walk you through. this a little bit later through CI/CD, you can just redeploy a different version of that Python file. With some changes that you've made. So but before you edit any of the code, the things that you want to do on the actual DAG side for your DAG runs, you can pause or delete the DAG absolutely fine and then you can test it, run it again. Maybe it was just a fluke. Or maybe there are some other services that were down for some reason within your architecture. Try it again. It might work or you may need to actually make those code adjustments, redeploy the DAG and then see either the same nodes, add additional logging. And chat check steps there too again and you. Can upload or deploy tt again. Well after you've been working with the code and again versioned control Is your absolute best friend here. Where do you actually go to look for those errors or see how Airflow is. Executing, logging those tasks? Well, there's a couple of. Different things that you can do. Inside of the Airflow UI the number one best place that you can look is those Airflow logs. I'll show you what that looks like in just a second. In the level of granularity. That you actually get from those when it's executing each of those different tasks. Second, a lot of times these errors since Airflow and Composer are execution engines that orchestrate among other different services, it'll largely I've seen it becomes a permission error so you can look for well it doesn't have the authority to run or execute data flow jobs it might be you need to authorize a service account. Or something like. That within the Cloud Console via IAM, So that's number two. The third step is say the orchestration logs, those error airflow logs all look good. But there's something within the service that you're calling like TensorFlow or Dataflow or BigQuery, Cloud Storage, Cloud Build, what have you that is not really going well and you're done looking at it from the Airflow side, where do you go next? Well, the third piece is looking into the application specific logs. So if you're calling a particular operator like BigQuery or Dataflow or Dataproc, you can look into their. Logs as well. Dataflow, for example, maybe it's the number of workers or other. Default arguments that you're passing into the job. That you might need. To tweak to make it work the next time. Lastly, some of the great things that you could do is cloud monitoring. You can essentially type all of your logs to monitoring and basically have just a single source of. Truth for all those logs in there as well. Particularly because Airflow vis a vis Cloud Composer is running in a Google Kubernetes engine container. You want to look at those container logs, look at the workers the schedulers and then the web. Server logs for anything there as well too. And this is especially, although it's a managed service Cloud Composer running Apache. Airflow, it's sometimes very beneficial to actually log into that Google. Kubernetes engine instance. And just take a look at the container. And all those different nodes that are on there. Lastly, one of the things that's going to get you really a long way away is the actual operator code. So if you're using the BigQuery operator, if you're using the GCS Hooks, if you're connecting to data from. AWS, the S3 operators. Looking into the actual code behind those operators, what actually happens. When you. Call a Python operator, the BigQuery operator, and looking to. See what the actual code does. Is the same way, like if you're executing a dataflow DAG as part. Of your Airflow instance. Understanding what that DAG is actually doing. Same principle for looking at. What the actual operator is doing. And looking at. Best practices. You can find those all detailed inside. Of the Airflow documentation page, which is awesome. Okay, so the Composer cloud monitoring, logging integration, so when you actually launch your Composer instance, you can actually tick a box that says integrate. With our monitoring, which is. Great. It's a centralized tool where you have both Airflow and all of your Google Cloud platform logging in one place and you can filter by product or keyword or name or. Tag or something like that too. And it offers you a real time look at all the things that are going on within your. Architecture for your orchestration engine. It's a great way, also above and beyond Airflow to actually manage and monitor the Composer containerized environment. There as well. In case you need to check on anything with permissioning or general orchestration. Of your Kubernetes container that's running behind the scenes. So I highly recommend checking that out. Now onto the actual logs itself. Inside of the logs, you'll notice the object that it gives you. when you expand these the payload. Of what's going on behind the scenes. As you see here, you can look at a couple of different things. The first thing is the actual activity. That's who did what, when, when certain events kicked off. The next is the Airflow scheduler and that's the task orchestration or scheduler. And that's where you can see a lot of really cool. Things come out from the Airflow alongside. And then is the Airflow worker. So this is the actual execution of those individual tasks that is, remember those tasks, these are what actually name. Those nodes in your. DAG and you can take a look to see if there's one particular task that's failing more than most or it's holding up the. Entire pipeline. Next, you can actually look at the Airflow web server logs to see what DAG's are actually being loaded. Naturally, you can have more than one DAG inside of. Your Cloud Composer for instance. Last couple Airflow monitoring, your actual Composer-agent where you can actually. Take a look at some of those logs in the Composer-agent side. And then blanket you can actually do Airflow catchall which is going to get all those different logs from airflow as well. But getting of familiarity with the logs in addition to actually practicing your individual DAG code for different. architectures is going to be an amazing piece as you build these pipelines. Inside of monitoring. One of the things that you can do is actually assess the healthiness and the performance overall for your or your environment, your orchestration. What you can do is you actually build these great dashboards if you haven't worked with it before or you can monitor anything that you want. You can monitor execution time, how long it takes to run each of these different tasks. If you can log it, you can build a dashboard. For it, which is great. So it's completely up to you how you want to do this through prebuilt Airflow metrics that you can latch. On to and bring it to a Dashboard. But ultimately it's up to you to put on what you. care about most for that view of that data. So what are some of the things you can actually measure and then plot. On those dashboards? Well, two major areas. You've the actual environment, which is the Composer environment. And some of those metrics that you'll find within there, even if you just start typing these into that Cloud Monitoring interface. 164 00:07:20,873 --> Environmental health, is it up or down? So you can imagine, like if the environment is down, you immediately want to send an email or shoot a chat over to one of your data engineers or your server reliability engineers and then get that back online environment health, you have the health of the database behind the scenes, the number of the tasks in the queue are your workers being absolutely overwhelmed and you maybe need to reposition it or optimize those. And the number of actual Airflow workers, which again, because Airflow is running within a Cloud Composer, which. Is running in a containerized deployment. Maybe some of the pods. Within that instance are down. Second broad resource category of the actual. Workflows that are running. What's the throughput of the actual workflows? How long is it taking to execute any of these individual tasks? How many tasks are running? And if you're just a data guy like me, it's so fun just watching. Once you set up these DAGS running on a schedule or haven't triggered to run, you just wake up and you just look at all of the successful completions, tracking the average time to completion in case, for example, maybe overnight your input dataset size doubled and you might you adjust your. Code behind the scenes to take care of additional data that's being processed. So you can watch for creep or scope creep or duration creep. Very easily through these monitoring dashboards as well. Here's an example. So one of the things that you can see here is for workflow counts, task counts, workflow duration in task duration. You can see over time in the operating quarter, the workflow duration itself is slowly starting to increase. Over time. And the same thing with Tasks as well. Whereas you're doing the same number of workload runs and task runs, but for example, again with scope creep or maybe your input dataset if you're pulling. From a data source somewhere Is changing. So you always want to once you launch something, make sure that you're. Monitoring and maintaining it as well. Here's another great example of when your workers are down. So now one of the things that you can do is you can create these alerting policies where, okay, well, I want to make sure that any time I have three workers, any one of them are down for more than a minute, as you can see here. Where it's Violated when the number of workers. Below a threshold of. Three I immediately want to email somebody in a notification channel there as well. So you can see it's very trivial. To set that up, which is amazing. You just turn it on, apply it to a resource. and metric that you have there. and how you want to be notified, which is great. You might have email, you might do it through a chat application or some other kind of push notification that you could do as well too. But again, this is half of your job is building a beautiful architecture. The other half of it is making sure that everything goes well asswimmingly while you actually Have the running, which is great. One of the things that we'll leave you with is we mentioned that Composer is running within the Google Kubernetes Engine,. Kubernetes being the orchestration for running all these different pods behind. The scenes that are your. Actual environment for Airflow. One of the things that you can do is you can inspect the Airflow container using kubectl and you can SSH into your actual Airflow pods. So you can see that we're just going to Google Cloud containers, get the credentials to actually work into that GKE cluster which. Is hosting our Cloud Composer Environment. And then we can see what are the actual pods that we have that are running airflow with that cube CTO get pods and then you can take a look at the actual when is it actually running? So we'll pull the scheduler and then you can see the actual running there and then you can see that. I think the second cron syntax there. And then lastly you can say, all right, well, I want to actually execute, go into the scheduler and then pull open the namespace and actually get. Into that scheduler right there.

### Lab - [Cloud Composer: Copying BigQuery Tables Across Different Locations](https://www.cloudskillsboost.google/course_templates/484/labs/368593)

In this lab, you create and run an Apache Airflow workflow in Cloud Composer to export tables from a BigQuery dataset located in Cloud Storage buckets in the US to buckets in Europe, and then import those tables to a BigQuery dataset in Europe.

* [ ] [Cloud Composer: Copying BigQuery Tables Across Different Locations](../labs/Cloud-Composer-Copying-BigQuery-Tables-Across-Different-Locations.md)

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/484/documents/368594)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
