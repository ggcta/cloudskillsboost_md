---
id: 99
name: 'Logging and Monitoring in Google Cloud'
datePublished: 2023-12-12
topics: []
type: Course
url: https://www.cloudskillsboost.google/course_templates/99
---

# [Logging and Monitoring in Google Cloud](https://www.cloudskillsboost.google/course_templates/99)

**Description:**

This course teaches participants techniques for monitoring and improving infrastructure and application performance in Google Cloud.

Using a combination of presentations, demos, hands-on labs, and real-world case studies, attendees gain experience with full-stack monitoring, real-time log management and analysis, debugging code in production, tracing application performance bottlenecks, and profiling CPU and memory usage.

**Objectives:**

- Explain the purpose and capabilities of Google Cloud’s operations suite.
- Implement monitoring for multiple cloud projects.
- Create effective monitoring dashboards and alerts.Create alerting policies, uptime checks and alerts.
- Explain how to collect logs using Cloud Logging and export for further analysis

## Introduction

Welcome to Logging, Monitoring and Observability in Google Cloud! We will cover the pre-requisites, audience and the course objectives.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/99/video/432477)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=EBfveWqLL4I)

Welcome to the two-part course on Logging, Monitoring, and Observability in Google Cloud. The core operations tools in Google Cloud break down into two major categories, The operations-focused components and the application performance management tools. The first course is Logging and Monitoring in Google Cloud which is this course and cover the operations focused components including Logging, Monitoring, and Service Monitoring. This course tend to be more for personnel who are primarily interested in infrastructure, and keeping that infrastructure up, running, and error-free. The second course is Observability in Google Cloud which covers the application performance management tools, including Error Reporting, Trace, and Profiler. This course in contrast, tend to be more for developers who are trying to perfect or troubleshoot applications that are running in one of the Google Cloud compute products. But it isn’t fair to think of these tools as belonging purely to either of these two groups. A developer would, of course, sometimes need access to logs or monitoring metrics, just like an operation team member might need to trace latency. This course is designed to equip Cloud Architects, Administrators, SysOps personnel, Cloud Developers, and DevOps personnel with the essential skills and knowledge needed to excel in logging and monitoring your applications and workloads on Google Cloud. The prerequisites for this course are: Basic scripting or coding ability Google Cloud Fundamentals: Core Infrastructure or equivalent experience Proficiency with command-line tools and Linux operating system environments In this course, we will delve into the critical aspects of Cloud Logging and Cloud Monitoring on Google Cloud. Logging and monitoring play a pivotal role in maintaining the health and security of your cloud resources. In this section, you will learn: The purpose and capabilities of Google Cloud operations suite How to Implement monitoring for multiple cloud projects Crete alerting policies, uptime checks and alerts to identify and resolve problems quickly Use Cloud Logging to collect logs and export for further analysis By the end of Part 1, you will have a solid grasp of how to implement and manage cloud logging and monitoring solutions within Google Cloud, enhancing your skills to maintain the reliability and security of your infrastructure. Let's get started!

## Introduction to Google Cloud Operations Suite

In this module, we will take some time to do a high-level overview of the various products which comprise Google Cloud's logging, monitoring, and observability suite.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/99/video/432478)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=IpcWn2-ZOJ0)

Hey everyone, welcome to the first module of this course, introduction to Google Cloud Operations suite. In this module I will provide you with a quick introduction to Google Cloud operations suite. We will cover what it is and why it is important. Google Cloud operations is a suite of products to monitor, diagnose and troubleshoot infrastructure, services, and applications at scale. It incorporates capabilities to let DevOps (development operations), SREs (site reliability engineering), or ITOps (information technology operations) users operate services in a manner similar to how Google SREs operate their own services. It offers integrated capabilities for monitoring, logging, and advanced observability services like Trace and Profiler. Let us look at the objectives of the module in the next slide. The operations suite consists of three broad categories, Logging, Monitoring and Application Performance Management. In this module, we will start with an overview of why we need these tools, and then we get to know both the operations and the Application Performance Management products. Cloud Logging collects and stores all of your application logs, so you can see what's happening under the hood. Cloud Monitoring collects metrics and traces, so you can track the performance of your applications and identify bottlenecks. And APM provides a unified view of your application's performance, so you can quickly identify and fix problems. We will explore what the Cloud Operations architecture consists of to understand how the three pieces Cloud Logging, Cloud Monitoring and APM are connected. So, let's get started!

### Video - [Need for Google Cloud observability](https://www.cloudskillsboost.google/course_templates/99/video/432479)

- [YouTube: Need for Google Cloud observability](https://www.youtube.com/watch?v=HKhHmF11sLg)

We start this section with an overview of why we need these tools, and then we’ll spend a little time understanding the role of monitoring in product reliability. We will explore the significance of the four golden signals in measuring the system’s performance and reliability. We then move on to explore the products in the Google Cloud Operations Suite. If you've ever worked with on-premises environments, you know that you can physically touch the servers. If an application becomes unresponsive, someone can physically determine why that happened. In the cloud though, the servers aren't yours—they're Google’s—and you can’t physically inspect them. So the question becomes, how do you know what's happening with your server, or database, or application? The answer is by using Google’s integrated observability tools. In this slide, we will dive a little deeper to understand the four distinct recurring user needs for observability. The first one is visibility into system health. Users want to understand what is happening with their application and system. They rely on a service that provides a clear mental model for how their application is working on Google Cloud. They need a report on the overall health of systems. The services should help answer questions such as “are my systems functioning?” or “”do my systems have sufficient resources available? ”. Error reporting and alerting: Users want to monitor their service at a glance through healthy/unhealthy status icons or red/green indicators. Customers appreciate any proactive alerting, anomaly detection, or guidance on issues. Ideally, they want to avoid connecting the dots themselves. Efficient troubleshooting: Users don’t want multiple tabs open. They need a system that can proactively correlate relevant signals and make it easy to search across different data sources, like logs and metrics. If possible, the service needs to be opinionated about the potential cause of the issue and recommend a meaningful direction for the customer to start their investigation. It should allow users to immediately act on what they discover. For instance, a metric indicating insufficient quota should be accompanied by a button that increases quota. The last and final one is performance improvement. Users need a service that can perform retrospective analysis. Generally, help them plan intelligently by analyzing trends and understand how changes in the system affect its performance. Let’s begin with monitoring. Monitoring is the foundation of product reliability. It reveals what needs urgent attention and shows trends in application usage patterns, which can yield better capacity planning, and generally help improve an application client's experience, and lessen their pain. In Google's Site Reliability Engineering book, which is available to read at landing.google.com/sre/books, monitoring is defined as collecting, processing, aggregating, and displaying real-time quantitative data about a system, such as query counts and types, error counts and types, processing times, and server lifetimes. An application client normally only sees the public side of a product, and as a result, developers and business stakeholders both tend to think that the most crucial way to make the client happy is by spending the most time and effort on developing that part of the product. However, to be truly reliable, even the very best products still must be deployed into environments with enough capacity to handle the anticipated client load. Great products also need thorough testing, preferably automated testing, and a refined continuous integration/continuous development (CI/CD) release pipeline. Postmortems and root cause analyses are the DevOps team's way of letting the client know why an incident happened and why it is unlikely to happen again. In this context we are discussing a system or software failure, but the term “incident” can also be used to describe a breach of security. Transparency here is key to building trust. Now that we know the role monitoring plays in product reliability, let's move on to understand what we need from products. We need our products to improve continually, and we need monitoring data to ensure that happens. We need dashboards to provide business intelligence so our DevOps personnel have the data they need to do their jobs. We need automated alerts because humans tend to look at things only when there's something important to look at. An even better option is to construct automated systems to handle as many alerts as possible so humans only have to look at the most critical issues. Typically, there's some triggering event: a system outage, data loss, a monitoring failure, or some form of manual intervention. The trigger leads to a response by both automated systems and DevOps personnel. Many times the response starts by examining signal data that comes in through monitoring. The impact of the issue is evaluated and escalated when needed, and an initial response is formulated. Finally, we need monitoring tools that help provide data crucial to debugging application functional and performance issues. We’ll look more closely at Google’s integrated monitoring tools a bit later in this module. Let's now understand the four golden signals. There are “four golden signals” that measure a system’s performance and reliability. They are latency, traffic, saturation, and errors. Latency measures how long it takes a particular part of a system to return a result. Latency is important because it directly affects the user experience. Changes in latency could indicate emerging issues. Its values may be tied to capacity demands. It can be used to measure system improvements. But how is it measured? Sample latency metrics include page load latency, number of requests waiting for a thread, query duration, service response time, transaction duration, time to first response, time to complete data return. The next signal is traffic, which measures how many requests are reaching your system. Traffic is important because it’s an indicator of current system demand. Its historical trends are used for capacity planning. It’s a core measure when calculating infrastructure spend. Sample traffic metrics include number of HTTP requests per second, number of requests for static vs. dynamic content, number of Network I/Os, number of concurrent sessions, number of transactions per second, number of retrievals per second, number of active requests, number of write iops, number of read iops and number active connections. The third signal is saturation. It’s important to note, though, that capacity is often a subjective measure, that depends on the underlying service or application. Saturation is important because it's an ideal indicator of current system demand. In other words, how full the service is. It focuses on the most constrained resources. It’s frequently tied to degrading performance as capacity is reached. Sample capacity metrics include, percentage of memory utilization, percentage of thread pool utilization, percentage of cache utilization, percentage of disk utilization, percentage of CPU utilization, Disk quota, Memory quota, number of available connections and number of users on the system. The fourth signal is errors, which are events that measure system failures or other issues. Errors are often raised when a flaw, failure, or fault in a computer program or system causes it to produce incorrect or unexpected results, or behave in unintended ways. Errors might indicate configuration or capacity issues, service level objective violations. That it's time to emit an alert. Sample error metrics include wrong answers or incorrect content, number of 400/500 HTTP codes, number of failed requests, number of exceptions, number of stack traces, servers that fail liveness checks and number of dropped connections. Now, let's return to the observability concept. Observability starts with signals, which are metric, logging, and trace data captured and integrated into Google products from the hardware layer up. From those products, the signal data flows into the Google Cloud operation's tools where it can be visualized in dashboards and through the Metrics Explorer. Automated and custom logs can be dissected and analyzed in the Logs Explorer. Services can be monitored for compliance with service level objectives (SLOs), and error budgets can be tracked. Health checks can be used to check uptime and latency for external-facing sites and services. And when incidents occur signal data can generate automated alerts to code or, through various information channels, to key personnel. Error Reporting can help operations and developer teams spot, count, and analyze crashes in cloud-based services. The visualization and analysis tools can then help troubleshoot what's happening in Google Cloud. Ultimately, you won't miss that easy server access, because Google provides more precise insights into your Cloud install than you ever had on-premises. Let’s explore the products most applicable for those in operations roles which are Cloud Monitoring, Cloud Logging, Error Reporting, Cloud Trace and Cloud Profiler.

### Video - [Cloud Monitoring](https://www.cloudskillsboost.google/course_templates/99/video/432480)

- [YouTube: Cloud Monitoring](https://www.youtube.com/watch?v=W1QZMgJ9VH8)

We defined general monitoring and its benefits in the previous section. Let us take a look at Cloud Monitoring features and benefits. Cloud Monitoring provides visibility into the performance, uptime, and overall health of cloud-powered applications. It collects metrics, events, and metadata from projects, logs, services, systems, agents, custom code, and various common application components, including Cassandra, Nginx, Apache Web Server, Elasticsearch, and many others. Monitoring ingests that data and generates insights via dashboards, Metrics Explorer charts, and automated alerts. Cloud Monitoring provides many advanced capabilities that helps address the monitoring challenges and these include automatic, free ingestion. On 100+ monitored resources, over 1,500 metrics are immediately available with no cost. Open source standards. Leverage Prometheus and Open Telemetry to collect metrics across compute workloads. Customization for key workloads. Cloud Monitoring offers custom visualization capabilities for GKE through Google Cloud Managed Service for Prometheus and for Google Compute Engine through Ops Agent. In-context visualizations & alerts: View relevant telemetry data alongside your workloads across Google Cloud

### Video - [Cloud Logging](https://www.cloudskillsboost.google/course_templates/99/video/432481)

- [YouTube: Cloud Logging](https://www.youtube.com/watch?v=E6re4onA_l0)

The next service we will explore is Cloud Logging. Google's Cloud Logging allows users to collect, store, search, analyze, monitor, and alert on log entries and events. Automated logging is integrated into Google Cloud products like App Engine, Cloud Run, Compute Engine VMs running the logging agent, and GKE. Cloud Logging also provides massive features that makes managing and exploring tons of logs easier. These include automatic, easy log ingestion: Immediate ingestion from Google Cloud services across your stack. Gain insight quickly. Tools like Error Reporting, Log Explorer, and Log Analytics let you quickly focus from large sets of data. Customize routing & storage. Route your logs to the region or service of your choice for additional compliance or business benefits. Leverage audit and app logs for compliance patterns and issues. Like I mentioned earlier, logging has multiple aspects such as collection, analysis, export and retention. Cloud logging enables you to automatically collect cloud events and configuration changes. You can aggregate and centralize logs at a organizational level, project level and folder level based on your needs. Most log analysis start with Google Cloud’s integrated Logs Explorer. Logging entries can also be exported to several destinations for alternative or further analysis. Export log data as files to Google Cloud Storage, or as messages through Pub/Sub, or into BigQuery tables. Pub/Sub messages can be analyzed in near-real time using custom code or stream processing technologies like Dataflow. BigQuery allows analysts to examine logging data through SQL queries. And archived log files in Cloud Storage can be analyzed with several tools and techniques. Logs-based metrics may be created and integrated into Cloud Monitoring dashboards, alerts, and service SLOs. Default log retention in Cloud Logging depends on the log type. Data access logs are retained by default for 30 days, but this is configurable up to a max of 3650 days. Admin logs are stored by default for 400 days. Alternatively you can also export logs to Google Cloud Storage or BigQuery to extend retention. Let us next look at a few use cases. A developer would love to get started quickly, thus we have out of the box collection of system metrics and logs and integration into popular logging SDKs and library. Cloud Logging also allows developers to do real time analysis, debugging and troubleshooting of your code. For convenient access and visibility, stack traces are automatically mapped to error types. Operators also take massive advantage of our offering. These include collecting telemetry that is not limited to Google Cloud, centralization of all the logs for users, teams and organizations. You are in control of retention periods and location of the logs. You can also understand log volume, cost and set alerts on important application metrics. You can export logs for storage, analysis and also integrate with third-party services. Lastly, security operations, or SecOps are in charge of ensuring that all access is authorized and that bad actors are not navigating your network. With audit logs, network telemetry and log analysis it can be achieved in a streamlined way.

### Video - [Error reporting](https://www.cloudskillsboost.google/course_templates/99/video/432482)

- [YouTube: Error reporting](https://www.youtube.com/watch?v=JK_mMqOxfgM)

The next service we will explore is Error Reporting. Error Reporting counts, analyzes, and aggregates the crashes in your running cloud services. Error reporting enables you to perform a lot of advanced functionalities that ensures your application runs smoothly. These include real time processing. Application errors are processed and displayed in the interface within seconds. Quickly view and understand errors: A dedicated page displays the details of the error: bar chart over time, list of affected versions, request URL and link to the request log. Instant notification: You do not wait have to wait for your users to report problems. Error Reporting is always watching your service and instantly alerts you when a new application error cannot be grouped with existing ones. It also helps to directly jump from a notification to the details of the new error. Crashes in most modern languages are exceptions which are not caught and are handled by the code itself. Its management interface displays the results with sorting and filtering capabilities. A dedicated view shows the error details: time chart, occurrences, affected user count, first- and last-seen dates, and a cleaned exception stack trace. You can also create alerts to receive notifications on new errors.

### Video - [Application Performance Management Tools](https://www.cloudskillsboost.google/course_templates/99/video/432483)

- [YouTube: Application Performance Management Tools](https://www.youtube.com/watch?v=-mnmydALNLU)

The next product we will be exploring is Cloud Profiler. Cloud Trace is a tracing system that collects latency data from your distributed applications and displays it in the Google Cloud console. Trace can capture traces from applications deployed on App Engine, Compute Engine VMs, and Google Kubernetes Engine containers. Performance insights are provided in near-real time, and Trace automatically analyzes all of your application's traces to generate in-depth latency reports to surface performance degradations. Trace continuously gathers and analyzes trace data to automatically identify recent changes to your application's performance. Now you can analyze changes to applications’ latency profiles through Google Cloud Console and on Android devices. Using the latency reports feature, you can: View performance insights in near-real time. Automatically analyze all of your application's traces to generate in-depth latency reports to surface performance degradations. Continuously gather and analyze trace data to automatically identify recent changes to application performance. Poorly performing code increases the latency and cost of applications and web services every day, without anyone knowing or doing anything about it. Cloud Profiler changes this by using statistical techniques and extremely low-impact instrumentation that runs across all production application instances to provide a complete CPU and heap picture of an application without slowing it down. With broad platform support that includes Compute Engine VMs, App Engine, and Kubernetes, it allows developers to analyze applications running anywhere, including Google Cloud, other cloud platforms, or on-premises, with support for Java, Go, Python, and Node.js. Cloud Profiler presents the call hierarchy and resource consumption of the relevant function in an interactive flame graph that helps developers understand which paths consume the most resources and the different ways in which their code is actually called. Overall, the Google Cloud Operations suite helps you explore both the known and unknown issues underlying your workloads. The products are user focussed designed to understand a customer’s journey with SLO monitoring, uptime checks, tracing and more. They are open, flexible and leverage popular open source projects like Prometheus, OpenTelemetry, and Fluentbit. Integrated for ease through automatic ingestion, connect data sets, in-context telemetry across Google Cloud service views. They also provide meaningful analysis and alerting through powerful analysis tools, leverage alerting for both automated and human-led resolutions.

### Video - [Module Summary](https://www.cloudskillsboost.google/course_templates/99/video/432484)

- [YouTube: Module Summary](https://www.youtube.com/watch?v=gZuzsMlOeV8)

In this module, we’ve explored the core operation tools in Google Cloud, including Cloud Logging, Cloud Monitoring, and Error Reporting, and the application performance management tools, including Trace, and Profiler. Now that we have a foundation, let’s move on to cover the various tools in greater detail.

### Quiz - [Quiz - Introduction to Monitoring in Google Cloud](https://www.cloudskillsboost.google/course_templates/99/quizzes/432485)

#### Quiz 1.

> [!important]
> **You want to calculate the uptime of a service and receive alerts if the uptime value falls below a certain threshold. Which tool will help you with this requirement?**
>
> - [ ] Error Reporting
> - [ ] Profiler
> - [ ] Logs Explorer
> - [ ] Cloud Monitoring

#### Quiz 2.

> [!important]
> **You want to examine messages generated by running code. Which tool might be best for doing this?**
>
> - [ ] Trace
> - [ ] Metrics Explorer
> - [ ] Profiler
> - [ ] Logs Explorer

#### Quiz 3.

> [!important]
> **Users have reported that an application occasionally returns garbage data instead of the intended results, but you have been unable to reproduce this problem in your test environment. Which tool might be of best help?**
>
> - [ ] Trace
> - [ ] Logs Explorer
> - [ ] Error Reporting
> - [ ] Profiler

#### Quiz 4.

> [!important]
> **You want a simple way to see the latency of requests for a web application you deployed to Cloud Run. What Google Cloud tool should you use?**
>
> - [ ] Profiler
> - [ ] Metrics Explorer
> - [ ] Trace
> - [ ] Logs Explorer

## Monitoring Critical Systems

Monitoring is all about keeping track of exactly what's happening with the resources we've spun up inside of Google's Cloud. In this module, we'll take a look at options and best practices as they relate to monitoring project architectures. We'll differentiate the core Cloud IAM roles needed to decide who can do what as it relates to monitoring. Just like architecture, this is another crucial early step. We will examine some of the Google created default dashboards, and see how to use them appropriately. We will create charts and use them to build custom dashboards to show resource consumption and application load. And, finally, we will define uptime checks to track liveliness and latency.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/99/video/432486)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=I_6ckDveyn0)

Let's spend a little time talking about how Google Cloud helps you monitor critical systems. Monitoring is all about keeping track of exactly what's happening with the resources that we launched inside of Google Cloud. In this module, let’s take a look at options and best practices as they relate to monitoring project architectures. It’s important to make some early architectural decisions before starting monitoring. We'll differentiate the core IAM roles needed to decide who can do what as it relates to monitoring. Just like architecture, this is another crucial early step. We will examine some of the default dashboards created by Google, and see how to use them appropriately. We will create charts and use them to build custom dashboards to show resource consumption and application load. And, we will define uptime checks to track liveliness and latency. We will also cover the purpose using MQL for monitoring.

### Video - [Monitoring Overview](https://www.cloudskillsboost.google/course_templates/99/video/432487)

- [YouTube: Monitoring Overview](https://www.youtube.com/watch?v=n0JuU8O8JsI)

We first start with an overview of monitoring. Then, we explore this concept in detail as we advance with the rest of the module. The DevOps Resource and Assessment (DORA) research program defines monitoring as: "The process of collecting, analyzing, and using information to track applications and infrastructure in order to guide business decisions. Monitoring is a key capability because it gives you insight into your systems and your work." Like I mentioned earlier in module 1, monitoring is the foundation of product reliability. It reveals what needs urgent attention and shows trends in application usage patterns, which can yield better capacity planning, and generally help improve an application client's experience, and reduce their problems.

### Video - [Cloud Monitoring achitecture patterns](https://www.cloudskillsboost.google/course_templates/99/video/432488)

- [YouTube: Cloud Monitoring achitecture patterns](https://www.youtube.com/watch?v=1jM0o2XVhIM)

Let's now learn more about some common monitoring architecture patterns. A typical Cloud Monitoring architecture includes 3 layers, a data collection layer, a data storage layer, and a data analysis and visualization layer. A data collection layer collects metrics, logs, and traces from cloud-based systems. In Cloud Monitoring, the data collection layer includes Google Cloud services such as GKE (Google Kubernetes Engine), GCE (Google Compute Engine), App Engine etc,. A data storage layer stores the collected data and routes to the configured visualization and analysis layer. In Cloud Monitoring, this layer includes the Cloud Monitoring API that helps triage the metrics collected to be stored for further analysis. A data analysis and visualization layer: This layer analyzes the collected data to identify problems and trends and presents the analyzed data in a way that is easy to understand. In Cloud Monitoring, this layer comprise of various features within Cloud Monitoring such as Dashboards to visualize data, Uptime checks to monitor applications, Alerting policies to configure alerts and notifications to notify of events that need immediate attention. One of the most common uses of Cloud Monitoring is platform monitoring Blackbox monitoring of the platform enables users to get visibility into the performance of their Google Cloud services. With Google Cloud, this is enabled by default and system metrics are automatically collected without any user effort. Google Cloud Monitoring is the recommended solution for Platform monitoring. System metrics from Google Cloud are available at no cost to customers. These metrics provide information about how the service is performing. Over 1500 metrics collected across more than 100 Google Cloud services automatically. For example, Compute Engine reports over 25 unique metrics for each virtual machine (VM) instance. However, if customers, e.g. in traditional enterprise cohorts, are using 3P products for monitoring and want to aggregate their Google Cloud metrics into those partner products, they can use Cloud Monitoring APIs to ingest these metrics. For applications or workloads deployed in GKE, many customers prefer a Prometheus-based solution for monitoring. We fully embrace that monitoring approach and provide customers a new way to leverage Prometheus based monitoring using Google Managed Prometheus (GMP). GMP is a part of Cloud Monitoring and it makes GKE cluster and workload metrics available as Prometheus data. It can ingest monitoring data exposed in Prometheus format, it supports PromQL compatible query language and has natively integrated the Prometheus expression browser, and Prometheus compatible rule evaluation. For application workloads in GKE, we recommend that customers use Google Managed Prometheus. For applications or workloads deployed in Compute Engine, customers should use the Ops Agent to collect in-process metrics and to collect metrics from 3rd party applications that run in your VMs. Ops agent today supports more than 30 plugins for different open source and ISV software along with a collection of richer and more fine grained metrics at the OS level for Windows and Linux (many flavors). The Ops agent is based on OpenTelemetry standards so custom applications developed by customers can leverage OTEL client libraries for instrumenting their code and generate the needed telemetry. The Ops agent can collect these custom metrics and make them available in Cloud Monitoring as well. While this ecosystem of 3P plugins will continue to expand, if users need support for other software products or services, consider using a partner product like Datadog or NewRelic. If you choose to use partner products, they can collect system metrics from the Google Cloud platform by using the native API-based integrations With Google's partner BindPlane by Blue Medora, you can import monitoring and logging data from both on-premises VMs and other cloud providers, such as Amazon Web Services (AWS), Microsoft Azure, Alibaba Cloud, and IBM Cloud into Google Cloud. The following diagram shows how Cloud Monitoring and BindPlane can provide a single pane of glass for a hybrid cloud. This architecture has the following advantages: In addition to monitoring resources like VMs, Blue Medora has built-in deep integration for over 150 popular data sources. There are no additional licensing costs for using BindPlane. BindPlane metrics are imported into Monitoring as custom metrics, which are chargeable. Likewise, BindPlane logs are charged at the same rate as other Cloud Logging logs.

### Video - [Monitoring multiple projects](https://www.cloudskillsboost.google/course_templates/99/video/432489)

- [YouTube: Monitoring multiple projects](https://www.youtube.com/watch?v=h-SxD2HTFDw)

We have explored a few architecture patterns, let us next explore how you can monitor multiple projects from a single project by using metrics scope. When you go to monitoring settings for a project, you can see that the current metrics scope only has a single project in it, the one it is currently viewing. When you create a Google Cloud project, that project hosts a metrics scope and becomes the scoping project for that scope. It stores the alerts, uptime checks, dashboards, and monitoring groups that you configure for the scope. For example, if you create a staging project and then access monitoring, you can see the metrics for the resources in the staging project. This happens for every project you create. Each project creates a metrics scope for itself and hosts monitoring configuration for itself. But what if you want to centralize how that data is stored and how it's accessed? Since it's possible for one metrics scope to monitor multiple projects, and also a project can be monitored from only a single metrics scope, you will have to decide which relationship will work best for your organizational culture, and this particular project. Let us explore option one where every project is monitored locally, in that project. The advantages for this approach include: Clear and obvious separation for each project. If the project contains development-related resources, it's easy to provide access to the dev personnel. Project resources and monitoring resources all in the same place. It is easy to automate, since monitoring becomes a standard part of the initial project setup. Wheres the disadvantages are: If the application is larger than a single project, you will have limited visibility into application performance. Strategy B: Single metrics scope is used for monitoring large units of projects. You can add multiple projects to an existing scope. Now, monitoring data for all projects in that scope will be visible. This will let you create dashboards, showing resources from all the projects in the scope, or alerting policies that apply to resources in multiple projects as long as they're in the metrics scope. Note that the recommended approach for production deployments is to create a dedicated project to host monitoring configuration data and use its metrics scope to set up monitoring for the projects that have actual resources in them. This way, should a project not be necessary anymore and get deleted, the monitoring configuration for all the other projects won't be impacted. The advantages of this approach is that it provides a single pane of glass that provides visibility into the entire group of related resources or projects. It also helps compare non-prod and prod environments easily. The disadvantages of this approach is Anyone with IAM permissions to access Cloud Monitoring will be able to see metrics for all environments. Monitoring in prod is typically divided among different teams. This approach would not preserve that separation. Although the metric data and log entries remain in the individual projects, any user who has been granted the role Monitoring Viewer (roles/monitoring.viewer) will have access to the dashboards and have access to all data by default. This means that a role assigned to one person on one project applies equally to all projects monitored by that metrics scope. Remember, metrics scope only affects and controls Google Cloud resources related to Cloud Monitoring. Other tools covered in this course, such as Cloud Logging, Error Reporting, and the Application Performance Management (APM) tools, are strictly project-based and do not rely upon the configuration of the metrics scope or the monitoring IAM roles.

### Video - [Data model and dashboards](https://www.cloudskillsboost.google/course_templates/99/video/432490)

- [YouTube: Data model and dashboards](https://www.youtube.com/watch?v=NegakPEBJwQ)

In this section we will explore more on the Monitoring Data model and see how we can visualize these metrics as charts in a dashboard. Let us start by understanding the Cloud Monitoring data model. In general terms, monitoring data is recorded in time series. Each individual time series includes four pieces of information relevant to this discussion: The metric field describes the metric itself and records two aspects: The metric-label that represents one combination of label values. The metric type specifies the available labels and describes what is represented by the data points. The resource field: The resource field records the resource-label and specific monitored resource. The resource label represents one combination of label values and the specific monitored resource from which the data was collected. The metricKind and valueType fields tell you how to interpret the values. The value type is the data type for the measurements. Each time series records the value type (type ValueType) for its data points. For measurements consisting of a single value at a time, the value type tells you how the data is stored: BOOL indicates a boolean. INT64 indicates a 64-bit integer DOUBLE indicates a double-precision float STRING indicates a string of characters. For distribution measurements, the value isn't a single value but a group of values. The value type for distribution measurements is DISTRIBUTION in upper case. Each time series includes the metric kind for its data points. The kind of metric data tells you how to interpret the values relative to each other. Cloud Monitoring metrics are one of three kinds. A gauge metric, a delta metric and cumulative metric. A gauge metric, in which the value measures a specific instant in time. For example, metrics measuring CPU utilization are gauge metrics; each point records the CPU utilization at the time of measurement. A delta metric, in which the value measures the change in a time interval. For example, metrics measuring request counts are delta metrics; each value records how many requests were received after the start time, up to and including the end time. A cumulative metric, in which the value constantly increases over time. For example, a metric for “sent bytes” might be cumulative; each value records the total number of bytes sent by a service at that time. The points value is an array of timestamped values. The metric type tells you what the values represent. The sample time series has an array with a single data point; in most time series, the array has many more values. Let us next take a look at an example. As mentioned earlier monitoring data is recorded in time series. The example shown here is a complete instance of a single time series. Most time series include a lot more data points; this one covers a one-minute interval. All time series have the same structure, with the following fields: The metric field indicates that the metric collected is a set of activity logs of the type log_entry_count with a severity level INFO. The resource field which indicates that the resource is a Compute Engine instance with the details on the specific instance and project ID. The metric kind is of the type DELTA and value type integer. Points are actual array of time stamp and values of the metric. Now that we understand what a Cloud Monitoring data model looks like and also seen an example. Let's move on to understand how we can go about monitoring your Google Cloud systems. You first start by identifying the Google Cloud monitoring resource you want to monitor. And then next check the Monitoring > Dashboards for an auto-created dashboard. When you can't find what you need, or need something specific, use the Monitoring and navigate to Metrics Explorer. Dashboards are a way for you to view and analyze metric data that is important to you. They give you graphical representations on the main signal data in such a way as to help you make key decisions about your Google Cloud-based resources. One of the changing aspects of monitoring is Google's commitment to providing more opinionated default information. Google Cloud sees that your project contains Compute Engine VMs, or a GKE Cluster, so Monitoring auto-creates dashboards for you that radiate the information that Google thinks is important for those two resource types. As you add more resources, Google will continue to add more default dashboards. These dashboards form a great monitoring foundation on which you can build. You can also use the Dashboard Builder to visualize application metrics that you are interested in. You can select the chart type and filter the metrics based on your requirements. Frequently, the easiest way to start chart creation is to build an ad-hoc chart with Google's Metrics Explorer. Metrics Explorer lets you build charts for any metric collected by your project. With it, you can: Save charts you create to a custom dashboard. Share charts by their URL. View the configuration for charts as JSON. Most importantly, you can use Metrics Explorer as a tool to explore data that you don't need to display long term on a custom dashboard. As seen on this slide, the Metrics Explorer interface consists of three primary regions: A configuration region, where you pick the metric and its options The chart that displays the selected metric The display panel, where you can configure the axis, set a threshold lines, and more. You define a chart by specifying both what data should display and how the chart should display it. Metric: To populate the chart, you must specify at least one pair of values, the monitored resource type, and the metric type. Filter: You can reduce the amount of data returned for a metric by specifying a filter. Filtering removes data from the chart by excluding time series that don't meet your criteria. The result is fewer lines on the chart and, hopefully, a better signal to noise ratio. When you click in the Filter field, a panel that contains lists of criteria by which you can filter appears. In broad strokes, you can filter by resource group, by name, by resource label, and by the metric label. In this example, we filter by machine type. The zone can then be compared to a direct value, like "=n1-standard-4," or by using the “=” operator, to any valid regular expression. You can check the documentation If you want to see the fully supported filter syntax. Group by: You can reduce the amount of data returned for a metric by combining different time series. To combine multiple time series, you typically specify a grouping and a function. Grouping is done by label values. The function defines how all time-series data within a group are combined into a new time series. Alignment: Alignment creates a new time series in which the raw data has been regularized in time so it can be combined with other aligned time series. Alignment produces time series with regularly spaced data. A time series is a set of data points in temporal order. To align a time series to break the data points into regular buckets of time, the alignment period is used. Multiple time series must be aligned before they can be combined. Alignment is a prerequisite to aggregation across time series, and monitoring does it automatically, by using default values. You can override these defaults by using the alignment options, which are the Alignment function and the Min alignment period. The min alignment period determines the length of time for subdividing the time series. For example, you can break a time series into one-minute chunks or one-hour chunks. The data in each period is summarized so that a single value represents that period. The default alignment period, which is also the minimum, is one minute. Although you can set the alignment interval for your data, time series might be realigned when you change the time interval displayed on a chart or change the zoom level. The alignment function determines how to summarize the data in each alignment period. The functions include the sum, the mean, and so forth. Valid alignment choices depend on the kind and type of metric data a time series stores. Lets next explore legend configuration. Click any of the legend column headers to sort by that field. The legend columns included in the chart's display are configurable. A chart's widget type and its analysis mode setting determine how the chart displays data. For example, when you create a line chart, each time series is shown by a line with a unique color. However, you can configure a line chart to display statistical measures such as the mean and moving average. There are three analysis modes: Standard mode displays each time series with a unique color. Stats mode displays common statistical measures for the data in a chart. X-Ray mode displays each time series with a translucent gray color. Each line is faint, and where lines overlap or cross, the points appear brighter. Therefore, this mode is most useful on charts with many lines. Overlapping lines create bands of brightness, which indicate the normal behavior within a metrics group. Threshold value: The Threshold option creates a horizontal line from a point in the Y-axis. The line provides a visual reference for the chosen threshold value. You can add a threshold that refers to a value on the left Y-axis or the right Y-axis. You use Compare to Past mode on a chart, the legend is modified to include a second “values” column. The current Value column becomes Today, the past values column is named appropriately—for example, Last Week. The Legend Alias field lets you customize a description for the time series on your chart. These descriptions appear on the tooltip for the chart and on the chart legend in the Name column. By default, the descriptions in the legend are created for you from the values of different labels in your time series. Because the system selects the labels, the results might not be helpful to you. To build a tedmplate for descriptions, use this field. You can enter plain text and templates in the Legend Alias field. When you add a template, you add an expression that is evaluated when the legend is displayed. To add a legend template to a chart, do the following: In the Display pane, expand Legend Alias. Click + (Plus) and select an entry from the menu. In this example, you can see the mix of the variable, ${resource.labels.zone} - ${metric.labels.instance_name}. You can see the resulting output in the chart legend.

### Video - [Query metrics](https://www.cloudskillsboost.google/course_templates/99/video/432491)

- [YouTube: Query metrics](https://www.youtube.com/watch?v=QJGwauzXJEE)

Before we wrap our discussion on dashboards, charts, and the Metrics Explorer, let’s examine a more versatile way of interacting with metrics by leveraging the query languages such as Monitoring Query Language, commonly referred to as MQL, and PromQL. Cloud Monitoring supports two query languages: MQL and PromQL. MQL: MQL is an advanced query language. It provides an expressive, text-based interface to Cloud Monitoring time-series data. By using MQL, you can retrieve, filter, and manipulate time-series data. PromQL provides an alternative to the Metrics Explorer menu-driven and Monitoring Query Language (MQL) interfaces for creating charts and dashboards. You can use PromQL to query and chart Cloud Monitoring data from the following sources: Google Cloud services, like Google Kubernetes Engine or Compute Engine, that write metrics described in the lists of Cloud Monitoring system metrics. User-defined metrics, like log-based metrics and Cloud Monitoring user-defined metrics. Google Cloud Managed Service for Prometheus, the fully managed multi-cloud solution for Prometheus from Google Cloud. For information about the managed service, including support from PromQL, see Google Cloud Managed Service for Prometheus documentation. You can also use tools like Grafana to chart metric data ingested into Cloud Monitoring. Available metrics include metrics from Managed Service for Prometheus and Cloud Monitoring metrics documented in the lists of metrics. These are a few examples of when you can use MQL: Create ratio-based charts and alerts Perform time-shift analysis (compare metric data week over week, month over month, year over year, etc.) Apply mathematical, logical, table operations, and other functions to metrics Fetch, join, and aggregate over multiple metrics Select by arbitrary, rather than predefined, percentile values Create new labels to aggregate data by, using arbitrary string manipulations including regular expressions. Whether you need to perform joins, display arbitrary percentages, or even make advanced calculations, the use cases for MQL are unlimited. MQL is built using operations and functions. Operations are linked together by using the common “pipe” idiom, where the output of one operation becomes the input to the next. Linking operations makes it possible to build complex queries incrementally. In the same way you compare and chain commands and data through pipes on the Linux command line, you can fetch metrics and apply operations by using MQL. For a more advanced example, suppose you built a distributed web service that runs on Compute Engine VM instances and uses Cloud Load Balancing, and you want to analyze error rate, which is one of the SRE “golden signals.” You want to see a chart that displays the ratio of requests that return HTTP 500 responses (internal errors) to the total number of requests; that is, the request-failure ratio. The loadbalancing.googleapis.com/https/request_count metric type has a response_code_class label, which captures the class of response codes. This query uses an aggregation expression built on the ratio of two sums: The first sum uses the if function to count 500-valued HTTP responses and a count of 0 for other HTTP response codes. The sum function computes the count of the requests that returned 500. The second sum adds up the counts for all requests, as represented by val(). The two sums are then divided, which results in the ratio of 500 responses to all responses. You can use queries by navigating to Metrics explorer and simply click the CODE button. You can use the radio button to switch between MQL and PromQL.

### Video - [Uptime checks](https://www.cloudskillsboost.google/course_templates/99/video/432492)

- [YouTube: Uptime checks](https://www.youtube.com/watch?v=1vgSvTi11fQ)

Another monitoring component before we wrap this module is uptime checks. Uptime checks can be configured to test the availability of your public services from locations around the world, as you can see on this slide. The type of uptime check can be set to HTTP, HTTPS, or TCP. The resource you can check are App Engine application, a Compute Engine instance, a URL of a host, or an AWS instance or load balancer. For each uptime check, you can create an alerting policy and view the latency of each global location. Uptime checks can help us ensure that our externally facing services are running and that we aren’t burning our error budgets unnecessarily. Here is an example of an HTTP uptime check. The resource is checked every minute with a 10-second timeout. Uptime checks that do not get a response within this timeout period are considered failures. So far, there is a 100% uptime with no outages. Uptime checks are easy to create. In Monitoring, navigate to Uptime Checks and click Create Uptime Check. Give the uptime check a name or title that is descriptive. Select the check type protocol, the resource type, and appropriate information for that resource type. A URL, for example, would need a hostname and an optional page path. A number of optional advanced options are available, including logging failures, narrowing the locations in the world from where test connections are made, the addition of custom headers, check timeout, and authentication. The interface also makes it easy to create an alert for failing uptime checks.

### Lab - [Monitoring and Dashboarding Multiple Projects](https://www.cloudskillsboost.google/course_templates/99/labs/432493)

In this hands-on lab, we link two projects into a single project, create a series of uptime checks, and a dashboard.

- [ ] [Monitoring and Dashboarding Multiple Projects](../labs/Monitoring-and-Dashboarding-Multiple-Projects.md)

### Video - [Module summary](https://www.cloudskillsboost.google/course_templates/99/video/432494)

- [YouTube: Module summary](https://www.youtube.com/watch?v=75VGCoW-PL0)

That's a wrap. In this module, you learned how to: Use Cloud Monitoring to view metrics for multiple cloud projects Explain the different types of dashboards and charts that can be built Create an uptime check Explain the purpose of using MQL for monitoring

### Quiz - [Quiz - Monitoring critical systems](https://www.cloudskillsboost.google/course_templates/99/quizzes/432495)

#### Quiz 1.

> [!important]
> **What is the name of the project that hosts a metrics scope?**
>
> - [ ] Hosting project
> - [ ] Scoping project
> - [ ] Evaluating project
> - [ ] Monitored project

#### Quiz 2.

> [!important]
> **You want to analyze the error rate in your load balancing environment. Which interface helps you view a chart with a ratio of 500 responses to all responses**
>
> - [ ] Metrics Explorer
> - [ ] Liveness probe
> - [ ] Uptime check
> - [ ] MQL

#### Quiz 3.

> [!important]
> **You want to be notified if your application is down. What Google Cloud tool makes this easy?**
>
> - [ ] Readiness probe
> - [ ] Health check
> - [ ] Uptime check
> - [ ] Liveness probe

## Alerting Policies

Alerting gives timely awareness to problems in your cloud applications so you can resolve the problems quickly. In this module, you will learn how to develop alerting strategies, define alerting policies, add notification channels, identify types of alerts and common uses for each, construct and alert on resource groups, and manage alerting policies programmatically.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/99/video/432496)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=WpqC7wfb2cQ)

In the previous module, we explored how to monitor resources using Cloud Monitoring. Let's us next learn how you can receive timely awareness to problems in your cloud applications so you can resolve the problems quickly. Upon completion of this module, you will be able to explain why SLI, SLO and SLA are important, define alerting policies and discuss alerting strategies. You will also be able to explain error budget, identify types of alerts and common uses for each and also use Cloud Monitoring to manage services.

### Video - [SLI, SLO, and SLA](https://www.cloudskillsboost.google/course_templates/99/video/432497)

- [YouTube: SLI, SLO, and SLA](https://www.youtube.com/watch?v=9ZiBQDOFdWo)

The three terms that are frequently used in this course are SLI, SLO and SLA. Before we cover alerting strategy, it is important to understand what SLI, SLO and SLA are. Service level indicators, or SLIs, are carefully selected monitoring metrics that measure one aspect of a service's reliability. Ideally, SLIs should have a close linear relationship with your users' experience of that reliability, and we recommend expressing them as the ratio of two numbers: the number of good events divided by the count of all valid events. A Service level objective, or SLO, combines a service level indicator with a target reliability. If you express your SLIs as is commonly recommended, your SLOs will generally be somewhere just short of 100%, for example, 99.9%, or "three nines." You can't measure everything, so when possible, you should choose SLOs that are S.M.A.R.T. SLOs should be specific. "Hey everyone, is the site fast enough for you?" is not specific; it's subjective. "The 95th percentile of results are returned in under 100ms." That's specific. They need to be based on indicators that are measurable. A lot of monitoring is numbers, grouped over time, with math applied. An SLI must be a number or a delta, something we can measure and place in a mathematical equation. SLO goals should be achievable. "100% Availability" might sound good, but it's not possible to obtain, let alone maintain, over an extended window of time. SLOs should be relevant. Does it matter to the user? Will it help achieve application-related goals? If not, then it’s a poor metric. And SLOs should be time-bound. You want a service to be 99% available? That’s fine. Is that per year? Per month? Per day? Does the calculation look at specific windows of set time, from Sunday to Sunday for example, or is it a rolling period of the last seven days? If we don't know the answers to those types of questions, it can’t be measured accurately. And then there are Service Level Agreements, or SLAs, which are commitments made to your customers that your systems and applications will have only a certain amount of “down time.” An SLA describes the minimum levels of service that you promise to provide to your customers and what happens when you break that promise. If your service has paying customers, an SLA may include some way of compensating them with refunds or credits when that service has an outage that is longer than this agreement allows. To give you the opportunity to detect problems and take remedial action before your reputation is damaged, your alerting thresholds are often substantially higher than the minimum levels of service documented in your SLA. For SLOs, SLIs and SLAs to help improve service reliability, all parts of the business must agree that they are an accurate measure of user experience and must also agree to use them as a primary driver for decision making. Being out of SLO must have concrete, well-documented consequences, just as there are consequences for breaching SLAs. For example, slowing down the rate of change and directing more engineering effort towards eliminating risks and improving reliability are actions that could be taken to get your product back to meeting its SLOs faster. Operations teams need strong executive support to enforce these consequences and effect change in your development practice. Here is an example of a SLA, which is to maintain an error rate of less than 0.3% for the billing system. Here error rate is a quantifiable measure which is the SLI and 0.3 is the specific target set which is the SLO in this case. If your service has paying customers, you probably have some way of compensating them with refunds or credits when that service has an outage. Your criteria for compensation are usually written into a service level agreement, which describes the minimum levels of service that you promise to provide and what happens when you break that promise. The problem with SLAs is that you're only incentivized to promise the minimum level of service and compensation that will stop your customers from replacing you with a competitor. When reliability falls far short of the levels of service that keep your customers happy, this contributes to a perception that your service is unreliable, customers often feel the impact of reliability problems before these promises are breached, Compensating your customers all the time can get expensive, so what targets do you hold yourself to internally? When does your monitoring system trigger an operational response? To give you the breathing room to detect problems and take remedial action before your reputation is damaged, your alerting thresholds are often substantially higher than the minimum levels of service documented in your SLA.

### Video - [Developing an alterting strategy](https://www.cloudskillsboost.google/course_templates/99/video/432498)

- [YouTube: Developing an alterting strategy](https://www.youtube.com/watch?v=2Eh1qx8bY9M)

Now that we have a solid understanding of SLA, SLI and SLO, let us look at how to develop an alerting strategy next. An alert is an automated notification sent by Google Cloud through some notification channel to an external application, ticketing system, or person. If you are wondering why is the alert being sent? The answer perhaps is that a service is down, or an SLO isn't being met. Regardless, an alert is generated when something needs to change. The events are processed through a time series: a series of event data points broken into successive, equally spaced windows of time. Based on need, the duration of each window and the math applied to the member data points inside each window are both configurable. Because of the time series, events can be summarized, error rates can be calculated, and alerts can be triggered where appropriate. A great time to generate alerts is when a system is heading to spend all of its error budget before the allocated time window. An error budget is perfection minus SLO. SLIs are things that are measured, and SLOs represent achievable targets. If the SLO target is "90% of requests must return in 200 ms," then the error budget is 100% - 90% = 10%. Four major attributes are considered when attempting to measure the accuracy or effectiveness of a particular alerting strategy. The first one is Precision. Precision is the proportion of alerts detected that were relevant to the sum of relevant and irrelevant alerts. It’s decreased by false alerts. Recall is the proportion of alerts detected that were relevant to the sum of relevant alerts and missed alerts. It’s decreased by missing alerts. Precision can be seen as a measure of exactness, whereas recall is a measure of completeness. Detection time can be defined as how long it takes the system to notice an alert condition. Long detection times can negatively affect the error budget, but alerting too fast can generate false positives. Reset time measures how long alerts fire after an issue has been resolved. Continued alerts on repaired systems can lead to confusion. Error budgeting 101 would state that when the error count, or whatever is being measured, is trending to be greater than the allowed error budget, an alert should be generated. Both the SLO itself, and the idea of “trending toward” require windows of time over which they are calculated. In this subject space, the window term is used in two main ways: The SLO itself will be measured over a window of time, say an availability SLO of 99.9% over every 30-day period. Alert triggering will have a window over which it watches for trends. For example, an alert might fire if the percentage of errors over any 60-minute period exceeds 0.1%. One of the alerting decisions you and your team have to make is window length. The window is a regular-length subdivision of the SLO total time. Imagine you set a Google Cloud spend budget of $1,000 a month. When do you want to receive an alert? When the $1,000 is spent? Or when the predicted spend is trending past the $1,000? Of course, the latter. Now, the same concept, but this time imagine a 99.9% availability SLO over 30 days. You don't want to get an alert when your error budget is already gone. By then it's too late to do anything about the problem. One option is small windows. Smaller windows tend to yield faster alert detections and shorter reset times, but they also tend to decrease precision because of their tendency toward false positives. In our 99.9% availability SLO for over 30 days, a 10-minute window would alert in 0.6 seconds if a full outage occurs and would consume only 0.02% of the error budget. Longer windows tend to yield better precision, because they have longer to confirm that an error is really occurring. But reset and detection times are also longer. That means you spend more error budget before the alert triggers. In our same 99.9% availability for SLO over 30 days, a 36-hour window would alert in 2 minutes and 10 seconds if a full outage occurs, but would consume a full 5% of the error budget. One trick might be to use short windows, but add a successive failure count. One window failing won’t trigger the alert, but when three fail in a row the error is triggered. This way, the error is spotted quickly but treated as an anomaly until the duration or error count is reached. This is what you do when your car starts making a sound. You don't immediately freak out, but you pay attention and try to determine whether it's a real issue or a fluke. The downside is that precision typically has an inverse relationship to recall. As the precision goes up, as you avoid false positives, you let the problem continue to happen. If the "pay attention but don't alert yet" duration is 10 minutes, a 100% outage for 5 minutes is not detected. As a result, if errors spike up and down, they might never be detected. So how do we get good precision and recall? This is achieved with multiple conditions. Many variables affect a good alerting strategy, including the amount of traffic, the error budget, peak and slow periods, to name a few. The fallacy is believing that you have to choose a single option. Define multiple conditions in an alerting policy to get better precision, recall, detection time, and rest time. You can also define multiple alerts through multiple channels. Perhaps a short window condition generates an alert, but it takes the form of a Pub/Sub message to a Cloud Run container, which then uses complex logic to check multiple other conditions before deciding whether a human gets a notification. See the SRE Workbook for more information. And alerts should always be prioritized based on customer impact and SLA. Don't involve humans unless the alert meets some threshold for criticality. You can use severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. You can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (for example, Webhook, Cloud Pub/Sub, PagerDuty). The notification channels were enhanced to accept this data—including Email, Webhooks, Cloud Pub/Sub, and PagerDuty. This enables further automation and customization based on importance wherever the notifications are consumed. High-priority alerts might go to Slack, SMS, and/or maybe even a third-party solution like PagerDuty. You can even use multiple channels together for redundancy. Low-priority alerts might be logged, sent through email, or inserted into a support ticket management system.

### Video - [Creating alerts](https://www.cloudskillsboost.google/course_templates/99/video/432499)

- [YouTube: Creating alerts](https://www.youtube.com/watch?v=dvHuUQe9Slo)

We've discussed some of the alerting concepts and strategies. Let's look at how to create alerts in Google Cloud. Google Cloud defines alerts by using alerting policies. An alerting policy has: A name One or more alert conditions Notifications Documentation For the name, use something descriptive so you can recognize alerts after the fact. Organizational naming conventions can be a great help. https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/channels/create Alert policies can also be created from the gcloud CLI, the API and Terraform. It starts with an alert policy definition in either a JSON or YAML format. One neat trick when learning the correct file format is to create an alert using the Google Cloud console. Then use the gcloud monitoring policies list and the describe commands to see the corresponding definition file. The alerting API and gcloud CLI can create, retrieve, and delete alerting policies. The alerting policies are of two types, metric based alerting and log based alerting. Metric based alerting: Policies used to track metric data collected by Cloud Monitoring are called metric-based alerting policies. You can add a metric-based alerting policy to your Google Cloud project by using the Google Cloud console. A classic example of a metric-based alerting policy is to notify when the application is running on a VM that has high latency for a significant time period. Log based alerting: Log-based alerting is used to notify anytime a specific message occurs in a log. You can add a log-based alerting policy to your Google Cloud project by using the Logs Explorer in Cloud Logging or by using the Cloud Monitoring API. An example of log-based alerting policy is to notify when a human user accesses the security key of a service account. The alert condition is where you spend the most alerting policy time and make the most decisions. The alert condition is where you decide what's being monitored and under what condition an alert should be generated. Notice how the web interface combines the heart of the Metrics Explorer with a configuration condition. You start with a target resource and metric you want the alert to monitor. You can filter, group by, and aggregate to the exact measure you require. Then the yes-no decision logic for triggering the alert notification is configured. It includes the trigger condition, threshold, and duration. There are three types of conditions for metric-based alerts: Metric-threshold conditions trigger when the values of a metric are more than, or less than, a threshold for a specific duration window. Metric-absence conditions trigger when there is an absence of measurements for a duration window. Forecast conditions predict the future behavior of the measurements by using previous data. These conditions trigger when there is a prediction that a time series will violate the threshold within a forecast window. An alert might have zero to many notification options selected, and they each can be of a different type. There are direct-to-human notification channels (Email, SMS, Slack, Mobile Push), and for third-party integration use Webhook and Pub/Sub. Manage your notifications and incidents by adding user-defined labels to an alerting policy. Because user-defined labels are included in notifications, if you add labels that indicate the severity of an incident, then the notification contains information that can help you prioritize your alerts for investigation. If you send notifications to a third-party service like PagerDuty, Webhooks, or Pub/Sub then you can parse the JSON payload and route the notification according to its severity so that so that your team doesn't miss critical information. A notification channel decides how the alert is sent to the recipient. Alerts can be routed to any third-party service. Email alerts are easy and informative, but they can become notification spam if you aren't careful. SMS is a great option for fast notifications, but choose the recipient carefully. Slack is very popular in support circles. The Google Cloud app for mobile devices is a valid option. PagerDuty is a third-party on-call management and incident response service. Webhooks and Pub/Sub are excellent options when you want to alert users to external systems or code. The documentation option is designed to give the alert recipient additional information they might find helpful. Use the documentation section to guide your troubleshooting. Include internal playbooks, landing links and dynamic labels The default alert contains information about which alert is failing and why, so think of this more like an easy button. If there's a standard solution to this particular alert, adding a reference to it here might be a good example of proper documentation inclusion. Then again, if it was that easy, automate it! Here, you see an alert notification sent out through an email. Notice how many details about exactly what went wrong are automatically included in the email body. The bottom documentation section can also be used to augment the provided information. When one or more alert policies are created, the alerting web interface provides a summary of incidents and alerting events. An event occurs when the conditions for an alerting policy are met. When an event occurs, Cloud Monitoring opens an incident. In the Alerting window, the Summary pane lists the number of incidents, and the Incidents pane displays the ten most recent incidents. Each incident is in one of three states: Incident firing, acknowledged incidents, alert policies. Incidents firing: If an incident is open, the alerting policy's set of conditions is being met. Or there’s no data to indicate that the condition is no longer met. Open incidents usually indicate a new or unhandled alert. Acknowledged incidents: A technician spots a new open alert. Before one starts to investigate, they mark it as acknowledged as a signal to others that someone is dealing with the issue. Alert policies displays the number of alerting policies created. As mentioned earlier, the Incidents pane displays the most recent open incidents. To list the most recent incidents in the table, including those that are closed, click Show closed incidents. Snooze displays the recently configured snoozes. When you want to temporarily prevent alerts from being created and notifications from being sent, or to prevent repeated notifications from being sent for an open incident, you create a snooze. For example, you might create a snooze when you have an escalating outage and you want to reduce the number of new notifications. Groups provide a mechanism for alerting on the behavior of a set of resources instead of individual resources. For example, you can create an alerting policy that is triggered if some resources in the group violate a condition (for example, CPU load), instead of having each resource inform you of violations individually. Groups can contain subgroups and can be up to six levels deep. One application for groups and subgroups is the management of physical or logical topologies. For example, with groups, you can separate your monitoring of production resources from your monitoring of test or development resources. You can also create subgroups to monitor your production resources by zone. Resources can belong to multiple groups as well. You define the one-to-many membership criteria for your groups. A resource belongs to a group if the resource meets the membership criteria of the group. Membership criteria can be based on resource name or type, Cloud Projects, network tag, resource label, security group, region, or App Engine app or service. Logs-based metrics are extracted from Cloud Monitoring and are based on the content of log entries. For example, the metrics can record the number of log entries that contain particular messages, or they can extract latency information reported in log entries. You can use logs-based metrics in Cloud Monitoring charts and alerting policies. As we covered earlier in this module, an alerting policy describes a set of conditions that you want to monitor. When you create an alerting policy, you must also specify its conditions: what is monitored and when to trigger an alert. The logs-based metrics serve as the basis for an alerting condition. To create alerting policies by using Terraform, start by creating a description of the conditions under which some aspect of your system is considered to be "unhealthy" and the ways to notify people or services about this state. Shown on slide is a basic monitoring alert policy with the name alert_policy. The code includes three required arguments: display name, combiner and conditions. Display_name: This argument helps identify the policy with a name that can be seen on the dashboard. Combiner: This argument defines how the results of multiple conditions have to combined. Conditions: This argument defines a list of conditions that are combined based on the combiner. A policy can have up to six conditions. For some more policy examples, visit the links listed on this slide.

### Lab - [Alerting in Google Cloud](https://www.cloudskillsboost.google/course_templates/99/labs/432500)

Deploy an application to Google Cloud and then create alerting policies for it

- [ ] [Alerting in Google Cloud](../labs/Alerting-in-Google-Cloud.md)

### Video - [Service Monitoring](https://www.cloudskillsboost.google/course_templates/99/video/432501)

- [YouTube: Service Monitoring](https://www.youtube.com/watch?v=a4e3fv40saE)

Now that we’ve examined alerts, their use, and their creation, let’s see how the Service Monitoring console and API can help. Modern applications are composed of multiple services connected together, and when something fails, it often seems like many things fail at the same time. To help manage this complexity, SLO monitoring helps with SLO and alert creation. With Service Monitoring, you get the answers to the following questions: What are your services? What functionality do those services expose to internal and external customers? What are your promises and commitments regarding the availability and performance of those services, and are your services meeting them? For microservices-based apps, what are the inter-service dependencies? How can you use that knowledge to double check new code rollouts and triage problems if a service degradation occurs? Can you look at all the monitoring signals for a service holistically to reduce mean time to repair (MTTR)? Cloud Monitoring can identify potential or candidate services for the following types: GKE namespaces GKE services GKE workloads Cloud Run services. The Service Monitoring consolidated services overview page is your point of entry. Services pane provides a summary of the health of your various services. Here, you can see the service name, type, SLO status, and whether any SLO-related alerts are firing. To monitor or view details for a specific service, click the service name. You can also filter by entering a value in the Filter text box to apply additional conditions. Service Monitoring can approach SLO compliance calculations in two fundamental ways. Request-based SLOs use a ratio of good requests to total requests. For example, we want a request-based SLO with a latency below 100 ms for at least 95% of requests. So It is convenient for us if 98% of requests were faster than 100 ms. Window-based SLOs use a ratio of the number of good versus bad measurement intervals, or windows. So each window represents a data point, instead of all the data points that comprise the window. For example, take a 95th percentile latency SLO that needs to be less than 100 ms for at least 99% of 10-minute windows. Here, a compliant window is a 10-minute period over which 95% of the requests were less than 100 ms. So It is convenient for us if 99% of 10-minute windows were compliant. Let's look at another pair of window-based versus request-based SLO examples. Imagine you get 1,000,000 requests a month, and your compliance period is rolling for 30 days. If you are looking for a 99.9% request-based SLO, that translate to 1,000 total bad requests every 30 days. However, a 99.9% windows-based SLO which is averaged across 1-minute windows, allows a total of 43 bad windows, or 43,200 total windows * 99.9% = 43,157 good windows. Windows-based SLOs can be tricky because they can hide burst-related failures. If the system returns nothing but errors, but only every Friday morning from 9:00-9:05, then you will never violate your SLO. However, not many people prefer to use the system first thing Friday morning. Service Monitoring makes SLO creation easy. On the Services overview page, select one of the listed services. If a service is built on a Google Cloud compute technology that supports Service Monitoring, it will be automatically listed. Next, click Create SLO. Select an option from the SLI metric. The options include availability, latency and other. Availability is a ratio of the number of successful responses to the number of all responses. Latency is the ratio of the number of calls that are below the specified Latency Threshold to the number of all calls. The option Other gives you the Metrics Explorer window and lets you create your own SLI from the beginning. As previously discussed, you can also select request-based or windows-based SLOs here. In the Compliance Period section, select the Period Type and the Period Length. The two compliance period types are calendar-based and rolling. In the Performance Goal section, enter a percentage in the Goal field to set the performance target for the SLI. Service Monitoring uses this value to calculate the error budget you have for this SLO. You can create an alert by just clicking Create alerting policy. Click an individual service on the Services Overview page to view its details. There, you can see existing SLOs and, by expanding them, their details. The SLI status, the error budget remaining, and the current level of SLO compliance are all displayed. If alerts have been set, their status is also displayed. Service Monitoring can trigger an alert when a service is heading to violate an SLO. The alerting policy uses a lookback window to examine a period for trends. The burn rate threshold is then used to determine whether an alert should be raised. In this example, we are using a 60-minute window. A burn rate threshold of 1 would use 100% of the error budget by the end of the 7-day period. In this case, if we see a trend that would burn through our total error budget in 1/10th of those seven days or faster, then we should raise an alert. After the SLO is created, it’s easy to monitor the SLI status, error budget, compliance, and alert status. In this case, I’ve selected to view the Service level indicator, so it’s displaying the current SLI values. You could also examine the Error budget or Alerts firing tabs for more details.

### Lab - [Service Monitoring](https://www.cloudskillsboost.google/course_templates/99/labs/432502)

Use the new Service Monitoring UI and API to create SLOs

- [ ] [Service Monitoring](../labs/Service-Monitoring.md)

### Quiz - [Quiz - Alerting Policies](https://www.cloudskillsboost.google/course_templates/99/quizzes/432503)

#### Quiz 1.

> [!important]
> **In the statement "Maintain an error rate of less than 0.3% for the billing system", what is an SLI?**
>
> - [ ] 0.3%
> - [ ] Error rate
> - [ ] Less than 0.3%
> - [ ] Billing system

#### Quiz 2.

> [!important]
> **Explain error budget.**
>
> - [ ] The proportion of alerts detected that were relevant to the sum of relevant alerts and missed alerts.
> - [ ] How long alerts fire after an issue is resolved.
> - [ ] How long it takes to send notifications in various conditions.
> - [ ] The proportion of alerts detected that were relevant to the sum of relevant and irrelevant alerts.

#### Quiz 3.

> [!important]
> **In evaluating your alerting policies, which below best describes precision?**
>
> - [ ] How long alerts take to be acknowledged.
> - [ ] The proportion of events detected that were significant.
> - [ ] How long it takes to send notifications in various conditions.
> - [ ] How long alerts fire after an issue is resolved.

## Advanced Logging and Analysis

In this module, we will examine some of Google Cloud's advanced logging and analysis capabilities. Specifically, in this module you will learn to identify and choose among resource tagging approaches, define log sinks, create monitoring metrics based on log entries, link application errors to Logging and other operation tools using Error Reporting, and export logs to BigQuery for long term storage and SQL based analysis.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/99/video/432504)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=KkUcR4OVPEk)

In this module, we examine some of Google Cloud's advanced logging and analysis capabilities. Specifically, in this module you will learn to: Use log explorer features Explain the features and benefits of log-based metrics Define log sinks (inclusion filters) and exclusion filters Explain how Big query can be used to analyze logs Use log analytics on Google Cloud

### Video - [Cloud Logging overview and architecture](https://www.cloudskillsboost.google/course_templates/99/video/432505)

- [YouTube: Cloud Logging overview and architecture](https://www.youtube.com/watch?v=7BGkqyfCUNc)

As we learned earlier, Cloud Logging allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud. It is a fully managed service that performs at scale and can ingest application and system log data from thousands of VMs. In this section we will go through logging architecture and understand its use cases. Logs is one of the top most visited sections in Google Cloud console and one of most transitional, which indicated that it is an important component of many scenarios. End users need logs for troubleshooting and information gathering but don’t want to be overwhelmed with the data. Logs are the pulse of your workloads and application. Cloud Logging helps you to: Gather data from various workloads:. This data is required to troubleshoot and understand the workload and application needs. It helps you analyze large volumes of data: Tools like Error Reporting, Log Explorer, and Log Analytics let you focus from large sets of data. Cloud Logging also helps you route your logs to the region or service of your choice for additional compliance or business benefits. You can also get Compliance Insights by leveraging audit and app logs for compliance patterns and issues. We will cover this in the Audit logs module in detail. Lets next understand the architecture of Cloud Logging. Cloud Logging architecture consists of the following components: Log collection, log routing, log sinks and log analysis. Log Collections: These are the places where log data originates. Log sources can be Google Cloud services, such as Compute Engine, App Engine, and Kubernetes Engine, or your own applications. The Log Router is responsible for routing log data to its destination. The Log Router uses a combination of inclusion filters and exclusion filters to determine which log data is routed to each destination. Log sinks are destinations where log data is stored. Cloud Logging supports a variety of log sinks, including Cloud Logging log buckets, Cloud Pub/Sub topics, BigQuery, cloud storage bucket. Each of these have various benefits. Cloud Logging log buckets are storage buckets that are specifically designed for storing log data. Whereas Cloud Pub/Sub topics are topics can be used to route log data to other services, such as third-party logging solutions. BigQuery is a fully-managed, petabyte-scale analytics data warehouse that can be used to store and analyze log data. Cloud Storage buckets provides storage of log data in Cloud Storage. Log entries are stored as JSON files. Log Analysis: Cloud Logging provides several tools to analyze logs. These include logs explorer, error reporting, logs-based metrics, and log analytics. Logs Explorer is optimized for troubleshooting use cases with features like log streaming. Error Reporting help users react to critical application errors through automated error grouping and notifications. Logs-based metrics, dashboards and alerting provide other ways to understand and make logs actionable. Log Analytics feature expands the toolset to include ad hoc log analysis capabilities.

### Video - [Log types and collection](https://www.cloudskillsboost.google/course_templates/99/video/432506)

- [YouTube: Log types and collection](https://www.youtube.com/watch?v=0imGTXK7FFo)

Let us start with Log Collection and move our way forward to routing and storage and finally visualization. The Google Cloud platform logs visible to you in Cloud Logging vary, depending on which Google Cloud resources you're using in your Google Cloud project or organization. Let's explore the key log categories. Platform logs are logs written by Google Cloud services. These logs can help you debug and troubleshoot issues, and help you better understand the Google Cloud services you're using. For example, VPC Flow Logs record a sample of network flows sent from and received by VM instances. Component logs are similar to platform logs, but they are generated by Google-provided software components that run on your systems. For example, GKE provides software components that users can run on their own VM or in their own data center. Logs are generated from the user's GKE instances and sent to a user's Cloud project. GKE uses the logs or their metadata to provide user support. Security logs help you answer "who did what, where, and when." Cloud Audit Logs provide information about administrative activities and accesses within your Google Cloud resources. Access Transparency provides you with logs of actions taken by Google staff when accessing your Google Cloud content. User-written logs are logs written by custom applications and services. Typically, these logs are written to Cloud Logging by using Ops Agent or Cloud Logging API or Cloud Logging client libraries You can programmatically send application logs to Cloud Logging by using client libraries or by using one of the Logging agents. When you can't use them, or when you only want to experiment, you can write logs by using the gcloud logging write command or by sending HTTP commands to the Cloud Logging API endpoint entries.write. If you're using one of the agents, then your applications can use any established logging framework to emit logs. For example, in container environments like Google Kubernetes Engine or Container-Optimized OS, the agents automatically collect logs from stdout and stderr. On virtual machines (VMs), the agents collect logs from known file locations or logging services like the Windows Event Log, journald, or syslogd. Serverless compute services like Cloud Run and Cloud Functions, include simple runtime logging by default. Logs written to stdout or stderr will appear automatically in the Google Cloud console.

### Video - [Storing, routing and exporting the logs](https://www.cloudskillsboost.google/course_templates/99/video/432507)

- [YouTube: Storing, routing and exporting the logs](https://www.youtube.com/watch?v=83IhRtWZfwc)

Now that we understand the log collection, let‚Äôs look at how logs can be routed and exported for long-term storage and analysis. What we call Cloud Logging is actually a collection of components exposed through a centralized logging API. The components include log router, log sinks and log storage. Entries are passed through the API and fed to Log Router. Log Router is optimized for processing streaming data, reliably buffering it, and sending it to any combination of log storage and sink (export) locations. By default, log entries are fed into one of the default logs storage buckets. Exclusion filters might be created to partially or totally prevent this behavior. Log sinks run in parallel with the default log flow and might be used to direct entries to external locations. Locations might include additional Cloud Logging buckets, Cloud Storage, BigQuery, Pub/Sub, or external projects. Inclusion and exclusion filters can control exactly which logging entries end up at a particular destination, and which are ignored completely. For each Google Cloud project, Logging automatically creates two logs buckets: _Required and _Default, and corresponding log sinks with the same names. All logs generated in the project are stored in one of these two locations: _Required: This bucket holds Admin Activity audit logs, System Event audit logs, and Access Transparency logs, and retains them for 400 days. You aren't charged for the logs stored in _Required, and the retention period of the logs stored here cannot be modified. You cannot delete or modify this bucket. _Default: This bucket holds all other ingested logs in a Google Cloud project, except for the logs held in the _Required bucket. Standard Cloud Logging pricing applies to these logs. Log entries held in the _Default bucket are retained for 30 days, unless you apply custom retention rules. You can't delete this bucket, but you can disable the _Default log sink that routes logs to this bucket. The Logs Storage page displays a summary of statistics for the logs that your project is receiving, including: Current total volume: The amount of logs your project has received since the first date of the current month. Previous month volume: The amount of logs your project received in the last calendar month. Projected volume by EOM: The estimated amount of logs your project will receive by the end of the current month, based on current usage. You can view the total usage by resource type for the current total volume. The link opens Metrics Explorer, which lets you build charts for any metric collected by your project. Log Router sinks can be used to forward copies of some or all of your log entries to non-default locations. Use cases include storing logs for extended periods, querying logs with SQL, and access control. Here, you see we‚Äôve started creating a sink by generating a log query for a particular subset of entries. We will pass that subset to one of the available sink locations. There are several sink locations, depending on need. Cloud Logging bucket works well to help pre-separate log entries into a distinct log storage bucket. BigQuery dataset allows the SQL query power of BigQuery to be brought to bear on large and complex log entries. Cloud Storage bucket is a simple external Cloud Storage location, perhaps for long-term storage or processing with other systems. Pub/Sub topic can export log entries to message handling third-party applications or systems created with code and running somewhere like Dataflow or Cloud Functions. Splunk is used to integrate logs into existing Splunk-based system. The Other project option is useful to help control access to a subset of log entries. The process for creating log sinks mimics that of creating log exclusions. It involves writing a query that selects the log entries you want to export in Logs Explorer, and choosing a destination of Cloud Storage, BigQuery, or Pub/Sub. The query and destination are held in an object called a sink. Sinks can be created in Google Cloud projects, organizations, folders, and billing accounts. Use Logs Explorer to build a query that selects the logs you want to exclude. Save the query to use when building the exclusion. Use the Log Explorer query to create an exclusion filter that filters the unwanted entries out of the sink. Give the exclusion a name and add the filter for log entries to exclude. It might be helpful to leave some representative events, depending on the exclusion. Create the exclusion and it will go into effect immediately. Use the Navigation menu to initiate editing of that entity. Take care here, because excluded log events will be lost forever. Over the next several slides, we will investigate some possible log export processing options. Here, for example, we are exporting through Pub/Sub, to Dataflow, to BigQuery. Dataflow is an excellent option if you're looking for real-time log processing at scale. In this example, the Dataflow job could react to real-time issues, while streaming the logs into BigQuery for longer-term analysis. Sink pipelines targeting Cloud Storage tend to work best when your needs align with Cloud Storage strengths. For example, long-term retention, reduced storage costs, and configurable object lifecycles. Cloud Storage features include automated storage class changes, auto-delete, and guaranteed retention. Here, we have an example organization that wants to integrate the logging data from Google Cloud, back into an on-premises Splunk instance. You can ingest logs into Splunk you can either stream logs using Pub/Sub to Splunk Dataflow or using the Splunk Add-on for Google Cloud. Pub/Sub is one of the options available for exporting to Splunk, or to other third-party System Information and Event Management (SIEM) software packages. A common logging need is centralized log aggregation for auditing, retention, or non-repudiation purposes. Aggregated sinks allow for easy exporting of logging entries without a one-to-one setup. The sink destination can be any of the destinations discussed up to now. There are three available Google Cloud Logging aggregation levels. We've discussed a project-level log sink. It exports all the logs for a specific project and a log filter can be specified in the sink definition to include/exclude certain log types. A folder-level log sink aggregates logs on the folder level and can include logs from children resources (subfolders, projects). And for a global view, an organization-level log sink can aggregate logs on the organization level and can also include logs from children resources (subfolders, projects). Security practitioners onboard Google Cloud logs for security analytics. By performing security analytics, you help your organization prevent, detect, and respond to threats like malware, phishing, ransomware, and poorly configured assets. One of the steps in security log analytics workflow is to create aggregate sinks and route those logs to a single destination depending on the choice of security analytics tool, such as Log Analytics, BigQuery, Chronicle, or a third-party security information and event management (SIEM) technology. Logs are aggregated from your organization, including any contained folders, projects, and billing accounts. There are a few naming conventions that apply to log entry fields. For log entry fields that are part of the LogEntry type, the corresponding BigQuery field names are precisely the same as the log entry fields. For any user-supplied fields, the letter case is normalized to lowercase, but the naming is otherwise preserved. For fields in structured payloads, as long as the @type specifier is not present, the letter case is normalized to lowercase, but naming is otherwise preserved. For information on structured payloads where the @type specifier is present, see the Payload fields with @type documentation. You can see some examples on the current slide. Here's a sample query over the Compute Engine logs. It retrieves log entries for multiple log types over multiple days. The query searches the last three days (today -2) of the syslog and apache-access logs. The query retrieves results for the single Compute Engine instance ID seen in the where clause. In this BigQuery example, we are looking for unsuccessful App Engine requests from the last month. Notice how the from clause is constructing the table data range. The status not equal to 200 is examining the HTTP status for anything that isn't 200. That is to say, anything that isn't a successful response.

### Video - [Query and view logs](https://www.cloudskillsboost.google/course_templates/99/video/432508)

- [YouTube: Query and view logs](https://www.youtube.com/watch?v=SldxCU_5FnA)

Once you have collected logs and routed to the right destination, now is the time to query and view logs. The Logs Explorer interface lets you retrieve logs, parse and analyze log data, and refine your query parameters. The Logs Explorer contains the following panes: The Action toolbar to refine logs to projects or storage views, share a link and learn about logs explorer. Query pane is where you can build queries, view recently viewed and saved queries and a lot more. Results Toolbar is used to quickly show or hide logs and histogram pane and create a log based metric or alert. Jump to now option helps query and view the current time results. Query results is the details of results with a summary and timestamp that helps troubleshoot further. Log fields pane is used to filter your options based on various factors such as a resource type, log name, project ID, etc,. Histogram is where the query result is visualized as histogram bars, where each bar is a time range and is color coded based on severity. Ultimately, it’s the query that selects the entries displayed by Logs Explorer. Queries may be created directly with the Logging Query Language (LQL), using the drop-down menus, the logs field explorer, or by clicking fields in the results themselves. Start with what you know about the entry you’re trying to find. If it belongs to a resource, a particular log file, or has a known severity, use the query builder drop-down menus. The query builder drop-down menu makes it easy to start narrowing your log choices. Resource lets you specify resource.type. You can select a single resource at a time to add to the Query builder. Entries use the logical operator AND. Log name lets you specify logName. You can select multiple log names at once to add to the Query builder. When selecting multiple entries, the logical operator OR is used. Severity lets you specify severity. You can select multiple severity levels at once to add to the Query builder. When selecting multiple entries, the logical operator OR is used. The next several slides are included for reference. Advanced queries support multiple comparison operators as seen here. The equal and not equal operators help filter values that match or not match a value assigned to a field name. These are useful when you search for a specific resource type or id that you want to evaluate. The numeric ordering operators are handy when searching for logs filtering a timestamp or duration. The colon operation helps check if a value exists. This is useful when you want to match a substring within a log entry field. To test if a missing or defaulted field exists without testing for a particular value in the field, use the :* comparison. If you’re looking for a specific set of log entries and have a rough idea when they would have been generated, start by narrowing to a specific time range. You can select one of the pre-created choices, set a custom range, or jump to a particular time. The Log fields panel offers a high-level summary of logs data and provides a more efficient way to refine a query. It shows the count of log entries, sorted by decreasing count, for the given log field. The log field counts correspond to the time range used by the Histogram panel. You can add fields from the Log fields panel to the Query builder to narrow down and refine a query by clicking a field. When a query is run, the log field counts are incrementally loaded as the log entries are progressively scanned. Once the query is complete, which is indicated by the completion of the blue progress bar, you see the total counts for all log fields. The histogram panel lets you visualize the distribution of logs over time. Visualization makes it easier to see trends in your logs data and troubleshoot problems. For example, the severity colors make it easy to spot an increasing number of errors even when the volume of requests is relatively constant. To analyze your log data, point to a bar in the Histogram panel and select Jump to time to drill into a narrower time range. A new query runs with that time-range restriction. Advanced queries support the AND, OR, and NOT boolean expressions for joining queries. A couple of things to keep in mind include: Ensure to use the all caps for the operator name. The NOT operator has the highest precedence, followed by OR and AND in that order. The Boolean operators AND and OR are short-circuit operators. Here is a simple recipe for finding entries. When you’re trying to find log entries, as mentioned earlier, start with what you know: the log filename, resource name, even a bit of the contents of the logged message might work. Full text searches are slow, but they may be effective. For example, you might search for “/score called”. If possible, restrict text searches to an entry region. You can use the built-in SEARCH function to find strings in your log data as shown on slide. Both forms of the SEARCH function contain a query argument, which must be formatted as a string literal. In the first form, the entire log entry is searched. In the second form, you specify the field in the log entry to search. The Logging query language supports different ways that you can search your log data. When searching for a string, it is more efficient to use the SEARCH function than to perform a global search or a substring search. However, you can't use the SEARCH function to match non-text fields. Lets expand this discussion and cover some of the tips on finding log entries quickly: Search for specific values of indexed fields, like the name of the log entry, resource type, and resource labels. Apply constraints on resource.type and resource.label field. Be specific on which logs you’re searching. As seen in the example, you are seaching by referring to it or them by name. Limit the time range that you’re searching to reduce the log data that is being queried.

### Video - [Using log-based metrics](https://www.cloudskillsboost.google/course_templates/99/video/432509)

- [YouTube: Using log-based metrics](https://www.youtube.com/watch?v=p7Ga8Qya-KM)

Let’s next focus about generating monitoring metrics from logging data. Logs-based metrics derive metric data from the content of log entries. For example, metrics can track the number of entries that contain specific messages or extract latency information that is reported in the logs. These metrics transform into time series data and use it in Cloud Monitoring Charts and Alerting Policies. There are two types of log-based metrics, System-defined log-based metrics and User-defined log-based metrics. System-defined log-based metrics, provided by Cloud Logging can be used by all Google Cloud projects. System-defined log-based metrics are calculated only from logs that have been ingested by Logging. If a log has been explicitly excluded from ingestion by Cloud Logging, it isn't included in these metrics. User-defined log-based metrics, created by you to track things in your Google Cloud project that are of particular interest to you. For example, you might create a log-based metric to count the number of log entries that match a given filter. Log-based metrics are suitable when you want to do any of the following: Count the occurrences of a message, like a warning or error, in your logs and receive a notification when the number of occurrences crosses a threshold. Observe trends in your data, like latency values in your logs, and receive a notification if the values change in an unacceptable way. Create charts to display the numeric data extracted from your logs. A refresher of the key IAM roles that relate to logging and monitoring. First, on the logging side: Logs Configuration Writers can list, create, get, update, and delete log-based metrics. Logs Viewers can view existing metrics. On the monitoring side, Monitoring Viewers can read the time series in log-based metrics. And finally, Logging Admins, Editors, and Owners are all broad-level roles that can create log-based metrics. There are three types of log-based metrics: counter or distribution. All predefined system log-based metrics are the counter type, but user-defined metrics can be either counter, distribution or boolean types. Counter metrics count the number of log entries matching an advanced logs query. So, if we simply wanted to know how many of our "/score called" entries were generated, we could create a counter. Distribution metrics record the statistical distribution of the extracted log values in histogram buckets. The extracted values are not recorded individually. Their distribution across the configured buckets is recorded, along with the count, mean, and sum of squared deviations of the values. Boolean metrics record where a log entry matches a specified filter. System-defined log-based metrics apply at the Google Cloud project level. These metrics are calculated by the Log Router and apply to logs only in the Google Cloud project in which they're received. User-defined log-based metrics can apply at either the Google Cloud project level or at the level of a specific log bucket: Project-level metrics are calculated like system-defined log-based metrics; these user-defined log-based metrics apply to logs only in the Google Cloud project in which they're received. Bucket-scoped metrics apply to logs in the log bucket in which they're received, regardless of the Google Cloud project in which the log entries originated. With bucket-scoped log-based metrics, you can create log-based metrics that can evaluate logs in the following cases: Logs that are routed from one project to a bucket in another project. Logs that are routed into a bucket through an aggregated sink. Before we create a log-based metric, let's generate some logging entries. Here we see a basic NodeJS app built with the simple and lightweight Express web server. The app is run as a managed container on the Cloud Run service. The code watches for a request to come into the server on the '/score' path. When a /score request arrives, the code generates a random 1-100 score, and it then creates a log entry. Earlier code, not shown on this slide, created a unique identifier for the container serving this request in containerID and a random value called funFactor. The log entry contains the text "/score called”, the random score, the container ID, and the fun factor. Lastly, a basic message, also containing the score, is sent back to the browser. Use the Query builder to access project logs. In the list of entries, we've located one of the "/score called" entries. Now we can filter to select those entries by clicking "/score called", and selecting Show matching entries. Imagine we've generated some load on our Cloud Run sample application, and we'd like to use the log events to generate a log-based metric. As defined earlier there are two fundamental log-based metric types, System log-based metrics and User-defined log-based metrics. The latter is what we are creating now. Note the Create Metric button at the top of the interface. This is an example of a basic flow for creating log-based metrics: You start by finding the log with the requisite data. Then you filter it to the required entries. Create a metric. Pick your metric type (Counter or Distribution). If Distribution, then set configurations. And finally, add labels as needed. Like many cloud resources, labels can be applied to log-based metrics. Their prime use is to help with group-by and filtering tasks in Cloud Monitoring. Labels allow log-based metrics to contain multiple time series—one for each label value. All log-based metrics come with some default labels and you can create additional user-defined labels in both counter-type and distribution-type metrics by specifying extractor expressions. An extractor expression tells Cloud Logging how to extract the value of the label from log entries. You can specify the label's value as either of the following: The entire contents of a named field in the LogEntry object. A part of a named field that matches a regular expression (regexp). You can extract labels from the LogEntry built-in fields, such as httpRequest.status, or from one of the payload fields, textPayload, jsonPayload, or protoPayload. Label with care. A metric can support up to ten user-defined labels, and once created, a metric cannot be removed. Also, each log-based metric is limited to about 30,000 active time series. Each label can grow the time series count significantly. For example, if your log entries come from 100 resources, such as VM instances, and you define a label with 20 possible values, then you can have up to 2,000 time series for your metric. User-defined labels can be created when creating a log-based metric. The label form requires: Name which is an identifier which will be used to label in Monitoring. Description which describes the label. Try to be as specific as possible. Label type, choose String, Boolean, or Integer. Enter the name of the log entry field that contains the value of the label. This field supports autocomplete. If the value of your label consists of the field's entire contents, then you can leave this field empty. Otherwise, specify a regular expression (regexp) that extracts the label value from the field value.

### Video - [Log analytics](https://www.cloudskillsboost.google/course_templates/99/video/432510)

- [YouTube: Log analytics](https://www.youtube.com/watch?v=KiJxEyihP_M)

Next us next take a look at one of the new feature in Cloud Logging, Log Analytics. Log Analytics gives you the analytical power of BigQuery within the Cloud Logging console and provides you with a new user interface that's optimized for analyzing your logs. When you create a bucket and activate analytics on it, Cloud Logging makes the logs data available in both the new Log Analytics interface and BigQuery; you don't have to route and manage a separate copy of the data in BigQuery. You can still query and examine the data as usual in Cloud Logging with the Logging query language. The three prominent use cases for Cloud Logging in Log Analyics are troubleshooting, log analytics and reporting. Logs are written to the Logging API via client libraries, stdout/fluentbit agent or directly via API,. Logs Explorer helps with troubleshooting and getting to the root cause with search, filter, histogram and suggested search. Log Router routes logs to the Logging Sink for the Logs Bucket. Log Analytics, analyze application performance, data access and network access patterns. Log Analytics pipeline maps logs to BigQuery tables and writes to BigQuery. Use the same logs data in Log Analytics directly from BigQuery to report on aggregated application and business data found in logs. The logs data in your analytics-enabled buckets is different than logs routed to BigQuery via traditional export in the following ways: Log data in BigQuery is managed by Cloud Logging. BigQuery ingestion and storage costs are included in your Logging costs. Data residency and lifecycle are managed by Cloud Logging. You can query your logs on Log Analytics-enabled buckets directly in Cloud Logging via the new Log Analytics UI. The Log Analytics UI is optimized for viewing unstructured log data. You can also access your logs data in BigQuery using a read-only view if you want to combine your logs data with other data in BigQuery. You can turn on or off access to your analytics-enabled buckets in BigQuery by turning on or off the option that connects the logs to BigQuery. When the option is enabled, you can query the logs directly from BigQuery, including joining your logs data with other BigQuery datasets. To create an analytics-enabled bucket by using the console: Navigate to Logs Storage. Click Create log bucket. Select Upgrade to use Log Analytics. Note that ppgrading a bucket to use Log Analytics is permanent. You can't downgrade the log bucket to remove the use of Log Analytics. Log Analytics is useful in multiple aspects. Let's look at different fields' perspectives, starting with DevOps. For a DevOps specialist it is important to quickly troubleshoot an issue that requires to reduce Mean Time to Repair (MTTR). Log Analytics includes capabilities to count the top requests grouped by response type and severity, which allows engineers to diagnose the issues. A security personal is interest in finding all the audit logs associated with a specific use over the past month. Log Analytics help better investigate the security -related attacks with queries over large volumes of security data. For more information, refer to the documentation. A IT/Network Operations is interested in identifying network issues for GKE instances that are using VPC and firewall rules. Log Analytics in this case provides better network insights and management through advanced log aggregation capabilities. For more information, refer to the documentation.

### Lab - [Log Analytics on Google Cloud](https://www.cloudskillsboost.google/course_templates/99/labs/432511)

In this lab you will learn how to use Cloud Logging to analyze your logs.

- [ ] [Log Analytics on Google Cloud](../labs/Log-Analytics-on-Google-Cloud.md)

### Video - [Module Summary](https://www.cloudskillsboost.google/course_templates/99/video/432512)

- [YouTube: Module Summary](https://www.youtube.com/watch?v=ahHvY0Nfv-w)

This brings us to the end of the module. In this module you learned to: How to use log explorer features Explain the features and benefits of log-based metric Define log sinks (inclusion filters)and exclusion filter Explain how Big query can be used to analyze logs Export logs to BigQuery for analysis Use log analytics on Google Cloud

### Quiz - [Quiz - Advanced Logging and Analysis](https://www.cloudskillsboost.google/course_templates/99/quizzes/432513)

#### Quiz 1.

> [!important]
> **Your manager wants a daily report of resource utilization by application. Where would the best export sink be?**
>
> - [ ] Pub/Sub
> - [ ] Cloud Storage
> - [ ] Spanner
> - [ ] BigQuery

#### Quiz 2.

> [!important]
> **You want to compare resource utilization for VMs used for production, development, and testing. What should you do?**
>
> - [ ] Name the VMs with a prefix like "dev-", "test-", and "prod-" and filter on the name property when reporting.
> - [ ] Put those resources in different projects and use dataflow to create an aggregation of log values for each.
> - [ ] Export all machine logs to Cloud Storage and use Cloud Functions to build reports based on the VM tags.
> - [ ] Add a label called "state" to your VMs with the values "dev", "test", and "prod" and group by that label in your monitoring chart.

## Working with Audit Logs

In this module, we will examine how to use Google's Cloud Audit logs. You will learn how to use Cloud Audit logs to answer the question, "Who, did what, and when?" We will also cover best practices for Audit Logging.

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/99/video/432514)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=OFkPbOMIKDM)

Welcome to the module, working with Cloud Audit logs. In this module, we investigate the core Cloud Audit Logs that Google Cloud collects. Upon completion of this module, you will be able to: Explain Cloud Audit Logs. Explain different audit logs and its features and functionalities also best practices to implement audit logs.

### Video - [Cloud Audit Logs](https://www.cloudskillsboost.google/course_templates/99/video/432515)

- [YouTube: Cloud Audit Logs](https://www.youtube.com/watch?v=pztIuhKpXo0)

We start with an overview of what audit logs do for us. Then we move to using the optional Data Access audit logs, explore the format of audit log entries, and wrap up with some logging best practices. In terms of sheer volume of useful information, probably the most important group of logs in Google Cloud are the Cloud Audit Logs. Cloud Audit Logs help answer the question, "Who did what, where, and when? ” It maintains four audit logs for each Google Cloud project, folder, and organization: Admin Activity audit logs System Event audit logs Data Access audit logs Policy Denied audit logs All Google Cloud services will eventually provide audit logs. For now, see the Google services with audit logs documentation for coverage details. Admin Activity audit logs: Admin Activity audit logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. For example, these logs record when users create VM instances or change Identity and Access Management permissions. They are always on, are retained for 400 days, and are available at no charge. To view these logs, you must have the IAM role Logging/Logs Viewer or Project/Viewer. System Event audit logs: System Event audit logs contain log entries for Google Cloud administrative actions that modify the configuration of resources. System Event audit logs are generated by Google systems; they are not driven by direct user action. They are always enabled, free, retained for 400 days. To view these logs, you must have the IAM role Logging/Logs Viewer or Project/Viewer. Data Access audit logs Data Access audit logs contain API calls that read the configuration or metadata of resources. Also, user-driven API calls that create, modify, or read user-provided resource data. Data Access audit logs don’t record the data-access operations on resources that are publicly shared (available to All Users or All Authenticated Users). Data Access audit logs also don’t record the data-access operations on resources that can be accessed without logging into Google Cloud. They are disabled by default (except for BigQuery), and when enabled, the default retention is 30 days. To view these logs, you must have the IAM roles Logging/Private Logs Viewer or Project/Owner. Policy Denied audit logs: When a security policy is violated, this type of audit logs records when access to a user or service account is denied by Google Cloud service. Policy Denied audit logs are generated by default and your Google Cloud project is charged for the logs storage. You can't disable Policy Denied audit logs. However, you can use exclusion filters to prevent these logs from being ingested and stored in Cloud Logging. To view and filter audit logs: Navigate to Logs Explorer Filter by using the Log name drop-down menu. Typing cloudaudit into the filter box is frequently quicker than scrolling. If one of the four audit logs is missing, that simply means it doesn’t currently have any entries. The example here filters the logs by a project and you can select the log entries you would like to audit. You can even use the query builder to filter audit logs. This query is auto populated in the query section when using the UI. For details check out the documentation page. Whether it's a hardware support engineer, or a rep working on a ticket, having dedicated experts manage parts of the infrastructure is a key benefit of operating in Google Cloud. Very similar to Cloud Audit logs, Access Transparency logs help by providing logs of accesses to your data by human Googlers (as opposed to automated systems). Enterprises with appropriate support packages can enable the logs, and receive the log events in near-real time. The log events are surfaced through the APIs, Cloud Logging, and Security Command Center. Access Transparency logs give you different information than Cloud Audit Logs. Cloud Audit Logs record the actions that members of your Google Cloud organization have taken in your Google Cloud resources, whereas Access Transparency logs record the actions taken by Google personnel.

### Video - [Data Access audit logs](https://www.cloudskillsboost.google/course_templates/99/video/432516)

- [YouTube: Data Access audit logs](https://www.youtube.com/watch?v=scnZDs1Egmw)

Let’s continue by looking at Data Access audit logs. Data Access audit logs can be enabled at various levels in the resource hierarchy. These levels include: Organization Folder Project, Resources, and Billing accounts You can also exempt principals from recording data access logs. The final configuration of Data Access audit logs is the union of the configurations. For example, at a project level, you can enable logs for a Google Cloud service. But you can't disable logs for a Google Cloud service that is enabled in a parent organization or folder. The added logging does add to the cost, currently: $0.50 per gigabyte for ingestion. Data Access audit logs are disabled by default, for everything but BigQuery. They may be enabled and configured at the organization, folder, project, or service level. You can control what type of information is kept in the audit logs. There are three types of Data Access audit logs information, admin read, data read and data write. Admin-read records operations that read metadata or configuration information. For example, you looked at the configurations for your bucket. Data-read records operations that read user-provided data. For example, you listed files and then downloaded one from Cloud Storage. Data-write records operations that write user-provided data. For example, you created a new Cloud Storage file. You can exempt specific users or groups from having their data accesses recorded. This functionality comes in handy when you want to reduce the cost and noise associated with the volume of logs that are not of your interest. Data Access audit logs can be of high volume, so cost associated is directly proportional to the volume of data logs. You can also use the Google Cloud CLI or the API to enable Data Access audit logs. If you're using the gcloud CLI frequently, the easiest way is to get the current IAM policies, as seen in step 1, and write them to a file. Then you can edit the /tmp/policy.yaml file to add or edit the auditLogConfigs. You can also add the log details per service, like this example is enabling logging for Cloud Run. You can even enable logging on all services. Then, as seen in step 3, you would set that as the new IAM policy.

### Video - [Audit logs entry format](https://www.cloudskillsboost.google/course_templates/99/video/432517)

- [YouTube: Audit logs entry format](https://www.youtube.com/watch?v=Bie2Vj1HTeE)

Now that you learned to enable the logs you need, let’s examine the logging entries themselves. Every audit log entry in Cloud Logging is an object of type LogEntry. What distinguishes an audit log entry from other log entries is the protoPayload field, which contains an AuditLog object that stores the audit logging data. Note the log name, which tells us that we’re looking at an example from Data Access audit logs. Identify the principal generating log by looking at the principalEmail. The operation field only exists for a large or long-running audit log entries. Google has a standard List of official service names. You can use this list as a handy reference. On this slide, you can tell we’re looking at a query that was run in BigQuery. If you expanded the serviceData field, you could actually see the query itself. So, when someone at your organization runs that unexpected, $40,000 query, you can see who ran it and what the query was.

### Video - [Best practices](https://www.cloudskillsboost.google/course_templates/99/video/432518)

- [YouTube: Best practices](https://www.youtube.com/watch?v=ryhVhjRwnwc)

Let’s go over a few best practices before finish this module. Like anything in the cloud, start by planning first. Spend time and create a solid plan for Data Access audit logs. Think organization, folder, then project. Like most organizations, some of your projects will be very specialized, but usually, they do break down into common organizational types. Then, create a test project and experiment to see if the logging works the way you expect. Then roll out the plan, and don't forget automation (Infrastructure as Code). Remember that Data Access audit logs can be enabled as high as the organization. The pro would be detailed information on exactly who accessed, edited, and deleted what, and when. The con is that Data Access logs can grow to be large, and are billed per gigabyte. This also results in higher queries per second based on the number of data access requests. Infrastructure as Code (IaC) is essentially the process of automating the creation and modifications to your infrastructure using a platform. The platform supports configuration files, which can be put through a CI/CD (Continuous integration and continuous deployment) pipeline, like with code. Terraform is an open source package from HashiCorp or paid for enterprise version. It isn't hosted directly in Google Cloud, though it’s installed by default in Cloud Shell. State management is a decision point for your organization. It can be remote or local. Remote options include using Cloud Storage or Terraform Cloud. Local storage involves setting up something local to your organization, or using the pay-to-use HashiCorp Enterprise service. Audit logs also inform you about the resources provisioned using an IaC tool. It is really useful if you want to control log sink filters at the project level. With terraform you can set default include/exclude filters and have them applied to every project. To manage your Google Cloud organization's logs, you can aggregate them from across your organization into a single Cloud Logging bucket. It is recommended to create user-defined buckets to centralize or subdivide your log storage. Based on compliance and usage requirements, customize your logs storage by choosing where your logs are stored and defining the data retention period. Some organization might have latency, compliance, and availability requirements in specific regions. Configure a default storage location to automatically apply a region in which buckets are create for log data. By default, Cloud Logging encrypts customer content stored at rest. Your organization might have advanced encryption requirements that the default encryption at rest doesn't provide. To meet your organization's requirements, instead of Google managing the key encryption keys that protect your data, configure customer-managed encryption keys (CMEK) to control and manage your own encryption. We've discussed the options and benefits of exporting logs. Again, make this part of your plan. Start by deciding what, if anything, you will export from Aggregated Exports at the organization level. Next, decide what options you will use, project by project, folder by folder, and so on. Then, carefully consider your filters—both what they leave in, and what they leave out. Filters apply to all logs, not just to exports. Lastly, carefully consider what, if anything, you will fully exclude from logging. Remember that excluded entries will be gone forever. Side-channel leakage of data through logs is a common issue. You need to be careful about who gets which kind of access and to which logs. Remember some of the discussions earlier in this course on monitoring metrics scope? And how to monitor a current project? That's where your security starts. Are you monitoring project by project, or are you selectively grouping work projects into higher-level monitored projects? Use appropriate IAM controls on both Google Cloud-based and exported logs, and only allowing minimal access required to get the job done. . Especially scrutinize the Data Access audit log permissions, because they often contain Personally Identifiable Information (PII). Log buckets store logs, including audit logs. Log views control access to logs in a log bucket. Custom log views can be created to control access to logs from specific projects or users. This helps protect sensitive data and ensures only authorized users have access. Lastly, a few access scenarios, starting with operational monitoring. Let’s explore your high-level teams and assignments. By job, CTO: resourcemanager.organizationAdmin, so they can assign permissions to the security team and service accounts. The CTO can then give the security team logging.viewer so they can view the Admin Activity audit logs. Also, logging.privateLogViewer, so they can view the Data Access audit logs. The view permissions are assigned at the organization level, so they are global. Access control to data exported to Cloud Storage or BigQuery will be secured selectively with IAM. Data in the Data Access audit logs is deemed as personally identifiable information (PII) for this organization. Integrating the application with Cloud DLP gives the ability to redact sensitive PII data when viewing Data Access logs whether they are in the Data Access audit logs or from the historical archive in Cloud Storage. Moving on to development teams. The security team is unchanged from the last slide. They already have logging.viewer, and logging.privateLogViewer from the global assignment. The dev team might get logging.viewer at the folder level so they can see the Admin Activity audit logs for the projects within their development control. They probably also need logging.privateLogViewer at the dev folder so they can see the Data Access audit logs. Limit data they test with though, so they aren't viewing actual customer information. Again, use Cloud Storage or BigQuery IAM to control access to exported logs. Prebuilding dashboards might also be a good option. For external auditors, provide pre-created dashboards where possible. If they need broad access, you can make them with Log Viewer role at the organization level. For BigQuery, they could be BigQuery Data Viewer on the exported dataset. For Cloud Storage, again, you could use IAM, but also remember the temporary access URLs that Cloud Storage supports.

### Lab - [Cloud Audit Logs](https://www.cloudskillsboost.google/course_templates/99/labs/432519)

Have some fun with admin activity and data access audit logs

- [ ] [Cloud Audit Logs](../labs/Cloud-Audit-Logs.md)

### Video - [Module Summary](https://www.cloudskillsboost.google/course_templates/99/video/432520)

- [YouTube: Module Summary](https://www.youtube.com/watch?v=Fe1vLuc1XyE)

THat's a wrap to the short module, working with audit logs. In this module, you learned to: Explain Cloud Audit Logs. List and explain different audit logs. Explain the features and functionalities of the different audit logs. List the Best Practices to implement audit logs.

### Quiz - [Quiz - Working with Audit Logs](https://www.cloudskillsboost.google/course_templates/99/quizzes/432521)

#### Quiz 1.

> [!important]
> **Why are the Data Access audit logs off by default? Select three.**
>
> - [ ] They can be small
> - [ ] May contain sensitive information
> - [ ] They cannot be filtered
> - [ ] They can be large
> - [ ] They are formatted incorrectly
> - [ ] They can be expensive to store

#### Quiz 2.

> [!important]
> **If you want to provide an external auditor access to your logs, what IAM role would be best?**
>
> - [ ] Project Viewer
> - [ ] Logging Viewer
> - [ ] Logging Admin
> - [ ] Project Editor

## Course Summary

We will summarize the topics covered in this couse.

### Video - [Course1 Summary](https://www.cloudskillsboost.google/course_templates/99/video/432522)

- [YouTube: Course1 Summary](https://www.youtube.com/watch?v=6Iu8WXYp7j0)

This brings us to the end of Logging and Monitoring in Google Cloud course. In this course we learned: The purpose and capabilities of Google Cloud operations suite How to implement monitoring for multiple cloud projects Crete alerting policies, uptime checks and alerts to identify and resolve problems quickly Use Cloud Logging to collect logs and export for further analysis If you’re interested in learning more about application performance management, please go to the next course ”Observability in Google Cloud”. See you next time!

## Course Resources

Student PDF links to all modules

### Document - [Module 1: Introduction to Google Cloud Operations Suite](https://www.cloudskillsboost.google/course_templates/99/documents/432523)

### Document - [Module 2:  Monitoring Critical Systems](https://www.cloudskillsboost.google/course_templates/99/documents/432524)

### Document - [Module 3: Alerting Policies](https://www.cloudskillsboost.google/course_templates/99/documents/432525)

### Document - [Module 4:  Advanced Logging and Analysis](https://www.cloudskillsboost.google/course_templates/99/documents/432526)

### Document - [Module 5: Working with Audit Logs](https://www.cloudskillsboost.google/course_templates/99/documents/432527)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
