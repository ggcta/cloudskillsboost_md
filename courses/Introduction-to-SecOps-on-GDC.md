---
id: 1199
name: 'Introduction to SecOps on GDC'
datePublished: 2025-01-24
topics:
- Dashboard
- Alerts
- Deployment
type: Course
url: https://www.cloudskillsboost.google/course_templates/1199
---

# [Introduction to SecOps on GDC](https://www.cloudskillsboost.google/course_templates/1199)

**Description:**

The first course provides a high-level overview of security fundamentals on the GDC platform.

**Objectives:**

- Describe the GDC offering and architecture, the SecOps roles in the GDC SOC, and the defining security principles.
- Identify the day-to-day processes and tools that you can use to keep the GDC deployment secure, both proactively and reactively.
- Describe the core GDC security monitoring approaches including default logs, dashboards, and alerts.

## Overview

This module introduces the audience, prerequisites, and agenda for the course.

### Video - [Course overview](https://www.cloudskillsboost.google/course_templates/1199/video/521976)

- [YouTube: Course overview](https://www.youtube.com/watch?v=XFbUM7MoVCE)

SPEAKER: Welcome to the Google Distributed Cloud, or GDC air-gapped Security Operations Fundamentals series of courses. Security itself is an all-encompassing endeavor. Within each of the three courses that make up this series, you'll focus on a specific aspect of GDC for security operations. The first course provides a high-level overview of security fundamentals on the GDC platform. You'll be introduced to the GDC offering and architecture, the SecOps roles in the GDC Security Operations Center, or SOC, and the definitions of the security principles. You'll also learn about the day-to-day processes and tools that you can use to keep the GDC deployment secure, both proactively and reactively. Finally, you will review the default logs, dashboards, and alerts which are at the core of GDC security monitoring. The second course provides you with a deep dive into the workflows of Tier 1 and Tier 2 security analysts. These workflows are monitoring, intake, and incident response. You will go through a variety of video demonstrations that mimic how you would tackle these workflows in GDC. The third course provides you with a deep dive into the workflows of Tier 3 analysts. This workflow will focus on vulnerability management, threat modeling, and security engineering. Once again, you will watch video demonstrations on how you would tackle these activities. To benefit fully from taking this course, you should have these prerequisite skills and knowledge prior understanding of SecOps, basic proficiency with Windows and Linux logs, basic understanding of Kubernetes terminology for logging, prior completion of the Google Cloud Fundamentals-- Core Infrastructure course, or equivalent experience with Google Cloud Services hosted by GDC. Throughout this series of courses, you'll learn about GDC in context by exploring how Cymbal Federal, a fictional organization, accomplishes its SecOps goals using the GDC air-gapped platform. Imagine that you have just joined the GDC Security Operations Center team for Cymbal Federal as a security analyst. Congratulations on your new role. Cymbal Federal is a government entity aligned to support the broad mission objectives of the executive branch of government. During a recent review, the operation and application capabilities of Cymbal Federal were designated mission critical. An organization-wide, multi-year digital transformation program is underway. Cymbal Federal aims to ensure compliance while modernizing its suite of applications with GDC. GDC offers a solution for use in an on-premises environment. Up to this point, the Cymbal Federal internal teams have mastered Google Cloud Fundamentals and Kubernetes and have defined the major applications to develop on GDC. The team is now focused on the creation of the first end-user applications and workloads on GDC. In preparation for the go-live, Cymbal Federal has awarded their existing third-party data center management partner the task of creating and running the mission operations center. This partner has started to establish the operate portion of the GDC program. This involves end-to-end integrated infrastructure management. As a new staff member, you are being onboarded in order to handle security as part of the GDC Security Operations Center for Cymbal Federal. Based on your previous work experience, you have some familiarity with SecOps. You also have a high-level understanding of the GDC platform. You now want to learn more about the security operations on GDC so that you are able to keep the platform secure. The Cymbal Federal case study attempts to mirror an enterprise-level scenario, and aims to help you understand how the different components of the platform fit together and what their capabilities are. You will revisit this case study throughout all three courses. Each course will include a variety of video presentations, quizzes, and video demonstrations. By completing this course, you'll learn everything you need to begin your role-specific operations in the security operations center of GDC. Let's get started.

## GDC platform overview

This module provides an introduction to the Google Distributed Cloud (GDC) platform. It begins with a general overview of the platform and its services, then delves into the roles and responsibilities of GDC users. The module also explores the two operating models for GDC (customer-operated and Google-operated) and provides a high-level look at the GDC architecture design, emphasizing its security features. Finally, it introduces the Operations Center (OC) as the central hub for security operations.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1199/video/521977)

- [YouTube: Module overview](https://www.youtube.com/watch?v=IgWx_rq0ndI)

SPEAKER: Welcome to Google Distributed Cloud GDC Air-Gapped Platform Overview. In this module, you will be introduced to GDC. You will start with an overview of the platform, including what the platform offers. Then you will review all the services hosted on GDC. This will help you familiarize yourself with what you are protecting. Next, you will be introduced to the three user roles on GDC and their respective responsibilities on the platform. You will also explore the two operating models for GDC, one that is operated by the customer, and one that is operated by Google. Then you will take a high-level look at GDC architecture design. This includes learning about the security guarantees that make GDC an air-gapped environment fit for the most secure of applications. Finally, you will learn about the Operations Center, or OC, which is where your work as security operators happens.

### Video - [What is GDC](https://www.cloudskillsboost.google/course_templates/1199/video/521978)

- [YouTube: What is GDC](https://www.youtube.com/watch?v=ZSiaAPq3xTA)

SPEAKER: So what exactly is GDC? GDC stands for Google Distributed Cloud. With GDC, Google offers connected and air-gapped options to meet your specific sovereignty needs. The air-gapped option does not require connectivity to Google Cloud or the public internet in order to manage infrastructure, services, APIs, or tooling. GDC is a ready-to-use, integrated hardware and software cloud solution that does not require external connectivity. GDC can be run in nearly any data center, colocation space, or on-premises facility. GDC delivers a complete local cloud platform with integrated hardware, operations tooling and marketplace services, all designed to be deployed in a way that is completely disconnected from Google Cloud or the outside internet. All highly compliant, sensitive workloads that run on GDC are fully managed within the boundaries of on-prem environment. GDC aligns with Google Cloud's digital sovereignty vision, which is based on three pillars. Software Sovereignty. Organizations can run workloads without dependence on or lock in to the provider's software. Data sovereignty. Organizations have full control over encryption and access to data. Operational Sovereignty. Organizations have visibility and control over provider operations. This assures cloud providers that they cannot compromise customer workloads. GDC makes this digital sovereignty vision a reality. GDC also optimizes additional considerations like functionality, cost, infrastructure, consistency, and developer experience. How does GDC support the highest level of digital sovereignty? The public cloud supports commercial users and data through core controls on access, encryption, and residency. For Controlled and Unclassified Information, CUI, data sovereignty controls need to be added. For highly sensitive data, operational sovereignty controls are also needed. With these types of data, external connections to the public internet may still be supported and desired. GDC allows you to completely disconnect from external connectivity in order to support classified, secret, and top secret data. GDC allows organizations to have workload by workload level policies and controls to meet specific requirements. This is because the meaning of sovereignty may differ between countries, between industries, and sometimes even between organizations within a given industry in a given country. GDC supports cloud operations within a highly advanced, air-gapped environment that is necessary for organizations that act within stringent data residency, security, or privacy requirements. These organizations can fall under three categories. First, the government sector. For example, government agencies, departments, and entities at various levels, including federal, state, or local levels of government. Second, enterprises that require heightened information security. For example, any organization requiring sovereignty. And third, regulated enterprises. Because these enterprises carry significant public impact, regulated enterprises are subject to government oversight and compliance measures to ensure adherence to specific rules and standards. The organization category of the use case discussed in this course Cymbal Federal is a government sector. Cymbal Federal is a government agency aligned to support the broad mission objectives of an executive branch of government. So how is GDC different from Google Cloud? GDC is meant to provide a development and operational experience that is consistent with that of Google Cloud. However, it is important to note that GDC is separate from Google Cloud and not just an altered version of it. GDC is a platform built from the ground up, which uses a different code base than Google Cloud. GDC has been designed to meet specific sovereignty requirements that can be customized for both hardware and software configurations, including without internet access.

### Video - [GDC services](https://www.cloudskillsboost.google/course_templates/1199/video/521979)

- [YouTube: GDC services](https://www.youtube.com/watch?v=fMrl-TarqwU)

SPEAKER: So which services does GDC provide? GDC delivers a complete local cloud platform with integrated hardware, operations tooling, and marketplace services, all designed to be deployed in a way that is completely disconnected from Google or the outside internet. This approach enables the delivery of Google services to customers with significant sovereignty or regulatory challenges. GDC is meant to provide a development and operational experience consistent with Google Cloud. GDC can extend into virtually any data center, co-location space, or on-premises facility. GDC is operated by either the customer directly or a service provider. The environment runs a targeted subset of first-party and third-party services. You can distinguish between the following. First-party services, which are based on Google Cloud offerings and are built into GDC. Third-party services, which require integration. Within a GDC deployment, users have access to a variety of built-in services. Let's explore these in more detail. Vertex AI provides artificial intelligence and machine learning functionalities with built-in models such as translate and speech-to-text. Ad hoc model development is available in Workbench. Networking provides GDC users with the ability to set up private networks, firewalls, and network address translation, or NAT. These functionalities support organizations that may require a completely airtight environment, as well as organizations that want to provide limited access to the public internet. Storage includes block and object storage. This supports both unstructured and structured data. The Kubernetes cluster service is the core functionality of GDC, and allows you to run customer workflows as containerized applications. GDC users have access to the full spectrum of capabilities for lifecycle management, including the container registry. The database service supports both PostgreSQL and Oracle databases with a bring-your-own-license, or BYOL, format. Security ensures that only authorized users and workloads have access to specific resources. Identity access management, or IAM, with key management is crucial in defining access. This is supported by audit logging for visibility and tracing around who accessed what and when. Operations involve the logging and monitoring of resources, especially Kubernetes. Marketplace provides users with a huge catalog of independent software vendor, ISV, solutions. These solutions can help organizations accelerate their digital transformation. The virtual machine service involves the provisioning and management of highly-configurable virtual machines. These virtual machines can vary by machine type with respect to memory, CPU, and also GPUs, as well as pre-installed software images. This catalog of built-in services allows users of GDC to run any custom workload within all-encompassing operational processes. Organizations should further extend their GDC deployment with third-party service integrations that are centered around identity provider, access management, observability, domain name system, or DNS, and public key infrastructure, or PKI. Note that custom DNS and PKI integrations are a pending feature at the moment, but fundamental for GDC to function properly. Let's explore the available third-party service integrations that are required by GDC. Organizations must integrate their own identity providers to support custom token-based authentication with both security assertion markup language, or SAML, and OpenID Connect, or OIDC, protocols. Organizations should integrate their own access management solutions to ensure zero trust network access to the organization's applications, data, and services. Network access is restricted by role-based access controls, often called RBACs, which are defined for each user or user group. Organizations can access detailed telemetry data generated inside the GDC platform and ingest it in their own visualization stack. GDC uses Grafana for visualization. Organizations can integrate their DNS to turn custom domain names into IP addresses recognized by the GDC network. Finally, the public key infrastructure creates and manages public keys for data encryption of customer workloads. To summarize, when thinking about GDC, keep this sentence in mind-- GDC is self-contained and configured to meet customer requirements. All services to operate, manage, and monitor the GDC platform exist within the platform. GDC provides all infrastructure, services, APIs, and tooling through a locally managed control plane that does not require connectivity. All of the components of the GDC platform can be tailored to customer use cases.

### Video - [GDC at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1199/video/521980)

- [YouTube: GDC at Cymbal Federal](https://www.youtube.com/watch?v=siVcaQkHwLY)

SPEAKER: As a Cymbal Federal security analyst, it is important to be aware of which customer workloads are running on the platform. Cymbal Federal has three major applications to deploy to the GDC platform. Document Archive is the most critical application. It provides audio transcription to text, and archival storage in accordance with regulations required by past legislation. Mission Logistics provides inventory, ordering, and delivery scheduling of supplies to partner locations. Mission Customer Relationship Management, or CRM, maintains vetted contractors and provides services on behalf of the agency. These applications require the following GDC services to run. Kubernetes or virtual machines, or VMs, for compute. Database service for data. Key management service, or KMS, for encryption. Cross-project networking. The Document Archive application also requires long-term storage for audio file archiving. Vertex AI speech-to-text machine learning for audio transcription. These are the customer workloads that you will be securing.

### Video - [The users of GDC](https://www.cloudskillsboost.google/course_templates/1199/video/521981)

- [YouTube: The users of GDC](https://www.youtube.com/watch?v=Tf2mFi3Nkvw)

SPEAKER: Separation of duties is the principle that no individual person, role, or group should be able to execute all parts of a process. This principle is an industry best practice for workload operations, and it guides all GDC processes. Following the principle of separation of duties, three separate roles exist on GDC. Application operators and platform administrators are the end users of GDC. Application operators act at the project level. And platform administrators act at the organization level. Infrastructure operators act at the instance level to support the GDC deployment. Application operators develop applications to deploy. They execute and manage the applications from end to end, in other words, from access control to monitoring. Application operators work at the project level within Kubernetes resources and typically have full access to a given project. Application operators' responsibilities include managing projects, managing user clusters and workloads, managing and creating VMs, and maintaining applications. As project owners, application operators hold the most accountability for ensuring that security compliance controls and customer workload requirements are met. Platform administrators manage all resources, including configurations, policies, and projects. As well, they manage permissions to maintain the platform for their organization. Platform administrators work at the organization level and typically have full access to all projects within the organization. Platform administrators' responsibilities include managing organizations, creating organization projects, managing user clusters and resource quotas, and maintaining the platform. As organization owners, they hold the highest level of customer administrative privilege, and they are accountable for organization maintenance. Only platform administrators can grant access to customer data. Infrastructure operators manage the deployment and infrastructure/hardware stack of GDC. This role is the highest-level administrator of the system. By default, and for security purposes, infrastructure operators do not have access to users, organizations, and projects. However, they can be granted access when needed. Infrastructure operators are accountable for the day-to-day operations that ensure the health of the system infrastructure. One of the critical infrastructure operator responsibilities is securing the platform. This is where you enter. The Security Operations Center is a specialized unit within the Operations Center that focuses on SecOps. As a security analyst, your responsibilities will include correlating logging for security analysis and response, identifying, reporting, and remediating security vulnerabilities, defending all GDC resources, and guarding against insider threats. You will learn more about your role in the next module, the Infrastructure Operator SecOps Roles in GDC. Let's briefly highlight how the three user roles interact with each other. Application operators and platform administrators collaborate on project setup and project access management. Platform administrators collaborate with infrastructure operators to coordinate resource quotas, support, and upgrades. As a security analyst, you will collaborate with platform administrators when needed to respond to security incidents. The workflows for each of the three roles are mainly self-contained. This ensures efficiency and compliance. Each GDC user group can be onboarded on the GDC platform autonomously. The separation of duties is particularly sharp between the end-user roles-- the roles that act as the owners of customer workflows on the infrastructure-- and the support roles-- the roles that act as the owners of the infrastructure itself. An important result of this separation of duties is that infrastructure operators don't have access to customer data. They are granted access temporarily, only if access is needed to support the platform's health. Unlike end-user roles, which must belong to the customer organization as part of a cloud development team, infrastructure operations roles can be performed by an external entity. Two operating models exist for infrastructure operations on GDC. The first is the partner operating model. With this model, the customer takes ownership of the infrastructure operations, either internally or through a local, third-party provider. Google provides a GDC Bill Of Material, or a BOM, which provides all the necessary information on software tools, including which licenses are required and how to configure tools. As well, the BOM gives information on the GDC instance components and provides instructions on how to set up and operate the GDC platform. It is important to note that GDC needs to be deployed within accredited security facilities. The second operating model is the Google operating model. With this model, Google fully manages the infrastructure operations and the management team behind the platform.

### Video - [The GDC operating model](https://www.cloudskillsboost.google/course_templates/1199/video/521982)

- [YouTube: The GDC operating model](https://www.youtube.com/watch?v=TYyPqgN6O78)

SPEAKER: Cymbal Federal has chosen not to manage infrastructure operations internally. Instead, they will rely on a third-party provider so that Cymbal Federal can focus on extracting business value from GDC without management overhead. Using this operating model, Cymbal Federal only manages and secures their data and workloads. These tasks are the responsibilities of the application operators and platform administrators. It is the third-party provider's responsibility to develop or extend operations to include the Level-1 or helpdesk capability for the GDC deployment and to develop the level-2 Subject Matter Expert, or SME capability as well. Authorized Level-2 operators can create and send support tickets to Google for Level-3 or advanced platform support at the regional, national, and global levels. The ticket will be routed to the appropriate queue for a response by Google's own infrastructure operations. Cymbal Federal can rely on agreed-upon Service Level Agreements, or SLAs, for operations with both their third-party partner and Google.

### Video - [GDC architecture](https://www.cloudskillsboost.google/course_templates/1199/video/521983)

- [YouTube: GDC architecture](https://www.youtube.com/watch?v=NTycwVl-Xjw)

SPEAKER: The architecture of GDC has been designed to ensure resource isolation, autonomy, and sovereignty. Resource isolation refers to segregating and restricting computing resources to prevent unauthorized access. Autonomy refers to granting independent decision making capabilities for platform components in order to enhance self-management efficiency. Sovereignty refers to ensuring compliance by maintaining control and jurisdiction over data and operations. Keep these principles in mind as you review the architecture of GDC. GDC is a managed cloud solution that comes with hardware, software, and best practice processes built on top. GDC is deployed in customer-designated, air-gapped data centers that support at most two data center sites. GDC hardware comes fully integrated into racks that are securely delivered to the premises. Google equips the GDC platform with best-in-class enterprise equipment by leveraging partnerships with original equipment manufacturers, OEM vendors. Standard maintenance and upgrade processes exist for infrastructure operators. This ensures there is no disruption in services. Also, the architectural design of the data center complies with local regulations such as ISO 27001 and IL5 accreditation. The hardware in the GDC data center consists of compute and network equipment that allow you to run and connect to GDC processes. Beyond the baseline deployment hardware, the exact hardware definition will vary based on individual customer requirements. Customers can always scale up the deployment by simply adding additional or more performant hardware. For the compute equipment, the data center could host four HP DL360 servers, two used as bare metal servers and two used to support system applications such as Active Directory, which you may sometimes see as AD, and file server. For the network equipment, the data center could host two Cisco Nexus switches, with two Palo Alto network firewall devices for networking. The switches are used to route access to the resource. The firewall devices are used for intrusion detection and protection. The hardware and network setup are designed to maximize security. This is achieved by ensuring resource isolation and minimal access, all while maintaining fault tolerance. Without going into too much detail, keep in mind that hardware and network isolation are both physical and virtual. This dual isolation ensures maximum segmentation between and across the administrative and infrastructure planes, following regulations and best practices. The administrative plane is used by the infrastructure operators, including security analysts in the operations center, or OC. The infrastructure plan is used by customers for their workloads. The OC hosts two Cisco network switches to handle network communication with the data center. Operators can access the GDC deployments through Dell workstations that are provided as part of the OC setup. For initial GDC customers, the most likely deployment scenario involves two data centers. [? Symbol ?] Federal also uses this two data center deployment scenario. This principle of redundancy through duplication permeates the architecture design at the hardware level and ensures failure tolerance and scalability. If one component goes down, the platform can rely on the mirror component to continue working. If the platform can scale up to double the resources, processes are in place for it to scale up to any desired number of components. The GDC architecture design ensures compliance when scaling up to two or theoretically even more data centers through the OC IT set up. OC IT refers to the end-to-end integrated management platform that supports infrastructure operations and security on GDC. This includes the OC core in the data center as well as the operations center itself. Each data center hosts the hardware for GDC compute, which stays isolated within its site for compliance purposes. Communications and access are handled by the OC core network racks. Each OC core connects to the local GDC cell as well as to every nonlocal GDC cell and OC core. Following this design pattern, the same OC IT can support GDC deployments across multiple geographically dispersed locations. The GDC platform offers a multi-tenant managed software solution that is based on a three-tier architecture. Each GDC deployment has an isolated platform instance associated with it. System operations and management are run in this isolated platform. Multiple organizations can belong to the same GDC deployment. Organizations cannot share the same server, guaranteeing that hard, multi-tenancy isolation exists between organizations. Isolation is also guaranteed through sandboxes. Then, within the same organization, multiple projects can be spun. These projects can belong to different teams. This soft multi-tenancy isolation is guaranteed by the containerization provided by Kubernetes. This resource hierarchy is useful for ownership and standardization. The lowest entity level is the service resource. All GDC services belong to a service resource. These include virtual machines, databases, storage buckets, containerized workloads, or backups. Typically, service resources have projects as parents. The exception is containerized workloads, which could have clusters as parents. Everything in GDC is Kubernetes. And as a security analyst, you will find yourself handling Kubernetes logs. GDC is self-contained. All services necessary to operate, manage, and monitor the platform exist within the platform. The GDC platform is powered by the principle of autonomy and does not require day to day human intervention in order to provide standard operations and maintenance of customer workloads. In general, you can differentiate between software used by GDC end users and software used by infrastructure and security operators. You have already discovered some of the end user services offered by GDC, including Vertex AI and storage options. Later, you will learn about the software and processes used for security operations. What is the architecture design of GDC at the software level? Software is organized in planes that separate the processes for end users and infrastructure operators. End user software is organized in the infrastructure plane, while infrastructure operator software is organized in the administrative plane, also known as OIC. The infrastructure plane is further divided into planes to separate the activities of end users at an even more granular level. The infrastructure plane is further composed of a data plane, a control plane, and a management plane. These planes are defined per organization. The data plane is where application operators manage the actual Kubernetes workloads, which serve customer traffic. The control plane is where platform administrators configure data plane components following management plane configurations and changes. The management plan is where CRUD resources are defined. CRUD stands for Create, Read, Update, and Delete. The goal of the management plane is constant reconciliation so that the observed current status matches user intent. The hardware and software design of GDC guides the processes performed on the platform. Processes are isolated in different admin clusters for each user role. The system and user clusters are accessed by application operators in order to run project workloads. The Org Admin Cluster is accessed by platform administrators for logging, monitoring, identity management, cluster management, and policy management. The Root Admin Cluster is accessed by the infrastructure operators. In the Root Admin Cluster, infrastructure operators can manage the system, infrastructure as code, and the organizations. Processes on GDC are run within UI-based experiences. Application operators and platform administrators on the end user side share the same UI, but with role scoped workflows. End user processes are out of scope for the GDC security operations fundamentals series of courses. However, it's important that you familiarize yourself with end user processes over time since they're part of what you're securing. Infrastructure operators have access to a catalog of preconfigured services and user flows that are tailored to the activity they are performing. For a security analyst, this could be the Splunk SIEM UI for monitoring or the ServiceNow UI for ticket management. You will learn more about this topic in later modules as well as in course 2 and course 3 of GDC security operations fundamentals.

### Video - [GDC security guarantees](https://www.cloudskillsboost.google/course_templates/1199/video/521984)

- [YouTube: GDC security guarantees](https://www.youtube.com/watch?v=ZZEiQBFrG0M)

SPEAKER: The architecture design of GDC provides built-in security guarantees for the platform. This is important for you as a security analyst. These security guarantees apply to the following domains-- facility, hardware, node/host, network, data, Software Development Life Cycle, or SDLC, and security operations. Let's briefly look at each. GDC is deployed in customer-designated air-gapped data centers which must comply with local regulations. This means that the data center must have multiple security measures in place, such as the following. Secure perimeter defense systems to prevent unauthorized access to the data center. Comprehensive camera coverage to monitor all activity in the data center. Physical authentication to ensure that only authorized personnel can access the data center. 24/7 guard staff to patrol the data center and respond to any security incidents. Strict access and security policies to regulate how personnel access and use the data center. These security measures are necessary to protect the data stored in the data center from unauthorized access, use, disclosure, disruption, modification, or destruction. GDC hardware is deployed within secure data centers and supplied following a rigorous process. All GDC hardware and subcomponents are purchased and assembled by partners who have been vetted and certified to meet customer-specific sovereignty requirements. All personnel who handle GDC hardware are background checked. This helps to ensure that only authorized personnel have access to the hardware. Google maintains close relationships with all vendors and teams who are using the hardware. This ensures that top-of-the-line security features are provided. These security measures are necessary to protect the hardware from unauthorized access, use, disruption, modification, or destruction. The virtual nodes and hosts in GDC are Kubernetes resources, which GDC has defined as reusable and scalable standardized security processes. GDC uses a secure network multi-stage boot mechanism with a custom package certified image of Linux that meets security and compliance requirements with respect to IL5, IL6, and others. All cryptographic modules are FIPS-1402 validated. Access happens through SSH, which is managed by a custom certificate authority, or CA, in the root admin cluster with unique host keys that support key encryption and rotation. GDC provides both antivirus and anti-malware detection systems for the operating system running on the bare metal nodes, and a solution to scan the storage systems. GDC uses advanced physical and virtual network settings to ensure resource isolation and access control. These include the following. Dedicated virtual lands and Virtual Routing and Forwarding, or VFS, are used for each organization. Admin servers reside in the external domain. Workload servers reside in the internal domain. Dedicated storage VMs are used. All network traffic passes through IPS firewall with logging. All network egress is blocked by default to protect against data exfiltration. GDC protects data at rest and in transit through encryption and network isolation. GDC encrypts data by default. All GDC data stored in persistent storage is encrypted at rest using multiple levels of encryption. All GDC traffic is encrypted using standard protocols such as TLS 1.2 [? Plus, ?] HTTPS, and IPsec tunnels. Each GDC organization has its own Key Management Service, or KMS, and Hardware Security Module, or HSM. The KMS is similar to that of Google Cloud, but is built ad hoc on Kubernetes for GDC so that customers have full sovereignty over their encryption keys. The public key infrastructure in GDC is offline and supports name-constrained cross-signed certificates. All GDC software is developed in accordance with the best practices recommended by the Supply-chain Levels for Software Artifacts, or SLSA framework. This framework was developed by Google in partnership with organizations, including the Cloud Native Computing Foundation or CNCF, and the Linux Foundation. Some of the best practices introduced are-- all changes to code require a second person to review and approve them. Builds are fully automated, isolated, and hermetic. Images are scanned against vulnerabilities and are securely signed. Each release contains a Software Bill Of Materials-- SBOM-- and changelogs. Finally, security operations are your area of responsibility. SecOps facilitates a unified, comprehensive, and adaptive approach to security across the GDC platform. There are a variety of built-in processes that GDC uses to support SecOps. Least privilege identity and access management, logging and monitoring, compliance, and system upgrades or updates are of interest to all infrastructure operations. GDC also provides built-in mechanisms for security, incident response, and vulnerability mitigation. This will be of particular interest to you as a security analyst. You will explore these mechanisms in course 2 and 3 of the GDC Security Operations Fundamentals series of courses. Lastly, data can only flow through five pathways into GDC. This is one of the biggest security guarantees from GDC, and it is how GDC can offer an air-gapped environment. These data pathways are only allowed to support critical operations such as monitoring processes and system updates. Additionally, GDC can be connected or air-gapped in order to align with specific sovereignty needs. GDC offers customizable security configurations to meet diverse requirements. Let's quickly explore the processes that these pathways support. Two data pathways support monitoring processes. First, vulnerability detection signatures refer to data originating from a detected cyber vulnerability within monitored end user devices. And second, threat intelligence is system data that has been aggregated, transformed, analyzed, interpreted, or enriched for security analysis. Three data pathways support system updates. Endpoint detection and response-- EDR-- definition updates improve on the EDR solution. EDR handles the continuous monitoring of end user devices. Firewall indicator compromise, IOC, and signature updates improve the solution for network traffic analysis. This traffic looks for clues or forensics in the network data. Core updates to the GDC platform binaries also occur through frequent, well-defined releases.

### Video - [The Operations Center (OC)](https://www.cloudskillsboost.google/course_templates/1199/video/521985)

- [YouTube: The Operations Center (OC)](https://www.youtube.com/watch?v=fG8SW0V0uPI)

SPEAKER: The Operations Center, or OC, is the place at the customer site where infrastructure and security teams operate GDC. The OC is located separately from the data center and customer offices. This setup follows the principles of separation of duties and resource isolation. The OC includes the actual office space and the workstations that are used by infrastructure operators for their day-to-day work. The OC also includes local infrastructure used to connect the data center where GDC is deployed with the OC IT internet environment. The OC IT internet environment is a separate physical infrastructure that provides you with commercial internet access. The OC workstations are preconfigured with all the software needed to install, troubleshoot, and maintain a GDC instance securely. This includes monitoring, billing, SecOps, and ticketing. Services useful for all infrastructure operators include-- an authentication service to support individual user accounts on the OC workstations; audit logging for user activities on the OC workstations; an identity and access management, or IAM, solution for authenticating infrastructure operations personnel to GDC; malware antivirus services for the OC workstations and servers. As a security analyst, you will find yourself interacting on a day-to-day basis with-- a security information and event management, Splunk SIEM platform, for logging and monitoring of the GDC platform; a ticketing system, ServiceNow, to provide audit trails for changes to the environment; a suite of SecOps tooling for vulnerability management and endpoint detection and response, or EDR; an incident response plan, IRP, continuously updated with the most impactful incident response procedures. You will explore this and all other workstation tooling in greater detail in later modules. How do you get started in the OC? You also have access to documentation within your workstations. A software bill of materials is provided to all infrastructure operators. This contains a complete asset inventory. For security analysts, this includes information on-- scanning and identity vulnerabilities, analysis and triage, mitigation and remediation, and finally, communications, including escalation processes. The GDC Security Operations Fundamentals series of courses will introduce you to these security activities, but please keep in mind that you have detailed documentation to support you in your work. You can easily access the documentation within GDC from the Documentation button on the home screen.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1199/video/521986)

- [YouTube: Module review](https://www.youtube.com/watch?v=2qgXKFVkt3I)

SPEAKER: In this module, you were introduced to the GDC platform. You learned about what the platform offers, the services hosted on GDC, and the three security analyst user roles-- Tier 1, Tier 2, and Tier 3 security analysts. In this module, you also learned about the security guarantees provided by the architecture design of GDC and were briefly introduced to the operations center, OC, which is where your work as a security operator happens. Now that you have a solid understanding of these topics, let's jump to the next module, which is about the infrastructure operator SecOps roles in GDC.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1199/quizzes/521987)

## The infrastructure operator SecOps roles in GDC

This module introduces the Security Operations Center's (SOC) role within GDC. You'll learn about the SOC's core functions, operating principles, and its role in defending the GDC platform. The module will also cover Mandiant's security contributions, the responsibilities of different tiers of security analysts, and the contrast between reactive and proactive security measures. Finally, you'll examine a typical SOC organizational structure, the SOC manager's role, and how the SOC collaborates with other GDC teams.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1199/video/521988)

- [YouTube: Module overview](https://www.youtube.com/watch?v=sjkjUrmNtJk)

SPEAKER: Welcome to the Infrastructure Operator SecOps Roles in Google Distributed Cloud, GDC, Air-Gapped. In this module, you will be introduced to Security Operations Center, or SOC, functionalities in GDC. You will learn about the role of the SOC, its primary guiding principles, and what the SOC defends the GDC platform against. You will discover how Mandiant secures GDC and supports the activities of the SOC. You will explore the responsibilities of Tier 1, Tier 2, and Tier 3 security analysts. You will also discover the difference between reactive cyber event activities and proactive advanced security services. Finally, you will review a typical SOC organization structure and learn about the role of the SOC manager. You will also discover how the SOC interacts with other GDC teams.

### Video - [The Security Operations Center (SOC)](https://www.cloudskillsboost.google/course_templates/1199/video/521989)

- [YouTube: The Security Operations Center (SOC)](https://www.youtube.com/watch?v=nO37SKFiXWc)

SPEAKER: The Security Operations Center, SOC, aims to defend the GDC infrastructure through 24/7/365 monitoring of the environment. This is achieved according to SecOps best practices. The SOC is a specialized and separate component found within the Operations Center. The SOC only focuses on security operations. The SOC team is tasked with defending the entirety of the lower-level GDC platform. This includes the GDC deployment, user activities in the infrastructure plane, and infrastructure operator activities in the administrative plane. Upper-level security, which includes end-user applications on Virtual Machines, or VMs, Vertex AI, and other GDC products, is the responsibility of platform administrators and application operators themselves. All of the GDC activity and audit logging go to the SOC for 24/7/365 detection alerting and response. In accordance with the principle of separation of duties, SOC activities are totally independent of the GDC platform. Through its independent security operations, the SOC is designed to achieve the following four objectives. First, minimize business impact by swiftly identifying and responding to security incidents. Second, maintain computing services within normal operations by mitigating attacks that could adversely affect day-to-day activities. Third, reduce the risk of data loss through data protective measures ranging from encryption to access control and continuous monitoring. And fourth, enable compliance by aligning security practices with applicable regulations and standards. To achieve these objectives, the SOC follows three guiding security principles-- people, process, and technology. Skilled personnel ensure a proactive and vigilant response to threats. Well-defined processes streamline incident detection and response. Advanced technologies fortify the overall security infrastructure with state-of-the-art security services. In this module, you'll focus on people. Processes and technologies will be explored in the next two modules of this course. In accordance with the principles of people, processes, and technology, the GDC SOC adheres to a foundational SecOps architecture. This architecture is built on repeatable processes and secure configuration management that achieves the following-- puts operators on target, reduces signal noise, increases efficiency in security monitoring, and minimizes damage from unwanted effects such as operational disruptions or exposure of sensitive data. Mandiant has been a key player in the design of the SOC and is recognized worldwide as the market leader in threat intelligence and cybersecurity. Mandiant has been contributing to the definition of processes, the configuration of tools, and the resource for advice on best practice guides in the SOC. These contributions are based on the threat research performed by Mandiant. These contributions are ever-evolving, and are introduced in every incremental release. For a Google operating model, Mandiant also offers support with staffing challenges. When performing their duties, SOC personnel can rely on the best practices introduced by the GDC foundational SecOps architecture, as well as by Mandiant. At a high level, the security functions of the SOC are as follows. Correlate logging for security analysis and response. Identify, report, and track security vulnerabilities. Defend all GDC platform assets through endpoint detection and response. Identify, contain, eradicate, and recover from threats. Through ongoing proactive and reactive monitoring of the system state, the SOC effectively and efficiently secures the GDC platform. What is the SOC securing GDC against? The SOC is securing GDC against vulnerabilities, threats, exploits, attacks, and compromises. While alerts for system and hardware issues are the responsibility of general infrastructure operators, they also may be escalated to the SOC if they pose a security threat. In the next video, you will explore how this is achieved.

### Video - [Defending the GDC platform](https://www.cloudskillsboost.google/course_templates/1199/video/521990)

- [YouTube: Defending the GDC platform](https://www.youtube.com/watch?v=yObLywC5szI)

SPEAKER: The GDC platform could have a weakness or flaw in its design, implementation, or operations which represents a security vulnerability for the system. This vulnerability can be a new discovery, or may be already known internally by the SOC. If it's a new threat, it is the responsibility of the SOC to define the threat in detail, including the impact and remediation plan. Each threat has a risk associated with it that is dependent on the probability of the risk occurring and the severity of its impact. A malicious actor can also discover the vulnerability and devise a plan of attack in order to exploit the vulnerability for some illegal or undesirable behavior. The malicious actor starts an attack on the system by putting the exploit into action. The attack is the series of events intended to compromise the confidentiality, integrity, or availability of a system or data. If the attack or series of attacks is successful, then the system is compromised. How can the SOC identify the compromise in order to start incident response and remediation operations? Security analysts can rely on Locard's exchange principle-- every contact leaves behind a trace. This principle is central to forensic science. If the attacker accesses sensitive data in the system they will leave a trace in the form of increased database read volumes. If the attacker is experienced and tries to cover their tracks, you can still find a trace in anomalous gaps in logging. The more advanced the attack, the more complex it will be to find the trace. As an air-gapped environment, you can expect the attacks on GDC to be advanced and non-standard. Traditional stock network-based attacks such as phishing, commodity malware, and browser attacks are less likely to happen. Attacks are more likely to happen in the form of advanced network-based attacks and insider threats, whether intentional or unintentional, through keyloggers, data exfiltration, or physical hacking through USBs or hardware compromise. These advanced threats are the reason why SOC operations and activities need to meet the strictest compliance requirements, including DOD Impact Level 6 and top secret accreditations. Everyone in the SOC should act according to a zero-trust model. This is in order to support the highly sensitive workloads of public sector, sovereign, and regulated enterprises. The idea behind zero-trust security is simple-- don't trust anyone by default, either from inside or outside the network. Verification is required at every step. Access is restricted through IAM, following the principle of least privilege for all users, devices, network traffic, applications, and data. With a zero-trust model, users interact with resources just in time, or JIT, with just enough access, or JEA, or just enough data, or JED. As part of the SOC, you will also follow this model in order to minimize the chances of security breaches.

### Video - [Roles in the SOC](https://www.cloudskillsboost.google/course_templates/1199/video/521991)

- [YouTube: Roles in the SOC](https://www.youtube.com/watch?v=7tZDayWTgkA)

SPEAKER: Security analysts are responsible for identifying, analyzing, and mitigating security threats as part of the SOC. Security analysts manage everything, including alerting, threat intelligence, and proactive security services. In general, security analysts work in three tiers, each with specific responsibilities. The exact division of responsibilities will depend on specific customer requirements for the SOC. You may find yourself acting across tiers within your team. Tier 1 analysts are the first responders for security alerts. Tier 1 analysts are responsible for monitoring and intake. Tier 2 analysts are the owners of incident response. Tier 3 analysts can support incident response for advanced attacks, but are mainly responsible for threat modeling, vulnerability management, security engineering, and other advanced security services. The main difference between the tiers is that Tier 1 and Tier 2 analysts act reactively to security threats, while Tier 3 analysts focus on proactive defense.

### Video - [Tier 1 and Tier 2 analysts in the SOC](https://www.cloudskillsboost.google/course_templates/1199/video/521992)

- [YouTube: Tier 1 and Tier 2 analysts in the SOC](https://www.youtube.com/watch?v=iQrTBjyvXCQ)

SPEAKER: Tier 1 analysts swiftly respond and intercept emerging threats in real time. This is achieved as part of a 24/7/365 support model. As a top priority, Tier 1 analysts are responsible for the initial analysis of security events during monitoring, and the primary triage of security events during intake. These security events are reported through automatic or manual alerting systems. Tier 1 analysts are also responsible for the end-to-end incident response to lower-security cybersecurity threats. This response includes both resolution and coordination. Finally, Tier 1 analysts are responsible for providing support to Tier 2 analysts for incident response in the event of medium and high cybersecurity threats. Tier 1 analysts also ensure transparent communication across all of their activities. Support and communication happen across all tiers. To monitor against security events, Tier 1 analysts perform the following. Analyze all new alerts generated by correlation alerting tools and other alert sources, including GDC end users review SOC dashboards for anomalous activity. When intaking alerts for security events, Tier 1 analysts perform the following-- conduct preliminary investigations to identify the potential security incident; categorize security events by determining the type, severity, and relevance of compromises, if any; preserve evidence for later reference or potential legal or administrative actions. For incident response to low security threats, Tier 1 analysts perform the following-- implement predefined mitigation actions ranging from containment to eradication and recovery; monitor the effectiveness of implemented controls until system health is fully guaranteed; document all details and actions identified during the incident response inside the incident report. What about Tier 2 analysts? Tier 2 analysts conduct in-depth analysis and remediation for cybersecurity incidents. The main responsibility of Tier 2 analysts is to manage incident response. Tier 2 analysts also act as mentors and points of escalation for Tier 1 analysts. This includes upskilling for tools and processes and providing Tier 3 analysts with support if needed. Internal communication is, once again, fundamental to their activities. Tier 2 analysts perform the following day-to-day activities in regards to incident response and end-to-end management-- act as a subject matter expert for incident response activities, ensure all actions identified during the incident response process are documented, keep the incident handler informed. For example, Tier 2 analysts conduct a verbal incident handoff to a designated incident handler at the end of the business day, conduct lessons-learned meetings, develop detection logic to assist in detecting malicious activity, enhance SOC workflows, processes, and documentation for end-to-end incident response. But who is the incident handler? One of Tier 2 analysts typically acts as the cyber incident handler, also known as the cyber incident commander. The cyber incident handler coordinates and directs efforts among SOC team members throughout the incident response lifecycle. They help to determine rank and response priorities, assign responsibilities for remediation execution, determine when an incident is officially resolved and closed, lead conference calls related to the incident, starting from the initial incident notification call, provide timely and relevant updates to appropriate executive stakeholders and sponsor leadership.

### Video - [Tier 3 analysts in the SOC](https://www.cloudskillsboost.google/course_templates/1199/video/521993)

- [YouTube: Tier 3 analysts in the SOC](https://www.youtube.com/watch?v=y6dB5Zf_2mg)

SPEAKER: Tier 3 analysts provide advanced security services to proactively defend against cyber threats. The advanced security services provided by the tier 3 analysts may vary across SOCs. At minimum, the GDC SOC should have tier 3 analysts that perform threat modeling, vulnerability management, and security engineering. These security activities require a high level of expertise, and it is not uncommon for tier 3 analysts to be specialized figures. Tier 3 analysts can act as a point of escalation and mentorship to tier 2 analysts for advanced incident response as well. Tier 3 analysts include threat hunters, vulnerability engineers, and security engineers. Let's explore each of these roles. Threat hunters identify and prioritize security threats in order to inform security measures. These threats come in the form of exploitation of core services and features. Threat hunters perform the following. Plan and scope threat hunt missions to verify threat hypotheses; analyze host and network logs, malware, and code to proactively detect advanced threats; report risk analysis and threat findings to appropriate stakeholders. Manage the development of resulting new security measures to include signatures, alerts, workflows, and automation. Vulnerability engineers identify and mitigate vulnerabilities to reduce the risk of system exploitation. Vulnerability engineers perform the following. Conduct on-site weekly or monthly scanning for vulnerabilities in the environment. This includes all applications, infrastructure, and systems. Identify potential vulnerabilities and their impact. Create, assign, and manage tickets for remediation. Present vulnerability findings to system stakeholders. Continuously monitor and evaluate the effectiveness of cybersecurity safeguards. Security engineers deploy, configure, and update systems and capabilities used within GDC to provide optimized integrated security. Security engineers perform the following. Develop and maintain SecOps systems, define best practices for configuring and architecting GDC systems, support the collection and extraction of data used to refine existing and new monitoring reports, analytics, and dashboards. Support the drafting, creation, and update of reports and dashboards based on end user requirements, support the refinement and better definition of audit data being collected to reduce false positives and false negatives from alerts.

### Video - [Security and the SOC](https://www.cloudskillsboost.google/course_templates/1199/video/521994)

- [YouTube: Security and the SOC](https://www.youtube.com/watch?v=MyyGHfEFt4Q)

SPEAKER: The security analysts in the SOC are responsible for reactive incident management and proactive advanced security services. Reactive security aims to fix immediate incidents and prevent a similar or repeat attack from happening. Tier 1 analysts are the first-line defenders, performing monitoring and intake of security events. Tier 2 analysts are responsible for incident response. Proactive security aims to identify, isolate, and eliminate any threats before an attack takes place. Tier 3 analysts are responsible for advanced security services. Tier 3 analysts can be specialized threat hunters, vulnerability engineers, and security engineers. Threat modeling, vulnerability management, and security engineering are the baseline core Tier 3 activities in the GDC SOC. However, other activities, such as insider threats or ethical hacking, may also be introduced to support customer requirements.

### Video - [SOC organization structure](https://www.cloudskillsboost.google/course_templates/1199/video/521995)

- [YouTube: SOC organization structure](https://www.youtube.com/watch?v=3bedc3uuEvk)

SPEAKER: Security analysts are part of the SOC, but what is the actual SOC organization structure? Who are the other team members and teams that participate in the SOC activities? Security operations staff are typically scaled to meet the needs of the varying sized organizations which employ them. The larger the organization, the more defined and stratified the SecOps team and its functions. Conversely, a smaller organization will have a smaller team in which members may support multiple functions. The basic tier functions include intake and monitoring, incident response, and threat hunting. Let's examine the SOC organization structure for Cymbal Federal. Most of the analysts are tier 1 and tier 2 analysts for a reactive incident response. These groups can be scaled upwards or downwards in number of needed staff based on organizational size. Similarly, tier 3 may have several different people specializing in each advanced security service or just a few who may cover more than one category. Also, it is likely that in the future, the SOC for Cymbal Federal could introduce roles for insider threats. The new roles will include a senior official, an insider threat lead, and insider threat analysts. At the top of the organizational structure is the SOC manager. Let's learn more about this leading role. The SOC manager manages, supervises, and coordinates cybersecurity operations in the Security Operations Center, SOC. SOC managers perform the following-- conduct program maturity analysis using TableTop Exercises, or TTXs, to suggest enhancements; ensure that the SOC is staffed 24/7, 365 with capable leadership who can take immediate action upon notification of a cybersecurity incident; provide gap assessment analysis for tools, visibility, and capability gaps that impact the information security posture; serve as a liaison between the SOC and other business functions and technical teams; provide timely and relevant updates to appropriate executive stakeholders and sponsor leadership. To ensure continuous security operations, the SOC manager ensures that the SOC is staffed as follows. For the first shift, which needs to align to core system use and sponsor activity, typically, 9:00 to 5:00 everyone is in the SOC. The SOC manager is on call for the other shifts. The SOC manager should be contacted in cases of events categorized as critical, and he or she will be available within 12 hours. Tier 1 activities need to be available 24/7, 365, so there must be at least two tier 1 analysts in the SOC at any given time. Eight-hour work shifts limit the impact of fatigue within a single workday. Tier 2 analysts are in the SOC for second shift and are on call for the third shift for events categorized as critical. Tier 2 analysts need to maintain a 50-mile radius of the Operations Center, or OC, during the on-call phase. If needed, tier 3 analysts can be called in for specific support. The SOC is part of the OC. When performing your security duties, you will sometimes find yourself interacting with OC team members. Let's briefly introduce the other roles internal to the OC. Please note that the exact definition of personnel varies on customer setup. But these OC team members reflect a traditional OC organization. The Frontline Support Engineer, or FSE, operates the system and resolves customer issues. The Air Traffic Controller, or ATC, is a frontline engineer who directly manages and supervises real-time operations to ensure SLAs are met. The Frontline Shift Manager, or FSM, manages the entire frontline team, from professional development and performance reviews to scheduling and training. The Backline Support Engineer, or BSE, is the highest level of support staff for novel or complicated customer service requests. The Backline Manager, or BM, is responsible for the entire backline team and the successful response to customer requests. The Data Center Operations Technician, or DCOT, manages all hands-on activities in the data centers based on the physical infrastructure. The Data Center Operations Manager, or DCOM, is responsible for all data center operations in the data center team. The network engineer maintains the upper- and lower-networking components of GDC as well as the OC IT network. The Cross-Domain Solution, or CDS, engineer operates and maintains the cross-domain solution behind GDC networking. The network operations manager manages the entire network operations and network team. The OC IT engineer upgrades and maintains the OC IT infrastructure that is used by the operations team. This includes workstations and access control. The OC IT admin manages the entire OC IT engineering team. The regional operations lead oversees all operations managers, including the SOC manager, and directs the mission of operations subteams. Other roles may exist to support specialized operations needs for a customer organization. You can rely on your manager for further guidance.

### Video - [Interacting with external teams](https://www.cloudskillsboost.google/course_templates/1199/video/521996)

- [YouTube: Interacting with external teams](https://www.youtube.com/watch?v=xZR9CzEvASo)

SPEAKER: While in the SOC, you may find yourself interacting with external teams when performing technical operations, business operations, or team operations. The specific makeup of external teams will differ for each organization. Let's explore the various ways that the SOC may collaborate with external teams. External collaboration with a customer organization can happen when performing technical operations, especially incident response. Security analysts in the SOC may require the support of platform administrators on the customer side in order to get temporary access or to do a deep dive into customer activities related to the security event. External collaboration with other teams belonging to the OC provider itself can happen for business operations or for internal SOC team operations. From a business perspective, the SOC manager leads external reporting and communications with the executive team in order to analyze the impact of security incidents and the general security posture. The SOC manager also works with the public relations team when public statements are needed in the event of a significant security incident. The SOC manager may also interact with the legal team in order to receive support on legal aspects of security incidents. For team operations, everyone in the SOC will interact with human resources for training, awareness, and any personnel-related topics. Also, the SOC team can interact with internal auditors to ensure compliance. Other external teams that the SOC may interact with for business operations around incidents and general compliance include the following-- Google platform engineering teams for level 3 support through service tickets, law enforcement, regulatory bodies, insurance providers. The SOC manager is responsible for business operations and, with the support of Tier 3 analysts, for coordinating technical updates and advancements with core Google platform engineering teams. These updates and enhancements ensure platform stability, effectiveness, and system compliance. As security analysts, your focus is on technical operations. You may find yourself interacting with external teams on rare occasions, but most of the time, you will be able to rely on existing internal processes and runbooks to perform your security operations independently.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1199/video/521997)

- [YouTube: Module review](https://www.youtube.com/watch?v=rDBP_rzD-Lg)

SPEAKER: Let's quickly recap what you've learned. In this module, you were introduced to the characteristics and principles that guide the Security Operations Center, known as the SOC. You explored how SecOps architecture influences the design of GDC, as well as the role of Mandiant in ensuring security on GDC. You revisited the Tier 1, Tier 2, and Tier 3 security analyst roles, and learned about their respective responsibilities and functions in the SOC. You also took a look at other roles within the SOC organization structure, and the external teams you may encounter when performing technical operations, business operations, or team operations. Now that you have a solid understanding of these topics, you're ready to advance to the next module.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1199/quizzes/521998)

## SOC processes in Google Distributed Cloud (GDC) air-gapped

This module provides an introduction to the security operations processes protecting the Google Distributed Cloud (GDC) platform. You'll learn about the central role of logs in SOC processes, the difference between manual and automated processes, and the importance of incident response plans. The module covers the three phases of incident response, the need for proactive cybersecurity, and introduces advanced security services like threat modeling and vulnerability management. This module lays the foundation for deeper exploration in subsequent modules and courses.

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1199/video/521999)

- [YouTube: Module overview](https://www.youtube.com/watch?v=qCk8DazrWOo)

SPEAKER: Welcome to Security Operations Center Processes in Google Distributed Cloud Air-Gapped. In this module, you'll be introduced to the security operations, or SecOps, processes that defend the Google Distributed Cloud, or GDC, air-gapped platform. First, you will learn how logs are behind all processes in the Security Operations Center, which is often called SOC. As well, you will explore the difference between manual and automated processes. Then you will review security services based on logging. These services are at the backbone of all SOC processes and include the security information and event management, or SIEM, system, which supports the manual review of logging and monitoring, the security orchestration, automation, and response, or SOAR, and the endpoint detection and response, or EDR. SOAR and EDR both support automated security processes. Next, you will explore the SOC processes behind incident management and discover the importance of an incident response plan and escalation. You will learn more about the three phases of an incident-- monitoring, intake, and incident response-- and review the role of logs and dashboards. Then you will learn about the need for proactive cybersecurity. Finally, you will explore the SOC processes behind advanced security services. These include threat modeling, vulnerability management, and security engineering. Please note that you will dive further into the processes introduced here in later modules, as well as in Course 2 and Course 3 of Security Operations Fundamentals. The purpose of this module is to introduce high-level concepts.

### Video - [Logs in the GDC SOC](https://www.cloudskillsboost.google/course_templates/1199/video/522000)

- [YouTube: Logs in the GDC SOC](https://www.youtube.com/watch?v=QeWvx5Y9QMo)

SPEAKER: The SOC is where all of the GDC network, audit, security, and operational logging go for 24/7 detection, alerting, and response. All security processes in the SOC begin with the indispensable currency of cybersecurity logs. All GDC software creates logs. Logs are collected from Kubernetes pods, bare metal machines, network switches, and storage appliances. By efficiently accessing all necessary infrastructure information, security processes can effectively protect the hardware and software assets of the platform. The automated collection and transmission of logging data is referred to as telemetry, from the Greek concept denoting remote measure. At the core of GDC's telemetry is a commitment to the separation of duties, upholding compliance with two fundamental principles. First, log access indicates that each log should only be visible to one user role, and second, log cleanliness indicates that each log should not contain any sensitive data. With respect to log access, each user role has full access to logs in their Kubernetes cluster and restricted access for lower-level clusters. Application operators have full access to customer workload logs in the system and user clusters. Platform operators have full access to logs in the org admin cluster and restricted access to customer workload logs. Infrastructure operators have full access to logs in the root admin cluster and restricted access to logs from the customer clusters. Access is restricted through log cleanliness. Let's explore that further. Log cleanliness is ensured via human vigilance and automated processes. Customers should ensure sensitive information is not disclosed in the logs. For example, application operators should use generic names for customer workloads instead of meaningful IDs. Platform administrators should make sure no credentials are provided with log data. Application operators and platform administrators have access to security best practices for their workflows. Also, infrastructure operators and security analysts cannot access customer data, which is encrypted. Instead, infrastructure operators and security analysts can access NetFlow data for customer workloads. NetFlow provides visibility into all of the metadata of every traffic flow and every network transaction. This is achieved without disclosing workload data or user information. For each workload, NetFlow information allows you to determine the following-- when the workload was up and for how long, where the workload was running by means of the IP address of the physical host, between whom the workload was running by means of the source and destination IP addresses linked to the DNS record, and what type of workload was running. This information enables you to determine the frequency of that type of workload and if a workload is authorized. Observability systems in the SOC have full access to audit and security logs. Audit and security logs report on all administrative operations and actions throughout the platform. Exceptions to this are customer workloads, for which security analysts can only access NetFlow data. Operational logs provide full access to standard output, or stdout, and standard error, or stderr, for operations running in the admin cluster. Platform administrators and application operators can access operational logs for their own workloads in a separate bucket. How you define, access, transform, and visualize these three log structures within the security systems will determine how you can most effectively detect, respond to, and mitigate security events.

### Video - [Manual and automated security processes](https://www.cloudskillsboost.google/course_templates/1199/video/522001)

- [YouTube: Manual and automated security processes](https://www.youtube.com/watch?v=ac_qhnaXRfI)

SPEAKER: Security systems can support manual or automated security processes. In a SOC, automated processes provide coverage against standard use cases. Manual processes focus on nonstandard security events which require human expertise. It's important to note that manual doesn't mean disorganized. Just like automated processes, manual processes must adhere to predefined and well-structured frameworks. Support for both manual and automated processes ensures reduced alarm fatigue, increased efficiency, faster incident resolution, and a reduced risk of human error. For manual security processes, security analysts manually review and analyze logs collected within SIEM. For automated security processes, SOAR and EDR systems can automatically triage and respond to security events. SOCs may operate other systems as well. But you'll focus on SIEM, SOAR, and EDR, as they're most relevant for GDC. In the following videos, you will look at each of these three services in more detail.

### Video - [Security Information and Event Management (SIEM)](https://www.cloudskillsboost.google/course_templates/1199/video/522002)

- [YouTube: Security Information and Event Management (SIEM)](https://www.youtube.com/watch?v=MtwFVztxpyo)

SPEAKER: SIEM stands for Security Information and Event Management. The SIEM system is the central hub from which security analysts take action. Security events would be difficult to manage if you could only see alerts for individual sources. SIEM aggregates and correlates events from multiple data sources. This provides the full context necessary for you to detect a potential security incident. Through this holistic view, SIEM helps security analysts monitor, detect, investigate, and respond to security events in the SOC. To perform as a central hub, 100% of all audit and security logging needs to be available in SIEM. GDC deploys SIEM in the Operations Center, or OC, environment. SIEM then collects real-time insights on log streams based on network and application data that is generated within both the data center and the OC itself. All logs originating throughout the platform are collected within the root admin cluster. Since logs are synced from the root admin cluster to SIEM, two copies of the log data always exist. This ensures robustness. SIEM collects the log data from the complete GDC platform. SIEM then processes the raw logs into useful analytics and monitoring for security analysts. First, SIEM normalizes and aggregates the collected data into a more meaningful representation for monitoring. Data from multiple sources is transformed into a consistent and interoperable format. This lays the groundwork for actionable insights. Insights are presented within dashboards and metrics. Alerts can then be built on top of any combination of logs, dashboards, and metrics. These insights support both near real-time analysis and historical analysis. While several SecOps tools provide dashboarding, metrics, and alerting, SIEM is where these insights are consolidated across platforms. Dashboards provide a visual display of real-time log data and key performance indicators. Metrics provide quantifiable measurements used to assess and analyze the effectiveness and performance of security processes and controls. Alerts provide notifications in response to detected anomalies, potential threats, or security incidents based on some predefined rule set. The GDC SIEM includes an extensive base set of dashboards, metrics, and alerts. But operators also have the ability to add custom content. Finally, security analysts can manually analyze analytic data. This enables security analysts to discover and detect threats, as well as be notified of security events. Typically, security events are discovered via automated alerts. But it is possible for analysts to discover threats while proactively monitoring SIEM. Whether it is a manual or automatic alert, a security event in SIEM is handled manually by security analysts. Security analysts review the insights, make decisions about those insights based on hypothesis testing, and react based on the discovered incident category. You'll deep-dive into incident response processes and tools, especially SIEM, in later modules of this course. To summarize, the SIEM objectives are as follows-- enable SecOps continuous monitoring for identification and analysis of cybersecurity incidents for eradication, containment, and recovery; correlates GDC platform logging for security analysis and response; ensures SIEM platform health, coverage, and uptime; provides logging aggregation and security analysis for GDC and Operations Center IT, or OCIT, assets.

### Video - [Security Orchestration, Automation, and Response (SOAR)](https://www.cloudskillsboost.google/course_templates/1199/video/522003)

- [YouTube: Security Orchestration, Automation, and Response (SOAR)](https://www.youtube.com/watch?v=HGqCVHiZn_o)

SPEAKER: SOAR stands for Security Orchestration, Automation, and Response. The SOAR system serves as a centralized security solution designed to automatically prioritize and respond to security incidents. This capability is enabled by machine learning-driven automation and orchestration. For effective incident response automation, the SOAR system requires access to both platform logs and security feeds. Similar to a SIEM system, SOAR collects logs from compute and network devices and platform workloads. Additionally, SOAR integrates with threat intelligence feeds and incident management operations from the SIEM system, as well as with other relevant SecOps tools. With the combination of these data sources, the SOAR system can access and monitor platform events. The SOAR system can also execute playbooks and response workflows when an alert is triggered. Let's look in more detail at how the SOAR system manages incident response. Let's use the example of a malware analysis workflow. Malware analysis is the study or process of determining the functionality, origin, and potential impact of a given malware sample. This may be a virus, worm, Trojan horse, rootkit, or backdoor. The SOAR system gathers data from various sources. This includes platform logs, SIEM, and malware analysis tools. Upon receiving an alert from the SIEM system about a potentially malicious file, the SOAR system initiates the incident response process and notifies security analysts. From this point forward, security analysts can stay informed about the automated actions SOAR will be taking. Security analysts can ensure human oversight in case further intervention or analysis is required. The SOAR system uploads the flagged file to a malware analysis tool for detonation. Detonation is the controlled and secure execution of potentially malicious files or code. Detonation occurs in a restricted environment in order to analyze behavior and determine the threat level. The SOAR system reviews the file detonation report generated by the malware analysis tool and determines whether the suspected file is indeed malicious. The detonation report is a detailed document summarizing the behavior and impact of a potentially malicious file. This includes insights into its network activity, file interactions, registry changes, memory analysis, API calls, and an overall risk assessment. If the file is deemed malicious, the SOAR system updates relevant watch lists and threat databases with the new information before concluding the playbook and incident. If the file is not malicious, the playbook can be closed without further remediation. SOAR not only facilitates malware analysis but also seamlessly integrates with all workflows in the incident response playbook. A SOAR system also leverages artificial intelligence to perform the following-- enhance incident response speed, integrate seamlessly with threat intelligence, employ smart automation workflows, enable seamless collaboration, incorporate robust case management, and offer user-friendly dashboards for real-time monitoring and reporting in the SOC. To summarize, the SOAR objectives are as follows-- streamlines and automates playbook security tasks for faster incident response; integrates security, IT operations, and threat intelligence tools for comprehensive analysis; ensures consistency and repeatability of incident response processes; fosters collaboration, real-time information-sharing, and detailed documentation for improved incident analysis and compliance.

### Video - [Endpoint Detection and Response (EDR)](https://www.cloudskillsboost.google/course_templates/1199/video/522004)

- [YouTube: Endpoint Detection and Response (EDR)](https://www.youtube.com/watch?v=-X7tFST2FiQ)

SPEAKER: EDR stands for Endpoint Detection and Response. The EDR system is a local security solution that acts on individual endpoints in order to make automated security decisions. These automated decisions include sending alerts and enabling proactive responses to mitigate potential threats. EDR can then integrate with the SIEM system for reporting purposes. By running security locally on the endpoint, EDR allows for continuous monitoring in real time. This enables rapid detection of security events and a quicker response to incidents. Also, EDR reduces network latency by decentralizing log analysis. EDR enhances security oversight by supporting detection and response when endpoints are offline. So what is an endpoint? An endpoint refers to any computing device connected to the local network and used to operate, run workloads, or access the platform. An endpoint can then be the following, an infrastructure operator's workstation, a customer device, a Kubernetes container, a bare metal server, or a network switch. Endpoint agents are installed within each endpoint and run as background processes that continuously collect data with minimal system performance degradation. Information collected by the local agents includes the following, process information, file events, network connections, user activities, system events, registry activities, system events, and behavioral data. The collected data is aggregated, organized, and compressed by the agent for efficient transmission. The aggregated data is securely transmitted from the endpoint to the centralized EDR platform. This transmission occurs over encrypted channels to protect the confidentiality and integrity of the data. The EDR platform stores the data in a centralized repository and performs analysis on the stored data. If suspicious activities emerge from the analysis, the EDR platform instantly notifies security teams of the incident. EDR applies some initial automated remediation actions that help contain the incident. However, in a typical scenario, security analysts intervene to provide oversight, conduct further investigation, and complete the incident response. Let's focus on the automated processes performed on the centralized EDR platform. This will enable you to discover the analysis and response actions the EDR provides. An EDR can support a variety of analysis techniques. These can be categorized as traditional or advanced. Traditional analysis techniques involve manually defining rules, heuristics, or signatures based on the knowledge and expertise of security professionals. Traditional techniques are most commonly supported by all EDRs and include signature-based analysis and heuristic analysis. Advanced analysis techniques incorporate elements of behavioral analysis and Machine Learning, or ML. Advanced analysis can identify anomalies, patterns, or behaviors that may not be explicitly defined in rules or may be unknown. Advanced analysis techniques are necessary to effectively detect unknown threats. Advanced ML-based analysis techniques include behavioral analysis, anomaly detection, malware analysis, and more. Specialized models can also be created and supported to respond to specific security requirements. Based on these analytics, an EDR system can perform any known and automatable remediation response. Examples of common security actions performed by an EDR include the following, quarantine or isolation of affected endpoints from the network in order to limit potential damage, process termination for a malicious or suspicious activities, quarantine or isolation of affected files to prevent them from being executed, and user account lockout to prevent unauthorized access. To summarize, the EDR objectives are as follows. Continuously monitors endpoint activities to promptly identify and alert to security incidents, facilitates in-depth investigation of security incidents by providing detailed endpoint data, automates response actions for predefined security workflows, and supports forensic analysis by providing the necessary historical data to reconstruct events.

### Video - [SIEM, EDR, and SOAR at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1199/video/522005)

- [YouTube: SIEM, EDR, and SOAR at Cymbal Federal](https://www.youtube.com/watch?v=qBMmdlma0dU)

SPEAKER: Let's learn about how SIEM, EDR, and SOAR fit within GDC by returning to the Cymbal Federal use case. Cymbal Federal is deploying an installation of GDC using standard processes. GDC comes with SIEM and EDR but not with SOAR. Why? GDC is an air-gapped environment where organizations can run highly sensitive workloads. While full automation can boost efficiency, manual control over incident response ensures actions are continuously and carefully vetted. Also, in an environment where the consequences of security incidents can be severe, manual oversight is necessary to mitigate risk. GDC uses Splunk SIEM and Trellix EDR. By combining SIEM and EDR, the SOC for GDC is provided with efficient and comprehensive incident-response capabilities, all while maintaining tight oversight. SIEM centralizes and analyzes data from diverse sources. For optimized manual processes, SIEM acts as the single point of contact for security analysts. EDR focuses on endpoint-level threats, quickly identifying mitigating potential risks on individual devices. This narrower focus allows for more granular control over automated security measures. And the human-in-the-loop approach ensures the right level of oversight. The SOC visibility triad is composed of SIEM, EDR for compute devices, and EDR for Network Devices, typically referred to as NDR.

### Video - [An introduction to incident management](https://www.cloudskillsboost.google/course_templates/1199/video/522006)

- [YouTube: An introduction to incident management](https://www.youtube.com/watch?v=DMZ_UqdUDvc)

SPEAKER: Incident management is the process of identifying, responding to, mitigating, and resolving security incidents within an organization. Incident management involves a systematic approach to both handling and learning from security incidents to enhance organizational resilience. While GDC is an air-gapped environment designed to maximize physical and virtual security of its systems, the unfortunate reality is that incidents are still expected to happen. Security incidents occur for various reasons, often rooted in the ever-evolving landscape of cyberthreats. Malicious actors driven by financial gain, ideology, or espionage continually develop sophisticated techniques to exploit vulnerabilities in systems and networks. Additionally, unintended incidents can result from human error, misconfigurations, or the introduction of insecure practices. Incident management is inherently complex due to the variety of internal and external factors. Incident management on GDC is particularly complex, as the platform is intended for highly sensitive workloads. The diversity of threats makes incident management complex. The cybersecurity landscape is dynamic with a wide array of threats ranging from ransomware and denial of service to Advanced Persistent Threats, or APTs. Each type of threat requires a tailored response, adding challenges to incident management. The sheer volume and velocity at which attacks occur is a challenge for incident management, as these attacks can overwhelm traditional manual processes. Well-defined and comprehensive processes and playbooks need to be available and constantly updated for swift incident management. IT infrastructures such as GDC are particularly intricate, as they incorporate containerized services, compute and network devices, and diverse software applications. Coordinating incident response across this complex ecosystem requires specialized technology and knowledge. The selection and configuration of SecOps tools and processes need to be well-refined and tuned. Finally, GDC must adhere to various stringent regulatory requirements. Failure to manage incidents in compliance with these standards can result in legal consequences. This is in addition to the business cost of security breaches. In handling the complexities of incident management, it's important to make two considerations. One, no two incidents are ever the same. And two, all responses require people, processes, and technical elements to seamlessly work together in order to be successful. As a security analyst, you'll find yourself integrating the unique characteristics of each security event within predefined incident management processes for effective resolution. Well-defined incident management protects organizations against risk. Risks can take the form of data breaches, operational disruption, financial impact, reputational damage, and legal or regulatory consequences. Failing to address incidents adequately erodes customer trust, disrupts business operations, and incurs substantial costs. These dire consequences demonstrate the critical importance of robust security measures and incident management practices. As incidents are inevitable, organizations must invest in comprehensive incident management strategies to mitigate risks, minimize impact, protect their assets, and safeguard their reputation and the continuity of their operations. The SOC mitigates risks by identifying and assessing potential security threats. The SOC then proactively implements measures to reduce their likelihood and impact. The SOC minimizes the impact of incidents by promptly containing them through fast remediation. This helps to prevent further damage and disruption. The SOC ensures all critical assets are protected from unauthorized access, disclosure, or compromise. This includes sensitive data, intellectual property, and infrastructure. The SOC safeguards the reputation and continuity of operations of the organization by minimizing downtime and maintaining productivity while handling security incidents. The SOC can achieve these objectives by means of well-defined incident response frameworks. These frameworks enable the emergency response team and those working to be self-organized and efficient while resolving an incident. These incident response frameworks ensure the 3Cs of incident management-- Coordinated response effort; Communication, both internal and external; and Control over the incident response. When something goes wrong with incident response, the reason is likely in one of these areas. Mastering the 3Cs is essential for effective incident response. Everyone in the SOC is involved in incident management. The SOC manager is responsible for the business side of incident management. This includes strategic direction and decision-making, oversight, and coordinating within the SOC and with external teams. Tier 1 analysts and tier 2 analysts handle the technical side of incident management. Tier 1 analysts perform initial monitoring and intake. And tier 2 analysts conduct in-depth investigations and response actions. All three user roles collaborate closely in a well-structured SOC to provide optimal incident management. Tier 3 analysts may also be involved to provide advanced support in cases of highly complex incidents.

### Video - [The activities behind incident management](https://www.cloudskillsboost.google/course_templates/1199/video/522007)

- [YouTube: The activities behind incident management](https://www.youtube.com/watch?v=3XNNADxZYbA)

SPEAKER: What does incident management involve? Incident management covers the entire incident life cycle. While incident management focuses on incident response, it also includes documentation, coordination, upskilling, and system improvement. It is important for you to remember the following. Incident response is central to incident management. However, in order to properly manage an incident, you need to make sure that relevant administrative activities happen, too. Let's now introduce the activities behind incident management. Let's take the same malware analysis example from before and expand on it to see how incident management could happen for Cymbal Federal. Let's start with incident response. During monitoring, the incident response team is alerted to a security event on a device by the EDR system. The incident response team intakes the threat by determining the impact and category of the security event. In this case, malware has been identified. Predefined response procedures exist to guide most incident responses, including malware analysis. Incident response involves investigation, containment/remediation, and recovery/report. For malware analysis, the predefined response procedure could be as follows. First, isolate the infected system from the network to prevent the malware from spreading. Isolation is achieved by disabling network interfaces, or by moving the system to a segmented network and adjusting firewall rules to prevent communication with other devices. Second, analyze the malware's behavior by running it in a sandbox or controlled environment, and/or using malware analysis tools. Then, develop a strategy to eradicate the malware by means of an antivirus or anti-malware tool. Next, restore the system from a clean backup that guarantees system integrity. Finally, apply the necessary security patches and update to address the vulnerabilities exploited by the malware. End-to-end incident response ensures the system's health through continuous monitoring of its state. Let's now look at administrative activities for incident management. Let's start with documentation. Documentation is at the core of successful incident management. Documentation enables all other incident management activities, and also ensures governance and compliance. The SOC needs to create and keep detailed records of all incidents. This includes timelines, actions taken, outcomes, and lessons learned. Reporting guidelines exist for different stages of the incident response. Documentation for the malware incident provides relevant information and is populated progressively during an incident response. Incident information that is captured includes current status, high-level description, impact assessment with a final severity level associated with the incident, indicators of compromise and evidence collection including files, network logs, system snapshots, and any other relevant artifacts, actions log with all timestamped records of incident events from detection to resolution, response plan with a step-by-step breakdown of containment, remediation, and recovery actions, chain of custody, other related incidents, and contact information. Next, incident management involves coordination. Coordination refers to the effective organization, communication, and collaboration of various stakeholders, and the processes necessary to respond to a security incident. The incident handler takes an important coordinating role by ensuring that coordination efforts are synchronized, key stakeholders are up to date with the incident information, and resources are utilized efficiently throughout the incident response process. Coordination may involve both internal and external teams. The incident handler needs to provide comprehensive, transparent, and timely details to relevant stakeholders to ensure that all parties are well-informed and equipped to perform their role in incident management. Internal teams such as IT, executive management, and/or other departments, are likely to be involved in most cybersecurity incidents. External teams may need to be involved for large-scale, high-impact, or very advanced incidents. What type of coordination efforts could happen for the malware incident? Since it's a small-scale incident with a self-contained resolution process, coordination is minimal. Internal stakeholders are automatically notified of the high-level incident details, for example, through the integration of a communication channel of choice with the ServiceNow ticketing system. During remediation, the incident handler or the security analyst may need to directly engage with the affected users for support. If the malware has compromised sensitive data, the incident handler may need to collaborate with the legal team to assess any legal and regulatory considerations. If necessary, the incident handler would connect the legal team to the affected users, too. The incident handler may also collaborate with the PR team, and potentially law enforcement, to assess the need for external communication. If necessary, they would work together to craft and deliver accurate and compliant messages to external entities including clients, regulatory bodies, and the broader public. What you've seen so far are the activities that are necessary to manage an incident as it happens. Incidents also offer a chance for improving the SOC through personnel upskilling and system improvements. The incident response could highlight gaps in skills or knowledge that can be addressed in follow-up training sessions. Typical training sessions for a SOC include tabletop exercises and simulated incident response scenarios to improve the team's preparedness for future incidents. Cross-training initiatives can also be useful to ensure team members have a more holistic understanding of processes on the platform. Additionally, the SOC can learn from an incident in order to refine their strategies and fortify their defenses against evolving cyber threats. Incident response policies, security controls, and preventative measures may be revised and updated based on lessons learned from past incidents.

### Video - [The end-to-end incident response process](https://www.cloudskillsboost.google/course_templates/1199/video/522008)

- [YouTube: The end-to-end incident response process](https://www.youtube.com/watch?v=T3R9HWW89YQ)

SPEAKER: Let's now put all the activities around incident management for incident response together and review the end-to-end process. Incident response is a multistep process that involves tier 1 analysts as first responders and tier 2 analysts as experts of incident response. Tier 1 analysts monitor and intake security events. If the security event can be solved without further investigation, then tier 1 analysts proceed with containment, remediation, recovery, and reporting. If the security event requires further investigation, then tier 1 analysts escalate the incident to tier 2 analysts. Tier 2 analysts then proceed with advanced incident response. Throughout the incident response process, tier 1 analysts and tier 2 analysts can rely on the support of the incident handler to oversee, communicate, collaborate, report, support, and notify relevant stakeholders of the incident. As security analysts, most of you will find yourselves dealing with incident response as your core daily activity. In the following videos, you will learn about each phase of the incident response in further detail.

### Video - [Monitor](https://www.cloudskillsboost.google/course_templates/1199/video/522009)

- [YouTube: Monitor](https://www.youtube.com/watch?v=Mfn8R0akNt8)

SPEAKER: Monitoring is an ongoing process that happens 24/7, 365 in the SOC. Analysts know the baseline of the normal state of the platform and aim to notice and identify an anomalous or unusual cyber event as soon as it happens. The monitor phase of the incident response relies on the backbone provided by alerts, logs, and dashboards to detect cyber events in a timely manner. Notification for a cyber event can come from an automated alert or from a manual discovery. With an automated alert, a cyber event has been detected by one of the automated systems, such as SIEM or EDR. These systems send a relevant alert to the tier 1 analyst. An internal or external team may also have manually discovered an unexpected behavior by reviewing logs and dashboards. An automated alert may come from the network intrusion detection sensor or from the antivirus on an infected device. A manual discovery may come from an infrastructure operator noticing a suspicious configuration change in the system log, or a user complaining about slow service. Alerts from automated detection systems include relevant details on the cyber event. For a manual report, tier 1 analysts will likely need to manually collect the initial relevant information to populate the alert. In the monitor phase, the tier 1 analysts collect relevant but minimal details on the cyber event. Details collected include a high-level description of the detected behavior, systems and users known to be affected, what triggered the alert, and timestamps. After a cyber event has been detected, the monitor phase aims to report the cyber event as quickly as possible. This is so further analysis can take place in order to determine if it is a reportable cyber event or incident. Given the collected initial details and the following guidelines, the tier 1 analyst can assess whether the cyber event is malicious or not. It may happen that a cyber event is clearly not malicious. However, typically, the cyber event will progress to the intake phase for further case categorization. If the cyber event is not malicious, then the tier 1 analyst can directly close the event after creating the relevant documentation that is necessary for governance. If the cyber event is malicious, then the tier 1 analyst needs to create a ticket, add all cyber event details to the ticket, and assign the ticket to the appropriate owner, possibly themselves, for intake.

### Video - [Intake](https://www.cloudskillsboost.google/course_templates/1199/video/522010)

- [YouTube: Intake](https://www.youtube.com/watch?v=5fxvLWFrS2Q)

SPEAKER: The threat posed by the security event needs to be understood quickly in an effective SOC in order to minimize risk. During intake, security analysis needs to effectively evaluate and prioritize incidents based on their potential impact, severity, and urgency. The intake phase starts with reviewing the incoming cyber event data that was provided in the ticket. Tier 1 analysts carefully scrutinize log entries, alerts, or any relevant contextual information within the ticket in order to gain a comprehensive understanding of the security event and to determine which guidelines they will need to follow. The intake phase then focuses on case categorization and impact assessment. The tier 1 analyst needs to identify what type of activity is occurring and determine the associated security risk. A standardized framework exists with a predefined list of incident categories and subcategories. This is to ensure consistency and clarity in the preliminary analysis process. The intake phase categorizes the cyber incident based on known details. The tier 1 analyst can match the cyber event based on incident type, threat indicators, attack vectors, and affected systems. Please note that it is not uncommon for more than one delivery vector to be used in a compromise. The intake phase assesses the impact of the cyber event. The tier 1 analysts can base their assessments on the severity level and compromise assessment criteria, highlighted for the identified case category, as defined in the pre-established framework. This assessment considers factors such as the potential impact on critical assets, data sensitivity, and overall system functionality. This helps to determine the correct low, medium, high severity level. The impact of an incident can be both technical and operational. Technical impact focuses on risk for the technical capabilities of the organization. These include network health status, data compromise or loss, and equipment downtime or destruction. Operational impact focuses on risk for the organization's ability to perform its mission. Operational impact could be direct, such as stolen national intelligence, corrupted databases, or hard drive data lost. It can also be indirect due to the inability to deliver liver services. The tier 1 analyst has now engaged in case categorization and impact assessment analysis based on the preliminary collected cyber event details. With this information, the analyst can decide whether the cyber event is harmful or not. This preliminary analysis does not aim to fully characterize the incident. Instead, the preliminary analysis performs the necessary background investigation to determine the nature of the cyber event. If the cyber event is not malicious, the tier 1 analyst can close the incident response by documenting the triage steps. If the cyber event is malicious, the tier 1 analyst may take initial remediation actions to prevent the incident from causing further damage and to ensure incident data can be acquired for forensics analysis. This will depend on the type of security incident. The tier 1 analyst will also need to compile all incident details in an initial report. This report will be taken over by the security analyst responsible for incident response. For simpler incidents, the incident response may be handled by the tier 1 analyst. However, most often the incident response will be escalated to a tier 2 analyst.

### Video - [Escalate](https://www.cloudskillsboost.google/course_templates/1199/video/522011)

- [YouTube: Escalate](https://www.youtube.com/watch?v=8tksZgOWU5I)

SPEAKER: The process of routing a ticket to a higher level of support or a different team for resolution is called escalation. Escalation may occur for various reasons, including the complexity of the incident, the need for specific technical skills, or the requirement for managerial or executive involvement. The goal of escalation is to ensure that incidents are addressed promptly, accurately, and with the appropriate level of expertise. Security analysts can rely on the guidelines for the identified incident case category in order to define whether and how escalation should happen. In general, escalation happens via the ticket managing system which acts as a central hub for incident response management. This is to ensure a coordinated and well-orchestrated effort. GDC uses ServiceNow for ticket management. You will learn all about ServiceNow in the next module. The ticket includes at least the following information-- title/subject, ticket number, assignee, requester, reporter, start date and time, severity, status, description, and activity log. The ticket uses a standardized format. This format offers a structured way to support end-to-end escalation, including documentation, communication, tracking and accountability, prioritization, and the creation of an audit trail. Make sure to familiarize yourself with the escalation process. An effective escalation procedure is fundamental to ensuring a timely response to security incidents.

### Video - [Investigate](https://www.cloudskillsboost.google/course_templates/1199/video/522012)

- [YouTube: Investigate](https://www.youtube.com/watch?v=lSOkKv4GJIA)

SPEAKER: The investigate phase diligently analyzes the incident. During the investigate phase, the tier 2 analyst identifies all necessary actions for a comprehensive recovery and restoration of system health. The investigate phase starts by reviewing the initial incident report. The tier 2 analysts need to peer review the assessment performed during intake and familiarize themselves with the incident events that have occurred so far. This initial step involves a meticulous review of the information gathered during the earlier phases, with a focus on data validation, evidence examination, and the establishment of a clear chain of custody for collected artifacts. The tier 2 analysts will expand upon the incident report throughout their incident response as more insights are gathered and actions are taken. Through correlation and contextualization, the tier 2 analyst works to uncover patterns, motives, and relationships with other security events. To provide a holistic understanding of the incident, the tier 2 analyst leverages various tools and methodologies based on forensics analysis to delve deeper into the digital artifacts associated with the incident. Incident correlation not only helps in understanding the incident but can also assist in identifying potential connections with previously encountered security events. The investigation progresses with incident reconstruction, during which the tier 2 analyst defines the chronological sequence of events, identifies attack vectors, and gains an understanding of the threat actor's movements within the environment. This incident reconstruction enables the analyst to identify the root cause of the security incident. Once the root cause analysis is completed, the tier 2 analyst has the required understanding of the incident's patterns of activity. This enables the tier 2 analyst to direct protective and defensive strategies. Once again, predefined procedures and actions exist to guide security analysts in their response to a variety of specific threat scenarios. Generally, tier 2 analysts need to synthesize the findings from forensics analysis, incident correlation, and root-cause analysis. This enables the tier 2 analysts to pinpoint all systems, workloads, and applications that have been compromised, and how they have been compromised. Then, tier 2 analysts can apply predefined measures to recover system health. If workflows don't exist, tier 2 analysts need to research actions in collaboration with other security analysts in the SOC and receive approval from the incident handler on the response actions. As part of the investigation, the tier 2 analyst captures the methods used in the attack in order to determine system weaknesses and vulnerabilities and which security controls could have prevented or mitigated the impact of the incident. This proactive approach involves recommendations for a general strengthening of the overall security posture. This includes patching vulnerabilities, adjusting security controls, or updating networking. Additionally, the captured insights contribute to threat intelligence. The investigate phase looks at the intake incident and identifies the compromise via forensics analysis, incident correlation, and root-cause analysis. The tier 2 analyst can use predefined workflows to identify attacks associated with the compromise and to define the remediation actions necessary to restore the health of the platform. Immediate actions are collected within a response plan in the incident report and added to a ticket. Approval from the incident handler might be required for nontrivial incidents prior to continuing with the incident response. Future actions aim to improve the overall security posture of the system. This is achieved by adding recommendations and lessons learned in the incident report that the SOC can use to improve its processes and security controls. Please note that investigation is an iterative process. An investigation may be required again as new insights may appear after applying remediation actions.

### Video - [Contain and remediate](https://www.cloudskillsboost.google/course_templates/1199/video/522013)

- [YouTube: Contain and remediate](https://www.youtube.com/watch?v=dLi0sk_hWQQ)

SPEAKER: The contain and remediate phase focuses on incident resolution, adhering to established policies, procedures, and quality standards. The contain and remediate phase starts with the response plan, which was defined after the investigation. The plan strategically details remediation actions to both contain the incident and eliminate the underlying threat. The term "remediation" serves as a comprehensive umbrella encompassing the following-- containment, which involves immediate actions to curtail the incident's impact, and eradication, which focuses on permanently addressing the root cause. During containment, swift actions are taken to mitigate the immediate impact of the security incident. Containment actions are temporary measures that may involve disconnecting or isolating affected systems from the network. These actions protect other connected devices, prevent further unauthorized access by suspending user accounts associated with suspicious activities, or blacklist applications to restrict the execution of unauthorized software. The primary objective of containment is to prevent further harm from the ongoing security incident. Eradication involves identifying and eliminating the root cause of the incident to prevent it from recurring. Eradication actions are permanent actions that may involve the following-- applying patches to eliminate the specific vulnerability that allowed the incident; removing malware via antivirus and/or EDR tools; restoring affected system files and configurations; modifying access controls to adjust user permissions; and enhancing user authentication mechanisms. Eradication actions aim to fortify the organization's security posture. The tier 2 analyst is guided by the response actions outlined in the incident report after the investigation. The tier 2 analyst applies temporary containment measures as well as permanent eradication measures. New insights may appear during remediation. These may require further investigation and potentially initiate a new investigation and remediation round. Once the threat has been completely eliminated, the last phase of the incident response can commence. This involves incident closure. It's worth noting that, for certain incidents, eradication may not be necessary or may be deferred until the recovery phase.

### Video - [Recover and report](https://www.cloudskillsboost.google/course_templates/1199/video/522014)

- [YouTube: Recover and report](https://www.youtube.com/watch?v=1lOcnHcql4A)

SPEAKER: The recover and report phase aims to promote normal operations after a security incident. This is achieved by recovering the healthy state of the system and ensuring the end-to-end incident response is documented. The recover and report phase completes the incident response by bringing systems and services back to a healthy, functional state. This encompasses the goal of minimizing downtime and restoring business continuity. This is the recover part of this phase. To complete incident response management, the corresponding ticket for the incident is closed. This marks the formal conclusion of the incident response process. Recovery actions are reactive and focus on recovering and verifying system data, rebuilding systems from trusted media, and improving other services that may have been impacted during the incident. This phase is crucial for restoring business continuity. This phase involves thoroughly testing and monitoring restored systems to ensure their functionality and integrity. This is required before they are treated as fully healthy. It's important to note that achieving a perfect restoration to the previous state is rare. It is common that there is generally some data loss or unrecovered processes. Once system recovery is assured, the incident ticket can be finally closed. This closure signifies the completion of the incident response process and involves the finalization of the incident response report in order to summarize recovery outcomes. Often, a postmortem is also held to contribute to the organization's continuous improvement in cybersecurity resilience. The postmortem analysis assesses the effectiveness of incident handling, identifies areas for improvement, and guides updates to incident response procedures. This ensures a proactive and adaptive security posture in the face of evolving threats. Let's summarize what you've learned about the recover and report phase into a structured workflow. After remediation actions, the security threat has been eliminated. Tier 2 analysts can complete their incident response by implementing system recovery actions. The Tier 2 analyst should test and monitor system health for a duration sufficient to safely ensure its function. Recovery actions should be documented in the incident report, after which the incident can finally be closed. The incident is officially considered closed when the corresponding service ticket is closed. In many cases, but not always, a postmortem session can be held to compile the lessons learned. Typically, the postmortem is held after the incident response ticket is closed, and when all stakeholders are available and not engaged in higher-priority activities.

### Video - [Recap of incident response management](https://www.cloudskillsboost.google/course_templates/1199/video/522015)

- [YouTube: Recap of incident response management](https://www.youtube.com/watch?v=S6D7spaJpAE)

SPEAKER: This marks the conclusion of the exploration into incident response management. Let's briefly revisit the high-level incident response process to solidify your knowledge. The incident handler oversees administrative aspects of incident management, including collaboration and coordination. Tier 1 analysts and Tier 2 analysts actively engage in incident response at a practical level. Tier 1 analysts serve as the initial responders, handling the monitoring and intake of cyber events. Tier 2 analysts serve as incident response experts. They take charge of investigating, containing, remediating, recovering, and reporting incidents. Incident response stands as the cornerstone of SOC functionality. Like all SOC activities, incident response needs to be compliant, given the sensitivity of user workflows. Incident response also needs to be fast to minimize damage and risk. This delicate balance between compliance and speed is achieved by providing SOC analysts with predefined procedures and runbooks for incident response. In the following video, you will explore where you can access these essential guidelines.

### Video - [The incident response plan (IRP)](https://www.cloudskillsboost.google/course_templates/1199/video/522016)

- [YouTube: The incident response plan (IRP)](https://www.youtube.com/watch?v=BJVKUczC4Uo)

SPEAKER: The incident response plan is a written document that outlines all the predefined procedures and guidelines that are required to respond to a cybersecurity incident. The incident response plan ensures the efficiency and compliance of incident response actions by defining steps for end-to-end incident management, including all the people, processes, and technology involved. In the incident response plan, you will find information on scope, governance, operations, coordination, and documentation for all the subphases of incident response. This encompasses intake, investigation, containment, eradication, recovery, and reporting. The incident response plan covers a variety of possible threats to GDC. Some of the workflows you'll find in the incident response plan include compromised host, internal reconnaissance, APT process, data loss and theft, Kubernetes process, movement process, denial of service, WebShell attacks, malware analysis, targeted malware, ransomware, and more. You've already learned a bit about malware analysis. And you will learn more about other workflows in later modules as well as in course 2 and course 3 of GDC Security Operations Fundamentals. When managing a security incident, you should always rely on the incident response plan for guidance on how to observe, orient, decide, and act during incident response.

### Video - [The challenges of proactive cyber security](https://www.cloudskillsboost.google/course_templates/1199/video/522017)

- [YouTube: The challenges of proactive cyber security](https://www.youtube.com/watch?v=RsXNG6Vg2l8)

SPEAKER: So far, you've learned about the reactive efforts around incident response. But the SOC also performs advanced security activities that proactively defend the infrastructure. Advanced security services in the SOC are proactive cybersecurity activities that preemptively identify security weaknesses to continually strengthen the security posture of the platform. At its core, proactive cybersecurity is about staying one step ahead of cybercriminals. The unfortunate truth is that the landscape of cyberthreats is fast evolving. Cyberattacks are continuously increasing in number, complexity, and speed. Organizations that do not acknowledge this threat open themselves to increased security risks. With proactive cybersecurity, organizations can reduce the risk of data breaches, financial losses, and reputational damage, the same risks as seen for incident response, by defending against new attacks before they happen. Proactive cybersecurity is inherently complex due to a variety of external and internal factors. As already mentioned, the continuously changing nature of cyberthreats makes it difficult to predict and prepare for emerging attack vectors. Hundreds of new cyberattacks occur every day. In 2023, new attacks averaged to around 2,200 per day, which means that, every 39 seconds, an organization experiences a new cyberattack. Also, attacks are becoming increasingly sophisticated, as simple exploits are all too well known and well protected against. For example, malicious actors are now introducing artificial intelligence technologies into their cyberattacks. From an internal perspective, the complexity of the technology infrastructure itself makes it challenging to introduce security controls. And the more air-gapped the environment, the more complex the technology infrastructure to protect. It can be challenging to balance these two priorities. On one side, the GDC architecture design has security mechanisms built in that make the platform more resistant to attacks. On the other side, the complexity of the platform design makes it particularly difficult to perform and coordinate proactive cybersecurity actions. This is due to GDC requiring isolated systems, strict access control, and separation of duties. Finally, proactive cybersecurity demands substantial resources, both in terms of technology and skilled personnel. This can be a limiting factor for many organizations. The majority of SOCs start in a foundational place with support for reactive incident response. It can take SOCs months or years to move to a level of maturity at which they can become highly proactive with threat intelligence, artificial intelligence, and so on. For GDC, different SOCs will have different tier 3 capabilities. But the GDC SOC can always rely on Google's guarantee of advancing and supporting the GDC platform's security posture. Proactive cybersecurity tackles these challenges by defining processes and activities based on the three core objectives of early threat detection, resilience-building, and risk-reduction. First, early threat detection involves the timely identification and analysis of potential cyberthreats. This enables organizations to respond swiftly and mitigate potential damage. Second, resilience-building focuses on enhancing the overall robustness of the cybersecurity infrastructure. This ensures that systems can withstand and recover from attacks. And finally, risk reduction aims to minimize vulnerabilities and potential attack surfaces. Together, these objectives lower the overall cybersecurity risk profile of the organization.

### Video - [Advanced security services in the SOC](https://www.cloudskillsboost.google/course_templates/1199/video/522018)

- [YouTube: Advanced security services in the SOC](https://www.youtube.com/watch?v=CeVq9iDgfpw)

SPEAKER: The SOC can establish a robust cybersecurity framework that effectively safeguards digital assets and operations. This is achieved by fostering a security culture that anticipates evolving threats, strategically prepares for potential incidents, and takes preemptive actions to minimize risks. Different advanced security services can be introduced in the SOC for proactive cybersecurity. Threat modeling identifies potential cybersecurity risks and vulnerabilities in order to design and implement effective security measures. Vulnerability management identifies and addresses weaknesses in software, systems, and networks to maintain a secure IT environment. Security engineering applies principles to the design and implementation of secure systems that integrate security into the development process. Penetration testing simulates cyber attacks to find and exploit vulnerabilities. This provides insights into security weaknesses. Insider threat identification refers to security risks originating from individuals within an organization who may unintentionally or intentionally compromise security. Red teaming comprehensively tests an organization's defenses, detection, and response capabilities by simulating realistic cyber threats. Threat intelligence analysis is a research effort focused on discovering new cybersecurity threats. Security patch management proactively identifies and applies patches to software and systems, addressing known vulnerabilities and maintaining a secure IT infrastructure. Tier 3 analysts are the owners of advanced security services. Typically, a tier 3 analyst is specialized in only one advanced security service, but it can happen that tier 3 analysts perform more than one activity. Tier 1 analysts and tier 2 analysts may occasionally be involved in proactive cybersecurity in order to provide hands-on support. On the business side, the SOC manager is responsible for activity management, which includes strategic direction and decision making, coordination within the SOC and with external teams, and oversight. Tier 3 analysts rely on the leadership provided by the SOC manager in order to manage their individual and coordinated efforts to improve the overall security of the platform. Others are also involved in proactive cybersecurity efforts. Google platform engineering teams are actively working on the improvement of the platform, which includes continuous enhancement of the security posture. New releases for the GDC infrastructure happen periodically, and infrastructure operators should incorporate these updates into the platform promptly. Also, Google offers ad hoc level three support for any platform vulnerabilities discovered by the SOC. Infrastructure operators can send service tickets to the Google platform engineering team. This team will support the request according to the service level agreements, with priority defined by severity level.

### Video - [Advanced security services at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1199/video/522019)

- [YouTube: Advanced security services at Cymbal Federal](https://www.youtube.com/watch?v=EnKRjovxGrc)

SPEAKER: Let's return to Cymbal Federal and explore the types of advanced security services they may want in their SOC. It's crucial to remember that each SOC makes distinct choices in proactive cyber security, but the setup at Cymbal Federal provides a nice foundation. You've already learned about the SOC organization for Cymbal Federal in the GDC Platform Overview module. Cymbal Federal has opted to onboard a threat hunter, a vulnerability manager, and a security engineer as tier 3 analysts. Cymbal Federal has been considering introducing an insider threat function, too, but has decided to focus on proactively securing the platform against external threats first. Cymbal Federal has opted to rely on the existing strict internal processes safeguarding national security and classified information as the initial mitigation against internal threats. The choice to onboard these three tier 3 analysts has been based on the need to uphold strict governance requirements. As a government entity, Cymbal Federal must maintain extended sovereignty over the security posture of its systems. The in-house threat hunter ensures a heightened level of scrutiny and responsiveness against advanced threats. The in-house vulnerability manager ensures direct control over the identification and mitigation of vulnerabilities. The in-house security engineer further enhances the SOC's capability to tailor the security measures of the platform to specific requirements if needed. Before introducing each of these activities in more detail, it's important to define something that's common to all of them, the asset inventory. To be able to proactively protect the platform, security analysts need to know exactly which assets make up the platform. This knowledge empowers you to assess potential vulnerabilities, identify critical areas that require heightened protection, and prioritize security efforts based on the asset's significance. You will learn more about the asset inventory in the following video.

### Video - [The asset inventory](https://www.cloudskillsboost.google/course_templates/1199/video/522020)

- [YouTube: The asset inventory](https://www.youtube.com/watch?v=dWgyn_Xc2dw)

SPEAKER: The GDC platform is a complex ecosystem encompassing a myriad of critical elements, including the following-- various data artifacts, both structured and unstructured; hardware components, such as computing infrastructure and network devices; a spectrum of software applications and systems; intricate processes governing operations; and the indispensable human factor, represented by an organization's personnel. Defining the precise list of assets that belong to these categories requires a meticulous asset discovery and inventory management initiative. Analyzing the complete operational state of the platform involves assessing the infrastructure itself, as well as the interactions happening on the platform. These interactions encompass aspects like the software development life cycle and incident response plans. A deep understanding of the end-to-end platform can only be achieved by combining offline analysis based on documents with online analysis based on workshops. Relevant offline documentation includes system documentation, data flow diagrams, network diagrams, and asset inventories. Workshops can involve different combinations of stakeholders, including system operators, end users, and business stakeholders. Through this dual-analysis approach, the SOC can define all the required details for every asset. Details include unique identifiers, types, descriptions, locations, ownership, priority levels, dependencies, configuration specifics, access controls, life cycle information, security controls, vulnerability status, historical incidents, compliance details, monitoring capabilities, documentation links, contacts, and risk assessments. Last but not least, each asset is assigned a priority. The prioritization of assets is essential to allow security analysts to focus their efforts on protecting the most critical elements of the organization. The priority assigned to each asset is based on a combination of business impact, data sensitivity, regulatory compliance, dependency, strategic importance, vulnerability susceptibility, operational relevance, and historical security incidents. This prioritization is a dynamic process that may evolve based on changes in the threat landscape, business priorities, or technology landscape. These changes may require periodic reassessments and adjustments to asset priorities. By collecting all of the assets into one structured catalog, the asset inventory enables one shared place of truth for all platform components. While the ideal scenario would be a real-time document, practical considerations have led to frequent updates of the asset inventory. The SOC for Cymbal Federal, like all SOCs protecting GDC, has access to a pre-compiled asset inventory in the Bill Of Materials, or BOM. The BOM is provided by Google. This BOM is updated when new features and services are added, and updates are shared as part of new GDC releases. Each SOC needs to keep track of intended and unintended asset modifications from this baseline. The asset inventory ensures that advanced security services can perform their security activities without overlooking any assets on the platform. With this premise, the following videos will explore the three advanced security services, starting with the first tier 3 activity of your SOC, threat modeling.

### Video - [Introduction to threat modeling](https://www.cloudskillsboost.google/course_templates/1199/video/522021)

- [YouTube: Introduction to threat modeling](https://www.youtube.com/watch?v=8Vt_p98FPPE)

SPEAKER: The first tier 3 activity in the Cymbal Federal SOC is threat modeling. Threat modeling is a systematic and proactive process employed to meticulously identify vulnerabilities within a system or application, comprehensively assess associated risks, and strategically implement mitigation measures. The aim of threat modeling is to proactively counter and minimize potential security risks and threats. Threat modeling expands on the current security posture of the organization. Threat modeling systematically discovers new potential attack vectors and determines the likelihood of various new threats. Threat modeling empowers security teams to implement targeted measures that strengthen defenses and adapt to the evolving cybersecurity landscape. There are many known, new, emerging cybersecurity threats to guard against, including artificial intelligence abuse, the rise of advanced hybrid threats, and loss of privacy. Threat modeling is an ongoing and iterative process that evolves with changes in the system, applications, or organizational environment. Threat modeling starts with the identification of critical assets that could be at high risk. For each of these assets, threat hunters identify possible threats, analyze vulnerabilities, and prepare a plan of action to mitigate the threats and eradicate the vulnerabilities. After threat hunters apply the planned countermeasures and safeguards, they need to validate if the threat has been mitigated. If the threat has not been resolved, then a new cycle of analysis and remediation starts. If the threat has been resolved, then the threat hunter needs to finalize a comprehensive threat modeling report, which should have been started and carried on throughout the threat modeling process. The steps from vulnerability analysis onward closely mirror the activities and processes of incident response, from investigation to remediation and recovery. As you are familiar with the processes behind incident response, let's discover more about the first two steps, identifying critical assets and identifying threats. Let's start with critical asset identification. Threat hunters carefully study important assets to understand what might motivate potential adversaries. They assess how valuable an asset might be to someone with harmful intentions by evaluating the sensitivity of the data it holds, its impact on key business functions, its strategic importance for causing disruption or gaining a competitive edge, and its appeal to adversaries seeking opportunities to cause regulatory noncompliance. Additionally, threat hunters scrutinize the interconnectedness and dependencies of assets, identify vulnerabilities that may serve as entry points, consider operational relevance, and acknowledge the potential impact on an organization's brand reputation. By exploring these nuanced factors, threat hunters can define high-priority assets for threat identification. With a clear understanding of critical assets, the next step is defining potential threats that could compromise or harm these high-priority assets. Remember that threats can be malicious cyberattacks or unintentional incidents. Threat identification applies threat intelligence to assets in order to identify new threats. Threat intelligence information is often collected by security researchers and made accessible through various channels, including public databases, proprietary solutions, and security communications outlets. Threat hunters can curate threat intelligence feeds that are relevant to the system's technology, industry, and risk profile. Threat intelligence feeds typically provide relevant information to gain and apply critical insights into emerging threats, attack methodologies, and proactive defense strategies. This information includes Indicators of Compromise, abbreviated as IoCs, such as malicious IP addresses, domains, file hashes, and signatures. Additionally, these feeds offer insights into the Tactics, Techniques, and Procedures, or TTPs, employed by threat actors. This enables organizations to develop effective countermeasures and enhance incident response capabilities. Finally, threat intelligence feeds also provide contextual information around threats detailing the motivations and capabilities of threat actors. Combined, this threat intelligence information helps security teams prioritize and respond to threats based on their potential impact and relevance to the organization. Threat identification ends with the creation of a categorized list of identified threats. Threat hunters will begin mitigation for higher priority threats first. This categorized list will follow a structured layout that includes threat categories, clear descriptions, sources, likelihood and impact assessments, mitigation strategies, priority levels, references, and timestamps for updates. This categorization format ensures that stakeholders can efficiently understand, prioritize, and address potential risks. This fosters effective decision-making and proactive security measures in response to the evolving threat landscape.

### Video - [Introduction to vulnerability management](https://www.cloudskillsboost.google/course_templates/1199/video/522022)

- [YouTube: Introduction to vulnerability management](https://www.youtube.com/watch?v=GBpniD_Yo38)

SPEAKER: The second tier-3 activity of the symbol federal SOC is vulnerability management. Vulnerability management is the systematic process that identifies, assesses, prioritizes, and mitigates security vulnerabilities within an organization's infrastructure, systems, or applications. Vulnerability management provides reassurances about the current security posture. By regularly assessing, prioritizing, and mitigating potential risks, vulnerability management establishes a dynamic and proactive security approach. This reassures stakeholders that the organization is actively and adaptively fortifying its defenses against emerging threats. In the best-case scenario, no new vulnerabilities are found. Vulnerability management has a very similar process to threat modeling. It is also an ongoing and iterative process that evolves with changes in the system, applications, or organizational environment. Vulnerability management starts with the identification of critical assets that could be at higher risk. For each of these critical assets, vulnerability managers identify vulnerabilities, typically through system scans. If a vulnerability is discovered, the vulnerability manager needs to further analyze it. They then prepare a plan of action for mitigation and eradication and apply the planned countermeasures and safeguards. Analysis and remediation are iteratively performed until the vulnerability is fully patched, and, as always, the vulnerability manager ensures to document the end-to-end process. Let's focus once more on the first two steps of this advanced security activity-- identifying critical assets and identifying vulnerabilities. Vulnerability managers define critical assets based on their susceptibility to security vulnerabilities. This assessment requires a multifaceted analysis. Vulnerability managers review historical vulnerabilities, consider the complexity of software and system configurations, examine challenges in patch management, recognize the sensitivity of data housed in assets, scrutinize dependencies on outdated technologies, evaluate external exposure, address configuration weaknesses, and understand industry-specific threat landscapes. Now that vulnerability managers have a clear understanding of critical assets, the next step is to discover potential vulnerabilities that could compromise or harm these assets. Vulnerabilities are systematically categorized into four main groups based on their location within an information technology ecosystem. Software vulnerabilities originate in software applications and operating systems. These vulnerabilities could be programming errors, software bugs, or flaws in the code that can be exploited by malicious actors. An example of a common software vulnerability is an injection vulnerability, such as SQL injection or code injection. This occurs when untrusted data is sent to an interpreter as part of a command or query, leading to unauthorized access or manipulation of data. Hardware vulnerabilities manifest in the physical components of computing systems. This category includes weaknesses in processors, memory modules, or other hardware components that, when exploited, can compromise the security of the entire system. An example of a hardware vulnerability is the side-channel attack. A side-channel attack extracts sensitive information by exploiting information leaked through the physical implementation of a system, such as power consumption or electromagnetic radiation. Firmware vulnerabilities arise in the embedded software that controls hardware components. Firmware is the low-level code that facilitates communication between hardware and software. An example of a firmware vulnerability is the UEFI BIOS vulnerability. This vulnerability initializes a computer's hardware, known as UEFI or BIOS, and can allow attackers to execute malicious code before the operating system loads. Configuration vulnerabilities result from misconfigurations or inadequate security settings in software, hardware, or network devices. These vulnerabilities often stem from human errors during system setup or maintenance. Vulnerabilities are identified during vulnerability management through a systematic process that can involve a variety of methods and tools. These include the following-- vulnerability scanning, which compares configurations with a known vulnerability database; penetration testing for simulating cyber attacks; manual code review focusing on code; and configuration audits focusing on configurations. Additionally, continuous monitoring of vendor advisories ensures awareness of new security patches. Collaboration with security communities enhances insights into the latest strategies and vulnerability management. Automated vulnerability scanning is the recommended approach for GDC. Specialized tools are used to automatically scan and analyze a system's configuration, comparing it against an extensive database of known vulnerabilities. This database contains detailed information about vulnerabilities, including affected software versions, risk descriptions, and recommended mitigation measures. This database is regularly updated to ensure that vulnerability-scanning tools have the latest information. This enables organizations to proactively detect and address potential security risks and enhance their overall cybersecurity posture. Adversarial inputs are also commonly used to test the system for vulnerabilities. These are random or crafted inputs designed to deceive the system and elicit unexpected results. Fuzz testing involves exposing software to random inputs to uncover vulnerabilities. Network scanning identifies network weaknesses by sending packets and analyzing responses. Vulnerability scanning is done periodically following a predefined cadence. Ideally, you would aim for continuous scanning, but, in practice, a monthly scan with signatures updated within 24 hours meets compliance requirements. As always, documentation is fundamental to security processes. All identified vulnerabilities need to be documented within a comprehensive report that provides detailed information about each identified vulnerability, including its severity, potential impact on the system, affected components, and, in some cases, suggested remediation steps. The vulnerability managers can then proceed with further investigation, remediation, and recovery. This procedure is similar to incident response. It is fundamental to fix vulnerabilities in order to improve the system's confidentiality, integrity, and availability.

### Video - [Introduction to security engineering](https://www.cloudskillsboost.google/course_templates/1199/video/522023)

- [YouTube: Introduction to security engineering](https://www.youtube.com/watch?v=bdgzUVHFd3U)

SPEAKER: Finally, let's look at the third tier 3 activity for your example SOC, security engineering. Security engineering is the systematic process that designs, implements, and maintains security controls in the SOC in order to safeguard the organization's infrastructure, systems, and applications. Security controls are defined as any specific measure or countermeasure that is put in place to protect an information system, network, application, or data from security threats and risks. Security engineering plays a critical role in fortifying an organization's security posture. By managing the SOC systems, security engineers ensure the efficiency and compliance of processes. At its core, security engineering is about managing security systems. First, the security engineer needs to gather the technical and business requirements that are behind the system design. Security engineers design, implement, test, and document relevant controls in order to create a resilient and adaptable security architecture that is aligned with identified security requirements. Following implementation, security engineers need to document requirements, procedures, and protocols to ensure that other users have the right resources. Finally, security engineers continuously monitor the security systems to ensure that the system is behaving optimally. If an update is required, security engineers continuously improve the system. Security engineering aims to fortify the organization's security. The first step is to identify and prioritize security needs. This involves a comprehensive analysis of assets, potential threats, and regulatory obligations. The objective is to strategically align security measures with overarching business objectives and compliance standards. This ensures that the security strategy safeguards assets and contributes to the overall success and resilience of the business in its operational landscape. Security engineers need to strategically introduce security controls into the infrastructure and ensure these controls align with the defined security requirements. Security engineers use a defense-in-depth approach, which recommends having multiple layers of security controls throughout the infrastructure. This ensures a diverse and interconnected defense system and reduces the likelihood of cyberattacks by requiring adversaries to overcome multiple barriers and safeguards at the data, application, hardware, network, and perimeter levels. Which types of security controls can be introduced? Security controls can be categorized in various ways. A useful way to think about them is based on the function that these security controls have on threats. Preventive controls aim to stop security incidents before they occur, for example, firewalls and encryption. Detective controls aim to identify and detect incidents as they happen, for example, intrusion-detection systems and SIEM. Corrective controls aim to mitigate the impact of security incidents after they have been identified, for example, incident-response plans and system backups. Security controls are often further categorized by their type. Physical controls involve measures used to prevent or detect threats to the physical environment, for example, gates, security badges, and motion sensors. Technical controls include hardware and software mechanisms used to safeguard assets, for example, antivirus and encryption. Administrative controls refer to policies, procedures, and training necessary to manage and enforce security practices within an organization. The two classifications of function and type can be combined. This enables security engineers to create a detailed classification that they can base their control design on. For example, a detective physical security control could be CCTV and other surveillance. A corrective administrative security control could be to introduce a new process in the incident-response plan. Security engineers in the GDC SOC focus on technical security controls and deriving administrative security controls. When implementing technical and administrative security controls, security engineers need to make sure to adhere to the organization's security approach, which is an ad hoc balance between operational efficiency, user experience compliance, and the ability to adapt to the dynamic cybersecurity landscape. While it may happen that entirely new tools or processes may be introduced, the implementation of security controls typically involves updating configurations and other existing processes. Many security engineers specialize in the SIEM platform and may update dashboards and other functionalities of SIEM as part of their day-to-day activities. This is due to SIEM's central focus on SOC activities. All newly implemented security controls need to be tested. Penetration testing and vulnerability assessments may be conducted to identify and address potential weaknesses. Simulation exercises may also be run to test the effectiveness of implemented controls in realistic scenarios. Security users are the ultimate testers. A feedback loop across the SOC team exists for continuous improvement. And all security controls are continuously monitored for performance analysis. Finally, documentation is fundamental to completing all security activities. Security engineers need to make sure that the SOC can successfully handle the new security controls. Success is defined by speed, compliance, and robustness. The documentation that needs updating may range from configuration guides to security policies and incident-response procedures. Maintaining reliable, predefined security processes in the SOC fosters a culture of awareness and efficient compliance with established security measures. It also ensures that individuals across the organization understand their roles and responsibilities in maintaining a secure environment.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1199/video/522024)

- [YouTube: Module review](https://www.youtube.com/watch?v=cvsBEPBl50A)

SPEAKER: Let's briefly recap what you've learned. This module focused on advanced security operations in the SOC. You learned how logs power security processes in the GDC SOC, and you were introduced to the SecOps processes that defend the GDC. The SIEM system, which supports the manual review of logging and monitoring, and SOR and EDR, which both support automated security processes. This module also highlighted the importance of an incident response plan and escalation. And you reviewed the three phases of an incident-- monitoring, intake, and incident response. You were also introduced to the SOC processes behind advanced security services, threat modeling, vulnerability management, and security engineering. This module served as a high level introduction to these important topics. All of these concepts will be further discussed in later modules, as well as in course two and course three of Security Operations Fundamentals. Now that you have a high level understanding, let's move to the next module, SOC tools for GDC.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1199/quizzes/522025)

## SOC tools for Google Distributed Cloud (GDC) air-gapped

This module introduces the SecOps tools used by security analysts to defend the GDC platform. You'll learn about Google's alignment with the MITRE ATT&CK framework and Mandiant's support for GDC SOC tooling. You'll gain a high-level overview of SOC tooling within the GDC architecture and explore key tools like Splunk SIEM, Grafana, and ServiceNow. The module focuses on introducing high-level concepts with a deeper dive into these tools in later courses. 

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1199/video/522026)

- [YouTube: Module overview](https://www.youtube.com/watch?v=d5OdBlynCos)

SPEAKER: Welcome to SOC tools for Google Distributed Cloud GDC air-gapped. In this module, you will be introduced to the security operations SecOps tools that you, as a security analyst, use to defend the GDC platform. First, you will learn how Google aligns with the MITRE ATT&CK framework. And you will revisit the support offered by Mandiant for the GDC Security Operations Center, or SOC, tooling setup. Then you will look at SOC tooling from a high-level perspective in order to discover where the tools reside in the GDC architecture. Next, you will explore each tool in detail. Note that you will focus on Splunk security information and event management, or SIEM, as a core SOC tool; Grafana as a general infrastructure observability tool; and ServiceNow as a management tool. Please note that the purpose of this module is to introduce high-level concepts. You will dive further into the tools introduced here in course 2 and course 3 of GDC Security Operations Fundamentals.

### Video - [Tools in the SOC](https://www.cloudskillsboost.google/course_templates/1199/video/522027)

- [YouTube: Tools in the SOC](https://www.youtube.com/watch?v=S2DSv1AvHoo)

SPEAKER: The SOC in GDC uses a variety of tools to monitor, detect, respond to, mitigate, observe, and manage security threats and incidents within the platform. These tools need to support all the processes you've learned about for the SOC and enable security analysts to defend the platform as effectively and efficiently as possible. The tools selected for the GDC SOC are tailored to support the most advanced security operations while also minimizing complexity. The tools in the GDC SOC need to be self-hosted. This is due to the need to run in an air-gapped environment without online connectivity. Security analysts in the GDC SOC can rely on industry standard tools which have been chosen for their effectiveness and relevance. By aligning with industry standards, security operations in the SOC can stay at the forefront of best practices. This enhances the overall resilience and responsiveness of the GDC security infrastructure. Except for security-specific activities, the SOC tools align seamlessly with those used by the infrastructure operators in the OC. This further fosters standardization and enhances overall system robustness. Also note that a non-Google proprietary tool set was intentionally created. This design choice serves a dual purpose. First, it mitigates potential conflicts of interest. And second, it enables an impartial evaluation of GDC's security posture against vulnerabilities. Finally, all tools need to meet the strict compliance requirements that come with an air-gapped environment. The setup and configuration of the tools in the SOC align with state-of-the-art security standards. Two guarantees are particularly relevant, alignment with the MITRE ATT&CK framework and support by Mandiant. You have already learned about Mandiant's contribution to the definition of processes, the configuration of tools, and advice on best practices guiding the SOC. In the next video, you will learn about the MITRE ATT&CK framework.

### Video - [The MITRE ATT&CK framework](https://www.cloudskillsboost.google/course_templates/1199/video/522028)

- [YouTube: The MITRE ATT&CK framework](https://www.youtube.com/watch?v=eeoEBOG0s5Q)

SPEAKER: The MITRE ATT&CK framework is short for Adversarial Tactics, Techniques, and Common Knowledge. This framework is a comprehensive knowledge base that outlines the tactics and techniques commonly employed by adversaries during cyberattacks. Introduced in 2013, the framework is designed and continuously updated to provide a standardized way of understanding and categorizing cyberattacks. The framework covers a wide range of platforms and environments, making it a foundational tool for cybersecurity professionals. The MITRE ATT&CK framework is based on the work of researchers who have emulated both adversary and defender behavior for known threats in the real world, with the aim of evaluating and improving post-compromise detection of threats using telemetry sensing and behavioral analysis. The behavioral model presented by ATT&CK follows the adversary's attack throughout the life cycle, categorizes adversary actions, and specifies related defense techniques. The framework follows a matrix structure in which each column is a short-term tactical adversary objective during an attack. The adversary objectives, called tactics, are presented in a linear format, from reconnaissance until impact. Each cell in a column represents one of the techniques by which adversaries achieve the corresponding tactical goal. Some techniques have subtechniques, which explain how an adversary carries out a specific technique in greater detail. Techniques and subtechniques are linked to documents called procedures. These documents provide detailed instructions on the adversary approach and other metadata. For example, one common adversary technique during reconnaissance is active scanning. This can be implemented through three subtechniques-- scanning IP blocks, vulnerability scanning, and wordlist scanning. MITRE ATT&CK has three different framework implementations or matrices, each focused on a specific domain across enterprise, mobile, and Industrial Control System, also called an ICS. GDC follows the ATT&CK for enterprise matrix, which focuses on adversarial behavior in Windows, Mac, Linux, and cloud environments. You can see here what the MITRE ATT&CK for enterprise frameworks with subtechniques hidden looks like. The set of techniques that are covered is extremely comprehensive and vendor-neutral. This forces security vendors to continuously and extensively improve and test their security controls through a proactive effort. To adhere to the MITRE ATT&CK framework, GDC has gone through the process of defining exactly which techniques apply across the almost 300 techniques presented in the framework. This prioritization was based on a balance between the difficulty of exploitation and the risk associated with the exploitation. The framework-recommended detection and remediation techniques are supported by the SOC tools and processes for GDC. Alerts in the GDC SOC come with a MITRE technique number and their name. During intake, you can define the threat type directly from the alert name. An incident response plan built on best practices is associated with that MITRE technique number and can be used for remediation. The design of the prepackaged content in Splunk SIEM also follows recommendations for the MITRE attack technique in order to streamline incident response.

### Video - [GDC platform security](https://www.cloudskillsboost.google/course_templates/1199/video/522029)

- [YouTube: GDC platform security](https://www.youtube.com/watch?v=KuuF7no6vfw)

SPEAKER: The SOC tools for GDC have been designed and pre-configured following state-of-the-art industry standards. These tools are subject to continuous monitoring and improvement by the Google platform engineering team. This gives GDC customers, and you as a security analyst, the guarantee that GDC is always at the forefront of cybersecurity. The toolset for the SOC enables security analysts to make the best security decisions for the GDC platform. Security analysts need to be able to continuously observe and monitor the platform. This includes looking for anomalies, potential threats, and suspicious activities within the network. Then, security analysts need to be able to orient themselves by contextualizing the observed data within the organization's architecture and business model. Next, security analysts need to be able to make decisions on security events in order to determine their severity, prioritize responses, and determine appropriate actions to investigate or remediate potential threats. Finally, analysts need to be able to take decisive action by implementing security countermeasures and controls that fortify the security posture of the platform. The SOC tooling needs to support the observability of the platform, then the analysis and decision-making to enable security actions, and finally, the management of end-to-end security processes.

### Video - [Categories of tools for the GDC SOC](https://www.cloudskillsboost.google/course_templates/1199/video/522030)

- [YouTube: Categories of tools for the GDC SOC](https://www.youtube.com/watch?v=F6-1ZOQ9XeQ)

SPEAKER: The SOC tools for GDC can be broadly divided into three categories. Core SOC tools-- these encompass foundational security components that are focused on threat detection and incident response, such as SIEM, Security Information and Event Management, vulnerability management, endpoint detection, and network protection systems. Observability SOC tools-- these include log management, metrics collection, and visualization solutions. These tools provide insights into system behavior, performance, and security events, enhancing visibility for proactive monitoring and analysis. Management SOC tools-- these involve platforms for case management, secure configuration management, and orchestration. These tools streamline administrative tasks, collaboration, and the overall governance of security operations. Later in this module, you will be briefly introduced to each of these tools. You will learn in some more detail about the three foundational tools for each category-- Splunk SIEM, Grafana for observability, and ServiceNow for incident case management. Before moving to each tool, you may have wondered, where are these tools actually deployed in order to ensure safety and compliance? This is a particularly relevant point for GDC, given that it is an air-gapped environment that needs to meet the highest security standards. To answer this question, let's refer once more to the principle of separation of duties and look at the users of these tools. Security analysts only have access to security-specific core and management tools. Splunk, Portswigger Burp, Tenable Nessus, and Trellix. All other tools except ServiceNow are shared across roles, from security analysts to all other infrastructure operators. As mentioned at the beginning of the module, this allows for robust and standardized processes that minimize security risks by reducing the number of attack points. In this picture, ServiceNow holds an unusual spot, since it needs to be available, not only to all infrastructure operators, but to end users too. As the central incident management platform, ServiceNow needs to provide a bridge between customers and operations so that customers can submit a support case to IOs and track case progress over time. As such, ServiceNow is part of the DeMilitarized Zone, the DMZ, of GDC, which is further protected by strict network policies and firewalls. The security team has access to an encrypted separate enclave called SIR to keep confidential details protected. Additionally, Role-Based Access Control, or RBAC, federated access management, ensures that only the relevant subset of functionalities of ServiceNow are accessible per user role. Role-based access management is used for all tools. Let's put all the information you've just seen into an architecture diagram. Most of the tools used by the SOC are hosted on the admin OIC plane, since they only need to be available to infrastructure operators. All tool agents running on the admin plane use Windows servers. However, the GDC plane is based on Linux servers. The GDC plane hosts ServiceNow in the demilitarized zone, which is accessible to all users. The Tenable Security Center is also hosted on the GDC plane, so that it can connect to its agents with minimal firewall complications. Finally, GitLab is hosted in the root admin cluster so that the infrastructure scripts have the right access to act on resources.

### Video - [Core tools in the GDC SOC: Splunk SIEM](https://www.cloudskillsboost.google/course_templates/1199/video/522031)

- [YouTube: Core tools in the GDC SOC: Splunk SIEM](https://www.youtube.com/watch?v=xRckB0nSEk0)

SPEAKER: Splunk is an industry-leading SIEM, providing a comprehensive platform designed to centralize, analyze, and visualize security data within a user-friendly interface. Splunk serves as a unified hub for aggregating and correlating all logs generated throughout the GDC platform. This includes both operation center and data center logs for GDC. These security streams are accessed within customizable dashboards that offer security analysts actionable insights in the form of aggregated logs, metrics, and plots. Whether through manual inspection or proactive alerting, the Splunk platform enables security analysts to detect security events in a timely manner. Firstly, Splunk needs to collect data from all the GDC machines. Splunk forwarders are installed on each machine and transmit data to the centralized Splunk instance through Windows Event Logs and Linux Messages. The configuration of these forwarders is managed centrally, and then the deployment server ensures that the settings are consistent across machines. The data transmitted by the forwarders is received, indexed, and stored by indexers. This makes the data searchable and available for analysis using the Splunk search and query functionalities. To efficiently support monitoring and analysis, Splunk provides smart ways to manage logs that enable an intuitive understanding of the security data. Tags are user-defined labels applied to fields, which make it easier to categorize, filter, and group data during searches. For example, you could tag events based on the source or the severity. Event types are similar to tags but applied to events. For example, you could define events as "failed login" or "network anomaly." Field extractions defined rules for parsing and extracting specific fields from raw data. For example, you could extract an IP address or an error code from your logs. Macros are reusable blocks of search query or processing logic that allow users to rerun queries without restarting every time. For example, you could have an error count or a time range macro. Lookups are tables of data that can be referenced during searches to provide added context. For example, you could define a user department or a product version lookup table. All these functionalities help in creating actionable insights. Security analysts can use the insights provided within Splunk for both proactive and reactive cybersecurity measures. This makes Splunk a versatile tool for enhancing your overall security posture. Security analysts use Splunk to seamlessly identify anomalies and potential threats in real time by looking for anomalies with the support of behavioral analysis. Splunk is particularly focused on incident management. The Splunk platform facilitates in-depth forensic analysis, enabling analysts to investigate events thoroughly. With Splunk serving as a centralized security hub, analysts can collaborate and coordinate seamlessly through an intuitive interface that allows teams to interact with and visualize the same data. Splunk administrators can centrally design and manage standardized dashboards that align with the organization's requirements. Access can be managed through RBAC. Individuals can customize their Splunk environment, but it is not recommended for the GDC SOC. This is in order to maintain a unified security experience and prevent divergent configurations. Team members can collaborate in the following ways-- through shared alerts and notifications, and by exporting saved searches, scheduled reports, and data.

### Video - [Core tools in the GDC SOC: Tenable Nessus](https://www.cloudskillsboost.google/course_templates/1199/video/522032)

- [YouTube: Core tools in the GDC SOC: Tenable Nessus](https://www.youtube.com/watch?v=OziZrGT6G2U)

SPEAKER: Nessus is a comprehensive remote vulnerability scanning tool offered by Tenable. This tool provides complete visibility into the security posture of distributed and complex IT infrastructures. Nessus sends packets of data to each port of all devices, applications, operating systems, cloud services, and other network resources that compromise the entirety of the Operation Center's IT, or OC IT, GDC assets. Nessus determines what service the port is running and then tests the service by analyzing its response to make sure there are no vulnerabilities that could be used by a hacker to carry out a malicious attack. The results of the scan are collected in a comprehensive report. This report includes details on identified vulnerabilities, their severity, and recommended remediation steps. Nessus maintains an extensive database of vulnerabilities, which is regularly updated to include new vulnerabilities as they are discovered. Nessus provides three categories of scanning capabilities that can be customized and have been customized for the GDC environment. Discovery scans tell you which hosts are active on the network. The information in this scan is useful to help you choose which hosts to target in a vulnerability scan. The discovery scan is also useful if you want to double check any missing hosts in a vulnerability scan report. Vulnerability scans are the core of the offering. They allow you to comprehensively scan the environment for a specific vulnerability or a group of vulnerabilities. Compliance scans, also known as configuration scans, check whether host configurations are compliant with various industry standards. These scans are based on the Security Content Automation Protocol, or SCAP, open standard, which collects policies and standards from a variety of sources, including OVAL, FDCC, and CVE, and makes them available as ZIP files. Let's briefly look at the scan templates offered within the vulnerability category. This comprehensive set of templates provides generalized and specialized scans. Templates are frequently updated to detect the latest vulnerabilities of public interest. The [? Solar ?] [? Gate ?] and Log4Shell scans are two examples of specialized scans aimed at detecting a specific vulnerability. For a generalized scan, you can use the basic network scan. This scan searches for vulnerabilities in all of Nessus's enabled plugins by using settings recommended by Nessus. You can also perform a generalized scan with customized settings via an advanced scan or an advanced dynamic scan. Let's explore an example scan report by imagining that you, as a Cymbal Federal tier 3 analyst, have run a basic network scan. From the report, you can access a detailed list of hosts and vulnerabilities discovered for each. Vulnerabilities are color coded by severity, and you can choose the severity definition. In this case, severity has been based on CVSS v3.0. From this high-level security overview, you can zoom into each of the discovered vulnerabilities in order to learn general information like name, family, occurrence count, description, solutions, if any are known, details of the plugin used to discover the vulnerability, and access other reference links. You will want to resolve vulnerabilities with a higher Vulnerability Priority Rating, or VPR. This is calculated from the vulnerability's CVSS scores. Nessus provides you with an overview of the VPR top threats identified in the scan to further support you in your vulnerability management decision making, and action planning.

### Video - [Core tools in the GDC SOC: Portswigger Burp](https://www.cloudskillsboost.google/course_templates/1199/video/522033)

- [YouTube: Core tools in the GDC SOC: Portswigger Burp](https://www.youtube.com/watch?v=v42NWjbIFaE)

SPEAKER: There is another vulnerability management tool, Burp. Burp is a suite of tools offered by Portswigger. Burp allows users to test for vulnerabilities in web applications. The suite is composed of two main tools. Burp Suite Enterprise Edition is a web-based UI application that allows for automated scanning at any scale and integration with software development processes. Burp Suite Professional is a penetration-testing toolkit that allows security analysts to manually perform faster and more reliable testing for individual applications. Burp Suite Enterprise Edition uses Burp Scanner's advanced web-scanning logic to automatically uncover dozens of different types of vulnerabilities. Burp Suite Enterprise uses centralized management and reporting features to facilitate efficient collaboration among security teams. This streamlines the process of securing complex web environments. Predefined or custom scan modes define the exact steps performed in the scanning as a collection of scan configurations that are applied at three levels of hierarchy-- folders, subfolders, and sites. The lowest level of configuration always takes precedence. For each scan mode, you can specify the scope of the scanning, advanced testing capabilities, and crawling options. For example, you can specify how Burp should handle application errors that arise during crawling. Let's explore the Burp Suite Enterprise Edition dashboard that you, as a Cymbal Federal tier 3 analyst, may access as part of your vulnerability management duties. The first thing to note is a little prerequisite. The Burp licensing is such that each user needs to activate and use their own license. This means that you must set up your license before being able to access the tool. The dashboard provides you with an intuitive overview of issues for your web applications, once again, color-coded by severity. For example, you can easily recognize which websites are most vulnerable. Then you may want to prioritize vulnerability management if these websites are critical for workflows. You can also keep track of new and resolved issues over time. Using the Issue tab, you can further explore each issue with an overview of details, location, remediation suggestions, and reference links. You can also always review the request response table that has caused the vulnerability to be detected. Burp Suite Enterprise Professional enables security analysts to perform in-depth analysis of web-application vulnerabilities. Security analysts can use expert-designed manuals and semi-automated security testing tools to optimize their workflow when troubleshooting vulnerabilities. Burp Suite Enterprise Professional comes with a variety of functionalities to support different penetration testing flows. Some of the Enterprise Professional functionalities include the following. Burp Intruder is used to customize and launch a variety of attack types. Burp Repeater is used to manipulate and resend individual HTTP requests for targeted testing. Content Discovery is used to uncover the locations of unlinked functionalities and content within an application. Everything you do is saved while on an engagement in order to continue or review at any time. Let's explore a report from Burp Suite Professional that you, as a Cymbal Federal tier 3 analyst, may have generated as part of your vulnerability management duties. The sitemap of Burp Suite Professional displays the information that Burp collects as you explore your target application. The tool builds a hierarchical representation of content from a number of sources and displays it as a tree view. When you click on a source URL, you can review the associated list of relevant contents, pull requests and responses, and complete information about discovered security issues. You can further filter and annotate this information to help you manage it. For any selected issue, you can report it, set its severity, set its confidence, or delete it. The Burp Scanner's reporting wizard automatically generates a formal report whenever you report an issue. Portswigger Burp Suite enables you, as a security analyst, to manage vulnerabilities in web applications while minimizing manual and tedious tasks.

### Video - [Core Tools in the GDC SOC: Trelix](https://www.cloudskillsboost.google/course_templates/1199/video/522034)

- [YouTube: Core Tools in the GDC SOC: Trelix](https://www.youtube.com/watch?v=cUlDMLhEItc)

SPEAKER: Trellix is a relatively new cybersecurity company that was launched in 2022 after Symphony Technology Group acquired McAfee Enterprise and FireEye in 2021. Trellix provides hardware, software, and services to investigate cybersecurity attacks, protect against malicious software, and analyze IT security risks. One of the specialized cybersecurity products that the company provides is for endpoint security. GDC has selected Trellix HX and Trellix ENS as the preferred choice for end-to-end, integrated endpoint security. Trellix Endpoint Security with HX and ENS provides an extended Endpoint Detection and Response, or EDR, system that detects hidden malware and irregular activities and prevents threats in compliance with the requirements of an air-gapped environment, such as GDC. The tool analyzes live memory without downloading images to discover. This is fundamental given no connectivity to the internet. The tool enables remote investigation without requiring access authorization. This is important when physical access to endpoints is restricted. The tool works for both clear and encrypted traffic. This supports NetFlow data for customer workloads. The tool provides protection for both Windows and Linux machines. Let's look at each service in more detail. Trellix HX implements a defense-in-depth approach powered by a signature-based engine that supports antivirus signatures, antispyware signatures, and vulnerability signatures. Everything in HX is defined as a module. System modules, fundamental for triage settings, alert configurations, and storytime functionalities are predetermined and cannot be disabled. They play a crucial role in managing processes, including the auto-triage package, agent health, and module administration. The default engine, comprised of these system modules, can be expanded through downloadable modules. These additional modules offer advanced functionalities, such as malware protection, ExploitGuard for behavioral analysis, or host containment. Each endpoint has an HX agent installed. The HX agent collects relevant, real-time security data as defined by the specifications of the installed modules. For example, the agents could collect unauthorized use of valid accounts, suspicious network traffic, or unauthorized file access. Evidence of potential compromise is reported to the endpoint security server where two processes start simultaneously. An alert is sent to security analysts, who can use the HX server to collect further data and triage the security event. Relevant remediation actions could be automatically performed, such as blocking traffic to high-risk and malicious IP addresses and URLs or upgrading indicators of compromise. Like Trellix HX, Trellix ENS also protects endpoints from malware and other suspicious activity. However, Trellix ENS does so by focusing on machine learning in order to provide advanced defenses against advanced threats. The machine learning behavior classification powering ENS Real Protect detects zero-day threats in near real time. This enables actionable threat intelligence. Zero-day threats are attacks that take advantage of security vulnerabilities that do not have a fix in place. The ENS machine-learning model automatically evolves behavior classifications to identify unknown behaviors and adds rules to identify future attacks. Trellix ENS follows the same deployment design as HX. We've already introduced the Real Protect offering of Trellix ENS. Other modules of interest include the following. The threat prevention module safeguards systems by preventing threats from accessing them, automatically scanning files upon access and conducting targeted scans for malware on client systems. The adaptive threat protection module analyzes content from your enterprise and decides how to respond based on file reputation, rules, and reputation thresholds. The firewall module monitors communication from the computer and resources on the network and the internet. The module then intercepts suspicious communications. The web control monitors web searching and browsing activity on client systems and blocks websites and downloads based on safety ratings and content. Please note that for ENS functionalities that overlap with HX, The module only needs to be activated once. Examples include anti-malware and exploit protection. Let's review an example compromise for Cymbal Federal. Once a compromise is detected, you receive an alert from Trellix HX or ENS with various information. On the monitoring page of the Trellix server, you can access all the threats associated with the compromise ranked by severity. This compromise has four threats associated with it. And you can see exactly which process triggered the alert, when, and the devices it affected. Trellix details the threat behavior for you by dividing it into techniques observed and suspicious indicators. This aligns with the MITRE ATT&CK matrix and provides a timeline of the process activity too. This is all the information necessary to intake a security event, all in one integrated dashboard. When following up with the investigation, Trellix automatically identifies key findings without requiring the manual evaluation of each individual artifact. Trellix also provides answers to commonly asked questions related to these types of attacks. Analysts can then collaborate to investigate the incident using the relevant endpoint information that was automatically collected and processed following the expert processes offered by Trellix.

### Video - [Core tools in the GDC SOC: Microsoft Defender Antivirus and ClamAV](https://www.cloudskillsboost.google/course_templates/1199/video/522035)

- [YouTube: Core tools in the GDC SOC: Microsoft Defender Antivirus and ClamAV](https://www.youtube.com/watch?v=v7g6cEpDNlU)

SPEAKER: Besides EDR, endpoints are also protected more traditionally with anti-malware software. Microsoft Defender Antivirus and ClamAV are two of the anti-malware solutions chosen to protect all GDC machines. Specifically, they respectively protect Windows and Linux machines. These two tools have been selected, since they both support the necessary antivirus functionalities, to protect an air-gapped environment, such as GDC, since the tools do the following-- provide audit logs of when scans are run and provide reports of scan results. Trigger alerts both when malware is detected and when scanning fails. These two tools are owned by infrastructure operators, who are responsible for uploading newly-released signature at most every seven days to stay compliant. Let's briefly look at Microsoft Defender Antivirus. Microsoft Defender Antivirus, also referred to as Microsoft Defender AV, provides a range of security features to protect Windows-based systems from various threats in real time, including viruses, malware, and other malicious activities. In GDC, operations center IT, OCIT workstations are Windows-based machines. And all are protected by Microsoft Defender Antivirus. Microsoft Defender Antivirus offers malware protection by applying a combination of static and dynamic signatures, with signatures manually updated by infrastructure operators. Static signatures match specific byte sequences or attributes associated with known malware. Static signatures are the most effective solution against well-established threats with identifiable patterns. Dynamic signatures analyze the actions or behaviors of programs to identify potential threats. Dynamic signatures can adapt to new or evolving malware that may not have a fixed pattern. Microsoft Defender AV protects the OCIT workstations from both known and novel cyber threats. ClamAV is an open source antivirus toolkit offered by Cisco. ClamAV can detect millions of viruses, worms, trojans, and other malware. The catalog of supported threats is updated daily through a community-driven effort. And infrastructure operators can use a command line scanner to update the virus database regularly. While ClamAV can protect windows machines too, it is not used for OCIT Windows workstations in GDC, since ClamAV only supports real-time protection for Linux. ClamAV defends all bare metal servers in the data centers. Additionally, a ClamAV sidecar provides scanning for a persistent storage that does not reside on bare metal machines, for example on NetApp. ClamAV operates through a multi-threaded daemon, which allows for customized detection. For GDC, you have two flows, a daily scan. The clamd scan daemon collects and submits files for scanning. Then, clamd loads the virus database and performs the scan. You can set up a cron job to automatically and possibly randomly start this process. A real-time scan. The clamonacc daemon submits active files on access in real time for clamd to scan. Finally, the fresh clam periodically updates the virus database. Let's return to Cymbal Federal to discover where you can access insights from Palo Alto firewalls in GDC. As a shared tool for all infrastructure operators, insights from anti-malware software are primarily accessed within the Grafana dashboard. ClamAV and Microsoft Defender automatically send alerts to Grafana. Grafana provides the information needed to easily identify which files are infected on which host and with which malware. You can then correlate logs from the anti-malware systems and from the endpoints themselves for further investigation. Please note that a new tool named Trellix ePolicy Orchestrator, EPO, will be introduced soon for end-to-end endpoint management. Trellix EPO provides a centralized security platform for end-to-end management of endpoints. The platform monitors and manages your network, collects data on events and alerts, creates reports, and automates workflows to streamline product deployments, patch installations, and security updates. With more than 150 third-party integrations, EPOs will also provide an anti-malware solution, which means that ClamAV and Microsoft Defender AV will soon be decommissioned.

### Video - [Core tools in the GDC SOC: Palo Alto](https://www.cloudskillsboost.google/course_templates/1199/video/522036)

- [YouTube: Core tools in the GDC SOC: Palo Alto](https://www.youtube.com/watch?v=XcZLjlIlaYk)

SPEAKER: One final core tool is Palo Alto. Palo Alto equips the GDC infrastructure with hardware based firewall form factors for network protection. These firewalls are physical devices that control inbound and outbound traffic to the GDC network. This traffic comes from Operations Center, OC, and customer devices. Palo Alto firewalls act as intrusion detection and prevention system. These firewalls not only filter traffic through policies and rule controls, but also examine inbound and outbound traffic for malicious activity. Palo Alto also uses an intrusion detection and prevention system that combines the IDS, Intrusion Detection, and IPS, Intrusion Prevention Systems. IDS is a passive intrusion detection system that scans traffic and reports back on threats. IPS is an online intrusion prevention system that takes automated actions when necessary. The Palo Alto Intrusion Detection and Prevention System, IDPS firewalls, act on allow but scan rules. For any traffic that matches the allow rule, the firewall automatically applies attached security profiles for detection and prevention. Security profiles that are commonly applied together are typically combined with a security profile group. Security profiles provide comprehensive coverage of typical network protection activities ranging from anti-virus, anti-spyware, and vulnerability protection to filtering of specific URLs and data patterns or blocking of specific file types. Advanced security profile functionalities include the following-- wildfire analysis that enables forwarding unknown files for analysis, denial of service or DoS protection to address flood and resource exhaustion attacks, zone protection to prevent issues arising from excessive traffic. Each security profile will have specific prevention actions associated with it. Common actions include allowing, alerting, dropping, IP blocking, or resetting for either or both client and server connections. Let's take a moment and revisit your role as a security analyst in the GDC SOC of Cymbal Federal. How do you get notified of any detected network intrusions? As a shared tool for all infrastructure operators, insights from Palo Alto devices are primarily accessed within the Grafana dashboard rather than Splunk SIEM. Splunk SIEM only mirrors relevant network protection feeds. As a security analyst, your first point of contact is always Splunk, but you may find yourself looking at Grafana in order to get more detailed firewall insights. If needed, you may get temporary elevated permissions to access the network firewall Command Line Interface, or CLI. This provides visibility into each firewall rule through the firewall policy API.

### Video - [Observability tools in the GDC SOC](https://www.cloudskillsboost.google/course_templates/1199/video/522037)

- [YouTube: Observability tools in the GDC SOC](https://www.youtube.com/watch?v=JMtEcf2e-ng)

SPEAKER: Unlike the core tools you've just explored, which act independently of one another, the observability tools are a connected ecosystem of functionalities that together support the end-to-end observability requirements of GDC. Observability aims to collect all logs and metrics from the platform to provide analytic capabilities via dashboards and alerting functionalities. If you remember from a previous module, two types of logs are collected. Operational logs, together with metrics, are useful for troubleshooting. Audit logs are necessary for in-depth security analysis and compliance. Observability in GDC is a multi-step process that is automated end-to-end. Prometheus collects infrastructure, system, and control plane metrics, with the optional collection of specialized metrics exposed by applications. All metrics are stored in Cortex. Cortex also offers a functionality called AlertManager, which provides automated alerts through a variety of configurable notification channels such as email, Slack, SMS, et cetera. FluentBit collects operational and audit logs. These logs are stored in Loki. All metrics and logs are visualized in Grafana through a variety of dashboards. Operators can also view and query logs and metrics directly with Grafana.

### Video - [LogMon](https://www.cloudskillsboost.google/course_templates/1199/video/522038)

- [YouTube: LogMon](https://www.youtube.com/watch?v=7_jTjtkO0ms)

SPEAKER: At the center of all observability is a Kubernetes custom resource definition, or CRD, with an operator called LogMon, which provides unified APIs for the end to end management of systems logs and metrics collection. LogMon is a CRD for logging and monitoring for Anthos. Anthos is Google Cloud's Kubernetes platform, which is at the center of all hybrid and multi-cloud deployments of Kubernetes, including on prem Kubernetes deployment of GDC. To understand how LogMon works, a brief recap of Kubernetes terminology is needed. A custom resource definition, or CRD, in Kubernetes is an extension of the built in resources in Kubernetes. The CRD stores a collection of custom API objects. Users interact with a CRD as they do with any other built in Kubernetes resources-- using Kubectl. When you Kubectl create the CRD, the API server stores the corresponding resource collection in the Etcd database with the Kubernetes scheduler keeping track of the new resource. On their own, custom resources let you store and retrieve structured data. You need a custom controller to perform actions on these custom resources. The controller is tasked with keeping the current state of Kubernetes resources in sync with your declared desired state. The process that controllers use is reconciliation, an infinite control loop running in a Kubernetes cluster that watches for create, update, delete events on relevant resources and triggers a reconcile function when needed to ensure that the system only has expected resources. A design best practice is to have one controller per resource. The Kubernetes operator combines all custom resources with their custom controllers. In this way, the Kubernetes operator can automatically manage the operations around the CRD as a human operator would. The LogMon CRD plus operator manages the collection of logs and metrics for both software and hardware in GDC. Remember, everything in GDC is a Kubernetes resource. LogMon provides an integrated and unified solution that orchestrates all observability tools in the Operations Center, OC, while offering the chance to customize deployments with respect to storage class, storage size, data retention time, agent resource requests/limits, additional metrics scraping, alerting rules, alerting notification channels, and log routing. Also, the workload LogMon CRD plus operator is currently under development and will enable you to support specialized logs and metrics for customer workloads.

### Video - [The GDC observability ecosystem](https://www.cloudskillsboost.google/course_templates/1199/video/522039)

- [YouTube: The GDC observability ecosystem](https://www.youtube.com/watch?v=kLNid-hIYPM)

SPEAKER: This high-level diagram showcases the GDC observability ecosystem that is managed in the background by LogMon CRD plus Operator. Observability encompasses both the user cluster, where the end users of the platform perform their customer workloads, and the admin cluster, where operations are held. All information coming from the user cluster passes through the Istio Ingress Gateway, which routes allowed traffic to the right resource within the admin cluster. Operational logs and audit logs are collected via a pull approach by FluentBit. GDC is not allowed to lose any audit logs generated within the system, so FluentBit has the fundamental task of collecting 100% of audit logs. Operational logs are collected automatically from stdout, stderr. Audit logs are collected from all GDC services and hardware components from relevant resources in the VAR log folder of the Kubernetes objects. FluentBit pushes logs to Loki for storage. Loki is hosted in the admin cluster. Remember, separation of duties always needs to be ensured. Log retention requirements will be defined by customer requirements. The minimum to ensure compliance requires all logs to be retained in searchable media for at least 30 days and audit logs to be further retained in archived or cold storage for at least one year for PA activities and five years for infrastructure operator activities. System and application metrics are pulled from all GDC assets to be stored within Prometheus in the admin cluster. Each user cluster has an instance of Prometheus installed locally. Metrics are synced inside the Prometheus instance of the admin cluster via a pull approach for components that can be scraped. A push proxy approach provides support for unscratchable components. In this case, metrics are pushed to an intermediary gateway as a short-lived, service-level batch job and then pulled within the admin Prometheus. The same retention policy as for operational logs applies. Metrics are collected on a per project, per cluster basis by Prometheus and then forwarded to a single tenant aware Cortex deployment. Cortex provides system-wide storage and querying for metrics. Cortex also offers automated rule-based alerting. When a rule is matched, Cortex uses the Alert Manager to route alerts to relevant notification channels. Lastly, this automated process concludes with the creation of the dashboard, the observability output that enables infrastructure operators in their day to day activities. Infrastructure and security operators can access a comprehensive set of predefined dashboards in Grafana. These dashboards provide an all-inclusive view of the platform based on both logs from Loki and metrics from Prometheus. Note that a sink operation also sends logs and metric streams to Splunk for dashboarding tailored to security operations. In the next video, you will learn about Grafana in more detail.

### Video - [Observability tools in the GDC SOC: Grafana](https://www.cloudskillsboost.google/course_templates/1199/video/522040)

- [YouTube: Observability tools in the GDC SOC: Grafana](https://www.youtube.com/watch?v=kqUfKLnns1M)

SPEAKER: Grafana is an open-source analytics and monitoring platform that is designed to help you visualize and analyze big data, particularly time series data. Grafana allows users to query data, visualize it within dashboards in Explorer, and create rules for alerts. What type of data can you query in Grafana? Grafana was originally created for the graphite monitoring system, and has since evolved to support a wide range of data sources and integrations. Grafana is now a top choice for those wanting to aggregate data from multiple sources into a unified dashboard. Data sources natively supported by Grafana include popular databases such as MySQL and PostgreSQL, logging systems, such as Loki and Tempo, and monitoring systems, such as Prometheus and AlertManager. Also, monitoring solutions from cloud providers are supported, such as Google Cloud Monitoring. For GDC, you will be interested in Loki, Prometheus, and Cortex AlertManager. A variety of plugins also exist to further extend this ecosystem. Grafana also includes three special built-in data sources. The Grafana data source provides information about the Grafana installation and links to test data. This can be helpful for testing visualizations and running experiments. The mixed data source is an abstraction that supports collecting multiple data sources within the same visualization. The dashboard data source uses the result set from another visualization in the same dashboard, including annotations attached to the visualization. Each data source comes with its own query language. To support this, Grafana provides a specialized Query Editor as part of each data source plugin. This allows users to take advantage of a specialized workflow built on the unique qualities of each query language within the Grafana UI. Depending on the data source, the Query Editor might provide autocompletion features, variable suggestions, data filters, hints, annotations, and other functionalities. For GDC, data sources of interest are Loki and Prometheus. The latter is also behind the AlertManager support. The Loki Query Editor supports the LogQL query language. And the Prometheus Query Editor supports PromQL.

### Video - [The query creation process in Grafana](https://www.cloudskillsboost.google/course_templates/1199/video/522041)

- [YouTube: The query creation process in Grafana](https://www.youtube.com/watch?v=dMNeGwaGd6E)

SPEAKER: The query creation process in Grafana is streamlined as follows. First, Grafana asks you to set up the data source and some query options which are specific to the query language. For example, both Loki and Prometheus allow you to set maximum data points and minimum intervals for the data source, as well as legend and query types for the output. Then, Grafana optionally helps you kick-start your query by listing common operations and components for the query language, with a step-by-step explanation of the underlying logic. Next, you choose between Builder or Code mode to build your query. The Code mode lets you write the query as if you are in the original system, which is Loki or Prometheus in this case. This can be useful for advanced users. The Builder mode lets you create the query dynamically by selecting operations and options prompted by the Query Editor interface. For example, the Prometheus Query Editor lets you explore metrics, while Loki Query Editor lets you search through labels and values to define relevant logs to query. Finally, you can manage the query output. For example, in Loki, you can attach annotations to the visualization to make navigating your dashboard more user friendly. In Prometheus, you can review statistics on query performance and data formats. Often, you may find yourself applying transformations to the outputs of your query, such as changing data types, sorting values, or combining values. This is useful to create tailored visualizations without having to rerun the same query with small modifications. Grafana uses a container, called a panel, to wrap the query and to provide you with all the controls necessary to set up the right visualization for the query. The catalog of visualization types is extensive, with almost 50 different visualization types ranging from classic graphs, such as time series and bar charts, to modern visualizations, such as node graphs and news feeds. You can customize all aspects of the visualization, from the legend to style, size, and embedded links. Query outputs are not only useful for visualization but also for alerting. Alerts can be set in each panel or within the centralized alerting page. Grafana Alerting is composed of an alert generator and an alert receiver. The alert generator is responsible for scheduling and evaluating alert rules. The alert receiver is responsible for grouping, inhibiting, silencing, and sending notifications. Both the generator and receiver use labels for fine-grained controls. Labels used in alert generation allow you to create multidimensional alerts. In the example, disk usage is not tracked for all servers but is tracked for each server disk. Labels are also useful for notification purposes to route alerts to specific contacts at specific times and via specific channels. Grafana supports 20 notification channels, including Email, Google Chat, Slack, Telegram, PagerDuty, and Alertmanager.

### Video - [Observability tools in the GDC SOC: Prometheus](https://www.cloudskillsboost.google/course_templates/1199/video/522042)

- [YouTube: Observability tools in the GDC SOC: Prometheus](https://www.youtube.com/watch?v=DBDz9z4GZH0)

SPEAKER: Prometheus is an open-source monitoring toolkit, originally developed by SoundCloud and now part of the Cloud Native Computing Foundation, or CNCF. Prometheus is designed to reliably record purely numeric data and cloud-native environments, including Kubernetes. While Prometheus also provides visualization and alerting capabilities, it is more common to use the many integrations Prometheus provides to connect it with other DevOps tools, namely Grafana, Splunk, and Alertmanager, for observability in GDC. Prometheus collects and stores all data on disk as a time series, following a streamlined HTTP-based process. Prometheus scrapes known targets returned by an HTTP endpoint, such as the Kubernetes API, or listed within a file. For each target, Prometheus sends an HTTP GET/metrics request and automatically records the stream of values of the same metrics with a millisecond precision timestamp. Metric values are in float64. Every metric is uniquely identified via name and labels which are operational key value pairs. By default, Prometheus attaches two labels that allow you to identify the target. Job in the configuration job name for the target, and instance is the host port part of the target's URL that was scraped. The metric format shown here is how metrics are referenced in PromQL. As a GDC security analyst, you don't interact with Prometheus directly, but rather with its outputs through Grafana, Splunk, and Alertmanager. Here, you can see an example dashboard that is monitoring a network switch based on logs provided by Prometheus. If needed, you may ask for temporary access to the Prometheus endpoint itself by requesting the Project Cortex Prometheus viewer role and using the default HTTP API provided by Prometheus. For example, you could query all values for the job label in order to see which targets are actively monitored.

### Video - [Observability tools in the GDC SOC: Cortex](https://www.cloudskillsboost.google/course_templates/1199/video/522043)

- [YouTube: Observability tools in the GDC SOC: Cortex](https://www.youtube.com/watch?v=UFCchI9xBXM)

SPEAKER: Cortex provides horizontally scalable and highly available storage for Prometheus. Prometheus stores data on disk at a local path without supporting multiple tenants. Cortex stores Prometheus data in a long-term, multi-tenant storage solution that is optimized through aggressive parallelization and caching for fast querying. The Cortex HTTP API allows you to set up automated tasks, adapt responses, and build integrations according to your use case. For GDC, the functionalities of interest are alerting rules and recording rules. Cortex supports alerting and recording rules via AlertManager Recording rules are time-series expressions that are run frequently or are computationally extensive and are thus pre-computed for efficiency. To define these rules, monitoring rule and logging rule custom resources, respectively for metrics and logs, are deployed into the namespace and can be accessed via the Kubernetes API as usual. The Observability pipeline custom resource tracks these resources at the organization level and launches the laghman operator. Rules are evaluated by Loki for logs and Cortex for Prometheus metrics, but alerts are triggered via Cortex for both. Cortex AlertManager receives and manages tenant-aware alerts. Where do you access alerts from? Infrastructure operators and security analysts are notified of critical and high-security alerts through ServiceNow. All alerts can be visualized through the AlertManager UI under the Operations menu from the GDC console and, if you get authorization, through the command line via the AlertManager API. For each alert, you receive relevant details such as status, message, rule that activated time, start time, duration, labels, and links to actions to take. You also have the option of silencing alerts.

### Video - [Observability tools in the GDC SOC:  Fluent Bit](https://www.cloudskillsboost.google/course_templates/1199/video/522044)

- [YouTube: Observability tools in the GDC SOC:  Fluent Bit](https://www.youtube.com/watch?v=l6nDepVgNnE)

SPEAKER: Fluentbit is an extra fast, lightweight and highly scalable log processor and forwarder that is specialized for cloud-native environments, including Kubernetes. Fluentbit collects data and logs from different sources, unifies them, and sends them to multiple destinations, including a central Fluentd server. For the GDC setup, Fluentbit is used to forward logs to Loki. Getting started with Fluentbit generally involves configuring input sources, defining parsers and filters if necessary, and specifying output destinations. Parsers and filters are implemented through a variety of default and custom plugins. Parsers and filters offer the ability to enrich logs before forwarding them. Incoming data is referred to as an event and consists of a timestamp, key value metadata, and payload. Events are automatically tagged based on manually configured tags that specify the source and type of the log. These tags are used to determine filtering, routing, parsing, modification, and output rules. The unstructured input can be parsed into a structured, typically JSON, format, filtered to match, exclude, or enrich the logs. The buffer offers a backup system to avoid data loss, and the routing sends the processed event to the right output channel.

### Video - [Observability tools in the GDC SOC: Loki](https://www.cloudskillsboost.google/course_templates/1199/video/522045)

- [YouTube: Observability tools in the GDC SOC: Loki](https://www.youtube.com/watch?v=ftmJBL48DdE)

SPEAKER: Loki is an open-source, horizontally scalable log aggregation system designed to efficiently handle and store log data. Loki is particularly well suited for containerized and microservice cloud-native environments. Loki was created by Grafana Labs and is meant to be used alongside Grafana for log visualization and analysis. For GDC, Loki also connects to Alertmanager and Splunk, as previously mentioned. Promtail is the name of the agent used by Loki for log collection and aggregation. Like Fluent Bit, Loki requires users to label data for efficient storage and querying. Labels and metadata are used for indexing. Log data is compressed and stored in chunks in object stores, such as S3 or GCS, or even locally on the file system. A small index and highly compressed chunks simplify the operation and significantly lower the cost of Loki. When you're querying Loki data, you should base your query on label matches and filters rather than content filters. This helps you maximize efficiency. But it is worth keeping in mind fewer labels lead to better performance, so label selection is fundamental. In GDC, Loki currently has two supported storage modes, persistent volume claim, or PVC Mode, and object storage mode. In PVC mode, Loki flushes logs into a Kubernetes persistent volume-- one per Loki instance. The PVC is constrained in capacity, with up to 20 gigabytes of storage space. If Loki runs out of space, logs stop being collected. That's why automated processes are set up after GDC bootstrapping to move to storage grid mode, where Cortex provides object bucket storage.

### Video - [Management tools in the GDC SOC: ServiceNow](https://www.cloudskillsboost.google/course_templates/1199/video/522046)

- [YouTube: Management tools in the GDC SOC: ServiceNow](https://www.youtube.com/watch?v=Y_6sCun2WQk)

SPEAKER: Management tools in the GDC SOC aim to streamline the overall governance of security operations for both teams and infrastructure. Case management and configuration management tools support team operations in managing and collaborating on incidents and configuration definitions. Image-hosting and infrastructure-hosting tools support the management of the underlying infrastructure of the SOC. Let's explore ServiceNow in detail and the other tools at a higher level. ServiceNow, also referred to as SNow, offers a centralized, cloud-based platform for incident management, case organization, reporting, and automation. ServiceNow creates a single recordkeeping system for all the service management processes within an organization. ServiceNow brings together systems, processes, people, and places using a single and automated operational strategy. The Now platform provides a consistent and intuitive user experience through the entire service management lifecycle. ServiceNow was founded in 2004 with an initial focus on IT Service Management, or ITSM. ServiceNow has since extended beyond IT services to now offer a comprehensive platform that spans various business functions and processes, from IT and SecOps to HR, accounting, and customer service. The aim of ServiceNow is to be a central hub for enterprise-wide service management and automation for a variety of industries. To achieve this objective, the Now platform needs to support technology, employees, customers, and creators, that is, the admins of ServiceNow themselves. To effectively manage this extensive range of operations, ServiceNow employs a modular structure in which each module is dedicated to addressing a specific business function and process. This modular approach not only ensures scalability but also provides organizations with unparalleled customization capabilities. By selectively adopting modules that align with their unique requirements, organizations can seamlessly tailor the Now platform to their specific context. This fosters adaptability and efficiency in organizational workflows. For security operations, the GDC SOC uses the Security Incident Response module from ServiceNow. The ServiceNow Security Incident Response, or SIR, module optimizes and orchestrates security operations, facilitating a rapid response to evolving threats. SIR empowers security analysts with a robust suite of capabilities and aligns with the MITRE ATT&CK framework. SIR capabilities include the following-- a comprehensive operations dashboard for real-time monitoring of security posture, the ability to manage detailed workflows to ensure repeatable and efficient processes, the implementation of security incident management that enables faster and more collaborative responses to security incidents. The Operations dashboard provides visibility into key metrics and indicators of SOC performance. The Operations dashboard offers an intuitive overview of the current state of the security posture and highlights where attention is needed to evolve the team and response workflows. The workflow management functionalities of SIR allow you to enhance processes by automating assignments and coordinating incident prioritization and remediation across IT and security. The incident management functionalities of SIR allow you to respond collaboratively to critical security incidents by offering a complete overview of the incident response. This includes incident details, collaboration, tasks, and impact. Each identified service event, including incidents and other security events, is captured as a ticket on the Now platform. Each ticket has a unique identifier, often referred to as a ticket number, and contains all relevant details for the underlying task, incident, or request. Ticket fields include description, assignment, state, priority, and additional custom fields as defined within the ticket template. Through ticketing, everyone in the organization can track, review, and respond to service events as defined by the user's access control from one centralized place. This ensures visibility, accountability, and efficiency.

### Video - [ServiceNow at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1199/video/522047)

- [YouTube: ServiceNow at Cymbal Federal](https://www.youtube.com/watch?v=8deQUC142eA)

SPEAKER: Let's get back to the simple federal use case to discover how security analysts in the GDC SOC use ServiceNow. Firstly, let's explore the four types of tickets that you interact with as a security analyst. A case ticket is any support request-- for example, bugs and general inquiries-- raised by customers. You create an incident ticket to capture infrastructure issues. System-generated alerts are automatically logged as incident tickets. You create a change ticket for toil tasks as well as to record configuration changes. You can create a major incident ticket for service outages and other issues that significantly impact many customers and disrupt their normal operations. Customers and infrastructure operators access ServiceNow through two different portals, with each providing different functionalities. This ensures the separation of duties. Platform administrators and application operators use the GDC support portal. Infrastructure operators, including you as a security analyst, access the Now Platform through a configurable workspace. During onboarding, customers need to set up their own identity provider, which allows them to control access to the platform. Customers can only create, update, and close cases. You, as an infrastructure operator, are authenticated on the platform using the OCIT identity provider. You can create, update, and close all ticket types. You can also request ticket escalation.

### Video - [Management tools in the GDC SOC: GitLab](https://www.cloudskillsboost.google/course_templates/1199/video/522048)

- [YouTube: Management tools in the GDC SOC: GitLab](https://www.youtube.com/watch?v=lLRzRenlA4k)

SPEAKER: GitLab is a web-based platform that provides a comprehensive set of tools for version control, source code management, continuous integration, issue tracking, and more. GitLab enables software development teams to collaborate efficiently on their code sets. GitLab supports both private and public repositories. This makes GitLab a versatile solution for teams of all sizes who wish to manage their software development lifecycle, from planning and coding to testing and deployment. For GDC, GitLab is installed locally in OCIT as an internal repository within Go GDC-ia, and is used to handle Infrastructure As Code, or IaC. The default deployment includes existing Docker images, the Omnibus . deb package, and Helm charts. Controllers and operators that need to enforce IAC will be required to first commit their definitions to this repository from which they will be synced into target clusters. GitLab also supports break-glass procedures. This is when authorized personnel are temporarily given privileged access to sensitive data or assets by providing robust version control and observability over the platform environment. As a security analyst, you won't find yourself using GitLab on a day-to-day basis. Instead, you will task the appropriate owner of any compromised asset to make relevant fixes. Infrastructure operators are notified of all fix requests, as shown in the screenshot, and can prioritize fixes based on the highlighted priority. As you can see, GitLab is a collaboration tool that you, as a security analyst, will use to coordinate with the engineering team.

### Video - [Management tools in the GDC SOC: Anthos Config Management (ACM)](https://www.cloudskillsboost.google/course_templates/1199/video/522049)

- [YouTube: Management tools in the GDC SOC: Anthos Config Management (ACM)](https://www.youtube.com/watch?v=7egBzROGMY0)

SPEAKER: The Kubernetes resource declared in the GitLab repository are synced to the GDC clusters via Anthos Config Management, or ACM. ACM is a service within Anthos that focuses on managing configurations for Kubernetes clusters in a multicluster environment. ACM is composed of three components which are designed to be used together but can also be used independently. Config Sync continuously reconciles the clusters to the configuration centrally stored in Git. Policy Controller enables the enforcement of fully programmable policies that represent constraints that guarantee the desired state. Config Controller is a hosted service that allows you to provision and orchestrate Anthos resources. Config Sync is only used for GDC. Anthos Config Management Config Sync supports configurations defined with kpt and Helm, amongst other formats. GDC combines these two Kubernetes management tools to support a layered declarative design for Kubernetes deployments. The declarative design ensures standardization and reproducibility. The Helm chart is a centralized, predefined Kubernetes resource definition that contains a default instantiation of the resource. Values can be customized using a values.yaml Helm file and then further extended using functions predefined in a kpt file. Kpt defines configurations as resource objects rather than as code libraries, which looks at configuration as data. Helm is used for packaging, and kpt is used for applying configurations. Infrastructure operators in the GDC SOC can easily set up and return to a baseline deployment of GDC, while being able to seamlessly customize the deployment if needed.

### Video - [Management tools in the GDC SOC: Fleet](https://www.cloudskillsboost.google/course_templates/1199/video/522050)

- [YouTube: Management tools in the GDC SOC: Fleet](https://www.youtube.com/watch?v=s8RWcOo0LAo)

SPEAKER: Fleet by Rancher is a container management and deployment engine designed to offer GitOps at scale. Fleet can manage up to a million clusters, but it is lightweight enough to work for a single cluster, too. Beyond scalability, Fleet provides users with a high degree of control over the local cluster. Fleet also offers visibility through constant monitoring. Fleet is fundamentally a set of Kubernetes Custom Resource Definitions, or CRDs, and controllers that manage GitOps for the cluster from the raw YAML files. Fleet makes it easy to customize and manage the cluster from a single point. The Fleet manager uses helm to deploy all resources in the cluster and effectively acts as an admin ClusterRole for coordinating and managing GitOps operations across the entire infrastructure. Fleet by Rancher was the missing part of the GitOps diagram previously introduced for GDC. Infrastructure operators in the GDC SOC can use the fleet manager as a single point of contact for managing updates to the GDC cluster.

### Video - [Management tools in the GDC SOC: Harbor](https://www.cloudskillsboost.google/course_templates/1199/video/522051)

- [YouTube: Management tools in the GDC SOC: Harbor](https://www.youtube.com/watch?v=hZ3edPCcvl8)

SPEAKER: While GitLab hosts code, Harbor hosts pre-built software images that contain relevant packages and artifacts. Harbor is an open-source, cloud-native container registry solution for Kubernetes. As a centralized image-hosting solution, Harbor provides a good set of safety guarantees. These are as follows. Harbor secures artifacts with policies that can be configured to define rules for imaging retention, expiration, and versioning. Harbor implements role-based access control to regulate access to container images and repository operations based on user roles and permissions. Harbor ensures images are scanned and free from vulnerabilities. Harbor supports image signing so that users can pool and deploy images with the guarantee that they have not been compromised. Harbor instances are automatically provisioned and managed by GDC and come with built-in integration with GDC's IAM and observability systems. As a security analyst in the GDC SOC, you may find yourself using Harbor when restoring a system or a file after a compromise. Harbor offers versioning and rollback capabilities, which means you can easily roll back to a trusted version of a system or artifact. In the example, you have six versions of an image for which you can easily review size, vulnerability scan outcomes, if it's signed, author, creation time, and other labels.

### Video - [Management tools in the GDC SOC: Red Hat Enterprise Linux (RHEL)](https://www.cloudskillsboost.google/course_templates/1199/video/522052)

- [YouTube: Management tools in the GDC SOC: Red Hat Enterprise Linux (RHEL)](https://www.youtube.com/watch?v=YhegWGPZuGA)

SPEAKER: Red Hat Enterprise Linux, or RHEL, is a leading enterprise-class Linux distribution that is known for its stability, security, and comprehensive support. RHEL integrates seamlessly with containerization technologies. This makes it well-suited for Kubernetes deployments. Being an enterprise solution, RHEL offers a variety of desirable functionalities, such as long-term support model, automation, performance optimizations, and highly compatible ecosystem. Most crucially, RHEL is a Security-Enhanced Linux, SELinux, distribution that enforces mandatory access controls based on fine-grained access policies, security labels, and a default-deny policy. In GDC, RHEL is the operating system used for all GDC containers. RHEL plays a crucial role in providing a reliable operating system foundation for running and managing containerized workloads, from databases to applications and more.

### Video - [Management tools in the GDC SOC: MariaDB](https://www.cloudskillsboost.google/course_templates/1199/video/522053)

- [YouTube: Management tools in the GDC SOC: MariaDB](https://www.youtube.com/watch?v=Wk6lfa0noAw)

SPEAKER: MariaDB is an open-source relational database management system compatible with MySQL and known for its focus on performance optimization. MariaDB includes features such as acid compliance, multiple storage engines, and support for high availability and clustering. This makes MariaDB suitable for a wide range of applications, from small-scale projects to large enterprise systems. MariaDB also provides relevant security features, including encryption options, role-based access control, RBAC, secure connections via SSL/TLS, and firewall support for network access control. In GDC, MariaDB is the back-end database that stores and manages data for ServiceNow. You can see some of the tools you've learned about coming together in this design diagram for the MariaDB deployment in GDC. Like everything in GDC, MariaDB is deployed as a container and has a persistent volume attached to contain the ticket data. MariaDB is deployed and upgraded via the GitLab Workflow based on Helm charts and image deployment in Harbor. A custom resource in Kubernetes called Ticketing Upgrade handles the routine that pulls and deploys the latest image from Harbor.

### Video - [Module summary](https://www.cloudskillsboost.google/course_templates/1199/video/522054)

- [YouTube: Module summary](https://www.youtube.com/watch?v=RyM7FIaD20M)

SPEAKER: Let's quickly review what you've learned about SOC tools for GDC. This module started by describing how the setup and configuration of tools in the SOC aligns with state-of-the-art security standards, namely the Mitre ATT&CK framework and support by Mandiant. You were then introduced to the GDC tooling architecture, focusing on three broad categories of tools. Core SOC tools encompass foundational security components that are focused on threat detection and incident response, vulnerability management, endpoint protection, and network protection systems. Observability tools provide insights into system behavior, performance, and security events, enhancing visibility for proactive monitoring and analysis. Management SOC tools streamline administrative tasks, collaboration, and the overall governance of security operations. You then reviewed the tools that comprised each category, with a particular focus on Splunk Security Information and Event Management, SIEM, as a core SOC tool, Grafana as a general infrastructure observability tool, and ServiceNow as a management tool. Now that you have a solid understanding of these topics, you're ready to advance to the final module of this course.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1199/quizzes/522055)

## Default logs, metrics, dashboards, and alerts in Splunk SIEM

This module introduces the Splunk security information and event management (SIEM) setup for GDC. You'll learn the basics of log collection, the types of information in GDC logs, and how to access them in Splunk. You'll explore metrics monitoring in Splunk, including accessing metrics through the Splunk UI. You'll learn to navigate Splunk dashboards, understand common chart types, and explore runbook and service dashboards. Using the Cymbal Federal use case, you'll learn how to navigate a runbook dashboard for incident response. Finally, you'll become familiar with the format of alerts and the alert inventory for the GDC Security Operations Center (SOC).

### Video - [Module overview](https://www.cloudskillsboost.google/course_templates/1199/video/522056)

- [YouTube: Module overview](https://www.youtube.com/watch?v=5jPWPr9dtx8)

SPEAKER: Welcome to Default Logs, Metrics, Dashboards and Alerts in Splunk SIEM. In this module, you will be introduced to the basics of the Splunk Security Information and Event Management, or SIEM, set up for Google Distributed Cloud, or GDC air-gapped. Splunk SIEM is the go-to tool for your day-to-day activities, so it's important to familiarize yourself with it. You will begin with an overview of logs. You will briefly review how logs are collected, then discover what information is included in different GDC logs, and then discuss which categories of logs you can access from Splunk. Then, you will review the metrics monitored in Splunk. You will learn how to access these metrics through the Splunk UI. Then, you will learn how to navigate dashboards in Splunk. You will discover the types of charts that are commonly used and learn about runbook and service dashboards. You will revisit the symbol federal use case and briefly explore how to navigate a runbook dashboard for incident response. Finally, you will be introduced to the format of alerts and the alert inventory for the GDC Security Operations Center, or SOC.

### Video - [Log types in the GDC SOC](https://www.cloudskillsboost.google/course_templates/1199/video/522057)

- [YouTube: Log types in the GDC SOC](https://www.youtube.com/watch?v=aUAm4ohj8Xc)

SPEAKER: Not all GDC logs are available through Splunk in GDC. While Grafana provides complete coverage of all GDC logs, Splunk only mirrors the logs that are meaningful for security. Specifically, Splunk provides 100% coverage for the security logs in GDC but only partial coverage for audit and operational logs. Metrics can then be generated from these logs to aid monitoring. Audit and security logs offer full visibility of all administrative operations and actions performed in the root admin, org admin, and user clusters of GDC. Visibility over the system cluster is more constrained. This is because customer workloads are sensitive, and GDC needs to ensure privacy and sovereignty for customer organizations that use GDC. For customer workloads on the system cluster, only NetFlow data is collected as audit logs. NetFlow data allows security analysts to see the metadata of what flows in the network at the project level. However, security analysts cannot see the data itself, as data is encrypted. Another type of log available in GDC is the operational log. Operational logs support security analysts in debugging their own user flows. This is achieved by accessing the standard output, or stdout, and standard error, or stderr, of applications that run in the admin cluster. Each type of log is collected using a specialized Kubernetes custom resource. This ensures tailored management and configuration. The Custom Resources, or CRs, are as follows-- AuditLoggingTarget CR for audit logs, SecurityLoggingTarget CR for security logs, LoggingTarget CR for operational logs. The CRs are managed through LogMon, which sends the collected logs to FluentBit. FluentBit acts as a syslog server that also provides some initial pre-processing of the logs before they are stored in Loki, and accessed and visualized in Splunk and Grafana. This summarizes at a high level how logs are collected. In the following videos, you will look at each log type in more detail.

### Video - [Audit logs](https://www.cloudskillsboost.google/course_templates/1199/video/522058)

- [YouTube: Audit logs](https://www.youtube.com/watch?v=oJKorMViv-Y)

SPEAKER: An audit log, also known as an audit trail, is a chronological record of system activities. Audit logs afford organizations on GDC the ability to capture, retain, and analyze events for deep insight regarding who, what, where and when. These insights can be used for compliance, debugging, or security purposes. Audit logs record every user and administrative activity on privileged operations. Audit logs provide information that should at least establish what type of event occurred, when the event occurred, where the event occurred, the source of the event, the outcome of the event, and the identity of any individuals or subjects associated with the event. Audit logs need to be collected throughout the GDC platform, which includes all hardware and software components. Bare metal and virtual machine nodes collect audit logs known as Out-Of-The-Box logs, or OOTB, since they are only related to the GDC deployment itself. To reduce disk consumption, logs on each node are rotated on a 10-minute basis. Logs that are older than 24 hours are removed. Logs from Kubernetes applications track every request sent to the Kubernetes API server generated by users, by applications that use the Kubernetes API, and by the control plane itself. Retention for these logs is even more strict. Platform administration logs must be retained for at least one year and infrastructure operations logs for five years. Beyond extended retention and 100% coverage of the platform, audit logs follow other very strict compliance and security requirements. First, audit logs need to be stored in a WORM bucket. A WORM bucket ensures that objects cannot be overwritten and that they are retained for the specified minimum period of time. Second, log isolation needs to be preserved between all access levels and tenants. And finally, no audit log can be lost. To support these compliance requirements, audit logs need to be extremely comprehensive and exhaustive. This, of course, means that audit logs come in high volume. To give you an idea of the size of audit logs, let's take, for example, an organization running idle with a default GDC deployment. Just the Kubernetes API server on its own generates approximately 400,000 records a minute and consumes approximately 20 to 25 gigabytes of disk space daily. Audit logging in GDC requires terabyte support. Also, Kubernetes is just one type, albeit the most expensive type of audit log source collected in GDC. Logs from Kubernetes record the sequence of actions in the GDC clusters. The observability audit logger collects audit logs for data access. And Service Mesh Envoys collect activity from network traffic as defined by requests received on the Istio service mesh. Each source type has its own log format, but they are pre-processed in Loki to conform to a common JavaScript Object Notation, or JSON, format and set of severity levels. The JSON format includes user/service account identity; role; timestamp; group/label; resource name; action; action type, such as create, read, update, or delete; event name summary; description; source IP address of caller; success or failure of IAM authorization; and error status. How fields are populated depends on the specific event. For example, error status will only populate for error events. And some fields may not apply to some components. There are eight levels of severity associated with each audit log. The lowest level of severity is debug, and the highest level is emergency. A default severity value can also be set. Audit logs aim to consistently track actions performed on the platform. This is in order to monitor for potential security breaches or internal misuses of information. Audit logs are required to ensure users follow all documented protocols and do not violate authorized role functions. At a high level, this is achieved by tracking the following four types of actions-- admin activity, data access, system/security event, and policy denied. For example, you want visibility when the source code is updated in GitLab, when someone accesses or modifies a sensitive file, when a user attempts or fails to log in, or when a security policy is violated. Here is the full list of collected audit logs classified by operable component. Operable components are a way of organizing the platform for operation, service, and maintenance so that an infrastructure operator can interact with the platform in a way that makes sense. This is the outside-in view of GDC. You'll become familiar with which resources are included in each operable component over time. For now, keep in mind that all audit logs are accessible via Grafana by running log QL queries and that a subset of these logs is available for Splunk too. This is highlighted here with a Nessus scanner.

### Video - [Audit logs at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1199/video/522059)

- [YouTube: Audit logs at Cymbal Federal](https://www.youtube.com/watch?v=M9cJeGhZw80)

SPEAKER: Let's look at two examples for Cymbal Federal to get a better idea of what type of information audit logs provide. The first example is a raw log taken from the admin cluster, which is populated for a data operation. In this audit log, the event occurred on November 23, 2022, at 23:29 and 31 seconds UTC. The user system:serviceaccount:gatekeeper sytem:gatekeeperadmin, made a list request with a limit of 500 items targeting the request URI, /apis/baremetal. cluster.gke.io/v 1alpha1/addonconfigurations. The add-on configurations resource is part of the baremetal.cluster.gke.io API group with version v1alpha1. The request was sent to the API server service of the GDC Fluent Bit pod anthos audit logs forwarder gc5d7 within the root admin cluster. Let's now look at an example log for netflow data. For this log, you don't get as much information. Netflow data lets you define the type of workload by providing information such as timestamps, IP addresses, and the number of packets and hops, but it doesn't provide any sensitive information on users or IDs.

### Video - [Security logs](https://www.cloudskillsboost.google/course_templates/1199/video/522060)

- [YouTube: Security logs](https://www.youtube.com/watch?v=3g8sDADREB0)

SPEAKER: Security logs are logs related to security operations in one of two ways. They are audit logs coming from security services, such as from intrusion events and malware activity. Or they're logs that require stronger integrity guarantees. Unlike audit logs, which can be accessed by all infrastructure operators and also by end users for customer workloads, only security analysts in the SOC monitor and access security logs. Similar formats and considerations introduced for audit logs mostly apply for security logs, too. But keep in mind that a different Kubernetes CR specifically serves these types of logs. You can learn more about the specific characteristics of these types of logs by reviewing the security logging target CR itself.

### Video - [Operational logs](https://www.cloudskillsboost.google/course_templates/1199/video/522061)

- [YouTube: Operational logs](https://www.youtube.com/watch?v=HID-2oT321k)

SPEAKER: Finally, operational logs record conditions, changes, and actions, as GDC users manage ongoing operations in applications and services. These are non-critical logs that don't require integrity guarantees, such as application error messages, warnings, and successful operations. Rather, these logs are useful for developers and operators to test and debug applications. As such, these logs have a shorter retention period of only 30 days. Operational logs are collected for all users-- infrastructure operators acting on the root admin cluster, platform administrators acting on the org admin cluster, and application operators acting on the user and system clusters. Infrastructure operators and security analysts only have access to their own operational logs, created in the root admin cluster. Infrastructure operators can ask for break-glass access to operational logs in the org admin cluster, too, if needed for a critical situation to ensure security. However, they can never access operational logs at the project level. This is to ensure the privacy of customer workloads. Operational logs are organized based on a hierarchical naming convention, with the primary category defining the high-level system or technology, such as Kubernetes or Linux, and the subcategory specifying the component or the type of logs, such as DNS or HTTP for stream logs.

### Video - [Operational logs at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1199/video/522062)

- [YouTube: Operational logs at Cymbal Federal](https://www.youtube.com/watch?v=mY_gU0DQY4s)

SPEAKER: Let's look at an example of a log from a DNS stream. These logs show the DNS query and response as well as relevant information on the message, such as type, size, IPs, and transport. It is important to note that this log does not provide snapshots of important system objects or values of critical variables, which is generally desired in order to keep operational logs secure. As a security analyst, you will find yourself reviewing DNS stream logs often when troubleshooting. By analyzing domain name resolution details, you can pinpoint connectivity issues, identify potential malicious activities, and expedite the resolution of network-related problems.

### Video - [Review: Logs in Splunk](https://www.cloudskillsboost.google/course_templates/1199/video/522063)

- [YouTube: Review: Logs in Splunk](https://www.youtube.com/watch?v=h_SMKddsPAQ)

SPEAKER: To conclude this overview of the default logs available to the SOC, let's look at all the logs accessible through Splunk SIEM. This is an unofficial categorization of the logs, but it is useful to present the insights you can get from the various log sources. Network security logs include firewall events, NetFlow data, and DHCP/DNS activities. Endpoint security logs monitor endpoint detection and response and virtualization services. Infrastructure management logs encompass both software, such as Bare Metal OS and Metal Load Balancers, and hardware-- for example, BMC and Storage Appliance components. IAM logs track OC Active Directory, Key Management Service, and Identity and Access Management activities. Storage and data management logs cover object storage and volume storage. Security logs include vulnerability scanning and Windows event logs. And application logs encompass API calls, Kubernetes Service Mesh, and workloads. Note that labels can be used to provide each security analyst role with different access to different logs. By default, all logs are accessible to everyone in the SOC.

### Video - [Metrics in Splunk](https://www.cloudskillsboost.google/course_templates/1199/video/522064)

- [YouTube: Metrics in Splunk](https://www.youtube.com/watch?v=-Z22b3V8h6w)

SPEAKER: Metrics are numerical measurements of a component's or a service's operational state over a period of time. Metrics are useful as a key indicator of both the health and performance of a resource. Each component and service of a distributed system generates a number of metrics that need to be collected, recorded, aggregated along with multiple dimensions, and transformed to get a better view of the operation of the system as a whole. The monitoring components of GDC collect metrics that become visible in Splunk dashboards in order to measure health and cluster performance. The process is as follows. First, GDC configures metric storage in Cortex. Second, Prometheus collects metrics through an HTTP endpoint. Third, metrics are labeled automatically, and additional labels can be added at any time. And finally, metrics are stored in Cortex. This process is handled via the monitoring target CR. As a security analyst, you have two options-- A, you can query and view existing metrics; and B, you can calculate new metrics from system data in Splunk. Metrics that are accessed often are displayed within visual dashboards. Ideally, you'd only rely on pre-existing metrics and Splunk to support your day-to-day activities as a security analyst.

### Video - [Baseline metrics](https://www.cloudskillsboost.google/course_templates/1199/video/522065)

- [YouTube: Baseline metrics](https://www.youtube.com/watch?v=Nprd62LdqIQ)

SPEAKER: Each GDC service aims to publish the following set of baseline metrics. Let's briefly explore each one. Note that custom metrics can also be provided. The request latency is the time it takes for a system to respond to a request. This metric is useful for the SOC to monitor the responsiveness of systems. Abnormal increases may indicate performance issues or potential security incidents. This aids in the early detection of threats. Traffic is the volume of data or requests transmitted over a network. Traffic helps a security analyst determine the overall load on the network. Sudden spikes or anomalies may suggest a network-based attack or abnormal activity. Requests per second are the number of requests processed by a system in one second. This metric indicates the workload on systems. Rapid increases in system workload may signal a potential Denial of Service or DoS attack, or unusual user behavior. Transactions Per Second, or TPS, are the number of transactions processed by a system in one second. This metric monitors the logic layer of the system. Anomalies identified may indicate fraudulent activities or abnormal system usage. The number of concurrent sessions defines the number of simultaneous connections or interactions with the system. SOC analysts can use this metric to identify the number of active users or devices. Sudden spikes may indicate a breach or unauthorized access. Errors are instances where a system fails to complete a requested operation. Errors flag potential issues or security incidents. A sudden increase in errors may indicate a system compromise or an attack attempt. The error rate is the proportion of errors compared to the total number of requests. This metric helps assess the severity of security events. A high error rate may indicate a critical problem or an ongoing attack. Saturation is the extent to which a resource, for example CPU and memory, is utilized or nearing its maximum capacity. Security analysts can use this metric to detect resource exhaustion or the potential for performance degradation. This metric helps to anticipate and address capacity-related security risks. CPU utilization defines the percentage of CPU capacity in use. This metric is useful to monitor system performance. Abnormal spikes may suggest a malware infection or a resource-intensive attack. Network bandwidth utilization is the percentage of available network bandwidth in use. SOC analysts can use this metric to identify abnormal network activity or potential DoS attacks. This metric helps ensure network performance and availability. The storage usage-to-limit ratio is the ratio of used storage to the total storage limit. This metric is useful to monitor storage health. Used storage exceeding limits may indicate a potential threat, data breach, or system misconfiguration. The billing metric measures the usage impact on billing, for example, GDC costs. This metric helps track resource allocation efficiently. Unusual billing patterns may indicate unauthorized usage or a compromised account. As a SOC analyst, you will find yourself mostly tracking Splunk performance. Other baseline security-specific metrics useful for Splunk tracking include activity metrics, such as daily, weekly, monthly license usage, usage violations, user activity by role or permissions, as well as outages; search performance metrics, such as search execution time, percentage of fast-mode searches, overview of low-performance queries, and categories of queries; data quality and quantity metrics, such as growth rate, volume of indexed data by source, data completeness, including percentage of missing or malformed fields, and rate of data validation errors; alerting and monitoring metrics, such as the rate of alerts triggered, the distribution of alert categories, and the effectiveness of alerting rules with respect to false positives and false negatives.

### Video - [Security-specific metrics in Splunk](https://www.cloudskillsboost.google/course_templates/1199/video/522066)

- [YouTube: Security-specific metrics in Splunk](https://www.youtube.com/watch?v=O_xeAEI0BDU)

SPEAKER: What security services are metrics collected for? Well, event logs can also be converted to metric data points in Splunk, either at indexing or at search time. You can look at two security metric types defined from logs, audits and operations. Audit logs include these sources in decreasing order of log volume-- Kubernetes API server, Linux audit, Nessus monitoring, SSH server, and other Linux audit types, such as chkrootkit and ClamAV. The JSON logs refer to the original logs before being parsed and indexed by Splunk in a more digestible and readable format. In an ideal world, you wouldn't use these, but they are still relevant for any unsupported stream. Audit logs provide crucial insights into system activities, user authentication, network scanning, and potential security threats detected by tools like Nessus and ClamAV, which are fundamental for compliance and security monitoring. Operations logs encompass network traffic and communication protocols. Operations logs include these sources in decreasing order of log volume-- DNS, Domain Name System; SMB, Server Message Block; ARP, Address Resolution Protocol; TCP, Transmission Control Protocol; LDAP, Lightweight Directory Access Protocol; ICMP, Internet Control Message Protocol; and DHCP, Dynamic Host Configuration Protocol. Metrics built on these logs are fundamental to gaining an understanding of user activity and service availability throughout the GDC platform.

### Video - [Using Splunk metrics](https://www.cloudskillsboost.google/course_templates/1199/video/522067)

- [YouTube: Using Splunk metrics](https://www.youtube.com/watch?v=_7yUwzj6qkY)

SPEAKER: The biggest benefit of managing the security posture of the GDC deployment is correlating metrics across various infrastructure components with Splunk. Security analysts can use metric correlation to detect usage and performance trends. This enables analysts to make decisions based on capacity changes. Security analysts can also efficiently manage incidents before they affect system performance by using alerts and remedial actions. A metric in Splunk is composed of a timestamp in the Unix format, a metric name, a numeric value, a measurement that provides the standard definition format of the metric, and a list of dimensions that detail additional custom information about the measurement. Host, source, and sourcetype, i.e. the data structure of the metric, are default and non-writable dimensions added by Splunk for all metrics. An index is also provided if a metric index is used. Metrics indices store metric data points in a format that provides faster search performance and more efficient data storage. Each role is assigned a default set of metric indexes that are relevant to that role. The same is true for logs through event indexes. You can access metrics in Splunk in three ways. First, Search lets you perform powerful search queries to extract specific metric data from logs or metric events. Second, Dashboards let you visualize and monitor key metrics in a customized layout. And third, the Analytics Workspace is an interactive environment that facilitates in-depth exploration and analysis of your metric data over time. You will primarily use metrics reported in Dashboards. Sometimes you may find yourself doing ad-hoc searches too.

### Video - [Splunk metrics at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1199/video/522068)

- [YouTube: Splunk metrics at Cymbal Federal](https://www.youtube.com/watch?v=PYdWrcH4mAA)

SPEAKER: Let's return to Cymbal Federal and look at how you can search for metrics when using Splunk. There are three commands you may find yourself using often to filter, aggregate, enumerate, view, and report metrics data. Let's look at two examples of insights that you, as a security analyst for the Cymbal Federal GDC SOC, might gather via metric search. Imagine you are responding to an incident and trying to figure out which systems have been impacted. You can use the mstats command, which lets you apply aggregate functions such as average, sum, count, and rate. These can help you isolate and correlate problems from different data sources. You can use an mstats query to analyze and visualize the average values of metric names matching the pattern cpu. over a one-minute time span. This query is optimized with an indexed search too. Let's look at another example. Let's say you want to check if users are trying to access resources without authorization. Mcatalog lets you enumerate metric names, dimensions, and values. You could use mcatalog to count all the IP addresses for the login.failure metric. mpreview enables you to view individual metric data points without aggregation. You could use mpreview to get all the IP addresses for the login.failure metric.

### Video - [Splunk dashboards](https://www.cloudskillsboost.google/course_templates/1199/video/522069)

- [YouTube: Splunk dashboards](https://www.youtube.com/watch?v=BB7Zxad0-VQ)

SPEAKER: A dashboard tells a story that analysts can use to guide their decision making. Dashboards transform complex and diverse data sources into meaningful visualizations. Dashboards guide users through a narrative that reveals insights, trends, and critical information. Through the smart design of dashboards, efficient decision making and fast actions are made possible. Let's review the most common dashboard charts used in Splunk for the GDC SOC. A line chart represents data points with connected lines illustrating trends over time. For example, a line chart can be used for the daily count of login failures. This can detect spikes, which can indicate potential brute force attacks or the trend of malware infections to identify sudden spikes over a specific time period. When time is used for the x-axis, the chart is typically referred to as a time chart. Bar and column charts represent data points through horizontal and vertical bars, respectively. These plots are often stacked, as shown in the example graph for the column chart. For example, a bar chart can be used to compare the distribution of incident categories or to list the top 10 IP addresses with the highest number of denied-access events. A column chart is especially useful to compare quantiles. It can be used to compare the frequency of attack types over a specified time period or to display the count of security incidents by severity level for quick prioritization. When a column chart represents the distribution of numerical values in bins, it is called a histogram. An area chart is similar to a line chart, but it emphasizes the cumulative area between the line and the axis. For example, an area chart can be useful to monitor the overall impact of DoS attacks on network bandwidth over time. It can also be used to visualize the gradual increase in the number of malware attacks received per day. A pie chart shows the proportion of a whole as slices of a circular pie. A pie chart can help illustrate security incidents by incident category and display the percentage breakdown of incident severities. A scatter chart shows the relationship between discrete data points. For example, a scatter chart could plot the correlation between the number of malware infections and the duration of the incident response. A scatter plot can also help you visualize the distribution of threat scores for different IP addresses. An event feed chart is a dynamic representation of real-time or historical events, displayed in chronological order, to provide a continuous feed of activities. For example, an event feed could display a continuous feed of real-time threat intelligence events or track the actions taken during incident response. A geo map displays data points on a map based on their geographical coordinates. A geo map could map the locations of IP addresses associated with suspicious login attempts in order to identify potential geographical patterns or visualize the global distribution of malware infections. A gauge provides a visual indicator of a metric's value within a predefined range. Types of gauges supported in Splunk are radial, filter, and marker. For example, a radial gauge is useful to indicate the overall security risk level based on a calculated risk score. A top-N visualization displays the top N items, based on a specified metric. A top-N visualization could be displayed through different charts. Bar charts and column charts are the most common choices. For example, you could view the top 10 users with the highest number of security incidents or the top 5 services with the most policy violations. A table displays data in rows and columns. For example, a table could list detailed information about recent firewall events, including source and destination IP addresses, ports, and protocols. A heatmap visualizes data intensity using color variations. A heatmap could be useful to visualize the intensity of DDoS attacks over time, or the concentration of malware infections across different services. These visualizations, along with others, are thoughtfully curated into dashboards that support security analysts in their daily activities. Within the Splunk framework for the GDC SOC, two distinct dashboard types play pivotal roles-- runbook dashboards and service dashboards. Runbook dashboards consolidate diagnostic metrics and essential information for a specific runbook into a unified dashboard. The purpose is to craft dashboards that specifically address the questions posed by the runbooks. This ensures a narrative alignment that maximizes incident response speed and efficiency. A health dashboard also exists for each service, which is designed to showcase the four golden signals-- latency, traffic, errors, and saturation, that is, how full the service is. Graphs representing these signals incorporate acceptance thresholds, which provide operators with clear benchmarks to promptly discern the health status of a component.

### Video - [Alerts in Splunk](https://www.cloudskillsboost.google/course_templates/1199/video/522070)

- [YouTube: Alerts in Splunk](https://www.youtube.com/watch?v=6-E15xLTnNI)

SPEAKER: Currently, alerts are set up in Splunk directly. In the near future, more advanced automation will be introduced. GDC will rely on Cortex Alertmanager to define and surface alerts to security analysts via direct integration with ServiceNow. Setting up alerts in Splunk is well documented, so we will explore the future setup now. Let's briefly recap the alerting pipeline that was presented in the previous module. First, two custom resources in Kubernetes collect metrics into Cortex and logs into Loki. Cortex and Loki have ruler components within them that define alert logic and forward alerts to the Cortex Alertmanager. Alertmanager then integrates with ServiceNow to notify you, as a security analyst, of alerts. The same is generally true for all infrastructure operators. The integration with ServiceNow is such that only two types of alerts surface-- high-severity alerts that require attention, and critical alerts that require action. An example of a high-severity alert is a threshold reaching the warning level. Examples of critical alerts include a service going offline or highly degraded system performance.

### Video - [Alerts at Cymbal Federal](https://www.cloudskillsboost.google/course_templates/1199/video/522071)

- [YouTube: Alerts at Cymbal Federal](https://www.youtube.com/watch?v=1T53PjYOZwM)

SPEAKER: Let's briefly look at how an alert is surfaced through ServiceNow by exploring an example alert for Cymbal Federal. As a security analyst for the GDC SOC, you receive alert emails from ServiceNow. The email links you to the incident page on ServiceNow. On the incident page, you can then review all the information about the alert in order to decide on the next best action via intake. How was this alert triggered? For the GDC SOC, alerts are triggered based on rules. Each alert comprises a name, an alert rule, the duration for which the conditions in the alert rule need to be true before sending the alert, and labels. Labels define severity, error code, resource-- for example, a service name-- and other application-specific characteristics. The alert is defined in PromQL for metrics or LogQL for logs within YAML scripts.

### Video - [Alert rules](https://www.cloudskillsboost.google/course_templates/1199/video/522072)

- [YouTube: Alert rules](https://www.youtube.com/watch?v=3gZ44sBPCoA)

SPEAKER: Different types of alert rules can be set depending on the logic you want to apply. Threshold alerts are triggered when a specified metric or count exceeds or falls below a predefined threshold, thus indicating abnormal behavior. Duration alerts trigger when an event or metric surpasses or falls below a specified threshold for a continuous duration, thus indicating a sustained deviation over time. Rate of change alerts are triggered based on the speed at which a metric is changing and are valuable for detecting sudden spikes or drops. Pattern matching alerts identify specific patterns or regular expressions within log entries or metric values. These are often used in conjunction with the first three alert types. Label-based alerts are similar to pattern matching alerts, but are specifically based on metadata labels. Please note that these are conceptual categories of alerts, but in practice these alerts are set in the same way. Let's look at some examples of these alerts. A threshold alert could be checking for the average CPU usage percentage going above 90. A duration alert could count error logs for a job and trigger if the number exceeds 1,000 over the last 15 minutes. A rate of change alert could check if the memory usage has decreased by more than 70% over the past two minutes, compared to the average change over the past five minutes, with a one-minute offset. A pattern matching alert could trigger if a log entry containing the pattern SQL injection is found for a job. A label-based alert uses the labels field of the alert and could check if a metric has the value warning for the severity label. The alert logic of these examples can also be combined into a more complex alert if required.

### Video - [The alert inventory](https://www.cloudskillsboost.google/course_templates/1199/video/522073)

- [YouTube: The alert inventory](https://www.youtube.com/watch?v=VSIID8fhCcw)

SPEAKER: All predefined alerts for the GDC SOC are collected within the alert inventory. The alert inventory is a collection of approximately 1,200 alerts. These alerts have been identified by analyzing communications traffic and event patterns for the GDC system, in order to develop profiles representing normal system behavior. These profiles are then used in the definition of the alerts based on anomalous and therefore suspicious behavior. Alerts are tuned weekly by applying validation, testing, and custom threat intelligence-driven indicators to the defined traffic and event profiles. The alert inventory is always growing, and the GDC engineering team is already working to introduce another approximately 1,000 alerts. As a security analyst, you can provide feedback on these alerts and ask security engineers, if available in the SOC, to tune them if needed. The alert inventory includes alerts at varying levels of severity-- critical alerts, such as brute force attack; medium severity alerts, such as suspicious process file path and Splunk protocol impersonation weak encryption configuration; low severity alerts, such as suspected network scanning. All in all, alerts track all unexpected behavior, ranging from unauthorized access and logical misuse to unexpected outages and physical misuse. Also note that meta monitoring is provided to send alerts for unexpected behavior of the monitoring stack itself, and black box monitoring is set up for third-party services. Black box monitoring probes the custom resource at the health endpoint and checks for a successful 200 response code. If the service is alive, the metric is set to 1. Otherwise, it is set to 0. As the alert inventory keeps improving over time, alerts will likely be grouped in various logical categories for easier navigation.

### Video - [Module review](https://www.cloudskillsboost.google/course_templates/1199/video/522074)

- [YouTube: Module review](https://www.youtube.com/watch?v=2M0nSEGQcUU)

SPEAKER: In this module, you were introduced to the basics of the SIEM setup for GDC. You learned about the types of logs that are collected and analyzed in Splunk, such as audit, security, and operational logs. You then looked at the pre-existing metrics that are available in Splunk. Correlating these metrics across various infrastructure components in Splunk enables you to detect usage and performance trends, as well as efficiently manage incidents before they affect system performance. You then explored some of the most common dashboard charts used in Splunk for the GDC SOC. Dashboards guide users through a narrative that reveal insights, trends, and critical information. Through the smart design of dashboards, efficient decision-making and fast actions are made possible. Finally, you looked at the different types of alert rules that can be set and how the alert inventory collects all alerts predefined for the GDC SOC.

### Quiz - [Knowledge check](https://www.cloudskillsboost.google/course_templates/1199/quizzes/522075)

## Resources

Review course content

### Document - [Course Slides](https://www.cloudskillsboost.google/course_templates/1199/documents/522076)

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/1199/documents/522077)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
