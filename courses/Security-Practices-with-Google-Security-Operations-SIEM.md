---
id: 442
name: 'Security Practices with Google Security Operations - SIEM'
type: Course
url: https://www.cloudskillsboost.google/course_templates/442
date_published: 2024-05-07
topics:
  - Deployment
  - SIEM
  - Chronicle
---

# [Security Practices with Google Security Operations - SIEM](https://www.cloudskillsboost.google/course_templates/442)

**Description:**

Learn the technical aspects you need to know about Chronicle and how it can help you detect and action threats.

**Objectives:**

* Understand Technical Concepts for Chronicle
* Learn how to use and deploy Chronicle

## Foundations of Chronicle

This module covers all information that is fundamental to working with Chronicle, covering Chronicle architecture, UI, and concepts like UDM.

### Video - [Overview: What is Chronicle, and why is it useful?](https://www.cloudskillsboost.google/course_templates/442/video/472748)

* [YouTube: Overview: What is Chronicle, and why is it useful?](https://www.youtube.com/watch?v=huAuaZ2pyVA)

[Music] [Music] welcome to this video series on chronicle in this video we will cover what is chronicle and why is it useful so let's start off by talking about what is chronicle well chronicle is a global security telemetry platform that is used for investigation and threat hunting within an enterprise network it makes security analytics instant easy and cost effective it is built on core google infrastructure and brings unmatched speed and scalability to analyze massive amounts of security telemetry it has the ability to provide threat detection and response capabilities that can help you detect and respond to modern threats at google speed and scale by ingesting multiple data sources you can quickly evaluate the alerts and understand your full attack details when you perform threat hunting using chronicle it also provides clear signals by giving you an unparalleled view into your security posture it unifies and enriches all of your security telemetry into a single timeline allowing you to leverage google threat intel and flexible rules within your data analytics so why is chronicle useful well let's consider just six key features the first being intelligent data fusion chronicle is able to map logs into a common model that enriches them automatically and then puts them together into timelines this gives you the ability to see the entire span of an attack easily and makes your investigation efforts much more effective next we have continuous ioc matching chronicle is continuously reevaluating log activity against threat intel when that threat intel changes it'll automatically reapply the threat intel against all historical activity that exists within chronicle you then have the ability to leverage google speed and scale since chronicle is built on google infrastructure it has the ability to query petabytes of data really really fast from a threat detection point of view you can leverage the power of yara which is an open standard for malware rule writing to further enhance your threat detection capabilities to detect different types of attacks chronicle is also self-managed meaning that you do not need to buy servers or databases in order to leverage chronicle this enables your team to focus purely on security operations and not hardware or database challenges finally from an economics point of view you have the ability to leverage your full security telemetry retention and analysis at a fixed predictable cost so this gives you some insight into what is chronical and why it is useful as you progress through the video series you will find some more in-depth information about the different features of chronicle

### Video - [Overview: Chronicle demo](https://www.cloudskillsboost.google/course_templates/442/video/472749)

* [YouTube: Overview: Chronicle demo](https://www.youtube.com/watch?v=eWrkLBsehco)

[Music] hello and welcome to this video on Chronicle my name is Ral and today I am super excited to show you a demonstration on Chronicle within this demo environment there is a lot of data that has been ingested in addition to quite a number of log entries Now searching through all of the data is a breeze for chronicle let let me explain to you why Chronicle leverages Google's core underlying system this same system is where YouTube Google cloud and Google search operate Chronicle operates as a peer to those systems so what does that mean for you well think of the vast amounts of compute and scalability that is available to Google search YouTube and Google Cloud that is exactly what is available to Chronicle so this makes Chronicle extra extremely fast to illustrate I will search across to ingest the data for a domain I take note how quickly I get the results back within the results Chronicle has gone and stitched together a lot of information it is showing us data about the hosts who have accessed the domain it's not just showing me a ton of logs but rather giving me information that is extremely useful within the main window we have information about the domain Chronicle has fetched W is data and augmented it into the results we can see the typical W is information along with how prevalent this domain is in relation to the data ingested let's prove it to an asset I will select the first asset and as I click on the asset notice how Chronicle shows me events related to that domain in relation to the asset accessing The Domain in this view I can see information about the asset it's IP address Mac address and so forth but I can also see the good indicators of when the specific host accessed the domain this is depicted by the green bars within the graph let's go ahead and pivot to the host by clicking on view details this will give me information specific to this host on the left hand side I have a timeline of activity related to this host and I can also take a look at the domains that the host has visited right now I'm looking at a time scape of 10 minutes but I would like to expand this to one day I can do that easily by using the slider and again Chronicle fetches the results really quickly this enables me now to quickly explore either a larger or shorter timeline related to this host's activity now as I extended the time range we have some bubbles that appear on the prevalent scale this means that Chronicle is looking at 590 events across all the paby of data and plotting the prevalence of activities across the organization in relation to this device I can further falter this prevalence by using procedural faltering as needed another great feature of Chronicle is Enterprise insights this is accessed from the menu and selecting Enterprise insights this view shows us The ioc Domain matches and the most recent alerts I can also expand the time scape to 25 days and now I can see 25 days worth of insights under the domain ioc matches we see one of the many features of Chronicle at work the feature that I'm talking about is continuous ioc matching let me explain how that works we can see that this domain was accessed we can see the source that reported the ioc and the assets that have been accessing this ioc along with the confidence and severity labels after that we see the ioc ingest time which indicates when the ioc feed was added into Chronicle for this specific domain it was added 7 months ago however take note of when this ioc was first seen that is 4 years ago this shows me that had I ingested the ioc and looked at any matches going forward I would not have known that these assets have visited the domain a while back so as you justest ioc's Chronicle will go and look at your historical data matching it to the ioc as well as any data coming in from the date of ingestion going forward when we take a look at recent alerts we can see the different alerts that have been flagged by Chronicle let's explore the first one which is suspicious download of offers that looks really interesting when I click on the alert name it'll show me all of the events related to that alert here I have details about the alert along with a timeline and as I expand the timeline I can see all of the events that match up to the detection alert here Chronicle is gone and stitched together the events that match this detection by selecting the first alert and clicking the drop- down arrow I can pivot to additional information here I can explore more details about the asset the file hashes or even the user so I'm going to go ahead and select the asset now as I explore the timeline I can see all of the events that led up to the detection and as I walk through the events notice how the prevalence graph identifies these events so the first event that we see is a process launch of aelex from here a file was opened and we can see the file name along with the path and hash of the file later there was a network connection that was made to a specific URL in this case we can also see the get request of along with the file that was downloaded now as I'm clicking through the events Chronicle is augmenting the data so that I can see information about the event and the domain Etc this domain dropped a file which was called client update.exe by selecting the event I have additional information that starts to come in from virus total that shows me that this file is a known malicious file now if I wanted to view the actual log I could expand the view and look at both the raw log and the normalized data which Chronicle leverages in a udm format this udm events I can further use to search or craft a detection rule for example if I copy the specific udm event and paste it in the search it will give me all of the events that match that file hash now I can PVE it one step further and click on view details next to the file hash this will give me a view of all of the assets where this file was seen and this gives you some insight into how you can use Chronicle for threat detection and to pervert your data to get actionable Intel on what's happening in another set of videos we will walk through threat detection in much more detail for now let's move on to the rule detection engine within Chronicle this can be viewed by clicking on the menu and selecting rules the first page that you are presented with is the rules dashboard this gives you a list of the rules that you have configured along with the activity related to that rule if I click on a specific rule name this will show me all of the events related to that detection Rule and from here I can utilize the same process by looking at the events and pting across the data the next component is the rules editor this is an area where you can build your own detection rule the detection rules are written in y format making it super easy to build let's take a look at an example this rule is a a little bit more complex and it is looking at a number of Events first it's looking at any events that relate to an executable or download next it goes and correlates this data with Excel activity and then finds different permissions via the entity graph and looks at a table context it then has a match criteria that matches over 5 minutes at a user level and calculates a risk score within the rule itself we've got a couple of options that we can set the first is making this rule a live rule meaning it will actively look for matches we can then also set it to produce alerts go ahead and perform retro hunts and more a retro hunt enables you to use the selected rule to search for detections throughout your existing data in chronicle remember that when you create and enable a new rule the rule begins searching for detections based on events received by your Chronicle account in real time you can also make use of the test rule results which ultimately allows you to test your rule against your data by clicking on The View rule detections this will take me to the detection view which will show me all of the events that match this Rule and from here I can go ahead and explore the data further we will cover the rule engine and writing of your own detection rules in a lot more depth in another set of videos Chronicle also has a dashboard functionality accessing this is done via the menu and dashboards and here we will see a set of default dashboards these are used for analysis and Reporting within the chronicle user interface the dashboards are built upon the capabilities of looker and bigquery looker acts as a visualization layer while bigquery acts as a data layer within the dashboards you can also adjust your time range the main dashboard which is what you see right now displays information about the status of the chronical data ingestion system along with a global map highlighting Geographic locations of the ioc's detected within your Enterprise the data ingestion and health dashboard provides information about the type and volume of data being ingested into your chronical account another interesting dashboard is the rule detections dashboard which provides insight into activity related to the detection engine and the configured rules since your security analysts configure these rules to search for specific threats this information might be particularly relevant to your organization so this concludes the demo on Chronicle keep in mind that this video is a highlevel overview of Chronicle and its functionality I encourage you to take a look at the rest of the videos which will dive into more depth on Chronicle

### Link - [Overview: Chronicle website](https://www.cloudskillsboost.google/course_templates/442/documents/472750)

* [Overview: Chronicle website](https://chronicle.security/)

### Link - [Overview: Chronicle help documentation](https://www.cloudskillsboost.google/course_templates/442/documents/472751)

* [Overview: Chronicle help documentation](https://cloud.google.com/chronicle/docs)

### Video - [User Interface: Structured query search](https://www.cloudskillsboost.google/course_templates/442/video/472752)

* [YouTube: User Interface: Structured query search](https://www.youtube.com/watch?v=LV_JysJr3Gw)

hi my name is winnie and i am a ux designer at chronicle the topic i will discuss and demo is called udm search when you come to the chronicle webpage you are presented with a field a search field and normally you could just type a hostname an email or a username to just search for so in order to get to udm search you just simply click on this button to switch to udm search sometimes called the structured query builder so if you click here you are now in the udm search query editor what you see here is a list of possible udm fields that you can use and if you're unaware chronicle udm is when our logs get ingested in the chronicle it gets converted into our unified data model allowing us to have structured data to search for or to write rules in it's quite daunting to see hundreds of udm fields so you can simply just type in what you might be looking for here and it'll auto complete and show possible udam fields you might be interested in to get a little bit more details on which what these fields are you can click on the right arrow here that will give you a little bit more information as well so let's go ahead and choose target hostname and we'll go ahead and see that it actually helps you build the query so you can finish building your query and you can set a date as well of the date range that you're looking for and then click query udm search will stream data in real time so you can actually see the results and analyze the data as it's streaming in for now because you're on a we're on a demo dev site a functionality might be just a little bit weird and this is a new feature so what you see here may change when you actually see it for yourself and when you play around with chronicle on your own we're adding new features to this constantly so while this is loading i actually have another one page to look at since it takes a while to load because it is a fairly large search you'll get a warning saying that you've reached a limit we we will have like a certain amount of limit just because we don't want you searching forever um you can help alleviate this by adding filters to this query so the more detailed and refined your query is the better it is in terms of like returning results that are within a range that you are able to analyze so let's pretend you want to look for maybe an event type and you want to filter out maybe a couple of these because it's a little bit too much so if you say i want to filter out network dns event types you'll see how it adds a pill to our udm filters you can clear it or you can apply it to the query and run it or you can edit it even further so clicking on the pill you can see that you can add a few more values in here as well so let's go ahead and add because the filters show this there's quite a bit let's go ahead and add a few more just to make it a little bit easier for us so now the filter is updated we could add more if we wanted to the results in here will will also update and you can see the changes applied to this bar here the bar could show you how many filters you have your original query and how many um filtered events you have as well so you can kind of compare compare how different your your values are based on your filters if you really like your filters and you now want to apply it to your query then you can rerun the whole query with these filters to get a full fresh new set of results so let's go ahead and do that so it's actually running and it's starting and and while it's loading i just want to point out that you can always change the date range here as well so now this loading is a little bit faster and you get a little bit more refined results here um you can also once you see your results and you want to analyze a little bit more you can click on any of these rows here and say we want to look a little bit more at this generic event right now it pops up our raw log viewer but in the future we plan on showing more rich detail and any context deal details like what entities are related to this event perhaps a process tree we plan on customizing it in a way that depending on the type of row that you've selected and the type of event will determine what you see in in this panel that pops up for now it's just the raw log as our first release but you will see more as we add on to it other new features that we're going to add is templates so templates when you click in here it'll provide like search templates that you can use and reuse and edit as you wish um and saved queries like if there's some query that you really enjoy using all the time you can share it and maybe eventually publish it to your peers to use as well those are just some of the features that we're planning on having for udm search thank you

### Video - [User Interface: Raw log scan](https://www.cloudskillsboost.google/course_templates/442/video/472753)

* [YouTube: User Interface: Raw log scan](https://www.youtube.com/watch?v=rh4df2Z-0kQ)

hi my name is winnie and i'm a ux designer at chronicle i'm going to show you how to do a raw log search or raw log scan normally when you do come to the chronicle site you can do any kind of search a normal search for user um an asset and if it finds a match it will go ahead and present you like possible matches here why would you use raw logs search is because if if the information you're looking for may not yet have been ingested or normalized yet in chronicle you then have the option to do the search directly in raw logs so an example would be if you happen to know there's a certain field in the raw logs maybe called user id you hit enter it doesn't find it in the existing normalized data so it will present you some other options udam search or raw log search so rollout search if you open this you can search for the you set the specific date you can also specify which log source you want to use by default it just shows all if you want a quicker search though you might want to limit it to certain log sources that you're you're wanting to search for so you would click search here now it's going to go through all the logs trying to find this string user id and it usually takes a while and here it returns some of the visits it will return events uh related to the search string that you're searching for so it sees here that you have user id um it's a string match here and an event it's tagged as an unparsed raw log if you open the raw log viewer you can see the actual raw log string and you can see the actual string match highlighted as well something else that we can do here as well is is regex so say you want to do another search right and let's go ahead and add i'm going to assume that you know regex if you don't the simple google search or chronicle documentation also has um explanation how to do regex in this example regex i'm just simply looking for like any matches of a string that has um digits with a dot followed by any alphabet uh characters as well so let's go get ahead give it a try uh and so you have to check this box here that says run query as regex and it's the same date settings that you want and again you can set your log sources you click search here and it will go ahead and do the same search through all your logs and here are your results all the events that match this search and if you again look and open up the raw log viewer you can see it it's found like a digit with some characters in here as well and and that's a a way to search through all your world logs if you can't find it through our normalized data

### Video - [User Interface: Chronicle Views (incl. IP view, Domain view, Hash view, Asset view)](https://www.cloudskillsboost.google/course_templates/442/video/472754)

* [YouTube: User Interface: Chronicle Views (incl. IP view, Domain view, Hash view, Asset view)](https://www.youtube.com/watch?v=FmX2vj20lBs)

hi my name is winnie and i am a ux designer at chronicle one of the core chronicle workflow is ability to search and find detailed information about the events and entities that you're looking at we provide various views to allow them to pivot to based on the common events or or relationships between the entities i will show you a few of our views to understand the layout of the information when the user comes to chronicle homepage they can search for anything very much like the google experience so let's go look at google.com from google.com it takes you to what we call a domain view and domain view you'll see that all of our views have very much the similar layout the very top will have detailed information about the domain and down here is what we call our prevalence graph and i'll get into that in just a moment below that is our insight cards insight will show even more detailed information about the domain that we're looking at on the left side we have a timeline of events all events that have occurred on this domain and assets all the assets have an access domain within your enterprise so within the list of assets you can also see when it was first accessed and when the last time they've accessed this domain as well and you can click and pivot to them at any time clicking in the row itself will highlight in the chart where this may have occurred so you can see this person or this machine has accessed it almost every day [Music] for the prevalence graph itself if you click on the day in the chart it can also show you all the assets that have accessed on october 3rd if you want to pivot you can click into the last access for the asset that you're interested in and now it takes us to the asset view and from the asset view you can see it's also very same layout information about the asset on the top a prevalence graph and here it shows a sub panel of domain because we had just pivoted from domain so the idea is that you can it'll automatically highlight and help you not lose context of what you were looking at previously you saw a bar chart of prevalence graph over here you see something what we call like a a bubble chart or actually like a a beaconing chart so if you click on the parent larger dot you see like the smaller dots and see how it begins over time prevalence is if if the number is high it's a very common uh occurrence if it's a low number then it's uncommon so if you see dots that are happening up here that's something you might want to look into from the asset view really there's something called procedural filtering that we have on the right side for all of our views you can expand and see properties that you can filter on or just get some sense of numbers of how many events or types of properties are involved with events that you see here on the left side you can filter them as well and you can also see a little bit more details about the event itself by opening up what we call raw log viewer the raw log viewer shows you the actual raw log and also shows you udm values fields and values as well there's another video that goes into udm and what it is and how you use it so i won't go into do much detail about it but a lot of users use this to get the information and what udn fields to use for future searches and queries the list of domains also is here under the tabs and users can same thing like click and view information on the sub panels of those domains so from here if the user wanted to they can pivot to even more information or they can continue to search we have like fake data so it's not the best example but if they want to look at userview it has the same layout except the graph is a little bit different the purpose of this graph was to show a heat map to show how often a user may log in into the machine it's a little bit of like user behavior analytics so normally you would see a heat map of a user who more more likely log in like monday through friday uh normal working hours but if you see a little outlier and they log in like at say 2200 hours on a sunday or saturday that might be a little bit suspicious and they might try to find some more information about it so they would click on that point and then see the event and see what has this been user been doing what what is going on some information about what ip target host name any information they can do to pivot off of and they can click anywhere in here as well to help pivot back into more information if they wish and just to hone in on on the whole layout i'm just going to show like ip view here's an example of an i oop ip view um same layout and same chart prevalence chart uh and inside cards each each view has a different insight card um you will see virustotal as a way to pivot outside of chronicle and into another product to do a little bit more research if you wish and ipvu has assets and domain tabs that you can also pivot to as well and filehash looks very similar let's throw in a file hash in here and there you go there's like the same prevalence graph a little time square for insights for this hash and assets and timeline of events related as well so you can see like all of these views very similar and users can pivot between them they can filter and look at more properties as well and the whole purpose is to help them just drill down help analysts drill down and follow events around related entities events view more detailed information and ultimately help complete the investigative workflow

### Video - [User Interface: Enterprise Insights](https://www.cloudskillsboost.google/course_templates/442/video/472755)

* [YouTube: User Interface: Enterprise Insights](https://www.youtube.com/watch?v=k6OnVPZ25CM)

hello you are looking at Enterprise insights to get the Enterprise insights you click on that app menu and choose Enterprise insights here in this view you will see a summary of all your recent alerts gcti alerts and oec domain matches up here is the date of when the data was received you can refresh it now or set it to Auto refresh if you wish you can also change the time range and what you're looking at and say you want to change it to the past seven days because a Year's worth of alerts may be too much for most people procedural filter is also applies here where you can filter all your alerts based on any specific properties you may be interested in for recent alerts you can click on the asset that generated the alert or you can click on the alert name clicking on alert name will take you to the rule that generated the alert there's a different video that will discuss rules so I won't get into it right now but basically it is a way for customers or rule writers to customize the type of alerts that are coming in GCT gcti is a Google internal team that helps write rules and it helps write the detections so your customers can give feedback to the alerts here so if the alerts are true positive false positive Earth is useful or the severity um and they get reflected in here and that helps the gcti team refine their roles a little bit more to make it uh more useful to the customers and finally like ioc domain matches uh you can see like the in just time and first scene and last scene it we highlight in yellow the more recent one so you're able to visually quickly see the more recent Flags um if you click on any of here it takes you to domain view if you click on first scene last scene it'll take you to the first scene and Lasting within the time frame in that domain View domain view is another review video that you can watch and learn more about how you can see more detail and information about the specific domains to determine whether or not the ilc is a something to investigate further all of these can pivot to more of the assets or the alerts to find out more information about them so this is pretty much all the overall view of Enterprise insights there are a lot of new features in the works that we plan on completely enhancing in Enterprise insights or possibly replacing it completely altogether with a newer dashboard ultimately we hope to help enhance the whole experience to help analysts complete their alert investigation and their mediation workflow

### Video - [User Interface: Dashboard Views](https://www.cloudskillsboost.google/course_templates/442/video/472756)

* [YouTube: User Interface: Dashboard Views](https://www.youtube.com/watch?v=LJIh_xqc7dU)

hi my name is panus kuyuris i am a software engineer at chronicle today i want to give you a quick demonstration of the chronicle dashboards dashboards give the chronicle user a visual overview of the status of their chronicle deployment from the landing page or any other page the chronicle user accesses the dashboards from the top right menu under dashboards each dashboard is formed from a number of tiles for instance we're currently looking at the main dashboard that has five tiles three are dynamic textiles the other two are visualizations a widget at the top left controls the time ratings it usually defaults to a reasonable number of days in the past but the user can always adjust it for instance changing the seven days here to three you see that the numbers in the text and the visualizations change at the top right the user can see when the dashboard was last updated and can refresh it the user can also refresh individual tiles by pressing clear cast and refresh at the top right of the tile chronicle dashboards can easily be used for reporting you do this from the top right menu click on download you can select the format you can select the page for example i'm going to select here a4 because i'm going to send it to a colleague in europe i'm going to pick landscape and then i click download and after a few seconds the pdf file gets downloaded and here you go comes with five default dashboards the data ingestion and health dashboard shows information about the ingestion of logs the user can see how many events were ingested whether there were any errors how they break down by log type and other useful information the ioc matches dashboard shows ioc matches over time by indicator and other visualizations like the top 10 ioc values the rule detections dashboard provides styles of analytics related to the chronicle rule detection runs in this one and in other tiles in all the dashboards there are links that you can that the user can click and land back into chronicle views for instance if i click here on the chrome browser security alert i'm going to open this in another tab so this one gets me to this page here where i can see there were six distinct detections on february 1st and 11 here on january 27th and i'm looking at this in the last three days so that's why i see only six which is this one over here the user sign-in overview dashboard provides information about user sign-in and sign-outs and includes a map for visualization of this information finally the main dashboard which is a dashboard one lens contains the most important tiles from all the other dashboards to give the user an overview of the state of their clinical deployment the chronicle user can create empty dashboards by pressing new here and add tiles or copy and modify existing dashboards the new dashboards can be in a personal dashboards workspace which is private to the user or a shared workspace shared by all the users of this chronicle customer the user can easily copy from the ones workspace to the other as a demonstration i'm going to copy the main dashboard to my personal workspace and then i'm gonna edit the dashboard for example i'm gonna go to this style and rename that as let's say of alerts and save it and i'm going to change the tile and you see when i save it here the title is reflected there of course you can do much more but i just wanted to show you in this quick demonstration an example and of course like i said you can copy that to the sale dashboard or you can eventually delete it chronicle dashboards are powered by looker a business intelligence and big analytics platform that is part of google cloud we're really excited to share this power with our chronicle users and we invite you to create your own explorations and dashboards you can find more information in the chronicle documentation under dashboards user's guide and also as you deep as you dive deeper into what you can do with look here there is a lot of useful information in the lookup documentation thanks for watching and happy dashboarding

### Video - [User Interface: Rules Views, Rule Dashboard, Managed Analystics,. Rule Editor](https://www.cloudskillsboost.google/course_templates/442/video/472757)

* [YouTube: User Interface: Rules Views, Rule Dashboard, Managed Analystics,. Rule Editor](https://www.youtube.com/watch?v=gmVLkEvCse4)

hello i will be talking about chronicle rules rules are written to customize detection and alerts from anywhere within chronicle you can view your rules dashboard from this main menu drop down here clicking on this option for roles will take you to the rules dashboard there are three tabs up here dashboard editor and manage analytics and we're going to go over all three of them for the roles dashboard this is where you can see the status of all your roles basically if it's been running live if the learning is turning on and off and activity for the past four weeks the interesting thing for the trend in activity is where you can see if there's been any lulls or spikes that you might want to look into and if there's like too many number of detections you might want to edit your rule so it's not detecting as much this column here shows how many events that have been hit today and if you want to change the learning status and the run frequency and the live status you can do that here there's a drop down choices for actions here where you can set the rule live and when you're setting it live you're essentially saying run the rule and give me the detections if you set the alerting on the detections become alerts so an example if you're just writing a brand new rule and you just want to test it out you could turn alerting off and turn live roles on so the detection will keep happening but it's not alerting anyone but once you're happy with your role and you do want to be alerted by it then you can turn alerting on the other options are retrohunt edit rules and you can view the actual details of the detections themselves and finally how to archive a role when you archive a rule you basically put it in this limbo state it's not going to be seen anywhere here but you can un-archive it later in the rules editor and i will show you that in a minute let's look at one of the rules so if you're looking at any of the if you're interested in any of these rules you can do the detections or you can simply just click on the rule name here this takes you to the detection view the detection view is similar to all our other views in terms of layout you will see the detection name at the top and a quick summary of like what this rule is about based on the metadata that has been provided on the left you'll see a timeline of the detections and the detection charts so you can see daily how many detections were counted and hovering over it gives you the actual numbers below here is it'll show you if there had been any retrohunts ran you can run ritual hunts these are the same rule options that you saw earlier in the dashboard you can run retrohands this way you can click retrohunt and choose a time frame that you want to start running it so if you want to set it to like say just a couple days three or two or three days and you run it it'll start running for that past time so normally when you write a role and you start enabling it live it'll start running from today onwards but if you want to run it for like last week or the week before is when you would run retrohunts when you want to edit your role you can edit the rule here it'll always warn you if you ever leave rules editor in a bad state like you close your browser there will be a pop-up that said hey there was some unsaved changes do you want to save it and that's what you saw earlier and here's the tabs that you saw earlier as well we could have just clicked on rules editor to go directly to rules editor but in our case we came from a detection and it will automatically highlight the rule that you came from and you immediately see the rule not logic now you can edit the rules here and there will be another video that talks about how to write a role so i won't get into details about that but you can test your rule here so if you ever wanted to test the rule without it ever affecting any of your results and say you just made some testing changes and you just want to test it before saving it you could run a test here and you'll see like a mini preview of the detections that were found from running this role from the time range that you set here this is different than retrohunt retro1 actually runs it and and keeps the detection in its records running a test once you close this window it'll go away you'll never see it again it really truly is just testing a role uh the same options again lives in here where you can set rule settings here and run frequency here one different option is that you can um duplicate a rule so if you really like this rule and you want to duplicate it you can essentially click duplicate and it'll create another role with that with the same logic and then you can edit it and that way if this rule is currently running which it is right now you're not you're not ruining any of the results another thing you can do is view versions of a rule so if i wanted to view a version of this rule i'm going to discard the change edits here i can either click this version history to see what was last saved or you could see like right here view versions so if i click versions at the last view history it saved versions i'm now in viewing versions mode and from here i can click on any of the past versions that i had saved to view it and you can also promote it and say i want to make any of these passwords and it's my now current role you can also compare versions as well and and you can still also test the role itself if you want to say see if the detection numbers are any different from a past version from a current version if you compare the versions you can quickly see like what was different from the previous version so you can always compare to the current version and see what changes there are in this example it's only some simple metadata um one thing i want to point out is like any time you save a new rule and create a new version all the detections will go back to zero you won't see any detections because we wanted to avoid the confusion of like version results so if you create if you edit a rule to a point where the detection numbers are drastically changed it won't make sense to see the numbers in the past to then count in the past it will just possibly cause confusion but anytime you want to view it doesn't go away if anytime you want to view a past previous versions detection you just simply click on one of the past views and say view detections so you will always see the detections living in the history that it was generated from one last one we went over rules dashboard rules editor and now i want to go over like a last tab the manage analytics this is new we've just introduced this this year and what it is is that we have created policies which are basically a set of pre-configured rule detection logic that was created by our own internal google team you can view the policy uh here well you can view like the settings here and change the settings here as well and you can see what within the minor attack framework tactics and techniques it covers as well and it will generate alerts if you turn on alerting and for both managed analytics and for rules that you have a living turned on you can now see them alerting in recent alerts for enterprise insights and you can see the sources here when it updates like where it came from like in this these examples are all from chronicle rule and the alert name is actually the rule name so clicking on here will take you back to the world's detection view and in full circle you can start like examining the details of the detections you can view the rule logic in rule editor and just overall just go back and forth and and observe all the detections that it found and you view any specific details so overall in general rules basically allows customers to have the power and flexibility to customize their detection and alerting workflow

### Video - [Other Fundamental Chronicle Concepts: UDM Overview](https://www.cloudskillsboost.google/course_templates/442/video/472758)

* [YouTube: Other Fundamental Chronicle Concepts: UDM Overview](https://www.youtube.com/watch?v=xu1p8Cfh9xs)

hello my name is Dave Harold and I'm a principal security strategist here at Google Cloud in this video we'll provide an overview of the chronicle unified data model or udm in this video we'll talk about the need for normalization when performing security Analytics we'll show you how udm normalization Works in chronicle and we'll give you some examples of udm events and entities section one the need for normalization you've no doubt heard it said before security is a data problem and it's true large organizations routinely gather multiple terabytes of security data each and every day and this mountain of data contains the record of nearly every activity that has taken place within an organization systems networks applications and Cloud environments security teams can use this data for everything from investigating past security incidents to hunting for adversaries acting in their environment today to alerting when malware is executed to detecting Fraud and Abuse or even just demonstrating compliance there are real challenges standing in the way of using this security Telemetry one of which is Simply Having the scale to collect store and process these enormous volumes of data now with Chronicle we have the power of Google's infrastructure and Engineering talent to solve that part of the problem but collecting and storing data is just one part we also need to make sense of that data here's what we mean by that most large-scale data sources are structured in some way they have schemas that Define rows and columns field names and relationships between records but security data is often not like that instead it's unstructured or multi-structured While most security data is formatted in some way every source of data can use a different format we might have Json CSV XML key value pairs or even just plain text log entries and it's not just that the formats are different when we do have field names those field names can vary widely from one data source to the next here's an example in this case we have a raw log event from a Palo Alto Network's firewall and a rawlog event from a Fortinet fortigate firewall in both cases the firewalls blocked a connection but you can see that the log that they created are quite different we have difference in formatting that's evident in the timestamps and we also have different names for fields that are clearly the same if you were trying to write an alert or do an analysis of firewall blocks you can see it would be nearly impossible because there's no consistency across devices and it's not just a problem for firewall logs this occurs with nearly every type of security Telemetry That We Gather it's pretty clear we need a common way to represent security data and it starts by normalizing field names as shown here in this scheme regardless of the type of firewall that blocked the traffic we have a unified view of what happened the normalization also standardizes the values of fields IP addresses are strings ports are integers and things like protocols can be enumerated into a list of known standard protocols let's talk quickly about normalization in chronicle it's important to be very familiar with udm because that is how security Telemetry is represented in chronicle while it's important to maintain an awareness of the raw logs most of your analysis will be on the fields and the values defined by udm let's take a look at how Chronicle approaches normalization here we have the same diagram with a few additions note the introduction of parsers parsers are a component within Chronicle that convert raw logs generated by things like firewall devices into udm once the data has been normalized it's available for detection rules for udm search dashboards and really any analytics inside of Chronicle here's an example this is a screenshot of udm search results from Chronicle in this case the raw log was created by a security product called tanium if you note the portions highlighted in red on the left you'll see the Rawl log just as titanium created it and as Chronicle ingested it and then on the right you see the udm event which is fully normalized there are many benefits to normalizing your data to udm in chronicle first of all normalized data can be indexed and that means that udm searches and Chronicle are lightning fast it also provides a common language for security Telemetry so your analysts will be speaking the same language as they work through security investigations normalization allows you to standardize and simplify detection rules and Analytics product specific terminology that fills raw logs is abstracted away simplifying the creation and maintenance of detection content by normalizing your data you can also take advantage of shared content repositories these content repositories are huge productivity enhancers because you can take advantage of rules that are written by people all around the world these rules are typically expressed using udm by normalizing your data you also avoid expensive search Time Field extraction that's found in some Legacy Sims in chronicle data is normalized once at ingest time and then you take advantage of that over and over and over again udm also supports entity data and entity relationships for contextual enrichment The Entity model which we haven't even talked about yet allows Chronicle to understand details about users and systems and applications and this contextual data is then used to enrich views in chronicle and to write better detection rules Chronicle makes normalization easy first of all udm is comprehensive it's structured to capture Complex events and entity data Chronicle ships with a large default parser Library these are parsers that cover hundreds of different security tools if a parser needs to be augmented in the field parser extensions allow customers to easily add an additional additional field normalization custom log formats are supported for partners and customers who might need that and raw Log search is available within Chronicle as well if you'd like to search against the data just as it was created by the source system now let's take a deeper look at some udm events here are some common components of udm events first of all you have an event type this is stored in the metadata portion of the udm event and its values allow you to represent common activities such as process launch user login file modification and network connections next you have the event timestamp also stored in the metadata portion of the udm event and this is an RFC 3339 compliant time stamp that allows you to understand when this event occurred as reported by The Source system udm events contain nouns nouns are the entities that either act or are acted upon during a security event common nouns are principle Target Source intermediary Observer and about you'll see these represented as things like principle.hostname principle dot user ID Target process.file.md5 these are all characteristics of nouns which help you tell a story from your security data most udm events contain a security result the security result tells you what happened during this event it can contain things like allow or block or any number of different common actions based on the event type your udm event will have additional required and optional fields as we wrap up our introduction to udm let's spend a few more minutes looking more closely at nouns in udm a noun represents a participant or entity in a udm event things like the device or user that performed the activity or the device or user that is the target of activity nouns can also describe attachments or URLs and nouns can represent other devices things that may be observed the activity but that were neither The Source nor the destination they can also represent an intermediary device or a component like an SMTP email relay recall that udm uses these nouns commonly principle the acting entity or the device that originates the activity this is perhaps the noun that you'll use most when searching and writing rules in chronicle Target is the device being referenced by the event or an object on the target device sometimes a udm event will contain a source noun that's the object being acted upon by the principle along with the device or process context for the source object oftentimes there is no Source noun and instead principle stands in its place but for some events you need both a principal noun and a source noun the intermediary as mentioned earlier is an intermediate device such as a proxy server or an SMTP email relay Observer is a device like a packet sniffer or a network-based vulnerability scanner and finally the about noun represents objects that are not otherwise described in the udm event

### Link - [Other Fundamental Chronicle Concetps: UDM Help Center Documentations](https://www.cloudskillsboost.google/course_templates/442/documents/472759)

* [Other Fundamental Chronicle Concetps: UDM Help Center Documentations](https://cloud.google.com/chronicle/docs/unified-data-model/format-events-as-udm?hl=en)

## Collecting and Parsing Data

This module will teach you all of the ways in which data can be ingested into Chronicle and how parsers and the syntax behind them are used to normalize this data. Includes topics such as: Chronicle Forwarder, 3rd Party API Feeds, and the Ingestion API.

### Link - [Getting Data: List of Supported data / log sources](https://www.cloudskillsboost.google/course_templates/442/documents/472760)

* [Getting Data: List of Supported data / log sources](https://cloud.google.com/chronicle/docs/supported-datasets)

### Video - [Getting Data: Methods of ingestion data into Chronicle](https://www.cloudskillsboost.google/course_templates/442/video/472761)

* [YouTube: Getting Data: Methods of ingestion data into Chronicle](https://www.youtube.com/watch?v=uUZ5Tn-_k8g)

welcome to the video in this session we're going to discuss the various methods of getting data into Google Chronicle the goal is for you to be able to describe how to get data into Chronicle and understand the use cases for each method the four methods of getting data and which we'll talk about here are using the chronicle folder pulling data from cloud sources pushing data into Chronicle through the ingestion API and directly fetching log data from Google Cloud first we're going to talk about the chronicle folder this is primarily used for Gathering data for a non-premises source but could equally be deployed in Cloud environments to gather data from systems there the folder is primarily deployed as a Docker container running on Linux VM although a Windows version also exists once deployed it supports the collection of a few different types of data most commonly used as syslog via TLs but it can also read from Kafka topics ingest Peak up data for DNS or DHCP data read local files on the hosting OS and query Splunk directly to ingest data it's possible to deploy multiple folders behind a load balancer in order to provide High availability and load balancing for pushing data and gesture requests to Chronicle more details on this can be found in the public folder documentation the folder is configured using a yaml file which defines the connection it will make to Chronicles ingestion API and also the various collectors to be configured on the folder with each of these being configured as a specific log type and ingestion method customers or Partners can modify this to add or remove data types and log sources each time the configuration is modified the container should be restarted to ensure the new configuration is Reloaded next we will discuss suggesting from cloud sources today's Enterprise environments have many sources of data which are either exposed event data directly to the internet secured through rest apis or natively dump data to cloud-based storage systems such as Google Cloud Storage for these sources it is much simpler for administrators to not have to deploy any infrastructure but to fetch directly from such sources Chronicle features the ability to ingest data directly from cloud storage such as GCS S3 or Azure blob storage as well as for rest apis or https endpoints used for cloud-based systems such as Microsoft 365 Azure active directory OCTA and many others the requirements for configuring these data sources vary by The Source but typically you need to configure an API token or credentials for programmatic access these types of data sources can be configured through the chronicle UI by an administrator by navigating to the settings and then feeds the add new button can be used to select the log type and enter credentials for and any other details needed for ingesting from that Source the details will then be validated and data will automatically be ingested on a fixed schedule Google Chronicle provides a full set of rest apis allowing you to query data within the system manage detection content configure the system and of course to get data into the system we'll now discuss the ingestion API and how it is used the ingestion API presents a way for technology partners and mssps to send data directly to Chronicle without using forwarders or Cloud Source feeds this can simplify the data ingestion process and remove the need to manage and maintain configurations or credentials the adjusted API provides four distinct endpoints which are authenticated using an oauth token these allow you to post data either in an unstructured log format letting Chronicle parse the data into its unified data model or udm format or is pre-formatted udm to create events or entities within Chronicle the final API endpoint returns a list of valid log types to assist with identifying programmatically which logs are currently supported the final method of ingestion that we'll discuss is direct ingestion this only applies to Google cloud data at this time and helps join Google cloud and Chronicle customers to get their data into Chronicle in a much simpler way recently introduced to the Google Cloud console you can now directly attach your Chronicle instance to your Google Cloud subscription with just a couple of clicks and automatically enable a collection of data from security Command Center Cloud audit logs and Cloud asset inventory filters allow you to further tune the data coming in to include or exclude specific projects or log types this provides Google Cloud customers with an extremely easy way to onboard their infrastructure security logs into Chronicle so we talked about the four main methods of getting data into Chronicle these being via the folder ingesting from cloud sources directly feeding in data through the ingestion API and directly ingesting login entity data from Google Cloud more information on the topics found here can be found in the accompanying documentation and on the public Chronicle documentation site

### Document - [Getting Data: How to guide for ingesting AWS Logs into Chronicle](https://www.cloudskillsboost.google/course_templates/442/documents/472762)

### Link - [Getting Data: Feed Management API](https://www.cloudskillsboost.google/course_templates/442/documents/472763)

* [Getting Data: Feed Management API](https://cloud.google.com/chronicle/docs/preview/feed-management-api/feed-management-api)

### Link - [Getting Data: How to guide for troubleshooting Forwarder issues / monitoring Forwarder health](https://www.cloudskillsboost.google/course_templates/442/documents/472764)

* [Getting Data: How to guide for troubleshooting Forwarder issues / monitoring Forwarder health](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/Forwarder%20FAQ%20(go_forwarder-faq).pdf)

### Link - [Getting Data: When to use the Ingest API vs. the Feed Management UI or Forwarder](https://www.cloudskillsboost.google/course_templates/442/documents/472765)

* [Getting Data: When to use the Ingest API vs. the Feed Management UI or Forwarder](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/%5BChronicle%5D%20When%20to%20use%20the%20Ingestion%20API%20(1).pdf)

### Link - [Getting Data: How-to guide: Overview Ingest API with example configuration](https://www.cloudskillsboost.google/course_templates/442/documents/472766)

* [Getting Data: How-to guide: Overview Ingest API with example configuration](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/%5BChronicle%5D%20How%20to%20use%20the%20Ingestion%20API.pdf)

### Link - [Getting Data: Help Center on Ingestion API](https://www.cloudskillsboost.google/course_templates/442/documents/472767)

* [Getting Data: Help Center on Ingestion API](https://cloud.google.com/chronicle/docs/reference/ingestion-api)

### Video - [Parsing data: Overview of writing parsers](https://www.cloudskillsboost.google/course_templates/442/video/472768)

* [YouTube: Parsing data: Overview of writing parsers](https://www.youtube.com/watch?v=QiXGcRS6mGo)

welcome to the video in this session we're going to provide an introduction to writing passes for chronicle we will talk about why parses are needed what the chronicle ingestion data pipeline looks like and an overview of the structure of parsers and finally talk about other resources which can help you get started with writing parsers parsers are used as part of the ingestion pipeline in chronicle their primary function is to convert roll logs which could be in any number of formats into our unified data model format this normalization process helps to ensure that the data from the log is in a format which can make search and detection logic standardized across log sources meaning that searching for or detecting an outbound network connection will be broadly similar regardless of whether the source is a firewall web proxy or endpoint an independence of the vendor or technology being used as we have said logs could be in one of multiple formats typically these will be in some type of syslog format and may include one or more of key value pairs Json data or XML data extracting data from these messages and mapping them into EDM becomes something which is fairly unique per log type and so having a catch-all passing methodology for all log types is not practical Google Chronicle provides over 500 out of the box ready to go parsers but where a customer needs to ingest data from a log source which does not have a parser already one will need to be written this can be done by Google by a partner or by the customer themselves when data comes into Chronicle but the Via a folder directly to the ingestion API or through a feed each log is queued to go through the parsing and indexing process the parsing process takes the default parser if available or a custom parser if one has been provided and will run through each log entry in the parsing process defined in the parser extracting data and mapping it into udm the final part of the process is that parse data will be enriched automatically and indexed ready for search and detection use cases Chronicle pauses are based on the log stash style syntax which includes many of the common functions available in logstash for splitting and writing data for those of you familiar with logstash syntax in chronicle we use only a single filter section we do not use inputs or outputs in our parsers there are additional materials available which go into the available syntax and examples of how to build parsers and references to these are included at the end of the video the overall workflow of a parser is firstly taking the log itself and using functions like grok Json KV and XML to split out the interesting parts of a message and deal with any data structures which exist within the message the next stage is to manipulate this data where needed to do that we can use logic or conversion methods to get the data into the format we need it finally we map the data into udm and write out the event Chronicle if something goes wrong in this process because the data is not in the expected format or up ours as an issue we may end up with an unpared log this is where we can still access the log entry within Chronicle for search but the parse udm data is not included within the message in these cases we can always fix the parser and replay the data through it again more details on this are included in additional materials on troubleshooting the buzzing process references to these are included at the end of the video so we've talked about what parsers are for why we need them what the data ingestion pipeline looks like for chronicle with respect to parting and covered some high level steps on how parsers work and the language they use there are a number of materials available to help you with learning how to write parsers and how to troubleshoot when things go wrong

### Video - [Parsing data: Parser API overview](https://www.cloudskillsboost.google/course_templates/442/video/472769)

* [YouTube: Parsing data: Parser API overview](https://www.youtube.com/watch?v=vH7isb2EkX8)

welcome to this video where we will talk about the chronicle powers of API we will discuss what it is for what you will need to access it which endpoints are available the life cycle of positive development using these endpoints and finally the tooling available to make using the parser API easy parsing data is a critical function in many Sim products getting the data in and making sure it's in a usable format sets custom result for simpler search and for a successful detection engineering experience there will be times when customers or Partners need to build custom parsing logic in order to meet their requirements and the parser API allows them to manage these customizations in a standard and Safe Way there are some requirements in using the parser API you will need a Chronicle instance in a credential with access to the relevant API to start using the API your Chronicle representative will be able to help with ensuring the account you need has been configured and that you have the credentials required to access it there are a number of API endpoints available to assist with the various functions of the passer API these are detailed on this page and can be used directly to access the various API endpoints we'll not talk in this video about authentication with the parser API but this is carried out in a similar way to the other Chronicle apis and is discussed in a separate document available in this training module in terms of building or modifying passes the steps in developments are described here note that each of these steps have equivalent commands using the parser development tool which we'll talk about in the next slide first thing is to generate some logs if there are logs already being sent into your critical instance you can use the endpoint shown to pull a set of logs from your Chronicle instance the next step is to either get the current parser or if you're starting from scratch to start building the new parser instructions on how to do this are outside the scope of this video which are available as part of the wider training materials we next use the API to test our parser with our sample logs and this will play each of the sample logs through the API with the parser you provide and return results telling you if the parsing was successful or not and what the output of the parsing is we now run through a develop test Loop to further develop and test the parser until we're happy with the output once we're content that the power parser is working as we expect we can submit it via the API at this point the API will run the parser against the number of logs in the customer environment making sure there are no unexpected browsing errors and either feedback any issues or a success message via submission endpoints the apis we've discussed provide the backbone on how to manage the life cycle of custom parsers the simplest way to use these is to use a CBN tool which is a python tool available on GitHub with this tool you simply pass your credentials file and the command you want to use and the script takes care of making the API calls the tool can be downloaded along with a company documentation from the links shown on the slide here are some additional materials available as part of this module which can help with building parsers and using the apis for submission of parsers thank you for listening

### Link - [Parsing Data: Supported Default Parsers](https://www.cloudskillsboost.google/course_templates/442/documents/472770)

* [Parsing Data: Supported Default Parsers](https://cloud.google.com/chronicle/docs/ingestion/parser-list/supported-default-parsers)

### Document - [Parsing data: When to use default parsers](https://www.cloudskillsboost.google/course_templates/442/documents/472771)

### Link - [Parsing Data: How-to: JSON parser example guide](https://www.cloudskillsboost.google/course_templates/442/documents/472772)

* [Parsing Data: How-to: JSON parser example guide](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/%5BChronicle%5D%20How%20to%20parse%20JSON%20data.pdf)

### Link - [Parsing Data: How-to: KeyValue example guide](https://www.cloudskillsboost.google/course_templates/442/documents/472773)

* [Parsing Data: How-to: KeyValue example guide](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/%5BChronicle%5D%20How%20to%20parse%20key-value%20data%20(1).pdf)

### Document - [Parsing data: How-to: GROK example guide](https://www.cloudskillsboost.google/course_templates/442/documents/472774)

## Access

This module will teach you how to access Chronicle and learn about roles, admin views, and data lakes. Partners will also learn how to create API keys for customers.

### Video - [Authentication: How to configure IdPs, using GCP as an example](https://www.cloudskillsboost.google/course_templates/442/video/472775)

* [YouTube: Authentication: How to configure IdPs, using GCP as an example](https://www.youtube.com/watch?v=_UzTWVQAU10)

welcome to the video in this session we're going to configure a saml application in our identity provider for integration into chronicle as a single sign-on provider in this example we're going to be using google cloud identity as our sso provider but you could use any saml 2.0 compliant identity provider including octa azure active directory jumpcloud and more we will create an application in our identity provider and this will allow us to download an xml file containing details of our samoa application this will then be uploaded to chronicle which will allow your organization to log in using your corporate credentials role-based access control or our back in chronicle allows you to configure individual users or groups with different levels of access to chronicle allowing a variety of users to access the system while ensuring their privileges are set appropriately for their role to create our saml application i'm going to log into our google admin console which contains our cloud identity users and groups we have some groups pre-created here these are admin editor and viewer we're going to send these group details to chronicle and our sample response if the user is a member of one of them but note the functionality to do this in cloud identity is in preview right now for other identity providers the process would be similar in our admin console here we go to apps and then web and mobile apps to create our application in here we will click add app and select add custom saml app here we may give the app a name description and add an icon if we want i'm just going to call my app chronicle demo right now and then click continue we can click continue at the next screen as we'll grab our metadata file once we've finished creating our application on this screen we'll just need to enter our acs url and entity id which i'm going to type in here these will be provided by google or by the partner creating the chronicle instance will be similar to the values i'm typing in here once that's done we can click continue again to move on on this screen we'll only see the group membership option if we're opted into the preview but here i'm going to add the three groups we talked about earlier and i'm going to map them to the group attribute name and click finish to set up finish setting up our application now we can click here to download the metadata and then we'll click the download metadata link and our xml file will be downloaded into our browser we can now give this xml file to google or to the partner creating the instance so now we've created our saml application and this has been configured in our chronicle instance we'll go over to chronicle where we can see our login to our application so if i go over here we can see up here that i'm logged in with my google credentials from our sso provider now if i go over to the settings page and look at my profile i can see my identity is being passed here and i can see the groups that i'm passing by assemble as well if i don't see any groups here it could be that my account is not in any groups or i misconfigured the section where we set up groups to be passed in the summer response now if we go to the users and groups tab we can go and add users or groups with the relevant permissions within here once we have our users and groups set up we can change the default role up here for anyone to access the application who doesn't fall into one of the users or groups we've added to in this section here this will help to prevent that they don't get prevent them from getting full administrative permissions within chronicle so here we've shown how to set up a saml application in our identity provider for use with chronicle integrated this application to provide single sign-on and shown how this can then be integrated with role-based access control for more information on how to configure specific identity providers for chronicle such as octa or active directory please see the accompanying documentation for this video

### Link - [Authentication: How to guide for configuring Okta IdP](https://www.cloudskillsboost.google/course_templates/442/documents/472776)

* [Authentication: How to guide for configuring Okta IdP](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/%5BChronicle%5D%20How%20to%20configure%20Okta%20as%20a%20Chronicle%20IdP.pdf)

### Link - [Authenication: How to guide for configuring Azure IdP](https://www.cloudskillsboost.google/course_templates/442/documents/472777)

* [Authenication: How to guide for configuring Azure IdP](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/%5BChronicle%5D%20How%20to%20configure%20Azure%20Active%20Directory%20as%20a%20Chronicle%20IdP.pdf)

### Link - [Authenication: How to guide for configuring Cloud Identity IdP](https://www.cloudskillsboost.google/course_templates/442/documents/472778)

* [Authenication: How to guide for configuring Cloud Identity IdP](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/%5BChronicle%5D%20How%20to%20configure%20Google%20Cloud%20Identity%20as%20a%20Chronicle%20IdP.pdf)

### Video - [Authorization: Role Based Access Control overview](https://www.cloudskillsboost.google/course_templates/442/video/472779)

* [YouTube: Authorization: Role Based Access Control overview](https://www.youtube.com/watch?v=osoIPVaYQ-E)

foreign [Music] [Music] so first of all what is our back our back stands for role-based access control and it allows customers to control which Chronicle features each user has access to based on their role it has been a top request from our customers and it also facilitates compliance our back is currently GA with a caveat that the customer needs to be using the Federated off all right with this let me go ahead and show you our back in action in this window I'm logged in as test at gandalfservice.com if we go to the settings page we can see this user's profile their ID the groups they belong to and that currently they are assigned the role of administrator as an administrator they have access to the rbac controls for example they can see users groups and the roles that are assigned to each of them and they also can see the standard roles that we have and the permissions that each role has we currently have four roles with the administrator being the most permissive and the viewer with no detect access being the least permissive one okay now let's go ahead and change some role assignments if we go back to this page the users and groups and here I'm interested in one particular user which is this one our back test one at gambleservice.com right now they have the role of default assigned to them so what is the default rule the default role is the role that the user gets if they don't have any roles assigned to them this can be set in this top right corner and currently the setting is so that the default role is the administrator role however this has been done just for the demo purposes and in fact we expect our customers to use one of the two viewer roles as their default rules all right to start with let's go ahead and see what this other user's experience is like in this window I'm logged in as our back test1 at gandalservice.com if we go to the settings page we can see their profile so we can see that currently their role is the default role which is an administrator and then as an administrator they have access to the same features as the other account that we saw before so looking at their profile I note that they belong to the group called arbag and thinking more about this I think I would actually want all the users belonging to this group to be viewers so not just this user but all of the users belonging to this group let's make this happen so we go back to the first user the test at candleservice.com user and we can see that there is currently no group assignment to the rbac group so let's create one we click on this button here and we're going to create a role assignment for the our back group and as I mentioned I would like them all to be viewers okay we click on assign now we can see that there's the rbac group and the signed role is the viewer so let's go back to our rbac test one account and let's reload the page all right so now we can see that this user's role changed to be the viewer because they belong to the rbat group and then as expected since they're a viewer and no longer the administrator they don't have access to the rbac controls to further verify that the reviewer is supposed to say editor we can go to the rules editor page and here we can verify that this user can create the new rules and also they cannot modify the existing ones so this works just as expected all right with that I'm going to stop here and recap what we've covered so first I introduce to you what our back was and I mentioned that it is currently ga then I demonstrated to you the rbac tools that an administrator has access to including showing you the roles that we currently have the default role and then I demonstrated how the administrator can set up a role for a new group thank you for your attention and I hope you enjoy using our back

### Link - [Authorization: Help Center: Role-Based Access Control (RBAC)](https://www.cloudskillsboost.google/course_templates/442/documents/472780)

* [Authorization: Help Center: Role-Based Access Control (RBAC)](https://cloud.google.com/chronicle/docs/administration/rbac?hl=en)

### Link - [Authorization:Help Center: Roles and permissions](https://www.cloudskillsboost.google/course_templates/442/documents/472781)

* [Authorization:Help Center: Roles and permissions](https://cloud.google.com/chronicle/docs/administration/rbac?hl=en#roles_and_permissions)

## Building Rules to Find Threats

This module will teach you how to detect threats using rules written in YARA-L, share common examples, and showcase how to use the Chronicle UI.

### Video - [Rules overview](https://www.cloudskillsboost.google/course_templates/442/video/472782)

* [YouTube: Rules overview](https://www.youtube.com/watch?v=TTfc6Ur99sQ)

[Music] [Music] hello and welcome to this video my name is richard pelay and in this video i'm going to give you an overview of the detection rules within chronicle the objectives of this video is to give you an understanding of the constructs that make up a detection rule and then we're going to go and actually dissect a simple rule splitting it up into the different constructs so that it's easily understandable if you think about traditional seam rules that exist today these tend to be based on old type of attacks for example attack throw an alert is that really helpful maybe to a certain degree but today the overwhelming majority of threats are malware based and malware is subtle and complex write in a rule that looks for a hash of a particular file that comes from any of then alters a specific registry key is not really what seams were designed to do or detect many of the miter attack scenarios have this type of complexity so how are you going to detect them well this is where yara comes into play yara is a great language that is used widely and it's an open source system for malware and other threat rules within chronicle we make use of yara more specifically yara l we have modified yara for event logs and embedded it into chronicles so that you can write rules that work on modern attacks in real time or retrospectively against your historical data so how do you go by constructing a rule well a rule in chronicle is essentially split into four key sections the first is the metadata section or the meta section which really uses some key values that you define to describe the rule next we have the events and this is where you would actually define the various type of events that you are looking for then you've got the match category and this could be optional as well but essentially it gives you the flexibility to define match criteria for example over time or if you are running a multi-event correlation then lastly you've got the condition this describes the events that must match and any aggregates that you may define so let's put this into perspective right now you are seeing a single event match rule and it's a really basic rule but it will help you to understand the various type of constructs that exist within a rule the first one being the header and footer which really includes the rule display name then you've got your meta section and this is the key value pairs that you use to identify for example the author of this rule you could also leverage the meta section to specific miter attack techniques then in the events section this is where you would define your various events that you are looking for in the rule for example here we are looking for any event where the principal user id is a specific username and then we've got the condition which essentially matches events that you define in the events field so now that we have covered an overview of the detection rules let's dive in to actually write in a detection rule in the next set of videos you will learn how to write your own detection rule and how to reference other detection rules that already exist to help you enhance your detection capabilities

### Link - [Help Center: Rules dashboard](https://www.cloudskillsboost.google/course_templates/442/documents/472783)

* [Help Center: Rules dashboard](https://cloud.google.com/chronicle/docs/detection/view-all-rules?hl=en)

### Video - [Rules Engine overview](https://www.cloudskillsboost.google/course_templates/442/video/472784)

* [YouTube: Rules Engine overview](https://www.youtube.com/watch?v=8XTAb_quTUI)

[Music] [Music] hello and welcome to this video on the rules engine within chronicle the objectives of this video is to give you an overview of the rules engine and to walk you through the ui of the rules dashboard and editor oracle detect comes with a powerful and easy to use rules engine that enables modern threat detection capabilities at scale in this video we're going to look at what the rules engine is and what we can use it for i'd like you to think about rules engine as a platform where you can run rules without having to manage the execution storage or underlying machines yourself the rules engine executes your rules on your environment's data and stores the detections along with their underlying events for later investigation these rules can be run on all of your udm data offering comprehensive coverage so let's take a look at the rules dashboard right now i'm on the chronicle page and i'm going to navigate to rules by expanding the menu and selecting rules now the rules dashboard is the control plane for your rules here you can see an overview of your rules and their detections found over time important raw metadata such as the rule severity and author is also shown here this dashboard lets you manage the lifecycle of your rules you can enable your rules as live to continuously run them over incoming data and you can run a retrohunt to scan over old data if you want to be alerted for detections based on a rule you can set that rule to be in alerting mode this causes future detections to show up as alerts under enterprise insights you'll also be able to receive these alerts from the chronicle detect api if your rule is no longer useful and you want to hide it you can also archive that rule let's move to the next step where we will see the rules editor the rules editor is an integrated development environment for your rules here you can interactively write and test url rules the editor also provides helpful development features like syntax checking syntax highlighting and code autocomplete now when it comes to prototyping your rule it is often useful to test it before publishing this is where the test rule feature comes into play test rule allows you to run your rule in real time and view detections right from the rules editor you can even see and compare past versions of your rules along with their associated detections let's take a deeper look at an individual rule and its detections from the rule detections page we can see the rule detections and errors you can easily drill down into individual detections to see all of the events that make up that detection you can even export these detections and events as csv these events are available both as raw logs and in normalized and enriched formats helping you contextualize the activity some rules may have too many detections we can quickly filter through them using procedural filters which allow you to filter on any udm field value so to sum up this video the rules engine is a powerful tool with many features in the next set of videos you will learn how you can use the rules engine to write your own detection rule

### Link - [Help Center: Rules editor](https://www.cloudskillsboost.google/course_templates/442/documents/472785)

* [Help Center: Rules editor](https://cloud.google.com/chronicle/docs/detection/manage-all-rules?hl=en)

### Video - [Demo: Building a YARA-L Rule](https://www.cloudskillsboost.google/course_templates/442/video/472786)

* [YouTube: Demo: Building a YARA-L Rule](https://www.youtube.com/watch?v=odDzXLPUNfg)

[Music] [Music] hello and welcome to this video my name is richard ballet and today we're gonna focus on building a url detection rule the objective of this video is to walk you through the rules editor and how you can use it to build a detection rule now before we get started i'd like to highlight something that's really important as you embark on writing various detection rules you need to ensure that you have the necessary logs in place so for example if you are writing a detection rule that relates to an operating system you need to have those operating system logs within chronicle the same applies for various cloud environments for example if you are writing a detection rule for google cloud you need to ensure that you are ingesting the google cloud logs so let's get started with writing our first detection rule this is done within chronicles integrated development environment called the rules editor we can access this from the chronicle home page by dropping down the menu and selecting rules this will take us to the rules dashboard but we would need to work within the rules editor this is where we will build our detection rule now in order to create a new rule all you need to do is click on the new button and this will populate the rule with the default fields it also goes ahead and gives you an example of a metadata event type so let's work through this and build our detection rule the detection rule that we are going to build is focused on users who may log in out of office hours so the first thing that we are going to do is modify the rule name i will call this rule out of hours login so the meta section we can modify this as we please so i will change the author the description and the severity of the rule now we come to the events section in this section we will filter and correlate different events building our detection logic for the detection rule so i'm going to remove the default event that exists and i'm going to start populating it with the detection logic based on what i'm looking to detect the first thing that i want to do is define the metadata variables in my case i am using an out of hours login variable and line number 18 is looking at a metadata event type of a user login i'm then looking at the target user email address and assigning it a variable of user and tying it in with an action that is an allow because i would like to catch all users that actually log in i'm then going to go ahead and define the user roles that i'm looking for in my case i'm only looking for users that have the administrator role this is defined by the target user role and the target user attribute roles type udm events the next thing that i'm going to do is define a variable which is called ts which stands for timestamp in seconds here i'm looking at the metadata event type of timestamp dot seconds so now i need to define the date range and the time range which relates to out of office hours the first thing that i'm going to do is define the days of the week and this is sunday and saturday since those days are generally out of hours i am defining sunday as the variable zero one and defining saturday as the variable zero seven i'm then leveraging the timestamp day of the week udms linking this to my ts variable now in addition to defining the days i also want to define the time so the next set of lines is looking for any successful authentications beyond office hours so this is defined as 7 pm all the way through to 7 a.m so the detection logic here is looking at the time stem for the hour and anything that falls between midnight and 6 a.m or anything that falls between 7 pm and later now i could also define some addresses that i'm not interested in so in my case i'm gonna define google.com but you can also define a specific email address if needed so that makes our event section the next section that we need to define is our match statement so here because we are calling the email addresses of the users a variable of dollar user we're going to be matching that over a time of one minute now chronicle also enables you to define risk criteria so i'm going to do that in this rule and i'm going to define a risk based on the activity so the next set of code is looking at any activity over the weekend and i'm defining a risk category to those days so for example remember that sunday was defined as 0 1 anybody that logs in on a sunday will get a risk score of 75 anybody that logs in on a saturday will get a risk score of 70. i would like to do the same for the time so using the same logic i'm defining after hours activity and any events that fall within the time range that i've defined would get a risk score of 50. so now i'm at the end of my row i need to define the condition statement which is going to look for the various variables to actually make the detection rule work my condition that i'm looking for is the out of hours login variable remember that this is what we have defined in the events section based on our detection logic so once you've got your rule defined the next step would be to obviously save the rule and then you've got some additional options that you can take on the rule for example dropping down the menu under rule itself i can enable this as a live rule which means that it's going to start running its detection logic by default this rule would not alert if i would like to enable alerting i would need to enable the box so that the rule will flag any alerts that match its detection logic i can also define the run frequency for the role and then i have some additional options i can use the retroarn functionality to go and actually retrohunt against my data to see if there's been any matches for the rule i could make a duplicate of the rule view the detections of the rule and view versions based on changes that have been made so to illustrate let me change the author and i'll now save the rule and as i go and view the versions you'll see that i've got two different versions the current version which is showing the new author and if i would like to revert to a previous version i can select that version and i will see the changes related to the version i could also compare the two versions and it would show me exactly what lines of code have changed from here if i would like to promote the previous version as the current detection rule i could click on promote or i could save it as a new rule so i'm going to leave this as it is now let's take a look at how we can test the rule at the bottom of the rules editor you will notice that we've got the test rule results section here we can define a time range and i will use the last couple of weeks and i can click on run test what this does is it takes the rule and it looks at your data based on the time range that you have defined and it will actually run the rule against the data to see if there are any matches so i'll give this a couple of minutes and we will see if there are any detections we see that there are detections related to this detection rule now another way that we can view the rules detections is by clicking on view rule detections this will go and actually search against my data and show me any detections related to the rule as you can see here we do have detections and based on the detections i can actually expand this and get information about the event from here you can also modify your columns to view the various fields based on your detection activity and here you will notice that we are starting to see the risk scores come into play so that was a quick overview of how you can build detection rules there are a number of different detection rules that we publish that are available on chronicles github page i encourage you to check it out and play around with the detection logic to build your detection rules

### Link - [YARA-L 2.0 language syntax](https://www.cloudskillsboost.google/course_templates/442/documents/472787)

* [YARA-L 2.0 language syntax](https://cloud.google.com/chronicle/docs/detection/yara-l-2-0-syntax)

### Link - [How to write a rule for a single / multi-event](https://www.cloudskillsboost.google/course_templates/442/documents/472788)

* [How to write a rule for a single / multi-event](https://cloud.google.com/chronicle/docs/detection/yara-l-2-0-overview)

### Link - [How to write a rule for EntityGraph](https://www.cloudskillsboost.google/course_templates/442/documents/472789)

* [How to write a rule for EntityGraph](https://cloud.google.com/chronicle/docs/detection/context-aware-analytics#writing_rules_for_context-aware_analytics)

### Video - [How to Deploy a rule using the Detection API](https://www.cloudskillsboost.google/course_templates/442/video/472790)

* [YouTube: How to Deploy a rule using the Detection API](https://www.youtube.com/watch?v=jcJgJchaNvM)

hello welcome to chronicle training in this video you will learn how to use the detection engine through detection api as you might already know chronicle has a powerful detection engine to detect security events and suspicious activities you can always interact with the detection engine through chronicle web user interfaces however there are also cases where you want to access the detection engine programmatically for those use cases you can try detection api like most other clinical apis detection api uses os2 protocol for authentication and authorization this means you must acquire the credential file first you need to contact your clinical representative to get the credential for detection api once you get the credential file you can establish a https session using oauth2 protocol with the credential file you can directly interface with ooz2 system but we do highly recommend using the existing google api client libraries these libraries are well documented and easy to use in our demo you will use python as a primary language suppose you get a credential file from the representative called chronicle credentials.json let's import the google api client library for the authorized https session and oauth2 let's try to generate a credential with the given credential file note that you should set the scope to chronicle backstory as keeping this screen use this generator credential to create https session you now have an active https session to interact with detection api now let's try to create a new rule through detection api to create a rule you should send a post request to the url through the session we just created in the request you should also specify the rule text you want to create for the demo let's have a sample rule like this now let's send a post request with this rule response is in in detection api all responses are provided in a json format let's see what's in response nice you can see some information about the newly created rule rule id version id are important as you need this information for further interaction with this rule so try to save this information in your system let's try to read a rule through the detection api this time you need to send the get request to this url note that the last part of the url should be either rule id or version id when a rule id is provided information on the latest version will be fetched let's send a get request again the response is in adjacent format the information you obtain is the same as the one you get when you create this rule suppose you want to modify the existing rule you can then use a create rule version api you can send a post request to this url note that the last part is the rule id you want to update let's send the post request with the new rule text you just created a new version of this rule you can check the updated information of this version in the response now let's enable the live rule you can call the enable library api you can send a post request to this url to enable live rule note that the last part is a rule id you do not need any data with the request this time for this api there is no data in the response request was made successfully chronicle has an official documentation of detection api in here you can find a full list of detection api methods and requests and response formats as well you can also review on how to authenticate using oauth2 protocol in this documentation the link is provided in this slide chronicle has a public github repository and you can find some sample python scripts using detection api these are good starting points to write your own script to use detection api the link for the repository is in this slide this video you learned how you can authenticate yourself to a detection api with os2 protocol and how you can call a couple of detection api methods for more details about detection api please visit our official documentation and chronicle public guitar repository thank you

### Link - [Detection API overview](https://www.cloudskillsboost.google/course_templates/442/documents/472791)

* [Detection API overview](https://cloud.google.com/chronicle/docs/reference/detection-engine-api)

### Video - [Rule Detections View 
(Finding detections of rule in the rule detection view UI)](https://www.cloudskillsboost.google/course_templates/442/video/472792)

* [YouTube: Rule Detections View 
(Finding detections of rule in the rule detection view UI)](https://www.youtube.com/watch?v=CVAadMy9CTc)

[Music] [Music] hello and welcome to this video on rule detections within chronicle the objectives of this video is to give you an overview of the rule detections and walk you through the user interface chronicle comes with a powerful and easy to use rules engine that enables modern detection capabilities at scale today we'll deep dive into the role detections view and see how we can leverage it during investigations starting from the chronicle home page we can access the rules dashboard from the menu in the top right and from here we can click on rules to navigate to the dashboard within the dashboard we can click on any rule to move to its detections view you can also access rule detections from the rules editor and enterprise insights so taking a look at a sample rule we can see that there is a lot of information so starting with the side panel each row is an individual detection on the left we see the detection timestamp this is the end of the time range that the detection was found in to the right you will see the url match variables and the corresponding values from our rule this is useful for quickly understanding what the detection is about drilling down into the detection we can see all the udm events and entities that make up this detection the data is available both as raw log and in normalized and enriched formats this helps you contextualize and verify the detection some detections have more events than others if there are multiple events these will be displayed in the ui and you can click on download all to see the full list it is often useful to pivot off a particular user artifact or asset to explore more information you can do this by clicking on the drop down circle and select in one of the options moving to the top of the side panel we can quickly search through our detections using the search bar another option that is useful is procedural filtering this enables you to filter your detections on the udm fields of the underlying logs and if you wanted to manually filter detections you can make this process easier by showing more columns or exporting the data as a csv file now let's focus on the center of the page here we can see a detection timeline and this timeline shows us when our detections occurred when a rule was run and any errors that might have occurred the rule detections view also allows you to manage your rules quickly from the rules options menu we can enable the rule as live turn on alerting and run a retro hunt you can also see a rules version history from view versions this lets you see the detections associated with each version and even compare the url rule text you may find that your rule is no longer useful and you want to hide it you can do this by archiving to rule so as a summary we have seen that the rule detections view enables you to quickly view your detections and pivot to more information for further investigation in the next set of videos you will learn how to use the detection api for your investigations

### Link - [Troubleshooting Rules: Community Help Forum](https://www.cloudskillsboost.google/course_templates/442/documents/472793)

* [Troubleshooting Rules: Community Help Forum](https://www.googlecloudcommunity.com/gc/Security/bd-p/cloud-security)

## Investigating Threats

This module will teach you how to investigate threats using Chronicle UI Looker Dashboards and APIs, as well as leverage context enrichment to perform accurate investigations and how to deal with false positives. For Exercises 1-6, these files need to be downloaded and uploaded to https://colab.sandbox.google.com/

### Video - [Ways to investigate a threat](https://www.cloudskillsboost.google/course_templates/442/video/472794)

* [YouTube: Ways to investigate a threat](https://www.youtube.com/watch?v=UZVq9d4_YZs)

[Music] [Music] hello and welcome to this video my name is richal and pelay and today i am super excited to show you an overview of ways that you can use chronicle to investigate a threat the objective of this video is to show you an overview of the different components within chronicle that you can use to investigate a threat you have learned in the previous set of videos that you can perform different searches directly on the chronicle homepage but what if you wanted to explore what type of threats currently exist within your organization well a good place to start would be looking at enterprise insights as i navigate to enterprise insight i will see a list of the different events that require my attention now i can expand my timeline to look at all the events but i'm going to leave this set to three days and let's look at what we have currently in this environment the ioc domain matches gives me a good indication of the domains that have been visited by various assets in my organization i can also see that these domains fall under different categories which gives me some good insight into the type of threats that could possibly exist now underneath the domain matches i will see the recent alerts and these alerts relate to assets and they are categorized by different severities from here i can navigate to a number of different entities for example under asset if i click on the asset name this will take me to the asset view let's try this out i'm going to click on alice benjamin's pc and you'll see now that chronicle has directed me to the asset view i can see information about the assets ip address the mac address and so forth and then under the prevalence graph i can see the list of domains that this asset has accessed now because i clicked on the asset within the alert view it took me directly to the events that led up to that alert so on the left hand side within the timeline i can explore the different type of events that happened leading up to this event so let's go through a quick exercise of investigating this threat as i scroll down the timeline i can see that the user has logged in there was a process launch of outlook.exe so the user probably was checking their email so we've got outlook and then we've got excel which was launched within that we can see the file that was launched in this file has the name of survey.xls immediately after that there was a network connection to manygoodnews.com and here we can see the get request that happened this get request actually downloaded a file which is called clientupdate.exe as i click on the file take a look at my additional data that chronicle is bringing in from feeds such as virustotal immediately i can see that this file is malicious i have details about the different malware families the metadata properties and i can further explore this file within virustotal by clicking on the view details so let's navigate back to chronicle and look at the rest of the events so client update was downloaded and then it started up and as we look through the rest of the events we can see that this file probably performed network connections to these other domains and you can see by the name of these domains that this could probably be malicious domains so that is a quick way that we can quickly investigate an asset and different types of events on the asset now what if i wanted to explore this file a bit further so looking at the file creation and the md5 hash i'm going to click on view details and now i'm directed to the file view here i can see information about the file itself but on the left hand side under assets i can now see the other assets that this file was seen on so now as i build my incident response plan i know that i need to go and investigate these additional machines so let's switch back to enterprise insights staying within the recent alerts i can investigate a specific user by clicking on the user within the alert this will take me to the user view and now i can view details about the user so i have information that's fed in from the idp and i can see more information about this user for example the title the department that they work in etc this will give me a good idea to understand if this user is a high value target in the sense of an attacker targeting that identity on the left-hand side i've got a timeline which relates to this user's activity now remember you can go and actually search for a much broader timeline by performing a raw log search here within the raw log search you can expand your time frame looking at a much larger timeline of data so i'm going to go ahead and switch back to enterprise insights so now let's go and explore the alert as i click on the alert i'm now brought to the detection view and this is related to a rule that has been created so here we can see the rule id the severity and we've got information about what the rule does i can also look at retro hunts that have taken place for this rule or i can go and modify the rule options and even go as far as editing the rule if needed on the left hand side i see the detections that relate to this rule so here we've got a detection alert which involves a pc connecting to a payments virtual machine now i would like to explore this further so i'm going to go and click on the timeline and now i can investigate further the different entities within this log file so for example if i wanted to explore the ip address i could simply click on this i can click on the user's pc the domain the file hashes and so forth so let's check out this user's pc this is now taking me back to the asset view but now i'm navigating to a different user's machine to see what were the events relating up to that detection alert so in the prevalence graph again we've got the prevalence of the different activity and domains but we can also see an alert that took place and this is depicted by the red triangle so clicking on the alert i've got the timeline on the left hand side and now i can see that this user logged in a usb device was inserted and i'm curious to know what type of usb device this is so looking at the logs i can see that this is a sandisk usb storage device so probably a thumb drive who knows but the point is that a usb device was inserted next we've got a process start of winscp now as you know windscp can be used to connect to ssh servers ftp servers and more and it can also be used to transfer files between the different machines we then see the alert and now i see something interesting because i see a file transfer taking place this file transfer involves a file name of transactions.py which leads me to believe that there is a possibility that some valuable information has been taken out or exfiltrated onto the usb stick and we can see that from the next set of events that the device was removed and the user logged out now again from here you can pivot to the different entities as you please so if i click on the file hash this will take me back to the file view and now i can see that in this specific alert there is another machine that has been involved and this is michael bolton pc looking at the timeline i can see the events related to this specific file hash which tells me that this file was also seen on michael bolton's pc so now again i need to go a little bit further and investigate that machine so as i navigate back to enterprise insights one component that we did not really touch on is the rules engine so looking at the rules engine this is where you would actually build different detection rules based on activity that you're looking for this is also a good way for you to see if any of your detection rules have actually picked up activity so for example looking at my roles dashboard i can see that my first few rules have seen activity within the past few weeks now in order to make use of the rules editor this is really simple because here we use yara l as the language and we have a number of different detection rules that are publicly available for you to use and as you've seen in the other videos you can actually build your own detection rule and work with the rules engine another great component that you can quickly look at to gain information is the dashboards here you've got the ability to modify the time range and you can quickly see information about the various events your global threat map and more now there are a number of different dashboards that exist so you can look at the different ones depending on what type of activity you are looking to investigate so this concludes this video here we've looked at an overview of the different ways that you can investigate a threat as you've seen chronicle gives you the ability to quickly explore threats in your organization it pulls together different information and pieces it into actionable intelligence that you can easily work through so i hope you've enjoyed this video

### Video - [Demoing the Chronicle search UI](https://www.cloudskillsboost.google/course_templates/442/video/472795)

* [YouTube: Demoing the Chronicle search UI](https://www.youtube.com/watch?v=Rnd6kkD-l00)

welcome to today's session on the chronicle search user interface my name is spencer and i'll be walking you through a variety of capabilities within chronicle search that make it an incredibly powerful and very fast experience for security analysts as well as some of the use cases typically associated with chronicle search in a real sock environment we're going to talk about the various components of search in chronicle and then work through a few use cases regarding three of the primary search interfaces that you'll find within chronicle and what the benefits are of each of these areas talking through the components briefly before we get into the product i'd like to highlight first the investigative search component within chronicle which is an extremely fast and highly opinionated view within chronicle search that provides an analyst with very curated enriched data across five specific views of asset domain ip user and file or hash within the chronicle search interface this will become more apparent when we move into the product this is a great interface to conduct very very quick curated and enriched search when you have a question that you're looking to answer and a piece of security data that fits into one of these categories next we have the chronicle unified data model or udm search udm search allows an analyst to search against the enriched and normalized udn data that chronicle parses against log records at the point of ingestion this is a medium to fast search experience we continue to improve it as we mature the product and it allows analysts a variety of more advanced operators such as boolean logic or even regex searches integer searches on greater than less than or equal to values you can even incorporate not operators into this udm search results appear in a table format and it's really more of a useful view for exploring data doing threat hunting and getting access to raw and parse normalized log records in a very quick manner the raw log search capability is one that we still have within the product it has utility for compliance driven use cases in particular it allows a lot of flexibility in search syntax you can type in nearly any string and search against the entire raw log record but it is much slower as a result of those logs simply being stored in more of a flat file format and so we don't recommend it for common use cases within the product but we will go through an example of what this looks like all right let's get to some of the exciting work of the actual product in action i'm going to move over to a demo environment of the product now first i would like to take you through a use case of a simple domain that we know and an investigative search to gain access to data on this domain very quickly you can see that i have a time picker here on the left on the chronicle landing page i can select a time period that i wish to search against i have my search bar here and then i have a little bit of information down here in log entries and bytes ingested about the overall data that is being ingested into chronicle at a top level so when i type in my domain and i select search i will immediately see that i have enriched investigative data on this domain in an investigative view that i'll select and quickly pivot to keep in mind this is searching against in the case of a domain nearly all of the data that is present in my environment so although i have selected a specific time period in question here in this case earlier today and this can be indicated by my green bar over here on the right i am seeing information on this domain across all time ranges and i can adjust my slider windows look at assets that have accessed this domain look at a timeline of events on this domain all in a single pane of glass with a variety of enriched information at the top of my page subject to some separate videos we'll be walking through more of these investigative layouts and what access to information you can gain from these but at a high level this is an investigative view of chronicle search on a domain let's pivot back to the home page and conduct a search on a user i type in a name here of a known user in my environment and i'm going to again select search i can see that i have both enriched asset information as well as user data on this entity in question in this case i'm going to choose a user view you'll see in a very quick manner again i will be pivoting over into the user investigative view of my product now in this case although i do have access to a wide range of data it's not presenting every user data point all up front to me it is choosing the time frame that i selected in this case today and it's going back a week in time i can quickly pivot and look at additional data points over time there's a lot of enriched information available to me in order to conduct my analysis on this user in question again subject of additional videos we'll go into more detail on what these views look like and how you can gain access to more information here move back to my home page and i'll work through this third use case of the unified data model query in chronicle so when i select this structure query builder i will be brought to a udm search view within my product and this will help me in constructing a query against the unified data model that i was speaking about in the case of this query i'm going to choose to conduct a multi-part search against some network data that i'm interested in exploring begin with getting some information on the event type which in this case is what's called an enumerated value within chronicle which simply means it's made up of a set of values that can be associated with this parsed field i'm going to choose network connections i'm going to add some operators associated with this search so we're going to choose network direction i'm going to specify that i would like this to not equal broadcast traffic add an additional clause here work with network sent bytes i'm going to choose these bytes are going to be greater than add another clause into here and say our principal port less than more common ports where i might see network traffic inbound or outbound things like tls data or maybe ssh data other common port types for network traffic i'm going to add some last clauses in here and look specifically against host names which i am hoping based on a theory i have i do not see a lot of traffic against just simply because i may know that these domains i'm sorry these host names are forbidden within my environment so perhaps in this case i'm looking to kind of validate that i'm not seeing traffic to these host names in question i'm going to select a time range go back the last two weeks five four until today i'm going to conduct my query you'll see as this loads that information is populated in a table format with a variety of different columns that are indicated at the top here you'll also see once this loads that i can adjust columns to my liking in order to conduct better analysis as well as very quickly unpack the raw log record and udm data associated with the search that i've conducted now given the breadth of the query that i'm conducting here we're gaining access to a pretty wide range of data so in this question i've got 940 events that have returned i may wish to limit this query down further but for the purposes of this demonstration you can see that i can sort you know chronologically or reverse chronologically here i can go through and look at a variety of these different columns quickly go in i could download these results as a csv file and then i can select my columns go in and potentially even adjust more columns and let's say i want some additional metadata information to show up and i could choose maybe event type and adjusted timestamp as two interesting areas and then pull this over you know and have it display in a different area for my analysis very quickly i can unpack and see assets associated with this data and i can also explode out a log view to look at the law rog and the udm event data we'll go into more detail on this view in other videos pivoting back to my home screen to do a last search here against raw log records so i'm going to type in a term healthcare for this example and say i'm looking for any log record that contains this term healthcare perhaps this is part of an audit that is being conducted i'm going to select search see that i don't have any enriched information available related to for example a user or an asset that's tagged with healthcare i can choose this raw log search section i could run this as a regex query in this case i'm just going to choose a one week period of time it's searching against all my different data and 17 gigabytes of log records to scan in this case and i'm going to execute this search you will see as this loads it is a little bit slower but i will get a view in some way similar to my udm view with a little less granularity on the log hits over time at the top and then a long timeline on the left of all of the log records and where that string hit was in question in this case healthcare i will also have the ability to download as a csv all of this information so in this case i have 813 log lines i can see this distribution over time i may choose to look on may 12th when i had the highest number of hits uh it will go to that data in question i could select a log record look at the resource information look at the raw log scroll down and see the raw log the normalized udm information and i can see here that this is where my hit came from this concludes the extent of the training that we have today on chronicle search user interface i hope you found this helpful and look forward to working with you on the use of the product

### Link - [Looker Help Center](https://www.cloudskillsboost.google/course_templates/442/documents/472796)

* [Looker Help Center](https://help.looker.com/hc/en-us)

### Link - [Chronicle Search API](https://www.cloudskillsboost.google/course_templates/442/documents/472797)

* [Chronicle Search API](https://cloud.google.com/chronicle/docs/reference/search-api?hl=en)

### Document - [Accessing the Chronicle Data Lake](https://www.cloudskillsboost.google/course_templates/442/documents/472798)

### Link - [Chronicle Data Lake structure - reference 
(incl. Dataset & Tables, Schema, Retention)](https://www.cloudskillsboost.google/course_templates/442/documents/472799)

* [Chronicle Data Lake structure - reference 
(incl. Dataset & Tables, Schema, Retention)](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/%5BChronicle%5D%20Data%20Lake%20structure%20-%20Reference%20Guide%20(1).pdf)

### Video - [What is BigQuery and how can you use it to hunt for and report threats?](https://www.cloudskillsboost.google/course_templates/442/video/472800)

* [YouTube: What is BigQuery and how can you use it to hunt for and report threats?](https://www.youtube.com/watch?v=X7CVwgpAc4M)

welcome to extending the power of Chronicle with bigquery for threat hunting our objectives today will include a brief overview of bigquery and then move to looking at capabilities that Chronicle users can use immediately for those not familiar with bigquery let's start with a brief overview and when I say a brief overview I mean leveraging the fine folks who provide Cloud bytes to tell you about bigquery in a minute go ahead and watch this I'll wait okay with that introduction out of the way let's briefly discuss what bigquery can do for chronicle users Chronicles integration with bigquery makes it easier for analysts to leverage complex massive security data sets for threat hunting using ad hoc queries on normalized and enriched events with this integration Chronicle users have access to petabytes of security Telemetry in bigquery Google Cloud serverless High scalability multi-cloud data warehouse introducing endless possibilities for security driven data science for example security teams can use bigquery to join the security Telemetry in Chronicles unified data model or udm with a data set of their choice or run custom analytics on top of udm data users can also work with other bigquery compatible tools such as looker which Chronicle uses for dashboarding Google data Studio grafana Google Sheets and Tableau to create visualizations with chronical data backed by this powerful tool set security analysts can create new visualizations on large-scale data sets that provide greater awareness and improve outcomes in the security operations center now that you have a greater understanding of the benefits of bigquery integration let's look at the tables where the data resides here are tables in bigquery that are available to be worked with you can see the details of each one next to the table name depending upon the question you are asking of your data will drive your use of these tables for threat hunting use cases the udm events tables are most likely where you will start now if you're new to bigquery or even SQL in general here are a few best practices to keep in mind as you get started there are numerous fields in the tables because of this avoid using select Star as your primary search and only select the columns that you need using the limit Clause with a select star does not affect the amount of data read so don't rely on that approach that said select star accept syntax can be used to exclude one or more columns just in summary try to avoid the star when possible there are schemas available within the bigquery UI for each table as well as a number of data preview options another tip is to use the underscore partition date or underscore partition time pseudo columns to filter partitions this will reduce the amount of data searched to just the relevant time windows if you are joining tables in your query order your joins by starting with the table with the largest number of rows first followed by the fewest rows and then any other Tables by decreasing size if you forget the SQL query Optimizer is still there but this is still a good recommendation to follow if you'd like more query best practices the link at the bottom of the screen provides some other nice tips and tricks let's look at a few bigquery example queries from data ingested into Chronicle and see what we can do let's generate a listing of 100 host names in principal IP values from the titanium data source for the past day notice we are querying the udm underscore events table here and bounding our search using the underscore partition time pseudo field another example is this query using the rule under underscore detections table to generate a listing of the rule underscore ID and the user associated with it these are the rules that have triggered our final example is searching the ioc underscore matches table for the assets that have ioc matches and limiting our results to a hundred results notice that we have searched three different tables in bigquery for information around ioc matches and systems users and detections and events on a certain day Group by host while we have only scratched the surface today I hope this provided you with a nice overview of the capabilities bigquery can provide to Chronicle users as they extend their use cases into threat hunting and working with petabytes of information thank you

### Link - [Excercise Files](https://www.cloudskillsboost.google/course_templates/442/documents/472801)

* [Excercise Files](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/Exploring%20Chronicles%20Datalake%20(BigQuery)%20(5))

### Link - [Reference: SQL functions](https://www.cloudskillsboost.google/course_templates/442/documents/472802)

* [Reference: SQL functions](https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators)

### Link - [Reference: Understanding repeated fields/ Joining Data & Enums](https://www.cloudskillsboost.google/course_templates/442/documents/472803)

* [Reference: Understanding repeated fields/ Joining Data & Enums](https://medium.com/google-cloud/bigquery-explained-working-with-joins-nested-repeated-data-1941646ccb5b)

## Responding to Threats

This module will teach you how Google Chronicle can be used by 3rd party or SOAR solutions, such as Google Cloud Siemplify, to respond to Threats.

### Video - [How to respond to threats, best practices, recommendation to use a SOAR for systematic responses](https://www.cloudskillsboost.google/course_templates/442/video/472804)

* [YouTube: How to respond to threats, best practices, recommendation to use a SOAR for systematic responses](https://www.youtube.com/watch?v=zC0P_TNZ2Yk)

hello and welcome in this video we'll showcase how simplify the leading security orchestration Automation and response solution complements Chronicle and allows responding to threats in a very short and efficient way both Chronicle and simplify integrate with various third-party solutions that enable broader flexibility of the incident handling flow let's go ahead and take a look at this simplify case Tab and walk through the incident handling flow and the whole experience together so this is the overall case View and first before we start looking at what pending actions our analysts are looking at let's take a look at the story that we'll be telling so let's first start with our miter attack mappings as you can see we're going to be looking at the tactics of execution that were detected as well as persistence we can come over here to the entities graph and see that a user is associated with a host name of Javier Suarez at symbol and the rest of the domain there we can see that some sort of connection was made to a malicious URL mini goodnews.com we know it's malicious because it is highlighted in red and we can see that some sort of file was downloaded which is associated with a known malicious hash we can also see that the Chrome process was involved there so as you can see the case overview really gives you a quick easy to understand storyline that you're going to be looking for in your investigation so the First Alert that came from Chronicle is the process launch which this is the file hash that executed and is now targeting the Chrome process and the neat thing about Chronicle is that it has the ability on its own to correlate events uh that happen in the common host names and IPS and such using its Alias capabilities during ingestion and so it's actually grouped these two events for us this first one coming from crowdstrike and this second one coming from z-scaler so this z-scaler event is actually the network connection made over HTTP which tracked it to the Target URL of many good news as you can see we have our miter enrichment on that user execution Technique we have the crowdstrike enrichment that was done by The Playbook as well as a full virus total readout this is a full HTML widget that's provided by virustotal and gives a really good amount of information about this known malicious Trojan G Suite enrichment here and then what's really powerful is the ability to use a Playbook step to go back and do some investigation on the asset so here we've actually looked up assets related to the hash from Chronicle because Chronicle has its ability again to really correlate data over time in large volume really fast and so as you can see it's been able to locate all the other assets that this hash has some sort of relation with and the one in particular I like to highlight is this gcpu compute instance that will be coming back to and then we have our next alert which has been grouped together by simplify and it's going to be looking at this new gcp service account creation and as you can see again the miter attack this is a persistence technique of creating an account now the really important part here is you can see that the Playbook actually sent an email automatically to the user asking them if this was in fact them that had sent it and in the Playbook it's able to say no the user responds no it wasn't me and this is actually picked up in the Playbook and we'll go ahead and look at where that's done so obviously we have our trigger initial case management because again you're really trying to understand the entire flow of the incident from triage to investigation and an assessment to remediation and then to closing and reports and you know possibly even Lessons Learned reports and metrics so what we have here though is I wanted to highlight was it sends the email and then it gets the response from the user this actually Gmail Wait for the email from the user will actually get that user response and then here's where it's going to say do we want to disable the account if that's the case it does it automatically and then it will run its enrichment blocks now these enrichment blocks are able to be used across playbooks so it's one of the many great functions and abilities of simplify that we can use these blocks over and over again especially when they do the same kind of virus total enrichments minor enrichments and such and then here's that manual action where it's asking do you actually want to proceed with the deletion if we look at the playbook for the suspicious execution you can see those same enrichment blocks being used the virus total block here's where it's doing that Chronicle investigation automatically enriching with crowdstrike G suite and then it's moving here to that question mark of are we going to go ahead and contain this endpoint and what I want to show you is how easy this actually is on the alert we have this pending action of contain the endpoint and all I have to do here as an analyst is Click respond and this is definitely a true positive I'm seeing this bad URL I'm seeing this bad hash I have the other Alert in context that yes this is an ongoing attack and it's actually moved into the persistence tactic and so we do want to make sure that we respond and so I'm going to go ahead and click yes and click done and now it shows that that action has been executed and I can go ahead and do the same thing when I come in here and delete the service account I can come to respond and I can say yes we're going to go ahead and proceed with the deletion and again I get both actions as completed now I have a new pending action and here is the important thing that I want to get across is that we follow the case management through all of its stages and it's very important that when I respond to approving the alert closure that I give a full reason and we have various ones malicious not malicious maintenance inconclusive these can also be edited to fit your environment so here we have all different reasons if it's malicious in this case it's going to be an external attack we can add comments an analyst can put as many comments as they feel necessary so that you have that information and you know any possible metrics you want and most importantly you can also put tags and in this case the really important tag that we want to put on here is going to be the true positive tag because if you're not tagging your alerts when you close them as true positives or false positives then you're not going to get that return on investment of knowing exactly what false positive rate you're being able to deal with in a much more efficient manner using simplify and see it even gives me an analyst notification telling me that it's waiting for my input which is very useful and so like I said though you want to be getting that metric of how many false positives are you Auto closing how many true positives are you stopping and if you're not putting these tags in there then maybe you know you're not getting those metrics it's already been tagged as a Chronicle case you can see that there and then one step that you might have in the playbook for instance after that or before the alert closure is you might have a ticketing system you use or you might have another email you need to do or some sort of reporting mechanism and you would put that in there before you close that alert out we do have emails being sent out if it gets escalated to say the sock manager in that case so there's really a lot of different options and flexibilities on how you want to handle these workflows in conclusion we just want to make sure you understand the flexibility and multiple third-party Integrations that both Chronicle and simplify hold together but even more we want to tell that story of how well the two of them execute the entire incident handling flow together so thank you for joining us today

### Link - [How-to guide for Siemplify integration](https://www.cloudskillsboost.google/course_templates/442/documents/472805)

* [How-to guide for Siemplify integration](https://2567647.fs1.hubspotusercontent-na1.net/hubfs/2567647/Chronical%20Technical%20Training/Chronicle%20&%20Siemplify%20Integration.pdf)

### Link - [Siemplify documentation (e.g. APIs)](https://www.cloudskillsboost.google/course_templates/442/documents/472806)

* [Siemplify documentation (e.g. APIs)](http://learn.siemplify.co/)

## Quiz

Please answer the following 40 questions to test your knowledge based on the course material you have just reviewed. If you earn 70% or higher, you will be awarded a Chronicle Technical Skills Badge.

### Quiz - [Chronicle Technical Training Quiz](https://www.cloudskillsboost.google/course_templates/442/quizzes/472807)

#### Quiz 1.

> [!important]
> **You are asked to assign the least privelaged role in Chronicle, which do you choose?**
>
> * [ ] Viewer
> * [ ] Editor
> * [ ] ViewerWithNoDetectAccess
> * [ ] Administrator

#### Quiz 2.

> [!important]
> **Each Chronicle Forwarder may have multiple connectors each corresponding to a data source?**
>
> * [ ] True
> * [ ] False

#### Quiz 3.

> [!important]
> **When using the ingestion API what formats are supported?**
>
> * [ ] UDM
> * [ ] Unstructured Data
> * [ ] All other answers are correct
> * [ ] Entity

#### Quiz 4.

> [!important]
> **What search component is used to search for non-indexed data?**
>
> * [ ] Investigative Search
> * [ ] UDM Search
> * [ ] No other answers are correct
> * [ ] Raw Log Scan

#### Quiz 5.

> [!important]
> **What section of a detection rule describes the events that must match?**
>
> * [ ] Meta
> * [ ] Events
> * [ ] Condition
> * [ ] Match

#### Quiz 6.

> [!important]
> **Ingest Health is found in which Dashboard?**
>
> * [ ] Data ingestion and health
> * [ ] IOC Matches
> * [ ] Main
> * [ ] User Sign in Overview

#### Quiz 7.

> [!important]
> **What view is used to utilize analytics from Looker?**
>
> * [ ] List Manager
> * [ ] Enterprise Insights
> * [ ] Dashboards
> * [ ] Rules

#### Quiz 8.

> [!important]
> **What collection method can be used to send data directly to Chronicle?**
>
> * [ ] Direct Ingestion
> * [ ] All of the other answers are correct.
> * [ ] Ingestion API
> * [ ] Forwarder
> * [ ] Cloud-Cloud Service

#### Quiz 9.

> [!important]
> **Which default dashboard reports on detection rules?**
>
> * [ ] IOC matches
> * [ ] Main
> * [ ] User Sign in Overview
> * [ ] Rule Detections

#### Quiz 10.

> [!important]
> **What must be enabled in order for a rule to be active?**
>
> * [ ] No other answers are correct
> * [ ] Live Status
> * [ ] Alerting
> * [ ] Retrohunt

#### Quiz 11.

> [!important]
> **On premise data requires the use of a __________ to collect data?**
>
> * [ ] Forwarder
> * [ ] Cloud-Cloud Service
> * [ ] Ingestion API
> * [ ] Direct Ingestion

#### Quiz 12.

> [!important]
> **What view shows IOC matches and recent alerts?**
>
> * [ ] Rules
> * [ ] Enterprise Insights
> * [ ] Dashboards
> * [ ] List Manager

#### Quiz 13.

> [!important]
> **You can use filters to further organize data ingestion when configuring the Chronicle Forwarder?**
>
> * [ ] True
> * [ ] False

#### Quiz 14.

> [!important]
> **What built in tool is used to write and test rules?**
>
> * [ ] Rules Dashboard
> * [ ] Rules Editors
> * [ ] Retrohunt
> * [ ] RBAC Controls

#### Quiz 15.

> [!important]
> **Adding and modifying rules is accomplished from what screen?**
>
> * [ ] Enterprise Insights
> * [ ] Dashboards
> * [ ] Home
> * [ ] Rules

#### Quiz 16.

> [!important]
> **Chronicle uses a standard schmea called ______?**
>
> * [ ] UDM
> * [ ] OSSEM
> * [ ] ELF
> * [ ] Common Log Format

#### Quiz 17.

> [!important]
> **A new rule has been developed and historical data over the last year must be investigated, you must run a ______ to perform the investigation?**
>
> * [ ] Retrohunt
> * [ ] No other answers are correct
> * [ ] Alerting
> * [ ] Live Status

#### Quiz 18.

> [!important]
> **How does the Chronicle Forwarder recognize what data format is being ingested?**
>
> * [ ] Channel Names
> * [ ] Tags
> * [ ] Topics
> * [ ] Data labels

#### Quiz 19.

> [!important]
> **The collection section of the Chronicle Forwarder's configuration contains what elements?**
>
> * [ ] Expected Data Types
> * [ ] Ingestion Methods
> * [ ] Expected Data Types & Ingestion Methods
> * [ ] IDs

#### Quiz 20.

> [!important]
> **What language is used for rules?**
>
> * [ ] Sigma
> * [ ] graphQL
> * [ ] SQL
> * [ ] YARA-L

#### Quiz 21.

> [!important]
> **What search component is used for extremely fast results on asset,domain,IP,User, and file/hash results?**
>
> * [ ] Raw Log Search
> * [ ] Investigative Search
> * [ ] No other answers are correct
> * [ ] UDM Search

#### Quiz 22.

> [!important]
> **What types of data can be posted to the Ingestion API?**
>
> * [ ] UDM Entities
> * [ ] CEF
> * [ ] Unstructured raw logs
> * [ ] UDM, entities & unstructured raw logs

#### Quiz 23.

> [!important]
> **Which of the following ways is the correct one when investigating threats in Chronicle?**
>
> * [ ] Rules Dashboard
> * [ ] IOC Search Bar
> * [ ] Enterprise Insights
> * [ ] Each of those provide a starting point, however for optimal experience it's recommended to start the investigation in the ticketing/investigation platform of choice

#### Quiz 24.

> [!important]
> **You are writing a rule that must only look within the last 20min, what section of the rule do you place this condition in?**
>
> * [ ] Events
> * [ ] Match
> * [ ] Meta
> * [ ] Condition

#### Quiz 25.

> [!important]
> **Chronicle dashboards are powered by Looker?**
>
> * [ ] True
> * [ ] False

#### Quiz 26.

> [!important]
> **GCP Logs can be ingested into Chronicle using ________?**
>
> * [ ] Ingestion API
> * [ ] Forwarder
> * [ ] Cloud-Cloud Service
> * [ ] Direct Ingestion

#### Quiz 27.

> [!important]
> **Name spaces can be used to identify distinct network segments when configuring the Chronicle Forwarder?**
>
> * [ ] True
> * [ ] False

#### Quiz 28.

> [!important]
> **What section of a detection rule contains filter conditions similar to a WHERE clause?**
>
> * [ ] Condition
> * [ ] Match
> * [ ] Events
> * [ ] Meta

#### Quiz 29.

> [!important]
> **The feeds option in settings allows you to add new data sources in the UI?**
>
> * [ ] True
> * [ ] False

#### Quiz 30.

> [!important]
> **What information do you need to store after creating a rule with the detection API?**
>
> * [ ] Rule ID and Rule Version
> * [ ] Rule Name
> * [ ] Rule Conditions
> * [ ] Rule Author

#### Quiz 31.

> [!important]
> **Chronicle provides the following major benefits _________?**
>
> * [ ] All other answers are correct
> * [ ] Self-managed
> * [ ] Continuous IoC Matching
> * [ ] Hunt at Google Speed

#### Quiz 32.

> [!important]
> **Threat Intelligence information can be found on what dashboard?**
>
> * [ ] Main
> * [ ] Data ingestion and health
> * [ ] IOC matches
> * [ ] User Sign in Overview

#### Quiz 33.

> [!important]
> **Multiple syslog data sources can use a single port on the Chronicle Forwarder?**
>
> * [ ] True
> * [ ] False

#### Quiz 34.

> [!important]
> **What search component is used for threat hunting and other indepth use cases?**
>
> * [ ] No others answers are correct
> * [ ] Raw Log Search
> * [ ] Investigative Search
> * [ ] UDM Search

#### Quiz 35.

> [!important]
> **Detection rules can be either alerting or non-alerting signals?**
>
> * [ ] True
> * [ ] False

#### Quiz 36.

> [!important]
> **What language are Chronicle parsers based on?**
>
> * [ ] Logstash
> * [ ] Python
> * [ ] HTML
> * [ ] GoLang

#### Quiz 37.

> [!important]
> **While writing a rule where do you put the descriptive information?**
>
> * [ ] Match
> * [ ] Events
> * [ ] Condition
> * [ ] Meta

#### Quiz 38.

> [!important]
> **What type of identity provider must be used for Single Sign On?**
>
> * [ ] Kerberos
> * [ ] SAML 1.0
> * [ ] SAML 2.0
> * [ ] Smart Card

#### Quiz 39.

> [!important]
> **In addition to logs Chronicle can also collect contextual data?**
>
> * [ ] True
> * [ ] False

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
