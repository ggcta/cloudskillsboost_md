---
id: 963
name: 'Data Transformation in the Cloud'
datePublished: 2024-05-07
topics:
- Data Governance
- Data
- Data Management
type: Course
url: https://www.cloudskillsboost.google/course_templates/963
---

# [Data Transformation in the Cloud](https://www.cloudskillsboost.google/course_templates/963)

**Description:**

This is the third of five courses in the Google Cloud Data Analytics Certificate. In this course, you'll begin by getting an overview of the data journey, from collection to insights. You'll then learn how to use SQL to transform raw data into a usable format. Next, you'll learn how to transform high volumes of data with a data pipeline. Finally, you'll gain experience applying transformation strategies to real data sets to solve business needs.

**Objectives:**

- Describe the steps of the data journey and how they are used to drive data-driven decision making.
- Recognize the role of data transformation in preparing data for analysis.
- Describe the benefits and challenges of transforming data in the cloud.
- Identify the key components of a data transformation plan.

## Introduction to data transformation in the cloud

In this module, you'll identify the phases of the cloud data journey and investigate the importance of a structured approach to collect and prepare data for analysis and to inform data-driven decision making. You'll explore the benefits and challenges of transforming data in the cloud and the common tools and methods used to collect, process, and store data.

### Video - [Introduction to course 3](https://www.cloudskillsboost.google/course_templates/963/video/472252)

- [YouTube: Introduction to course 3](https://www.youtube.com/watch?v=7TxTDEubIQk)

Hello there! You’re making great progress on your path to becoming a data professional. Congratulations. Throughout your learning expedition so far, you’ve explored the basics of cloud data analytics, and how to store and manage data in the cloud. Next, you’ll learn how to work with the data to prepare it for analysis. In this course, you’ll learn to identify the quality of the data you’re working with, and explore methods for transforming and processing that data. It’s important to keep in mind that the cloud data professional will transform and process data before analyzing it, which makes the steps you’re about to get familiar with essential. But first, let me introduce myself. My name's Alyx Stevens. I work as a Data & Analytics Customer Engineer at Google. This means that I lead a customer or prospective customer's data and analytics technical projects, which can be anything from technology advocacy to architecting solutions. I first learned about data transformation in my first job out of college. In my role, I was tasked with identifying fraudulent transactions on the company website. Since the website had millions of visitors a day, it was important to cut through the noise and transform the data in a way that was useful and applicable for our analysis, ultimately keeping bad actors off the website and creating a positive environment for our users in the process. Learning about data transformation in my first role laid down the foundation for the rest of my career. Now, let’s get back to what you’ll learn. We’ll start with an overview of the data journey and its importance, including collection, processing, storage, and transformation of data. Next, we’ll explore the data pipeline and the various methods for extraction, transformation, and loading. Then, you’ll discover how to use a data transformation plan to meet the business needs of your organization. You’ll also learn transformation strategies. Learning data transformation is key to preparing quality data that will lead to actionable insights. Don’t forget, I’m here to help you every step of the way! You can work at your own pace, so review videos and resources anytime you need to. You’ve got this! By the end of this course, you’ll know how to process and transform data into a consistent, workable dataset. Let’s get started!

### Document - [Course 3 overview](https://www.cloudskillsboost.google/course_templates/963/documents/472253)

### Video - [Alyx: Data analysts help others learn from data](https://www.cloudskillsboost.google/course_templates/963/video/472254)

- [YouTube: Alyx: Data analysts help others learn from data](https://www.youtube.com/watch?v=rPA6NXNF-Gk)

Hi, there! I’m Alyx! I’m a Data and Analytics Customer Engineer. A typical day for me may include demoing solutions, working with customers to develop their analytical strategies, and helping solve data problems. I’ve been interested in STEM for as long as I can remember. I studied science and engineering throughout school. I especially loved being able to stay up late and use the campus telescope for my research on planets and stars. I started my career as an academic researcher working with astronomy and physics. I loved the analytical aspects of that work! My pivot to the tech industry was a huge period of growth for me as I was moved into a field I didn't have much experience in but was able to learn, grow, and thrive. One of the biggest challenges I faced early on was learning how to write SQL to query data. While the language was new to me, I was familiar with problem solving and was able to pick it up with enough practice and studying. Writing SQL to answer analytical questions has been the foundation for my career in tech. Now, I love serving as a data practitioner helping companies —and learners like you— to apply strategies to learn the most insights from their data!

### Document - [Helpful resources and tips](https://www.cloudskillsboost.google/course_templates/963/documents/472255)

### Document - [Lab technical tips](https://www.cloudskillsboost.google/course_templates/963/documents/472256)

### Document - [Explore your Course 3 scenario: TheLook eCommerce](https://www.cloudskillsboost.google/course_templates/963/documents/472257)

### Video - [Welcome to module 1](https://www.cloudskillsboost.google/course_templates/963/video/472258)

- [YouTube: Welcome to module 1](https://www.youtube.com/watch?v=JJOZV1xDLuY)

All aboard! Let’s steam ahead with the data journey! Coming up, we’ll introduce the data journey and how it relates to your role as a cloud data analyst. First, we’ll review the data journey process and its role in improving data-driven decision making. Next, we’ll review data collection techniques. Then, we’ll explore data transformation, and the different methods used for transforming data. Finally, we’ll introduce you to a variety of methods and tools for data processing and storage. When you’re ready to get started, get your passenger ticket and meet me in the next video!

### Video - [Stages of the data journey](https://www.cloudskillsboost.google/course_templates/963/video/472259)

- [YouTube: Stages of the data journey](https://www.youtube.com/watch?v=REMqPPlp0b0)

In this video, we’ll explore how data moves through the cloud using a process called the data journey. The data journey begins when you, as a data cloud professional, locate data. And the journey ends when you present the data analysis to stakeholders. The data journey has five stages: collect, process, store, analyze, and activate. In this course we’ll cover the first three stages: collect, process, and store. The last two stages, analyze and activate, will be covered in a separate course. Understanding the data journey is important because organizations need a structured way to collect data to prepare for analysis and inform data-driven decision making. It is equally important to understand the unique role of each stage of the data journey. The first stage is to collect data. During this stage, you’ll find and gather the data you need. The second stage is to process data. This stage can also be called transformation because it is the process of transforming data into a usable format. During this stage, you’ll review and explore the data to identify issues. You’ll also clean, organize, and standardize the data. The third stage is to store data. Once you’ve located and processed the data, you’ll need a place to keep it. You’ll have several options to choose from depending on your business needs, including storing data locally or in the cloud. The fourth stage is to analyze data. During this stage, you’ll identify trends and patterns to uncover insights your users need. The fifth and final stage is to activate data. This is the final result where you’ll present your visualizations to your stakeholders and use the insights from the data to make decisions and take action. As you move through the stages of the data journey, it’s important to know that the process isn’t linear. The data journey is an iterative process, meaning you may move back and forth between the stages. For example, after you collect the data and move forward into the processing stage, you may realize you need to go back and find more data to answer the stakeholders’ business question. Also, the data journey isn’t a one-time process and is instead, repeated. You'll need to go through this process multiple times. And, the data journey is tailored to each project. You’ll likely find differences between projects that change the process. For example, if the data requires extensive cleaning, you may need to spend more time in the processing stage. It’s important to call out that the tools you use can also affect the process. For example, with some tools, storing data happens before processing, or it can be the other way around. Together, the stages of the data journey represent a foundation you can build on to analyze your data, and provide insights for your users.

### Document - [The data journey is a flexible path](https://www.cloudskillsboost.google/course_templates/963/documents/472260)

### Document - [Impacts of the data journey on decision making](https://www.cloudskillsboost.google/course_templates/963/documents/472261)

### Video - [Steps for effective data collection](https://www.cloudskillsboost.google/course_templates/963/video/472262)

- [YouTube: Steps for effective data collection](https://www.youtube.com/watch?v=Uw4h3kIW9MY)

So, imagine as a cloud data professional your current project is to create a dashboard for a sales team. Where do you start? Well, you start a data journey! The first stage of the data journey is collect. You might also refer to this as the gathering stage. Data collection is the process of identifying and finding the data that can be used to meet a specific business need, and bring it all together. Because it’s the first stage of the data journey, data collection is essential for creating a solid foundation for your data, analysis, and visualizations. There are four steps within the collection stage. Before you can find the right data, you need to know what you’re looking for. So, the first step is to identify specific questions you want to answer with your data. To do this, you’ll work with your stakeholders to determine their needs. Once you’ve figured out the questions you’re trying to answer, you can move into the second step, data discovery. During this step, you’ll find the data you need. Data is usually stored in various formats and in different sources. You'll also need to explore how the data sources you select relate to the things you are trying to measure and understand —to identify the data you need to gather. The third step is data gathering. Keep in mind, you’ll likely be looking for data in multiple locations. Also, the data will be in different formats, so you will have to find how to work with each data source and evaluate how often you will need to update the data. This can be a time consuming process. When you identify the data’s location, you can collect it into a single, usable staging area. The final step is data staging. Once the data has been gathered, you will need to find a way to bring together the data into a single, usable staging area. Once the data has been brought together, it is ready for the next stage in the data journey: process. As the first stage of the data journey, good data collection is essential. Become comfortable with this process so that you begin to create a good foundation for your data, analysis, and visualizations.

### Quiz - [Test your knowledge: Introduction to the data journey and collecting data](https://www.cloudskillsboost.google/course_templates/963/quizzes/472263)

### Video - [The process stage ensures clean and consistent data](https://www.cloudskillsboost.google/course_templates/963/video/472264)

- [YouTube: The process stage ensures clean and consistent data](https://www.youtube.com/watch?v=ffZu_AiM3Kw)

At this point in your journey as a cloud data professional to create a dashboard for a sales team, you’ve identified what data you need, you’ve collected the data, and you think you’re ready to move forward to the second stage, process. But not so fast! You’re going to need to work with the data before you can use it for analysis. Raw data can be in different formats and have a lot of problems that need to be fixed. For example, during the collect stage, you might find data that’s incomplete or missing. You might also find duplicated or incorrect data. These issues will cause errors in your analysis. So, it’s important to identify them now in this process stage, and correct any errors, and make sure the data is in a format you can use before moving forward to the third stage, store. Data transformation is the process of taking raw data and converting it into a usable format. Basically, it is changing inconsistent data formats into a consistent format that you can use for analysis and visualization development to ensure that the data is error free and in a format that you can use. Sometimes, the term data processing is also used to refer to data transformation, but they are not the same. Data processing is a big term that covers a lot of different things. It can include collecting data, cleaning it up, transforming it, analyzing it, and visualizing it. Data transformation is a more specific type of data processing. It's the process of converting data from one format to another, or from one structure to another. The goal of data transformation is to provide your data team with data that you can all access and use. This includes addressing issues like correcting errors, adding new information to the data, and reducing unnecessary data detail. While there are many different ways to transform data, there are six basic types of data transformation, including data smoothing, attribution construction, data generalization, data aggregation, data discretization, and data normalization. The exact method you use will depend on the type of data you're transforming, and how it will be used. There are two basic methods to approach data transformation. You will either perform data transformation manually with SQL or another common language, or you’ll use an automated pipeline. The method you choose will vary by project, and the type of data you’re using. Now, you’re set to move on and decide whether to perform data transformation manually or automatically, so that you create a solid foundation for your data analysis. Taking time here to make sure your foundation is solid will help ensure the success of your future data visualizations.

### Document - [[Supplemental] Process and summarize data](https://www.cloudskillsboost.google/course_templates/963/documents/472265)

### Document - [Techniques used to transform data, part 1](https://www.cloudskillsboost.google/course_templates/963/documents/472266)

### Document - [Techniques used to transform data, part 2](https://www.cloudskillsboost.google/course_templates/963/documents/472267)

### Video - [Manual and automated transformation](https://www.cloudskillsboost.google/course_templates/963/video/472268)

- [YouTube: Manual and automated transformation](https://www.youtube.com/watch?v=i5tAlqIObm0)

Ok, so, as a cloud data professional, you’re well on your way to completing a project to create a dashboard for a sales team. It’s time to transform your data and you’ll have to figure out the best method to use. You can select either manual or automated data transformation. Both data transformation types have the same goal: to prepare data for your analysis. Let’s explore both methods. Manual transformation is the use of coding languages to affect data transformation, without the aid of software programs. This includes languages, like SQL, Python, and R. SQL, or Structured Query Language, is what you’ll be working with for the remainder of this course. SQL is used to get data out of relational databases. Python is a high-level, general-purpose programming language that works with libraries, like Pandas, to analyze, and even visualize data. Pandas is an open source data analysis and manipulation library that can be used with Python. And R is a programming language used for statistical computing. The packages in R provide a variety of methods for implementing data simulations to transform data, like arrange, filter, and select. While you can use manual transformation on datasets of any size, it’s best for smaller datasets because of the time, effort, and accuracy involved to code. Not only does the code have to be created, but you’ll spend additional time testing, troubleshooting, and maintaining it. Automated transformation is the use of processing and scripting tools with less or no programming, compared to manual transformation. These processes are usually combined to create an automated workflow. These tools make working with your data so much easier than manually transforming data alone. Although you don’t have to use much code with these tools, you might still end up using coding languages, like SQL and Python, to make modifications within the tool. Automated transformation tools are best for large or high velocity data sets and can be local or in the cloud. The tools you use will be based largely on the needs of your organization and the goals you’re trying to reach with your data. So, which transformation method should you use? There are several factors that will influence your decision. The size of the datasets is probably the most important. The larger the dataset, the harder and more time-consuming it will be to transform manually. Manually transforming a larger dataset also increases the chances for errors. It’s also important to consider how long it will take to process the data. If speed is important, you might choose to use automated transformation, so that you can more quickly and accurately transform the data. The use of automated transformation tools is also dependent on tool availability. Since these tools cost money, they might not always be available, so the manual process could be your only option. You might also want to use a combination of transformation methods. Even with access to automated transformation tools, you may have to manually use programming languages to make some changes to your data. Each method of data transformation has its advantages, and you’ll likely use both over your cloud data professional career.

### Document - [Storage options for transformed data](https://www.cloudskillsboost.google/course_templates/963/documents/472269)

### Lab - [Practice transformation methods](https://www.cloudskillsboost.google/course_templates/963/labs/472270)

Explore public datasets in BigQuery

- [ ] [Practice transformation methods](../labs/Practice-transformation-methods.md)

### Quiz - [Test your knowledge: The processing and storage stages of the data journey](https://www.cloudskillsboost.google/course_templates/963/quizzes/472271)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/963/video/472272)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=AKvwbAlRcEY)

You’ve made it through another topic! During this course, you learned about the steps in the data journey and its importance in your role as a cloud data analyst. First, you reviewed the concept of the data journey, and its role in improving data-driven decision making. Next, you learned about data collection techniques and explored additional details for data collection. Then, you explored the critical importance of data transformation, and the different methods for transforming data. Finally, you learned about the various methods and tools used for processing and storing data. Good job on your progress so far! We’ve come to our last stop of this section in the data journey and the train is ready to move to the next station!

### Document - [Glossary terms from module 1](https://www.cloudskillsboost.google/course_templates/963/documents/472273)

### Quiz - [Module 1 challenge](https://www.cloudskillsboost.google/course_templates/963/quizzes/472274)

## Handle raw data with data pipelines

In this module, you'll explore what a data pipeline is, the steps that data travels through in a data pipeline from source to storage and analysis, and the methods used in each phase of the process. You'll also have an opportunity to gain practical experience building a simple SQL pipeline.

### Video - [Welcome to module 2](https://www.cloudskillsboost.google/course_templates/963/video/472275)

- [YouTube: Welcome to module 2](https://www.youtube.com/watch?v=x87zPeigxG0)

Hi and welcome back! Congratulations on your progress so far! Recall the five stages of the data journey: collect, process, store, analyze, and activate. Once you’ve started a data analysis project and you’re in the midst of the process stage, it’s time to explore how data pipelines can help you address business needs. Let’s cover the three main stages of a data pipeline: extract, transform, and load. We’ll start by discussing how data pipelines convert raw data into a format that's ready for storage and analysis. Then, we’ll dive into the stages of a data pipeline in more detail. We'll begin with the extract stage, and discover how data is collected from various sources and ingested. Next, we’ll discuss the role of the transform stage in the pipeline process, and how data is cleaned and manipulated. Finally, we’ll review the load stage, the stage where data is loaded into destination storage and made available for storage and analysis. Let’s get started!

### Video - [Data pipelines in the cloud](https://www.cloudskillsboost.google/course_templates/963/video/472276)

- [YouTube: Data pipelines in the cloud](https://www.youtube.com/watch?v=pz43g_V1NCo)

In cloud data analysis, you can think of data like a set of raw materials that determines how a project moves forward. The data needs to be produced efficiently and in a consistent way. As you may already know, a data pipeline is a series of processes that transports data from different sources to their final destination for storage and analysis. There are three main stages in a data pipeline: extract, transform, and load. Let’s consider an example, a toy factory uses an assembly line to make toy cars. In the first stage, one worker gathers all the parts they need to make a toy car and puts them on a tray. Then, they pass the tray down the conveyor belt to another worker, who assembles the car. After the car is assembled, the third worker packages it. As a cloud data analyst, instead of toy cars, you’ll be working with data. Let’s explore each step in a data pipeline. Extract is the stage of retrieving data from one or more sources in a data pipeline. In this first stage, the team gathers raw data from various sources and moves the data to a temporary staging area. Here’s an example. An animal rescue organization has a mission to locate and protect missing and lost pets. The data team for this organization wants to create a data pipeline. They have two sources of data. One is a CSV file that includes the registration number of each animal’s microchip. The second is a database that includes information about pet owners, including contact information and the registration number for each pet. During the extract stage, the raw data from both of these sources is copied and moved into a staging area using techniques that are specific to each source. Once the data team collects all the data they need into the staging area, the extract stage is complete and the data is ready to be moved to the next stage: transform. Transform is the stage of taking data, cleaning it, and putting it into a standard format in a data pipeline. Before the data team can convert, store, and use the data for analysis, they check the data for duplicate entries or obvious errors. Once the data has been transformed, it's ready for the final stage: the load stage. Load is the stage of inserting data into the target database, data store, data warehouse, or data lake in a data pipeline. In the case of the animal rescue organization, the transformed data will be loaded into a data warehouse. This will enable the data team to store the data for future analysis and visualization. Pro tip, data pipelines can be complex. And not all data pipelines work exactly the same. Just like a factory assembly line, a data pipeline is tailored to the specific tasks needed to get a specific data job done in an efficient and consistent way. For this reason, the stages in a data pipeline may not always happen in the same order. And the processes within each step may change depending on the data. But one thing that all data pipelines have in common is that they're like data assembly lines. They're a valuable tool for automating the process of extracting, transforming, and loading data. Data pipelines can also help organizations save time and resources, improve data accuracy, and get more value from their data.

### Video - [Alyssa: Improve systems by collecting data and asking questions](https://www.cloudskillsboost.google/course_templates/963/video/472277)

- [YouTube: Alyssa: Improve systems by collecting data and asking questions](https://www.youtube.com/watch?v=BdjlHu5dZM4)

What I love about this career path is that every single day is different. Hi, my name is Alissa and I am an Account Executive here at Google with a background in data and analytics. I get to work with all different types of organizations to help them figure out how do they solve their challenges with data. So basically taking any sort of complex data problem and helping make those connections. I've always been a very curious person, you can ask my parents. I was constantly asking them questions, and so being able to lean into that in this role has been awesome, but I also had to develop my skills over time. So when it came to any of those more technical skills, I had to lean into being more formalized with my learning and being more systematic about my approach, but having that underlying curiosity and hunger for learning has really served me well. One of my very first jobs was working as an attendant at tennis courts. I was in charge of cleaning up the courts. I was in charge of making sure that everyone was sharing very effectively, and I was also in charge of managing all of the lessons and activities that would happen on that court. When you look at that now, you might say, "How does that relate to data and analytics?" And what I found was that I was constantly collecting information about all of the people who were participating in the courses, all of the people who were trying to get time on the courts, and then I was trying to figure out, "How do we make this more efficient?" When you look at any type of problem, it all comes down to "What is the information we're trying to gather "and what is the goal outcome that we're trying to achieve?" When I first was graduating, I didn't know what I wanted to do and the first role I went into, I was very fortunate where I got to spend a little bit of time working with all different cloud technologies, but I definitely gravitated towards the data and analytics solutions because I could see so tangibly how we were solving problems with them. When I was starting out, I would've loved to have known that nobody has all the answers. And so that ties into my whole ethos around asking questions and being curious. You might feel sometimes like you should know the solution right off the bat, but nobody does, and if they are, they're probably pretending. So I found that if I had been more comfortable earlier on with just going in as a learner that you'll get the better outcome. The best advice that I can give you is to stay persistent. There are going to be times when the problems are really hard to solve or you'll have no idea where to start, but if you stay persistent, you will figure it out and you will continue to progress.

### Document - [Overview of data pipeline stages and benefits](https://www.cloudskillsboost.google/course_templates/963/documents/472278)

### Video - [The difference between ELT and ETL](https://www.cloudskillsboost.google/course_templates/963/video/472279)

- [YouTube: The difference between ELT and ETL](https://www.youtube.com/watch?v=crghapUATcU)

As a data analyst, how you organize your data pipeline can have a huge impact on the efficiency and effectiveness of your data storage and analysis. To start, choose an integration technique that works best for the business and its data needs. The data pipeline integration techniques we’ll discuss here include extract, transform, load stages, or ETL, and an alternative order of those stages: ELT. Let’s dive in and briefly review what each of the stages entails. 'E' stands for 'extract'. Extract is the stage of collecting data from one or more sources. 'T' stands for 'transform'. Transform is the stage of taking data, cleaning it, and putting it into a common format. 'L' stands for 'load'. Load is the stage of inserting that formatted data into the target database, data store, data warehouse, or data lake. Extract, transform and load are all stages in a pipeline process that data analysts use to move data from the source and get it ready for storage and analysis. Now that you know what E, T, and L stand for, it’s time to dive into how ETL and ELT pipelines work. ETL, or extract, transform, and load, is a type of data pipeline that enables data to be gathered from source systems, converted into a useful format, and brought into a data warehouse, or other unified destination system. ETL has been around for decades as a way to integrate and manage large amounts of data. But recently, data teams have adapted their process, moving their data sources and storage solutions to the cloud. ETL brings data from various sources together in one place. This makes it possible for data teams to analyze the data and identify trends and insights that can help drive data-driven decision making. For example, a university might use ETL to integrate data from its student information system, financial records, and course catalog. This allows the university to track student enrollment, financial trends, and course performance, so the university can make informed decisions about which courses to continue, which courses to expand, and how many faculty to employ each semester. An alternative to ETL is the ELT pipeline, where the loading and transformation stages are swapped. ELT, or extract, load, and transform is a type of data pipeline that enables data to be gathered from data lakes, loaded into a unified destination system, and transformed into a useful format. The ELT pipeline is a popular option for organizations that need to process large data sets in the cloud. But why load data before transforming it? There are three reasons: save time, enable scalability, and increase flexibility. Transformation can be time-consuming, especially if you have a lot of data to process. When speed matters, like with streaming data and near-real-time analytics, ELT may be a better choice. Loading your data first is also a great way to take advantage of the scalability in the cloud. And in some cases, you may not know exactly how you will need to transform your data. In this case, ELT gives you the flexibility to load your data first and then transform it later. For example, a manufacturing company wants to track their production data as soon as it's available, or in near-real time, so that they can make data decisions quickly. A manufacturing company’s data comes from a variety of sources including a production control system, machine sensors located around the manufacturing floor, and a quality control system. The company’s data team could use the traditional ETL pipeline to extract the data from each source, transform it into a common format, and load it into a data warehouse. But, that would be slower and could impact performance. If the company uses the ELT pipeline instead, they can load the data before transforming it. Then, they can make the data available in near-real time. This gives the data team more flexibility, since they can change the way transformation is done as needed. To wrap up, ETL and ELT are both data pipelines that move data from one location to another. As a cloud data analyst, you’ll decide whether to use either the ETL or ELT pipeline, depending on when you need to transform data.

### Quiz - [Test your knowledge: Introduction to data pipelines](https://www.cloudskillsboost.google/course_templates/963/quizzes/472280)

### Video - [Data ingestion methods that suit your needs](https://www.cloudskillsboost.google/course_templates/963/video/472281)

- [YouTube: Data ingestion methods that suit your needs](https://www.youtube.com/watch?v=w2Tj0UB7fGk)

When organizations need to make decisions, they rely on a variety of sources to get the data they need. The range of possible data sources is tremendous! Just a fraction of potential data sources includes websites, point-of-sale systems, social media, and even machinery sensor data. It’s important to call out that each individual source only makes up part of an organization’s data needs. To have a more complete understanding, organizations must have a way to bring their data sources together. This is where data ingestion can help. Data ingestion is the process of collecting data from different sources and moving it to a staging area. As a data analyst, data ingestion is an important first step in a data pipeline to get your data ready for further processing and analysis. But, not all data ingestion techniques work the same. So, it's important to choose the technique that works best for your data. And one important factor that you’ll rely on when deciding which ingestion technique to use is whether the data is time sensitive. Time sensitive data is data that must be acted on within a specific time frame, or it loses value. When planning a time sensitive travel itinerary, you might consider which mode of transportation you are going to use to get from one destination to the next. And, you will probably also consider how quickly you need to arrive at each location. Busses are a great and economical option if you don't mind waiting a bit. You can show up at the bus stop at the scheduled time, wait with other riders, and the bus will take you to your destination. But, if you need to get somewhere in a hurry, a taxi is a faster option. In a taxi, you can go straight to your destination without waiting or sharing your ride. As a data analyst, the same is true for how you plan data ingestion. Like the bus example, if your data can wait a bit before you process it, batch ingestion is usually the best option. But, if your data is time sensitive, like the taxi example, you’ll probably use streaming ingestion. Let’s dive into how each technique works in more detail. Batch ingestion is a method of collecting data over time and processing it in groups, also called batches. During batch ingestion, when data becomes available, it's not immediately processed. Instead, the data waits over a period of time before it’s processed as a group. This processing happens on a regular schedule, which makes it a good choice when working with a high volume of data, or when it's not critical for data to be ingested right away. As an additional benefit, this processing schedule also makes batch ingestion the more economical choice because it requires less computing power and storage space. Here’s an example. A large, international non-profit organization uses batch ingestion to collect data on donations made to its website and over the phone. On average, it receives over 1,000 donations a day and these donations come in around the clock. Once a day at a scheduled time, the non-profit uses a batch ingestion tool to collect the donation data received in the past 24 hours from both sources, sort and group the data into a single batch, and load it into a staging area. Once in the staging area, the data is ready to be processed, stored, and used for analysis. The non-profit can use this data to generate reports on donation trends, like the number of daily donations, the average donation amount, and donor count by country. The second main way to ingest data is streaming ingestion. Streaming ingestion is a method of collecting and processing data as soon as it becomes available. Streaming ingestion is best for data teams that need to process and act quickly. Here’s an example. A pharmaceutical manufacturing company uses sensors to monitor the temperature of their dryers. The company’s data team uses streaming ingestion of the sensor’s temperature data as soon as it becomes available. This allows the company to track temperature changes in near-real time to prevent any issues in their pharmaceutical products. If the dryer temperature starts to rise or fall below a safe threshold, the company can respond quickly to return the dryer to a safe temperature. By monitoring the temperature of its dryer in near real time, the company can ensure the quality and safety of its products, and keep their equipment in working order. Now you understand the data ingestion techniques and how to consider time sensitivity when you choose a technique. Choose batch ingestion or streaming ingestion to get the data you need, when you need it!

### Document - [Strategies and tools for efficient data ingestion](https://www.cloudskillsboost.google/course_templates/963/documents/472282)

### Video - [Data mapping helps ensure consistent data](https://www.cloudskillsboost.google/course_templates/963/video/472283)

- [YouTube: Data mapping helps ensure consistent data](https://www.youtube.com/watch?v=3wXyZGhwmQc)

Data mapping is a key part of the data pipeline and can be a complex process. But you may have done something similar before. For example, let's say you're an avid bird watcher. While at a park, you spot a bird species that you've never seen before. So, how do you identify it? Well, first you closely observe the bird and identify its size, shape, color, markings, behavior, and call. Then, you can compare these features that you observed to descriptions of known bird species using a field guide. By matching the bird’s features to the field guide, you can try to identify the bird. That’s what data mapping is like! Data mapping is the process of matching fields from one data source to another. And like identifying birds with a field guide, data mapping compares information to a known entity. Data mapping can use an entity like a schema, to make an identification. Data mapping is also a critical part of the data pipeline. As a data analyst, once you ingest your data, you map the data so you can easily understand and analyze it. This ensures the data is consistent and standardized. Here’s how it works! For example, the data team from a large, public library wants to combine data from two sources to track the twelve-month history of their book circulation and number of new requests patrons’ make. The first data source is a library catalog that includes the International Standard Book Number, or ISBN, title, author, publisher, and publication date for each book they own. The second data source is the circulation database that includes the barcode, title, author, and due date for each book in their collection. To combine the data, the data team will need to find a way to match the fields between the data sources in a way that makes sense. To do this, they can use data mapping. First, the team identifies the fields that need to be mapped. In this case, the ISBN, title, author, publisher, and publication date fields. Next, the data team standardizes the naming conventions for the fields. For example, the title field in the circulation database is named "Book Title," while the title field in the library catalog is just called "Title." But both fields mean the same thing. So, the data team decides on one name —in this case, “Title”— to use for both fields. Next, the data team creates their data mapping rules. Data mapping rules are a set of instructions that define how the fields will be matched. For example, the library team defines a process for converting the barcode in the circulation database to an ISBN number. This is because the two fields need to match when the data team begins to test and analyze the data. Once the data team defines data mapping rules, the data team will next test the rules on a small subset of data. This will ensure the rules are working correctly before applying them to all of the data. Next, the data team creates a map that defines how the data fields relate to one another. Finally, they take all of the data from the two sources, and combine it into a single dataset. Now, the data team can process and analyze the dataset. Of course, this is just a simple example. Data mapping is usually much more complex. Data mapping can also be challenging to do manually, and may be time-consuming and error-prone. That's why many data teams rely on data mapping tools that can automatically match one field to another. When a data team is deciding between manual or automated data mapping, the choice will largely depend on the structure of the data the data team needs to map, the size of the project, and the tools they have available. No matter how an organization maps their data, data mapping is a key part of the data pipeline. It ensures that data is standardized and consistent, making it easier to use and analyze. This can lead to better data quality, which can help organizations make better decisions with their data.

### Quiz - [Test your knowledge: The extract stage of data pipelines](https://www.cloudskillsboost.google/course_templates/963/quizzes/472284)

### Document - [Data transformation steps to ensure reliable data](https://www.cloudskillsboost.google/course_templates/963/documents/472285)

### Video - [Introduction to profiling and cleaning data](https://www.cloudskillsboost.google/course_templates/963/video/472286)

- [YouTube: Introduction to profiling and cleaning data](https://www.youtube.com/watch?v=k1JAoIaH8KY)

When you go to a store, the shelves are full of items of all shapes and sizes. But how does the store’s management track all these items? Stores regularly conduct inventories. This is the process of counting items, making sure everything is in order, and ensuring the expected number of available products are either on the shelves or in the warehouse. Profiling and cleaning data is like taking inventory of a store's items. Data profiling is the process of exploring data to identify quality issues. It gathers information about the data's structure, format, values, and relationships. Some quality issue examples might include: missing values, duplicate records, inaccurate data, and inconsistent data formats. Once data profiling has identified quality issues, data cleaning can begin. Data cleaning is the process of fixing or removing data quality issues. This ensures that the data is accurate, consistent, and complete. To understand how data cleaning works, consider an on-the-job example with a data analyst, named Arpa. Arpa is a data analyst for a retail chain. Every month, each store in the chain sends Arpa data about the items in stock. Arpa's job is to get this data ready for storage and analysis. The tasks to fulfill Arpa’s job include: profile the data and clean the data, which includes fixing any data quality issues. First, Arpa profiles the data. This is important because it helps ensure the data is accurate and complete before it's transformed, stored, and used for analysis. Data profiling can be done manually, but it's usually more efficient and effective to use a data profiling tool. A data profiling tool can gather information about the data's structure, format, values, and relationships. For example, Arpa might use a data profiling tool to identify the different columns in the data set, like item name, description, price, and quantity in stock. Arpa might also determine the data type for each column, like a string for the name, and a number for stock level. Arpa might also consider missing or duplicate values. Now Arpa has a profile of the data and can start to clean it. This involves fixing or removing any data quality issues identified during the profiling process to ensure the data is accurate. For example, Arpa might find that there are a number of duplicate item names or that some prices are missing. Finally, Arpa needs to fix any data quality issues to make the data complete. So to recap, data profiling and cleaning are important ways to prepare data to help improve data quality. By regularly profiling and cleaning data, cloud data analysts can ensure a business has the best possible data to make informed decisions.

### Video - [Common ways to manipulate data](https://www.cloudskillsboost.google/course_templates/963/video/472287)

- [YouTube: Common ways to manipulate data](https://www.youtube.com/watch?v=j451u-GO_dA)

When you think of a data pipeline, you might imagine a linear process where data is collected, processed, and analyzed in a specific order. But this isn't always the case. In reality, data pipelines can be more complex and flexible. And the order of the processes and techniques a data analyst uses in a data pipeline varies depending on the specific needs of the project! For example, consider you’re a data analyst for a large school district and you have a dataset of student data with errors and different formats. In general, it's best to clean the data before manipulating it. But if the data is full of errors, you might want to use data manipulation to remove the most obvious errors first. This can help speed up the data cleaning process and improve the accuracy of the results. The techniques that you use and the order depend on the specific data. That is why, as a cloud data analyst, it's important to have an understanding of the techniques available to you, and how you can use them in various ways to get the data results you need. Let’s dive into an example using three common ways to manipulate data and make it usable for storage and analysis: data standardization, enrichment, and conversion. Data standardization is the process of ensuring that all the data in a dataset is in a common format. This makes the data consistent and reliable, so it's easier to process and analyze. Here’s an example. Kyle is a data analyst who is responsible for managing the product catalog for a large online retailer. When adding new products to the retailer's database, Kyle notices the product names are inconsistent. Some of the product names are in all uppercase letters, while others are in lowercase letters. To standardize the data, Kyle decides to use a data standardization tool to check the format of the product names, and make all names in lowercase letters. This will make the data more consistent, and easier to process and analyze. Kyle also wants to add the Stock Keeping Unit, or SKU number to each product to help track product stock quantities. To do this, Kyle needs data enrichment. Data enrichment is the process of adding additional information to data. This can be done by adding new fields to the data, or joining the data with other data sources. Kyle uses the data enrichment tool to join product names from the database to SKU’s in a product catalog. This will add an SKU for each matching product in the database. Kyle then uses these SKU’s to help the retailer better track inventory and manage sales. Once Kyle standardizes the data to make it consistent, and enriches it with new information, Kyle then needs to format the data in a way that works with the destination storage. Data conversion is the process of changing the format of data to improve compatibility, readability, or make data more secure. This can be done for a variety of reasons, like to make the data compatible with a different system or application, saving storage space, or to make it easier to understand and use. Because the data is large and complex, Kyle needs to compress the data before moving it to the destination storage. So, Kyle uses a series of tools to read the CSV and convert it into Parquet. Parquet is an open source file format that stores data as columns and optimizes the data for efficiency and analysis. So to recap, data standardization, enrichment, and conversion are three techniques you can use to manipulate your data and make it more useful. And as a cloud data analyst, ultimately the techniques you use will depend on your data and your business needs. Understanding the different options and how you can apply them will help you choose the best way to prepare and manipulate your data. This will ensure that your data is in the best shape possible for storage and analysis.

### Document - [Data transformation techniques that achieve different business needs](https://www.cloudskillsboost.google/course_templates/963/documents/472288)

### Document - [Transformation approaches for securing data](https://www.cloudskillsboost.google/course_templates/963/documents/472289)

### Document - [Data anonymization with generative models](https://www.cloudskillsboost.google/course_templates/963/documents/472290)

### Quiz - [Test your knowledge: The transformation stage of data pipelines](https://www.cloudskillsboost.google/course_templates/963/quizzes/472291)

### Video - [Different approaches to loading data](https://www.cloudskillsboost.google/course_templates/963/video/472292)

- [YouTube: Different approaches to loading data](https://www.youtube.com/watch?v=u1r3m8fpnFc)

When data moves through a data pipeline it typically goes through three stages: extract, transform, and load, or ETL. In the extract stage, data is collected from various sources. In the transform stage, data is cleaned, manipulated, and enriched. And in the load stage, data is moved to a new place where it can be used for analysis. To help you better understand what happens during the load stage, let's start with an example. Let’s say you want to donate some items to a local charity. First, you sort through your belongings and collect what you want to donate. Then, you clean up everything to make sure all the items are in tip-top shape. Finally, you load the items into a charity truck so they can be used by others. Collecting the items to donate is like the extract stage in a data pipeline. Cleaning up the data is like the transform stage. And loading the items onto the charity truck is like the load stage. The load stage of the data pipeline moves the data into the destination storage. While the load stage is typically the last stage of a data pipeline, this is not always the case. In an ETL pipeline, which stands for extract, transform, and load, the load stage is the last stage of a data pipeline. But sometimes as a data analyst, you will load the data before you transform it. When this happens it is called an ELT, pipeline meaning extract, load, transform. But, no matter when the load stage happens, the load stage has the same goal: taking data from a staging area and putting it in storage. Here’s how it works. Before loading data can be put into storage, you first need to prepare the destination storage. This will ensure the destination is ready to receive the data. Depending on the data, this might involve creating new tables or directories, or configuring the destination so it can accept the data. Then, once you prepare the destination, you’re ready to load the data. Common ways to load data include batch loading, streaming loading, and incremental loading. Batch loading is a method where data is moved to destination storage in groups called batches, at a predetermined schedule. Batch loading is the most common way to load data. It's efficient for large datasets, but it can overload the destination storage if the data volume is high. Streaming loading is a method where the data is moved to destination storage in a continuous stream, as soon as it becomes available. Streaming loading can be a good choice for time-sensitive data. This is because streaming loading ensures that the data is available in near-real time, which can be critical for making timely decisions. Streaming loading can also help prevent the destination system from being overloaded. Unlike batch loading, the data in streaming loading is processed as a continuous stream, even one record at a time. Incremental loading is a method where only the data that has changed since the last load is moved to destination storage. Incremental loading is a great way to save time and resources when loading data, especially for large datasets that change frequently. How often data needs to be incrementally loaded will depend on a few factors, like the size of the dataset, how frequently it changes, and the performance requirements of the destination system. After loading data, it's important to verify its integrity and accuracy with a final check. This ensures the data is ready for analysis. When you work with large datasets, you may find data loading is a complex process. That’s why many data teams use automated tools to help load data efficiently, and avoid data loss and errors. Even if your team uses automated tools, it's still important to understand how the data loading process works. The data loading process impacts the quality of the data that you'll analyze, and helps drive decision-making across your organization.

### Video - [Overview of data validation strategies and rules](https://www.cloudskillsboost.google/course_templates/963/video/472293)

- [YouTube: Overview of data validation strategies and rules](https://www.youtube.com/watch?v=f-auO40nd8I)

An important role on any assembly line is the quality control inspector. The quality control inspector’s job is to make sure that all the products meet the required standards and are ready to be sent to consumers. For example, a quality control inspector on a toy car assembly line may inspect each car for mistakes, like missing or damaged parts. They may also compare each car to pre-written specifications to ensure they meet the required standards. All this is done to ensure that each product meets quality standards and is ready to be used by consumers. An assembly line inspection is a lot like data validation in a data pipeline. Data validation is the process of checking and rechecking the quality of data so that it's complete, accurate, secure, and consistent. Data validation is part of the extract, transform and load, or ETL stages, in the data pipeline. The data analyst can do this at any of these stages. But it's especially important in the load stage, when the data analyst moves the data into destination storage. That's because it's the last chance to fix errors before the data is used for analysis and reporting. Like other processes, the exact technique a data analyst uses to validate data will depend on the data they process, and the organization’s business needs. Some common ways to validate data that a data analyst can use include checking for type, format, uniqueness, correctness, and null values. Let’s explore each in more detail. For example, consider working as a data analyst for an international non-profit organization that collects data on donations made to its website and over the phone. The non-profit’s data team stores this data in a donor database that includes information such as donor name, donor zip code, and donation amount. For the donor zip code field, you want to make sure that it's a number data type, and not a string. You can use type validation to make sure the data is the right type. You can use format validation to make sure the data is in the correct format. For example, if the values in a field should be formatted as dates, your validation process should check to make sure they're formatted in the same way. You can use duplicate validation, also called uniqueness validation, to check that there are no extra copies of records in the dataset. For example, if you have a table of donor records, you want to make sure that there aren’t two donors with the same name or with the same email address. You can use range validation to check that the data is within a valid range. For example, if you have a field for the donor's age, you would want to make sure that the age is a number that is between 0 and 130. You can use null validation to check for null values in the dataset. Null values are empty or missing values, and they can cause problems with data analysis and reporting. So what happens when you can’t validate data? That depends on the validation rule. Validation rules are a set of instructions that specify the standards that must be met for data to be valid, and how to handle data errors. Sometimes, you may discard the data you can’t validate. You might do this if the errors are too complex to fix, or if the data isn't critical. For example, if a donor's age is outside of the valid range, you might remove this data. You may also load the data into storage as flagged. Flagging data that you can’t validate alerts users that they need to fix the data before use. For example, if a donor's email address is misspelled, you might flag the data and fix it manually. Finally, your data may be corrected automatically. For example, if a donor's zip code is invalid, the address may be matched against a database to correct the mistake automatically. To sum up, data validation is like quality control for data. As data moves through a data pipeline, it's important to make sure the data you're analyzing is complete, accurate, and consistent. This will help ensure that when you use the data for analysis, the insights are reliable, and you can get the most out of your data.

### Document - [Data dictionaries and monitoring help maintain data integrity](https://www.cloudskillsboost.google/course_templates/963/documents/472294)

### Document - [Analysis and visualization after the data pipeline](https://www.cloudskillsboost.google/course_templates/963/documents/472295)

### Lab - [Creating and managing SQL pipelines](https://www.cloudskillsboost.google/course_templates/963/labs/472296)

Create a BigQuery schema

- [ ] [Creating and managing SQL pipelines](../labs/Creating-and-managing-SQL-pipelines.md)

### Quiz - [Test your knowledge: The load stage of data pipelines](https://www.cloudskillsboost.google/course_templates/963/quizzes/472297)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/963/video/472298)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=yB57Monvqy0)

Hi, there! Congratulations on completing the section on data pipelines! As a data analyst, now you know how data pipelines can help you address business needs and can put them to work! During your journey, you learned the importance of data pipelines, and how they can help you address your business needs. Let’s review what we covered in more detail. To get you started, we reviewed data pipelines, their place in the data journey, and their role in transformation. Then we explored the three stages of a data pipeline: extract, transform, and load, or ETL. First, you learned about the extract stage, including the differences between batch and streaming data ingestion. Then, you explored the transform stage and how data is cleaned and manipulated for storage and analysis. Finally, you reviewed the load stage of a data pipeline, including data validation and monitoring. Data pipelines can be complex but learning about how data is collected, transformed, and made ready for storage and analysis is important for a cloud data analyst. Congratulations on continuing your journey through cloud data analytics!

### Document - [Glossary terms from module 2](https://www.cloudskillsboost.google/course_templates/963/documents/472299)

### Quiz - [Module 2 challenge](https://www.cloudskillsboost.google/course_templates/963/quizzes/472300)

## Cloud data optimization strategies

In this module, you'll identify the key benefits and challenges of implementing a cloud data strategy capable of handling the volume and velocity of modern data and optimizing the data for analysis. You'll also gain hands-on experience applying common data transformation strategies to a dataset to clean data, generate summary metrics, join data, and create derived data. 

### Video - [Welcome to module 3](https://www.cloudskillsboost.google/course_templates/963/video/472301)

- [YouTube: Welcome to module 3](https://www.youtube.com/watch?v=JA9dQnbqTY8)

Hi there, and welcome! Now it's time to make some room in your toolbox for some new data analytics skills! In this section, you'll explore the challenges of data transformation in the workplace and learn the strategies you can use to get the most out of your data. You'll also be ready to apply these strategies to meet business needs on the job. Just like a fine craftsperson, first, you’ll take steps toward measuring the benefits and challenges of data transformation. As a cloud data analyst, you’ll work with the volume, velocity, and variety of data that large organizations generate daily. And you’ll consider a blueprint on how you can help organizations make use of that data. Next, you’ll learn the importance of designing a data transformation plan that uses effective strategies to find the value in these vast volumes of data being generated every day. After that, you'll dive into how data transformation is used in the workplace. This includes how to extract meaningful insights using aggregations and deduplication strategies to handle duplicate data. Finally, you’ll learn strategies to join and derive data effectively. Data transformation is a key skill for cloud data analysts in organizations of all sizes. Are you up for the challenge?

### Document - [Benefits of data transformation plans](https://www.cloudskillsboost.google/course_templates/963/documents/472302)

### Video - [Challenges of data transformation](https://www.cloudskillsboost.google/course_templates/963/video/472303)

- [YouTube: Challenges of data transformation](https://www.youtube.com/watch?v=oeXAzo4fxrY)

Hi! In this video you will learn about the challenges that face data teams when transforming data in the workplace and why it's important to have a reliable and high-quality data transformation plan. Data is an essential raw material that drives business decision-making. But in recent years, the amount of data being managed and stored by organizations has grown tremendously. As a cloud data analyst working with large datasets, you'll uncover the value of the vast, ever-growing intake of data to help provide critical insights to you, your data team, and your stakeholders. But, implementing a data transformation plan can be challenging, especially when it comes to resources and data integrity. Let’s break these two challenges down. Data transformation can be resource-intensive! First, you'll need access to enough computational power to process large datasets. In the past, this meant investing in expensive computer hardware that could perform a very large amount of calculations, also called computational loads. But with the advent of cloud computing, you can now access computational power virtually. This is a game-changer for data teams, as they can now perform data transformations without having to invest in expensive hardware. But, even though cloud resources are easy to use and cost-effective, you still have to include the cost of cloud services when planning your project. Data transformation can also require a lot of storage. Extract, transform, load, or ETL, and its counterpart, extract, load, transform, or ELT, are pipelines for data transformation. As a cloud data analyst, you use pipelines to take raw data from various data sources, process the data, and then store it in a warehouse or a lakehouse for use in analysis and visualization. But, sometimes you don’t use all of the stored data. This can cost organizations unnecessary storage fees. To prevent these fees, you must effectively manage storage usage. This means regularly reviewing the data that's stored and deleting any data that you no longer need. Finally, it can take a lot of time and effort to transform data. Data transformation requires many people with the right skills to execute the data transformation plan. For organizations with smaller budgets, this can be a challenge. So it’s important to utilize the resources of cost-effective cloud services. Not all the tools you use for data transformation are the same. Some tools are better for certain tasks than others. As a cloud data analyst, you'll need to learn about the different tools available when you work with data in the cloud, and how to use the right tools for your specific needs. While resources are usually the largest costs for an organization, data integrity may be the most significant ongoing challenge for a data team. This is because even a small error in data can have a big impact on the results of a data transformation project! Data integrity is the accuracy, completeness, consistency, and trustworthiness of data throughout its life cycle. Errors in the data can impact data integrity before and during the transformation process. Errors can occur in three primary ways. Mistakes, like typos and other data entry errors, can introduce issues into data. And machines can also insert bulk reading errors into the data. These errors can cause data integrity issues if the data is not cleaned before transformation. Errors can also be introduced during the data transformation process. For example, converting data from one format to another can introduce errors, like incorrect data types, missing values, and incorrect formatting. Likewise, using the wrong aggregation method can cause misleading results. If these errors aren't caught before the data is stored and accessed for analysis, mistakes can end up in the data used for reports and dashboards. And this can compromise the integrity of the data used for decision-making! While you can never eliminate errors completely, it's important to have an effective plan to help manage data integrity and minimize mistakes. This will help ensure that stakeholders have the best data they can to make the decisions they need to drive the business forward. As a cloud data analyst, you'll be responsible for finding value in the increasingly complex and massive amount of data that flows into your organization. This can be a daunting task, so it's essential for making informed decisions and driving business growth. To do this, you'll need to have a data transformation plan. This plan will help you manage your resources and ensure data integrity so that you can provide critical insights to your data team and your stakeholders.

### Document - [Steps for developing a data transformation plan](https://www.cloudskillsboost.google/course_templates/963/documents/472304)

### Document - [Activity: Analyze a data transformation plan](https://www.cloudskillsboost.google/course_templates/963/documents/472305)

### Quiz - [Activity Quiz: Analyze a data transformation plan](https://www.cloudskillsboost.google/course_templates/963/quizzes/472306)

### Quiz - [Test your knowledge: Data transformation plans](https://www.cloudskillsboost.google/course_templates/963/quizzes/472307)

### Video - [Data aggregation can make it easier to extract meaningful insights](https://www.cloudskillsboost.google/course_templates/963/video/472308)

- [YouTube: Data aggregation can make it easier to extract meaningful insights](https://www.youtube.com/watch?v=2SAx0yIWFjM)

Hi! In this video, you’ll learn how to use data aggregation! Aggregation is one of the most common transformation techniques you can use to gain meaningful insights from your data. Data is coming from everywhere —smart devices and machines, websites, social media, videos— and more and more of it's arriving in near-real time. All this data is an opportunity to foster a more data-driven culture. But it's also a challenge for data analysts because the sheer volume, variety, and velocity of this data can be overwhelming. One technique that can help you get control of that data is data aggregation. Data aggregation is the process of gathering data and expressing it in a summary form. Aggregation can help you extract meaningful insights from data in three ways: by managing data, by making data more accessible, and by helping you observe data trends. Let’s break down each one. Due to an increase in the volume and velocity of data in recent years, businesses around the world face a rising need to acquire more and more data storage. In reality, businesses don’t actually need all the data they store! And here’s a pro tip: Not all data is relevant or should be stored forever! Irrelevant data takes up storage space and makes it more difficult and labor-intensive to find the data insights you need. To reduce the data “clutter” and reduce irrelevant data, you can use aggregations. Let’s explore an example. Every time a person clicks a link on a company’s website, multiple data points are generated, including the exact time the link is clicked, the IP address, and the browser type to name just a few. On a busy site, this can create a massive amount of data being generated. But, not all of the data generated is relevant and useful. Aggregation can help you clean the data so that only meaningful data is stored, and irrelevant data is removed. To return to our example, two important metrics the businesses’ data team wants to track include the number of times a specific link is clicked per hour, and the date and time of each click. This data will help the marketing team plan their ad buys and forecast future traffic. The rest of the data generated each time the link is clicked is not relevant to the business question. So, by scheduling an aggregation to count the total clicks each hour, the raw data is reduced to a single data point, the average number of clicks. This aggregated metric can then be stored instead of the raw data. Using this strategy allows you to better manage the amount of storage needed, while providing a metric that's useful for analysis. Aggregation can also help make data more accessible to the data team by reducing the amount of data they need to work with. For example, since the single data point —the average number of clicks per hour— is stored, the data team can simply query that metric, without performing calculations. This can save time and effort. Aggregations also make it easier to spot trends in data. Knowing the data trends helps data teams provide analysis and visualizations so the business can make informed decisions. For example, since the data team can easily query the average number of clicks per hour, they can chart the data to better understand how the data changes over time. The marketing team can then use this insight to determine when the website link receives the most clicks. They can use this information to better plan their ad buys and forecast future traffic. Of course, that's just one example of how data teams can use aggregation to manage storage, make data more accessible, and help users spot trends. Aggregation is a versatile tool to use in an organization's overall data transformation plan. The possible benefits to a business are endless!

### Document - [Nulls and aggregate data](https://www.cloudskillsboost.google/course_templates/963/documents/472309)

### Video - [Consequences of duplicate data and how to eliminate it](https://www.cloudskillsboost.google/course_templates/963/video/472310)

- [YouTube: Consequences of duplicate data and how to eliminate it](https://www.youtube.com/watch?v=u8U4YBm9RTI)

Hi! In this video you will learn about one of the most common errors you'll deal with as a cloud data analyst: duplicate data. Duplicate data is a record that repeats the information —in whole or in part— of another record. There are two types of duplicate data: partial duplicates and exact duplicates. Partial duplicate is a record where only part of the data repeats another record Let’s explore how this works with an example. A science organization database holds information about members from around the world. But they may have a duplicate! Two records have the first name “Eli,” the last name “Arnez,” and the exact same Madrid address. But, one of the records has the phone number “1223 555,” and the other one has no phone number listed at all. Exact duplicate is a record where all the data repeats another record. For example, let’s say the organization’s data analyst reviews the database and notices that one entry was entered —with exactly the same information, including name, address, and phone number— twice. No matter how the duplication happens, partial and exact duplicates can damage data integrity. This can lead to inaccurate reporting, wasted resources, and increased storage costs. Let’s consider another example. In this case, the organization’s data analyst wants to know the average price of merchandise sold in the last month. But when they aggregate the monthly sales, the result is higher than expected. What went wrong? They take a closer look at the database and spot the error. There were duplicates! One shirt that sold for $20 had been entered three times. This overrepresentation of that value skewed the data. Ultimately, when the data was aggregated using average, the results were skewed. But once they remove the duplicate data, the dataset is no longer skewed and they get the accurate metric. Duplicate data can also cause wasted resources for organizations. Next, consider an example of wasted resources. In the hopes of improving merchandise sales for the next quarter, the head of the science organization’s marketing team wants to send an advertisement with a list of all the merchandise available to each member. They know that each time an address appears in the dataset, the member will receive an advertisement and if the member’s address appears more than once in the dataset, the member may receive multiple ads. So they ask the organization’s data analyst for help. Good thing, too: the data analyst knows what to do! They find and remove any duplicates in the data before the ads are sent out, making sure that each member receives only one ad. This action preserves the organization’s marketing budget by avoiding extra costs. Of course, in these examples, the datasets are small, making the errors easy to spot, and the costs of wasted resources negligible. But imagine scaling these errors to the size of big data, where there may be millions of rows and many duplicates hidden in the data. With datasets of that size, finding duplicates can be tricky. Duplicates also take up storage space, which can lead to redundant data that bloats your storage and increases storage costs. Deduplication is the process of eliminating a dataset's redundant data. And deduplication is an important part of an organization’s data transformation plan. Data analysts can perform deduplication manually, but they usually use automated deduplication tools. Data analysts use manual deduplication to compare rows of data to identify duplicate values. This is a good option to fine-tune small datasets, but it can be time-consuming and inefficient for large datasets. Deduplication tools use algorithms to compare chunks or blocks of data. This is a more efficient way for data analysts to find duplicates in large datasets, especially partial duplicates. Whether working with small datasets or big data, duplicates negatively impact data quality and can cause inaccurate insights. That’s why it's so important for data teams to have a plan to deal with duplicate data.

### Quiz - [Test your knowledge: Data aggregation and deduplication](https://www.cloudskillsboost.google/course_templates/963/quizzes/472311)

### Video - [Overview of how joins combine data from different tables](https://www.cloudskillsboost.google/course_templates/963/video/472312)

- [YouTube: Overview of how joins combine data from different tables](https://www.youtube.com/watch?v=3GR8qlK-xAc)

Hey there! In this video, we'll explain joins, a powerful tool that data analysts use to combine data from different tables. As a cloud data analyst, joining tables efficiently can be tricky. Two common challenges you may find when joining tables are missing data and null values. Missing data is when the rows you expect to be returned do not appear in the joined table. But, not all missing data is a problem! Missing data is usually the result of the type of join that a data analyst selects. This is why it's important to understand inner and outer joins and their impact on the data returned. An inner join is a join that returns the rows that match in both tables. Let's consider an example to find out how this works. Kenji, a data analyst, wants to join an inventory and information table. Both tables are related by a "shoe ID" column, which contains the shoe inventory number. To find the matching data, Kenji chooses an inner join. But, when Kenji reviews the joined table, some rows are missing. This is because an inner join only returns rows that match in both tables. Any rows that do not match are omitted from, or dropped from the join results. For example, the row with the "shoe ID" of 3 appears in both the inventory and information tables, so it's included in the joined table. But, the row with the "shoe ID" of 9 only appears in the information table, so it's dropped. To drop fewer rows and return more data, Kenji can use an outer join instead. An outer join returns both matched and unmatched rows from one or both tables. There's three types of outer joins. Left outer joins return all the rows from the left side of the join clause and only matched rows from the right. Right outer joins return all the rows from the right side of the join clause and only matched rows from the left. Full outer joins return all the rows from both sides of the join clause. Kenji decides to use the most common outer join: a left outer join. Kenji places the inventory table to the left of the join clause to return both matched and unmatched rows from that table. Kenji then adds the information table to the other side of the join clause. Only the matched rows return from this table. The left outer join results in more rows being returned, including both matched and unmatched rows, but it also introduces a new challenge for Kenji. Some rows in the joined table will have null values. The unmatched values in the inventory table are represented as null values in the joined table. Let’s explain what these null values are. A null value means there's no available value for a field. In the case of an outer join, a null means that no match could be found, so the value for that field is absent. Here’s a pro-tip: null values are not zeros or blank data. It's also important to remember that null values are also not the same as zeros. This can actually be useful to you as a data analyst. For example, when you return all of the data from one table, even if there's no match in the other table, you provide a placeholder for a value that you have yet to determine or collect and add to the table data. While null values are the intended behavior of outer joins, they can be confusing. For example, it can be hard to tell the difference between null values that are a result of no match with the join, and null values that existed in the data before the join was done. This can impact your results. When using an outer join, it's important to think about how your specific data set and how null values may impact your analysis before deciding on an outer join. So, there you have it! While joins can be tricky, understanding the why behind missing and null values will help you, as a data analyst, join data more efficiently and provide useful insights. But like all data transformation strategies, the best approach is the one that makes the most sense for your data needs.

### Document - [SQL strategies to join data effectively](https://www.cloudskillsboost.google/course_templates/963/documents/472313)

### Video - [Data derivation to combine data and obtain new insights](https://www.cloudskillsboost.google/course_templates/963/video/472314)

- [YouTube: Data derivation to combine data and obtain new insights](https://www.youtube.com/watch?v=IFZYEgoB4Ww)

To find the answers to tough data questions, sometimes you need to combine data in a new way. For example, say you want to find the total points earned by the top three players in a chess tournament. You have the points of each player, but to find the answer, you need to find the top three player scores and then combine these scores to create a new metric: the total points scored by the top three players. In data analysis, to unlock insights like this you can use data derivation. Data derivation is the process of combining and processing existing data using an algorithm to create new data. The data derivation process starts with raw data, sometimes called base data, that already exists in the dataset. This base data is then transformed using an algorithm, which is a process or set of rules followed for a specific task. The algorithm derives new data. Data analysts can use this new data to uncover insights that aren't available directly from the base data alone. Let’s explore how this works. Anya, the manager of a large shoe company, has a business request for the data team. The accounting department has identified that shoes that stay on the warehouse shelf too long are usually only sold at a steep discount, cutting into profits. To help track how long shoes stay on the shelves, Anya has asked Kenji, the data analyst, to create a daily report of the shoes that have been on the shelf for 30 days or more. But, that raw data does not currently exist in the dataset. To provide Anya with the data she needs, Kenji will need to use data derivation. To start, Kenji explores the existing data. Kenji’s goal is to find and transform the base data to create the report Anya needs. Kenji learns that warehouse workers scan and date-timestamp each pair of shoes when the shoes reach a warehouse shelf. Warehouse workers record this information as the “arrival date-timestamp.” Kenji realizes that it’s possible to use this arrival date-timestamp data to create the new, derived data. In this case, the algorithm that Kenji needs is a simple calculation that uses the shoe inventory number and the arrival date-timestamp to calculate how long each pair of shoes is on the warehouse shelf, and then convert the calculation into a formatted report to help Anya make decisions. Using this algorithm, the arrival timestamp value for each pair of shoes is processed resulting in derived data. Kenji can then query the derived data —as if it were an actual table in the database— to analyze and visualize the data as a daily report for Anya and her team. This daily report gives Anya a simple way to interact with the most up-to-date information about the warehouse shoe stock so she can make timely data-driven decisions. Anya will know which shoes in the warehouse should move to the sales floor before they lose more value. While working with derived data is a great way to increase performance and provide more in-depth insights to stakeholders, data derivation does pose some challenges for you as a cloud data analyst. One of the challenges of derived data is accuracy. Derived data is processed data, and is not always as accurate as the original, base data. This is because, when you create derived data, you take raw data and apply an algorithm to it. If the algorithm has an error, it can insert errors into the data. Also, the base data itself can have errors or may even change after the data is derived. So, while you can use derived data for analysis, you need to be careful when using derived data to create new derived data! You should also be aware of data ownership and privacy issues when working with derived data. For example, let’s revisit the shoe company. When Anya hires a new worker to staff the warehouse, the worker provides personally identifiable information, or PII, as part of the hiring process. At that time, the new hire was aware that the data was being collected and consented to its use. But this consent does not apply to future uses of the data. This makes it essential that any derivations involving PII are handled with the utmost care, and follow all existing policies and regulations. So to recap, transforming existing data into new data is a valuable tool for cloud data analysts. It can help you, as a data analyst, unlock insights that aren't readily available in the existing data alone. And, when done with care, it can be a powerful strategy for answering more complex questions with data.

### Lab - [Apply RFM method to segment customer data](https://www.cloudskillsboost.google/course_templates/963/labs/472315)

Apply RFM Method in BigQuery

- [ ] [Apply RFM method to segment customer data](../labs/Apply-RFM-method-to-segment-customer-data.md)

### Quiz - [Test your knowledge: Joins and data derivation](https://www.cloudskillsboost.google/course_templates/963/quizzes/472316)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/963/video/472317)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=tdnlPsVEr6k)

Congrats! You’ve made it! In this learning journey, you’ve delved into the challenges of transforming data in the cloud and how to get more value from your data. You learned about different strategies and practiced using them. First, you focused on the benefits and challenges of data transformation for organizations that work to find value in huge volumes, velocity, and variety of data. Then, you applied what you learned to design a data transformation plan that helped a marketing team solve a job-ready business problem. Next, you dove into data transformation strategies, including ways to extract meaningful insights using aggregation and data deduplication strategies. You also explored ways to create new insights with derived data, work with nulls, and effectively use joins to gain a more holistic view of your data. Congratulations on your progress so far!

### Video - [Lauren: What makes candidates stand out](https://www.cloudskillsboost.google/course_templates/963/video/472318)

- [YouTube: Lauren: What makes candidates stand out](https://www.youtube.com/watch?v=LNyMwfAWhd0)

Confidence is very important, and something that one of my first managers ever told me, it's not what you know, it's how you say it. And it's something that I continue to tell myself day after day. My name is Lauren, and I'm a cloud sourcing lead. What that means is my team is responsible for reaching out to candidates for our open roles within the cloud organization. The three top things that a candidate should do when updating or creating their resume is first tailor your resume to the job description. You wanna make sure you're pointing out keywords and key indicators that match your experience to ensure that the recruiters can see pretty quickly the experience that you have that's relevant to the job description. Secondly, do not exaggerate or do not lie on your resume. That probably goes without saying, but anything on your resume is technically fair game. It can be asked in an interview, so you wanna make sure that you're being as accurate as possible. My third tip would be, keep it concise, keep it short. We definitely wanna make sure that we can get all of your experience, but you can cut things out. That's okay too, if it's not relevant to the job. What makes a resume stand out for a recruiter or a hiring manager is an organized resume. You wanna make sure you have clear headers, it's in chronological order, and you have short and concise bullet points. Some ways that candidates can demonstrate their technical skills in interviews is to talk out their thought process in the answer to the questions they are asked. The process of how you get there is just as important as getting to the correct answer. Some common interview question types are problem-solving-based questions, experience-based questions and also leadership-based questions. The best way to answer a hypothetical question is really just talking through your thought process. You're gonna get a scenario, something that you may or may not have experienced before. And your interviewer just wants to see your thought process, how you're connecting the dots as you're talking through your answer. The best way to answer a behavioral-based interview question is really leaning on your own personal experience. This could be a personal experience from a previous job, from school, from a class, and talking through that experience and that thought process. It's very important to ask follow-up questions throughout the interview. First and foremost, you wanna make sure you're understanding the question that's being asked. You have a very limited amount of time with your interviewer and you wanna make sure that you're spending it as productively as possible. I think some candidates worry that it's bad to ask questions. I love when candidates ask me questions. I wanna make sure that my candidate knows what I'm asking so we can get to the best solution. Workplace skills are absolutely important during the interview process. These are skills that can't be taught. Are you kind? Are you empathic? How are you showing up? Do you have that eagerness to learn or to thrive in ambiguity? We really wanna see candidates that have that self-drive and that self-determination. A candidate can best position those workplace skills just on how they're simply answering their questions. Especially in those behavioral questions, lean on those examples that show your grit, that show maybe a struggle that you had and how you overcame it. I'm not looking specifically for something that happened perfectly the right way. I'm looking for someone that maybe had to overcome some obstacles to get to the right solution. Candidates hold a lotta power in the interview process. We want you to be successful. We want reasons to say yes. So please show up, be your authentic self, be confident, and show us all the amazing skills that you could bring to the company.

### Video - [Lauren and Ela: Interview role play](https://www.cloudskillsboost.google/course_templates/963/video/472319)

- [YouTube: Lauren and Ela: Interview role play](https://www.youtube.com/watch?v=jstLYyqaQlI)

Hi, I'm Lauren. Hi, I'm Ela. Let's step into an interview room and watch an interview that's in progress. In this interview, the questions will focus on cloud data analytics topics related to the data journey, from collection to insights. They hope this will give you some tips for your next interview. What interests you in a job in cloud data analytics? What are you looking for in a role? I'm really fascinated by the way that cloud data analytics is going to transform businesses, so the ability to access and analyze massive data sets, and datas, and gain a deeper understanding about customers, organizations, and markets is going to revolutionize different industries. And one thing that I'm really passionate about cloud data analytics is how it plays a role with machine learning and artificial intelligence. Imagine having access to large amounts of data with very powerful algorithms, you can develop sophisticated models that can help you predict outcomes, automate some tasks, and also create personalized recommendations for customers. For my next role, I'm looking to be in more challenging projects and use my cloud data analytics skills to solve real world problems. Tell me about a cloud data analytics projects that you worked on. What did you like most about it and what were some challenges? The most interesting cloud data analytics project that I worked on was a project with a large retail company. The goal of the project was to gain a deeper understanding of the company's customer base and how we can create targeted marketing campaigns. The project involved collecting and analyzing data from variety of sources. This would be their demographics, online behavior, their purchase history, and we use a variety of data analytics tool sets to retrieve and clean and prepare this data. And our results was understanding that there are distinct customer segmentations within this company, and we provided them with a set of recommendations on how to target these segmentations and how to create relevant marketing messages. And because of what we did in this project, the company was able to increase their conversion rates and customer engagements because now they're able to relay a little bit more targeted messages to their customers. The most enjoyable aspect of the project was the ability to have access and work with large amounts of data sets, and learning, along the way, new cloud data analytics tools and techniques. Let's suppose you're working on a project to help a cloud data team combine data from different sources, what considerations would you think about to make sure users got the most out of their data? - There are a couple things that comes to mind mind. One is data consistency and data quality, the second one would be data schema and relationships, third would be data security and governance, and lastly, would be data accessibility and usability. Tell me about a time when you had to derive data from a raw data set that you received. Describe your process and any challenges that you experienced. I worked for the large e-commerce company. The raw data included product clicks, page views, and their online behavior. The process included me using machine learning techniques, like natural language processing to understand their customer behaviors in different segmentations and based on the product reviews. And then later I used clustering algorithms to put customers based on their behaviors into different segmentations. And because of the machine learning techniques that I use, I was able to automate some tasks, this really accelerated the process and increased the accuracy of our results. We weren't able to get to the results that we did without these techniques. Great. Do you have any questions for me? What are some key attributes that would make me successful in this role? Great question. Some key attributes to being successful in this role is really just bringing that natural curiosity, problem solving and determination into your day-to-day. This is a very collaborative role, so you'll be working closely with your teammates to get to the right solution. In this scenario, Ela demonstrated how to explain impact with answering questions. When interviewing, make sure to explain the beneficial effects of any process, concept, or tool discussed.

### Document - [Interview tip: Explain impact](https://www.cloudskillsboost.google/course_templates/963/documents/472320)

### Document - [Glossary terms from module 3](https://www.cloudskillsboost.google/course_templates/963/documents/472321)

### Quiz - [Module 3 challenge](https://www.cloudskillsboost.google/course_templates/963/quizzes/472322)

### Video - [Course wrap-up](https://www.cloudskillsboost.google/course_templates/963/video/472323)

- [YouTube: Course wrap-up](https://www.youtube.com/watch?v=D0nbfzb2qT4)

Congratulations on finishing this course! I hope you’re ready to use the new data analytics tools in your toolbox and put your new skills to work! My favorite part about working in cloud data analytics at Google is helping customers get to that "ah-ha!" moment while analyzing their data. This moment usually comes from finding a key insight they previously didn't know about before, which can drive really impactful decision making and have a positive impact on their business. While it might seem daunting at first to find the key insight in the massive amount of data businesses produce every day, the techniques we are learning can help accelerate your insight investigation and lead to truly transformative work. Now, let’s go through all the things you learned in this course. You started by learning about the data journey including data collection, processing, storage, and transformation. Next, you explored the data pipeline and the various methods for extraction, transformation, and loading. Then, you discovered how to meet business needs with a data transformation plan and transformation strategies. You now know all about data transformation, the key to preparing quality data that leads to actionable insights. Great work throughout this course. Congratulations on this milestone!

### Document - [Course 3 resources and citations](https://www.cloudskillsboost.google/course_templates/963/documents/472324)

### Document - [Glossary terms from Course 3](https://www.cloudskillsboost.google/course_templates/963/documents/472325)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
