---
id: 1159
name: 'Introduction to Agent Assist and its GenAI Capabilities'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1159
date_published: 2024-09-19
topics:
  - Sentiment Analysis
  - Large Language Models
---

# [Introduction to Agent Assist and its GenAI Capabilities](https://www.cloudskillsboost.google/course_templates/1159)

**Description:**

In this course you will learn how Contact Center AI Agent Assist can enhance the productivity of human agents while interacting with customers through the Chat channel

**Objectives:**

* Understand Agent Assist vision, its features, and limitations
* Reflect on Agent Assist Architecture.
* Understand smart reply feature along with cloud environment setup
* Learn about Smart Compose feature
* Learn LLM baseline summarization and the steps for implementation.
* Learn about generative knowledge assist and integrating it with with GenAI agent
* Understanding sentiment analysis and how to enable it
* Learn about upcoming GenAI capabilities in Agent Assist.
* Understand requirements for delivering an Agent Assist Proof of Concept
* Understand the various phases of the delivery lifecycle.

## Introduction to Agent Assist

After completing this module, you will develop an understanding of Agent Assist Vision and its features. You will also be able to explain regionalization with Agent Assist and recognize its limitations.

### Video - [Introduction to Agent Assist](https://www.cloudskillsboost.google/course_templates/1159/video/508897)

* [YouTube: Introduction to Agent Assist](https://www.youtube.com/watch?v=gMPNvB3SSkM)

in this training module we will equip you with the necessary tools to facilitate the delivery of agent assist functionalities this empowers your agents to provide highquality support a sample user who can benefit from agent assist chat is a contact center agent in any industry with contact center operations such as telecommunications Health Care finance and Retail the main objectives of this section are developing and understanding the agent assist Vision exploring the various chat features in agent assist explaining the concept of regionalization with agent assist and reviewing regionalization limitations in agent assist the vision behind agent assist is to revolutionize the way we approach customer service Imagine A system that not only learns but adapts agent assist does just that by absorbing the skills of your top performing agents and integrating them into your entire service operation every business is unique an agent assist gets that it customizes its approach to meet business specific needs ensuring that the services provided are on brand with the business that it supports agent assist harnesses the power of AI by cutting down on repetitive tasks it frees up agents to focus on what truly matters delivering stronger customer support and lastly this isn't about reactive support agent assist is proactive it offers contextually relevant help boosting both productivity and Effectiveness imagine your agents having the right information at the right time every time that's what agent assist brings to the table let's delve deeper into how agent assist specifically empowers human agents through the power of AI imagine human call center agents equipped with a digital sidekick that offers in the- moment assistance guiding them through complex customer interactions the core idea here is productivity with agent assist your agents can handle more queries more efficiently it's like having an expert Whispering the right answers at the right time this not only speeds up response times but also ensures the quality of each interaction for chat support we have Smart reply this feature streamlines chat interactions by proposing accurate and contextually appropriate responses saving precious time for both agents and customers then there's the generative knowledge assist imagine AI agents providing proactive generative answers pulling from a wealth of knowledge articles it's like having an encyclopedia at the agent's fingertips about everything there is to know about the business always relevant and fast to retrieve the llm summarization feature which is also known as V2 summarization Baseline offers realtime abstractive summaries for both voice and chat in this course we will only explore the ones for chat those for voice will be covered in a subsequent course regardless of the channel llm summarization is customizable it allows agents to focus on specific areas and adapt to different writing styles think of it as having an instant accurate and tailored recap of every interaction with a customer these features collectively Empower agents to be more effective and efficient redefining customer service as we know it lastly agent assist supports sentiment analysis to better understand customer interactions at a deeper level sentiment is represented by score and magnitude values metrics that are returned in the response the score of the sentiment ranges between minus one which is negative and one which is positive and it corresponds to the overall emotional leaning of the text the magnitude value indicates the overall strength of emotion both POS positive and negative within the given text between 0.0 and infinite as we take a look at agent assist it's important to appreciate both where we are and where we're headed the features just introduced in which we will dive deeper in the course are already in full swing but the future looks even more exciting coming up we'll introduce custom llm summarization which will take our summarization capabilities to the next level with even more advanced AI get ready for the proactive gen knowledge assist enhancing the way knowledge is used and shared we'll also preview llm live translation breaking down language barriers in real time alongside this we'll debute the AI coach and build your own assist under preview offering unparalleled support and guidance to agents now that we have a high level knowledge of the key agent assist capabilities let's dive into the regionalization aspects regionalization provides data residency to keep customers data at rest physically within a specified geographical region or location this means that when the customer specifies a region their data at rest is not stored outside that region regionalization is especially helpful in scenarios when your system has regulatory or policy requirements that govern where your data must reside your network latencies might be improved when the data is in the same region as your customers an important point to keep in mind is that the agent assist regionalization and data residency does not apply to data in use or data in transit more details about available regions and available features for regional servings can be found in the additional resources section keep in mind two limitations associated with the regionalization when working with agent assist the first is that model training and agent assist does not support regionalization this this means that your data may get routed outside the region during this process the second relates to CCI transcription CCI transcription currently only supports multi-region data in use and data at rest if not using the speech adaptation feature in the European Union United States and Canada

### Quiz - [Introduction to Agent Assist Quiz](https://www.cloudskillsboost.google/course_templates/1159/quizzes/508898)

## Agent Assist Architecture

After completing this module, you will be able to explain the basics of the Agent Assist Architecture.

### Video - [Agent Assist architecture](https://www.cloudskillsboost.google/course_templates/1159/video/508899)

* [YouTube: Agent Assist architecture](https://www.youtube.com/watch?v=pxMMAhDedJE)

Next, let's discuss the Architecture of Agent Assist. To understand the architecture, consider an example where a chat from a customer is transferred seamlessly from a virtual agent to human agent with all the context. To connect to Dialogflow bot, the customer can use a chat client in the form of a chat interface on a website or any other third party tools. Upon connecting to the Dialogflow bot, the customer is greeted by the Virtual Agent. The conversation will occur between the customer and Virtual Agent until a handoff to the Human Agent is required ( for example in the case of an escalation by the customer). When the handoff is initiated, the conversation is forked from Dialogflow, establishing a connection with Agent Assist and with a Human Agent. Conversation data is sent to the Agent Assist  N-L-U module which can match Dialog intents and knowledge base articles based on the configuration. Knowledge services can provide two types of documents: FAQ documentation and articles. Agent Assist tries to match either the FAQ documentation or the content in the articles and provide suggestions through an API to the agent desktop. In the Agent Desktop, the human agent can access to the full conversation transcript for immediate context as well as historical analysis. As the conversation continues, the human agent will continue receiving suggestions from Agent Assist. It can provide feedback on whether those suggestions were accurate or useful in order to continuously improve the model.

### Quiz - [Agent Assist Architecture Quiz](https://www.cloudskillsboost.google/course_templates/1159/quizzes/508900)

## Smart Reply

After completing this module, you will be able to describe the features of Smart Reply and the Cloud Environment Setup. In addition, you will be able to explain the process for training, evaluating, configuring, and monitoring the performance of a Smart Reply model.

### Video - [Introduction to Smart reply](https://www.cloudskillsboost.google/course_templates/1159/video/508901)

* [YouTube: Introduction to Smart reply](https://www.youtube.com/watch?v=mb7tUaMhaOk)

now let's Deep dive into the smart reply feature the main objectives of this section are to introduce smart reply understand the prerequisites and the cloud environment setup understand data preparation for training train the smart reply model and learn about the evaluation of the model we'll then learn how to review the list of messages in the allow list configure communication between the agent assist smart reply feature and agent desk top monitor the performance of the model troubleshoot some common issues and learn about the upcoming features of smart reply customers generally contact businesses to help to resolve their issues while this communication happens a large amount of data is generated the frequency of the data is high due to daily recording agents generally use similar messages to deal with similar kinds of problems but different agents have different writing skills and styles of communication this generates a different customer experience in each interaction smart reply is a feature of agent assist which aims to assist live agents during their conversations with customers by providing useful and relevant message suggestions smart reply uses the long short-term memory or lstm model integrated with agent assist smart reply learns about the conversations and generates the most relevant response suggestions to the agent this helps maintain maintain the consistency and quality of the messages the agent sends it also helps reduce response time since the agent can click to send the response instead of writing the whole message smart reply surfaces responses to agents that are most relevant to the customer based on conversation context and historical data this means that in just one click the agent can provide customers with aners best cater to their needs this can greatly improve the average handle time of a call and the agent satisfaction and it has the potential to increase revenue from Cross and upselling this can easily turn generalist agents into specialist agents thanks to the real-time coaching offered by smart reply similarly specialist agents can be more like generalist agents because they can handle a wide variety of requests with coaching the bottom line is smart reply can be a personal assistant for every kind of agent it helps onboard and Coach them which trans plates in a great agent and customer experience

### Video - [Prerequisites and cloud environment setup](https://www.cloudskillsboost.google/course_templates/1159/video/508902)

* [YouTube: Prerequisites and cloud environment setup](https://www.youtube.com/watch?v=3Jor5phAWpE)

In this module, we'll introduce you to the prerequisites for training and using the Smart Reply feature with its corresponding cloud setup. The prerequisite for the enablement of the Smart Reply feature in Agent Assist is a Cloud project owned by the customer. Agent Assist uses Dialogflow API in the backend so the usage of the Dialogflow API needs to be turned on. Team members who work on the Smart Reply must also be Dialogflow and Storage API Admins. Remember that all resources (like models, or allowlist) generated from the chats created under the project, are wiped following project deletion. Let's go through the steps needed to setup the Cloud Environment. The first step is to locate a Google Cloud Platform project. You can visit console-dot-cloud-dot-google-dot-com and select the project you want to have Smart Reply in. Next you need to authenticate yourself in order to enable few APIs. Once authenticated, you can start Cloud Shell by clicking the command line icon from the top-right of the Cloud console. A Cloud Shell session is a command interpreter hosted by a virtual machine running in Google's cloud. It should only take a few moments to provision and connect to Cloud Shell. This virtual machine is loaded with all the development tools you need. It offers a persistent 5 gigabyte home directory and runs in Google Cloud, greatly enhancing network performance and authentication. Next, verify the setup. Cloud Shell has few environment variables, including GOOGLE CLOUD PROJECT which contains the name of your current Cloud project. You may use the command mentioned here to confirm whether gcloud command has complete access to the project. The command g-cloud config list project lists all the projects. On that list, you'll find the name of the project you intend to use for Smart Reply. Now, you can set the project using the command g-cloud config set project project-id where project-id is the name of the project. You can use echo GOOGLE CLOUD PROJECT command to get the project currently set. Remember, GOOGLE CLOUD PROJECT is the environment variable. Once the project is setup, you can enable necessary APIs which are: Dialogflow storage and data labeling APIs. As the Cloud Setup is ready with the APIs, we need to make sure the conversation data is in the required format. We can use them by uploading to a G-C-S Bucket. Conversation Data must be received in JSON or C-S-V file. If the conversation data is provided by via google drive, it can be unzipped and upload to the gcs storage under the customer's cloud project. The conversation entry must follow a specific structure as shown in the image, Each entry should be formatted as follows: It must have Transcript ID, Role, Content, user ID and Timestamp. The Transcript ID is a unique identifier for every conversation. This is used to store the conversation in the file named as Transcript ID dash dot dash json. Role is the actor in the conversation. It may be an Agent or a Customer. Messages in the conversation are stored as values to text key. User id is a key with value 1 for Customer and 2 for Agent. The timestamp of every message is stored in the key start underscore timestamp underscore u sec. There are some specific requirements to accept the data: Be aware if the shared text in the conversation is somewhat normalized (like punctuation marks removed or all letters being capitalized or lowercase). Personal Identifiable information (or P-I-I). Sentences that contains PII information should be less than 2% (verify that in approximately 100 sentences). Make sure that Bot and system messages are cleaned up from the transcripts. Such formatting to the training data may not affect the success of model training. However, it could significantly impact the quality of the model, especially when you try to present a demo with regularly formatted messages (like messages with punctuation and with proper capitalization). If you find any issues in the shared conversation data, please verify the proper format of the csv, size, pii, script and clean it up, as required. Once data is received, you need to make sure that it follows certain standard requirements. This is because the quality of the data is crucial for Smart reply Model training. The model uses the messages from the transcripts only. This means that these messages are potential candidates for suggestions, so so they must be as accurate as possible. You can use following steps to review the data. First verify the Data set Size: You need to have 200 thousand transcripts to train the model for accurate results. 100 thousand transcripts are acceptable to train the model but it may not generate expected results. The minimum requirement is 30 thousand transcripts. We can train a model with 30 thousand transcripts to understand how the feature works. However, the model may not perform as expected. Your data should have no more than 2% of PII in the information. So ensure that you verify the absence of Personally Identifiable Information in the data. This includes data like: Names, Addresses, Phone Numbers, IDs, And card Numbers. Finally, you need to verify the Conversation Content: Every column must be a string value and it must be valid. In the case of JSON, every key must have an appropriate value associated with it. For example, role should be either Agent or Customer only. Messages should be like real contact center messages only. There should not be system generated messages like "Call is transferring to the Agent." Let's review the architecture so we understand the flow of the end-to-end delivery. These are the steps we will discuss in detail. First, Prepare Data: In this stage we collect and upload transcripts to G-C-S bucket in the required format. Next, Setup the Dataset. Here, we import transcripts from GCS and create a dataset which will serve as a knowledge Base. Then Train the Model. In the UI, we can configure and train the model. Step four is to generate a human evaluation sheet for manual evaluation to review the responses of the smart reply model using a test data set. The next step is to review the list of messages that can be used as a suggestion. This list of potential messages is called Allowlist. Analyse the allowlist messages and move them from unreviewed to Allowed or Blocked State. Then there’s Configuration. Deploy the model and create a conversation profile. Integration is the process of connecting the Agent Assist feature with the Agent Desktop (or any other runtime that the customer uses for the Contact Center.) The Final step is setting the feedback loop. The feedback loop is set up with customer who want to store all ongoing conversations so that they can be directly used in the future to retrain a new version

### Video - [Prepare data](https://www.cloudskillsboost.google/course_templates/1159/video/508903)

* [YouTube: Prepare data](https://www.youtube.com/watch?v=7BHgSq5gSsY)

Now let’s deep dive into the specifics of how to ingest the data into an Agent Assist Dataset. As previously mentioned, data must be provided preferably in a GCS Bucket and it must be in a required format. This is important in order to store the raw and intermediate data for the ingestion process. Another option is receiving data through Google Drive. In this case, data has to be downloaded and uploaded to a google owned project, as the Agent Assist Smart Reply model can accommodate data processing only in a GCS Bucket. So let’s explore the processes for: Creating a GCS Bucket And Validating Data Format If a GCS bucket doesn’t already exist in your environment navigate to the Cloud Storage console and click on “Create” icon in Buckets section Provide a globally unique name to the bucket. Select the region in which you want to store your data. Choose a storage class for your data. Once all necessary information has been included, click create to create a bucket. Data format validation is required before we begin the ingestion process. As mentioned earlier, this will also be a manual process. We must ensure that it follows the proper JSON structure, which can be found in a document in the additional resources section of this training. As we can see in this example, any metadata about conversation can be added in the conversation underscore info field which can be used later to train Smart Reply Model. Transcripts should be covered inside entries field and should have 4 important fields: start timestamp Usec and Text, which contains the actual chat, role of the user and user id. Once the JSON files are ready in the required format, the data is ready to be uploaded to a GCS bucket. Generally, a tenant Google Cloud project is set for every customer. If that’s unavailable, a Google-owned project can be used. All the shared data needs to be uploaded in a bucket under the same project. Data can be uploaded using scripting, gcloud utilities, or the upload folder feature in the Google Cloud Console UI.

### Video - [Create dataset](https://www.cloudskillsboost.google/course_templates/1159/video/508904)

* [YouTube: Create dataset](https://www.youtube.com/watch?v=Zrh9e-_vwJk)

The next step is to import JSON transcripts from the GCS Bucket to Agent Assist Console and create a dataset. It converts data to “Text Stream Entry” format for further training. First, we need to visit Agent Assist Console and access a customer project by selecting it. You can reach this agent assist console by visiting agentassist. cloud.google.com and clicking on the dataset link as shown in the image. Next, click on the create button to create new dataset. All the previously created datasets can be found on this page. Select the customer’s cloud project if available. If the model is intended to be trained in Google’s owned project, select the same. When completing the form, you need to provide parameters such as: The Dataset Name, The Dataset Description, And the U-R-I of the data which will help you in identifying the data set in future. If similar names are used for the dataset, the description will help in differentiating. Smart Reply is available in multiple languages so select the matching one. Here, English is selected by default. Finally, provide the U-R-I of the bucket where the files are stored. Note that wildcard characters work here in the same way. Asterisk is used to select all the transcript files. Once you’ve completed the form, click next to start importing. Finally, return to the dataset again and check the progress. It will show the importing status until all the conversations are processed from the GCS bucket. Also, from here you will see the number of conversations imported while the process continues.

### Video - [Train the model](https://www.cloudskillsboost.google/course_templates/1159/video/508905)

* [YouTube: Train the model](https://www.youtube.com/watch?v=Ut9PDxFDd0c)

Once the dataset is ready we can use it to train the model. Training the model using the Console is very straight-forward. First, click on the Models from the left panel to display all the trained models in the right panel. Next, click on the create button to show the form to train the model. Next, provide all the parameters to train the model as follows: First, select Smart Reply (as model training is available for multiple features). Next, provide a display name which can be used to identify the Model. Then select the language. Make sure it is the same language as the one you selected to create the dataset. Finally, you need to select the dataset you want to create the model from. Click on the Next button to trigger the model training step. Training a model generally takes 8 to 12 hours. There might be a few issues that occur while training the model. For example, the Operation might fail because the dataset is too small. Or the model training job runs for more than 24 hours. If no actionable error message is provided to help you troubleshoot, then please notify the Google team and provide the model id. This will allow them to create a bug under the right component in Buganizer.

### Video - [Model evaluation](https://www.cloudskillsboost.google/course_templates/1159/video/508906)

* [YouTube: Model evaluation](https://www.youtube.com/watch?v=X_Mlns00QPA)

Once the model is trained, we need to evaluate it. In this section, we’ll explore the process of model evaluation. Model evaluation is automatically generated when you train a model from the Agent Assist Console. If the model is trained directly on the backend, you need to make an R-P-C Call. Click on the models for details of the evaluation metrics generated after the training. The two matrics are: Message coverage which tracks the percentage of the agent messages that are in the allowlist. And Recall, which tracks the percentage of the suggestion that matches what the agent said. While training, the pipeline itself split data to generate this evaluation A model is accepted if it has Message coverage greater than 15% and Recall is greater than 20%. Select the model with best metrics (in most case, recall) and deploy it. Next, we can check the messages and their status in the tabs beside the Details tab. Allowlist is a list of messages that can be used for suggestions. It has three statuses: Unreviewed, reviewed, and blocked. Here, you can see that all messages are in unreviewed state. The total number of messages in unreviewed, reviewed and blocked state are 9-1-5-0-7, 0, and 0 respectively. In the model overview page, click “Generate human evaluation”. Once this process is done, download the file, and upload it into the google sheet or excel. This requires the selection of a Testing Dataset. Create a dataset and select it from here with the data that is not used for training. Human evaluation helps confirm if the suggestions created by model make sense to a human Please note that sometimes the model can trick an offline evaluation process. For example, greetings are always okay to send to customer such as “thank you’. In this case, the model will be able to achieve a proper recall assuming that message is really useful as it is sent by customer. But, in reality, it may not be useful to move the conversation towards a resolution. Also, the answer to “Would you send it as the next message of agent?” should be higher than 50%. If evaluation metrics are bad, or suggestions in human evaluation do not make sense, double check the requirements outlined in the prerequisites section and retrain your model. If the problem persists, please notify the Google team so that they can file a bug. Please make sure that your evaluation file contains the following columns: A context column with a snippet of about 10 messages from a conversation. And a suggestion column with 1 to 3 messages suggested as the next agent reply. You also need two questions as follows: Would you send the suggestion as the agent’s next message? (if appropriate) And does the suggestion help to resolve the issue? And you require an Actual reply of what the agent actually said following the messages in a conversation context. The following image is an example of what the manual evaluation file looks like. The questions next to Suggestions are used to calculate the metrics Appropriateness and Completeness. Suggestions may be always appropriate but not necessarily helpful to resolve the issue. Appropriateness is the number of the suggestions which can be used to send to agent regardless of their capabilities to move conversation forward. For example greetings, gratitude messages, and so on. Completeness is the number of suggestions which move the conversation forward towards resolution, for example, “May I have your order ID?”, or “Can you please elaborate your concern?” The manual evaluation sheet usually contains 100 conversations for evaluation.

### Video - [Review allowlist](https://www.cloudskillsboost.google/course_templates/1159/video/508907)

* [YouTube: Review allowlist](https://www.youtube.com/watch?v=Qs8ADhwdcDg)

Let’s now explore how are suggestions are generated. When you train a model, it generates an allowlist document with the list of frequent messages observed in chat logs These messages are unreviewed at this stage. The allowlist is generated from the training data, which is inherently based on the training data set. If the training dataset is free of issues then the allowlist will likely not have many issues either. However, it is possible that some messages may need to be updated or blocked from suggestions. One way to improve a message is to review it manually to make sure: It aligns with the brand’s voice, It doesn’t have any sensitive information, And it is appropriate and useful to move the conversation along. Once a message is reviewed, it can be moved to the ALLOWED or BLOCKED state. We can interchange the state between ALLOWED and BLOCKED. However, message can not be moved to UNREVIEWED state once it is moved to ALLOWED or BLOCKED state. Each smart reply model is trained with an allowlist that contains up to 50 thousand candidate messages. During serving time, the smart reply suggestions returned are picked from the allowlist. Each message may have Unreviewed, Reviewed or Blocked state. As mentioned earlier, each business may have specific rules for modifying the message after review. The following are just a few examples: Block Messages containing REDACTED. Remove messages with specific patterns, like “Hello, my name is…” Correct the grammar of messages. And block messages that violate specific business policies. Allowlist is processed internally to provide better suggestions. This helps to prevent suggestions of messages in the BLOCKED state. And Messages are grouped based on their similarity. For example, messages like “Thank you” and “Thank you so much”, are grouped. This is done so that semantically similar suggestions are not shown to customers. Note that once a message or group of messages are moved to Blocked State it take 5 to 30 minutes in order to update it. If you receive some suggestions within this timeline, it is because the message status is not updated yet. This image shows the steps to change the state of a message. You can select multiple messages if need be. Whenever you need to change the state, first visit Console and click on the “Models” tab. All messages are un reviewed by default when a model is trained. So click on the “Unviewed messages” tile at the top. Next step is to select the message. As soon as a message (or messages) are selected, the options “Move to allowed” and “blocked” are available. The final step is to move the message to required state. Note that once we move message to “Allowed” or “Blocked” from “Unreviewed”, we cannot move it back to the “Unreviewed” state. However, we can change the state from Allowed to Blocked and Blocked to Allowed. For example, It may be possible to allow certain message at the time of deployment, but remove them later. In such cases, we can block allowed messages by changing state. Next, you just need to hover the mouse on the message and a pencil icon to edit will appear. You just need to click on the same and update the message.

### Video - [Configuration](https://www.cloudskillsboost.google/course_templates/1159/video/508908)

* [YouTube: Configuration](https://www.youtube.com/watch?v=qC4yPFLeuw8)

Next is configuration. Configuration refers to the Conversation Profile creation in Agent Assist. It’s a way to integrate Agent Assist to receive live traffic from customers environment. You can create a conversation profile in the Agent Assist Console with your newly trained model with the following inputs: The first is the model id: this is the deployed conversation model that you trained. You also need the Confidence threshold: When the server creates a smart reply suggestion, it assigns it with a confidence score. Only when the confidence score is higher than the threshold should the suggestion should be returned to the customer. And there’s also the Max result count: You should default it to 3. This way, for each request, the server returns 3 candidate messages that it is most likely the agent would say. While the newly created profile is being created, a spinner will show up in the Agent Assist Console. You can learn more about these topics from the assets linked in the additional resources document. Please note that the simulator can also be used for demos. You can reference the human evaluation result and draft a sample demo conversation script. Common issues relate to: Lack of suggestions: In this case please lower confidence threshold in the conversation profile and start over. And bad suggestions: In this case please increase the confidence threshold and start over. For a link to the simulator, refer to the additional resources section.

### Video - [Performance monitoring](https://www.cloudskillsboost.google/course_templates/1159/video/508909)

* [YouTube: Performance monitoring](https://www.youtube.com/watch?v=qXbFowsPkEk)

Once we start receiving traffic through the profile, we can start monitoring the performance of the model through metrics. Performance can be monitored using the “Request Level Click through Rate” (or C-T-R) and the “Suggestions Trigger Rate” (or T-R). The Click through rate is the percentage of suggestions clicked by agents. It is comparable to the “offline evaluation metrics Recall” previously covered. The Trigger Rate measures how frequently we return smart reply suggestions. So the rate of percentage of messages suggestions versus total number of messages agent faces. Ideally agents would always receive suggestions, so the trigger rate should be 100%. For more information on performance monitoring, including guidance on Agent Assist and Buganizer, refer to the additional resources.

### Video - [Common issues and troubleshooting](https://www.cloudskillsboost.google/course_templates/1159/video/508910)

* [YouTube: Common issues and troubleshooting](https://www.youtube.com/watch?v=4QwKUP3S6i4)

Here are some common issues when it comes to Smart Reply and how to troubleshoot them. When the model fails to perform as expected there are four steps that you should attempt: First, check the model's evaluation metrics against the training dataset. Next, check integration. Then check the conversations sent in live traffic. Finally, run an evaluation on smart reply model against the live traffic. Let us understand the first issue which may be due to issues with the training data. The symptom can be lower C-T-R. If such symptoms are monitored, we can understand that suggestions are not as good as expected. Let’s understand what can we do to resolve the issue. If the C-T-R is low, then review the metrics generated from the allowlist. In general there should be 20 to 100 thousand there. Message coverage should be higher than 10%, and the recall should be higher than 15%. If these criteria are not met, then check the following: First, check the quality of training data: Check if the messages in the training conversations are in the correct order, that the sender of each message is correctly set, and that the conversations are not too short (they should be more than 5 sentences). Then check the size of the training data: 30 thousand is minimal. Larger training data should promise better performance. For more information on model performance issues, refer to the model performance issues hosted in the additional resources document.

### Quiz - [Smart Reply Quiz](https://www.cloudskillsboost.google/course_templates/1159/quizzes/508911)

### Lab - [Building a Smart Reply model using Agent Assist Console](https://www.cloudskillsboost.google/course_templates/1159/labs/508912)

Explain the process of building a Smart Reply model using Agent Assist Console

* [ ] [Building a Smart Reply model using Agent Assist Console](../labs/Building-a-Smart-Reply-model-using-Agent-Assist-Console.md)

## Smart Compose

After completing this module, you will be able to describe Smart Compose and the use cases that it enables in the Contact Center.

### Video - [Smart Compose](https://www.cloudskillsboost.google/course_templates/1159/video/508913)

* [YouTube: Smart Compose](https://www.youtube.com/watch?v=bGOGQHRIzf0)

Now let us understand what Smart Compose is and how can it be useful. Smart Compose is a very popular feature like Smart Reply which allows agent to autocomplete the next word or phrase in live traffic. Unlike Smart Reply, it does not suggest a whole sentence but completes upcoming words in the sentence. The prerequisites, dataset preparation, and dataset creation steps are the same as Smart Reply. While training the model, we just need to select Smart Compose in place of Smart Reply.

### Quiz - [Smart Compose Quiz](https://www.cloudskillsboost.google/course_templates/1159/quizzes/508914)

## LLM Baseline Summarization

After completing this module, you will be able to describe LLM Baseline Summarization, outline its requirements and steps for implementation.

### Video - [Introduction to LLM baseline Summarization](https://www.cloudskillsboost.google/course_templates/1159/video/508915)

* [YouTube: Introduction to LLM baseline Summarization](https://www.youtube.com/watch?v=G-Jwtves8UY)

In this section, we’ll introduce you to CCAI Agent Assist Baseline summarisation, and its features, use-cases, architecture and target customers. In this section we’ll introduce LLM Baseline Summarization and review the requirements and key steps for its implementation. Let’s start with the introduction to LLM Baseline Summarization. LLM Baseline Summarization is an AI-powered Agent Assist feature that automatically summarizes customer conversations. There are two main ways to summarize conversations: using a pre-trained LLM baseline summarization model or a custom model trained on customer data. In this module, we will discuss the pre-trained LLM (baseline) summarization model because the custom model is in preview as of February 2024. You might also hear LLM Summarization referred to as the V2 Summarization baseline model. This model enables users to customize the content of the summary by selecting from predefined sections. This feature is designed to support both Voice and Chat data. In this course, we will be focusing on chat data. The out-of-the-box model will generate summaries for the following predefined sections: First is Situation: The Situation section highlights the core of the customer's query or concern It captures the essence of what they're reaching out for. The other is Action: which details the steps or measures taken by the agent to assist the customer. This section provides insights into the operational responsiveness of the model. Resolution indicates the final result of the conversation. In other words, was the query resolved? The result is Yes, No, Partially resolved or Not applicable. Customer Satisfaction is a crucial metric for any customer-centric business. Simply put, it's a gauge on how the customer felt by the end of the interaction. Were they satisfied or not? There’s also Reason for Cancellation: Should the discussion veer towards service cancellation, this metric is vital for businesses aiming to reduce churn and understand pain points in the customer journey. The Entities section extracts the crucial key-value pairs from the conversation. These could range from product names and time frames, to specific issues. It offers a concise snapshot of the conversation's content It’s imperative to maintain high-quality summaries, which is why the model has a safeguard in place. Any summary that doesn’t meet a quality benchmark is deleted, resulting in an empty return. This ensures that the summaries you receive are the most relevant. At the heart of this feature is the benefit it offers to our customer service agents. Imagine handling countless customer requests daily. Having a concise yet informative summary of prior interactions (whether with another agent or a bot) can drastically improve the quality of service. At the culmination of an interaction, the system doesn't just leave the agents to draft summaries from scratch. Instead, it proposes a draft. This isn't just a time-saver but also ensures that the summaries are consistent in format and quality. The end result? Agents can focus more on solving issues and less on administrative tasks, amplifying productivity. Please note that LLM Baseline Summarization isn't just for human-to-human interactions. It's equally proficient at summarizing bot-to-human dialogues. The results are a game changer for contact center operations. There’s a reduction of average summarization time from 90 seconds to 9 seconds. Throughput for unedited summaries is ten times faster. Observed results from 80 to 100 seconds were reduced to 6 to 10 seconds For edited summaries we observed that throughput was three times faster. The solution not only helps improve productivity and effectiveness, but also regulates the operational costs of the contact center.

### Video - [API service enablement](https://www.cloudskillsboost.google/course_templates/1159/video/508916)

* [YouTube: API service enablement](https://www.youtube.com/watch?v=V_J0WC0Oe0c)

What does it take to set up the LLM Summarization feature in Agent Assist? The first step is to enable APIs which are going to be used for LLM Summarization, like V2 Summarization Baseline. Let’s discuss these in detail. Let’s start with the Cloud Data Loss Prevention API. This tool is paramount for businesses today as it identifies, monitors, and protects sensitive information across your datasets, ensuring that confidentiality is always maintained. Next is the Vertex AI Search and Conversation API. This is a next- generation tool designed to enhance search capabilities and foster meaningful conversations using AI. Whether it's refining search results or powering chatbots, this API is at the forefront of AI-driven customer experiences. Moving on, we have the Cloud Storage API. In the era of Big Data, having a secure, scalable, and durable storage infrastructure is a non-negotiable. This tool ensures your conversations remains accessible and protected, regardless of the volume. With the Cloud Dialogflow API, we're stepping into the realm of natural language processing and understanding. This tool empowers developers to generate summaries from audio transcripts. For more information on these APIs, refer to resources 20, 21, 22 and 23 in the additional resources.

### Video - [Implementation - Data ingestion](https://www.cloudskillsboost.google/course_templates/1159/video/508917)

* [YouTube: Implementation - Data ingestion](https://www.youtube.com/watch?v=ALQRnnaA25Q)

next let's review the data ingestion process first we create the cloud storage bucket required to host our data if not already done navigate to the cloud storage console and click on the create icon in the bucket section provide a globally unique name to the bucket then select a region ensure that the same region or sub region is selected for the rest of the steps in the pipeline select standard storage class and then click on on create when working with GCS organizing and uploading chat transcripts is straightforward please review resource 24 in the additional resources to determine which formats are supported let's review these Concepts in a demo as mentioned summarization of conversations can be done using a pre-trained llm baseline summarization model or a custom model trained on customer data in this demo we'll discuss a pre-trained llm Baseline summarization model we'll also cover the key benefits that it offers in helping agents understand customers requests and previous conversations with call summary it also suggests a summary draft at the end of a conversation we'll also look at the process of loading data to the cloud and finally we'll examine how to verify transcribed files and understand corresponding file formats let's dive in hi my name is Omar and I'm A Cloud AI engineer with Google today I'll be demoing agent assist llm Baseline summarization before we get started make sure that you have a billing project created and ensure to enable apis like cloud storage API Cloud speech to text API Cloud DLP API dialog flow API also make sure you have proper IM am permissions to access CCI insights cloud storage bucket Cloud DLP so what is llm Baseline summarization it is an AI powered agent assist feature that automatically summarizes customer conversations summarization of conversations can be done either using a pre-trained llm baseline summarization model or a custom model trained on customer data in this demo we will discuss pre-trained llm Baseline summarization model llm summarization is also known as V2 summarization Baseline model and supports voice and chat data the key benefits of summarization are that it helps agents understand customers request and previous conversation with call summary and suggest summary draft at the end of a conversation this in turn helps agents to write better formatted summary with much less effort as a next step we will start by loading data navigate to Google cloud storage and create a bucket if not already created make sure that the region of the bucket Remains the Same as other resources to avoid any cross region data transfer once the bucket is created load Lo the chat conversation files into the bucket to get more information on how to load the files into the bucket visit this link also to understand more about supported file formats visit this link while loading the files it is good to understand the format of the conversation file which is in Json format the file will have two major sections the conversation info section and the entry section the conversation info section will have the conversation ID which is an identifier for this conversation and an agent ID specific in the agent handling the conversation the entries will be a list which will contain all the turns of a conversation each turn will be specified as a single Json entry within the list each entry will have details like start timestamp of that turn in microsc format the text field is the utterance by either the user or the agent in that ter the role will specify if the thread is for the customer or the agent and the user ID field will have the ID of either the customer or the user based on who the turn belongs to great now that we have our source conversations in the GCS bucket the next step is to convert the chat conversations into a format which can be given to summarization first create a folder inside our bucket to store the output files next navigate to The Script which will be used to convert our chat conversation files to CSV this script will need us to have some configuration values handy which the script will use starting with the project name the input bucket URI name of the folder in the bucket which holds the input conversation files name of the folder which will store the transcribed output files once we have the configuration in place navigate to the utility function section of the script focus on redact pii using Google DLP section this code will be invoked during the file conversion process to redact the pi information to get more information on redaction of pii data using DLP please visit this link once the script finishes running navigate to the configured file path verify that an output file is created in the desired location download the output file and open it in the editor here we have converted the CSV file into sheets for better readability the First Column specifies the file name to which the conversation belongs the second column specifies the turn ID the third column specifies to which entity whether the customer or the agent the turn belongs to and the last column specifies the utterance all Al pay attention to the redacted entities of the atance such as person name which is a result of applying DLP on the input conversations to summarize we saw how to load chat conversation files to the cloud and then apply DLP to redact sensitive information we created and verified the output CSV file which can now be passed as input to the summarization next we will see how to generate summarization from the conversations the demo that you just reviewed covered a lot of material there are some use use ful resources to help you understand these Concepts in the additional resources for more info in Python client for storage bucket refer to Resource 25 in the additional resources for more information on the conversation data format refer to Resource 26 in the additional resources

### Video - [Create a conversation profile](https://www.cloudskillsboost.google/course_templates/1159/video/508918)

* [YouTube: Create a conversation profile](https://www.youtube.com/watch?v=1E2YG5Y-YsQ)

Now let’s explore how to create a conversation profile. Implementing LLM Summarization involves a few key steps in the Agent Assist Console. Click on the Summarization tab and tailor the model to your needs. This means choosing sections such as Situation, Action, and Customer Satisfaction, among others. Ensure you integrate a new or existing conversation profile when setting up your summarization model If you require a detailed walkthrough, a detailed guide is available in the additional resources document at the end of the training.

### Video - [Implementation - LLM Baseline Summarization configuration](https://www.cloudskillsboost.google/course_templates/1159/video/508919)

* [YouTube: Implementation - LLM Baseline Summarization configuration](https://www.youtube.com/watch?v=svuTc9HUvUM)

next let's review the detailed steps on how to set up llm Baseline summarization the prerequisites for this section is to have a basic understanding of the Python programming language and Jupiter notebooks when utilizing our Jupiter notebook in Google Cloud a number of parameters need to be correctly configured let's walk through them start by defining the project name which identifies your gcp project the GCS bucket URI represents the root path of your Google Cloud Storage bucket it should be in the format GS colon forward SL SL folder name transcript input folder prefix is the folder within your GCS bucket where you'll upload transcript files summarization output folder prefix is designated for storing the summarized outputs from your conversations the conversation profile ID represents a unique identifier linked to a specific conversation profile it's an autogenerated value that takes a form similar to that shown on screen with a specific project name and location this ID is essential as it's used to correlate your audio files with a correct profile this ensures tailored transcription based on the specified settings from that profile it's essential to provide accurate configurations for every audio transcription task misconfigurations can lead to incorrect transcriptions so always double check your parameters as we delve deeper into our configurations let's touch upon a unique but optional aspect fine-tuning the DLP or data loss prevention redaction by default our notebook utilizes a set of predefined info types these are patterns or types that our DLP system uses to identify and redact potentially sensitive information however every business has unique needs for example you might want the DLP system to recognize certain patterns specific to your domain agent assist allows you to do that to best cater the implementation to your business needs after setting up the configuration run all the cells of the notebook the summary of the audio files will appear in your Cloud bucket in the summarization output folder prefix folder in this demo we'll focus on how to obtain the conversation summary using conversation files as input to llm Baseline summarization the demo will cover how to create a conversation profile how to enable conversation summarization and how to verify the summarization output hi my name is Omar and I'm A Cloud AI engineer with Google we are looking at agent assist llm Baseline summarization in the previous demo we saw how to upload conversation files to the to the cloud and processing when using Cloud DLP to redact pii data we also converted the input conversation files to a format which can be used as input to summarization in this demo we will focus on how to get conversation summary using conversation files as input to llm Baseline summarization before we get started make sure you have dialog flow API enabled and I am permissions to access CCI insights next step is to create a conversation profile a conversation profile configures a set of parameters that control the suggestions made to an agent these parameters control the suggestions that are surfaced during runtime each profile configures either a dialog flow virtual agent or a human agent for a conversation to create a conversation profile navigate to agent assist console and click on summarization on the left panel after that you will see two options use a pre-trained model or creating custom model in this demo we will focus on using the pre-trained model add a display name to the profile click Start next step is to select a baseline model agent assist now supports a new V2 summarization Baseline model for voice and chat data this model enables users to customize the content of the summary by selecting from a set of predefined sections as for this demo we will select the Baseline version as 2.0 next step is to to select custom sections for your summary selected sections will show up as a part of the summary the situation section is what the customer needs help with or has questions about the action section is what the agent does to help the customer the resolution section is result of the customer service the customer satisfaction section is unsatisfied if the customer is unsatisfied at the end of the conversation and satisfied otherwise the reason for cancellation section has value if the customer requests to cancel service any otherwise and the entity section is key value pairs of important entities extracted from the conversation let's go ahead and select all the sections for this demo for each feature you select you can select either a model for smart reply and summarization suggestions or a knowledge base for FAQ assist or article suggestion for now we will leave this blank next give a display name to the profile and select the conversation type as chat leave the rest of the settings to default and hit complete verify that you are able to see the conversation profile that you created next step is to get summarization for the conversations using summarization API navigate back to the notebook we were using in the previous demo verify the parameters required to call the summarization API the summarization output folder prefix should point to a location in the GCS bucket containing the input CSV file we created in the previous demo this is also the location where the script will store the summarization output the conversation profile ID variable should point to the conversation profile that we have created to get the value of the conversation profile variable navigate back to the agent assist console and click on conversation profiles look for the conversation profile we created and copy the name name integration ID navigate back to the script update the project ID and the integration ID part of this variable leave the rest of the part of the variable as it is now navigate to the functions needed for calling summarization API section of the file and run it these are all helper functions which will help get the summarization using the API focus on the dialog flow conversations client and dialog flow participants client all the commun iation is handled via these clients which are a part of dialog flow client library next navigate to The Calling summarization API for transcript summarization section this section is responsible for getting the conversation file from the GCS location and return the summarization of the conversations which will be stored in the output location once the script is done navigate to the configured output location to verify that the summary file is generated now download and verify the summarization file the output file is a CSV file which for visualization purposes we have converted to a sheet each row in the sheet corresponds to a row in the CSV file this file will have three columns transcript ID the full conversation and the summary the transcript ID column will contain the name of the transcribed file the full conversation column contains the transcribed conversation from the audio file the summary column will contain the summary returned by the summary API the summary will contain all the sections that were selected while creating the conversation profile so in this demo we saw how to pass transcribed conversations to the summary API to get back the conversation summary based on the configured conversation profile as seen in the demo one of the most critical aspects of the implementation Journey for llm summarization is evaluating the llm summarization models performance once the summaries of audio files are generated the following guidelines should be used for evaluating the performance of the pre-trained Baseline summarization model the first is situation the reason for the call next is action this includes agent actions changes or brief discussions with a customer finally there's resolution the outcome of the customer service there's also customer satisfaction ction of the service when the conversation ends and there's the reason for cancellation if the customer requests a cancellation and provides a reason then describe the reason for the cancellation otherwise Mark as not applicable and there's entities these are key value pairs of entity name and entity value in the conversation here is an example of an evaluation template for llm Baseline summarization the first metric is accuracy it's crucial that the information encapsulated in the summary is true to the original conversation next is completeness while brevity is key we don't want to miss out on vital details a good summary should be both concise and comprehensive also important is compliance our summaries must adhere to established writing rules ensuring they are suitable for a wide audience pay attention to grammar and spelling linguistic quality is non-negotiable our summaries should be free from grammatical errors and misspellings finally you must also avoid repetition redundancy can dilute the effectiveness of a summary it's essential that our summaries avoid any repeated phrases or sentences next is resolution it checks if customer requests are resolved by the end of the conversation the responses range from yes for full resolution to no or partial and even not applicable in cases where no request is made another crucial metric is customer satisfaction this measures the customer sentiment at the conversations end ranging from unsatisfied to satisfied we also examine if cancellation requests and their reasons are accurately captured with a simple yes or no response we also examine if cancellation requests and their reasons are accurately captured with a simple yes or no response lastly we look at an accurate entities and incomplete entities these metrics count the number of errors or emissions in the summarization regarding specific entities discussed in the conversation together these metrics provide a comprehensive view of our llm summarization models Effectiveness ensuring we deliver precise and valuable summaries every time through a meticulous evaluation process we ensure that our llm summarization model is not just technically sound but also ethically respons responsible and user friendly it's about striking the right balance between technology and human values you can refer to the additional resources for an evaluation template as a reminder it is crucial to ensure that our system is not only producing relevant summaries but also adhering to compliance guidelines no matter how good a summary is if it violates compliance or security guidelines it's problematic summaries must strictly avoid capturing any sense sensitive or personal information or piis in addition to avoiding sensitive information summaries should be objective we want to steer clear of capturing personal emotions opinions or accusations it's about stating facts not opinions evaluating our model's performance isn't just about the accuracy of summaries writing good summaries is a balance between accuracy relevance and compliance all these elements together ensure that our system is efficient ient and trustworthy Beyond factual accuracy our focus is also on ensuring our summaries are unbiased comprehensive and of high linguistic quality in today's diverse world its imperative our summaries avoid any biased language this includes references to gender race or age our model aims to capture the essence of the conversation without resorting to these specific identifiers let's look at an example of a matrix template to address these compliance and fairness considerations first we assess repetition this checks for any redundant content in the summary a simple yes or no answer determines if there are repeated phrases or sentences next is grammar and spelling it's essential that our summaries are free from errors so this metric evaluates the presence of any spelling or grammatical mistakes then we have safety this crucial metric ensures the summary is devoid of any toxic or abusive language maintaining professionalism and respect in all Communications fairness is another vital metric it checks for biases in the summary be it related to race age or any other Factor we aim for unbiased Equitable content lastly privacy examines if the summary inadvertently includes any personal identifiable information like credit card numbers or ssns it's imperative for us to safeguard privacy at all times together these metrics ensure that our llm summarization model not only performs well but also adheres to the highest standards of compliance and ethical practice

### Video - [LLM baseline Summarization configuration](https://www.cloudskillsboost.google/course_templates/1159/video/508920)

* [YouTube: LLM baseline Summarization configuration](https://www.youtube.com/watch?v=NVeY5rxURsM)

Next, let's review the detailed steps on how to set up LLM Baseline Summarization The prerequisites for this section is to have a basic understanding of the python programming language and Jupyter notebooks When utilizing our Jupyter notebook in Google Cloud, a number of parameters need to be correctly configured. Let's walk through them. Start by defining the PROJECT NAME, which identifies your G-C-P project. The GCS BUCKET URI represents the root path of your Google Cloud Storage bucket. It should be in the format g-s colon forward -slash forward-slash folder-name “TRANSCRIPTS INPUT FOLDER PREFIX” is the folder within your GCS bucket where you'll upload transcript files. “SUMMARIZATION OUTPUT FOLDER prefix” is designated for storing the summarized outputs from your conversations. The “Conversation Profile ID” represents a unique identifier linked to a specific conversation profile. It's an auto-generated value that takes a form similar to that shown on screen, with a specific project name and location. This ID is essential as it's used to correlate your audio files with the correct profile. This ensures tailored transcription based on the specified settings from that profile. It's essential to provide accurate configurations for every audio transcription task. Misconfigurations can lead to incorrect transcriptions, so always double-check your parameters. As we delve deeper into our configurations, let’s touch upon a unique but optional aspect: Fine-tuning the DLP or (Data Loss Prevention) Redaction. By default, our notebook utilizes a set of predefined INFO TYPES. These are patterns or types that our DLP system uses to identify and redact potentially sensitive information. However, every business has unique needs. For example you might want the DLP system to recognize certain patterns specific to your domain. Agent Assist allows you to do that, to best cater the implementation to your business needs. After setting up the configuration, run all the cells of the notebook. The summary of the audio files will appear in your cloud bucket, in the SUMMARIZATION OUTPUT FOLDER prefix folder In this demo we’ll focus on how to obtain the conversation summary using conversation files as input, to LLM Baseline Summarisation. The demo will cover: How to create a Conversation Profile. How to enable conversation summarization. And how to verify the Summarization output. Hi my name is Omar and I'm A Cloud AI engineer with Google we are looking at agent assist llm Baseline summarization in the previous demo we saw how to upload conversation files to the cloud and processing when using Cloud DLP to redact pii data we also converted the input conversation files to a format which can be used as input to summarization in this demo we will focus on how to get conversation summary using conversation files as input to llm Baseline summarization before we get started make sure you have dialog flow API enabled and IAM permissions to access CCAI insights next step is to create a conversation profile a conversation profile configures a set of parameters that control the suggestions made to an agent these parameters control the suggestions that are surfaced during runtime each profile configures either a dialog flow virtual agent or a human agent for a conversation to create a conversation profile navigate to agent assist console and click on summarization on the left panel after that you will see two options use a pre-trained model or creating custom model in this demo we will focus on using the pre-trained model add a display name to the profile click Start next step is to select a baseline model agent assist now supports a new V2 summarization Baseline model for voice and chat data this model enables users to customize the content of the summary by selecting from a set of predefined sections as for this demo we will select the Baseline version as 2.0 next step is to to select custom sections for your summary selected sections will show up as a part of the summary the situation section is what the customer needs help with or has questions about the action section is what the agent does to help the customer the resolution section is result of the customer service. The customer satisfaction section is unsatisfied if the customer is unsatisfied at the end of the conversation and satisfied otherwise the Reason for cancellation. section has value if the customer requests to cancel service any otherwise and the entity section is key value pairs of important entities extracted from the conversation let's go ahead and select all the sections for this demo for each feature you select you can select either a model for smart reply and summarization suggestions or a knowledge base for FAQ assist or article suggestion for now we will leave this blank next give a display name to the profile and select the conversation type as chat leave the rest of the settings to default and hit complete verify that you are able to see the conversation profile that you created next step is to get summarization for the conversations using summarization API navigate back to the notebook we were using in the previous demo verify the parameters required to call the summarization API the summarization output folder prefix should point to a location in the GCS bucket containing the input CSV file we created in the previous demo this is also the location where the script will store the summarization output the conversation profile ID variable should point to the conversation profile that we have created to get the value of the conversation profile variable navigate back to the agent assist console and click on conversation profiles look for the conversation profile we created and copy the name integration ID navigate back to the script update the project ID and the integration ID part of this variable leave the rest of the part of the variable as it is now navigate to the functions needed for calling summarization API section of the file and run it these are all helper functions which will help get the summarization using the API focus on the dialog flow conversations client and dialogue flow participants client all the commun ication is handled via these clients which are a part of dialog flow client library next navigate to The Calling summarization API for transcript summarization section this section is responsible for getting the conversation file from the GCS location and return the summarization of the conversations which will be stored in the output location once the script is done navigate to the configured output location to verify that the summary file is generated now download and verify the summarization file the output file is a CSV file which for visualization purposes we have converted to a sheet each row in the sheet corresponds to a row in the CSV file this file will have three columns transcript ID the full conversation and the summary the transcript ID column will contain the name of the transcribed file the full conversation column contains the transcribed conversation from the audio file the summary column will contain the summary returned by the summary API the summary will contain all the sections that were selected while creating the conversation profile so in this demo we saw how to pass transcribed conversations to the summary API to get back the conversation summary based on the configured conversation profile as seen in the demo one of the most critical aspects of the implementation Journey for llm summarization is evaluating the llm summarization models performance once the summaries of audio files are generated the following guidelines should be used for evaluating the performance of the pre-trained Baseline summarization model the first is situation the reason for the call next is action this includes agent actions changes or brief discussions with a customer finally there's resolution the outcome of the customer service there's also customer satisfaction of the service when the conversation ends and there's the reason for cancellation if the customer requests a cancellation and provides a reason then describe the reason for the cancellation otherwise Mark as not applicable and there's entities these are key value pairs of entity name and entity value in the conversation here is an example of an evaluation template for llm Baseline summarization the first metric is accuracy it's crucial that the information encapsulated in the summary is true to the original conversation next is completeness while brevity is key we don't want to miss out on vital details a good summary should be both concise and comprehensive also important is compliance our summaries must adhere to established writing rules ensuring they are suitable for a wide audience pay attention to grammar and spelling linguistic quality is non-negotiable our summaries should be free from grammatical errors and misspellings finally you must also avoid repetition redundancy can dilute the effectiveness of a summary it's essential that our summaries avoid any repeated phrases or sentences Next is 'resolution'. It checks if customer requests are resolved by the end of the conversation. The responses range from yes for full resolution to no or partial, and even not applicable in cases where no request is made another crucial metric is customer satisfaction this measures the customer sentiment at the conversations end ranging from unsatisfied to satisfied we also examine if cancellation requests and their reasons are accurately captured with a simple yes or no response we also examine if cancellation requests and their reasons are accurately captured with a simple yes or no response lastly we look at an accurate entities and incomplete entities these metrics count the number of errors or emissions in the summarization regarding specific entities discussed in the conversation together these metrics provide a comprehensive view of our llm summarization models Effectiveness ensuring we deliver precise and valuable summaries every time through a meticulous evaluation process we ensure that our llm summarization model is not just technically sound but also ethically responsible and user friendly it's about striking the right balance between technology and human values you can refer to the additional resources for an evaluation template as a reminder it is crucial to ensure that our system is not only producing relevant summaries but also adhering to compliance guidelines no matter how good a summary is if it violates compliance or security guidelines it's problematic summaries must strictly avoid capturing any sensitive or personal information or (P-I-Is). In addition to avoiding sensitive information summaries should be objective we want to steer clear of capturing personal emotions, opinions or accusations it's about stating facts not opinions evaluating our model's performance isn't just about the accuracy of summaries writing good summaries is a balance between accuracy relevance and compliance all these elements together ensure that our system is efficient and trustworthy Beyond factual accuracy our focus is also on ensuring our summaries are unbiased comprehensive and of high linguistic quality in today's diverse world it's imperative our summaries avoid any biased language this includes references to gender race or age our model aims to capture the essence of the conversation without resorting to these specific identifiers let's look at an example of a matrix template to address these compliance and fairness considerations first we assess repetition this checks for any redundant content in the summary a simple yes or no answer determines if there are repeated phrases or sentences next is grammar and spelling it's essential that our summaries are free from errors so this metric evaluates the presence of any spelling or grammatical mistakes then we have safety this crucial metric ensures the summary is devoid of any toxic or abusive language maintaining professionalism and respect in all Communications fairness is another vital metric it checks for biases in the summary be it related to race age or any other Factor we aim for unbiased Equitable content lastly privacy examines if the summary inadvertently includes any personal identifiable information like credit card numbers or S-S-Ns. It's imperative for us to safeguard privacy at all times together these metrics ensure that our llm summarization model not only performs well but also adheres to the highest standards of compliance and ethical practice

### Quiz - [Introduction to LLM Baseline Summarization Quiz](https://www.cloudskillsboost.google/course_templates/1159/quizzes/508921)

### Lab - [End to End Baseline Chat Summarization](https://www.cloudskillsboost.google/course_templates/1159/labs/508922)

In this lab, you build a end to end Baseline Chat Summarisation pipeline.

* [ ] [End to End Baseline Chat Summarization](../labs/End-to-End-Baseline-Chat-Summarization.md)

## Generative Knowledge Assist (GKA)

After completing this module, you will be able to define Generative Knowledge Assist and integrate data stores with Agent Assist.

### Video - [Introduction to Generative Knowledge Assist](https://www.cloudskillsboost.google/course_templates/1159/video/508923)

* [YouTube: Introduction to Generative Knowledge Assist](https://www.youtube.com/watch?v=8LNNNPteqIc)

In this section, we’ll focus on the Generative Knowledge Assist. We’ll provide you with a high level understanding of Generative Knowledge Assist (also called as G-K-A) to then explain how to integrate it with a GenAI Agent (also known as data store) The prerequisites to best consume the content in this section include: The prior knowledge of data store agents (see a link to the CCAI Academy GenAI Agents training module in the additional resources). And the availability of data that the generative AI agent can use to answer questions. Both public and private data stores are acceptable as long as they contain the relevant information that can solve the customer’s request. The objectives of Generative Knowledge Assist is to analyze a live conversation between a human agent and an end-user and suggest relevant knowledge documents or FAQ answers to the agent to best answer the customer’s queries. For information on how to create a knowledge base in Agent Assist, please refer to the resource 34 in the additional resources document. Next, let's delve into the best practices that can help optimize this process. The quality of the content is the backbone of any knowledge document. However, not all content adds value, especially when it comes to quick, effective agent support. That’s why it's essential to keep our documents concise and relevant. The 'Article Suggestion' tool presents the initial sentences of a document as snippets. These snippets are designed to provide agents with a quick understanding of the document's content. This allows the agents to quickly use the opening lines as suggestions. Imagine being an agent and the snippet you receive is only about the last-modified date or a navigation bar. Not only is this unhelpful, but it can also mislead agents and hinder their ability to provide quick resolutions to end-users. So to summarise, always start your knowledge documents with the most vital information that can aid agents. Ensure that the beginning is free from distractions and is directly related to the document's core subject. When considering document format, it's also important to ensure compatibility with the support tools at your disposal and optimizing the Document Format. Firstly, remember that 'Article Suggestion' and 'FAQ Assist' focus on text. So, documents heavy on audio, video, or images aren't ideal. For lengthy documents, especially those surpassing 1000 words, consider breaking them down. This improves suggestion quality and eases the agent's task in locating the required information. A well-structured document ensures both efficient tool processing and ease of use for your agents. A crucial aspect of a reliable knowledge base is also the relevance and usefulness of its content. So tailor your knowledge base according to the objectives of your customer channels. For example, for tier-2 technical support, prioritize technical content often referred by the agents over sales content. Maintaining a knowledge base that’s up-to-date can be a lot of work. So try to be proactive and create a process for decluttering it from documents that are outdated, rarely accessed, or inactive. These don't add value and may even hinder the agent's process. Now that you have a good understanding of what a good knowledge base looks like, let’s explore the 'Generative Knowledge Assist' feature. Consider this example: A customer mentions they're a Home Internet user and inquires about discounts to save money on their account. The agent's original query is simply 'Discount'. Even with little context, the Generative Knowledge Assist steps in and transforms this query into a more specific one: 'What discounts are available for Home Internet customers?' This rewritten query is not just more precise; it's tailored to the customer’s specific context, greatly enhancing the relevance and accuracy of the response. This feature works hand-in-hand with a data store agent, which links agents' questions to the conversation context, providing generative answers and citations. It can even support a private Knowledge Base and utilizes conversation context as user metadata. In Generative Knowledge Assist understands and rephrases queries leading to more meaningful and targeted assistance. Knowledge documents play a vital role in the efficiency of our Agent Assist tools. Whether it's website links for suggesting content, or FAQ documents for assisting with common queries, having a well-organized knowledge base can greatly enhance the user and agent experience.

### Video - [Integrating GenAI Agent with Agent Assist](https://www.cloudskillsboost.google/course_templates/1159/video/508924)

* [YouTube: Integrating GenAI Agent with Agent Assist](https://www.youtube.com/watch?v=8vkdrbe8qcw)

Now that we have mentioned the role that gen AI Agents (also known as data store agents) play in Agent Assist, let's walk through the process for their integration. First go to Agent Assist Console and select project. Then go to “Conversation profiles”. Then click on “Create” to create a new conversation profile. Then, check “Generative knowledge assist”. From the dropdown, select the built GenAI agent and save the conversation profile. Here is what the outcome will look like. Kevin, a customer of a Telecommunication company, purchased an new iphone last week, but he is considering returning it so asks about the return policy. GKA follows in real time the conversation and automatically generates a tile with a summary of the return policy for Apple products. That summary also has hyperlinks to the sources the summary is based on. The Agent responds based on the information provided in the tile that a 30 day window is available, else the customer will incur a fee. Mentioning the fee sparked another question for the customer, who asks whether he will incur in any penalty for not paying his bill on time this month. Through the same mechanism, the GKA queries the relevant information in the Late Fee Policy documentation. All the relevant information is at the fingertips of the agent to provide the most accurate and responsive service to the customer.

### Quiz - [Generative Knowledge Assist Quiz](https://www.cloudskillsboost.google/course_templates/1159/quizzes/508925)

### Lab - [Setting up Generative Knowledge Assist in the Agent Assist console](https://www.cloudskillsboost.google/course_templates/1159/labs/508926)

This lab provides a step-by-step approach to creating a Data Store, adding documents to the Data Store, creating a GKA infobot agent and attaching the Data Store to it, setting up an Agent Assist conversation profile, and linking it to the GKA Agent for Knowledge Assist using the Agent Assist console.

* [ ] [Setting up Generative Knowledge Assist in the Agent Assist console](../labs/Setting-up-Generative-Knowledge-Assist-in-the-Agent-Assist-console.md)

## Sentiment Analysis

After completing this module, you will be able to define and enable Sentiment Analysis.

### Video - [Sentiment analysis](https://www.cloudskillsboost.google/course_templates/1159/video/508927)

* [YouTube: Sentiment analysis](https://www.youtube.com/watch?v=4R0-un7szVs)

Next, in this module we talk about entiment Analysis feature of Agent Assist. In this section we will briefly talk about sentiment analysis under Agent Assist in two parts: First we’ll develop an understanding of the concept of sentiment analysis. Then, we’ll enable sentiment analysis. Agent Assist supports Sentiment Analysis to better understand customer interactions at a deeper level. This feature analyzes messages during a conversation between a human agent and an end-user to determine emotional intent. The Sentiment is represented by a score and magnitude values, metrics that are returned in the response. The score of the sentiment ranges between minus 1.0 (which is negative) and 1.0 (which is positive) and corresponds to the overall emotional leaning of the text. The magnitude value indicates the overall strength of emotion (both positive and negative) within the given text, between 0.0 and plus infinity. Sentiment Analysis can be enabled during conversation profile creation or editing. If you are editing an existing conversation profile, you will only see the effects in conversations after the conversation profile updates. You also have the option to enable sentiment analysis when you create a conversation profile using the Agent Assist console. To do that: First Set “Enable Sentiment Analysis” to true in “Message Analysis Configuration”. Next, send a “Create Conversation” request using a Conversation Profile with this feature enabled. Then Sentiment results are then returned. If you have enabled Cloud Pub/ Sub integration in Agent Assist, sentiment result will also appear in the New Message Payload. More details can be found in the additional resources section.

### Quiz - [Sentiment Analysis Quiz](https://www.cloudskillsboost.google/course_templates/1159/quizzes/508928)

## Delivery Lifecycle

After completing this module, you will be able to describe the various phases of the Agent Assist delivery lifecycle.

### Video - [Delivery lifecycle](https://www.cloudskillsboost.google/course_templates/1159/video/508929)

* [YouTube: Delivery lifecycle](https://www.youtube.com/watch?v=bzhxkrIGppQ)

Once the SOW is in place, what will a typical delivery lifecycle look like for a POC or full implementation? It is usually divided into four phases, which we are going to explore next. These phases include: Prerequisites, The Discovery Phase, The 8 00:00:17,279 --> 00:00:19,680 Design Phase,And Implementation and Testing. The Discovery Phase starts with Identifying the customer requirements, the features in scope and assessing the channel that the customer wants AA capabilities for. Agent Assist can be configured for both Voice and Chat. Also it is important in this phase to determine whether the customer requires any level of customization. Understanding the customer's current ecosystem, Agent Desktop and integration planning is also critical in this phase. The second step of the Discovery Phase requires you to understand how the historical data can be made accessible. You must also understand the format that it is in so that it can be effectively processed. For example, raw audio needs to be run through S-T-T or be converted to JSON-formatted files that match the response format from Cloud Speech-to-Text recognition. This information is key for the improvement or fine-tuning of the underlying models for Agent Assist features, if applicable. In the third step of the Discovery Phase, you need to understand the current GCP setup. The right GCP environment needs to be in place with access to all required GCP resources such as Dialog Flow, and STT Cloud Storage. For example, the CCAI suite of products (like DFCX, AA and Insights) may live in one project, while the conversations and documents live in another project. Account for this in this phase. Additionally for UI UX integration, make sure the supported business has the right access to deploy the backend modules and setup pub-subs for the AA event lifecycle. In the last step of the Discovery process, you need to make sure that the implementation team has the expertise to implement the AA UI modules in their Agent Desktop / Frontend. If they have the expertise, great. If not, then the service provider (Googler or Partner) can help the customer with Sales Force integration for demoing the end-to-end AA functionality. You also need to ask if the implementation team can set-up AA Backend modules in their environment? If they can, that’s great. If not,then as their service provider we can help the customer set up the default Backend module in their GCP environment for demoing the end-to-end AA functionality, without any customization. Now, let’s move on to the Design phase. Let's assume the customer uses chat client in the form of a chat interface on their website or any other third party tools, while the Agent uses the Agent Desktop Client. Both of them can be connected to each other through a chat chat backend generally using websocket connection for seamless flow. The chat backend will be responsible for integration with Agent Assist. This means that every time there's a new message, the chat backend will call the analyze content API to get the agent assist suggestions and will forward those suggestions to Agent Desktop in order to help the agent solve the customer's queries. We can also make use of export conversation from Agent Assist in order to dump the conversation in to a GCS bucket. This is a sample blueprint of a typical Agent Assist Architecture coming out of the Design Phase. Once the implementation team has a vision for the end state architecture, they can move to the Implementation and test phase. During implementation, each Agent Assist feature should be under close scrutiny to ensure proper configuration. Thankfully testing is made easy in Agent Assist by leveraging its simulator. Let's take the example of summarization evaluation. You should run the summarization for around 30 to 50 historical conversations and ask the customer to rate them. Make sure that different critical metrics like accuracy, completeness and correctness of the summaries are included in the rating system. Another example can be made for Generative Knowledge Assist. For GKA evaluation, you can use a predefined set of queries on GKA and rate the GKA generated answers. You should also evaluate for associated documents and links that GKA suggests answers from. We need to make sure the overall evaluation meets the standard as defined by the customers. For additional information on how to assess data store generated responses, please refer to the Virtual FAQ (Generative AI Agents) training module of the CCAI Academy. The Smart Reply feature can also be evaluated using the similar methodology as G-K-A Summarization. In this case you should run a set of historical conversations and record the smart reply suggestion for each custom text. You can then compare whether the historical agent response is the same or similar to that suggested by the smart reply model. Finally, to ensure that the entire UI-UX integration with the Agents Desktop and Chat Backend is working as expected, you should perform end-to-end testing to ensure each functionality is working as expected. This can be done by simulating a few rounds of conversation. Once all of the Agent Assist features are implemented and tested, it's time to move them into the production environment. When working with custom modeling, there are several issues to consider regarding development and production environments. If the custom 144 model (or models) was developed in a test or development instance, then be sure to migrate them to the production environment. Also make sure to validate that the model is working as expected on the initial set of conversations. This approach also needs to be replicated for the conversation profile. Make sure all the configurations are set properly in the production environment conversation, especially the pub-sub configuration [If applicable] Monitor an end-to-end UI integration. You can do this by starting with 1 percent of the traffic and subsequently scaling up to 10 percent, then 20 percent and so on. Enjoy building in Agent Assist

### Quiz - [Delivery Lifecycle Quiz](https://www.cloudskillsboost.google/course_templates/1159/quizzes/508930)

## Additional Resources

This module includes the list of additional resources that complement the course learning.

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/1159/documents/508931)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
