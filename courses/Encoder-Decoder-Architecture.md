---
id: 543
name: 'Encoder-Decoder Architecture'
datePublished: 2024-06-21
topics:
- Machine Learning Models
- Machine Learning
- Keras
type: Course
url: https://www.cloudskillsboost.google/course_templates/543
---

# [Encoder-Decoder Architecture](https://www.cloudskillsboost.google/course_templates/543)

**Description:**

This course gives you a synopsis of the encoder-decoder architecture, which is a powerful and prevalent machine learning architecture for sequence-to-sequence tasks such as machine translation, text summarization, and question answering. You learn about the main components of the encoder-decoder architecture and how to train and serve these models. In the corresponding lab walkthrough, you'll code in TensorFlow a simple implementation of the encoder-decoder architecture for poetry generation from the beginning.

**Objectives:**

- Understand the main components of the encoder-decoder architecture.
- Learn how to train and generate text from a model by using the encoder-decoder architecture.
- Learn how to write your own encoder-decoder model in Keras.

## Encoder-Decoder Architecture: Overview

This module gives you a synopsis of the encoder-decoder architecture, which is a powerful and prevalent machine learning architecture for sequence-to-sequence tasks such as machine translation, text summarization, and question answering. You learn about the main components of the encoder-decoder architecture and how to train and serve these models. In the corresponding lab walkthrough, you'll code in TensorFlow a simple implementation of the encoder-decoder architecture for poetry generation from the beginning.

### Video - [Encoder-Decoder Architecture: Overview](https://www.cloudskillsboost.google/course_templates/543/video/488401)

- [YouTube: Encoder-Decoder Architecture: Overview](https://www.youtube.com/watch?v=qZxtqYlZHnM)

Hello everybody, my name Benoit Dherin. I am machine learning engineer at the Google Advanced Solutions Lab. If you want to know more about the Advanced Solutions Lab, please follow the link in the description box below. There is lots of excitement currently around Generative AI and new advancements including new Vertex AI features such as (GenAI Studio, Model Garden, Gen AI API ). Our objective in these short courses is to give you a solid footing on some of the underlying concepts that make all the GenAI magic possible. Today, I am going to talk about the encoder-decoder architecture, which is at the core of large language models. We will start with a brief overview of the architecture. Then I’ll go over how we train these models. And at last, we will see how to produce text from a trained model at serving time. To begin with, the encoder-decoder architecture is a sequence-to-sequence architecture. This means it takes, say, a sequence of words as input, like the sentence in english “The cat ate the mouse” and it outputs, say, the translation in French “Le chat a mange la souris” The encoder-decoder architecture is machine that consumes sequences and spits out sequences. Another input example is the sequence of words forming the prompt sent to a large language model. Then the output is the response of the large language model to this prompt. Now we know what a encoder-decoder architecture does. But how does it do it? Typically, the encoder-decoder architecture has two stages. First, an encoder stage that produces a vector representation of the input sentence. Then this encoder stage is followed by a decoder stage that creates the sequence output . Both the encoder and the decoder can be implemented with different internal architectures. The internal mechanism can be a recurrent neural network as shown in this slide or a more complex transformer block as in the case of the super powerful language models we see nowadays. A recurrent neural network encoder takes each token in the input sequence one at a time, and produces a state representing this token as well as the previously ingested tokens Then this state in used in the next encoding step as input along with the next token to produce the next state. Once you are done ingesting all the the input tokens into the RNN, you output a vector that represents the full input sentence. That’s it for the encoder. What about the decoder part? The decoder takes the vector representation of the input sentence and produces an output sentence from that representation. In the case of a RNN decoder it does it in steps, decoding the output one token at a time using the current state and what has been decoded so far. Okay, now that we have a high level understanding of the encoder-decoder architecture, how do we train it? That’s the training phase. To train a model, you need a dataset, that is a collection of input/ouput pairs that you want your model to imitate. You can then feed this dataset to the model, which will correct its weights during training on the basis of the error it produces on a given input in the dataset. This error is essentially the difference between what the neural network generates given an input sequence and the true output sequence you have in your dataset. Okay. But then how do you produce this dataset? In the case of the encoder-decoder architecture this is more complicated than for typical predictive models. First of all you need a collection of input and output texts. In the case of translation that would be sentence pairs where one sentence is in the source language and the other is in the target language. You’ll feed the source language sentence to the encoder and then compute the error between what the decoder generates and the actual translation. However, there is a catch. The decoder also needs it own input at training time! You’ll need to give the decoder the correct previous translated token as input to generate the next token rather than what the decoder has generated so far. This method of training is called teacher forcing, because you force the decoder to generate the next token from the correct previous token. This means that in your code you’ll have to prepare two input sentences, the original one fed to the encoder, and also the original one shifted to the left that you’ll feed to the decoder. Another subtle point is that the decoder generates at each step only the probability that each token in your vocabulary is the next one. Using these probabilities, you’ll have to select a word. And there are several approaches for that. The simplest one, called greedy search, is to generate the token that has the highest probability. A better approach that produces better results is called beam search. In that case you use the probabilities generated by the decoder to evaluate the probability of sentence chunks rather than individual words. And you keep at each step the most likely generated chunk. That’s how training is done. Now let’s move onto serving. After training, at serving time, when you want to say generate a new translation or a new response to a prompt, You’ll start by feeding the encoder representation of the prompt to the decoder along with a special token like “GO” This will prompt the decoder to generate the first word. Let’s see in more detail what happens during the generation stage. First of all the start token needs to be represented by a vector using an embedding layer. Then the recurrent layer will update the previous state produced by the encoder into a new state. This state will be passed to a dense softmax layer to produce the word probabilities Finally the word is generated by taking the highest probability token with greedy search or the highest probability chunk with beam search. At this point you repeat this procedure for the second word to be generated. And for the third word Until you're done! So what’s next? Well. The difference between the architecture we just learned about and the ones in the large language models is what goes inside of the encoder and decoder blocks. The simple RRN network is replaced by transformer blocks which is an architecture discovered here at Google and which is based on the attention mechanism. If you are interested in knowing more about these topics, we have two more overview courses in that series: Attention Mechanism: Overview, and Transformer Models and BERT Model: Overview. Also, if you liked this the course today, have a look at Encoder-Decoder Architecture: Lab Walkthrough Where I’ll show you how to generate poetry in code using the concepts we have seen in this overview. Thanks for your time! Have a great day!

### Video - [Encoder-Decoder Architecture: Lab Walkthrough](https://www.cloudskillsboost.google/course_templates/543/video/488402)

- [YouTube: Encoder-Decoder Architecture: Lab Walkthrough](https://www.youtube.com/watch?v=FW--2KkTQ1s)

Hello, everybody! My name is Benoit Dherin, a machine learning engineer at Google's Advanced Solutions Lab. If you want to know more about what the Advanced Solutions Lab is, please follow the link below in the description box. There is lots of excitement currently around generative AI and new advancements, including new Vertex AI features such as GenAI Studio, Model Garden, GenAI API. Our objective in this short session is to give you a solid footing on some of the underlying concepts that make all the Gen AI magic possible. Today, I’ll go over the code that’s complementary to the “Encoder-Decoder Architecture Overview” course in the same series. We will see together how to build a poetry generator from scratch using the encoder-decoder architecture. Using the angular decoder architecture you’d find to set up instructions in our GitHub repository. Okay, let's now have a look at the code. To access our lab, go in the asl-ml-immersion folder. Then the notebooks folder. Then the text models folder. And in the solutions from there you'll find the text generation notebook. That's the lab that we'll cover today. In this lab we will implement a character based text generator based on the encoder decoder architecture. Character based means that the tokens consumed and generated by the network are characters and not words. We will use plays as a data set. They have a special structure which are that of people talking with each other. And here you see an example of a piece of text that has been generated by the trained neural network. When the sentences are not necessarily making sense nor are grammatically correct. This is remarkable in many ways. First of all, remember, it's character based. So it means that it learns to predict only the most probable characters. Despite that, it was able to learn pretty well The notion of words separated by blank spaces. And also the basic structure of a play with the characters talking to each other. So going of what is a very small network, as you will see, it's based on the rnn and architecture and only trained for 30 epochs in the vertex air workbench, which is a pretty fast training time. So let's look at the code now. So the first thing is to import the libraries that we need. In particular, we could or encoder decoder architecture using TensorFlow Keras to impart that. Then we download our data set using TF Keras utils.get file. So now the dataset is on disk and we just need to load it into a variable cortex. So the text variable now contains the whole string representing all the all the plays in that Shakespeare dataset. Can I have a quick look at what it is? And you see if we printed the first 250 characters. You have the first citizens speaking to everybody and everybody else is speaking to the first citizen. The cell computes the number of unique characters that we have in that in the text dataset, and we see that we have 65 unique characters, right? These characters would be the tokens that the neural network will consume during training and will generating during this service. So the first step here now is to vectorize the text. What do we mean by that? It means that first of all, we will need to extract from the actual string sequence of characters, which we can do with TensorFlow by using TF strings Unicode split. So now, for example, texts here are transformed into a list of sequences of characters. A neural network cannot consume immediately. The characters. We need to transform that into numbers. So we need to simply map each of the characters to a given id. For that we have the TF Keras layer string look up to which you just need to pass to the list of your vocabulary. The 65 unique character that we have in our corpus and that we produce a layer that when passed the characters will produce corresponding ids. So within that, that layer you have a mapping that has been generated between the characters and the id. To get the inverse mapping, you use the same layer of string lookup with the exact same vocabulary that you retrieve from the first of the year by using get vocabulary. But you set that parameter to be true, invalid, equal equal true, and that will compute the invert mapping, which is the mapping from id to chars Right. And indeed, if you pass to this mapping sequence of ID’s, the ID’s, it gives you back the corresponding characters. Using the mapping that start in the memory of this layer. So that's that. Okay. Now let's that's been the dataset that we will train our neural network with. For that we are using the TF data Dataset API, which has this nice method from tons of slices which will convert. That answer of instance represents or whole corpus of text of plays as id it will store that into it to have data data sets. So at this point, the elements of these datasets are just the individual characters. So that's not great for us. But we want to feed our neural network with our sequences of the same length but not just one character. We need to predict the next character. So but luckily the dataset API has this nice function batch that will do exactly that for us. So if we pass, if we invoke the batch method on our ID dataset, to which we pass a given sequence length, which we said to be 100 here, now the elements, the data points that are stored in our dataset are no longer characters, but the sequences of 100 characters. So here you see an example. If we take just one element, they are no longer characters, but sequences of hundreds of their character IDs you want not characters, but character IDs. Okay, it's not completely we are not completely done here. We still need to create the input sequences that we were going to pass to the decoder and also the sequences that we want to predict. Right? And what are the sequences that are just the sequences of the next character in the input sequence? So for instance, here, if we have the sequence TensorFlow and the sequence TensorFlow at the beginning, then the input sequence we can do from it is tens-or-flow, We know the W and the target sequence that we want to predict is the same sequence, but just shifted by one on the right, so ensor-low and you see that E is the next character for Ring T and is the next there for E, etc. So basically this little function does exactly that. It takes an original sequence, creates an input sequence from that by just truncating that sequence removing the last character and that just the target sequence is created by started at starting add the first character. So how we do that, we just map the split input target function to our sequence dataset. Okay. And it's already does it. Now let's see how to build the model. First off, we set a number of variables the vocabulary size, the size of the vectors. We want to represent the characters will I think That would be 256 and a number of neurons or recurrent layer we'd have. For the model itself. It's a relatively simple model. We create it by using the Keras subclass API. We create just a new class called my model and we subclass here from TF Keras model. When you do that you only have to override two functions, the constructor and the call function. So let's see what each of these function does. The first function takes essentially the hyper parameters of your model, the vocabulary size, the embedding dimension, the number of neuron that number of neurons for your recurrent layer, and it just constricts the layers you will need and store them as variables of the class. Okay. Now really how these layers are connected, all that is specified in the call function, the architecture of your network, if you will. If you want. Let's see where to the body does. Here. Take the input which are sequences of ids representing the characters. We have a first layer that we'll create for each of the inits a vector representing that. So that's the training layer. So as the training progresses, this vector is representing the characters. We'll start to be more and more meaningful. At least that's the idea. Then these static representations of the characters is passed to the recurrent layer that we'll somehow modify these for representation according to the context of what I've seen with what has been seen previously and generate a state of what is seen previously, that would be a reuse in the next step. Finally, we pass the output of the recurrent layer to a dense layer that will output as many numbers that we as we have in our vocabulary, which means one score for each of the possible 65 characters and the score represent the probability of the character being the next one. So that's all that the model does. Then we instantiated. Once we have done that, we can look at the structure of the model using model summary, and you see here you have the I'm building the year, the recurrently year and the dust layer that we just encoded implemented in our, in our model does that. So let's train the model. Before we train the model, we need a loss and that's the loss function that we compared the output of the model with the truth, right? Since that's essentially a classification problem with many classes and the classes being each of the possible characters to be the next, the loss would be the sparse, categorical cross entropy loss. And also because the neural network output, the logits are not directly the probability we configure this loss to be computed not from the probability scores, but from the logits scores. Okay, once we have the loss, we can combine our model, which means that basically we tied to it a loss and also an optimizer. And that will update the weights during training to decrease the loss as much as possible. Basically, it then here we have a little bit of a callback that we will use and that will save the weights during training, which is a useful item. And we are all set up now to start the training. So we do a model.fit on the data set. We choose a number of epochs we want to be trained on. An epoch is a full pass on the data set. So here we we have a look at ten, ten times the corpus of plays we have in our text vector and we give the callback to make sure that the weights are saved during the the training, that's it. So that's relatively simple. We train them my data. We have a train model now what do we do with it? And that's a bit of a complication in the encoder. Decoder architecture is that you cannot through the immediately use your model, you need to write a sort of a decoding function that's here that will decode the generated text a step at a time using the trained model. Okay. So here in this case, we chose to implement this decoding function as a Keras model. So we subclass from the TF Keras Model. The main method in that model is to generate one step. It's a quick look to what it does, so it takes the inputs so the input can be to prompt the initial prompt initial the sequence of character you want to the encoder-decoder model to complete, to predict, to generate new new characters. So you bypass the input it transform that text into a sequence of character, and then the sequence of characters into a sequence of ids. Using the idea from council. Here we have a setup previously, and then we call our model or encoder-decoder model that has been previously trained. And what does it do? It takes this input of ids and output the predicted logits. So this calls for the most probable token the most probable character in this case, along with the state that summarizes what has been seen previously. From the predicted logits, we can compute, we can select the most likely tokens or characters. But before doing that there is a little bit of a trick, which is that we divide the logits by a temperature, by a number. So basically if the temperature is one, nothing happens. But if the temperature is very high, what it will do, it will makes the scores associated to each of the token to be predictive. Next will be relatively similar, close to zero. This means that actually this token would be more and more likely to be chosen, right? So there would be more variety, more a more stuff can be predicted if the temperature is higher. So it's a bit more creative If you have a two high temperature, of course, the neural network would just predict the gibberish. Okay. And if you have a true temperature, the highest probability score will be just multiply by a very large number because it's divided by a small number, it's a number between zero and one, which means that the highest score will be become much, much bigger than the other scores, giving a much higher chance to be selected, which gives you more of the deterministic behavior. Okay, that's the temperature. That's an important parameter, as in this type of architecture. Okay. And that's what it does. Okay. So now we have the predicted logits we use TFrandom categorical to just sample from these probability scores the most likely idea is to be next. We transform that back to a character and that's what we return. Okay, So that's essentially what the decoding function does and most decoding function at the very same structure. There is also this temperature trick that you can see as a as a parameter in the case of large language models. Okay, so let's use our decoding function. So typically you use that in the loop. So here we are going to predict 1000 characters by repeatedly making a call to the decoding function generated one step, to which you feed what has been predicted before, along with the state summarizing what happened before, and it predict the next character along with a new state. And we start the process. We do sort of a prompt here. That's Romeo. What are you going to say? And then the there are let's let's see what the neuron that generates, right? Says no good corona at least take your feetle and if I seem to my as you... so you see it's not it doesn't make a lot of sense here. Remember I've trained it only a few minutes on the work bench, AI work bench in Vertex AI Workbench, which are great by the way, but here that's a small instance which just one GPU So it was a very small training. The model is written in a few lines, but yet you still see that it can really pick up a lot of things in the structure of the of the input data. It detects patterns that you have characters. So Romeo, that was our input, but then Leontes was generated by the network and then what Leontes says. So okay, that's it. If you like this presentation, you'll find more on our ASL GitHub repository with 90 plus machine learning and notebooks. Don't forget it. If you find it useful, please star our repo. Thanks for your time.

### Quiz - [Encoder-Decoder Architecture: Quiz](https://www.cloudskillsboost.google/course_templates/543/quizzes/488403)

#### Quiz 1.

> [!important]
> **What is the purpose of the decoder in an encoder-decoder architecture?**
>
> - [ ] To generate the output sequence from the vector representation
> - [ ] To predict the next word in the output sequence
> - [ ] To convert the input sequence into a vector representation
> - [ ] To learn the relationship between the input and output sequences

#### Quiz 2.

> [!important]
> **What are two ways to generate text from a trained encoder-decoder model at serving time?**
>
> - [ ] Teacher forcing and beam search
> - [ ] Teacher forcing and attention
> - [ ] Greedy search and attention
> - [ ] Greedy search and beam search

#### Quiz 3.

> [!important]
> **What is the purpose of the encoder in an encoder-decoder architecture?**
>
> - [ ] To learn the relationship between the input and output sequences
> - [ ] To predict the next word in the output sequence
> - [ ] To generate the output sequence from the vector representation
> - [ ] To convert the input sequence into a vector representation

#### Quiz 4.

> [!important]
> **What is the difference between greedy search and beam search?**
>
> - [ ] Greedy search considers multiple possible words and selects the one with the highest combined probability, whereas beam search always selects the word with the highest probability.
> - [ ] Greedy search always selects the word with the lowest probability, whereas beam search considers multiple possible words and selects the one with the lowest combined probability.
> - [ ] Greedy search always selects the word with the highest probability, whereas beam search considers multiple possible words and selects the one with the highest combined probability.
> - [ ] Greedy search considers multiple possible words and selects the one with the lowest combined probability, whereas beam search always selects the word with the lowest probability.

#### Quiz 5.

> [!important]
> **What is the name of the machine learning architecture that takes a sequence of words as input and outputs a sequence of words?**
>
> - [ ] Large stream text manipulation
> - [ ] Regressive neural networking
> - [ ] Encoder-decoder
> - [ ] Collaborative natural network

### Link - [Encoder-Decoder Architecture: Lab Resources](https://www.cloudskillsboost.google/course_templates/543/documents/488404)

- [Encoder-Decoder Architecture: Lab Resources](https://github.com/GoogleCloudPlatform/asl-ml-immersion/blob/master/notebooks/text_models/solutions/text_generation.ipynb)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
