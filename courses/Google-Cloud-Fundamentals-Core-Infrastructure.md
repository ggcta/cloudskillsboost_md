---
id: 60
name: 'Google Cloud Fundamentals: Core Infrastructure'
type: Course
url: https://www.cloudskillsboost.google/course_templates/60
date: 2025-04-03
datePublished: 2025-01-23
topics:
- IaaS
- Cloud Deploy
- Cloud Computing
---

# [Google Cloud Fundamentals: Core Infrastructure](https://www.cloudskillsboost.google/course_templates/60)

**Description:**

Google Cloud Fundamentals: Core Infrastructure introduces important concepts and terminology for working with Google Cloud. Through videos and hands-on labs, this course presents and compares many of Google Cloud's computing and storage services, along with important resource and policy management tools.

**Objectives:**

- Identify the purpose and value of Google Cloud products and services
- Define how infrastructure is organized and controlled in Google Cloud
- Explain how to create a basic infrastructure in Google Cloud
- Select and use Google Cloud storage options

## Course Introduction

This section welcomes learners to the Google Cloud Fundamentals: Core Infrastructure course, and provides an overview of the course structure and goals.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/60/video/521820)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=xVmam01f4Fw)

Hi, and welcome to the Google Cloud Fundamentals: Core Infrastructure training course. The goal of this course is to provide you with an overview of Google Cloud. Google Cloud offerings can be broadly categorised as compute, storage, big data, machine learning, and application services for web, mobile, analytics, and back-end solutions. Through a combination of videos, quizzes, and hands-on labs, you'll learn the value of Google Cloud and how cloud solutions factor into business strategies. The intended target audience of today's course consists of solutions developers, systems operations professionals, and solution architects planning to deploy applications and create application environments on Google Cloud. The course will also be useful for business decision makers evaluating Google Cloud. While you should be happy to hear that we'll be finding out about services and concepts that are specific to Google Cloud in this course, do keep in mind that, as a 'fundamentals' level course, some content will be geared towards learners who are entirely new to cloud technologies. This course has no prerequisites, although it's helpful be familiar with application development, Linux operating systems, systems operations, and data analytics/machine learning to best understand the technologies covered. There are seven key learning objectives that we're hoping to achieve. By the end of this course, you should be able to: Identify the purpose and value of Google Cloud products and services. Define how infrastructure is organized and controlled in Google Cloud. Explain how to create a basic infrastructure in Google Cloud. Select and use Google Cloud storage options. Describe the purpose and value of Google Kubernetes Engine. Identify the use cases for serverless Google Cloud services. And combine Google Cloud knowledge with prompt engineering to improve Gemini responses. OK, all set? Let's begin!

## Introducing Google Cloud

This section identifies some of the key benefits of using Google Cloud. It's here that we introduce the components of the Google network infrastructure, and explore the differences between infrastructure as a service (IaaS) and platform as a service (PaaS).

### Video - [Cloud computing overview](https://www.cloudskillsboost.google/course_templates/60/video/521821)

- [YouTube: Cloud computing overview](https://www.youtube.com/watch?v=ph5hjgOAf40)

Let's start at the beginning with an overview of cloud computing. The cloud is a hot topic these days, but what exactly is it? The US National Institute of Standards and Technology created the term cloud computing, although there is nothing US-specific about it. Cloud computing is a way of using information technology (IT) that has these five equally important traits. First, customers get computing resources that are on-demand and self-service. Through a web interface, users get the processing power, storage, and network they require without the need for human intervention. Second, customers get access to those resources over the internet, from anywhere they have a connection. Third, the cloud provider has a big pool of those resources and allocates them to users out of that pool. That allows the provider to buy in bulk and pass the savings on to the customers. Customers don't have to know or care about the exact physical location of those resources. Fourth, the resources are elastic–which means they're flexible, so customers can be. If customers need more resources they can get more, and quickly. If they need less, they can scale back. And finally, customers pay only for what they use, or reserve as they go. If they stop using resources, they stop paying. And that's it, that's the definition of cloud. But why is the cloud model so compelling nowadays? To understand why, we need to look at some history. The trend towards cloud computing started with a first wave known as colocation. Colocation gave users the financial efficiency of renting physical space, instead of investing in data center real estate. Virtualized data centers of today, which are the second wave, share similarities with the private data centers and colocation facilities of decades past. The components of virtualized data centers match the physical building blocks of hosted computing—servers, CPUs, disks, load balancers, and so on—but now they're virtual devices. With virtualization, enterprises still maintain the infrastructure; but it also remains a user-controlled and user-configured environment. Several years ago, Google realized that its business couldn't move fast enough within the confines of the virtualization model. So Google switched to a container-based architecture— a fully automated, elastic third-wave cloud that consists of a combination of automated services and scalable data. Services automatically provision and configure the infrastructure used to run applications. Today, Google Cloud makes this third-wave cloud available to Google customers. Google believes that, in the future, every company, regardless of size or industry, will differentiate itself from its competitors through technology. Increasingly, that technology will be in the form of software. Great software is based on high-quality data. This means that every company is, or will eventually become, a data company.

### Video - [IaaS and PaaS](https://www.cloudskillsboost.google/course_templates/60/video/521822)

- [YouTube: IaaS and PaaS](https://www.youtube.com/watch?v=C7cb6kFhNmw)

The move to virtualized data centers introduced customers to two new types of offerings: infrastructure as a service, commonly referred to as IaaS, and platform as a service, or PaaS. IaaS offerings provide raw compute, storage, and network capabilities, organized virtually into resources that are similar to physical data centers. Compute Engine is an example of a Google Cloud IaaS service. PaaS offerings, in contrast, bind code to libraries that provide access to the infrastructure application needs. This allows more resources to be focused on application logic. App Engine is an example of a Google Cloud PaaS service. In the IaaS model, customers pay for the resources they allocate ahead of time; in the PaaS model, customers pay for the resources they actually use. As cloud computing has evolved, the momentum has shifted toward managed infrastructure and managed services. Leveraging managed resources and services allows companies to concentrate more on their business goals and spend less time and money on creating and maintaining their technical infrastructure. It allows companies to deliver products and services to their customers more quickly and reliably. Serverless is yet another step in the evolution of cloud computing. It allows developers to concentrate on their code, rather than on server configuration, by eliminating the need for any infrastructure management. Serverless technologies offered by Google include Cloud Run, which allows customers to deploy their containerized microservices based application in a fully-managed environment. and Cloud Run functions, which manages event-driven code as a pay-as-you-go service. While it's outside the scope of this course, you might have heard about software as a service, SaaS, and wondered what it is and how it fits into the Cloud ecosphere. SaaS provides the entire application stack, delivering an entire cloud-based application that customers can access and use. Software as a Service applications are not installed on your local computer. Instead, they run in the cloud as a service and are consumed directly over the internet by end users. Popular Google applications such as Gmail, Docs, and Drive, that are a part of Google Workspace, are all examples of SaaS.

### Video - [The Google Cloud network](https://www.cloudskillsboost.google/course_templates/60/video/521823)

- [YouTube: The Google Cloud network](https://www.youtube.com/watch?v=0LIJioph_nY)

Google Cloud runs on Google's own global network. It's the largest network of its kind, and Google has invested billions of dollars over many years to build it. This network is designed to give customers the highest possible throughput and lowest possible latencies for their applications by leveraging more than 100 content caching nodes worldwide. These are locations where high demand content is cached for quicker access, allowing applications to respond to user requests from the location that will provide the quickest response time. Google Cloud's locations underpin all of the important work we do for our customers. From redundant cloud regions to high-bandwidth connectivity via subsea cables, every aspect of our infrastructure is designed to deliver your services to your users, no matter where they are around the world. Google Cloud's infrastructure is based in five major geographic locations: North America, South America, Europe, Asia, and Australia. Having multiple service locations is important because choosing where to locate applications affects qualities like availability, durability, and latency, the latter of which measures the time a packet of information takes to travel from its source to its destination. Each of these locations is divided into several different regions and zones. Regions represent independent geographic areas and are composed of zones. For example, London, or europe-west2, is a region that currently comprises three different zones. A zone is an area where Google Cloud resources are deployed. For example, if you launch a virtual machine using Compute Engine it will run in the zone that you specify to ensure resource redundancy. You can run resources in different regions. This is useful for bringing applications closer to users around the world, and also for protection in case there are issues with an entire region, say, due to a natural disaster. Some of Google Cloud's services support placing resources in what we call a multi-region. For example, Spanner multi-region configurations allow you to replicate the database's data not just in multiple zones, but in multiple zones across multiple regions, as defined by the instance configuration. These additional replicas enable you to read data with low latency from multiple locations close to or within the regions in the configuration, like The Netherlands, and Belgium. Google Cloud currently supports 121 zones in 40 regions, although this number is increasing all the time. You can find the most up-to-date numbers at cloud.google.com/about/locations.

### Video - [Environmental impact](https://www.cloudskillsboost.google/course_templates/60/video/521824)

- [YouTube: Environmental impact](https://www.youtube.com/watch?v=yOoOz6umhz0)

The virtual world, which includes Google Cloud's network, is built on physical infrastructure, and all those racks of humming servers use huge amounts of energy. Altogether, existing data centers use roughly 2% of the world's electricity. With this in mind, Google works to make their data centers run as efficiently as possible. Just like our customers, Google is trying to do the right things for the planet. We understand that Google Cloud customers have environmental goals of their own, and running their workloads on Google Cloud can be a part of meeting those goals. Therefore, it's useful to note that Google's data centers were the first to achieve ISO 14001 certification, which is a standard that maps out a framework for an organization to enhance its environmental performance through improving resource efficiency and reducing waste. As an example of how this is being done, here's Google's data center in Hamina, Finland. This facility is one of the most advanced and efficient data centers in the Google fleet. Its cooling system, which uses sea water from the Bay of Finland, reduces energy use and is the first of its kind anywhere in the world. In our founding decade, Google became the first major company to be carbon neutral. In our second decade, we were the first company to achieve 100% renewable energy. By 2030, we aim to be the first major company to operate completely carbon free.

### Video - [Security](https://www.cloudskillsboost.google/course_templates/60/video/521825)

- [YouTube: Security](https://www.youtube.com/watch?v=BggWZl8qTzk)

Nine of Google's services have more than one billion users each, and so you can be assured that security is always on the minds of Google's employees. Design for security is prevalent throughout the infrastructure that Google Cloud and Google services run on. Let's talk about a few ways Google works to keep customers' data safe. The security infrastructure can be explained in progressive layers, starting from the physical security of our data centers, continuing on to how the hardware and software that underlie the infrastructure are secured, and finally, describing the technical constraints and processes in place to support operational security. We begin with the Hardware infrastructure layer which comprises three key security features: The first is hardware design and provenance. Both the server boards and the networking equipment in Google data centers are custom-designed by Google. Google also designs custom chips, including a hardware security chip that's currently being deployed on both servers and peripherals. The next feature is a secure boot stack. Google server machines use a variety of technologies to ensure that they are booting the correct software stack, such as cryptographic signatures over the BIOS, bootloader, kernel, and base operating system image. This layer's final feature is premises security. Google designs and builds its own data centers, which incorporate multiple layers of physical security protections. Access to these data centers is limited to only a very small number of Google employees. Google additionally hosts some servers in third-party data centers, where we ensure that there are Google-controlled physical security measures on top of the security layers provided by the data center operator. Next is the Service deployment layer, where the key feature is encryption of inter-service communication. Google's infrastructure provides cryptographic privacy and integrity for remote procedure call ("RPC") data on the network. Google's services communicate with each other using RPC calls. The infrastructure automatically encrypts all infrastructure RPC traffic that goes between data centers. Google has started to deploy hardware cryptographic accelerators that will allow it to extend this default encryption to all infrastructure RPC traffic inside Google data centers. Then we have the User identity layer. Google's central identity service, which usually manifests to end users as the Google login page, goes beyond asking for a simple username and password. The service also intelligently challenges users for additional information based on risk factors such as whether they have logged in from the same device or a similar location in the past. Users can also employ secondary factors when signing in, including devices based on the Universal 2nd Factor (U2F) open standard. On the Storage services layer we find the encryption at rest security feature. Most applications at Google access physical storage (in other words, "file storage") indirectly via storage services, and encryption using centrally managed keys is applied at the layer of these storage services. Google also enables hardware encryption support in hard drives and SSDs. The next layer up is the Internet communication layer, and this comprises two key security features. Google services that are being made available on the internet, register themselves with an infrastructure service called the Google Front End, which ensures that all TLS connections are ended using a public-private key pair and an X.509 certificate from a Certified Authority (CA), as well as following best practices such as supporting perfect forward secrecy. The GFE additionally applies protections against Denial of Service attacks. Also provided is Denial of Service ("DoS") protection. The sheer scale of its infrastructure enables Google to simply absorb many DoS attacks. Google also has multi-tier, multi-layer DoS protections that further reduce the risk of any DoS impact on a service running behind a GFE. The final layer is Google's Operational security layer which provides four key features. First is intrusion detection. Rules and machine intelligence give Google's operational security teams warnings of possible incidents. Google conducts Red Team exercises to measure and improve the effectiveness of its detection and response mechanisms. Next is reducing insider risk. Google aggressively limits and actively monitors the activities of employees who have been granted administrative access to the infrastructure. Then there's employee U2F use. To guard against phishing attacks against Google employees, employee accounts require use of U2F-compatible Security Keys. Finally, there are stringent software development practices. Google employs central source control and requires two-party review of new code. Google also provides its developers libraries that prevent them from introducing certain classes of security bugs. Additionally, Google runs a Vulnerability Rewards Program where we pay anyone who is able to discover and inform us of bugs in our infrastructure or applications. You can learn more about Google's technical-infrastructure security at cloud.google.com/security/security-design.

### Video - [Open source ecosystems](https://www.cloudskillsboost.google/course_templates/60/video/521826)

- [YouTube: Open source ecosystems](https://www.youtube.com/watch?v=gYZGSrNffF8)

Some organizations are afraid to bring their workloads to the cloud because they're afraid they'll get locked into a particular vendor. However, if, for whatever reason, a customer decides that Google is no longer the best provider for their needs, we provide them with the ability to run their applications elsewhere. Google publishes key elements of technology using open source licenses to create ecosystems that provide customers with options other than Google. For example, TensorFlow, an open source software library for machine learning developed inside Google, is at the heart of a strong open source ecosystem. Google provides interoperability at multiple layers of the stack. Kubernetes and Google Kubernetes Engine give customers the ability to mix and match microservices running across different clouds, while Google Cloud Observability lets customers monitor workloads across multiple cloud providers.

### Video - [Pricing and billing](https://www.cloudskillsboost.google/course_templates/60/video/521827)

- [YouTube: Pricing and billing](https://www.youtube.com/watch?v=PRRf8y-Y5Bo)

To round off this section of the course, let's take a brief look at Google Cloud's pricing structure. Google was the first major cloud provider to deliver per-second billing for its infrastructure-as-a-service compute offering, Compute Engine. In addition, per-second billing is now also offered for users of Google Kubernetes Engine (our container infrastructure as a service), Dataproc (which is the equivalent of the big data system Hadoop, but operating as a service), and App Engine flexible environment VMs (a platform as a service). Compute Engine offers automatically applied sustained-use discounts, which are automatic discounts that you get for running a virtual machine instance for a significant portion of the billing month. Specifically, when you run an instance for more than 25% of a month, Compute Engine automatically gives you a discount for every incremental minute you use for that instance. Custom virtual machine types allow Compute Engine virtual machines to be fine-tuned with optimal amounts of vCPU and memory for their applications so that you can tailor your pricing for your workloads. Our online pricing calculator can help estimate your costs. Visit cloud.google.com/products/calculator to try it out. Now, you're probably thinking, "How can I make sure I don't accidentally run up a big Google Cloud bill?" You can define budgets at the billing account level or at the project level. A budget can be a fixed limit, or it can be tied to another metric; for example, a percentage of the previous month's spend. To be notified when costs approach your budget limit, you can create an alert. For example, with a budget limit of $20,000 and an alert set at 90%, you'll receive a notification alert when your expenses reach $18,000. Alerts are generally set at 50%, 90% and 100%, but can also be customized. Reports is a visual tool in the Google Cloud Console that allows you to monitor expenditure based on a project or services. Finally, Google Cloud also implements quotas, which are designed to prevent the over-consumption of resources because of an error or a malicious attack, protecting both account owners and the Google Cloud community as a whole. There are two types of quotas: rate quotas and allocation quotas. Both are applied at the project level. Rate quotas reset after a specific time. For example, by default, the GKE service implements a quota of 3,000 calls to its API from each Google Cloud project every 100 seconds. After that 100 seconds, the limit is reset. Allocation quotas govern the number of resources you can have in your projects. For example, by default, each Google Cloud project has a quota allowing it no more than 15 Virtual Private Cloud networks. Although projects all start with the same quotas, you can change some of them by requesting an increase from Google Cloud Support.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/60/quizzes/521828)

#### Quiz 1.

> [!important]
> **What type of cloud computing service lets you bind your application code to libraries that give access to the infrastructure your application needs?**
>
> - [ ] Hybrid cloud
> - [ ] Virtualized data centers
> - [ ] Software as a service
> - [ ] Infrastructure as a service
> - [ ] Platform as a service

#### Quiz 2.

> [!important]
> **What is the primary benefit to a Google Cloud customer of using resources in several zones within a region?**
>
> - [ ] For getting discounts on other zones
> - [ ] For expanding services to customers in new areas
> - [ ] For better performance
> - [ ] For improved fault tolerance

#### Quiz 3.

> [!important]
> **Why might a Google Cloud customer use resources in several regions around the world?**
>
> - [ ] To offer localized application versions in different regions.
> - [ ] To bring their applications closer to users around the world, and for improved fault tolerance
> - [ ] To earn discounts
> - [ ] To improve security

## Resources and Access in the Cloud

This section explores how resources get organized with projects, and how access to those resources gets shared with the right part of a workforce through a tool called Identity and Access Management (IAM). It's also in this section that we identify different ways to interact with Google Cloud.

### Video - [Google Cloud resource hierarchy](https://www.cloudskillsboost.google/course_templates/60/video/521829)

- [YouTube: Google Cloud resource hierarchy](https://www.youtube.com/watch?v=zdxQZh2iOFE)

In this section of the course we'll look at the functional structure of Google Cloud. Google Cloud's resource hierarchy contains four levels, and starting from the bottom up they are: resources, projects, folders, and an organization node. At the first level are resources. These represent virtual machines, Cloud Storage buckets, tables in BigQuery, or anything else in Google Cloud. Resources are organized into projects, which sit on the second level. Projects can be organized into folders, or even subfolders. These sit at the third level. And then at the top level is an organization node, which encompasses all the projects, folders, and resources in your organization. It's important to understand this resource hierarchy because it directly relates to how policies are managed and applied when you use Google Cloud. Policies can be defined at the project, folder, and organization node levels. Some Google Cloud services allow policies to be applied to individual resources, too. Policies are also inherited downward. This means that if you apply a policy to a folder, it will also apply to all of the projects within that folder. Let's take a look at the second level of the resource hierarchy, projects, in a little more detail. Projects are the basis for enabling and using Google Cloud services, like managing APIs, enabling billing, adding and removing collaborators, and enabling other Google services. Each project is a separate entity under the organization node, and each resource belongs to exactly one project. Projects can have different owners and users because they're billed and managed separately. Each Google Cloud project has three identifying attributes: a project ID, a project name, and a project number. The project ID is a globally unique identifier assigned by Google that can't be changed after creation. They're what we refer to as being immutable. Project IDs are used in different contexts to inform Google Cloud of the exact project to work with. Project names, however, are user-created. They don't have to be unique and they can be changed at any time, so they are not immutable. Google Cloud also assigns each project a unique project number. It's helpful to know that these Google-generated numbers exist, but we won't explore them much in this course. They're mainly used internally by Google Cloud to keep track of resources. Google Cloud's Resource Manager tool is designed to programmatically help you manage projects. It's an API that can gather a list of all the projects associated with an account, create new projects, update existing projects, and delete projects. It can even recover projects that were previously deleted,and can be accessed through the RPC API and the REST API. The third level of the Google Cloud resource hierarchy is folders. Folders let you assign policies to resources at a level of granularity you choose. The resources in a folder inherit policies and permissions assigned to that folder. A folder can contain projects, other folders, or a combination of both. You can use folders to group projects under an organization in a hierarchy. For example, your organization might contain multiple departments, each with its own set Google Cloud resources. Folders allow you to group these resources on a per-department basis. Folders also give teams the ability to delegate administrative rights so that they can work independently. As previously mentioned, the resources in a folder inherit policies and permissions from that folder. For example, if you have two different projects that are administered by the same team, you can put policies into a common folder so they have the same permissions. Doing it the other way--putting duplicate copies of those policies on both projects–could be tedious and error-prone. if you needed to change permissions on both resources, you would now have to do that in two places instead of just one. To use folders, you must have an organization node, which is the very topmost resource in the Google Cloud hierarchy. Everything else attached to that account goes under this node, which includes folders, projects, and other resources. There are some special roles associated with this top-level organization node. For example, you can designate an organization policy administrator so that only people with privilege can change policies. You can also assign a project creator role, which is a great way to control who can create projects and, therefore, who can spend money. How a new organization node is created depends on whether your company is also a Google Workspace customer. If you have a Workspace domain, Google Cloud projects will automatically belong to your organization node. Otherwise, you can use Cloud Identity, Google's identity, access, application, and endpoint management platform, to generate one. Once created, a new organization node will let anyone in the domain create projects and billing accounts, just as they could before. folders underneath it and put projects into it. Both folders and projects are considered to be "children" of the organization node.

### Video - [Identity and Access Management (IAM)](https://www.cloudskillsboost.google/course_templates/60/video/521830)

- [YouTube: Identity and Access Management (IAM)](https://www.youtube.com/watch?v=Di1T4RyO9yg)

When an organization node contains lots of folders, projects, and resources, a workforce might need to restrict who has access to what. To help with this task, administrators can use Identity and Access Management, or IAM. With IAM, administrators can apply policies that define who can do what and on which resources. The "who" part of an IAM policy can be a Google account, a Google group, a service account, or a Cloud Identity domain. A "who" is also called a "principal." Each principal has its own identifier, usually an email address. The "can do what" part of an IAM policy is defined by a role. An IAM role is a collection of permissions. When you grant a role to a principal, you grant all the permissions that the role contains. For example, to manage virtual machine instances in a project, you must be able to create, delete, start, stop and change virtual machines. So these permissions are grouped into a role to make them easier to understand and easier to manage. When a principal is given a role on a specific element of the resource hierarchy, the resulting policy applies to both the chosen element and all the elements below it in the hierarchy. You can define deny rules that prevent certain principals from using certain permissions, regardless of the roles they're granted. This is because IAM always checks relevant deny policies before checking relevant allow policies. Deny policies, like allow policies, are inherited through the resource hierarchy. There are three kinds of roles in IAM: basic, predefined, and custom. The first role type is basic. Basic roles are quite broad in scope. When applied to a Google Cloud project, they affect all resources in that project. Basic roles include owner, editor, viewer, and billing administrator. Let's look at these basic roles in a bit more detail. Project viewers can access resources but can't make changes. Project editors can access and make changes to a resource. And project owners can also access and make changes to a resource. In addition, project owners can manage the associated roles and permissions and set up billing. Often companies want someone to control the billing for a project but not be able to change the resources in the project. This is possible through a billing administrator role. A word of caution: If several people are working together on a project that contains sensitive data, basic roles are probably too broad. Fortunately, IAM provides other ways to assign permissions that are more specifically tailored to meet the needs of typical job roles. This brings us to the second type of role, predefined roles. Specific Google Cloud services offer sets of predefined roles, and they even define where those roles can be applied. Let's look at Compute Engine, for example, a Google Cloud product that offers virtual machines as a service. With Compute Engine, you can apply specific predefined roles—such as "instanceAdmin"—to Compute Engine resources in a given project, a given folder, or an entire organization. This then allows whoever has these roles to perform a specific set of predefined actions. But what if you need to assign a role that has even more specific permissions? That's when you'd use a custom role. Many companies use a "least-privilege" model in which each person in your organization is given the minimal amount of privilege needed to do their job. So, for example, maybe you want to define an "instanceOperator" role to allow some users to stop and start Compute Engine virtual machines, but not reconfigure them. Custom roles will allow you to define those exact permissions. Before you start creating custom roles, please note two important details. First, you'll need to manage the permissions that define the custom role you've created. Because of this, some organizations decide they'd rather use the predefined roles. And second, custom roles can only be applied to either the project level or organization level. They can't be applied to the folder level.

### Video - [Service accounts](https://www.cloudskillsboost.google/course_templates/60/video/521831)

- [YouTube: Service accounts](https://www.youtube.com/watch?v=xoo5NfLqePY)

Imagine you have a Compute Engine virtual machine running a program that needs to access other cloud services regularly. Instead of requiring a person to manually grant access each time the program runs, you can give the virtual machine itself the necessary permissions. This is where service accounts come in. Service accounts allow you to assign specific permissions to a virtual machine, so it can interact with other cloud services without human intervention. Let's say you have an application running in a virtual machine that needs to store data in Cloud Storage, but you don't want anyone on the internet to have access to that data - just that particular virtual machine. You can create a service account to authenticate that VM to Cloud Storage. Service accounts are named with an email address, but instead of passwords they use cryptographic keys to access resources. So, if a service account has been granted Compute Engine's Instance Admin role, this would allow an application running in a VM with that service account to create, modify, and delete other VMs. Service accounts do need to be managed. For example, maybe Alice needs to manage which Google accounts can act as service accounts, while Bob just needs to be able to view a list of service accounts. Fortunately, in addition to being an identity, a service account is also a resource, so it can have IAM policies of its own attached to it. This means that Alice can have the editor role on a service account, and Bob can have the viewer role. This is just like granting roles for any other Google Cloud resource.

### Video - [Cloud Identity](https://www.cloudskillsboost.google/course_templates/60/video/521832)

- [YouTube: Cloud Identity](https://www.youtube.com/watch?v=EZccX9nFaiI)

When new Google Cloud customers start using the platform, it's common to log in to the Google Cloud Console with a Gmail account and then use Google Groups to collaborate with teammates who are in similar roles. Although this approach is easy to start with, it can present challenges later because the team's identities are not centrally managed. This can be problematic if, for example, someone leaves the organization. With this setup, there's no easy way to immediately remove a user's access to the team's cloud resources. With a tool called Cloud Identity, organizations can define policies and manage their users and groups using the Google Admin Console. Admins can log in and manage Google Cloud resources using the same usernames and passwords they already use in existing Active Directory or LDAP systems. Using Cloud Identity also means that when someone leaves an organization, an administrator can use the Google Admin Console to disable their account and remove them from groups. Cloud Identity is available in a free edition and also in a premium edition that provides capabilities to manage mobile devices. If you're a Google Cloud customer who is also a Google Workspace customer, this functionality is already available to you in the Google Admin Console.

### Video - [Interacting with Google Cloud](https://www.cloudskillsboost.google/course_templates/60/video/521833)

- [YouTube: Interacting with Google Cloud](https://www.youtube.com/watch?v=KJS0FnXF7Kg)

There are four ways to access and interact with Google Cloud. The Cloud Console, the Cloud SDK and Cloud Shell, the APIs, and the Google Cloud App. Let's explore each of those now. First is the Google Cloud Console, which is Google Cloud's graphical user interface, or GUI, that helps you deploy, scale, and diagnose production issues in a simple web-based interface. With the Cloud Console, you can easily find your resources, check their health, have full management control over them, and set budgets to control how much you spend on them. The Cloud Console also provides a search facility to quickly find resources and connect to instances via SSH in the browser. Second is through the Cloud SDK and Cloud Shell. The Cloud SDK is a set of tools that you can use to manage resources and applications hosted on Google Cloud. These include the Google Cloud CLI, which provides the main command-line interface for Google Cloud products and services, and bq, a command-line tool for BigQuery. When installed, all of the tools within the Cloud SDK are located under the bin directory. Cloud Shell provides command-line access to cloud resources directly from a browser. Cloud Shell is a Debian-based virtual machine with a persistent 5 gigabyte home directory, which makes it easy to manage Google Cloud projects and resources. With Cloud Shell, the Cloud SDK gcloud command and other utilities are always installed, available, up to date, and fully authenticated. The third way to access Google Cloud is through application programming interfaces, or APIs. The services that make up Google Cloud offer APIs so that code you write can control them. The Cloud Console includes a tool called the Google APIs Explorer that shows which APIs are available, and in which versions. You can try these APIs interactively, even those that require user authentication. So, suppose you've explored an API, and you're ready to build an application that uses it. Do you have to start coding from scratch? No. Google provides Cloud Client libraries and Google API Client libraries in many popular languages to take a lot of the drudgery out of the task of calling Google Cloud from your code. Languages currently represented in these libraries are Java, Python, PHP, C#, Go, Node.js, Ruby, and C++. And finally, the fourth way to access and interact with Google Cloud is with the Google Cloud app, which can be used to start, stop, and use SSH to connect to Compute Engine instances and see logs from each instance. It also lets you stop and start Cloud SQL instances. Additionally, you can administer applications deployed on App Engine by viewing errors, rolling back deployments, and changing traffic splitting. The Google Cloud app provides up-to-date billing information for your projects and billing alerts for projects that are going over budget. You can set up customizable graphs showing key metrics such as CPU usage, network usage, requests per second, and server errors. The app also offers alerts and incident management. You can download the Google Cloud app at cloud.google.com/app.

### Lab - [Google Cloud Fundamentals: Getting Started with Cloud Marketplace](https://www.cloudskillsboost.google/course_templates/60/labs/521834)

In this lab you use Cloud Marketplace to quickly and easily deploy a LAMP stack on a Compute Engine instance. The Bitnami LAMP Stack provides a complete web development environment for Linux that can be launched in one click.

- [ ] [Google Cloud Fundamentals: Getting Started with Cloud Marketplace](../labs/Google-Cloud-Fundamentals-Getting-Started-with-Cloud-Marketplace.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/60/quizzes/521835)

#### Quiz 1.

> [!important]
> **Which of these values is globally unique, permanent, and unchangeable, but can be modified by the customer during creation?**
>
> - [ ] The project number
> - [ ] The project's billing credit-card number
> - [ ] The project name
> - [ ] The project ID

#### Quiz 2.

> [!important]
> **Order these IAM role types from broadest to finest-grained.**
>
> - [ ] Custom roles, predefined roles, basic roles
> - [ ] Predefined roles, custom roles, basic roles
> - [ ] Basic roles, predefined roles, custom roles

#### Quiz 3.

> [!important]
> **Choose the correct completion: Services and APIs are enabled on a per-__________ basis.**
>
> - [ ] Project
> - [ ] Folder
> - [ ] Organization
> - [ ] Billing account

## Virtual Machines and Networks in the Cloud

This section of the course explores how Google Compute Engine works, with a focus on virtual networking.

### Video - [Virtual Private Cloud networking](https://www.cloudskillsboost.google/course_templates/60/video/521836)

- [YouTube: Virtual Private Cloud networking](https://www.youtube.com/watch?v=SFRCZvJN650)

In this section of the course, we're going to explore how Google Compute Engine works with a focus on virtual networking. Many users start with Google Cloud by defining their own virtual private cloud inside their first Google Cloud project or by starting with the default virtual private cloud. So, what is a virtual private cloud? A virtual private cloud, or VPC, is a secure, individual, private cloud-computing model hosted within a public cloud – like Google Cloud! On a VPC, customers can run code, store data, host websites, and do anything else they could do in an ordinary private cloud, but this private cloud is hosted remotely by a public cloud provider. This means that VPCs combine the scalability and convenience of public cloud computing with the data isolation of private cloud computing. VPC networks connect Google Cloud resources to each other and to the internet. This includes segmenting networks, using firewall rules to restrict access to instances, and creating static routes to forward traffic to specific destinations. Here's something that tends to surprise a lot of new Google Cloud users: Google VPC networks are global. They can also have subnets, which is a segmented piece of the larger network, in any Google Cloud region worldwide. Subnets can span the zones that make up a region. This architecture makes it easy to define network layouts with global scope. Resources can even be in different zones on the same subnet. The size of a subnet can be increased by expanding the range of IP addresses allocated to it, and doing so won't affect virtual machines that are already configured. For example, let's take a VPC network named vpc1 that has two subnets defined in the asia-east1 and us-east1 regions. If the VPC has three Compute Engine VMs attached to it, it means they're neighbors on the same subnet even though they're in different zones. This capability can be used to build solutions that are resilient to disruptions yet retain a simple network layout.

### Video - [Compute Engine](https://www.cloudskillsboost.google/course_templates/60/video/521837)

- [YouTube: Compute Engine](https://www.youtube.com/watch?v=Oxwz5HbYUF8)

Earlier in the course, we explored infrastructure as a service, or IaaS. Now let's explore Google Cloud's IaaS solution: Compute Engine. With Compute Engine, users can create and run virtual machines on Google infrastructure. There are no upfront investments, and thousands of virtual CPUs can run on a system that's designed to be fast and to offer consistent performance. Each virtual machine contains the power and functionality of a full-fledged operating system. This means a virtual machine can be configured much like a physical server: by specifying the amount of CPU power and memory needed, the amount and type of storage needed, and the operating system. A virtual machine instance can be created via the Google Cloud console, which is a web-based tool to manage Google Cloud projects and resources, the Google Cloud CLI, or the Compute Engine API. The instance can run Linux and Windows Server images provided by Google or any customized versions of these images. You can also build and run images of other operating systems and flexibly reconfigure virtual machines. A quick way to get started with Google Cloud is through the Cloud Marketplace, which offers solutions from both Google and third-party vendors. With these solutions, there's no need to manually configure the software, virtual machine instances, storage, or network settings, although many of them can be modified before launch if that's required. Most software packages in Cloud Marketplace are available at no additional charge beyond the normal usage fees for Google Cloud resources. Some Cloud Marketplace images charge usage fees, particularly those published by third parties, with commercially licensed software, but they all show estimates of their monthly charges before they're launched. At this point, you might be wondering about Compute Engine's pricing and billing structure. For the use of virtual machines, Compute Engine bills by the second with a one-minute minimum, and sustained-use discounts start to apply automatically to virtual machines the longer they run. So, for each VM that runs for more than 25% of a month, Compute Engine automatically applies a discount for every additional minute. Compute Engine also offers committed-use discounts. This means that for stable and predictable workloads, a specific amount of vCPUs and memory can be purchased for up to a 57% discount off of normal prices in return for committing to a usage term of one year or three years. And then there are Preemptible and Spot VMs. Let's say you have a workload that doesn't require a human to sit and wait for it to finish–such as a batch job analyzing a large dataset. You can save money, in some cases up to 90%, by choosing Preemptible or Spot VMs to run the job. A Preemptible or Spot VM is different from an ordinary Compute Engine VM in only one respect: Compute Engine has permission to terminate a job if its resources are needed elsewhere. Although savings are possible with preemptible or spot VMs, you'll need to ensure that your job can be stopped and restarted. Spot VMs differ from Preemptible VMs by offering more features. For example, preemptible VMs can only run for up to 24 hours at a time, but Spot VMs do not have a maximum runtime. However, the pricing is, currently the same for both. In terms of storage, Compute Engine doesn't require a particular option or machine type to get high throughput between processing and persistent disks. That's the default, and it comes to you at no extra cost. And finally, you'll only pay for what you need with custom machine types. Compute Engine lets you choose the machine properties of your instances, like the number of virtual CPUs and the amount of memory, by using a set of predefined machine types or by creating your own custom machine types.

### Video - [Scaling virtual machines](https://www.cloudskillsboost.google/course_templates/60/video/521838)

- [YouTube: Scaling virtual machines](https://www.youtube.com/watch?v=YQK8u563me4)

As we've just seen, with Compute Engine, you can choose the most appropriate machine properties for your instances, like the number of virtual CPUs and the amount of memory, by using a set of predefined machine types, or by creating custom machine types. To do this, Compute Engine has a feature called Autoscaling, where VMs can be added to or subtracted from an application based on load metrics. The other part of making that work is balancing the incoming traffic among the VMs. Google's Virtual Private Cloud (VPC) supports several different kinds of load balancing, which we'll explore shortly. With Compute Engine, you can in fact configure very large VMs, which are great for workloads such as in-memory databases and CPU-intensive analytics, but most Google Cloud customers start off with scaling out, not up. The maximum number of CPUs per VM is tied to its "machine family" and is also constrained by the quota available to the user, which is zone-dependent. Specifications for currently available VM machine types can be found at cloud.google.com/compute/docs/machine-types

### Video - [Important VPC compatibilities](https://www.cloudskillsboost.google/course_templates/60/video/521839)

- [YouTube: Important VPC compatibilities](https://www.youtube.com/watch?v=UtNlJbm8s2Q)

Now let's explore some of the most important Virtual Private Cloud compatibility features. Much like physical networks, VPCs have routing tables. VPC routing tables are built-in so you don't have to provision or manage a router. They're used to forward traffic from one instance to another within the same network, across subnetworks, or even between Google Cloud zones, without requiring an external IP address. Another thing you don't have to provision or manage for Google Cloud is a firewall. VPCs provide a global distributed firewall, which can be controlled to restrict access to instances through both incoming and outgoing traffic. Firewall rules can be defined through network tags on Compute Engine instances, which is really convenient. For example, you can tag all your web servers with, say, "WEB," and write a firewall rule saying that traffic on ports 80 or 443 is allowed into all VMs with the "WEB" tag, no matter what their IP address happens to be. You'll remember that VPCs belong to Google Cloud projects, but what if your company has several Google Cloud projects, and the VPCs need to talk to each other? With VPC Peering, a relationship between two VPCs can be established to exchange traffic. Alternatively, to use the full power of Identity Access Management (IAM) to control who and what in one project can interact with a VPC in another, you can configu

### Video - [Cloud Load Balancing](https://www.cloudskillsboost.google/course_templates/60/video/521840)

- [YouTube: Cloud Load Balancing](https://www.youtube.com/watch?v=HWJQ3LNagXc)

Previously, we explored how virtual machines can autoscale to respond to changing loads. But how do your customers get to your application when it might be provided by four VMs one moment, and by 40 VMs at another? That's done through Cloud Load Balancing. The job of a load balancer is to distribute user traffic across multiple instances of an application. By spreading the load, load balancing reduces the risk that applications experience performance issues. Cloud Load Balancing is a fully distributed, software-defined, managed service for all your traffic. And because the load balancers don't run in VMs that you have to manage, you don't have to worry about scaling or managing them. You can put Cloud Load Balancing in front of all of your traffic: HTTP or HTTPS, other TCP and SSL traffic, and UDP traffic too. Cloud Load Balancing provides cross-region load balancing, including automatic multi-region failover, which gently moves traffic in fractions if backends become unhealthy. Cloud Load Balancing reacts quickly to changes in users, traffic, network, backend health, and other related conditions. And what if you anticipate a huge spike in demand? Say, your online game is already a hit; do you need to file a support ticket to warn Google of the incoming load? No. No so-called "pre-warming" is required. Google Cloud offers a range of load balancing solutions that can be classified based on the OSI model layer they operate at and their specific functionalities. Application Load Balancers operate at the application layer and are designed to handle HTTP and HTTPS traffic, making them ideal for web applications and services that require advanced features like content-based routing and SSL/TLS termination. Application Load Balancers operate as reverse proxies, distributing incoming traffic across multiple backend instances based on rules you define. They are highly flexible and can be configured for both internet-facing (external) and internal applications. Network Load Balancers operate at the transport layer and efficiently handle TCP, UDP, and other IP protocols. They can be further classified into two types: Proxy Network Load Balancers also function as reverse proxies, terminating client connections and establishing new ones to backend services. They offer advanced traffic management capabilities and support backends located both on-premises and in various cloud environments. Unlike proxy Network Load Balancers, passthrough Network Load Balancers do not modify or terminate connections. Instead, they directly forward traffic to the backend while preserving the original source IP address. This type is well-suited for applications that require direct server return or need to handle a wider range of IP protocols.

### Video - [Cloud DNS and Cloud CDN](https://www.cloudskillsboost.google/course_templates/60/video/521841)

- [YouTube: Cloud DNS and Cloud CDN](https://www.youtube.com/watch?v=TYB1cur47mk)

One of the most famous free Google services is 8.8.8.8, which provides a public Domain Name Service to the world. DNS is what translates internet hostnames to addresses, and as you might imagine, Google has a highly developed DNS infrastructure. It makes 8.8.8.8 available so that everyone can take advantage of it. But what about the internet hostnames and addresses of applications built in Google Cloud? Google Cloud offers Cloud DNS to help the world find them. It's a managed DNS service that runs on the same infrastructure as Google. It has low latency and high availability, and it's a cost-effective way to make your applications and services available to your users. The DNS information you publish is served from redundant locations around the world. Cloud DNS is also programmable. You can publish and manage millions of DNS zones and records using the Cloud console, the command-line interface, or the API. Google also has a global system of edge caches. Edge caching refers to the use of caching servers to store content closer to end users. You can use this system to accelerate content delivery in your application by using Cloud CDN - Content Delivery Network. This means your customers will experience lower network latency, the origins of your content will experience reduced load, and you can even save money. After an Application Load Balancer is set up, Cloud CDN can be enabled with a single checkbox. There are many other CDNs available out there, of course. If you are already using one, chances are, it's a part of Google Cloud's CDN Interconnect partner program, and you can continue to use it.

### Video - [Connecting networks to Google VPC](https://www.cloudskillsboost.google/course_templates/60/video/521842)

- [YouTube: Connecting networks to Google VPC](https://www.youtube.com/watch?v=uTYwgmOEbWA)

Many Google Cloud customers want to connect their Google Virtual Private Cloud networks to other networks in their system, such as on-premises networks or networks in other clouds. There are several effective ways to accomplish this. One option is to start with a Virtual Private Network connection over the internet and use Cloud VPN to create a "tunnel" connection. To make the connection dynamic, a Google Cloud feature called Cloud Router can be used. Cloud Router lets other networks and Google VPC, exchange route information over the VPN using the Border Gateway Protocol. Using this method, if you add a new subnet to your Google VPC, your on-premises network will automatically get routes to it. But using the internet to connect networks isn't always the best option for everyone, either because of security concerns or because of bandwidth reliability. So, a second option is to consider "peering" with Google using Direct Peering. Peering means putting a router in the same public data center as a Google point of presence and using it to exchange traffic between networks. Google has more than 100 points of presence around the world. Customers who aren't already in a point of presence can work with a partner in the Carrier Peering program to get connected. Carrier peering gives you direct access from your on-premises network through a service provider's network to Google Workspace and to Google Cloud products that can be exposed through one or more public IP addresses. One downside of peering, though, is that it isn't covered by a Google Service Level Agreement. If getting the highest uptimes for interconnection is important, using Dedicated Interconnect would be a good solution. This option allows for one or more direct, private connections to Google. If these connections have topologies that meet Google's specifications, they can be covered by an SLA of up to 99.99%. Also, these connections can be backed up by a VPN for even greater reliability. Another option we'll explore is Partner Interconnect, which provides connectivity between an on-premises network and a VPC network through a supported service provider. A Partner Interconnect connection is useful if a data center is in a physical location that can't reach a Dedicated Interconnect colocation facility, or if the data needs don't warrant an entire 10 GigaBytes per second connection. Depending on availability needs, Partner Interconnect can be configured to support mission-critical services or applications that can tolerate some downtime. As with Dedicated Interconnect, if these connections have topologies that meet Google's specifications, they can be covered by an SLA of up to 99.99%, but note that Google isn't responsible for any aspects of Partner Interconnect provided by the third-party service provider, nor any issues outside of Google's network. And the final option is Cross-Cloud Interconnect. Cross-Cloud Interconnect helps you establish high-bandwidth dedicated connectivity between Google Cloud and another cloud service provider. Google provisions a dedicated physical connection between the Google network and that of another cloud service provider. You can use this connection to peer your Google Virtual Private Cloud network with your network that's hosted by a supported cloud service provider. Cross-Cloud Interconnect supports your adoption of an integrated multicloud strategy. In addition to supporting various cloud service providers, Cross-Cloud Interconnect offers reduced complexity, site-to-site data transfer, and encryption. Cross-Cloud Interconnect connections are available in two sizes: 10 Gbps or 100 Gbps.

### Lab - [Getting Started with VPC Networking and Google Compute Engine](https://www.cloudskillsboost.google/course_templates/60/labs/521843)

In this lab, you create an auto-mode VPC network with firewall rules and 2 VM instances. Then, you explore the connectivity for the VM instances.

- [ ] [Getting Started with VPC Networking and Google Compute Engine](../labs/Getting-Started-with-VPC-Networking-and-Google-Compute-Engine.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/60/quizzes/521844)

#### Quiz 1.

> [!important]
> **For which of these interconnect options is a Service Level Agreement available?**
>
> - [ ] Carrier Peering
> - [ ] Standard Network Tier
> - [ ] Direct Peering
> - [ ] Dedicated Interconnect

#### Quiz 2.

> [!important]
> **What is the main reason customers choose Preemptible VMs?**
>
> - [ ] To use custom machine types.
> - [ ] To reduce cost.
> - [ ] To improve performance.
> - [ ] To reduce cost on premium operating systems.

#### Quiz 3.

> [!important]
> **In Google Cloud VPCs, what scope do subnets have?**
>
> - [ ] Multi-regional
> - [ ] Regional
> - [ ] Zonal
> - [ ] Global

#### Quiz 4.

> [!important]
> **How does Cloud Load Balancing allow you to balance HTTP-based traffic?**
>
> - [ ] Across multiple Compute Engine regions.
> - [ ] Across multiple Google Cloud Platform services.
> - [ ] Across multiple physical machines in a single data center.
> - [ ] Across multiple virtual machine instances in a single Compute Engine region.

## Storage in the Cloud

This section of the course showcases five core Google Cloud storage products: Cloud Storage, Bigtable, Cloud SQL, Spanner, and Firestore.

### Video - [Google Cloud storage options](https://www.cloudskillsboost.google/course_templates/60/video/521845)

- [YouTube: Google Cloud storage options](https://www.youtube.com/watch?v=8Dn-RAP7fzA)

Every application needs to store data, like media to be streamed or perhaps even sensor data from devices, and different applications and workloads require different storage database solutions. Google Cloud has storage options for structured, unstructured, transactional, and relational data. In this section of the course, we'll explore Google Cloud's five core storage products: Cloud Storage, Cloud SQL, Spanner, Firestore, and Bigtable. Depending on your application, you might use one or several of these services to do the job.

### Video - [Cloud Storage](https://www.cloudskillsboost.google/course_templates/60/video/521846)

- [YouTube: Cloud Storage](https://www.youtube.com/watch?v=pLMkOa4FbwI)

Let's begin with Cloud Storage, which is a service that offers developers and IT organizations durable and highly available object storage. But what is object storage? Object storage is a computer data storage architecture that manages data as "objects" and not as a file and folder hierarchy (file storage), or as chunks of a disk (block storage). These objects are stored in a packaged format which contains the binary form of the actual data itself, as well as relevant associated meta-data (such as date created, author, resource type, and permissions), and a globally unique identifier. These unique keys are in the form of URLs, which means object storage interacts well with web technologies. Data commonly stored as objects include video, pictures, and audio recordings. Cloud Storage is Google's object storage product. It allows customers to store any amount of data, and to retrieve it as often as needed. It's a fully managed scalable service that has a wide variety of uses. A few examples include serving website content, storing data for archival and disaster recovery, and distributing large data objects to end users via Direct Download. Cloud Storage's primary use is whenever binary large-object storage (also known as a "BLOB") is needed for online content such as videos and photos, for backup and archived data and for storage of intermediate results in processing workflows. Cloud Storage files are organized into buckets. A bucket needs a globally unique name and a specific geographic location for where it should be stored, and an ideal location for a bucket is where latency is minimized. For example, if most of your users are in Europe, you probably want to pick a European location, so either a specific Google Cloud region in Europe, or else the EU multi-region. The storage objects offered by Cloud Storage are immutable, which means that you do not edit them, but instead a new version is created with every change made. Administrators have the option to either allow each new version to completely overwrite the older one, or to keep track of each change made to a particular object by enabling "versioning" within a bucket. If you choose to use versioning, Cloud Storage will keep a detailed history of modifications -- that is, overwrites or deletes -- of all objects contained in that bucket. If you don't turn on object versioning, by default new versions will always overwrite older versions. With object versioning enabled, you can list the archived versions of an object, restore an object to an older state, or permanently delete a version of an object, as needed. In many cases, personally identifiable information may be contained in data objects, so controlling access to stored data is essential to ensuring security and privacy are maintained. Using IAM roles and, where needed, access control lists (ACLs), organizations can conform to security best practices, which require each user to have access and permissions to only the resources they need to do their jobs, and no more than that. There are a couple of options to control user access to objects and buckets. For most purposes, IAM is sufficient. Roles are inherited from project to bucket to object. If you need finer control, you can create access control lists. Each access control list consists of two pieces of information. The first is a scope, which defines who can access and perform an action. This can be a specific user or group of users. The second is a permission, which defines what actions can be performed, like read or write. Because storing and retrieving large amounts of object data can quickly become expensive, Cloud Storage also offers lifecycle management policies. For example, you could tell Cloud Storage to delete objects older than 365 days; or to delete objects created before January 1, 2013; or to keep only the 3 most recent versions of each object in a bucket that has versioning enabled. Having this control ensures that you're not paying for more than you actually need.

### Video - [Cloud Storage: Storage classes and data transfer](https://www.cloudskillsboost.google/course_templates/60/video/521847)

- [YouTube: Cloud Storage: Storage classes and data transfer](https://www.youtube.com/watch?v=A5a8sPL6PjY)

There are four primary storage classes in Cloud Storage. The first is Standard Storage. Standard Storage is considered best for frequently accessed, or "hot," data. It's also great for data that's stored for only brief periods of time. The second storage class is Nearline Storage. This is best for storing infrequently accessed data, like reading or modifying data on average once a month or less. Examples might include data backups, long-tail multimedia content, or data archiving. The third storage class is Coldline Storage. This is also a low-cost option for storing infrequently accessed data. However, as compared to Nearline Storage, Coldline Storage is meant for reading or modifying data, at most, once every 90 days. And the fourth storage class is Archive Storage. This is the lowest-cost option, used ideally for data archiving, online backup, and disaster recovery. It's the best choice for data that you plan to access less than once a year, because it has higher costs for data access and operations and a 365-day minimum storage duration. Although each of these four classes has differences, it's worth noting there are several characteristics that apply across all of these storage classes. These include: Unlimited storage with no minimum object size requirement, worldwide accessibility and locations, low latency and high durability, a uniform experience, which extends to security, tools, and APIs, and geo-redundancy if data is stored in a multi-region or dual-region. This means placing physical servers in geographically diverse data centers to protect against catastrophic events and natural disasters, and load-balancing traffic for optimal performance. Cloud Storage also provides a feature called Autoclass, which automatically transitions objects to appropriate storage classes based on each object's access pattern. The feature moves data that is not accessed to colder storage classes to reduce storage cost and moves data that is accessed to Standard storage to optimize future accesses. Autoclass simplifies and automates cost saving for your Cloud Storage data. Cloud Storage has no minimum fee because you pay only for what you use, and prior provisioning of capacity isn't necessary. And from a security perspective, Cloud Storage always encrypts data on the server side, before it's written to disk, at no additional charge. Data traveling between a customer's device and Google is encrypted by default using HTTPS/TLS, which is Transport Layer Security. Regardless of which storage class you choose, there are several ways to bring data into Cloud Storage. Many customers simply carry out their own online transfer using gcloud storage, which is the Cloud Storage command from the Cloud SDK. Data can also be moved in by using a drag and drop option in the Cloud Console, if accessed through the Google Chrome web browser. But what if you have to upload terabytes or even petabytes of data? Storage Transfer Service enables you to import large amounts of online data into Cloud Storage quickly and cost-effectively. The Storage Transfer Service lets you schedule and manage batch transfers to Cloud Storage from another cloud provider, from a different Cloud Storage region, or from an HTTP(S) endpoint. And then there is the Transfer Appliance, which is a rackable, high-capacity storage server that you lease from Google Cloud. You connect it to your network, load it with data, and then ship it to an upload facility where the data is uploaded to Cloud Storage. You can transfer up to a petabyte of data on a single appliance. Cloud Storage's tight integration with other Google Cloud products and services means that there are many additional ways to move data into the service. For example, you can import and export tables to and from both BigQuery and Cloud SQL. You can also store App Engine logs, Firestore backups, and objects used by App Engine applications, like images. Cloud Storage can also store instance startup scripts, Compute Engine images, and objects used by Compute Engine applications.

### Video - [Cloud SQL](https://www.cloudskillsboost.google/course_templates/60/video/521848)

- [YouTube: Cloud SQL](https://www.youtube.com/watch?v=mtIATfMV6PI)

Google Cloud's second core storage option is Cloud SQL. Cloud SQL offers fully managed relational databases, including MySQL, PostgreSQL, and SQL Server as a service. It's designed to hand off mundane, but necessary and often time-consuming, tasks to Google—like applying patches and updates managing backups, and configuring replications—so your focus can be on building great applications. Cloud SQL doesn't require any software installation or maintenance. It can scale up to 128 processor cores, 864 GB of RAM, and 64 TB of storage. It supports automatic replication scenarios, such as from a Cloud SQL primary instance, an external primary instance, and external MySQL instances. Cloud SQL supports managed backups, so backed-up data is securely stored and accessible if a restore is required. The cost of an instance covers seven backups. Cloud SQL encrypts customer data when on Google's internal networks and when stored in database tables, temporary files, and backups. And it includes a network firewall, which controls network access to each database instance. A benefit of Cloud SQL instances is that they are accessible by other Google Cloud services, and even external services. Cloud SQL can be used with App Engine using standard drivers like Connector/J for Java or MySQLdb for Python. Compute Engine instances can be authorized to access Cloud SQL instances and configure the Cloud SQL instance to be in the same zone as your virtual machine. Cloud SQL also supports other applications and tools that you might use, like SQL Workbench, Toad, and other external applications using standard MySQL drivers.

### Video - [Spanner](https://www.cloudskillsboost.google/course_templates/60/video/521849)

- [YouTube: Spanner](https://www.youtube.com/watch?v=fUrnmk4f77U)

The third core storage option offered by Google Cloud is Spanner. Spanner is a fully managed relational database service that scales horizontally, is strongly consistent, and speaks SQL. Battle tested by Google's own mission-critical applications and services, Spanner is the service that powers Google's $80 billion business. Spanner is especially suited for applications that require a SQL relational database management system with joins and secondary indexes, built-in high availability, strong global consistency, and high numbers of input and output operations per second. We're talking tens of thousands of reads and writes per second or more.

### Video - [Firestore](https://www.cloudskillsboost.google/course_templates/60/video/521850)

- [YouTube: Firestore](https://www.youtube.com/watch?v=Q1kSFl1LYMc)

Google Cloud's fourth core storage option is Firestore. Firestore is a flexible, horizontally scalable, NoSQL cloud database for mobile, web, and server development. With Firestore, data is stored in documents and then organized into collections. Documents can contain complex nested objects in addition to subcollections. Each document contains a set of key-value pairs. For example, a document to represent a user has the keys for the firstname and lastname with the associated values. Firestore's NoSQL queries can then be used to retrieve individual, specific documents or to retrieve all the documents in a collection that match your query parameters. Queries can include multiple, chained filters and combine filtering and sorting options. They're also indexed by default, so query performance is proportional to the size of the result set, not the dataset. Firestore uses data synchronization to update data on any connected device. However, it's also designed to make simple, one-time fetch queries efficiently. It caches data that an app is actively using, so the app can write, read, listen to, and query data even if the device is offline. When the device comes back online, Firestore synchronizes any local changes back to Firestore. Firestore leverages Google Cloud's powerful infrastructure: automatic multi-region data replication, strong consistency guarantees, atomic batch operations, and real transaction support.

### Video - [Bigtable](https://www.cloudskillsboost.google/course_templates/60/video/521851)

- [YouTube: Bigtable](https://www.youtube.com/watch?v=bfztO0_4fhw)

The last of Google Cloud's core storage options we're going to explore is Bigtable. Bigtable is Google's NoSQL big data database service. It's the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail. Bigtable is designed to handle massive workloads at consistent low latency and high throughput, so it's a great choice for both operational and analytical applications, including Internet of Things, user analytics, and financial data analysis. When deciding which storage option is best, customers often choose Bigtable if: They're working with more than 1TB of semi-structured or structured data. Data is fast with high throughput, or it's rapidly changing. They're working with NoSQL data. This usually means transactions where strong relational semantics are not required. Data is a time-series or has natural semantic ordering. They're working with big data, running asynchronous batch or synchronous real-time processing on the data. Or they're running machine learning algorithms on the data. Bigtable can interact with other Google Cloud services and third-party clients. Using APIs, data can be read from and written to Bigtable through a data service layer like Managed VMs, the HBase REST Server, or a Java Server using the HBase client. Typically this is used to serve data to applications, dashboards, and data services. Data can also be streamed in through a variety of popular stream processing frameworks like Dataflow Streaming, Spark Streaming, and Storm. And if streaming is not an option, data can also be read from and written to Bigtable through batch processes like Hadoop MapReduce, Dataflow, or Spark. Often, summarized or newly calculated data is written back to Bigtable or to a downstream database.

### Video - [Comparing storage options](https://www.cloudskillsboost.google/course_templates/60/video/521852)

- [YouTube: Comparing storage options](https://www.youtube.com/watch?v=jAPJ_aVxCbQ)

Now that we've covered Google Cloud's core storage options, let's do a comparison to help highlight the most suitable service for a specific application or workflow. Consider using Cloud Storage if you need to store immutable blobs larger than 10 megabytes, such as large images or movies. This storage service provides petabytes of capacity with a maximum unit size of 5 terabytes per object. Consider using Cloud SQL or Spanner if you need full SQL support for an online transaction processing system. Cloud SQL provides up to 64 terabytes, depending on machine type, and Spanner provides petabytes. Cloud SQL is best for web frameworks and existing applications, like storing user credentials and customer orders. If Cloud SQL doesn't fit your requirements because you need horizontal scalability, not just through read replicas, consider using Spanner. Consider Firestore if you need massive scaling and predictability together with real time query results and offline query support. This storage service provides terabytes of capacity with a maximum unit size of 1 megabyte per entity. Firestore is best for storing, syncing, and querying data for mobile and web apps. Finally, consider using Bigtable if you need to store a large number of structured objects. Bigtable doesn't support SQL queries, nor does it support multi-row transactions. This storage service provides petabytes of capacity with a maximum unit size of 10 megabytes per cell and 100 megabytes per row. Bigtable is best for analytical data with heavy read and write events, like AdTech, financial, or IoT data. Depending on your application, it's possible that you might use one, or several, of these services to do the job. You may have noticed that BigQuery hasn't been mentioned in this section of the course. This is because it sits on the edge between data storage and data processing, and is covered in more depth in other courses. The usual reason to store data in BigQuery is so you can use its big data analysis and interactive querying capabilities, but it's not purely a data storage product.

### Lab - [Google Cloud Fundamentals: Getting Started with Cloud Storage and Cloud SQL](https://www.cloudskillsboost.google/course_templates/60/labs/521853)

In this lab, you create a Cloud Storage bucket and place an image in it. You’ll also configure an application running in Compute Engine to use a database managed by Cloud SQL.

- [ ] [Google Cloud Fundamentals: Getting Started with Cloud Storage and Cloud SQL](../labs/Google-Cloud-Fundamentals-Getting-Started-with-Cloud-Storage-and-Cloud-SQL.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/60/quizzes/521854)

#### Quiz 1.

> [!important]
> **Why would a customer consider the Coldline storage class?**
>
> - [ ] To save money on storing frequently accessed data.
> - [ ] To save money on storing infrequently accessed data.
> - [ ] To use the Coldline Storage API.
> - [ ] To improve security.

#### Quiz 2.

> [!important]
> **What is the correct use case for Cloud Storage?**
>
> - [ ] Cloud Storage is well suited to providing RDBMS services.
> - [ ] Cloud Storage is well suited to providing durable and highly available object storage.
> - [ ] Cloud Storage is well suited to providing the root file system of a Linux virtual machine.
> - [ ] Cloud Storage is well suited to providing data warehousing services.

#### Quiz 3.

> [!important]
> **Which relational database service can scale to higher database sizes?**
>
> - [ ] Cloud SQL
> - [ ] Spanner
> - [ ] Bigtable
> - [ ] Firestore

## Containers in the Cloud

This section of the course explores containers and how they can be managed with Kubernetes and Google Kubernetes Engine.

### Video - [Introduction to containers](https://www.cloudskillsboost.google/course_templates/60/video/521855)

- [YouTube: Introduction to containers](https://www.youtube.com/watch?v=G7lpck7t9Vw)

In this section of the course we'll explore containers and help you understand how they are used. Infrastructure as a service, or IaaS, allows you to share compute resources with other developers by using virtual machines to virtualize the hardware. This lets each developer deploy their own operating system (OS), access the hardware, and build their applications in a self-contained environment with access to RAM, file systems, networking interfaces, etc. This is where containers come in. The idea of a container is to give the independent scalability of workloads in PaaS and an abstraction layer of the OS and hardware in IaaS. A configurable system lets you install your favorite runtime, web server, database, or middleware, configure the underlying system resources, such as disk space, disk I/O, or networking, and build as you like. But flexibility comes with a cost. The smallest unit of compute is an app with its VM. The guest OS might be large, even gigabytes in size, and take minutes to boot. As demand for your application increases, you have to copy an entire VM and boot the guest OS for each instance of your app, which can be slow and costly. A container is an invisible box around your code and its dependencies with limited access to its own partition of the file system and hardware. It only requires a few system calls to create and it starts as quickly as a process. All that's needed on each host is an OS kernel that supports containers and a container runtime. In essence, the OS is being virtualized. It scales like PaaS but gives you nearly the same flexibility as IaaS. This makes code ultra portable, and the OS and hardware can be treated as a black box. So you can go from development, to staging, to production, or from your laptop to the cloud, without changing or rebuilding anything. As an example, let's say you want to scale a web server. With a container, you can do this in seconds and deploy dozens or hundreds of them, depending on the size of your workload, on a single host. That's just a simple example of scaling one container running the whole application on a single host. However, you'll probably want to build your applications using lots of containers, each performing their own function like microservices. If you build them this way and connect them with network connections, you can make them modular, deploy easily, and scale independently across a group of hosts. The hosts can scale up and down and start and stop containers as demand for your app changes or as hosts fail.

### Video - [Kubernetes](https://www.cloudskillsboost.google/course_templates/60/video/521856)

- [YouTube: Kubernetes](https://www.youtube.com/watch?v=4NTyB0eEB_A)

A product that helps manage and scale containerized applications is Kubernetes. So to save time and effort when scaling applications and workloads, Kubernetes can be bootstrapped using Google Kubernetes Engine (GKE). So, what is Kubernetes? Kubernetes is an open-source platform for managing containerized workloads and services. It makes it easy to orchestrate many containers on many hosts, scale them as microservices, and easily deploy rollouts and rollbacks. At the highest level, Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called a cluster. The system is divided into a set of primary components that run as the control plane and a set of nodes that run containers. In Kubernetes, a node represents a computing instance, like a machine. Note that this is different to a node on Google Cloud which is a virtual machine running in Compute Engine. You can describe a set of applications and how they should interact with each other, and Kubernetes determines how to make that happen. Deploying containers on nodes by using a wrapper around one or more containers is what defines a Pod. A Pod is the smallest unit in Kubernetes that you can create or deploy. It represents a running process on your cluster as either a component of your application or an entire app. Generally, you only have one container per Pod, but if you have multiple containers with a hard dependency, you can package them into a single Pod and share networking and storage resources between them. The Pod provides a unique network IP and set of ports for your containers and configurable options that govern how your containers should run. One way to run a container in a Pod in Kubernetes is to use the kubectl run command, which starts a Deployment with a container running inside a Pod. A Deployment represents a group of replicas of the same Pod and keeps your Pods running even when the nodes they run on fail. A Deployment could represent a component of an application or even an entire app. To see a list of the running Pods in your project, run the command: $ kubectl get pods. Kubernetes creates a Service with a fixed IP address for your Pods, and a controller says "I need to attach an external load balancer with a public IP address to that Service so others outside the cluster can access it." In GKE, the load balancer is created as a network load balancer. Any client that reaches that IP address will be routed to a Pod behind the Service. A Service is an abstraction which defines a logical set of Pods and a policy by which to access them. As Deployments create and destroy Pods, Pods will be assigned their own IP addresses, but those addresses don't remain stable over time. A Service group is a set of Pods and provides a stable endpoint (or fixed IP address) for them. For example, if you create two sets of Pods called frontend and backend and put them behind their own Services, the backend Pods might change, but frontend Pods are not aware of this. They simply refer to the backend Service. To scale a Deployment, run the kubectl scale command. In this example, three Pods are created in your Deployment, and they're placed behind the Service and share one fixed IP address. You could also use autoscaling with other kinds of parameters. For example, you can specify that the number of Pods should increase when CPU utilization reaches a certain limit. So far, we've seen how to run imperative commands like expose and scale. This works well to learn and test Kubernetes step-by-step. But the real strength of Kubernetes comes when you work in a declarative way. Instead of issuing commands, you provide a configuration file that tells Kubernetes what you want your desired state to look like, and Kubernetes determines how to do it. You accomplish this by using a Deployment config file. You can check your Deployment to make sure the proper number of replicas is running by using either kubectl get deployments or kubectl describe deployments. To run five replicas instead of three, all you do is update the Deployment config file and run the kubectl apply command to use the updated config file. You can still reach your endpoint as before by using kubectl get services to get the external IP of the Service and reach the public IP address from a client. The last question is, what happens when you want to update a new version of your app? Well, you want to update your container to get new code in front of users, but rolling out all those changes at one time would be risky. So in this case, you would use kubectl rollout or change your deployment configuration file and then apply the change using kubectl apply. New Pods will then be created according to your new update strategy. Here's an example configuration that will create new version Pods individually and wait for a new Pod to be available before destroying one of the old Pods.

### Video - [Google Kubernetes Engine](https://www.cloudskillsboost.google/course_templates/60/video/521857)

- [YouTube: Google Kubernetes Engine](https://www.youtube.com/watch?v=_-TPCaJYwR0)

So now that we have a basic understanding of containers and Kubernetes, let's talk about Google Kubernetes Engine, or GKE. GKE is a Google-hosted managed Kubernetes service in the cloud. The GKE environment consists of multiple machines, specifically Compute Engine instances, grouped together to form a cluster. You can create a Kubernetes cluster with Kubernetes Engine, but how is GKE different from Kubernetes? From the user's perspective, it's a lot simpler. GKE manages all the control plane components for us. It still exposes an IP address to which we send all of our Kubernetes API requests, but GKE takes responsibility for provisioning and managing all the control plane infrastructure behind it. It also eliminates the need of a separate control plane. Node configuration and management depends on the type of GKE mode you use. With the Autopilot mode, which is recommended, GKE manages the underlying infrastructure such as node configuration, autoscaling, auto-upgrades, baseline security configurations, and baseline networking configuration. With the Standard mode, you manage the underlying infrastructure, including configuring the individual nodes. Let's examine the benefits and functionality of Autopilot in more detail. Autopilot is optimized for production. Autopilot also helps produce a strong security posture. And Autopilot also promotes operational efficiency. The GKE Standard mode has the same functionality as Autopilot, but you're responsible for the configuration, management, and optimization of the cluster. Unless you require the specific level of configuration control offered by GKE standard, it's recommended that you use Autopilot mode. You can create a Kubernetes cluster with Kubernetes Engine by using the Google Cloud console or the gcloud command that's provided by the Cloud software development kit. GKE clusters can be customized, and they support different machine types, number of nodes, and network settings. Kubernetes provides the mechanisms through which you interact with your cluster. Kubernetes commands and resources are used to deploy and manage applications, perform administration tasks, set policies, and monitor the health of deployed workloads. Running a GKE cluster comes with the benefit of advanced cluster management features that Google Cloud provides. These include: Google Cloud's load-balancing for Compute Engine instances, Node pools to designate subsets of nodes within a cluster for additional flexibility, Automatic scaling of your cluster's node instance count, Automatic upgrades for your cluster's node software, Node auto-repair to maintain node health and availability, And logging and monitoring with Google Cloud Observability for visibility into your cluster. To start up Kubernetes on a cluster in GKE, all you do is run this command: $> gcloud container clusters create k1

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/60/quizzes/521858)

#### Quiz 1.

> [!important]
> **Where do the resources used to build Google Kubernetes Engine clusters come from?**
>
> - [ ] App Engine
> - [ ] Cloud Storage
> - [ ] Bare-metal servers
> - [ ] Compute Engine

#### Quiz 2.

> [!important]
> **What is a Kubernetes pod?**
>
> - [ ] A group of clusters
> - [ ] A group of containers
> - [ ] A group of VMs
> - [ ] A group of nodes

## Applications in the Cloud

The focus of this section of the course is developing applications in the cloud. It's here that we'll explore Cloud Run and Cloud Functions.

### Video - [Cloud Run](https://www.cloudskillsboost.google/course_templates/60/video/521859)

- [YouTube: Cloud Run](https://www.youtube.com/watch?v=R3Ec_4tC3oc)

So far in this course, we've provided an introduction to Google Cloud and explored the options and benefits related to using virtual machines, networks, storage, and containers in the Cloud. In the final section of the course, we'll turn our attention to developing applications in the Cloud. We'll begin with Cloud Run, which is a managed compute platform that runs stateless containers via web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It's built on Knative, an open API and runtime environment built on Kubernetes. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges only for the resources used, calculated down to the nearest 100 milliseconds, so you'll never pay for over-provisioned resources. The Cloud Run developer workflow is a straightforward three-step process. First, you write your application using your favorite programming language. This application should start a server that listens for web requests. Second, you build and package your application into a container image. And third, the container image is pushed to Artifact Registry, where Cloud Run will deploy it. Once you've deployed your container image, you'll get a unique HTTPS URL back. Cloud Run then starts your container on demand to handle requests, and ensures that all incoming requests are handled by dynamically adding and removing containers. Because Cloud Run is serverless, it means that you, as a developer, can focus on building your application and not on building and maintaining the infrastructure that powers it. For some use cases, a container-based workflow is great, because it gives you a great amount of transparency and flexibility. Sometimes, you're just looking for a way to turn source code into an HTTPS endpoint, and you want your vendor to make sure your container image is secure, well-configured and built in a consistent way. With Cloud Run, you can do both. You can use a container-based workflow, as well as a source-based workflow. The source-based approach will deploy source code instead of a container image. Cloud Run then builds the source and packages the application into a container image. Cloud Run does this using Buildpacks - an open source project. Cloud Run handles HTTPS serving for you. That means you only have to worry about handling web requests, and you can let Cloud Run take care of adding the encryption. The pricing model on Cloud Run is unique; as you only pay for the system resources you use while a container is handling web requests, with a granularity of 100ms, and when it's starting or shutting down. You don't pay for anything if your container doesn't handle requests. Additionally, there is a small fee for every one million requests you serve. The price of container time increases with CPU and memory. A container with more vCPU and memory is more expensive. You can use Cloud Run to run any binary, as long as it's compiled for Linux sixty-four bit. Now, this means you can use Cloud Run to run web applications written using popular languages, such as: Java, Python, Node.js, PHP, Go, and C++. You can also run code written in less popular languages, such as: Cobol, Haskell, and Perl. As long as your app handles web requests, you're good to go.

### Video - [Development in the cloud](https://www.cloudskillsboost.google/course_templates/60/video/521860)

- [YouTube: Development in the cloud](https://www.youtube.com/watch?v=zoq8Lajo7dE)

Many applications contain event-driven parts. For example, an application that lets users upload images. When that event takes place, the image might need to be processed in a few different ways, like converting the image to a standard format, converting a thumbnail into different sizes, and storing each new file in a repository. This function could be integrated into the application, but then you'd have to provide compute resources for it–whether it happens once a millisecond or once a day. With Cloud Run functions, you write a single-purpose function that completes the necessary image manipulations and then arrange for it to automatically run whenever a new image is uploaded. Cloud Run functions is a lightweight, event-based, asynchronous compute solution that allows you to create small, single-purpose functions that respond to cloud events, without the need to manage a server or a runtime environment. These functions can be used to construct application workflows from individual business logic tasks. Cloud Run functions can also connect and extend cloud services. You're billed to the nearest 100 milliseconds, but only while your code is running. Cloud Run functions supports writing source code in a number of programming languages. These include Node.js, Python, Go, Java, . Net Core, Ruby, and PHP. For more information about the supported specific version, refer to the runtimes documentation. Events from Cloud Storage and Pub/Sub can trigger Cloud Run functions asynchronously, or you can use HTTP invocation for synchronous execution.

### Lab - [Hello Cloud Run](https://www.cloudskillsboost.google/course_templates/60/labs/521861)

In this lab you'll learn how to get started with Cloud Run by deploying and running a stateless container serverless-ly (with the infrastructure abstracted away).

- [ ] [Hello Cloud Run](../labs/Hello-Cloud-Run.md)

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/60/quizzes/521862)

#### Quiz 1.

> [!important]
> **Which scenario is best suited for using Cloud Run instead of Cloud Run functions?**
>
> - [ ] Sending an email notification whenever a new document is added to a specific folder in Cloud Storage.
> - [ ] Hosting a dynamic web application that allows users to upload and share photos.
> - [ ] Generating thumbnails for images uploaded to a Cloud Storage bucket.
> - [ ] Resizing images on demand when requested by a user through a web interface

#### Quiz 2.

> [!important]
> **Which of these statements about Cloud Run functions are correct? Select three.Cloud Run functions:**
>
> - [ ] Require servers or VMs to be provisioned.
> - [ ] Is integrated with Cloud Logging.
> - [ ] Can only be invoked by sending HTTP requests.
> - [ ] Is a scalable functions-as-a-service platform.
> - [ ] Can be used to extend Cloud services.

#### Quiz 3.

> [!important]
> **Why might a Google Cloud customer choose to use Cloud Run functions?**
>
> - [ ] Cloud Run functions is a free service for hosting compute operations.
> - [ ] Their application contains event-driven code that they don't want to provision compute resources for.
> - [ ] Cloud Run functions is the primary way to run C++ applications in Google Cloud.
> - [ ] Their application has a legacy monolithic structure that they want to separate into microservices.

## Prompt Engineering

This section looks at generative AI tools, how they work. We'll explore how to combine Google Cloud knowledge with prompt engineering to improve Gemini responses.

### Video - [Prompt Engineering](https://www.cloudskillsboost.google/course_templates/60/video/521863)

- [YouTube: Prompt Engineering](https://www.youtube.com/watch?v=5zoKVf-cnf4)

Generative AI and large language models are proving to be powerful tools, but to leverage their capabilities, it's important to understand their architecture. It's also important to consider recommended practices when implementing these technologies. The goal of this module, titled Google Cloud: Prompt Engineer Guide, is to help with these important steps. In this guide to prompt engineering, you'll get answers to the questions: What is generative AI? What is a large language model? What is prompt engineering? You'll also explore prompt engineering best practices. Before we delve into this lesson, let's define the interchangeably used terms such as 'generative AI' and 'Large Language Model' (LLM). While both terms describe AI models capable of generating human-like responses based on input prompts in many references, it's important to note they're not identical. Generative AI encompasses a broader range of models capable of generating various types of content beyond just text, while LLM specifically refers to a subset of generative AI models focusing on language tasks. We'll thoroughly explore each term in this lesson. So let's begin with an important question: What is generative AI? Generative artificial intelligence, which is commonly referred to as gen AI, is a subset of artificial intelligence that is capable of creating text, images, or other data using generative models, often in response to prompts. It has grown in popularity hugely since 2021 but artificial intelligence has been around since the mid 1950s. By the way, a prompt is a specific instruction, question, or cue given to a computer program or user to initiate a specific action or response, but we examine this more later. In its current format, gen AI models are like conversational programs that can generate content based on the inputs supplied. Gen AI models learn the patterns and structure from input training data and then create new data with similar characteristics. Generative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, and sales. However, rather than exploring all generative AI applications, this training will specifically focus on articulating prompts to harness the power of gen AI effectively. Let me introduce a scenario that we'll make reference to throughout this section of the training. We've included it to help put some of this theory into practice. Meet Sasha, a cloud architect, who needs to create a prototype design of Google Cloud VPC network architecture for Cymbal Bank. Sasha wants to save time by combining her existing knowledge of cloud architecture and generative AI tools to create a usable prototype design. Sasha was excited to learn about Gemini, since having the tool inside the Google Cloud Console means that she can access it without any additional installs. We'll check back in with Sasha later. Let's spend some time exploring large language models, which are a highly sophisticated computer programs trained on gigantic amounts of data that can be text or images. But how are they trained? And why do they need training at all? Large language models refer to large, general-purpose language models that can be pre-trained and then fine-tuned for specific purposes. In this context, large refers to: The size of the training dataset, which can sometimes be at the petabyte scale. And the number of parameters. Parameters are the memories and knowledge that the machine has learned during model training. They determine the ability of a model to solve a problem, such as predicting text, and can reach billions or even trillions in size. General-purpose means that the models can sufficiently solve common problems. This is thanks to the commonality of a human language, regardless of the specific tasks. Saying LLMs are pre-trained and fine-tuned, means… …that they have been pre-trained for a general purpose with a large dataset… ...and then fine-tuned for specific goals with a much smaller dataset. But how are LLMs trained? When you submit a prompt to an LLM, it calculates the probability of the correct answer from its pre-trained model. The probability is determined through a task called pre-training. Pre-training an LLM involves feeding a massive dataset of text, images, and code to the model so that it can learn the underlying structure and patterns of the language. This process helps the model to understand and generate human language more effectively. In this way, the LLM works like a fancy autocomplete, suggesting the most common correct response to the prompt. But sometimes the LLM gives a completely wrong answer. This is called a hallucination. Hallucinations are words or phrases that are generated by the model that are often nonsensical or grammatically incorrect. This happens because LLMs can only understand the information they were trained on. This means that they might not be aware of your business's proprietary or domain-specific data. Also, they do not have access to real-time information. To make matters worse, LLMs only understand the information that is explicitly given to them in the prompt. In other words, they often assume that the prompt is true. They also do not have the ability to ask for more context information. Ultimately, an LLM does not know anything outside of what it was trained on, and it cannot truly know if that information is accurate. But what causes a hallucination. Hallucinations can be caused by a number of factors, including: The model is not trained on enough data. The model is trained on noisy or dirty data. The model is not given enough context. The model is not given enough constraints. Hallucinations can be a problem for LLMs because they can make the output text difficult to understand. They can also make the model more likely to generate incorrect or misleading information. But we will see in the Prompt Engineering section that there are things we can do to minimize this problem. OK, but knowing where the sun is will not help Sasha with her current task. Lucky for Sasha, Google Cloud offers a generative AI model called Gemini, [[Pause here for 5 seconds]] which can act as an always-on collaborator. This gen AI-powered assistant can help a wide range of Google Cloud users, including developers, data scientists, and operators. To provide an integrated assistance experience, Gemini is embedded in many Google Cloud products. Gemini has access to a massive range of data, including Google Cloud documentation, tutorials, and samples. With the right prompts, it can produce detailed suggestions and guides on what resources will best suit Sasha's current challenge and their configuration. Gemini can even create detailed gcloud commands and insert them into Cloud Shell for her. She just needs to articulate her needs in a way that gets the best response from Gemini. For example, if she uses the prompt "How can I create a network that uses IPv4 and IPv6 addresses?", she will get a response that details how to do just that. You've learned that a large language model is a huge object model containing a massive dataset of text. But how can you extract the information you need from this dataset? This is where prompt engineering comes in. A prompt is the text that you feed to the model, and prompt engineering is a way of articulating your prompts to get the best response from the model. The better structured a prompt is, the better the output from the model will be. Let's explore what this means. Prompts can be in the form of a question, and are categorized into four categories: zero-shot, one-shot, few-shot, and role prompts. Zero-shot prompts do not contain any context or examples to assist the model. For example, the prompt "What's the capital of France?" does not provide any examples of what a capital is. Clearly, that is not too important for this example. But for more specific and technical prompts, an example would help refine the scope of the response from Gemini. One-shot prompts, however, provide one example to the model for context. Here, we ask for the capital of France again, but we provide Italy and Rome as an example. And few-shot prompts provide at least two examples to the model for context. Here, our prompt is updated to also include Japan and Tokyo in our examples. And then, there are role prompts which require a frame of reference for the model to work from as it answers the questions. In our example, we state "I want you to act as a business professor. I'll give you a term, and you will correctly explain its meaning. Make sure your answers are always right. What is ROI? " For Sasha's needs, using role prompts might be the best solution. She can define what is required and in what context. This means that the LLM will have a clear point of reference when supplying an answer. Now that you've seen the types of prompts you can create, let's explore the two elements of a prompt: the preamble and the input. The preamble refers to the introductory text you provide to give the model context and instructions before your main question or request. Think of it as setting the stage for the LLM to better understand what you want. It can include the context for the task, the task itself, and some examples to guide the model. The input is the central request you're making to the LLM. It's what the instruction or task will act upon, for example "Comment: I don't know what to think about the video. The review is:" Based on the preamble, Gemini reviews the input and suggests if the review is positive, neutral, or negative. It is worth noting that not all the components are required for a prompt, and the format can change depending on the task at hand. The element order can also change. Let's amend Sasha's original prompt "How can I create a network that uses IPv4 and IPv6 addresses?" and add a role context to the input fed into Gemini. She also adds the detail of needing a dual stack subnet. The new prompt is "I want you to act as a cloud architect in Google Cloud. How can I use gcloud to create a network that uses IPv4 and IPv6 subnets?" But since Gemini maintains its own interaction context, she could have just asked "I want you to act as a cloud architect in Google Cloud. How can I adjust the gcloud command provided to create a subnet to ensure the subnet is dual stack?" Now that you've had a chance to explore what Gen AI is, what large language models are and how they're trained, and what prompt engineering is, it's time to explore some prompt engineering best practices. The first best practice is to write detailed and explicit instructions. The more vague the prompt, the more chance that the model will produce a result that is not usable. Be clear and concise in the prompts that you feed into the model. Next, be sure to define boundaries for the prompt. It's better to instruct the model on what to do rather than what not to do. If the model gets stuck, give it a few 'fallback' outputs that work in various situations. For example, something like "I'm still learning about that" to use when unsure. Another best practice is to adopt a persona for your input. Adding a persona for the model can provide meaningful context to help it focus on related questions, which can help improve accuracy. This prompt would help Sasha, the cloud architect, get started with prototyping a network architecture for Cymbal Bank. And finally, it's a recommended practice to keep each sentence concise. Longer sentences can sometimes produce suboptimal results. It's best to break long sentences in a prompt into a series of shorter sentences and simpler tasks. So, let's return to Sasha, and use what we have learned so far. Sasha updates her prompt to: "You're a cloud architect. You want to build a Google Cloud VPC network that can be centrally managed. You also connect to other VPC networks in your company's other regions. You don't want to have many different sets of firewall policies to maintain. What sort of network architecture would you recommend?" With this new prompt, Gemini proposes a hub-and-spoke network architecture, which fits Sasha's needs exactly. By refining and amending her prompts, Sasha has articulated her requirements in a way that Gemini can respond with the correct focus and level of detail.

### Quiz - [Quiz](https://www.cloudskillsboost.google/course_templates/60/quizzes/521864)

#### Quiz 1.

> [!important]
> **How can you avoid hallucinations when using a large language model?**
>
> - [ ] Train the model on a clean, curated dataset.
> - [ ] Do not add any context to your prompts.
> - [ ] Train the model to operate with very few constraints.
> - [ ] Train the LLM on a smaller dataset.

#### Quiz 2.

> [!important]
> **Which one of the following is not a type of prompt?**
>
> - [ ] Zero-shot
> - [ ] Role prompt
> - [ ] One-shot
> - [ ] Two-shot

#### Quiz 3.

> [!important]
> **Which of the following is not a good practice for constructing prompts?**
>
> - [ ] Include long and detailed sentences.
> - [ ] Vary your prompts.
> - [ ] Avoid ambiguity.
> - [ ] Keep it short and simple.

#### Quiz 4.

> [!important]
> **Generative AI is a type of artificial intelligence that can ____.**
>
> - [ ] Generate text, images, or other data using generative models.
> - [ ] Generate responses on real-time data.
> - [ ] Make predictions about future events.
> - [ ] Perform complex calculations and mathematical operations.

## Course Summary

In this final section, we review what was presented in this course and discuss the next steps to continue your cloud learning journey.

### Video - [Course summary](https://www.cloudskillsboost.google/course_templates/60/video/521865)

- [YouTube: Course summary](https://www.youtube.com/watch?v=nr7cP5tVXfk)

Congratulations on completing the Google Cloud Fundamentals: Core Infrastructure training course. Before you go, let's take a few minutes to review what we've covered. In module 1, you were introduced to Google Cloud and cloud computing. Specifically, you explored: The concept of managed infrastructure and managed services, through IaaS, infrastructure as a service, and PaaS, platform as a service. The Google Cloud network. Google Cloud's focus on security throughout our infrastructure. How Google publishes key elements of technology using open source licenses. And Google Cloud's pricing structure and billing tools. In module 2, you learned about the Google Cloud Resource Hierarchy, which is made up of four levels: resources, projects, folders, and an organization node. You also learned about: Defining policies and their downward inheritance. When to use Identity and Access Management, or IAM, And the four ways to access and interact with Google Cloud: through the Google Cloud console, the Cloud SDK and Cloud Shell, APIs, and the Google Cloud App. In module 3, you explored how Compute Engine works, with a focus on virtual machines and virtual networking. You were introduced to: The VPC, or virtual private cloud. Compute Engine's Autoscaling feature. And important Google Virtual Private Cloud compatibility features, like routing tables, firewalls, VPC peering, and shared VPC, all of which result in the need for less network management. You also explored Cloud Load Balancing, a fully distributed, software-defined, managed service for all your traffic. Finally, you compared how on-premises or other-cloud networks can be interconnected with a Google VPC. In module 4, you explored Google Cloud's five core storage options: Cloud Storage, Bigtable, Cloud SQL, Spanner, and Firestore. You also examined the four storage classes that make up Cloud Storage: Standard Storage, which is used for frequently accessed hot data, Nearline Storage and Coldline Storage, which are used for less-frequently accessed cool data, and Archive Storage. In module 5, you learned about containers, which are invisible boxes around your code and its dependencies. You were introduced to containers, along with: Kubernetes, an open-source platform for managing containerized workloads and services. And Google Kubernetes Engine (GKE), a Google-hosted managed Kubernetes service in the cloud. In module 6, the focus was on developing applications in the cloud. You explored: Cloud Run, a managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. And Cloud Run functions, a lightweight, event-based, asynchronous compute solution to create single-purpose functions. Finally, in module 7, you explored how to combine Google Cloud knowledge with prompt engineering to improve Gemini responses. You discovered answers to the following questions: What is generative AI? What is a large language model? And what is prompt engineering? You ended the module by identifying prompt engineering best practices. We hope that this course is just the beginning of your Google Cloud journey. For more training and hands-on practice, explore the different learning paths available at cloud.google.com/training. And if you're interested in validating your expertise and showcasing your ability to transform businesses with Google Cloud technology, you might consider working toward a Google Cloud certification. You can learn more about Google Cloud's certification offerings at cloud.google.com/certification. Thanks for completing this course. We'll see you next time!

### Document - [Course resources](https://www.cloudskillsboost.google/course_templates/60/documents/521866)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
