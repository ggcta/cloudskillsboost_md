---
id: 52
name: 'Building Resilient Streaming Analytics Systems on Google Cloud'
datePublished: 2025-01-21
topics:
- Streaming Analytics
- Data Analysis
- Pub/Sub
type: Course
url: https://www.cloudskillsboost.google/course_templates/52
---

# [Building Resilient Streaming Analytics Systems on Google Cloud](https://www.cloudskillsboost.google/course_templates/52)

**Description:**

Processing streaming data is becoming increasingly popular as streaming enables businesses to get real-time metrics on business operations. This course covers how to build streaming data pipelines on Google Cloud. Pub/Sub is described for handling incoming streaming data. The course also covers how to apply aggregations and transformations to streaming data using Dataflow, and how to store processed records to BigQuery or Bigtable for analysis. Learners get hands-on experience building streaming data pipeline components on Google Cloud by using QwikLabs.

**Objectives:**

- Interpret use-cases for real-time streaming analytics.
- Manage data events using the Pub/Sub asynchronous messaging service.
- Write streaming pipelines and run transformations where necessary.
- Interoperate Dataflow, BigQuery and Pub/Sub for real-time streaming and analysis

## Introduction

This module introduces the course and agenda

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/52/video/521624)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=c7gBLqT61dg)

Damon: Welcome to Building Resilient Streaming Analytic Systems on Google Cloud, I'm Damon and I am a Technical Curriculum Developer at Google. Building resilient streaming analytic systems on Google Cloud is the third course of the data engineering on Google Cloud core series and it covers building resilient streaming analytic systems. Those systems allow organizations to make accurate and timely decisions from data points generated in real-time. This course discusses what streaming data processing is, how it fits in your overall big data architecture, win streaming data processing make sense and what Google Cloud technologies and products you can choose from to build your own resilient streaming analytic solutions. Here's how the course is broken down. We start off with what streaming data is and the challenges associated with processing streaming data. Like variable volumes and latency. Next we look at using pub/Sub, Dataflow and BigQuery to help us ingest, process and derive insights from data as it streams in. We dive into each product and learn about its streaming capabilities. Then we look at big table when higher throughput is a requirement. Finally, we review some of BigQuery's advance analytic capabilities like GIS functions and ways to improve query performance.

## Introduction to Processing Streaming Data

This modules talks about challenges with processing streaming data

### Video - [Processing streaming data](https://www.cloudskillsboost.google/course_templates/52/video/521625)

- [YouTube: Processing streaming data](https://www.youtube.com/watch?v=O-AzCMdbLoI)

This module discusses what stream processing is, how it fits into a big data architecture, when stream processing makes sense, and also, the challenges associated with streaming data processing. As this module is all about streaming, I’ll be discussing that part of the reference architecture. Data typically comes in through Pub/Sub, then that data goes through aggregation and transformation in Dataflow. Then you use BigQuery or Bigtable depending on whether the objective is to write aggregates or individual records coming in from streaming sources. Let’s look at streaming ideas first. Why do we stream? Streaming enables us to get real-time information in a dashboard or another means to see the state of your organization. In the case of the New York City Cyber Command, Noam Dorogoyer stated the following: “We have data coming from external vendors, and all this data is ingested through Pub/Sub, and Pub/Sub pushes it through to Dataflow, which can parse or enrich the data,” If data comes in late, especially when it comes to cybersecurity, it’s no longer valuable, especially during an emergency. So, from a data engineering standpoint, the way we constructed the pipeline is to minimize latency at every single step. If it’s maybe a Dataflow job, we designed it so that as many elements as possible are happening in parallel so at no point is there a step that’s waiting for a previous one. ” The amount of data flowing through the Cyber Command varies each day. Dorogoyer said that on weekdays during peak times, it could be 5 or 6 terabytes. On weekends, that can drop to 2 or 3 terabytes. As the Cyber Command increases visibility across agencies, it will deal with petabytes of data. Security analysts can access the data in several ways. They run queries in BigQuery or use other tools that will provide visualizations of the data, such as Looker Studio. Streaming is data processing on unbounded data. Bounded data is data at rest. Stream processing is how you deal with unbounded data. A streaming processing engine provides: low latency, speculative or partial results, the ability to flexibly reason about time, controls for correctness, and the power to perform complex analysis. You can actually use streaming to get real-time data warehouses and then create a dashboard of real-time information. For example, you could see in real-time the positive versus negative tweets about your company's product, use it to detect fraud, use for gaming events, or for finance back office apps, such as stock trading, anything dealing with markets, etc. So, when you look at the challenges associated with streaming applications, you’re talking about the 4 V’s, variety, volume, velocity, and veracity. First, data could come in from a variety of different sources and in various formats. Imagine hundreds of thousands of sensors for self-driving cars on roads around the world. The data is returned in various formats such as number, image, or even audio. Now consider point-of-sale data from a thousand different stores. How do we alert our downstream systems of new transactions in an organized way with no duplicates? Next, let’s increase the magnitude of the challenge to handle not only an arbitrary variety of input sources, but a volume of data that varies from gigabytes to petabytes. You’ll need to know whether your pipeline code and infrastructure can scale with those changes or whether it will grind to a halt or even crash. The third challenge concerns velocity. Data often needs to be processed in near-real time, as soon as it reaches the system. You’ll probably also need a way to handle data that arrives late, has bad data in the message, or needs to be transformed mid-flight before it is streamed into a data warehouse. And the fourth major challenge is veracity, which refers to the data quality. Because big data involves a multitude of data dimensions resulting from different data types and sources, there’s a possibility that gathered data will come with some inconsistencies and uncertainties. Challenges like these are common considerations for pipeline developers. The three products you are going to examine here are: Pub/Sub, which will allow you to handle changing and variable volumes of data, Dataflow, which can assist in processing data without undue delays, and BigQuery, which you will use for your ad-hoc reporting, even on streaming data. Let’s take a look at the steps that happen. First, some sort of data is coming in, possibly from an app, a database, or an Internet of Things, or IoT. These are generating events. Then, an action takes place. You are going to ingest those and distribute those with Pub/Sub. This will ensure that the messages are reliable. This will give you buffering. Dataflow, then is what aggregates, enriches, and detects the data. Next, you will write into a database of some kind, such as BigQuery or Bigtable, or maybe run things through a Machine Learning model. For example, you might use this streaming data as it is coming in to train a model in Vertex AI. Then, finally, Dataflow or Dataproc could be used for batch processing, backfilling, etc. So, this is a pretty common way to put things together in Google Cloud.

### Quiz - [Introduction to Processing Streaming Data](https://www.cloudskillsboost.google/course_templates/52/quizzes/521626)

#### Quiz 1.

> [!important]
> **When performing batch and streaming data processing, which Google product performs data aggregation and transformation?**
>
> - [ ] Sheets
> - [ ] Bigtable
> - [ ] Dataflow
> - [ ] Pub/Sub

#### Quiz 2.

> [!important]
> **Which of the following options offered by Dataflow makes it easy to create resilient streaming pipelines when working with unbounded data? Select TWO correct answers.**
>
> - [ ] Ability to flexibly reason about time
> - [ ] Controls to ensure correctness
> - [ ] SQL support to query in-process results
> - [ ] Global message bus to buffer messages

## Serverless Messaging with Pub/Sub

This module talks about using Pub/Sub to ingest incoming streaming data

### Video - [Module introduction](https://www.cloudskillsboost.google/course_templates/52/video/521627)

- [YouTube: Module introduction](https://www.youtube.com/watch?v=N_wjjsUzZXk)

person: Now that we have a good understanding of the process of streaming data let's dive into Pub/Sub to see how it works. We'll start by understanding how Pub/Sub works and is used in decoupling systems. Then we'll discuss the distribution of messages and different patterns through push and pull delivery models. Finally, we'll focus on actual implementation, looking at how things are set up in Pub/Sub. This will include a hands-on lab where you will publish streaming data into Pub/Sub.

### Video - [Introduction to Pub/Sub](https://www.cloudskillsboost.google/course_templates/52/video/521628)

- [YouTube: Introduction to Pub/Sub](https://www.youtube.com/watch?v=Bh8lkXUvbvE)

As we begin this module, I would ask you to keep your mind open to new ways of doing things. Pub/Sub does streaming differently than probably anything you have used in the past. It may be a different model than what you have seen before. Pub/Sub provides a fully managed data distribution and delivery system. It can be used for many purposes. It is most commonly used to loosely-couple parts of a system. You can use Pub/Sub to connect applications within Google Cloud, and with applications on premise or in other clouds to create hybrid Data Engineering solutions. With Pub/Sub the applications do not need to be online and available all the time. And the parts do not need to know how to communicate to each other, but only to Pub/Sub, which can simplify system design. First, Pub/Sub is not software; it is a service. So, like all of the other serverless services we have looked at, you don’t install anything to use Pub/Sub. Pub/Sub client libraries are available in C#, GO, Java, Node.js, Python, and Ruby. These wrap REST API calls which can be made in any language. It is also highly available. Pub/Sub offers durability of messages. By default it will save your messages for seven days in the event your systems are down and not able to process them. Finally, Pub/Sub is highly scalable as well. Google processes about 100 million messages per second across their entire infrastructure. This was actually one of the use cases for Pub/Sub early on at Google. To be able to distribute the search engine and index around the world because we keep local copies of search around the world, as you might imagine, in order to be able to serve up results with minimal latency. If you think about how you would architect that, we are crawling the entire world wide web, so we need to send the entire world wide web around the world. Either that or we would need to have multiple crawlers all over the world, but then you have data consistency problems; they would all be getting different indexes. Therefore, what we do is use Pub/Sub to distribute. As the crawler goes out, it grabs every page from the world wide web, and we send every single page as a message on Pub/Sub and it gets picked up by all local copies of the search index so it can be indexed. Currently, Google indexes the web anywhere from every two weeks, which is the slowest, to more than once an hour, for example on really popular news sites. So, on average, Google is indexing the web three times a day. Thus, what we are doing is sending the entire world wide web over Pub/Sub three times a day. This should explain how Pub/Sub is able to scale. Pub/Sub is a HIPAA-compliant service, offering fine-grained access controls and end-to-end encryption. Messages are encrypted in transit and at rest. Messages are stored in multiple locations for durability and availability. You control the qualities of your Pub/Sub solution by the number of publishers, number of subscribers, and the size and number of messages. These factors provide tradeoffs between scale, low latency, and high throughput. How does Pub/Sub work? The model is very simple. The story of Pub/Sub is the story of two data structures, the Topic and the Subscription. Both the Topic and the Subscription are abstractions which exist in the Pub/Sub framework independently of any workers, subscribers, etc. The Pub/Sub client that creates the Topic is called the Publisher. And the Pub/Sub client that creates the Subscription is called the Subscriber. In this example, the Subscription is subscribed to the Topic. To receive messages published to a topic, you must create a subscription to that topic. Only messages published to the topic after the subscription is created are available to subscriber applications. The subscription connects the topic to a subscriber application that receives and processes messages published to the topic. A topic can have multiple subscriptions, but a given subscription belongs to a single topic. In essence, it’s an enterprise message bus. So, how does it work? As you see here, there’s an HR Topic that relates to New Hire Events. For example, a new person joins your company and this notification should allow other applications that need to be notified about a new user joining to subscribe and get that message. What applications could tell you that a new person joined? One example is the Company Directory. This is a client of the Subscription also called a Subscriber. However, Pub/Sub is not limited to one Subscriber or one Subscription. Here there are multiple Subscriptions and multiple Subscribers. Maybe the Facilities System needs to know about the new employee for badging, and the accounting provisioning system needs to know for payroll. Each Subscription guarantees delivery of the message to the service. These subscriber clients are decoupled from one another and isolated from the publisher. In fact, we will see later that the HR System could go offline after it has sent its message to the HR Topic, and the message will still be delivered to the subscribers. These examples show one Subscription and one Subscriber. But you can actually have more Subscribers for a single Subscription. In this example, the Badge Activation System requires a human being to activate the badge. There are multiple workers, but not all of them are available all the time. Pub/Sub makes the message available to all of them. But only one person needs to fetch the message and handle it. This is called a Pull Subscription. The other examples are Push Subscriptions. Now, a new contractor arrives. Instead of entering through the HR System, they go through the Vendor Office. The same kinds of actions need to occur for this worker. They need to be listed in the company directory. Then, Facilities need to assign them a desk. Account provisioning needs to set up their corporate identity and accounts. And the badge activation system needs to print and activate their contractor badge. A message can be published by the Vendor Office to the HR Topic. The Vendor Office and the HR System are entirely decoupled from one another but can make use of the same company services. You can see from this illustration how important Pub/Sub is. Therefore, it gets the highest priority. When you receive messages from a subscription with a filter, you only receive the messages that match the filter. The Pub/Sub service automatically acknowledges the messages that don't match the filter. You can filter messages by their attributes. In the example, the filter sorts messages between those with the name attribute and the value of "com" and those without. You don't incur egress fees for the messages that Pub/Sub automatically acknowledges. You incur message delivery fees and seek-related storage fees for these messages. You can create a subscription with a filter using the Google Cloud console, the gcloud command-line tool, or the Pub/Sub API. You have learned generally what Pub/Sub does. Next, you will learn how it works and many of the advanced features it provides.

### Video - [Pub/Sub Push versus Pull](https://www.cloudskillsboost.google/course_templates/52/video/521629)

- [YouTube: Pub/Sub Push versus Pull](https://www.youtube.com/watch?v=GEffvtNZ-Aw)

person: From the HR analogy previously, you saw how Pub Sub works and how it's used in decoupling systems. Let's now discuss the technical details associated. We'll start by understanding the distribution of messages and different patterns. Here, the different colors represent different messages. The first pattern is just a basic straight through flow, where one publisher publishes messages into a topic, which then get consumed by the one subscriber through the one subscription. The second pattern is fan-in or load balancing, multiple publishers can publish the same topic, and multiple subscribers can pull from the same subscription, leveraging parallel processing. What you see here are two different publishers sending three different messages all on the same topic. That means the subscription will get all three messages. The third pattern is fan out, where you have many use cases for the same piece of data, and all data is sent to many different subscribers. As you can see here, we have two subscriptions. So both are going to get the messages, both the red message and the blue message. Pub Sub allows for both push and pull delivery. In the pull model, your clients are subscribers, and will be periodically calling for messages, and Pub Sub will just be delivering the messages since the last call. In the pull model, you're going to have to acknowledge the message as a separate step. So what you see here is we initially make the call to the subscribers, it pulls the messages, it gets a message back, and then separately, it acknowledges that message. The reason for this is because the pull queues are often used to implement some kind of queueing system for work to be done. So you don't want to acknowledge the message until you firmly have the message and have done the processing on it. Otherwise, you might lose the message if the system goes down. Therefore, we generally recommend you wait to acknowledge until after you have gotten it. In the pull model, the messages are stored for up to seven days. In the push model, it actually uses an HTTP endpoint. You register a web hook as your subscription, and Pub Sub infrastructure itself will call you with the latest messages. In the case of push, you just respond with status 200OK for the HTTP call, and that tells Pub Sub the message delivery was successful. It will actually use the rate of your success responses to self-limit so that it doesn't overload your worker. The way the acknowledgments work is to ensure every message gets delivered at least once. What happens is when you acknowledge a message, you acknowledge on a per subscription basis. So if you have two subscriptions, you have one acknowledge and the other one doesn't. The one that acknowledged will continue to get the messages. Pub Sub will continue to try to deliver the message for up to seven days until it is acknowledged. There is a replay mechanism as well that you can rewind and go back in time and have it replay messages. But in any case, you will always be able to go back seven days. You can also set the acknowledgment deadline and do that on a per subscription. So if you know that on average it takes you 15 seconds to process a message in your work queue, then you might set your acknowledgment deadline to 20 seconds. This will ensure it doesn't try to redeliver the messages. Configuring a topic with message retention gives you more flexibility, allowing any subscription attached to the topic to seek back in time and replay previously acknowledged messages. Topic message retention also allows a subscription to replay messages published before a subscription was created. Snapshots are utilized to make replay highly efficient. If topic message retention is enabled, storage costs for the messages retained by the topic are to be built to the topic's project. Subscribers can work individually or as a group. If we have just one subscriber, it is going to get every message delivered through that subscription. However, you can set up worker pools by having multiple subscribers sharing the same subscription. In this case, it is going to distribute the message, so one and three go to subscription one and two goes to subscription two. And it is just random based on when it pulls from messages throughout the day. In the case of a push subscription, you only have one web endpoint so you will only have one subscriber typically, but that one subscriber could be an App Engine standard app or cloud run container image which auto scales. So it is one web endpoint but it can have auto scale workers behind the scenes and that is actually a very good pattern.

### Video - [Publishing with Pub/Sub code](https://www.cloudskillsboost.google/course_templates/52/video/521630)

- [YouTube: Publishing with Pub/Sub code](https://www.youtube.com/watch?v=I1xy7MiWXwc)

person: With the theoretical background covered in the previous lesson, let's now shift our focus on actual implementation. How are things set up in Pub/Sub? Let's look at a little bit of code now. This example is using the client library for Pub/Sub. If we want to publish the message, we first create the topic. Then, we can publish the topic on the command line. More commonly, it will be done in code. Here, we get a publisher client, create a topic, and publish the message. Notice the letter B in front of my first message. This is because Pub/Sub just sends raw bites. This means that you aren't restricted to just text. You can send other data, like images, if you wanted to. The limit is 10 Megabytes. There are also extra attributes that you can include in messages. In this example, you see author='dylan'. Pub/Sub will keep track of those attributes to allow your downstream systems to get metadata about your messages without having to put it all in the message and parse it out. So instead of serializing and deserializing, it will just keep track of those key value pairs. Some of them have special meaning. We will see some of those shortly. To subscribe with Pub/Sub using the pull method, the code is similar. Select the topic, name the subscription. This is a pull subscription, so we will define a callback. When you are doing a pull subscription, it looks like this. You can pull the messages from the command line. You will see this in the lab. By default, it will just turn one message, the latest message. But there is a --limit you can set. Maybe you want 10 messages at a time. You can try that in the lab. You can also batch publish messages. This just prevents the overhead of the call for individual messages on the publishing side. This allows the Pub/Sub publishing engine to wait and send 10 or 50 at a time. This increases efficiency. However, if you are waiting for 50 messages, this means the first one now has latency associated with it. So it is a trade-off in your system. What do you want to optimize? But in any case, even if you batch publish, they still get delivered one at a time to your subscribers. We will practice this technique in the lab. Here's how you would set the batch in Python code. Explore the documentation for other command options and settings. If messages have the same ordering key and are in the same region, you can enable message ordering and receive the messages in the order that the Pub/Sub service receives them. When the Pub/Sub service redelivers the message with an ordering key, the Pub/Sub service also redelivers every subsequent message with the same ordering key, including acknowledged messages. If both message ordering and a dead letter topic are enabled on a subscription, the ordering may not be true, as Pub/Sub forwards messages to dead letter topics on a best effort basis. To receive the messages in order, set the message ordering property on the subscription you receive messages from. Please note that receiving messages in order might increase latency. You can set the message ordering property when you create a subscription using the Cloud Console, the gcloud command-line tool, or the Pub/Sub API. Pub/Sub is also going to help us with streaming resilience or buffering. What happens if your systems get overloaded with large volumes of transactions, like Black Friday? What you really need is some sort of buffer or backlog, so that you can feed messages only as fast as the systems are able to process them. Pub/Sub has this as a built-in capability. Let's recap this, then. When you look at the example on this slide, in the example on the left, and overload of arriving data causes a traffic spike. This overdrives the resources of the application, as illustrated by the smoke. One solution to this problem is to size the application to handle the highest traffic spike, plus some additionally capacity as a safety buffer. This is not only wasteful of resources, which must be retained at top capacity, even when not being used, but it provides a recipe for distributed denial of service attack by creating an upper limit at which the application will cease to behave normally and will exhibit nondeterministic behavior. The solution on the right uses Pub/Sub as an intermediary, receiving and holding data until the application has resources to handle it, either through processing the backlog of work or autoscaling to meet the demand. Erroneous records may cause your pipeline to get stuck or fail outright. I highly recommend implementing a dead letter queue and error logging to prevent these failure modes. These can help catch problems in user code and/or data shape. The idea behind exponential back off is to add progressively longer delays between retry attempts. After the first delivery failure, Pub/Sub will wait for a minimum back off time before retrying. For each consecutive failure on that message, more time will be added to the delay, up to a maximum delay. The maximum and minimum delay intervals are not fixed, and should be configured based on local factors to your application. Cloud Audit Logs maintains three audit logs for each Google Cloud project, folder, and organization: Admin activity, data access, and system event. Admin activity audit logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. Data access audit logs contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data. Admin activity audit logs are always written. You can't configure or disable them. Data access audit logs are disabled by default because they can be quite large. They must be explicitly enabled to be written. Pub/Sub reports metrics to Cloud Logging, including Pub/Sub topic and Pub/Sub subscription, which you can monitor against service quota utilization in a dashboard, and for setting notifications and alerts. Authentication is provided by service accounts. You can also directly authenticate users by their user accounts and their identity is reported in audit logs. But user account authentication is not recommended. Access control is provided by IAM.

### Video - [Summary](https://www.cloudskillsboost.google/course_templates/52/video/521631)

- [YouTube: Summary](https://www.youtube.com/watch?v=igjTHXqrLA8)

Person: When working with Pub/Sub, there are a few things to keep in mind. First, data may be delivered out of order. But that doesn't mean you have to process the data that way. You can write an application that handles out-of-order and replicated messages. This is different from a true queueing system. In general, they will be delivered in order, but you can't rely on that with Pub/Sub. This is one of the compromises made for scalability, especially since it's a global service. We have a mesh network, so a message might take another route. And if it happens to be a slower route, you could have an earlier message arriving later. For example, you wouldn't use this to implement a chat application, because it will be awkward when messages arrive out of order. Therefore, we will handle ordering using other techniques. Finally, we need to be ready for duplication. You can use data flow in conjunction with Pub/Sub to solve some of the problems we just discussed. Dataflow will de-duplicate messages based on the message ID, because in Pub/Sub, if a message is delivered twice, it will have the same ID in both cases. BigQuery can also be used for this purpose, but has limited capabilities. Dataflow will not be able to order in the sense of providing exact sequential order of when messages were published. However, it will help you deal with late data. Using Pub/Sub and Dataflow together allows you to get a scale that wouldn't be possible otherwise. In the next module, you will look at Dataflow's streaming capabilities in greater detail.

### Video - [Lab Intro: Publish Streaming Data into Pub/Sub](https://www.cloudskillsboost.google/course_templates/52/video/521632)

- [YouTube: Lab Intro: Publish Streaming Data into Pub/Sub](https://www.youtube.com/watch?v=jsbCbBo_1EY)

Person: Now let's practice publishing streaming data into Pub/Sub. In this lab, you create a Pub/Sub topic and subscription and simulate San Diego traffic data into Pub/Sub.

### Lab - [Streaming Data Processing: Publish Streaming Data into PubSub](https://www.cloudskillsboost.google/course_templates/52/labs/521633)

Streaming Data Processing: Publish Streaming Data into PubSub

- [ ] [Streaming Data Processing: Publish Streaming Data into PubSub](../labs/Streaming-Data-Processing-Publish-Streaming-Data-into-PubSub.md)

### Quiz - [Serverless Messaging with Pub/Sub](https://www.cloudskillsboost.google/course_templates/52/quizzes/521634)

#### Quiz 1.

> [!important]
> **True or False?
Pub/Sub guarantees that messages delivered are in the order they were received**
>
> - [ ] True
> - [ ] False

#### Quiz 2.

> [!important]
> **Which of the following delivery methods is ideal for subscribers needing close to real time performance?**
>
> - [ ] Pull Delivery
> - [ ] Push Delivery

#### Quiz 3.

> [!important]
> **Which of the following about Pub/Sub is NOT true?**
>
> - [ ] Pub/Sub simplifies systems by removing the need for every component to speak to every component
> - [ ] Pub/Sub connects applications and services through a messaging infrastructure
> - [ ] Pub/Sub stores your messages indefinitely until you request it

#### Quiz 4.

> [!important]
> **Which of the following about Pub/Sub topics and subscriptions are true?
(Select all 2 correct responses)**
>
> - [ ] 1 or more publisher(s) can write to the same topic
> - [ ] Each topic MUST have at least 1 subscription
> - [ ] 1 or more subscriber(s) can request from the same subscription
> - [ ] Each topic will deliver ALL messages for a topic for each subscriber

## Dataflow Streaming Features

This module revisits Dataflow and focuses on its streaming data processing capabilities 

### Video - [Module introduction](https://www.cloudskillsboost.google/course_templates/52/video/521635)

- [YouTube: Module introduction](https://www.youtube.com/watch?v=_8sMQB5_ykw)

Person: In the previous module, we discussed using Pub/Sub to receive streaming data from a variety of sources. Now we are ready to process it or prepare it for further analysis. Let us see how Dataflow can help you to do this. More specifically, let's look at Dataflow's streaming features. In this module, we'll discuss some of the challenges associated with streaming data and then the different windowing capabilities provided by Dataflow.

### Video - [Streaming data challenges](https://www.cloudskillsboost.google/course_templates/52/video/521636)

- [YouTube: Streaming data challenges](https://www.youtube.com/watch?v=E6-O-bdN3Y8)

Person: Let's start by identifying some of the challenges associated with processing streaming data. Dataflow, as we already know, provides a serverless service for processing batch and streaming data. It is scalable and for streaming has a low latency processing pipeline for incoming messages. We've discussed that we can have bounded and unbounded collections, so now we are examining an unbounded pipe that results from a streaming job. All of the things we've done thus far, like branching merging, we can do as well with Dataflow for streaming pipelines. However, now every step of the pipeline is going to act in real time on incoming messages, rather than in batches. What are some of the challenges with processing streaming data? One challenge is scalability-- being able to handle the volume of data as it gets larger and/or more frequent. The second challenge is fault tolerance. The larger you get, the more sensitive you are to going down unexpectedly. The third challenge is the model being used--streaming or repeated batch. Another challenge is timing or the latency of the data. For example, what if the network has a delay or a sensor goes bad and messages can't be sent? Additionally, there is a challenge around any kind of aggregation you might be trying to do. For example, if you are trying to take the average of data, but it is in a streaming scenario. You cannot just plug values into the formula for an average-- the sum from one to n-- because n is an ever-growing number. So in a streaming scenario, you have to divide time into windows, and we can get the average within a given window. This can be a pain if you have ever had to write a system like this, you can imagine it can be difficult to maintain windowing, time, roll threads, etcetera. The good news is that Dataflow is going to do this for you automatically. In Dataflow, when you are reading messages from Pub/Sub, every message will have a timestamp that is a Pub/Sub message timestamp. And then you will be able to use this timestamp to put the data into the different time windows and aggregate all of those windows. Message ordering matters and there may be a latency between the time that a sensor is read and the message is sent. You may need to modify the timestamps if this latency is significant. If you want to modify a timestamp and have it based on some property of your data itself, you can do that. Every message that comes in for example, the sensor provides its own date timestamp as part of the message. Element Source adds a default date timestamp, or DTS, which is the time of entry to the system rather than the time the sensor data was captured. A PTransform extracts the date timestamp from the data portion of the element and modifies the DTS metadata so the time of data capture can be used in window processing. Here's the code used to modify the date timestamp by replacing the message timestamp with a timestamp from the element data. If Pub/Sub IO is configured to use custom message IDs, Dataflow de-duplicates messages by maintaining a list of all the custom IDs it has seen in the last 10 minutes. If a new message's ID is in the list, the message is assumed to be a duplicate and discarded.

### Video - [Dataflow windowing](https://www.cloudskillsboost.google/course_templates/52/video/521637)

- [YouTube: Dataflow windowing](https://www.youtube.com/watch?v=EAf6F7jazkU)

person: Next, let's look at Dataflow windowing capabilities. This is really Dataflow's strength when it comes to streaming. Dataflow gives us three different types of windows: fixed, sliding, and sessions. Fixed windows are those that are divided into time slices. For example, hourly, daily, monthly. Fixed time windows consist of consistent, non-overlapping intervals. Sliding windows are those that you use for computing. For example, give me 30 minutes' worth of data and compute that every 5 minutes. Sliding time windows can overlap. For example, in a running average. Sliding windows are defined by a minimum gap duration, and timing is triggered by another element. Session windows are defined by a minimum gap duration, and the timing is triggered by another element. Session windows are for situations where the communication is burst-y. It might correspond to a web session. An example might be if a user comes in and uses four to five pages and leaves. You can capture that as a session window. Any key in your data can be used as a session key. It will have a time-out period, and it will flush the window at the end of that time out period. Here's how we can set these different types of windows in Python. In the fixed time window example, we can use the functions beam. WindowInto and window. FixedWindows with argument 60 to get fixed windows starting every 60 seconds. In the second example, with the sliding time window, we use window. SlidingWindows with argument 30 and 5. Here, the first argument refers to the length of the window, that is 30 seconds. And the second argument refers to how often new windows open, that is 5 seconds. Finally, we have the example of a session window. We use windows. Sessions with an argument of 10, multiplied by 60 to define a session window with time out of 10 minutes. That is, 600 seconds. How does windowing work? All things being equal, this is how windowing ought to work. If there was no latency, if we had an ideal world. If everything was instantaneous, then these fixed time windows would just flush at the close of a window. At the very microsecond at which is become 8:05, a 5-minute window terminates and flushes all of the data. This is only if there is no latency. But in the real world, latency happens. We have network delays, system backlogs, processing delays, Pub/Sub latency, et cetera. So when do we want to close the window? Should we wait a little bit longer than 8:05? Maybe a few more seconds? This is what we call the watermark, and Dataflow keeps track of it automatically. Basically, it is going to keep track of the lag time, and it is able to do this, for example, if you are using the Pub/Sub connector because it knows the time of the oldest, unprocessed message in Pub/Sub. And then it know the latest message it has processed through the Dataflow. It then takes this difference, and that is the lag time. So what Dataflow is going to do is continuously compute the watermark, which is how far behind we are. Dataflow ordinarily is going to wait until the watermark it has computed has elapsed. So if it is running a system lag of 3 or 4 seconds, it is going to wait 4 seconds before it flushes the window, because that is when it believes all of the data should have arrived for that time period. What then happens to late data? Let's say it gets an event with a time stamp of 8:04, but now it is 8:06. It is 2 minutes late, 1 minute after the close of the window. What does it do with this data? The answer is, you get to choose that. The default is just to discard it. But you can also tell it to reprocess the window based on those late arrivals. Beams default windowing configuration tries to determine when all data has arrived based on the type of data source, and then advances the watermark past the end of the window. This default configuration does not allow late data. The default behavior is to trigger at the watermark. If you don't specify a trigger, you are actually using the trigger after watermark. After watermark is an event time trigger. We could also apply any other trigger using event time. The message's time stamps are used to measure time with these triggers. But we could also add custom triggers. If the trigger is based on processing time, the actual clock, real time, is used to decide when to omit results. For instance, you can decide to omit exactly every 30 seconds, regardless of the time stamps of the messages that have arrived to the window. After count is an example of a data-driven trigger. Rather than omitting results based on time, here we trigger based on the amount of data that has arrived within the window. The combination of several types of triggers opens a world of possibilities with streaming pipelines. We may omit some results early, using after processing time, and then again at the watermark, when data is complete, and then for the next five messages that arrive late, after the watermark. Now we know different techniques to handle accumulation late arrival data. We also know that triggers are used to initiate the accumulation, and watermarks help in deciding the lag time and related corrective actions for computing accumulations. The code in this example creates a sample trigger. As you can see in the code, we are creating a sliding window of 60 seconds, and it slides every 5 seconds. The function after watermark method gives us details about when to trigger the accumulation. The code uses two options. First, early or speculative figuring, which is set to 30 seconds. Second, late for each late-arriving item. The second code segment demonstrates the composite trigger. The composite trigger will get activated either after 100 elements are available for accumulation, or every 60 seconds irrespective of watermark. This code segment uses a fixed window of 1 minute's duration. This is how the window reprocesses. This late processing works in Java and Python. When you set a trigger, you need to choose either accumulate mode or discard mode. This example shows the different behaviors caused by the intersection of windowing, triggers, and accumulation mode.

### Video - [Lab Intro: Streaming Data Pipelines](https://www.cloudskillsboost.google/course_templates/52/video/521638)

- [YouTube: Lab Intro: Streaming Data Pipelines](https://www.youtube.com/watch?v=2InF4gDrp8o)

Person: In this lab, you use Dataflow to collect traffic events from simulated traffic sensor data made available through Pub/Sub, process them into an actionable average, and store the raw data in BigQuery for later analysis. You will learn how to start a Dataflow pipeline, monitor it, and lastly, optimize it.

### Lab - [Streaming Data Processing: Streaming Data Pipelines](https://www.cloudskillsboost.google/course_templates/52/labs/521639)

Streaming Data Processing: Streaming Data Pipelines

- [ ] [Streaming Data Processing: Streaming Data Pipelines](../labs/Streaming-Data-Processing-Streaming-Data-Pipelines.md)

### Quiz - [Dataflow Streaming Features](https://www.cloudskillsboost.google/course_templates/52/quizzes/521640)

#### Quiz 1.

> [!important]
> **What element is applied by Pub/Sub to messages that allows Dataflow to perform aggregation of different time windows?**
>
> - [ ] Watermarks
> - [ ] Triggers
> - [ ] Timestamps
> - [ ] Message Groups

## High-Throughput BigQuery and Bigtable Streaming Features

This modules covers BigQuery and Bigtable for streaming data

### Video - [Module introduction](https://www.cloudskillsboost.google/course_templates/52/video/521641)

- [YouTube: Module introduction](https://www.youtube.com/watch?v=1JCgXwNBtd4)

Welcome to the module: High-Throughput BigQuery and Bigtable Streaming Features. In this module, you will learn about the analysis of streaming data and the throughput constraints associated with it. These analysis systems are primarily responsible for analyzing data in real time and helping make timely business decisions. We're going to talk about both BigQuery and Bigtable.

### Video - [Streaming into BigQuery and visualizing results](https://www.cloudskillsboost.google/course_templates/52/video/521642)

- [YouTube: Streaming into BigQuery and visualizing results](https://www.youtube.com/watch?v=-4g6fCYxai4)

person: In this first lesson, we'll discuss the streaming of data into BigQuery and using Google Data Studio to visualize results. Streaming data is not added to BigQuery via a load job. There is a separate BigQuery method called Streaming Inserts. Streaming Inserts allows you to insert one item at a time into a table. New tables can be created from a temporary table that identifies the schema to be copied. Usually the data is available within seconds. The data enters a streaming buffer where it is held briefly until it can be inserted into the table. Data availability and consistency are considerations. Candidates for streaming or analysis are applications that are tolerant of late or missing data or data arriving out of order or data that is duplicated. The stream can pass through other services, introducing additional latency and the possibility of errors. Since streaming data is unbounded, you need to consider the streaming quotas. There is both a daily limit and a concurrent rate limit. You can find more information about these in the online documentation. You can disable best effort de-duplication by not populating the insert ID field for each row inserted. When you do not populate insert ID, you get much higher streaming ingest quotas for the U.S. region, one million per second versus 500,000 inserts per second. This leads to a pertinent question, "When should you ingest a stream of data rather than use a batch approach to load data?" The answer is, when the immediate availability of the data is a solution requirement. And the reason, well, in most cases, loading batch data is not charged. Loading streaming data is a charged activity, so use batch loading or repeating batch loading rather than streaming, unless that real time is a requirement of the application. Here's an example of the code used to insert streaming data into a BigQuery table. In this example, the message body has already been decoded. In a full example, a step would be required to extract the appropriate message elements to be inserted. After streaming into a BigQuery table has been initiated, you can review the data in BigQuery by querying the table receiving the streaming data. When working with data in BigQuery, including streaming data, you can use Data Studio to explore the data further. After you execute a query, you can choose Data Studio from the explore data options to immediately start creating visualizations as part of a dashboard. This is the Data Studio home page. There are two ways to create a new report from scratch. Select blank report in the templates panel in the middle of the screen, or click the create button in the navigation pane on the left of the screen. Note that you can have any or all of these data sources in a single Data Studio report. In addition to the Google connectors, there is an increasing list of partner connectors to choose from as well. Since Data Studio reports can be shared, you should be aware of the ramifications of adding a data source. When you add a data source to a report, other people who can view the report can potentially see all the data in that data source. And anyone who can edit the report can use all the fields from any added data sources to create new charts with them. Click add to report. Having selected a dataset, you can specify what elements of the dataset you wish to visualize. This includes selecting the dimensions and metrics that you want to use from the available fields of your dataset. The edit data source picker in front of the data source name can be selected to edit the dataset fields. Easily change your data table view to a chart by clicking chart in the properties panel and selecting a chart type from the options provided. You can edit the style of your chart or even change your chart type selection later. You can also revert back to a data table view. You can also add separate charts by selecting add a chart from the toolbar, resize the components on the canvas to arrange data tables and different chart types as required. In the same way that you defined dimensions and metrics earlier, do the same for your chart by adding selections from the available fields list. Tip: the sequence of the fields under metric will determine the order in which the data is displayed in the chart. Use the drag feature to easily change the sequence of the fields. Give your report a name. Since Data Studio is based on Google Drive, note that you can have duplicate file names. Click the view toggle button to view the end-user version of the report. And here's your report. Notice, it looks very similar to when you were editing it, but as a viewer you can't modify the report. When a viewer mouses over the chart, they're able to view live data. In this example, the viewer is able to see that in the year 2000 there were 31 natural disasters attributable to extreme temperature. Note that users cannot edit your reports unless you give them permission. One of the Google Cloud products that helps manage the performance of dashboards is BigQuery BI Engine. BI Engine is a fast, in-memory analysis service that is built directly into BigQuery and is available to speed up your business intelligence applications. Historically, BI teams would have to build, manage and optimize their own BI service and OLAP cubes to support reporting applications. Now with BI Engine, you can get sub-second query response time on your BigQuery datasets without having to create your own cubes. BI Engine is built on top of the same BigQuery storage and compute architecture and service as a fast, in-memory, intelligent caching service that maintains state.

### Video - [Lab intro: Streaming Data Processing: Streaming Analytics and Dashboards](https://www.cloudskillsboost.google/course_templates/52/video/521643)

- [YouTube: Lab intro: Streaming Data Processing: Streaming Analytics and Dashboards](https://www.youtube.com/watch?v=CxM9Mru4wxo)

Person: In this streaming analytics and dashboards lab, you will connect to a BigQuery data source from Google data studio and create reports and charts to visualize the BigQuery data.

### Lab - [Streaming Data Processing: Streaming Analytics and Dashboards](https://www.cloudskillsboost.google/course_templates/52/labs/521644)

Streaming Data Processing3: Streaming Analytics and Dashboards

- [ ] [Streaming Data Processing: Streaming Analytics and Dashboards](../labs/Streaming-Data-Processing-Streaming-Analytics-and-Dashboards.md)

### Lab - [Generate Personalized Email Content with BigQuery Continuous Queries and Gemini](https://www.cloudskillsboost.google/course_templates/52/labs/521645)

In this lab, you learn how to generate personalized email content with BigQuery continuous queries, Gemini, Pub/Sub, and Application Integration triggers.

- [ ] [Generate Personalized Email Content with BigQuery Continuous Queries and Gemini](../labs/Generate-Personalized-Email-Content-with-BigQuery-Continuous-Queries-and-Gemini.md)

### Quiz - [Streaming Analytics and Dashboards](https://www.cloudskillsboost.google/course_templates/52/quizzes/521646)

#### Quiz 1.

> [!important]
> **True or False?
Looker Studio can issue queries to BigQuery**
>
> - [ ] False
> - [ ] True

#### Quiz 2.

> [!important]
> **Which of the following is true for Looker Studio?**
>
> - [ ] Looker Studio supports data ingest through multiple connectors.
> - [ ] Looker Studio is part of BigQuery and requires data to already exist in tables.
> - [ ] Looker Studio can only ingest files stored in Cloud Storage buckets.
> - [ ] Looker Studio is part of Dataflow and requires a streaming pipeline for data ingest.

### Video - [High-throughput streaming with Bigtable](https://www.cloudskillsboost.google/course_templates/52/video/521647)

- [YouTube: High-throughput streaming with Bigtable](https://www.youtube.com/watch?v=bSRkTGV6Yys)

So far, we looked at how to do queries on data even if it's streaming in using BigQuery, and displaying the data using Looker Studio. BigQuery is a very good general purpose solution, something that would work in most cases. But every once in a while, you will come across a situation where the latency of BigQuery is going to be problematic. In BigQuery, the data that's streaming in is available in a matter of seconds, but sometimes you will want lower latency than that. You will want your information to be available in milliseconds or microseconds. You may also have run into issues where the throughput of BigQuery may not be enough and you may want to deal with a higher throughput. So what we will be looking at next is how to handle such throughput or latency requirements when BigQuery is not enough. Where do you go? We will talk about Bigtable, which is suited to high-performance applications. We will look at how to design for Bigtable, specifically how to design schemas, how to design the row key from Bigtable. We will look at how to ingest data into Bigtable. To use Bigtable effectively you have to know a lot about your data and how it will be queried up-front. A lot of the optimizations happen before you load data into Bigtable. Bigtable is ideal for applications that need very high throughput and scalability for non-structured key/value data, where each value is typically no larger than 10 MB. Bigtable is not well suited for highly structured data, transactional data, small data volumes less than 1 TB, and anything requiring SQL Queries and SQL-like joins. Here are a few examples of data engineering requirements that have been solved using Bigtable. Machine learning algorithms frequently have many or all of these requirements. Applications that use marketing data, such as purchase histories or customer preferences. Applications that use financial data such as transaction histories, stock prices, or currency exchange rates. Internet of Things - IoT data, such as usage reports from meters, sensors, or devices. Time-series data, such as resource consumption like CPU and memory usage over time for multiple servers. Bigtable is most often used in real-time lookup capacity for an application where high-throughput is a necessity. Bigtable stores data in a file system called Colossus. Colossus also contains data structures called Tablets that are used to identify and manage the data. And metadata about the Tablets is what is stored on the VMs in the Bigtable cluster itself. This design provides amazing qualities to Bigtable. It has three levels of operation. It can manipulate the actual data. It can manipulate the Tablets that point to and describe the data. Or it can manipulate the metadata that points to the Tablets. Rebalancing tablets from one node to another is very fast, because only the pointers are updated. Bigtable is a learning system. It detects "hot spots" where a lot of activity is going through a single Tablet and splits the Tablet in two. It can also rebalance the processing by moving the pointer to a Tablet to a different VM in the cluster. So its best use case is with big data -- above 300 GB -- and very fast access but constant use over a longer period of time. This gives Bigtable a chance to learn about the traffic pattern and rebalance the Tablets and the processing. When a node is lost in the cluster, no data is lost. And recovery is fast because only the metadata needs to be copied to the replacement node. Colossus provides better durability than the default 3 replicas provided by HDFS. Bigtable stores the actual data elements in tables. And to begin with, it is just a table with rows and columns. However, unlike other table-based data systems like spreadsheets and SQL databases, Bigtable has only one index. That index is called the Row Key. There are no alternate indexes or secondary indexes. And when data is entered, it is organized lexicographically by the Row Key. The design principle of Bigtable is speed through simplification. If you take a traditional table, and simplify the controls and operations you allow yourself to perform on it, then you can optimize for those specific tasks. It is the same idea behind RISC - Reduced Instruction Set Computing - Simplify the operations. And when you don't have to account for variations, you can make those that remain very fast. In Bigtable, the first thing we must abandon in our design is SQL. This is a standard of all the operations a database can perform. And to speed things up we will drop most of them and build up from a minimal set of operations. That is why Bigtable is called a NoSQL database. The green items are the results you want to produce from the query. In the best case you are going to scan the Row Key one time, from the top-down. And you will find all the data you want to retrieve in adjacent and contiguous rows. You might have to skip some rows. But the query takes a single scan through the index from top-down to collect the result set. The second instance is sorting. You are still only looking at the Row Key. In this case the yellow line contains data that you want, but it is out of order. You can collect the data in a single scan, but the solution set will be disordered. So you have to take the extra step of sorting the intermediate results to get the final results. Now think about this. What does the additional sorting operation do to timing? It introduces a couple of variables. If the solution set is only a few rows, then the sorting operation will be quick. But if the solution set is huge, the sorting will take more time. The size of the solution set becomes a factor in timing. The orderliness of the original data is another factor. If most of the rows are already in order, there will be less manipulation required than if there are many rows out of order. The orderliness of the original data becomes a factor in timing. So introducing sorting means that the time it takes to produce the result is much more variable than scanning. The third instance is searching. In this case, one of the columns contains critical data. You can't tell whether a row is a member of the solution set or not without examining the data contained in the critical column. The Row Key is no longer sufficient. So now you are bouncing back and forth between Row Key and column contents. There are many approaches to searching. You could divide it up into multiple steps, one scan through the Row Keys and subsequent scans through the columns, and then perhaps a final sort to get the data in the order you want. And it gets much more complicated if there are multiple columns containing critical information. And it gets more complicated if the conditions of solution set membership involve logic such as a value in one column AND a value in another column, or a value in one column OR a value in another column. However, any algorithm or strategy you use to produce the result is going to be slower and more variable than scanning or sorting. What is the lesson from this exploration? To get the best performance with the design of the Bigtable service, you need to get your data in order first, if possible, and you need to select or construct a Row Key that minimizes sorting and searching and turns your most common queries into scans. Not all data and not all queries are good use cases for the efficiency that the Bigtable service offers. But when it is a good match, Bigtable is so consistently fast that it is magical. Here is an example using airline flight data. Each entry records the occurrence of one flight. The data include city of origin and the date and time of departure, and destination city and date and time of arrival. Each airplane has a maximum capacity, and related to this is the number of passengers that were actually aboard each flight. Finally, there is information about the aircraft itself, including the manufacturer (called the make), the model number, and the current age of the aircraft at the time of the flight. In this example, the Row Key will be defined for the most common use case. The Query is to find all flights originating from the Atlanta airport and arriving between March 21st and 29th. The airport where the flight originates is in the Origin field. And the date when the aircraft landed is listed in the Arrival field. If you use Origin as the Row Key, you will be able to pull out all flights from Atlanta -- but the Arrival field will not necessarily be in order. So that means searching through the column to produce the solution set. If you use the Arrival field as the Row Key, it will be easy to pull out all flights between March 21st and 29th, but the airport of origin won't be organized. So you will be searching through the arrival column to produce the solution set. In the third example, a Row Key has been constructed from information extracted from the Origin field and the Arrival field -- creating a constructed Row Key. Because the data is organized lexicographically by the Row Key, all the Atlanta flights will appear in a group, and sorted by date of arrival. Also, with the word “arrival” present in the key there is no need to verify that the timestamp is for the Arrival rather than the Departure. Using this Row Key you can generate the solution set with only a scan. In this example, the data was transformed when it arrived. So constructing a Row Key during the transformation process is straightforward. Bigtable also provides Column Families. By accessing the Column Family, you can pull some of the data you need without pulling all of the data from the row or having to search for it and assemble it. This makes access more efficient. The most common query is for the current arrival delay from Atlanta. That will involve averaging flight delays over the last 30 minutes. Hence, origin arrival. We want this at the top of the table, hence the reverse timestamp or RTS. You can reverse timestamps by subtracting the timestamp from your programming language's maximum value for long integers, such as Java's java.lang. Long. MAX VALUE, for example LONG MAX timestamp.millisecondsSinceEpoch. By reversing the timestamp, you can design a row key where the most recent event appears at the start of the table instead of the end. As a result, you can get the N most recent events simply by retrieving the first N rows of the table. When you delete data, the row is marked for deletion and skipped during subsequent processing. It is not immediately removed. If you make a change to data, the new row is appended sequentially to the end of the table, and the previous version is marked for deletion. So both rows exist for a period of time. Periodically, Bigtable compacts the table, removing rows marked for deletion and reorganizing the data for read and write efficiency. Distributing the writes across nodes provides the best write performance. One way to accomplish this is by choosing row keys that are randomly distributed. However, choosing a row key that groups related rows so they are adjacent makes it much more efficient to read multiple rows at one time. In our airline example, if we were collecting weather data from the airport cities, we might construct a key consisting of a hash of the city name along with a timestamp. The example row key shown would enable pulling all the data for Delhi, India, as a contiguous range of rows. Whenever there are rows containing multiple column values that are related, it is a good idea to group them into a column family. Some NoSQL databases suffer performance degradation if there are too many column families. Bigtable can handle up to 100 column families without losing performance. And it is much more efficient to retrieve data from one or more column families than retrieving all of the data in a row. There are currently no configuration settings in Bigtable for compression. However, random data cannot be compressed as efficiently as organized data. Compression works best if identical values are near each other, either in the same row or in adjoining rows. If you arrange your row keys so that rows with identical data are adjacent, the data can be compressed more efficiently. Bigtable periodically rewrites your table to remove deleted entries, and to reorganize your data so that reads and writes are more efficient. It tries to distribute reads and writes equally across all Bigtable nodes. In this example, A, B, C, D, E are not data, but rather pointers or references and cache, which is why re-balancing is not time-consuming. We are just moving pointers. Actual data is in tablets in the Colossus file system. Based on the learned access patterns, Bigtable re-balances data accordingly, and balances the workload across the nodes. With a well-designed schema, reads and writes should be distributed fairly evenly across an entire table and cluster. However, in some cases, it is inevitable that certain rows will be accessed more frequently than others. In these cases, Bigtable will redistribute tablets so that reads are spread evenly across nodes in the cluster. Note that ensuring an even distribution of reads has taken priority over evenly distributing storage across the cluster. In 2019 Spotify ran the largest Dataflow job ever at the time with Bigtable "...used as a remediation tool between Dataflow jobs in order for them to process and store more data in a parallel way, rather than the need to always regroup the data" By using Bigtable, Spotify was able to break down Dataflow jobs into smaller components — and reusing core functionality — and was able to speed up jobs and make them more resilient.

### Video - [Optimizing Bigtable performance](https://www.cloudskillsboost.google/course_templates/52/video/521648)

- [YouTube: Optimizing Bigtable performance](https://www.youtube.com/watch?v=D2i9Rus4i3M)

person: We will look now at how you can further optimize Bigtable performance. There are several factors that can result in slower performance. The table schema is not designed correctly. It's essential to design a schema that allows reads and writes to be evenly distributed across the Bigtable cluster. Otherwise individual nodes can get overloaded, slowing performance. The workload isn't appropriate for Bigtable. If you are testing with a small amount, less than 300 gigabytes of data, or for a very short period of time, seconds rather than minutes or hours, Bigtable won't be able to properly optimize your data. It needs time to learn your access patterns, and it needs large enough shards of data to make use of all the nodes in your cluster. The Bigtable cluster doesn't have enough nodes. Typically, performance increases linearly with the number of nodes in a cluster. Adding more nodes can therefore improve performance. Use the monitoring tools to check whether a cluster is overloaded. The Bigtable cluster was scaled up very recently. While nodes are available in your cluster almost immediately, Bigtable can take up to 20 minutes under load to optimally distribute cluster workload across the new nodes. The Bigtable cluster uses HDD disks. Using HDD disks instead of SSD disks means slower response times and a significantly lower cap on the number of read requests handled per second, 500 QPS for HDD disks versus 10,000 QPS for SSD disks. There are issues with the network connection. Network issues can reduce throughput and cause reads and writes to take longer than usual. In particular, you'll see issues if your clients are not running in the same zone as your Bigtable cluster. Because different workloads can cause performance to vary, you should perform tests with your own workloads to obtain the most accurate benchmarks. This is an example of some of the numbers that are possible in terms of throughput. With 100 nodes, you can handle 1 million queries per second. Throughput scales linearly well into the hundreds of nodes. A higher throughput means more items are processed in a given amount of time. If you have larger rows, then fewer of them will be processed in the same amount of time. In general, smaller rows offer higher throughput and therefore are better for streaming performance. Bigtable takes time to process cells within a row, so if there are fewer cells within a row, it will generally provide better performance than more cells. Finally, selecting the right row key is critical. Rows are sorted lexicographically. The goal when optimizing for streaming is to avoid creating hot spots when writing, which would cause Bigtable to have to split tablets and adjust loads. To accomplish that, you want the data to be as evenly distributed as possible. Reading delays adding to processing delays leads to response time. Replication for Bigtable enables you to increase the availability and durability of your data by copying it across multiple regions or multiple zones within the same region. You can also isolate workloads by routing different types of requests to different clusters. Use gcloud bigtable clusters create to create a cluster of Bigtable replicas. If a Bigtable cluster becomes unresponsive, replication makes it possible for incoming traffic to failover to another cluster in the same instance. Failovers can be either manual or automatic depending on the app profile an application is using and how the app profile is configured. The ability to create multiple clusters in an instance is valuable for performance, as one can be for writing and the replica cluster exclusively for reading. Bigtable also supports automatic failover for high availability. The generalizations of isolate the right workload, increase number of nodes, and decrease row size and cell size will not apply in all cases. In most circumstances, experimentation is the key to defining the best solution. A performance estimate is given in the documentation online for write-only workloads. Of course, the purpose of writing data is to eventually read it, so the baseline is an ideal case. At the time of this writing, a 10-node SSD cluster with 1-kilobyte rows and a write-only workload can process 10,000 rows per second at a 6-millisecond delay. This estimate will be affected by average row size, the balance and timing of reads distracting from writes and other factors. You will want to run performance tests with your actual data and application code. You need to run the tests on at least 300 gigabytes of data to get valid results. Also, to get valid results, your test needs to perform enough actions over a long enough period of time to give Bigtable the time and conditions necessary to learn the usage pattern and perform its internal optimizations. Key Visualizer is a tool that helps you analyze your Bigtable usage patterns. It generates visual reports for your tables that break down your usage based on the row keys that you access. Key Visualizer automatically generates hourly and daily scans for every table in your instance that meets at least one of the following criteria: During the previous 24 hours, the table contained at least 30 gigabytes of data at some point in time. During the previous 24 hours, the average of all reads or all writes was at least 10,000 rows per second. The core of a Key Visualizer scan is the heat map, which shows the value of a metric over time broken down into contiguous ranges of row keys. The X-axis of the heat map represents time, and the Y-axis represents row keys. If the metric has a low value for a group of row keys at a point in time, the metric is cold, and it appears in a dark color. A high value is hot, and it appears in a bright color. The highest values appear in white.

### Video - [Lab intro: Streaming Data Processing: Streaming Data Pipelines into Bigtable](https://www.cloudskillsboost.google/course_templates/52/video/521649)

- [YouTube: Lab intro: Streaming Data Processing: Streaming Data Pipelines into Bigtable](https://www.youtube.com/watch?v=d7JgTRzyHMw)

Person: Next is the second hands-on lab for this module: Streaming Data Pipelines into Bigtable. In this lab, you will launch a Dataflow pipeline to read from Pub/Sub and write into Bigtable, and open an HBase shell to query the Bigtable database.

### Lab - [Streaming Data Processing: Streaming Data Pipelines into Bigtable](https://www.cloudskillsboost.google/course_templates/52/labs/521650)

Streaming Data Processing: Streaming Data into Bigtable

- [ ] [Streaming Data Processing: Streaming Data Pipelines into Bigtable](../labs/Streaming-Data-Processing-Streaming-Data-Pipelines-into-Bigtable.md)

### Quiz - [High-Throughput Streaming with Bigtable](https://www.cloudskillsboost.google/course_templates/52/quizzes/521651)

#### Quiz 1.

> [!important]
> **Which of the following can help improve performance of Bigtable?
(Select all 3 correct responses)**
>
> - [ ] Use HDD instead of SDD
> - [ ] Change schema to minimize data skew
> - [ ] Add more nodes
> - [ ] Clients and Bigtable are in same zone

#### Quiz 2.

> [!important]
> **Which of the following are true about Bigtable?
(Mark all 3 correct responses)**
>
> - [ ] Support for SQL
> - [ ] Offers very low-latency in the order of milliseconds
> - [ ] Ideal for >1TB data
> - [ ] Great for time-series data

#### Quiz 3.

> [!important]
> **True or False?
Bigtable learns access patterns and attempts to distribute reads and storage across nodes evenly**
>
> - [ ] False
> - [ ] True

## Advanced BigQuery Functionality and Performance

This module dives into more advanced features of BigQuery

### Video - [Module introduction](https://www.cloudskillsboost.google/course_templates/52/video/521652)

- [YouTube: Module introduction](https://www.youtube.com/watch?v=gdgS8GmhcVc)

person: In this final module of the building resilience streaming systems on Google Cloud course you'll learn about some of the advanced features of BigQuery. In this module you'll learn about analytic Window functions and the use of width clauses to make complex queries more manageable. We'll introduce you to some of the GIS functions built into BigQuery. And finally we'll share best practices to consider for BigQuery performance.

### Video - [Analytic window functions](https://www.cloudskillsboost.google/course_templates/52/video/521653)

- [YouTube: Analytic window functions](https://www.youtube.com/watch?v=IHEB1s-GPV4)

person: Let's start by looking at how to use analytic window functions for advanced analysis. BigQuery like other databases has built-in functions to allow for rapid calculation of results. These include window functions to support advanced analysis. Three groups of functions exist standard aggregations, navigation functions, and ranking and numbering functions. The Count function is a frequently used and self-explanatory function. Other standard aggregation functions are listed here with more detail and how to properly use them available in the documentation. The LEAD function will return a value for a subsequent row in relation to the current row. In the example, the next bike rental time is listed along with the current rental row. Navigation functions generally compute some value expression over a different row in the window frame from the current row. Here are a few commonly used navigation functions. Rank returns the ordinal one based rank of each row within the ordered partition. In the example, each station's duration is returned in ranked order descending, the longest duration returned first. This example shows the ranking of employees by tenure using the start date within each department. First the rows are partitioned by department then ordered by start date. And then finally ranked. This is the SQL code used to perform the rank operation from the ranking of employees by tenure, example on the previous slide. Addition, ranking and numbering exists for specific use cases. The example seen here are frequently used when determining relationships between the rows of data rather than by an external measurement. WITH clauses are instances of a named subquery in BigQuery. WITH clauses are an easy way to isolate SQL operations and make complex queries more manageable.

### Video - [GIS functions](https://www.cloudskillsboost.google/course_templates/52/video/521654)

- [YouTube: GIS functions](https://www.youtube.com/watch?v=tQNIyTW5U_Y)

person: BigQuery has many built-in geographic information system or GIS features, you'll learn about some of them in this lesson. In the example shown, a zip code is used to determine how many bike stations are within one kilometer of the zip code and have at least 30 bikes available. ST_Geog point and ST_ DWithin are used together to pinpoint the stations of interest. ST simply means spatial type. ST_DWithin is used in conjunction with the geospatial boundaries of U.S. zip codes. The latitude and longitude of the bike stations join together in ST_Geog point to create a geospatial object and the value of 1000 for 1000 meters, which is one kilometer as the distance between the objects, zip code boundary and station point. This will return only those within one kilometer. ST_Geog point creates a geospatial object in well-known text or WKT from values provided within the database. In this case, we use latitude and longitude. If the latitude and longitude are provided in JSON format, the function ST_Geog from GeoJSON can be used to generate a geospatial object. To allow for quick testing of geospatial data Google Cloud provides the lightweight BigQuery Geo Viz application. This application will allow rendering of GIS data with minimal configuration. As mentioned earlier, ST_Geog point is used to create a geospatial object from relevant data. The image shows the exact coordinates of the ID values on a map of London. ST_MakeLine and ST_MakePolygon are two additional geospatial functions that can be used to overlay information on a map to help highlight relationships in the data. As mentioned in our earlier example, ST_DWithin can be used to determine the relative location of two points or objects. This image shows cities that are all within 150 kilometers linear distance from Terre Haute, Indiana. The functions ST_Intersects, ST_Contains and ST_CoveredBy allow reporting on the overlay or co-location of geospatial objects.

### Video - [Demo: GIS Functions and Mapping with BigQuery](https://www.cloudskillsboost.google/course_templates/52/video/521655)

- [YouTube: Demo: GIS Functions and Mapping with BigQuery](https://www.youtube.com/watch?v=kg9AwT0JJw8)

person: Welcome to another BigQuery demo. Here we're looking at some advanced BigQuery functions, specifically some Geographic Information System functions, or GIS functions. Whatever you think of latitude and longitude data, there's some really neat built-in GIS functions and some mapping capabilities built into BigQuery and some really cool plug-ins as you're going to see. So first, we need a really cool dataset. I'm going to be using the BigQuery Public Dataset on London Bike Share. This is millions of bike rides happening around the city of London, where they're coming from, where they're going to because the city of London has bike commuting stations that are fixed, that people can just rent bikes from and then take them from between places. So first off, what are the tables that we're going to be looking at? You've got your cycling stations, which is just the fixed locations around London that has just basic information about where you can rent the bikes, and really important part here I just want to cover later is it has a latitude and longitude, which means we can put it on a map, which is awesome. And then we need some activity of who's renting the bikes, how long are they spending on the bikes, how fast they're going. We'll actually show you how to impute that in a pretty easy, straight-line fashion. That's cycle_hire. So our end goal is, we're going to create a map for the first problem, is to find the fastest bike commuters inside of London. Those people who have the fastest average kilometers per hour, because we're in the UK, we use kilometers per hour, going from a starting station to a ending station. Average, fastest average speed for rides that must be more than 30 minutes, so people who are really getting a workout in. So I'm going to copy this first seemingly long query, and I'm going to break down every part of it for you before we run it so it all makes sense. So paste it in there, in to BigQuery, and we have a couple different tables. I'm going to get Alec to explore the tables first. So we have the stations themselves, so I want to pull the stations up and how many stations do we have? And the details, we have 778 stations, assuming that each row represents an individual station. Very small amount of data. Here's what the data actually looks like. Station's got an ID. Really important for GIS, it's got latitude and longitude. These data types, as you can see, for latitude and longitude, are floats. They're not GIS data types yet. Just because we have latitude and longitude does not mean it's a geographic point yet. I'm going to show you the very easy function called ST_GeogPoint, geography point. It's going to turn those floats in to a geographic point that you can then put on a map. So you've got lat and long, which is great. That's pretty much all we need from here because we can get the name of the station which is useful for our map label as well . So, how do I get this raw data and get it in to a good GIS format? That's the stations, the activity is in the cycle hires. Rentals table and you get what you might expect for a transactional table. How long the bike was in duration for, primary key, rental ID, need to account on those, get how many renters per station. A same bike can be rented more than once, which is interesting for bike maintenance. And then, where it was started and where it ended, but honestly you don't need these from the bikes table because you already have the station ID and a lot more information because the latitude and longitude for that station is not in here, so we can kind of ignore the ending station there. But, we do have the when it was rented and when the rental ended, with the end date there. So, let's talk about at this query. I call this staging because I like to get my data in a good format. This is the preprocessing before we actually go. So, what do we actually do? With the width clause, the width clause says, "Hey, get this named subquery, everything below here, between these two parenthesis, lines one through 34, is a named subquery. Technically you could stuff all of this in to the from clause if the below table, but it won't be really readable. What are we actually doing? So, I'm basically saying select as a struct. A struct is kind of like pre-jointed table, it's just a simple container. I do it for readability, to basically say, "All of these fields, I want you to prepend or prefix it with the word stating. " So it will be starting.name or starting.point, because you're going to have a lot of similar sounding column names because you have a starting station and a ending station. I prefer to use a struct just for that reason. So, I've got a lot of information about where it came from. The station name. We've used the ST Geography Point to turn the longitude and the latitude in to an actual geographic point, which BigQuery natively supports. Some other information about the station, like how many docks, bike docks, does it have, when the station was installed, lots of other good information in there as well. So I'm actually going to throw just a quick limit on this query. You can see, I mean, if you look at the cycle hire tables, it is a very, very large table. This is 24 million records. And, let's throw just a limit on there to see what some of this data actually looks like. So running this. And again, we're doing the joins -- we're doing two joins actually because we're going to do kind of like a self-join to basically get at the both the starting and ending station. Here's what it is actually going to look like in the end. And again, this is -- you can store this table if you want. I'm going to talk a little bit more about why you may or may not want to do that. We have this starting.name for the station, and then that's where the bike started from, that's how many bikes are at that dock, that's when the station was installed. Where did it go? It went from New Springs Garden Walk to Waterloo Place, St. James. Point to point, and everything that starts with bike.whatever -- that's the bike struct, has this information. Here's the line, so I'll show you if you actually scroll down in the query you can get some really interesting data with Geographic Information Systems functions, GIS functions. You can basically say, "Give me two points and I'll give you the straight line distance between them. " ST_Distance. "Also, give me two points and I'll draw a line string for you," if you're going to visualize this later on and map. So the distance is going to output in meters, for ST_Distance, and ST_MakeLine basically says, "Hey, if you're a GIS visualizer, here's a line string," So you can see that the bike distance is 2,184 meters between those two stations I just mentioned, and this is the point to point to actually draw that line as you're going to see when we actually dump this out in to a GIS visualizer a little bit later. So, we have, again, we're looking for average speed. So we got the bike duration in -- I think it's seconds, 1,980 seconds, and we have the distance that it covered, so we can get the average speed from there on out, and that's exactly what the latter half of this query actually does. So, we have all that information to stage that data. I dump it in and with clause you can see what it's doing. Yes, you can dump it into a table, but you'll lose the benefit if you're filtering on WHERE clauses. The WHERE clause here, as I noted, later on down inside of the query here allows you to do what's called a automatic predicate pushdown. It's a really cool, almost magical, part of BigQuery where yes, you can store all this data inside of a permanent materialized table, but if your users are constantly just filtering for a very small sliver of that later on, like we're filtering for the duration that's greater than 30 minutes, than what BigQuery can do is it can take this WHERE clause filter and actually, while -- before it actually executes that, as part of the query execution plan, bring that up in to the WHERE clause, you're not going to see this on the UI or anything like that, and then filter that data before it gets processed. So again, the argument against materializing this out to a permanent table is your users might be continuously filtering on this, so allow them the benefit of predicate pushdown. But honestly, it really just depends on your user case. So once we have all that raw data, the actual query that does the average speed is pretty easy. We want to return some basic dimensional information, the starting station, ending station, and distance in kilometers since it's going from meters to kilometers, rounded to just two decimal places. This is the line the trip has made, just uniting together, matching together all those lines. It's essentially how to do a distinct on the trip line for our use case. And, the total trips that were made between those two stations, that's just the count there, and then this is literally just the very simple average of the bike distance over the duration. So, the distance, meters over second, that's going to give you your speed. We actually want that in kilometers per hour, so you're taking meters divided by thousand, and then second divided by 60 to get minutes divided by 60 to get in to hours, and that's how you get to kilometers per hour, where bikes have at least 30 minute durations and the stations themselves have at least 100 trips between them and order by the fastest average pace first limit 100, so let's go ahead and run that. I should have been giving this explanation while this query is running because I think churning through all this data, I'm creating all this geographic information systems point. It's probably going to take at least -- You know, I think, when I ran this last, it was 30 to 60 seconds. So while that's running, that is going to be the aver speed. And, boom, here are the results. We don't have to wait for ours to finish. It actually looks like between Finlay Street and Fulham, and King Edward Street and St. Paul, the distance is 9 kilometers. And folks on a bike share -- And again, this is the average. This isn't even the max. The average people are putting out is about 10 miles an hour, 16.6 kilometers per hour, and that's 103 trips in this dataset. So, once you have the distance to tripline these geographic points, isn't it better to visualize this type of information point to point using a map? So you can use this kind of open tool, BigQuery Geography Viz, to just plot your BigQuery map data. So, let's see. This actually finished in 22 seconds, and we actually, I'm going to save these results somewhere. So, let's say, I'm just going to dump them to a BigQuery table. Let's see, do I have a cool one? I'm going to use business. And then, I'll call this just GIS demo, or something like that, and boom. Dump them in to a table. Oh, it's on the EU server, so I have to create a dataset in the EU. So, let me create a dataset in the EU because that's where this data wants to live. And let's see, create a dataset. First of all, let me disable turning off cache. So hopefully, I won't have to rerun that whole query again. We will do a GIS demo, will be the dataset ID. Perfect. And then, we'll say that it's in the European Union, because that's where this London data exists, and for the BigQuery poll dataset. And then let's hope -- Did it save it in cache? No because I think I just saved it to cache. Oh, well, we'll wait 22 seconds for that to actually come in to play, and then we'll save that as a table. Once you have that as a table, one of the cool things that you can do is inside of this BigQuery Geo Viz, you just literally say, "Hey, here's my projects." So let's get a project ready while we have this -- oh, it's already done. It's pretty fast. And, let's save the results, BigQuery table. I have GIS visualization. We'll just call this demo for speed, and then we'll dump it in to there. And my project, that's fine, GIS demo. Here it is. Here's the demo table. I'm just going to query this just so I have that query give me everything that from that table. So for this given project, how BigQuery GIS works is just coming Appspot project publicly available. You paste in your project ID and then, much like inside of BigQuery, you just dump the query in to the query editor that you see here, and it brings a little bit more on screen. Oh, I've got to make sure that I'm opening this in the same incognito window, so let me actually open this here. And I'm going to authorize it, yes, as my Qwiklabs account that I already have here, allow it to view and manage the data. Now I can paste it as a student because previously it was trying to do it from my Google e-mail address. And, boom, dump that in to there. And now let's get some GIS data in here, shall we? So, oh, what's this magical wizard things looks? I'm going to run this processing location, auto-select, sure. Go nuts. Whoa. Look at that. That's awesome. It automatically recognized, "Hey, you've got some lines inside of here," and it immediately zoomed us in onside of London which is super cool. So we've got this. Now let's look at the data. Geometry column, tripline, excellent. I want to style it though. I want the larger lines to represent a faster average speed trips. So I think inside of my demo, what do we have? What do we have? Yeah. Down here, it's the stroke weights, which is the weight of the line. We want to get a linear function with average kilometers per hours. Let me just try to make that. So we want the stroke weight, which is the width of the line, to be data driven, sure. And, it's in a linear function, and it's going to be the average kilometers per hour, so we want the thicker lines there, and then we want this to be -- I honestly just messed around with this and basically said these are all the different ranges that you can have. You can zoom out on this. I ended up. I played with this a little bit and found that if you did -- What did I do? The domain itself is -- This is just because you want the bigger lines to show up a little bit better, but honestly a lot of this is half an art and a science. So I'm just copying from what I did earlier, and you can see, as we zoom in to the map, you find that largest line, that is the highest average speed, so I'm going to go ahead and click on that, and this is exactly what we saw. So it started on Finlay Street, went to King Edward Street, the distance is 9 kilometers. There's 103 trips that made that and the average speed of all them was 16.6. So we went full circle and, again, if you reduce this, if you increase this, this is just changes the weights of those lines as well. How big -- how much of a disparity, deviation, rather, between the sizing of those as well. So you can see, not only was that one pretty fast, but there was two other ones that were pretty fast in commuting there as well. And you can make all sorts of assumptions and interpretations about why this one was the fastest route. Maybe people were just, "Hey, we don't want to go around town and explore a lot," because you can imagine not all these bikes are just going from station to station. Maybe the ones in the inner city of London are tourists that take a long time. Maybe they're going superfast. But they take a long, long, long, long, long, actual distance because what the distance that we're measuring here is just literally in a straight line distance between stations. So maybe one assumption is if you're riding from Fulham to wherever this was ending up, that most likely you're just going point to point. You're not using the bike as, like, a touring route as well. Again, just all assumptions until you actually go in and take a look at the data, maybe interview some of the folks who are starting from station to station there, and see that maybe the majority of them are actually just bike commuters instead of tourists. So, that's a basic recap. You just went from a raw dataset converted with GIS functions, and then ultimately visualize it in BigQuery Geo Viz. Nice work. So in our demo, you saw me use ST_Distance, but that's just one of the many GIS functions supported by BigQuery. Here's an example where we take New York City bike station locations against zip code boundaries to see how many stations are inside of that zip code polygon with ST_DWITHIN. A major takeaway is that all Latin long values should be converted with ST Geography Point as a WKT, or a well-known text, which is much more efficient to store and query from. In the demo, we covered ST_MakeLine, but you can also make polygon areas as well, as you see here with these three points in the triangle. Here's an example in which queries, which point are within 150 kilometers of another given point, and then draws those lines to match against that intersection. Again, these are not driving time estimates, but simple straight lines. You can get pretty advanced with GIS, like seeing if locations intersect with ST_Intersects, and all the other functions that are listed here. If this interests you, I'd really encourage you to read up on the GIS documentation and other examples from BigQuery and see how you can make some pretty awesome GIS insights and maps with your data.

### Video - [Performance considerations](https://www.cloudskillsboost.google/course_templates/52/video/521656)

- [YouTube: Performance considerations](https://www.youtube.com/watch?v=Hnp6K4eag10)

person: This lesson is a recap on BigQuery performance and pricing topics. The goal for virtually every information system is to promote fast and smart decisions. Here are a few best practices to consider. Use Dataflow to do the processing and data transformations. Create multiple tables for easy analysis. Use BigQuery for streaming analysis and dashboards and store data in BigQuery for low cost long-term storage. Also create views for common queries. Exploring a dataset through SQL is more than just writing good code. You need to know what destination you're heading towards and the general layout of your data. Good data analysts will explore how the dataset is structured even before writing a single line of code. People often analyze data and develop a schema at the beginning of a project and never revisit those decisions. The assumptions they made at the beginning may have changed and are no longer true, so they attempt to adjust the downstream processes without ever reviewing and considering changing some of the original decisions. Look at the data. Perhaps it was evenly distributed at the start of the project, but as the work has grown, the data may have become skewed. Look at the schemas. What were the goals then? Are those the same goals now? Is your organization of the data optimized for current operations? Stop accumulating work that could be done earlier. Analogy, dirty dishes; if you clean them as you use them, the kitchen remains clean. If you save them, you end up with a sink full of dirty dishes and a lot of work. There are five key areas for performance optimization in BigQuery, and they are input and output. How many bytes were read from disk? Shuffling, how many bytes were passed to the next query processing stage? Grouping, how many bytes were passed through to each group? Materialization, how many bytes are written permanently out to disk? Lastly, functions and UDFs, how computationally expensive is the query on your CPU? There's an old Silicon Valley saying, don't scale up your problems. Solve them early while they're small. Here's a cheat sheet of best practices that you should follow. Don't select more data columns than you need. That means avoid Select at all costs when you can. If you have a very large dataset, consider using approximate aggregation functions instead of regular ones. Next, make liberal use of the Where clause at all times to filter data. Then don't use an Order by on a Why clause or subqueries or any other subqueries that you have, only apply Order by as the last operation that you will perform. For Joins, put the larger table on the left if you can. That'll help BigQuery optimize it and how it does its Joins. If you forget, BigQuery will likely do those optimizations for you, so you might not even see any difference. You can use wildcards and table suffixes to query multiple tables, but try to be as specific as possible as you can with those wildcards. For your Group Bys, if you're grouping by the names of every Wikipedia author ever which means high distinct values or high cardinality, that's a bad practice or an anti-pattern. Stick to low unique value Group Bys. Lastly, use partition tables whenever you can. Earlier, we talked about how to build a data warehouse. We mentioned that you can optimize the tables in your data warehouse by reducing the cost and amount of data read. this can be achieved by partitioning your tables. Partitioning tables is very relevant to performance, so let's revisit, in the next few slides, some of the main points already covered. You enable partitioning during the table creation process. The slide shows how to migrate an existing table to an ingestion time-partitioned table. Just use destination_table. It will cost you one table scan. BigQuery creates new date-based partitions automatically with no need for additional maintenance. In addition, you can specify an expiration time for data in the partitions. New data that is inserted into a partitioned table is written to the raw partition at the time of insert. To explicitly control which partition the data is loaded to, your load job can specify a particular data partition. In a table partitioned by a date or time-stamped column, each partition contains a single day of data. When the data is stored, BigQuery ensures that all the data in a block belongs to a single partition. A partition table maintains these properties across all operations that modify it, query jobs, data manipulation language, DML statements, data definition language, DDL statements, load jobs and copy jobs. This requires BigQuery to maintain more metadata than a non-partitioned table. As the number of partitions increases, the amount of metadata overhead increases. Although more metadata must be maintained, by ensuring that data is partitioned globally, BigQuery can more accurately estimate the bytes processed by a query before you run it. This cost calculation provides an upper bound on the final cost of the query. A good practice is to require that queries always include the partition filter. Make sure that the partition field is isolated on the left-hand side, since that's the only way BigQuery can quickly discard unnecessary partitions. Clustering can improve the performance of certain types of queries, such as queries that use filter clauses and those that aggregate data. When data is written to a clustered table by a query or load job, BigQuery sorts the data using the values in the clustering columns. These values are used to organize the data into multiple blocks in BigQuery storage. When you submit a query containing a clause that filters data based on the clustering columns, BigQuery uses the sorted blocks to eliminate scans of unnecessary data. Similarly, when you submit a query that aggregates data based on the values in the clustering columns, performance is improved because the sorted blocks colocate rows with similar values. In this example, the table is partitioned by event date and clustered by user ID. Now, because the query looks for partitions in a specific range, only two of the five partitions are considered. Because the query looks for user ID in a specific range, BigQuery can jump to the row range and read only those rows for each of the columns need. You set up clustering at table creation time. Here we are creating the table, partitioning by event date and clustering by user ID. We are also telling BigQuery to expire partitions that are more than 3 days old. The columns you specify in the cluster are used to colocate related data. When you cluster a table using multiple columns, the order of columns you specify is important. The order of the specified columns determines the sort order of the data. Over time, as more and more operations modify a table, the degree to which the data is sorted begins to weaken, and the table becomes only partially sorted. In a partially sorted table, queries that use the clustering columns may need to scan more blocks compared to a table that is fully sorted. You can re-cluster the data in the entire table by running a Select query that selects from and overrides the table, but guess what? You don't need to do that anymore. The great news is that BigQuery now periodically does auto re-clustering for you, so you don't need to worry about your clusters getting out of date as you get new data. Automatic re-clustering is absolutely free and automatically happens in the background. You don't need to do anything additional to enable it. Partitioning provides a way to obtain accurate cost estimates for queries and guarantees improved cost and performance. Clustering provides additional cost and performance benefits in addition to the partitioning benefits. BigQuery supports clustering for both partitioned and non-partitioned tables. When you use clustering and partitioning together, the data can be partitioned by a data or time-stamped column and then clustered on a different set of columns. In this case, data in each partition is clustered based on the values of the clustering columns. Partitioning provides a way to obtain accurate cost estimates for queries. Keep in mind if you don't have partitioned columns, and you want the benefits of clustering, you can create a fake date column of type date and have all the values be null. If you create a large multistage query each time you run it, BigQuery reads all the data that is required by the query, all the data that is read each time the query is run. Intermediate table materialization is where you break the query into stages. Each stage materializes the query results by writing them to a destination table. Querying the smaller destination table reduces the amount of data that is read. In general, storing the smaller materialized results is more efficient than processing the larger amount of data. The analogy is air travel from Sunnyvale, California, USA to Japan. There is one direct flight or a series of four shorter connecting flights. The direct flight has to carry the fuel for the entire journey. The connecting flights only need enough fuel for each leg of the trip. The total fuel used in landing and taking off, an analogy for storing the intermediate tables, was less than the total fuel used for carrying everything in the entire journey. Here is a tip. Compare costs of storing the data with cost of processing the data. Processing the large dataset will use more processing resources. Storing the intermediate tables will use more storage resources. In general, processing data is more expensive than storing data, but you can do the calculations yourself to establish a breakeven for your particular use case. A different way to check how many records are being processed is by clicking on the explanation tab in the BigQuery UI after running a query. We started with 9.1 million rows and filtered down to roughly 36,000. The query stages represent how BigQuery mapped out the work required to perform the query job. Approximate functions are a great way to improve performance. The Approx Count distinct function returns an approximate result for count distinct expression. The result is less accurate, but it performs much more efficiently. An easy way to understand the performance of your BigQuery operations is through Cloud Monitoring, a default component of every Google Cloud project. These charts show slot utilization, slots available and queries in flight for a 1-hour period.

### Video - [Lab Intro: Optimizing your BigQuery Queries for Performance](https://www.cloudskillsboost.google/course_templates/52/video/521657)

- [YouTube: Lab Intro: Optimizing your BigQuery Queries for Performance](https://www.youtube.com/watch?v=j4ASuviBMRA)

person: In this lab you'll optimize your BigQuery queries for performance. Specifically, you'll use BigQuery to minimize input and output from your queries. You'll cash results from your previous queries, learn about performing efficient joints, avoid overwhelming single workers with your query and lastly use approximate aggregation functions. Good luck.

### Lab - [Optimizing your BigQuery Queries for Performance 2.5](https://www.cloudskillsboost.google/course_templates/52/labs/521658)

In this lab, we will look at a number of techniques for reducing query execution times and cost in BigQuery.

- [ ] [Optimizing your BigQuery Queries for Performance 2.5](../labs/Optimizing-your-BigQuery-Queries-for-Performance-2.5.md)

### Video - [Cost considerations](https://www.cloudskillsboost.google/course_templates/52/video/521659)

- [YouTube: Cost considerations](https://www.youtube.com/watch?v=vKGU5vZ-US0)

person: Storage pricing is based on the amount of data stored in your tables when it is uncompressed. The size of the data is calculated based on the data types of the individual columns. Active storage pricing is prorated per megabyte per second. If a table is not edited for 90 consecutive days, it is considered long-term storage. The price of storage for that table automatically drops by approximately 50 percent. There is no degradation of performance, durability, availability or any other functionality. If the table is edited, the price reverts back to the regular storage pricing and the 90-day timer starts counting from zero. Anything that modifies the data in a table resets the timer, including loading data into a table, copying data into a table, writing query results to a table, using the data manipulation language, using the data definition language, streaming data into the table. All other actions do not reset the timer, including querying a table, creating a view that queries a table, exporting data from a table, copying a table to another destination table, and patching or updating a table resource. Because you don't get to see the VMs running behind the scenes, BigQuery exposes slots to help you manage resource consumption and costs. BigQuery automatically calculates how many slots are required by each query depending on query size and complexity. The default slot capacity and allocation work well in most cases. You can monitor slot usage in Cloud Monitoring. Candidate circumstances where additional slot capacity might improve performance are solutions with very complex queries on very large datasets with highly concurrent workloads. You can read more about slots in the online documentation or contact a sales representative. Fixed-rate pricing is $10,000 per 500 slots per month. A 25 percent discount is offered for customers choosing a term length of at least 1 year, $7,500 for 500 slots. Capacity is sold in increments of 500 slots with a current minimum of 500 slots. Flex Slots are an option available for you to purchase BigQuery slots for short durations. Flex slots allow you to purchase BigQuery slots for short durations, as little as 60 seconds at a time. A slot is the unit of BigQuery analytics capacity. Flex slots let you quickly respond to rapid demand for analytics and prepare for business events such as retail holidays and app launches. Flex slots give BigQuery reservation users immense flexibility without sacrificing cost, predictability or control. Flex slots are priced at $0.04 per slot per hour, and are available in increments of 100 slots. It usually takes just a few minutes to deploy Flex slots in BigQuery reservations. Once deployed, you can cancel after just 60 seconds and you will only be billed for the seconds Flex slots are deployed. You can seamlessly combine Flex slots with existing annual and monthly commitments to supplement steady-state workloads with bursty analytics capability. For many businesses, specific days or weeks of the year are crucial. Retailers care about Black Friday and Cyber Monday. Gaming studios focus on the first few days of launching new titles. And financial services companies worry about quarterly reporting and tax season. Flex slots enables such organizations to scale up their analytics capacity for the few days necessary to sustain the business event and scale down thereafter, only paying for what they consumed. There are a number of considerations for flat-rate pricing. Flex slots are a special commitment type. The commitment duration is only 60 seconds. You can cancel Flex slots any time thereafter. You're charged only for the seconds your commitment was deployed. Flex slots are subject to capacity availability. When you attempt to purchase Flex slots, success of this purchase is not guaranteed. However, once your commitment purchase is successful, your capacity is guaranteed until you cancel it. Monthly commitments cannot be canceled for 30 days after your commitment is active. After the first 30 calendar days, you can cancel or downgrade at any time. If you cancel or downgrade, the charges are prorated per second at the monthly rate. For example, you cannot cancel on day 29. If you cancel during the first second of day 31, you're charged for 30 days and 1 second. And if you cancel at the mid-point of the third month, you're charged 50 percent of your monthly rate for that month. Prior to the anniversary of your commitment date, you can choose to renew for another year, or convert it to a monthly or Flex commitment. If you move to the monthly rate, you can cancel any time and you're charged per second at the monthly rate. For example, if you renew for another year after your annual commitment date, you enter into a new annual commitment and you continue to be charged the yearly commitment rate. Also, if you don't renew for another year after your annual commitment date, you can cancel at any time and charges are prorated per second at the monthly rate. If you determine you need more BigQuery slots, you can purchase additional increments of 500. However, doing so will create a new commitment. When you purchase a flat-rate plan, you specify the allocation of slots by location. To use slots in multiple locations, you much purchase slots in each location. A project can use either flat-rate or on-demand pricing. If you have multiple projects in a given location, you can choose which projects use flat-rate pricing and which projects use on-demand pricing. Lastly, to discontinue a flat-rate pricing plan, you must cancel or downgrade your commitment, but only after the initial commitment period, 30 days or 1 year. BigQuery doesn't support fine-grained prioritization of interactive or batch queries. To avoid a pileup of BigQuery jobs and timely execution, estimating the right BigQuery slots allocation is critical. Currently, BigQuery times out any query taking longer than 6 hours. If one query is executing within BigQuery, it has full access to the amount of slots available to the project or reservation -- by default, 2,000. If we suddenly execute a second query, BigQuery will split the slots between the two queries, with each getting half the total amount of slots available -- in this case, 1,000 each. This subdividing of compute resources will continue to happen as more queries are executed. This is a long way of saying, it's unlikely that one resource-heavy query will overpower the system and steal resources from other running queries. In flat-rate pricing, organizations have a fixed number of slots. Concurrency is fair across projects, users and queries. That is, if you have 2,000 slots and two projects, each project can get up to 1,000 slots. If one project uses less, the other project will be able to use all of the remainder. If you have two users in each project, each users will be able to get 500 slots. And if each of the two users of the two projects runs two queries, they'll each get 500 slots. This is a long way of saying, they won't likely degrade performance by adding projects. Note that if you want to prioritize one project over another, you can set up a hierarchical reservation. Let's say you have an ETL project that is somewhat lower priority than your dashboarding project. You can give the ETL project 500 slots as a sub-reservation, and the dashboarding project will be in the outer one. If both projects are fully using their reservations, the ETL project can never get more than 500 slots. When one project is lightly used, the other project will be able to take the remaining slots.

### Quiz - [BigQuery advanced functionality and performance considerations](https://www.cloudskillsboost.google/course_templates/52/quizzes/521660)

#### Quiz 1.

> [!important]
> **Which of the following practices help optimize BigQuery queries?**
>
> - [ ] Avoid using unnecessary columns
> - [ ] Put the largest table on the left
> - [ ] Use COUNT(DISTINCT) instead of APPROX_COUNT_DISTINCT
> - [ ] Filter early and often

## Course Summary

This module recaps the topics covered in course

### Video - [Course summary](https://www.cloudskillsboost.google/course_templates/52/video/521661)

- [YouTube: Course summary](https://www.youtube.com/watch?v=-0vW5RuNfEo)

person: You've reached the end of this course on building resilient streaming analytic systems on Google Cloud. Let's recap what you've learned. We started off with what streaming data is and the challenges associated with processing streaming data. We said it was important to be able to ingest fairing amounts of data, because you could have spikes in your data. It is important to be able to deal with unexpected delays because latency is a fact of life. We want to be able to derive real-time insights from the data, even as the data is streaming in. In order to do that we look at the architecture that consisted of ingesting the data with Pub/Sub, processing the data in stream using Dataflow and streaming it into BigQuery for a durable storage and interactive analysis. We also spent time talking about how Bigtable is a better solution when a much higher throughput is desired. And finally we went back to BigQuery to look at some of its advanced analysis capabilities with window functions and GIS functionalities, as well as reviewed ways to optimize query performance. Congratulations on completing building resilience streaming analytics system on Google Cloud. Smart analytics, machine learning and AI on Google Cloud is the fourth and final course of the data engineering on Google Cloud Core series and is covered next, we hope to see you there.

## Course Resources

PDF links to all modules

### Document - [Building Resilient Streaming Analytics Systems on Google Cloud](https://www.cloudskillsboost.google/course_templates/52/documents/521662)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
