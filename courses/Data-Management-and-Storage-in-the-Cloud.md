---
id: 962
name: 'Data Management and Storage in the Cloud'
datePublished: 2024-05-06
topics:
- Data Governance
- Data
- Data Management
type: Course
url: https://www.cloudskillsboost.google/course_templates/962
---

# [Data Management and Storage in the Cloud](https://www.cloudskillsboost.google/course_templates/962)

**Description:**

This is the second of five courses in the Google Cloud Data Analytics Certificate. In this course, you'll explore how data is structured and organized. You'll gain hands-on experience with the data lakehouse architecture and cloud components like BigQuery, Google Cloud Storage, and DataProc to efficiently store, analyze, and process large datasets. 

**Objectives:**

- Explain how data is defined and structured in BigQuery and Google Cloud Storage.
- Identify the key components that make up a data lakehouse architecture.
- Explain how partitioning can improve query performance and reduce costs.
- Define key components of data governance.

## Introduction to data management and storage in the cloud

In this module, you'll explore how data is stored and moves across systems. You'll learn about types of data, their storage structures, and how data can be extracted from them, including methods of moving data between systems (APIs and connectors).

### Video - [Introduction to Course 2](https://www.cloudskillsboost.google/course_templates/962/video/471989)

- [YouTube: Introduction to Course 2](https://www.youtube.com/watch?v=Gi5ohogn31Q)

Hello, data enthusiast! I’m so happy to welcome you to this course about cloud storage and data management in the cloud! My name is Eric, and I work as a Product Analyst at Google. I began my career as a recruiter in aerospace, where I aimed to quantify what attributes set apart especially great job candidates. I’ve since worked across several industries (aerospace, entertainment, tech) and several departments (recruiting, real estate, product). Across all these experiences, the importance of data infrastructure has been a throughline —deriving insights from a dataset fundamentally requires that data to be organized & accessible. I can’t wait to share more and help guide you through the next series of topics —all about how data is structured and organized. You’re about to gain tons of practical experience in data organization, which means you’ll be equipped to establish a plan that ensures you can find the data you need, in a useful format, anytime you need it. You’ll examine data lakehouse architecture, which, as you know, is not how to build a house on a lake, but an effective way to store, process, and analyze vast amounts of data. Here’s the breakdown: You’ll start with data storage, data connections, data types, and data structures. Then, you’ll move on to table schemas, along with batch and streaming data processing. Next, you’ll explore denormalized data, data governance, metadata, data catalogs, the key components of data lakehouse architecture, and more. Then, you’ll discover Dataplex and how it can be used to identify data sources in BigQuery. You’ll learn to identify and trace data sources, and how to access data libraries. Finally, you’ll explore data reference architectures, how to manage tables in BigQuery, add and export data, and query tables. You’ll learn to access data from Google Cloud services and manage a Dataproc cluster. Then, you’ll explore the benefits of data partitioning. These lessons are specifically designed to give you a solid foundation in organizing and structuring data, which any employer will appreciate! You’ve made a great start in your journey as a data analyst so far, but we’ve got lots more fun ahead. Let’s keep the momentum up and move on!

### Document - [Course 2 overview](https://www.cloudskillsboost.google/course_templates/962/documents/471990)

### Video - [Eric: Data analytics skills translate across industries and roles](https://www.cloudskillsboost.google/course_templates/962/video/471991)

- [YouTube: Eric: Data analytics skills translate across industries and roles](https://www.youtube.com/watch?v=uR51e2EAdBE)

Hi, I’m Eric! Here at Google I’m a product analyst focused on Google’s technical infrastructure. This means that I help Google use the many machines across its data warehouses as efficiently as possible. In other words, I improve the stuff that helps run Search, Gmail, and every other Google product. I'm excited to help you ramp up your analytics skills! The vast majority of my analytics and data science skills are self-taught. My educational background is actually in psychology and communications. I began my career in recruiting —I supported an internship program for a space company and represented the company at career fairs across the organization. Ultimately, I recognized that to recruit efficiently was actually an analytics problem. I needed to quantify HR data to answer questions like, what is most predictive of a candidate's success? And, what factors most contribute to an intern's employment satisfaction? I ultimately learned —mostly via searching online— optimal ways to measure and present that information. Fortunately, analytics techniques are often shared across many departments and industries. This has allowed me to bounce between industries, like aerospace, entertainment, and tech. And departments, like HR, real estate, finance, and product. These experiences enabled me to recognize how analytics concepts translate across business contexts. I started my analytics career with a social sciences undergrad degree. While statistics and computer science degrees are certainly helpful, they're not necessary if you're hungry enough to learn on your own. As people work to grow in the cloud data analytics space, there are a few things that were helpful for me that I recommend: First, start with clean data. An insight is only as valid as the data it’s derived from —before you start, make sure your data sources are correct and accurately represent the big picture. Also, while we work with numbers, analytics is really about storytelling. Don’t just focus on the numbers —focus how to communicate those numbers such that folks understand what they mean and how to respond.

### Document - [Helpful resources and tips](https://www.cloudskillsboost.google/course_templates/962/documents/471992)

### Document - [Lab technical tips](https://www.cloudskillsboost.google/course_templates/962/documents/471993)

### Document - [Explore your Course 2 scenario: TheLook eCommerce](https://www.cloudskillsboost.google/course_templates/962/documents/471994)

### Video - [Welcome to module 1](https://www.cloudskillsboost.google/course_templates/962/video/471995)

- [YouTube: Welcome to module 1](https://www.youtube.com/watch?v=PQPIYHniqgw)

Hey there! Are you ready to get moving? And, by that, I mean moving data, of course. This module will get into how data moves within and across systems. We’ll identify different types of data, their appropriate storage structures, and various data extraction methods, including ways to transfer data between systems. The first topic you’ll explore is modern data architectures. After that, you’ll review the concept of a data lakehouse and examine its benefits and challenges. Next, you’ll consider different data types, highlighting their key characteristics and considerations. Finally, you’ll gather essential techniques and strategies for handling both batch and streaming data sources. You’ll then be able to effectively manage and process data in diverse systems. I can’t wait to help you start this journey!

### Video - [Data storage and connections](https://www.cloudskillsboost.google/course_templates/962/video/471996)

- [YouTube: Data storage and connections](https://www.youtube.com/watch?v=a8kBUL4sBJg)

Picture this: you’re lounging, scrolling your favorite website and then, bam, you notice an ad for a product you’ve never thought about before, but now you have to have it. You know, something that exactly matches your interests and style, like a tea kettle disguised as houseplant. How did they know?! Most likely, a business has tracked data related to your past purchases to send you ads, offers, and coupons that will make sure you come back again and again. It can be mind-boggling to consider how perfectly that ad figured out what you, personally, would like. Now, say you work as a data analyst for a company that has millions of different customers. The company needs to know a lot about their customers’ likes and needs to recommend the right product to each customer. How can companies manage all this data? And how do they handle multiple sources and formats? A key piece is data storage, and how data is connected across systems. And that’s what we’re going to explore in this video! Cloud data storage is a solution that enables organizations to keep, access, and maintain digital data in off-site, cloud-based servers that they don’t need to own or manage. The data can include files, business data, videos, images, and tons more. In the context of these data storage tasks, data analysts generally focus on input and output data. Input data is any information that a user sends to a computer, like the text you type in a document, the information you enter when filling out an online form, or an image you scan with a scanner. Conversely, output data is any information that a computer sends to a user. Some examples include a spreadsheet outputting a pie chart, or a photo editing program outputting a PDF, or an online store outputting a sales receipt or order summary. Data storage systems can be used to store both input and output data. Some common storage systems you might want to connect to in order to access data might be systems of record, transactional databases, and cloud storage. A system of record is a data-storage system that serves as the source of truth for an organization’s data related to processes or systems. This includes any proprietary information and data used to maintain compliance. All of the data associated with these systems is uniform in a system of record. The system of record is the authoritative data source for data related to the process or system which it supports. Next are transactional databases. A transactional database is a data-storage system that stores each transaction, or interaction, and their fields as individual rows. Transactional databases are common in ecommerce, online banking, and other businesses that may record customer transactions. A transactional database can also be one of your systems of record. For example, imagine you work for a company that sells clothing and equipment for outdoor enthusiasts. Each time a customer purchases a pair of hiking boots, a new row is created in your transactional database. The row contains the customer name, the inventory name of the boots, the date of purchase, and the sales price. All of the rows together make up the transactional database. That’s a mountain of data! Finally, there’s cloud data storage, which is a solution that enables organizations to keep, access, and maintain digital data on off-site, cloud-based servers. A cloud provider hosts and securely stores the data, making it easily scalable when you need to increase or decrease capacity. So, it’s very likely that you’ll be working with multiple sources of data in your cloud career. And to make sure that all of the data can work together, you’ll want to be able to connect to all of your data sources. A data connection is a link formed between a data source and another tool in order to access the data source from the tool. In order to view and interact with your data, you will want to work with a tool that allows you to easily connect to all your data systems from a single place. Usually this is a business intelligence or analytics tool from which you want to be able to connect to all your business’s data in order to create dashboards, reports, pivot tables, or other types of analyses. These tools provide data connectors to let you access the data you might have in transactional databases, cloud storage, or other storage systems. Now you know the essential tools you’ll use to gather, store, and access data. And you’ve learned about how data connections help keep everything working smoothly together. With this know-how, you’ll be ready to find and transport your well-organized data!

### Video - [Gerrit: Experience with a variety of tools can help you as an analyst](https://www.cloudskillsboost.google/course_templates/962/video/471997)

- [YouTube: Gerrit: Experience with a variety of tools can help you as an analyst](https://www.youtube.com/watch?v=t-7lIDE7178)

I think data analytics has a profound impact on the world. Yeah, I think it ultimately helps us to truly capitalize on the potential that digitization has and really helps us getting to a better, more equitable, more sustainable world. Hey, I'm Gerrit, I'm the Head of Data Analytics at Google, and I'm building services that process and analyzes large amounts of data. Data analytics is really about helping our customers to unlock the value from the data that they are having. So you can think about data like all of those secrets and hidden patterns that surround us every day. We want our customers to be able to understand them, see them, and ultimately harness them to drive business value for them, improve their customer experience, or ultimately just take better decisions quicker. The beginning of my career was in software development. I started as a junior software developer in a database development team, and learned from the very first lines of code that data processing systems are awesome. I studied computer science at college, yes. One of the biggest lessons though of my life is that learning is not an institution. You know, learning is an attitude when you at work that of course, you know, doing your job, but also having an interest in all of the areas outside of it and trying really to get a very broad skills profile. Because once you progress in your career, you really draw on the breadth of your experiences, on the breadth of your knowledge much more than on something that you have deeply specialized. So I think there is a big opportunity in learning broadly and wildly, even outside of the regular career and work paths. I think the most important part is to really appreciate learning and appreciate what learning is. You know, learning is not applying. Learning is not being proficient. You know, learning can be very frustrating actually, and it's hard. But once you understand that it's a hard and rewarding and sometimes, you know, lengthy process and accept that, you can make it, I think really a part of your life and of your life experience in a very positive way. Data analytics and cloud really go hand in hand. Data is vast. There is no limit that we conceive to how big data can get. It's being generated every day. It's generated in the greasing rates and the cloud actually provides the capacity. It provides the systems that actually can store and process these vast amounts of data in an efficient and even in a conceivable way. So you do need both, right? You do need to have the digitization of everything, and you need to cloud as the foundational infrastructure to process that data. So I think rather than limiting to, I wanna learn one language or I wanna learn one technology stack, I would really recommend learning and appreciating the breadth of Google's Cloud. Feeling I'm prepared is part of the journey to growth. I think it's quite normal to feel unprepared or overwhelmed when you get into a new job or into a new opportunity. You know, I think what everyone experiences, what I have experienced all throughout my career, and it's really the trigger point, right? It's basically telling you that, hey, here's something interesting. Here's something new to learn. Once you appreciate for what it is, you know, really growing yourself, it kind of gets real fun and exciting.

### Video - [Common ways to store data](https://www.cloudskillsboost.google/course_templates/962/video/471998)

- [YouTube: Common ways to store data](https://www.youtube.com/watch?v=dF6e0MCrg74)

Hi there! It’s great to have you here. Thanks for joining me in this video about getting data organized! This is a really important topic —and not just because data organization is a big part of the data profession. It will help you be more time savvy, too! How much data are we talking? An astounding 2.5 quintillion bytes of data. That’s 2.5 billion gigabytes. Or, to help you comprehend this better, that’s roughly 10 million laptops’ worth of storage space generated globally per day. Wow. It's no surprise that effective data organization is so valued by analysts and their organizations! All right, let’s begin exploring the different types of data storage tools. These include the relational database, data warehouse, data mart, CSV file, and data lake. Each of these has benefits and drawbacks, depending on the type of data being stored. So, as a data analyst, it’s helpful to understand which option is best for whatever type of data project you’re working on. First, the relational database. A relational database is a database that contains a series of tables that can be connected to form relationships. Relational databases are frequently used in businesses of all sizes to manage their data because they allow users to perform complex queries on data from multiple tables at the same time. Then, there is the data warehouse. The data warehouse is a database that consolidates data from multiple source systems for data consistency, accuracy, and efficient access. They’re optimized for analytical queries, making them ideal for organizations that require complex data analysis. Next up, the data mart. The data mart is a subject-oriented database that can be a subset of a larger data warehouse. Being subject-oriented just means that it’s associated with a specific area of interest or department of a business. This makes data marts useful for business intelligence and reporting because they provide easy access to specific data subsets. Next is the CSV, which stands for comma-separated values. A CSV file is a delimited text file that uses a comma to separate field values. While CSV files are a simple way to store tabular data, like in a spreadsheet, they lack scalability and are ineffective when working with large or complex datasets. Finally, the data lake. The data lake is a database system that stores large amounts of raw data in its original format until it’s needed. Data lakes are a versatile solution when you’re working with diverse sources and data types. Data lakes allow you to process data without restriction to size and format. But no matter what data you encounter, managing and storing it effectively is crucial to making sense of it —and making smart, data-driven decisions!

### Document - [[Supplemental] Common data storage systems](https://www.cloudskillsboost.google/course_templates/962/documents/471999)

### Document - [AI-based predictive data management](https://www.cloudskillsboost.google/course_templates/962/documents/472000)

### Video - [Structured, unstructured, and semi-structured data](https://www.cloudskillsboost.google/course_templates/962/video/472001)

- [YouTube: Structured, unstructured, and semi-structured data](https://www.youtube.com/watch?v=q6lbgMGt9ss)

Welcome back, data devotees! Today we’ll be discussing a crucial part of your data career: the main ways in which data is stored. These include structured, semi-structured, and unstructured data. Being able to recognize these three main ways of storing data will set you up for success when it comes to finding and querying the right data for each particular project you work on. Ok, first let’s start with some definitions. Then, we’ll take a deeper dive into how to store different types of data. Structured data is data organized in a certain format, like rows and columns. For example, you might find structured data in a table or spreadsheet. This makes it easy to understand and process by computers. Structured data is easily stored in traditional databases, like a SQL database. One of the essential features of a SQL database is its ability to establish relationships among tables. For two tables to have a relationship, one or more of the same columns must exist inside both tables. These columns are called keys. There are two types of keys. A primary key and a foreign key. Each table has a primary key, which is an identifier that references a column in which each value is unique. You can think of a primary key as the unique identifier for each row in a table. As an analyst, you may need to create tables. If you do decide to include a primary key, it should be unique, meaning no two rows can have the same primary key. Also, it cannot be null or blank. There are also foreign keys. A foreign key is a column within a table that is a primary key in another table. In other words, a foreign key is how one table can be connected to another. That’s what makes it possible to establish the relationships among the tables. Pro tip: It’s important to keep in mind that more than one foreign key is allowed to exist in a table. With these keys, users can combine data from multiple tables and query them simultaneously, making it easier to analyze complex relationships and gain valuable data insights. Next, let’s check out semi-structured and unstructured data. Unstructured data is data that is not organized in any easily identifiable way. It lacks a predefined structure and can include text, images, videos, and more. Examples of unstructured data include social media posts and audio recordings. Unstructured data is valuable because it can be used for such a wide variety of purposes. But, it’s even more challenging to process and interpret. For example, cloud-based data lakes are well-suited to unstructured data. Some of these tools include Google Cloud Storage, or GCS, and Google Cloud DataProc. They don't require a fixed schema. Instead, they store data as documents or files, and this can include any number of fields and nested data structures, or no structure whatsoever. This allows a data analyst to quickly store, search, retrieve, and analyze data as needed no matter the data format. It’s also easier to modify the data without impacting the application. Imagine a social media app. If a user posts a text caption along with a video, both that text and video file are examples of unstructured data that cannot be mapped to a predefined structure since they could contain anything. Finally, now that you have an understanding of structured and unstructured data, let’s move onto semi-structured data. Semi-structured data is data that has some structure but is not as precisely organized as structured data. This means it’s more flexible than structured data, which makes it great for storing data that might not fit into a traditional table or spreadsheet format. One common example of semi-structured data is email. Email messages have a header that contains structured data, such as the sender and recipient names, subject, date, and time. This structured data makes it easy to find and store email messages. But the body of the email is unstructured and can contain a variety of data types, including text of various lengths, images, links, and attachments. This makes emails flexible and easy to use for a range of tasks. In your data career, you’ll come across tons of different data sources. They’re the true foundation of any data-driven decision-making process. And that’s why it’s crucial to be able to recognize the various data structures, so you can determine the best way to get that data moving! Great work tackling this important topic.

### Document - [Overview of data lakehouse architecture](https://www.cloudskillsboost.google/course_templates/962/documents/472002)

### Video - [Example of a data lakehouse](https://www.cloudskillsboost.google/course_templates/962/video/472003)

- [YouTube: Example of a data lakehouse](https://www.youtube.com/watch?v=uCdxwZ3uoi8)

In the early days of data analytics, as businesses started collecting and analyzing large amounts of data, the term “data warehouse” emerged to describe the architectural concept used to store and organize this data. The analogy of a “warehouse” illustrates the idea of a centralized, structured, and organized storage space where a data analyst can efficiently manage data and make it accessible for analysis. Then as businesses accumulated vast amounts of data from various sources, including unstructured and semi-structured data like text documents, images, videos, social media posts, and sensor data, traditional data warehouses faced challenges in efficiently storing and processing this data. To address these limitations and accommodate the diverse and rapidly growing data, the concept of “data lake" emerged. In this video, you’ll explore an example of a data lakehouse and witness just one way they’re revolutionizing the way data is managed and analyzed. A data lakehouse is a hybrid data architecture that combines the features of a data lake with those of a data warehouse. This creates a unified platform for storing, processing, and analyzing vast amounts of structured and unstructured data from diverse sources. By leveraging the strengths of both data warehouses and data lakes, a data lakehouse offers improved data management capabilities, seamless integration with analytics tools, scalability, and the flexibility to efficiently handle various workloads. Let’s examine this with a business example from an agricultural company. The business has a data infrastructure which includes an on-site data warehouse and a cloud-based data lake. Hosting data in separate infrastructures is hindering the ability to adapt to changing business needs and extract timely insights. To potentially resolve the issue, the data analytics team wants to migrate to a data lakehouse. They initiate a comprehensive migration plan. It begins with assessing their existing data infrastructure and identifying any pain points and bottlenecks. It turns out that their traditional data warehouse is struggling to handle the growing data volumes and diverse data types. There are JSON documents, supplier call recordings, and free text from vendor and partner surveys. Storing and processing hundreds of terabytes of data is going very slowly and sometimes overwhelms the servers to the point of crashing, or breaking the system. At the same time, the company’s data lake lacks the necessary structure and governance mechanisms. Data analysts are struggling to understand if the data has quality issues, like blank fields and product categories that don’t match other tables. Data discovery, integration, and quality control are significant challenges, impeding efficient data analysis and decision-making. The company’s data team then implements the data lakehouse to unlock the true potential of their data. Now, they can store and process data in its raw, unaltered form, while providing necessary structure and organization for efficient analysis. Plus, they can scale resources on-demand, ensuring optimal performance during peak data processing periods. The data lakehouse also addresses their data governance concerns. With defined data schemas, access controls, and security measures, they establish strict policies and adhere to compliance requirements. Things are going so well that the company decides to take it a step further and tap into the cloud's analytical capabilities. They integrate advanced analytics and machine learning tools into their data lakehouse architecture, enabling them to uncover hidden patterns, perform predictive modeling, and generate actionable insights. Their data scientists and analysts explore vast amounts of data, experiment with different algorithms, and drive innovation. This company now enjoys significantly better agility when creating reports, dashboards, and queries. Teams can respond more quickly to market changes and customer demands. And the enhanced data governance framework instills confidence in their stakeholders, ensuring the privacy and security of sensitive information. Through the strategic move to a data lakehouse from a data warehouse, the company has harnessed the full potential of its data and empowered its people. And now you'rn empowered with the knowledge of data lakehouses. Great work!

### Document - [Comparison of data warehouses and data lakehouses](https://www.cloudskillsboost.google/course_templates/962/documents/472004)

### Quiz - [Test your knowledge: Data storage options](https://www.cloudskillsboost.google/course_templates/962/quizzes/472005)

### Video - [Aspects of table schema](https://www.cloudskillsboost.google/course_templates/962/video/472006)

- [YouTube: Aspects of table schema](https://www.youtube.com/watch?v=FSACShd9m1o)

Schemas are powerful tools that help people make sense of the world. Schemas provide a way of describing how something is organized, and people use them constantly, usually without even knowing it! For example, you may use a schema when planning to visit a new museum. The schema for a museum might include a description of the different types of art on display, the size of the museum, the entry fee, and the location. All of this information can help you understand what to expect when you get there. Schemas are also an essential part of understanding tables. In this video, you’ll explore table schemas in BigQuery. There are several parts of a table schema that provide important information essential for understanding a table and its columns: the column name, the data type, and the mode. Let's explore each one. First, let’s start with the First, let’s start with the column name. Each column in a BigQuery table has a unique identifier called a column name. You use column names to refer to columns in queries and to access the data in the columns. Column names can contain letters, numbers, and underscores, and they must start with a letter or underscore. It is also important that each column name is unique! This means that each column name needs to be different from other column names so that the columns are easy to find and use. Next, let’s move on to the data type. The data type of a column specifies the kind of data that can be stored in the column. For example, a column with a data type of string can store text data, while a column with a data type of integer can store numeric data. The exact data types you can use will depend on your cloud tool. BigQuery supports a wide range of data types, including numbers, strings, date and time, location, time interval, JSON objects, struct, and array. Number types can be used to store numeric data, which is any data that can be represented as a number. This includes integers, floating-point numbers, and decimals. Common number types are numeric, integer, and float64. String types in BigQuery can be used to store any sequence of characters, including text data, such as names, addresses, and phone numbers as strings as well as binary data as bytes. While numbers and strings are the most common, there are other data types you can use in BigQuery. Date and time types, like date and datetime, can be used to store date and time data, such as order dates and shipping times. Location types, like geography, can be used to store geographic coordinates, such as latitude and longitude. Time interval types, like interval, can be used to store periods of time, such as how long it took to fulfill an order or how long a customer was on the phone with support. BigQuery also has two main complex data types: arrays and structs. Arrays are called repeated columns in BigQuery, which means they can store multiple values of the same data type in the same column. Structs are called record and they can store multiple values of different data types in the same column. In BigQuery, you can also use JSON. JSON objects can be used to store key-value pairs of data. Now, let’s discuss the mode. The mode is another important part of a schema. The mode tells you whether the column can contain empty values. A nullable column can contain empty values, while a required column cannot. Mode is also important because you can turn a column into an array by setting the mode to "repeated." By looking at the column names, data types, and mode for each column in a schema, you can learn how the data in the table is structured. This will help you write queries, group data, create tables, and share the data with others. Keep practicing using BigQuery to explore table schemas and how they organize data into logical groups to streamline your workflows. You're well on your way to becoming a BigQuery schema data pro!

### Document - [Overview of BigQuery's schema editing abilities](https://www.cloudskillsboost.google/course_templates/962/documents/472007)

### Document - [Components of BigQuery table schema](https://www.cloudskillsboost.google/course_templates/962/documents/472008)

### Document - [Complex data types in BigQuery](https://www.cloudskillsboost.google/course_templates/962/documents/472009)

### Video - [Introduction to nested data structure](https://www.cloudskillsboost.google/course_templates/962/video/472010)

- [YouTube: Introduction to nested data structure](https://www.youtube.com/watch?v=Cs3Gm_DXdY4)

Hello, data virtuoso! Thanks so much for being here to learn about nested data structures! Understanding the concept of nested data is super-important to the field of data analytics. The good news is that you likely already have a basic understanding of something that’s nested. Have you ever reviewed a company’s organizational chart? In this nested structure, the company is the main object or structure and the departments are the substructures contained within it. Those departments further subdivide into teams and the teams further subdivide to individual employees! A website is another everyday example. The homepage is the main object with the other pages being the substructures. Each page further subdivides into sections and the sections further subdivide into different types of content. Similarly, a nested data structure is a structure that organizes data within other data structures, forming a hierarchy of information. They provide a highly effective way to store, process, and retrieve information. This allows a data analyst to ensure data integrity and facilitate efficient data management. Nested data also plays a major role in representing complex associations, like parent-child relationships or multi-level categorizations. By nesting data, it’s possible to capture these connections and dependencies, enabling more advanced data analysis. There are various types of nested data used in data processing, but not all tools support all data structures. Two common data structures used for nested data include struct and arrays. A struct is a way to group multiple columns together. A struct is called a record in BigQuery. This is useful when you want to organize data that is part of a common thing. For example, imagine you have a group of columns that each contain one important piece of the information about a customer’s address. To group this data together, you can use a struct called address. In this address struct you can nest the related columns, like the first and second line of the address, the city name, state or province, postal code, and country. By grouping the related columns together in a single column using a struct, you can make it easier to find and query the customer data. An array is a list that contains values of the same data type. An array is called repeated in BigQuery. Arrays can be used to store a range of data types such as numbers, strings, structs, or even other arrays. When you use an array, you can store an entire list of data in a single column, as long as they are the same data type. For example, you can use an array to store a list of email addresses from a customer in one place. Arrays can even store nested data! An array of structs is a list of structs stored in a single column. This is especially useful for storing lists of complex, repeated items, such as different addresses. For example, each customer may have multiple addresses for different purposes, such as shipping, billing, home, and work addresses. By grouping all customer addresses together in an array of structs, you can keep all the customer’s address information organized and easier to access when querying. Each type of nested data has its unique characteristics and use cases, providing versatile tools to manage and analyze complex datasets. Additionally, patterns within the data —like recurring structures or hierarchical relationships— can help you decide when using a nested data structure may help make it easier to work with the data. Now that you can identify nested data structures, you can start to understand how the different parts of the data relate to each other. This is important because querying nested data is different from querying regular columns. To write queries that work with nested data, you need to know how to recognize nested structures and how the nested data elements work. Just like a business’ organizational chart helps you locate where your data team members fit in the organization and a website homepage links to its many pages to help you find the content you need, you can identify and understand nested data structures to help you pinpoint the right information!

### Document - [Guide to BigQuery](https://www.cloudskillsboost.google/course_templates/962/documents/472011)

### Lab - [Explore flat and nested data types in BigQuery](https://www.cloudskillsboost.google/course_templates/962/labs/472012)

Explore flat and nested data types in BigQuery and learn how to use these data types effectively in queries

- [ ] [Explore flat and nested data types in BigQuery](../labs/Explore-flat-and-nested-data-types-in-BigQuery.md)

### Quiz - [Test your knowledge: Data types and organization in BigQuery](https://www.cloudskillsboost.google/course_templates/962/quizzes/472013)

### Video - [Overview of data processing methods](https://www.cloudskillsboost.google/course_templates/962/video/472014)

- [YouTube: Overview of data processing methods](https://www.youtube.com/watch?v=2Lau5ftPRrs)

Imagine observing, from the rooftop of a building, the intersection of two bustling roads. One represents the world of batch data, where information is collected, processed, and then delivered in regular, steady intervals. Just like the cars waiting at the light before zooming down the road, batch processing enables data professionals to collect large volumes of data over a period of time, then process it all at once. The other road represents fast-paced, streaming data. Streaming data is processed right when it’s received, like the constant flow of traffic. Now, please ride along with me on this road trip to catch the sights of the fascinating landscape of batch and streaming data! Let’s begin with batch processing. Batch processing is a method of collecting large volumes of data over a period of time, then processing it all at once. Batch processing uses various data sources and each has their own characteristics and advantages. The most common formats include CSV, or comma-separated values, found in spreadsheets and many types of databases; JSON, or Java Script Object Notation, which provides easy-to-read representation of complex data structures; and Parquet, a columnar storage format for quick compression and improved query performance. The frequency of batch data processing varies based on the specific use case, ranging from hourly, to daily, to weekly, and beyond. Now, on the other hand, streaming data processing is a method of processing data as it’s received. It might come from a device in the internet of things, or IOT, a social media feed, and more. The frequency of streaming data processing is typically in real-time or near-real-time. With streaming data, you’ll encounter file formats and systems specifically designed to handle velocity and agility. One is Avro, which is a file format that helps programs understand and share data using schemas and seamless handling of data structure changes. There’s Apache Kafka, an open-source distributed event streaming platform that allows you to publish, subscribe, store, and process streams of records. This is particularly useful for building data pipelines and applications that require handling large amounts of data. And finally, there’s Apache NiFi, an open-source data integration and data flow automation tool which is particularly useful for collecting, transforming, and moving large volumes of data. Both batch and streaming data processing have advantages and disadvantages, depending on the use case. Batch processing is great for handling large volumes of data efficiently, like in data warehousing and analytics. It also enables organizations to perform complex analytical operations —like aggregations, transformations, and statistical calculations— on huge datasets. And it provides the necessary time window to process historical data and generate insights for long-term planning and decision-making. However, it takes time to generate these insights. Conversely, streaming data processing is instrumental in near real-time analytics and monitoring scenarios where timely insights and continuous monitoring are vital. Organizations use it to monitor live data streams to detect anomalies, trends, or critical events in near real time. Streaming data processing supports implementing alert systems and triggers actions based on predefined conditions, enabling companies to respond swiftly to new situations. However, handling streaming data is often more complex and expensive. Selecting the right data processing paradigm is a critical decision that affects data processing efficiency and compatibility. To make the best choice, consider factors like data requirements and data characteristics, processing speed and latency constraints, and integration capabilities with existing systems. As a cloud data professional, it's crucial to evaluate the pros and cons of each format in different scenarios to make informed decisions about data processing strategies. With this understanding, you’ve now got what it takes to enhance your organization’s data processing efficiency and navigate with a tank full of potential data!

### Document - [Batch versus streaming data processing](https://www.cloudskillsboost.google/course_templates/962/documents/472015)

### Lab - [Identify different batch and streaming data sources](https://www.cloudskillsboost.google/course_templates/962/labs/472016)

Differentiate between batch and streaming data sources, formats, and frequency

- [ ] [Identify different batch and streaming data sources](../labs/Identify-different-batch-and-streaming-data-sources.md)

### Quiz - [Test your knowledge: Batch and streaming data sources](https://www.cloudskillsboost.google/course_templates/962/quizzes/472017)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/962/video/472018)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=tfBhEnoYlIs)

Wow! You’ve come to the end of this section all about data types, organization methods, and how data moves. You’ve learned a lot of valuable information that will be incredibly useful as you continue on your path in the cloud data profession! You began this section exploring data storage and connections, plus data types and structures. You then moved on to table schemas, and data processing methods. You were introduced to the concept of a data lakehouse, uncovering its advantages and challenges. And you investigated important aspects of modern data architectures, giving you an overview of the entire data transformation process. There have also been many new terms and definitions in this section of the course. Be sure to check out the glossary to make sure you are comfortable and confident with these concepts. Thanks for joining me. You’ve got this!

### Document - [Glossary terms from module 1](https://www.cloudskillsboost.google/course_templates/962/documents/472019)

### Quiz - [Module 1 challenge](https://www.cloudskillsboost.google/course_templates/962/quizzes/472020)

## Key components of data organization

In this module, you'll explore key components of data governance, normalized and star schemas, data catalogs, and the data lakehouse architecture. You'll also understand how these concepts relate to modern data management and how they can be applied in practice.

### Video - [Welcome to module 2](https://www.cloudskillsboost.google/course_templates/962/video/472021)

- [YouTube: Welcome to module 2](https://www.youtube.com/watch?v=VKBpcCxTtlo)

Hey there, and welcome to another exciting section of the program! I’m really excited to dive deeper into more ways you, as a data professional, can build your skills. You’ll start by learning more about key elements of normalized and denormalized data. Next, you’ll explore data governance, technical and business metadata, and master data management. Then, you’ll check out data catalogs and their different components and types. Finally, you’ll spend a bit more time with data lakehouses and get some great tips for implementation. There’s a lot to cover, and all of it’s specifically designed to give you a comprehensive understanding of data organization best practices. After completing these lessons, you’ll be able to easily define your role in both data organization and the entire data life cycle. Let’s get started!

### Video - [Denormalized data](https://www.cloudskillsboost.google/course_templates/962/video/472022)

- [YouTube: Denormalized data](https://www.youtube.com/watch?v=d8lkUde3HzM)

Hey there, data fan! Thanks for joining me for an in-depth examination of denormalized data. You’re about to learn some common denormalization techniques, but don't worry, this examination has no wrong answers! But first, let’s understand normalized data. Normalization organizes related fields into different tables, and maintains defined relationships between columns in these different tables. For example, one table can hold data related to employee information like employee ID, employee name, manager ID, and so on. Meanwhile another table can contain information about the department they work in. We can join these two tables together using the manager ID. When data is normalized like this, it’s generally easier to avoid duplicate data and inconsistencies, and apply updates. On the other hand, denormalized data stores repeated information in one or more tables. Denormalized data is ideal for gathering information quickly. It reduces overall complexity and it creates the ability to scale quickly. But denormalized data has disadvantages like duplicated data, increased storage needs, and the potential for inconsistencies. There are different ways to denormalize data, which improves query performance. The most common way is to add duplicate columns in each table. Then a data analyst doesn’t need to join the tables themselves. Other methods are to split tables into smaller ones that only have rows or columns that each application needs, mirror tables by making copies of tables for easier reading, or create summary tables that hold ready-made totals like counts or averages. Let’s check this out with an example. A data analyst named Juliana works for a sports company that developed a sports training application. When customers purchase the application they provide information like email addresses and phone numbers. Juliana’s company stores this data in several tables. Now the company has a new business objective to offer their customers an option to share their activity time with the company. The company can then learn more about their customer’s sport activity times. And, as a data analyst, Juliana stores this new time data in a new table. While her team can search customer data like names, email addresses, and activity times, Juliana would have to join the tables to find all three pieces of information at once. So, the company can consider their options on how to structure the data —and whether to move from normalized to denormalized— based on the business goals. This may include additional application features, how fast the data team needs to retrieve data, and the different ways the data team uses the data.

### Document - [Normalized and denormalized data](https://www.cloudskillsboost.google/course_templates/962/documents/472023)

### Quiz - [Test your knowledge: Ways to organize data](https://www.cloudskillsboost.google/course_templates/962/quizzes/472024)

### Video - [Data governance for effective data management](https://www.cloudskillsboost.google/course_templates/962/video/472025)

- [YouTube: Data governance for effective data management](https://www.youtube.com/watch?v=xt33DV1nRDQ)

Our modern world is awash in data. It's everywhere we look from the news feeds we read to the websites we visit. This data can lead to incredible business outcomes, but it can also be overwhelming and difficult to manage. If companies aren’t careful, they can easily get lost, swimming in a sea of stormy data. To prepare you to navigate these complex waters, this video will explain data governance. This is a process for ensuring the formal management of a company’s data. It helps organizations maintain data accuracy, reliability, and security. Data governance consists of several essential elements that work in concert. These are data policies and standards, data quality management, data privacy and security, and data stewardship and ownership. Let’s check these out. Data policies and standards define the rules, guidelines, and procedures for data management within an organization. They establish the expectations for data handling, use, and sharing, ensuring consistency and adherence to best practices. Next, data quality management focuses on maintaining the accuracy, completeness, consistency, and reliability of data. It involves processes for data cleaning, validation, and ongoing monitoring to ensure that data meets predefined quality standards. Continuing on, data privacy and security are critical aspects of data governance. Activities here include implementing security protocols, access controls, and compliance with data protection laws. And lastly, data stewardship and ownership involves the assignment of roles and responsibilities for data management and oversight. It ensures that there are designated individuals or teams responsible for data governance activities. Data governance aims to achieve several important goals, which drive the effective management and use of data within an organization. First, it helps achieve data integrity and accuracy throughout the data lifecycle. By implementing data quality controls and validation processes, organizations can ensure that data is trustworthy and reliable. With the increasing number of data protection regulations, data governance also plays a crucial role in compliance. This enables organizations to establish policies and practices that align with regulations and protect privacy rights. Another key objective is to make data more accessible and usable for authorized users. This is made possible by implementing proper data classification, access controls, and metadata management. And data governance provides a foundation for making informed decisions based on reliable, timely, and accurate data. By enhancing the quality and integrity of their data, companies can realize more reliable insights and informed decision-making. This also leads to more robust security measures, reducing the risk of data breaches and privacy violations. This builds trust and can help protect the organization's reputation. With proper data governance, organizations can establish a culture of trust and credibility in their data-driven initiatives. And finally, data governance streamlines data management processes, reducing redundancies and inefficiencies. This leads to improved organizational efficiency and effectiveness when using data as a strategic asset. Maintaining data governance so that it stays on the right track is key to an organization’s growth and success. So, let’s go through a quick, suggested step-by-step guide on how to implement data governance. To begin, set clear goals and objectives for a data governance project. Then, get support from top leaders in the organization. After that, put together a data governance team with representatives from different departments like IT, legal, finance, and operations. Next, clearly define the roles and responsibilities of the data management team and ensure that they know and understand them. Finally, create a data governance plan that includes policies, procedures, standards, and guidelines for data management. Once the data governance plan is created, it’s time to invest in tools and technologies that will help automate and streamline data management processes. Train the data management team on how to use these tools and follow best practices. Keep track of how well the data governance plan is working with key performance indicators, or KPIs. Also, promote a data-driven culture where everyone values data and shares insights. And, the processes, policies, and technologies used in the data governance plan should grow and evolve with the organization. And that’s it for an overview of data governance! The ability to manage data assets is an essential skill for every data professional. The strategies you learned in this video will not only enable you to navigate a sea of data in your future role as a cloud data professional!

### Video - [MK: Risk management in a cloud-first world](https://www.cloudskillsboost.google/course_templates/962/video/472026)

- [YouTube: MK: Risk management in a cloud-first world](https://www.youtube.com/watch?v=ZujlUW7RC6c)

I was a special agent in the Federal Bureau of Investigation for 22 years. It was during that time in the FBI where I was assigned an investigation that included a cybersecurity element to it, and that's what I would call my pivot moment that really was the deciding factor for me. I need to invest time, cycles, develop my intelligence on this, take courses, do whatever I need to do in order to get smart on it. My name is MK Palmore. I'm a director in the office of the CISO for Google Cloud. My team is responsible for helping customers onboard safely and securely into the cloud. I'm a child of the '80s, and for those of us in Gen X that come from that generation everything in the '80s was really about the opening of the door to all things computers. Even families that, like mine, that were socially and economically disadvantaged, we even had opportunities to avail ourselves of computers and computing technology. And so this interest in computing and what it might evolve into, I think the seeds were planted. Cybersecurity's important in a cloud-first world because cybersecurity is the number one topic for business enterprise risk. I made a pointed decision on my own to change my career path and begin investing in the opportunity to learn and become educated in cybersecurity. The skill that I think is most important for cybersecurity practitioners is a willingness to understand that you don't know everything. I'm a huge supporter of certificate programs, certifications in the industry, because that is the pathway that I took in order to get educated. I feel that the more training that we can make available to a wider audience to give folks that opportunity to get educated in this field, I just think the better off we're all gonna be. Cloud is probably the most exciting industry to be in right now because the possibilities, I think, are endless. The benefits of cloud to business, to individuals, to society overall, I think that trajectory is still on an upward arc. So when you combine cloud with cybersecurity and you think about it in terms of the availability of jobs in the job market, this is a market that will continue to expand for years to come.

### Document - [Components and objectives of data governance](https://www.cloudskillsboost.google/course_templates/962/documents/472027)

### Video - [Introduction to master data management](https://www.cloudskillsboost.google/course_templates/962/video/472028)

- [YouTube: Introduction to master data management](https://www.youtube.com/watch?v=31cpwBWoV9Q)

Picture a data analytics team at a construction company. Every analyst has access to the organization’s main database, but some employees require keeping datasets stored on their local hard drives because they do not always have online access to the datasets. Unfortunately, when new data streams in about engineering plans or updates are made to their lumber and concrete inventory records, those employees with locally stored datasets will not have the updated inventory information. So over time, datasets will become outdated and error-prone. And when team members share data insights with stakeholders, there is a risk that results won’t line up with what other analysts communicated. This business needs help. It’s time for the data team to prioritize having a single source of truth. And that’s made possible through master data management. Master data management —or MDM— is a discipline and framework for managing and maintaining critical data assets in order to achieve a single, consistent view of data across the organization. These critical data assets, also known as master data, include core entities like users, products, locations, and employees. There are four key components that work together to ensure effective MDM. The first is data governance, which is a process for ensuring the formal management of a company’s data assets. Data governance establishes policies, standards, and guidelines for data management. It defines roles, responsibilities, and processes to ensure data integrity, security, and compliance. Second, data integration describes incorporating master data through different systems and applications. It ensures that data flows harmoniously, allowing different departments and systems to access and use up-to-date information from a single source of truth. Next, data quality management focuses on maintaining and improving the accuracy, completeness, and consistency of master data. It involves data cleaning, validation, and enrichment to ensure that data meets predefined quality standards. And lastly, data stewardship assigns responsibilities for data ownership, maintenance, and governance. Data stewards are accountable for ensuring data accuracy, resolving data-related issues, and promoting data governance best practices. MDM offers many essential business benefits. First, it improves data quality by establishing standardized rules, processes, and controls for data management. It ensures consistency in how data is captured, stored, and maintained across systems, eliminating duplicate or conflicting information. With MDM, you can trust your data is accurate, up-to-date, and consistent. Second, it empowers data-driven decision-making by providing a holistic and complete view of master data. Having this accurate, consistent, and reliable information brings about well-informed decisions that drive business growth and competitive advantage. Maybe most importantly, MDM creates that single source of truth. It eliminates data silos and discrepancies —like the ones happening at the construction company. By synchronizing data across systems, everyone accesses the newest, most accurate, and most reliable information. This leads to improved operational efficiency because employees can rely on the data they work with. Likewise, by streamlining data management processes and eliminating redundancies, MDM saves effort, money, and time spent on costly mistakes, rework, reconciliation, and manual data entry. By implementing and supporting a master data management framework at your organization, you’ll help your employer boost data quality and consistency, enhance accuracy, increase operational efficiency, and improve the bottom line. And that’s the truth!

### Quiz - [Test your knowledge: Data governance](https://www.cloudskillsboost.google/course_templates/962/quizzes/472029)

### Video - [Introduction to data catalogs](https://www.cloudskillsboost.google/course_templates/962/video/472030)

- [YouTube: Introduction to data catalogs](https://www.youtube.com/watch?v=3vS34Es7gB8)

Hello there, data fans! In a world where every click or tap leaves a data footprint at a website's door and a website visitor shares content like photos and video clips, an organization can end up collecting a vast amount of data to keep track of. This doesn’t even include the internal data an organization amasses like structured and unstructured data, reports, and more! In this video, you’ll learn all about data catalogs, a valuable tool for organizing data and getting data analysts on the right path to amazing insights. A data catalog is a centralized inventory of an organization's data assets. Think of it like a compass, guiding users through the data landscape and helping them reach the datasets and information they need. It provides a comprehensive view of available data, enabling users to easily discover, understand, and use data assets. This level of visibility and accessibility is the foundation of efficient and effective data management. This means that all of the data your organization owns can be cataloged, making it easy to search and to find the exact piece of data you need. Let’s say you work as a data analyst for a sports team. Your data catalog will include structured data, unstructured data, reports, dashboards and visualizations, and any automations and machine learning models you have created. What does this mean for how the sports team’s management builds and grows the team and tracks the data they collect for each player? Structured data includes all statistics on player performance collected over the team’s history. Unstructured data includes all of the news articles, photos, social media content, and video clips captured by the organization. Reports may include payroll reports and player performance reports, while dashboards and visualizations may focus specifically on player performance. Finally, automations and machine learning models include attempts to make predictions about player performance. You have an incredible amount of data at your fingertips, and you would love a way to be able to search it all to build a profile for each individual player. A data catalog can help you make this search possible. Let's explore the key elements of a data catalog that make a data catalog an essential tool for any data-driven organization. One of the key elements of a data catalog is metadata. Metadata —or data about data— is the key to unlocking the data’s true value. And data catalogs enable metadata management by organizing and storing important details about the datasets. Data catalogs also enable organizations to track the origin and transformation of data, ensuring reliability and accuracy. With the data lineage information supported by data catalogs, users follow the journey of data, identify any potential issues, and have confidence in its integrity. Data catalogs facilitate data governance by establishing policies, controls, and guidelines for data use, security, and privacy. Data catalogs advance collaboration by providing this centralized platform where users can work together on data assets and share insights. Okay! You’ve been working on a project to compile individual player data. You have had an easy time searching for a single player because the metadata allowed you to search the entire data catalog. Now, let’s move along to the many benefits of data catalogs. First, the comprehensive metadata provided in data catalogs can enhance data understanding, leading to more accurate analysis, decision-making, and business strategies. Second, users can quickly locate and access important data assets. This helps data teams be much more productive. And, by streamlining both metadata management and data discovery, data catalogs can decrease time spent on data preparation. Data catalogs also enforce governance practices and safeguard data integrity, privacy, and security. This ensures organizations meet compliance requirements. Finally, data catalogs create a wonderful culture of innovation because they empower people organization-wide to become more data-literate and, therefore, more confident sharing their ideas. I’m confident that you are discovering some great new ideas to share with your future data team as well! Data catalogs are just the beginning, so keep learning about how you can contribute to cloud data, no matter where your compass may lead you.

### Document - [Data catalog components](https://www.cloudskillsboost.google/course_templates/962/documents/472031)

### Video - [Technical and business metadata](https://www.cloudskillsboost.google/course_templates/962/video/472032)

- [YouTube: Technical and business metadata](https://www.youtube.com/watch?v=VSv549mF4Yc)

Every time you search the web, your search results are built around something called metadata. The simplest browser search can return vast amounts of information describing each of the web pages in your search results, including the page titles, brief descriptions, keywords, and the sources or authors. This is metadata. Metadata is just data about data. And in the world of metadata —just as with a web search— the possibilities are limitless! So get ready to discover the true potential of your data! Metadata describes, organizes, and contextualizes data, making it easier to understand and use. Metadata works behind the scenes, playing a critical role in ensuring data is searchable and meaningful by a data analyst. Metadata encompasses various attributes that describe data. It includes details about data structure, format, origin, quality, and meaning. Think of metadata as the what, when, where, and how providing crucial information about the data’s characteristics and properties. There are a couple of different types of metadata that data professionals are most likely to encounter in their work. First is technical metadata. This focuses on the detailed aspects of data, like file formats, structure, source, and lineage, which is where the data has moved throughout a system and how it has transformed over time. Technical metadata is the foundation for seamless data accessibility, flow, exchange, and integration, ensuring compatibility and interoperability across systems and applications. The second type is business metadata, which focuses on data’s context and meaning. Business metadata includes the meaning of the data, associated business rules, and information about data ownership and relationships. It provides the necessary context to understand the significance and relevance of data in the context of business objectives. Business metadata provides a business important context and enables data-driven decision-making. It helps bridge the gap between technical complexities and business requirements. These two types of metadata are complementary, together forming a comprehensive approach to managing and using data effectively. But it's still very important to be able to distinguish between them. So, keep in mind that technical metadata focuses on the details, while business metadata provides the business context and meaning. Thanks for joining me in this exploration of metadata. Data about data can feel a bit complex, so being able to use metadata to describe and understand information is going to be a valuable skill for your future cloud data career!

### Quiz - [Test your knowledge: Foundations of accessible data](https://www.cloudskillsboost.google/course_templates/962/quizzes/472033)

### Video - [Overview of data lakehouse architecture](https://www.cloudskillsboost.google/course_templates/962/video/472034)

- [YouTube: Overview of data lakehouse architecture](https://www.youtube.com/watch?v=BzyPOPnWOlo)

Hi there! Thanks for joining me in this video all about data lakehouse architecture. Here, you’ll review the history of data lakes and data lakehouses, and take a deeper dive into how data lakehouses can increase a data team’s agility when creating reports, dashboards, and queries. Now, you’ll first check out a quick history lesson. In the early days of data analytics, the term "data warehouse" emerged to describe the architecture used to store and organize data. This analogy likened the warehouse to a large, organized storage space. Companies need to store thousands of terabytes of data, including tricky unstructured data, like product reviews, social media posts, images, and near real-time data streams. As a cloud data professional, your organization may already have their data lakehouse in place. But it will help you to know about the selection and building of data lakehouse architecture so that you’re aware of how your databases work and how your data moves. An architecture is a framework that defines the design of a technical solution. This includes various components and technologies that are arranged to let organizations pool, share, and scale resources over a virtual network. For your purposes, a data warehouse architecture defines the data structure, from the data ingestion to the data presentation. Data ingestion is the process of obtaining, importing, and processing data for later use or storage. Before we explore lakehouse architectures, let’s start with the data warehouse. The architecture for a data warehouse relies on the idea that all data is routed to a single area: the data warehouse. The basic architecture of a data warehouse includes the data sources; the extract, transform, and load, or ETL process; ingestion into the data warehouse; and moving to visualization, business intelligence, and reporting programs. Now that you have a basic understanding of data warehouse architectures, consider what might happen in an organization with continuous streams of data. As technology advances and the internet becomes more common, the speed, volume, and variety of data increases exponentially. As the amount of data increases, traditional data warehouse limitations become apparent to the data team. To address these challenges, data teams developed data lakes. A direct contrast to the highly organized data warehouse, data lakes are unstructured, fluid, and vast. A data lake is a database system that stores large amounts of raw data in its original format until it’s needed. Data lakes are composed of cost-effective storage systems that accumulate vast amounts of data. Rather than carefully planned table structures with rows, columns, and limited data types, data lakes typically use a file system with folders that are named for each set of data. Data lakes make data ingestion easier and facilitate quicker access to data. But quality and consistency become more difficult to ensure within the unstructured data lake environment. Enter data lakehouse architecture, an approach that combines the best features of data warehouses and data lakes. A data lakehouse stores data in its native format, maintaining the accessibility benefits of a data lake. And a data lakehouse incorporates structured elements and organization, similar to a data warehouse. This ensures data quality and consistency. You’ll notice that some of the components and technologies in a data lakehouse architecture are also found in data warehouse architecture. We’ll cover the advantages of data lakehouse architecture in a moment. But first, the data lake architecture starts with all data and moves into the ETL process. From here, the data moves into the data lake. Then, it is transformed before moving into a data warehouse, a visualization program, or into a machine learning model. Next, here are some of the data lakehouse architecture advantages. First, it provides scalability, enabling businesses to store and process vast amounts of data without requiring additional hardware or software to transform it. Second, it offers flexibility, allowing companies to leverage various data tools and technologies without compatibility concerns. And it saves money on storage and processing costs. This sounds great, right? But what happens if you have a need for a structured data warehouse? You can have a two-tier architecture that includes both the data warehouse and a lakehouse. Data lakehouses empower organizations to store, process, and analyze data in a much more efficient way. This is only possible because data lakehouses are built on an effective data architecture. And by understanding that architecture, you’ve now added another building block to your own well-designed data career!

### Video - [Components of data lakehouse architecture](https://www.cloudskillsboost.google/course_templates/962/video/472035)

- [YouTube: Components of data lakehouse architecture](https://www.youtube.com/watch?v=Im2mOOJ7PPI)

Architecture is the art and science of designing buildings and other physical structures. It encompasses everything from conceptualizing initial design to overseeing construction. This video is all about architecture, not of buildings, but of data. In particular, data lakehouses! Let’s begin with the significance of data lakehouse architecture in modern data management. Data lakehouse architecture is a comprehensive approach to managing and analyzing data. Data lakehouse architecture combines a data lake with a data warehouse. The data lake is a repository of data in its original form. And the data warehouse is organized sets of structured data. The lakehouse allows you to combine both of these —structured and unstructured datasets— into a single platform. Since the lakehouse combines two data structures, the architecture is crucial to developing pipelines. An architecture is a framework that defines the design of a technical solution. The architecture of a data lakehouse combines two separate platforms. It also allows you to connect all of your tools to a single data source. There are five layers of a data lakehouse architecture to keep and organize all the data. These include ingestion, storage, metadata, application programming interface or API, and consumption. Think of these as the building blocks for any data lakehouse architecture! Let’s review an example of what goes into each of these layers! First, is the data sources layer. This includes structured, semi-structured, and unstructured data. Then, there is the ingestion layer, which can bring all of these data types into the data lakehouse, either through batch or streaming processes. Next up is the storage layer. In this layer, some data goes into the data lake, and other data goes through ETL and into the data warehouse. From here, our data goes through a metadata layer where governance rules are applied and indexing occurs. From here, APIs are used to abstract and simplify data and metadata access for the final layer. This is the consumption layer where business intelligence, visualizations, and machine learning occurs. As a data cloud professional, once you add the data to the data sources layer, let’s follow what happens in each of the other five layers in a little more detail next. The primary purpose of the ingestion layer is to pull data from the data sources layer and deliver it to the storage layer. In some cases, this is just using batch and streaming strategies to move data into the data lake. For structured data, you may implement an ETL structure to move data into a warehouse. By leveraging scalable storage, you can efficiently store structured, semi-structured, and unstructured data in the data lakehouse. This versatility allows you to seamlessly integrate diverse data sources and ensure that no valuable information is left behind. The metadata layer plays a major role in ensuring data integrity, privacy, and compliance. This includes data governance , metadata management, data quality, and security. One of the key strengths of the data lakehouse is the metadata layer. It catalogs all of your data, both structured and unstructured, and data in your data lake and your data warehouse! Next up is the API layer. This layer is designed to help you integrate APIs that process tasks faster. This is also the stage where you might integrate machine learning. Finally, there is the consumption layer. This layer provides you tools and interfaces to extract valuable insights from the business’ data. Business intelligence tools —including visualization dashboards and data querying interfaces— give you self-service analytics capabilities, so you can easily explore the data, create reports, and gain near real-time insights. Data lakehouse architecture offers a transformative approach to managing and maximizing data. It combines scalable storage, powerful compute capabilities, effective control, and user-friendly business intelligence layers. And your data lakehouse know-how will help you empower any future employer to make the most of each of these valuable features!

### Document - [Data lakehouse implementation best practices](https://www.cloudskillsboost.google/course_templates/962/documents/472036)

### Lab - [Explore a lakehouse](https://www.cloudskillsboost.google/course_templates/962/labs/472037)

Use a data lakehouse and combine data on GCS with BigQuery

- [ ] [Explore a lakehouse](../labs/Explore-a-lakehouse.md)

### Quiz - [Test your knowledge: Data lakehouse architecture](https://www.cloudskillsboost.google/course_templates/962/quizzes/472038)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/962/video/472039)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=Gx9ybN6b8v0)

Congratulations! You’ve reached the end of another section of this program! Through these topics, you explored data organization, data governance and data lakehouses, and the various aspects of data organization that can affect decision-making processes. First, you discovered the basics of normalized and denormalized data. Then, you were introduced to data governance, technical and business metadata, and master data management. Next, you explored the components and types of data catalogs. Last, you learned about the benefits of data lakehouse architecture and how a data lakehouse can transform an organization’s data management. Awesome job on your progress so far! Can’t wait to catch up with you again in the next section!

### Document - [Glossary terms from module 2](https://www.cloudskillsboost.google/course_templates/962/documents/472040)

### Quiz - [Module 2 challenge](https://www.cloudskillsboost.google/course_templates/962/quizzes/472041)

## Steps to find data

In this module, you'll find, trace, access, and store data with Google's cloud data storage and management tools, including BigQuery and Dataplex.

### Video - [Welcome to module 3](https://www.cloudskillsboost.google/course_templates/962/video/472042)

- [YouTube: Welcome to module 3](https://www.youtube.com/watch?v=izC6iBpkJEE)

Hello, and welcome to the next section of this course! I’m excited to guide you through these topics all about exploring and finding data! One of the coolest things you’re going to do is query large amounts of data in BigQuery. This is going to be incredibly valuable for your future career in cloud data analytics! In addition, you’ll discover proven methods for tracing data to its source. And you’ll learn how to access and explore data libraries. First, you’ll get right into an example of how to find data using BigQuery. This is essential. After all, you can’t analyze data if you can’t find it! Next, you’ll learn what it means to trace a data source and how to use data lineage, a feature that helps a cloud data professional follow their data’s journey through systems: where the data came from, where it went, and what changes were made to it along the way. Then, you'll get an introduction to Analytics Hub and all the tasks you can perform. Finally, you’ll explore data discovery, curation, and unification, as well as why they’re important in the cloud setting. Once you’ve finished this section, you'll have the know-how to effectively use these tools to find data. And I’ll find you in the next video!

### Video - [Ryan: Curiosity can help you understand and connect data](https://www.cloudskillsboost.google/course_templates/962/video/472043)

- [YouTube: Ryan: Curiosity can help you understand and connect data](https://www.youtube.com/watch?v=9zFBqlhBlcM)

What gets me excited about my role is being able to design some of the largest data systems in the world. Hey, I'm Ryan. I'm a Cloud Data Engineer here at Google Cloud. What I love about working in this field is peeling back the covers on technology and data systems and being able to understand what is actually driving decisions and driving the services that I use on a daily basis as an end user. As a kid, I was always curious about how things worked, and within my role as a cloud data engineer, I look at data and try to derive insights out of it. And with that curiosity that I developed as a kid, I'm able to apply that to my daily role, where I'm always looking for how to connect different datasets together to help derive better business value for customers. When I was a young software engineer, I was always met with resistance, with trying to push the boundaries within the organization that I was in. I was always interested in trying to find new ways of doing things, new technologies to apply to common problems. And throughout my career, I've sort of found my path in innovating along the way to actually satisfy that itch for curiosity and innovation. At the time, I was making a career transition into a cloud-based role, and in order to prepare for that role, I did a lot of reading tutorials, a lot of watching YouTube videos, and then I also built my website on the cloud itself to help me stand out from the rest of the applicants for the role that I was applying for, and that really helped me understand a little bit more about cloud services and helped me ramp up along the way. I feel like I have a large impact within my role because the customers that I work with are able to provide new features, new insights, and new data to their customers. I faced a significant challenge working with a customer in the health industry because the health industry has very high stakes, because the decisions that you make directly impact people's lives. We had to make sure that our design was very, very comprehensive and that we had exhausted all different points of failure, such that we were able to ensure that when we went live with the system, that we were very, very confident that we weren't gonna encounter any issues. The margin of error was very, very small. If we didn't deliver a notification to somebody who needed a notification at that time, they wouldn't be able to take the appropriate action, such as getting their child to the hospital or getting their child to a doctor. The experience helped me grow in my role because it taught me what it takes to deliver on a solution and to never give up. Ultimately, at the end of the day, you're not going to always have the answer from the beginning, and to be able to navigate through complexities and be able to learn along the way really taught me about perseverance. The advice that I'd give to someone starting within this career is to stay curious. There's always so much to learn, and so just having that natural growth mindset or that curiosity really helps you develop and become a better data engineer.

### Document - [How to find data using BigQuery](https://www.cloudskillsboost.google/course_templates/962/documents/472044)

### Video - [Data lineage and traceability](https://www.cloudskillsboost.google/course_templates/962/video/472045)

- [YouTube: Data lineage and traceability](https://www.youtube.com/watch?v=bOxfY4ZOAbw)

Humans are usually intrigued by finding out something’s origin or source. Sometimes, we investigate things to better appreciate their meaning and significance, like a cultural tradition or a work of art. Other times, we check things because we care about certain issues, like confirming our food was grown sustainably, or that the products we buy were manufactured ethically. Knowing where things come from is also important in the data world! This is because understanding a data source’s lineage that clearly shows where the data has moved throughout its life cycle and how it has transformed over time ensures its integrity and lets data professionals share insights with confidence. So, what exactly is a data source? A data source is the place where data was generated. Data sources can take various forms, including databases, application programming interfaces, file, or even streaming platforms. Databases usually hold structured sets of data and store and manage large volumes of data. Application programming interfaces or APIs, are used to gather data from external systems like social media feeds. Files can come in many types like Excel, CSV, or JSON files. These types enable the import and export of data between systems. Streaming platforms provide a steady flow of live, current data that can be collected and processed for near real-time analysis. For example, analyzing near real-time user interactions on a website. This wide variety of data sources can create challenges related to compatibility, integration, and understanding the data. Each one requires a unique approach, so it's important to understand what they are used for in order to use them most effectively. All right, so tracking lineage for data sources involves following data from its point of origin to its current state in order to better understand where it’s been and how it may have been affected along the journey. Determining lineage for a data source can also lead to better understanding of the context surrounding the data, provide traceability, and help audit its reliability and accuracy. And it helps data professionals ask better questions and make informed decisions. Maybe most importantly, data lineage provides a clear understanding of its authenticity, quality and integrity. And it helps organizations maintain control over their data assets. This is why data tracing is so important to data governance, compliance, and regulatory requirements. Just like reading the label on your food or researching a company before buying its products, data lineage and traceability provides critical transparency, accountability, and trust. And, as data really does shape our world, it’s essential for you as a cloud data professional to analyze and verify the data. In your data career, working with trusted data will help your business achieve better outcomes. Just one of the many reasons why data professionals are so incredible!

### Document - [Dataplex's data lineage feature](https://www.cloudskillsboost.google/course_templates/962/documents/472046)

### Document - [How to use the Dataplex data lineage feature](https://www.cloudskillsboost.google/course_templates/962/documents/472047)

### Quiz - [Test your knowledge: Strategies for understanding data sources](https://www.cloudskillsboost.google/course_templates/962/quizzes/472048)

### Video - [Introduction to Analytics Hub](https://www.cloudskillsboost.google/course_templates/962/video/472049)

- [YouTube: Introduction to Analytics Hub](https://www.youtube.com/watch?v=P0_AzrZWizk)

The world’s oldest, continually operating library is thought to be in Fez, Morocco. Opened in 859, its maze of rooms hold thousands of rare books and manuscripts containing information about people, philosophy, medicine, astronomy, physics, mathematics, law, and much more. Today, people visit this library from all around the world to glimpse these fascinating texts. Databases are kind of like “modern libraries.” Instead of being filled with books, they’re filled with data. And just as people visit a library to find books about anything and everything, databases enable users to locate diverse datasets. One such resource is Google Analytics Hub, a data exchange and “library” of internal and external assets. In this video, you'll learn all about Analytics Hub, its architecture, user tasks, and more. Let’s get going! First, just like you can use a library's catalog to search and check out books and other media, Analytics Hub organizes and secures data, then helps users find the data they need for an analytics project. Acting as a connector between producers and users, Analytics Hub provides a process of sharing and using data among organizations, while ensuring safety and privacy. Analytics Hub is built on a publish-and-subscribe model of BigQuery’s datasets. This means data producers, or publishers, make their datasets available to data consumers, or subscribers. The separation of compute and storage enables data producers to share data with many users without having to duplicate it. Data producers only pay for the space to store data, and consumers only pay when they access or run queries on the shared data. So, a data publisher first identifies the datasets they want to share in BigQuery, then creates a listing of the datasets in Analytics Hub. After the listings have been published, they manage the use of their shared datasets. After a subscriber browses Analytics Hub to find a dataset, they can subscribe to it. Then, a read-only link to the dataset is made available in the subscriber's BigQuery project. Now, they’re ready to query the data. There are three key data management features: shared dataset, data exchange, and listing. And the subscriber workflow has a fourth feature, called the linked dataset. Let’s check these out. Shared datasets are collections of data, tables, and views that data publishers share in BigQuery. Data subscribers receive a version, called a linked dataset. This is similar to any other BigQuery dataset, but it’s read-only. In this way, the dataset always remains the same, and the subscriber doesn’t have to pay to store it. Also, each piece of data is uniquely identified as a listing. These listings include a link to the dataset, a brief description, and related documentation. Data exchanges can either be open to every user or restricted to certain users, with exchange administrators managing who can access the data. By default, the exchanges are set to private. Now, let’s discuss what users can accomplish with Analytics Hub. There are four main types of users: publisher, subscriber, viewer, and administrator. First, an Analytics Hub publisher can generate income with instant data sharing within their organization or with network partners. It can also use listings to share data without creating duplicates. Also, an Analytics Hub publisher can build a catalog of data sources ready for analysis and create detailed permissions that ensure the data reaches the right users. And it can manage subscriptions to the listings. Next, there’s the Analytics Hub subscriber who can merge shared data with existing data. And when a user subscribes to a listing, a linked dataset is created in their BigQuery project, and they can use the built-in tools of BigQuery to analyze the data. Then, there’s the Analytics Hub viewer who can browse through datasets and ask for permission to use the shared data. And finally, there’s an Analytics Hub administrator who can create data exchanges that enable data sharing and give access permission to both data publishers and subscribers. Now, there are a few Analytics Hub limitations for publishers to be aware of when using Analytics Hub. We’ll discuss just a few now. First, if a publisher creates a listing for a shared dataset that’s encrypted for more control over key operations with a customer-managed encryption key, subscribers won’t have the cloud key to access the dataset. Second, there is a limit of 1,000 linked datasets to a shared dataset. And third, when creating a listing, a dataset with unsupported resources can’t be shared. For example, not all routines are supported in shared datasets. And there you have it! Analytics Hub connects those who create datasets with those who need datasets. Just like a library with thousands of interesting titles, Analytics Hub offers tons of well-managed and easily accessible datasets. As a cloud data professional, you’ll benefit from the process of being able to share and use data among organizations!

### Document - [Analytics Hub enables data sharing](https://www.cloudskillsboost.google/course_templates/962/documents/472050)

### Document - [How to use Analytics Hub](https://www.cloudskillsboost.google/course_templates/962/documents/472051)

### Quiz - [Test your knowledge: Tools for sharing data](https://www.cloudskillsboost.google/course_templates/962/quizzes/472052)

### Video - [Data discovery, curation, and unification](https://www.cloudskillsboost.google/course_templates/962/video/472053)

- [YouTube: Data discovery, curation, and unification](https://www.youtube.com/watch?v=kasSi0vT-f8)

In today's data-filled world, many industry professionals like to think of information as the “currency of business success.” But a big challenge is separating the data gems from the pebbles to find the insights that will empower effective analytics. In this video, you’ll learn how to make your own data sparkle with key tools and techniques for managing and maximizing data in the cloud. But first, some fundamentals. To lay the groundwork, we’ll need some data discovery. This is the process of finding and understanding data assets within a dataset or data ecosystem, and identifying relevant patterns and relationships. For data discovery to work well, you need the right tools, and you need to understand what a business wants to achieve. If you know what questions to ask and what insights to look for, you can help a business make the most out of their data. The next step is data curation. This is the process of selecting and organizing data to ensure it’s useful and of high quality. Curating the data creates a solid foundation for analysis and decision-making. Data curation is a critical part of the data analytics process because it also involves data preservation and access, which preserves data for future use and makes data available for those who need it. Then, data unification is the process of integrating data from diverse sources to create a consolidated view. Usually, data exists in different formats and locations, making it challenging to analyze comprehensively. Data unification brings together disparate data sources, harmonizes their structures, and enables holistic analysis. Data unification ensures that data analysts are working with a complete and single view of all the data, which leads to more accurate insights and better decision-making. All right, now let's explore the benefits of data discovery, curation, and unification in the cloud. The cloud offers unique advantages that can revolutionize your data management and analysis processes. One significant advantage is scalability. Resources can easily expand or contract based on data discovery, curation, and unification needs. When working with a small dataset or a massive data ecosystem, the cloud provides the flexibility to adapt to user requirements. Access is also a key benefit of cloud-based data management. Cloud solutions allow seamless access to data discovery, curation, and unification tools from anywhere, at any time. This supports collaboration and remote work while breaking down geographical barriers. When it comes to data discovery, curation, and unification, cloud-based technologies also bring powerful automation capabilities. Consider the process of data curation: In traditional data management, curating large datasets can be time-consuming and resource-intensive. But with cloud-based automation tools, these tasks are streamlined by automatically identifying and resolving quality issues, performing data transformation, and applying predefined rules and policies. This accelerates data management and analysis workflows. The cloud also provides security measures and compliance frameworks to protect data during discovery, curation, and unification. Cloud service providers prioritize data security and offer advanced encryption, access controls, and regular compliance audits. This ensures the confidentiality, integrity, and availability of your data. And, of course, all of these benefits combine to equal some serious cost-savings. And with all your organization’s cost-savings, you’ll have even more resources to dedicate toward finding more of those sparkly data gems!

### Document - [Overview of Dataplex](https://www.cloudskillsboost.google/course_templates/962/documents/472054)

### Video - [Benefits of using Dataplex](https://www.cloudskillsboost.google/course_templates/962/video/472055)

- [YouTube: Benefits of using Dataplex](https://www.youtube.com/watch?v=wR9Q1_MJVwg)

Have you ever read an ad that claimed a certain percentage of dentists recommend a particular chewing gum? Or that the majority of people in a taste test like the flavor of one pasta brand more than another? The numbers can be compelling and might even make consumers want to purchase these products. But statistics should always be regarded with caution. In particular, it’s essential to consider the context in which they exist. Without context, data can be both meaningless and misleading. Maybe the dentists who participated in the survey weren’t given the option to choose no chewing gum at all. Or maybe they were presented with just a few brands to choose from, rather than all the gum on the market. Likewise, the people taking the taste test can easily be led in a particular direction by the pasta company. For example, just displaying one dish in a more appetizing way than the others can influence a taster’s decision. Context is the condition in which something exists or happens. It’s all about the surrounding circumstances affecting the data, like the specific questions asked in the dentist’s survey and the way the pastas were presented. In the same way, identifying the context surrounding a dataset is crucial for meaningful and truthful analysis. Dataplex is a centralized data catalog that unifies search and data discovery, which helps data professionals understand the source of data. Dataplex simplifies the process of data source identification, as users can easily navigate through complex data environments to find just what they’re looking for and confirm that it’s reliable. Dataplex provides a centralized hub where users can explore, catalog, and manage data assets. They gain a comprehensive view of the data ecosystem, enabling informed decisions and valuable insights. Dataplex provides a user-friendly interface for exploring data sources within BigQuery. BigQuery allows users to store and analyze massive datasets. It offers fast query performance and scalability, making it ideal for handling large volumes of data. To use Dataplex to identify data sources in BigQuery, first, log in to the Dataplex platform. Dataplex scans many data sources automatically within Google Cloud so you can start exploring right away! So, you want to find a dataset. Click on the dataset tab and browse through the datasets that appear. If you already have a dataset in mind, start typing its name —or even just the first letters— in the search bar. Then click search. A list of datasets will appear, and you can easily select the one you want. You can also filter data assets based on different criteria. If you head over to the filter section, there are some cool options. If you want BigQuery datasets, then select BigQuery and datasets. Instantly, a list of BigQuery datasets will appear. But if you’re interested in tables, you can filter further by choosing BigQuery and tables, and you’ll get a list with just BigQuery tables. This is a quick and easy way to find exactly what you want. Once you’ve selected a data source, you can examine its metadata to better understand its structure. How the data is organized, stored, and connected, its schema —a blueprint of how the data is organized within the data source—, its lineage —a history of the data’s journey—, and other important details. Keep exploring the capabilities of Dataplex to make sure you know how to identify and select meaningful and trustworthy data sources. Whether your data analysis project is about chewing gum, pasta, or anything else, knowing the context of each and every dataset you use will be an essential part of your future career!

### Document - [How to search for data with BigQuery](https://www.cloudskillsboost.google/course_templates/962/documents/472056)

### Lab - [Navigate Dataplex](https://www.cloudskillsboost.google/course_templates/962/labs/472057)

Use dataplex to identify data sources in BigQuery and Dataproc

- [ ] [Navigate Dataplex](../labs/Navigate-Dataplex.md)

### Quiz - [Test your knowledge: Dataplex and BigQuery for accessing data](https://www.cloudskillsboost.google/course_templates/962/quizzes/472058)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/962/video/472059)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=FLc4BNWaseQ)

Check out the progress you’ve made! You’re almost done with this section of the course! In the previous section, you explored how to find data in BigQuery. Next, you explored what a data source is and how data lineage and traceability is useful. Then, you investigated Analytics Hub, its architecture, and common tasks that you can perform as an analytics hub user. Finally, you shifted to data discovery, curation, and unification. You also got a close-up view of Dataplex and the importance of identifying data context. Congratulations on another important milestone in your voyage to data analysis in the cloud!

### Document - [Glossary terms from module 3](https://www.cloudskillsboost.google/course_templates/962/documents/472060)

### Quiz - [Module 3 challenge](https://www.cloudskillsboost.google/course_templates/962/quizzes/472061)

## Techniques to access data

In this module, you'll describe how to interact with data tables and use optimization techniques to improve queries. You'll also learn how to use a cloud-based data lake or a cloud-based data warehouse to connect to data sources and determine how to answer business questions using the data.

### Video - [Welcome to module 4](https://www.cloudskillsboost.google/course_templates/962/video/472062)

- [YouTube: Welcome to module 4](https://www.youtube.com/watch?v=FT_01ZMS5Yw)

Hello there, data virtuoso! I’m so glad to be with you again. Together, we’re going to learn about accessing data using cloud tools! You’ll discover how to interact with data tables and use optimization techniques to improve your queries. You’ll also use cloud-based data lakes and data warehouses to link up with helpful data sources and answer important business questions. Ok, now let’s check out exactly what’s coming up. First, you’ll explore data patterns and how to create table schemas in BigQuery. Then, you’ll get an intro on tools for Google Cloud integration, like Vertex AI and Google Colab. You’ll even explore some fascinating machine learning concepts! Then, you’ll work with partitioned tables —which are large tables divided into smaller segments— and gain some great strategies to help you better understand the many benefits of partitioning. After that, you’ll discover how to manage partitioned tables with information about data subsets and query partitioned tables. Finally, you’ll end with an exploration of Dataproc for scripts, managing Dataproc clusters, and more! This section is full of information that will prepare you for the next exciting steps of your data career. So let’s get started!

### Video - [Methods for defining BigQuery table schemas](https://www.cloudskillsboost.google/course_templates/962/video/472063)

- [YouTube: Methods for defining BigQuery table schemas](https://www.youtube.com/watch?v=A31w5yritWY)

Hello, and welcome to this video about table schemas in BigQuery! Schemas are one of the most important tools for data professionals. They help describe how data is organized, which provides a lot of valuable clarity, context, and structure. And structuring data is an important part of ensuring that it’s usable. Here’s an example. Maybe a data team at a commercial real estate agency is tasked with compiling surveys from clients who have recently leased a commercial property. The inputs on the surveys include scaled responses. Clients rank the effectiveness of real estate agents from 1 to 5, or least effective to most effective. The survey also includes questions that have a true or false response option. And the clients are invited to add comments in open-response questions. When analysts begin to compile the data, they decide they want to import all of the data about every agent into a single table. To complete this import, they will need to create some sort of organization and structure for the data. They need a schema to create this table! A schema is a way of describing how something, like data, is organized. A database schema is a way of describing how an entire database is organized, including all of its tables and their relationships. A table schema is a way of describing how each individual table within a database is organized, such as its columns and data types. There are three primary ways that table schemas are created. First, as a data analyst, you can create the schema as you load the data into a table from the Google Cloud console. Then, you assign the schema either in the console. To design the schema in the console, provide each column’s name and data type. For each field, use the “Add field” option, and specify field name, type, and mode. You also need to know what columns the data has before you begin. Or, if creating a new table without any data, it’s important to have a plan for the columns the data will have as it’s collected. The second option you have to create a schema is to use schema auto detection. This is a feature in BigQuery that infers the schema based on CSV, JSON or Google Sheets data. Just import the existing data, and BigQuery will figure out the schema! To set up schema auto detection in BigQuery, select auto-detect schema when creating a table. Then, BigQuery determines the data type for each column. BigQuery determines the data type for each column by selecting a random file in the data source. Then, it scans up to five-hundred rows of data to create a representative sample. Once complete, BigQuery assigns a data type to each field based on those values. Pro tip: it’s always a good idea to check the Google Cloud console or the command-line to confirm the accuracy of the data types assigned to each field. All right, now the third option you have to specify a data schema is to use a JSON schema file. This file includes the Column Name, Column Mode, Column Data Type, and Column Description, and can be used when creating a table from the command line, but cannot be used from the console. To create a new table from JSON file with schema auto-detection, in the BigQuery Explorer pane, click “Create Table” in the “Dataset info” section. In the Source section, select “Google Cloud Storage” under the “Create table from” section. There, you can select your JSON file. Be sure to select JSONL under “File format.” If you’d like to auto-detect the schema, you’ll select “Auto-detect.” And that should do it! You should have your JSON data uploaded into BigQuery in no time! So, returning to the client surveys, it’s now time to ingest them. The data team decides to use schema auto-detection. They upload all of the data, and BigQuery creates column names for each of the datasets. But, after reviewing the results, the data team supervisor suggests having a bit more control over the column names. No problem! One of the analysts just recreates the schema with Google Cloud console. Perfect, now the schema is working great. The commercial real estate agency is able to easily understand agent performance and provide any helpful feedback or training to make the client experience even better. And now you know several ways to create a database schema, making your data analysis experience better, as well! You’re on your way to structuring data in a meaningful way, and you have more experience working in the Google Cloud console and BigQuery. Great work!

### Document - [Auto-detection of schemas in BigQuery](https://www.cloudskillsboost.google/course_templates/962/documents/472064)

### Video - [Basic SQL commands for querying data](https://www.cloudskillsboost.google/course_templates/962/video/472065)

- [YouTube: Basic SQL commands for querying data](https://www.youtube.com/watch?v=FRLQZZRXhV8)

Hello! Let’s start this section by thinking about language. One common aspect of many languages around the world is dialects, which are variations in a language that may present in grammar, vocabulary, or pronunciation! The data field also has dialects, especially when it comes to programming languages. Programming language dialects are typically very small differences that don't change the intrinsic nature of the code. One of the most popular programming languages is SQL, usually referred to as “sequel.” SQL works with many different databases, with some minor variations. So, learning the basics will give you a great foundation for programming with SQL and setting up database tables. And that’s what you’re going to learn in this video! Ok, to start, a SQL dialect is a version of SQL that’s unique to a specific database. It’s important to recognize that all SQL languages have the same basic structure as standard SQL. For the most part, the key commands are identical or very similar. But, there may be a few differences in the syntax across the various dialects, or small differences in the code. Syntax is the predetermined structure of a language that includes all required words, symbols, and punctuation, and their proper placement. Commands are an important part of SQL syntax. And as a data analyst, as you learn to construct SQL syntax, there are four basic commands that you need to know when working with data: retrieving information, selecting columns, sorting, and creating a table or database. Retrieving information involves pulling information from a table. Statements used to retrieve information include 'select', 'from' ,and 'where'. 'Select' identifies the needed information. This is done by selecting either individual columns or all columns. 'From' identifies which table to use. And 'where' adds conditions on the data being returned. For example, requesting only data that contains a certain value, or only data with a particular keyword. Now we select columns. This requires both 'select' and 'from' statements. 'Select' identifies the column and 'from' identifies the table that the column should come from. Great! Now, we’ve come to sorting. To sort data, use the same first two statements —'select' and 'from'— to identify the column of data to be retrieved. Then, the 'order by' statement specifies which column to sort by. 'Order by' will automatically sort in ascending order. If the 'desc' command is added, 'order by' will then sort in descending order. Finally, let’s check out how to create a table using SQL. To do this, type 'create table'. Then, list all of the column names and describe the type of data to be found in each column. The data type of a column defines the kind of data that can be stored in it. This is important to ensure that the data is stored efficiently and that you can query data correctly. Common data types include numeric, string, and boolean, to name a few! The exact data types available may vary depending on the SQL dialect that you use. Let’s try this out. Imagine you want to create a table in the Customers dataset called 'Customer Names.' This table will have two columns: 'Customer ID' and 'Customer Name.' The 'Customer ID' column will store a unique identifier for each customer. This column will be of type 'int64', which is a numeric data type that can store large numbers. This makes it a good choice for storing identifiers, which are typically large, sequential numbers. The 'Customer Name' column will store the customer's name. This column will be of type 'string', which means that it can store text data. This is a good choice for storing names, addresses, and even phone numbers! When you run this query, an empty table called 'Customer Names' will be created in the Customers dataset. This table will have two columns, 'Customer ID' and 'Customer Name,' and each column will store its own type of data. Nice! You now know the foundation of SQL dialects. These simple commands will set you up for successful interactions with whatever database you encounter. As a new data analyst, working on a variety of databases helps you better understand SQL dialects and makes it possible for you to contribute useful code right away!

### Document - [[Supplemental] SQL query terms](https://www.cloudskillsboost.google/course_templates/962/documents/472066)

### Lab - [Compare data analytics with BigQuery and Dataproc](https://www.cloudskillsboost.google/course_templates/962/labs/472067)

Create and import data using parquet files

- [ ] [Compare data analytics with BigQuery and Dataproc](../labs/Compare-data-analytics-with-BigQuery-and-Dataproc.md)

### Quiz - [Test your knowledge: Data schemas and queries in BigQuery](https://www.cloudskillsboost.google/course_templates/962/quizzes/472068)

### Video - [Steps and models for accessing data with machine learning](https://www.cloudskillsboost.google/course_templates/962/video/472069)

- [YouTube: Steps and models for accessing data with machine learning](https://www.youtube.com/watch?v=06xt11bJCCo)

Data holds the key to transforming industries and shaping the marketplaces of the future. Add to that the exciting power of machine learning —or ML— and you can open a whole new world of insights and groundbreaking advancements. But there's a catch: Traditional methods of data analysis, like statistics, limit the potential of ML. In this video, you’ll learn some simple steps for using cloud computing to take ML to the next level. But to start, consider the definition of machine learning. Machine learning is the use and development of algorithms and statistical models to teach computer systems to analyze and discover patterns in data. ML algorithms and models rely on vast amounts of high-quality data to make accurate predictions and drive smart decision-making. Accessing the right data at the right time is crucial for the success of ML projects, and that's where the cloud becomes a game-changer. To consider how this works, let’s consider a real-world example with a data analyst, named Zane. Zane’s organization wants to predict annual farming yields in an agricultural community. But his organization has only been tracking crop yields from public datasets that include communities all over the country. He doesn't have any information about crop yields in your community. Zane knows that he’ll create a machine learning model to help with the prediction, but he’s missing some data! This is where the cloud comes to the rescue! Zane can request datasets from various organizations. And he can use these datasets to train his ML models. In this case, he’ll ingest data from internal datasets from local farmers and from the national weather organization. Zane will take data about both forecasts and actual reports. And he’ll use that to start to train the model! Pro tip: Datasets from private organizations require some coordination to access. After you receive permission to access, you’ll need to find a way to transfer the data. As you start to work with machine learning models, you may find that the datasets you access provide the reference points you need to train your models. Now, let’s get back to Zane. It’s time to get the necessary datasets. First, he chooses the appropriate cloud service provider based on his organization’s particular needs and requirements. Then, he sets up the necessary credentials and permissions to ensure secure access to your data. Finally, Zane establishes connections to the data resources. Before you ingest data from external datasets, you first need credentials. Credentials are similar to a login you’d use on a personal account. These are a username and password. An organization may give you credentials that allow you to access and transfer data. Once you have credentials, you will need to use an interface that allows for the transfer of the data from an external organization to your own. There are a few interfaces you may come across in your work, login credentials, application programming interfaces, or APIs, and software development kits, or SDKs. Each of these interfaces allows you to ingest data directly from an external dataset to your own dataset. An API is a protocol that enables one application to interact with another application. In your case, you’d use an API to access and transfer data. An SDK is a set of software-building tools that includes groups of code libraries. As a data analyst, you might use an analytics SDK to access data about users and their interactions with your software. For Zane’s work with the crop-prediction model, he is able to access an API. This allows him to transfer data directly from the national agriculture databases and into his own database. Ingesting data directly can streamline your process and create a direct pathway to your machine learning model. This means your model can get to work! At this point, you can select a common model or create your own. And the best part is, once you set the initial parameters, your model will be able to train itself! Ok, now let’s check out some common types of models you can create with machine learning. First is a classification model. You may train a classification model to classify items or group items into a predefined category based on common characteristics. This could be as simple as determining whether something is a mammal or a reptile, or as complex as determining whether a new product is likely or unlikely to sell. Next is a clustering model, which involves grouping data points that are similar to each other in some way. In this instance, machine learning will decide how to group items together based on similarities determined by the model. For example, you may try to determine what geographic area has customers that are most likely to purchase products. Another type of model is a linear regression model. Linear regression is a technique that estimates the linear relationship between a continuous dependent variable and one or more independent variables. With linear regression, you may try to predict sales, or even the amount of rain that might fall next week! Finally, there are ranking models. A ranking model uses characteristics to order items by likelihood. For example, entertainment streaming services use ranking models to recommend music or movies that customers are likely to enjoy. These models try to find similarities in what users listen to or watch, and rank new entertainment options by the likelihood that users will enjoy the content! So, back to Zane’s crop prediction. After reviewing the data, he uses a regression model to try to predict the amount of crops his town can expect to produce in the upcoming year. So, now he has datasets and he knows what type of machine learning model he will use. Zane is well on his way to building a model that can predict which years will have the most crop production. The cloud is your gateway to data access and to maximize the incredible capabilities of ML. By working with the cloud, you’ll ensure ML can continuously improve its own models. Keep exploring the world of data-driven discovery!

### Document - [Cloud-based machine learning can train predictive models](https://www.cloudskillsboost.google/course_templates/962/documents/472070)

### Video - [Introduction to machine learning with Vertex AI and BigQuery](https://www.cloudskillsboost.google/course_templates/962/video/472071)

- [YouTube: Introduction to machine learning with Vertex AI and BigQuery](https://www.youtube.com/watch?v=WLjjYJA0fMk)

Machine learning, or ML, is really incredible. It has so many capabilities! For example, it can identify objects in photos and analyze medical images. It can understand and process natural language for chatbots, voice assistants, and translations. And it can recommend products, services, and content to help users find just the right online store, movie, or new playlist. In your data career, you may have the opportunity to train and deploy ML models. To prepare you for this really cool opportunity, this video will explore Vertex AI —an ML platform— and how it combines with BigQuery for data collection. But first, the definition of machine learning. ML is the use and development of algorithms and statistical models to teach computer systems to analyze and discover patterns in data. ML helps data professionals make better decisions, automate processes, and uncover valuable insights. Vertex AI takes ML to the next level by providing a comprehensive platform for developing, deploying, and managing ML models at scale. It allows you to use machine learning operations to manage and govern all of your ML workloads. This means that your data enters and moves through your pipeline quickly, so you can focus on innovating. For example, consider an online store and worldwide marketplace that sources products from other sites and personalizes search returns based on users’ past activity. You’re a new data analyst at this online store, and your team is responsible for increasing the accuracy of projected ship times for each product. In the past, your organization has built machine learning models and ingested data monthly to update projected ship times. But, this process is slow and usually the monthly updates provide customers with inaccurate projections. One of the first projects that your team tackles is moving the on-premises data warehouses into Google Cloud. From there, your team develops a pipeline to ingest new data in near real time and that is scalable when there are influxes or reductions in data. Once your data is prepared and made available for ML, your team’s next step is to automate hyperparameter tuning. Hyperparameters are parameters whose value is used to control the learning process. This is the part where the power of machine learning begins and the model starts to update itself. Your team will set the hyperparameter metrics you want to optimize before the machine learning process begins. This will help the model to train itself. Even more exciting, with Vertex AI your team can accelerate the hyperparameter tuning as Vertex AI will auto adjust your hyperparameters to help you determine the best values to set. Next, your team will focus on updating the delivery time predictions. Your team can rely on Vertex AI to automate your machine learning models and update predictions. Given the hyperparameters, your team’s model will constantly update itself using the data that pours into your organization’s site on a daily basis. There are a couple of options for moving your organization’s on-premises databases into the cloud. One option for cloud data storage is BigQuery. BigQuery provides powerful querying and data access capabilities. You might have already learned about how you can use BigQuery to access your data. But BigQuery also has built-in machine learning capabilities that you can use to build ML models directly within the platform. In an on-premises database —or a physical server that houses your database— you would need to have advanced programming knowledge and access to specialized frameworks to develop your ML model. With BigQuery, you can use your SQL knowledge to develop machine learning models. Let’s go back to the online marketplace. You and your team solved the problem of calculating shipping times. Now, you can use BigQuery SQL to solve another problem! Your team needs you to generate a daily report of projected sales for the next day. You know that you’ll need constant data input and a model that projects sales every twenty-four hours based on multiple inputs, including products, past sales by date, past sales by hour, and past sales on holidays. You use BigQuery’s built-in linear regression model to forecast sales on a daily basis. Linear regression is a technique that estimates the linear relationship between a continuous dependent variable and one or more independent variables. You can input your information, including products, past sales by date, past sales by hour, and past sales on holidays. Then, using SQL, you can create a model and use the pre-built linear regression model. BigQuery will get to work! Now, you have access to daily reports about projected sales! And the best part is, as more data is input, your model will train itself and become even more accurate! Machine learning is a super-valuable technology that can be used to answer all kinds of questions and solve a wide variety of problems. And with automated options, you can build and upkeep your own models easily!

### Video - [Overview of Google Colab](https://www.cloudskillsboost.google/course_templates/962/video/472072)

- [YouTube: Overview of Google Colab](https://www.youtube.com/watch?v=C3-cHR7EMeo)

Welcome back! You may have noticed that a lot of work with data is done with a team. Sometimes the tools we have may not be dynamic enough for teams to work collaboratively. The good news is that Google Colab can help with that! In this video, you’ll learn about Colab. You’ll also discover how Jupyter Notebook and Python make the capabilities of Colab even more amazing. Let’s get started! Colab, or Colaboratory is a cloud-hosted version of Jupyter Notebook that allows users to write and execute Python in a browser, with no configuration required. Data analysts working in Colab have access to graphics processing units for free, and their work is easily shareable. The two main contributors to Colab are Jupyter Notebook and Python. Jupyter Notebook is an open-source, web-based platform for creating and sharing documents that consist of code, equations, narrative text, and visualizations. It can be used for many tasks, like numerical simulation, machine learning, and data visualization. Jupyter Notebook, or Jupyter, is most frequently used within a Python environment. With Jupyter, data professionals can write in multiple programming languages, including Python, Java, R, Julia, Matlab, Octave, Scheme, Processing, Scala, and many others. Jupyter is also used for data visualizations, bash scripts, markdowns, and mathematical and scientific notations. Python is an object-oriented programming language. It’s flexible and easily adaptable to changes, which makes it a popular tool for data analysts. Python is the primary programming language used with Colab, and users write Python code through most popular web browsers, including Google Chrome, Mozilla Firefox, and Safari. Then, users can run their code on a browser without a command line interface or a runtime environment. Colab notebooks allow users to combine executable code and rich text in one document, along with images, HTML, LaTeX, and more. After creating a Colab notebook, it’s stored in the user’s Google Drive. The notebooks can be easily shared with collaborators who can then add edits or comments. In the data field, data analysts use Python libraries for data analysis and visualization with Colab. Python libraries are collections of code that can be used to automate some functions. They can import data into Colab notebooks from spreadsheets and many other sources. Machine learning programmers also use Colab for their data work. They can import an image dataset, train an image classifier with that image, and assess the model, just with a few lines of code. And because Colab Notebooks run code on Google’s cloud servers, data professionals of all kinds can tap into Google’s hardware —including graphical processing units and tensor processing units— without worrying about the limitations of their personal computers. All they need is an internet browser! Now that you’re familiar with the uses of Colab, consider the benefits of Colab notebooks for data analysis. First, Colab’s free version uses graphical processing units and tensor processing units for up to twelve hours at no cost. This means that users can access Colab’s powerful computing resources to do things like analyze large datasets. Also, users can create shareable links for Colab files stored on their Google Drives for easier collaboration and notebook sharing. And users can install libraries like AWS S3, GCP, SQL, MySQL, and others that are not available in code samples. Libraries are useful for accessing and analyzing data. They allow users to tap into a variety of databases, cloud storage services, and other resources. Another benefit is that Colab has a bunch of pre-installed libraries, like NumPy and Keras. This means that users can immediately start to code without needing to set up these libraries. Plus, since Colab saves everything in Google Drive storage, users can pick up where they left off from any computer using their Google Drive account. If you’re a Github user, you can even connect your Github account to Colab and move your code files between them easily. Also, Colab is compatible with all sorts of data sources, which is great for machine learning and AI training projects. And, just in case you’re worried about losing any changes, Colab keeps track of every single change you make from the moment you create a file. Pretty cool, right? Ok, a few uses. To ensure Colab remains free for users, its resources are not guaranteed and not unlimited, and the usage limits may vary. This means that while Colab provides valuable computational resources at no cost, there can be fluctuations in their availability. Also, when sharing a Colab Notebook, all its content —including text, code, output, and comments— will be shared. But users can omit code cell output from being saved or shared, enabling other users to run the cells and see the results themselves. To omit code cell output from being saved or shared in a Colab notebook, users can go to Edit, select Notebook settings or Preferences, check the box for Omit code cell output when saving this notebook, and close the settings dialog. Google Colab combines the power of a text editor with a Python coding environment, all within an internet browser. It’s like having a high-powered coding laboratory right inside a regular sketchbook. Keep practicing to discover all the new possibilities!

### Document - [Managed notebooks](https://www.cloudskillsboost.google/course_templates/962/documents/472073)

### Quiz - [Test your knowledge: Integration of Google Cloud tools](https://www.cloudskillsboost.google/course_templates/962/quizzes/472074)

### Video - [Essentials of database partitioning](https://www.cloudskillsboost.google/course_templates/962/video/472075)

- [YouTube: Essentials of database partitioning](https://www.youtube.com/watch?v=re1mEvIxg88)

Hi there, future data pro! Thanks so much for joining me in this video about database partitioning. You’re going to learn how to make your data work much more efficient by dividing data into manageable and efficient segments. Let’s get started! Database partitioning is the process of dividing data into separate data segments, or partitions that can be managed and accessed separately. Database partitioning occurs across servers or databases. It’s important to call out that database partitioning is different from table partitioning, which involves segmenting data within a single table without distributing data across hardware. Database partitioning offers many benefits, but the most important are improved scalability, availability, and performance. Let’s discuss each of these benefits. First, database partitioning helps with improved scalability by spreading data across multiple partitions. By dividing the data into manageable parts, the system can handle more work efficiently, ensuring continued excellent performance. Second, partitioning helps with improved data availability by dividing the data across multiple servers. This also prevents a single point of failure. And, data partitioning includes a built-in backup feature that ensures the service remains available if a failure occurs. Third, database partitioning helps with improved performance by allowing the system to query a smaller part of the database instead of the entire thing. This makes the service more efficient. When partitioning databases, there are three key strategies that data professionals employ: horizontal partitioning, also known as sharding, vertical partitioning, and functional partitioning. Horizontal partitioning is a process for dividing data into separate segments in which each partition, or shard, is its own data storage, but all shards follow the same organizational layout. Each shard keeps a unique piece of the whole data, like all orders from a certain group of customers. Consider a sneaker inventory that is divided into shards based on the SKU or ID number. This sneaker inventory is an example of horizontal partitioning. Next, vertical partitioning is a process for dividing data into separate segments in which each partition maintains a unique piece of the fields for items in the data store. These fields are divided based on how often they are used. So, fields that are used often might be put into one vertical partition and fields that aren’t accessed as frequently might be placed in another. In the sneaker inventory example, one partition holds frequently accessed inventory data, including sneaker name, color, and price. Another partition contains other inventory data, like current inventory and location. Lastly, functional partitioning, is a process for dividing data into separate segments in which data is grouped based on how it’s used in different parts of the database system. With functioning partitioning, data is grouped based on how it’s used in different parts of the database system. For example, sneaker brands may be placed in one partition based on what the sneaker was designed for, like training or running. Simultaneously, they could be assigned to another partition based on pricing. There are three key considerations when incorporating partitioning into an already active system. First, it may be necessary to change how data is accessed in the system. Second, dividing data into different partitions will require the migration of large amounts of existing data. And third, users will want to keep using the system during the data migration. Now, dive into important considerations when partitioning data. First up, parallel processing. Parallel processing enhances query performance by dividing the data into smaller parts that can be processed at the same time. Parallelism is the system’s ability to perform multiple tasks simultaneously on different database partitions. Next, consider application requirements. Application requirements refer to how the data will be used, queried, and changed in order to design a system that performs well, responds quickly, and remains reliable. Application requirements are important because database partitioning can make system design and development more complex. And a pro tip: Users must rebalance partitions to address uneven distribution of traffic when there is a surge. This is done by creating a new strategy and moving data from the old partitioning scheme to a new one. Nice work! You now understand how database partitioning improves query performance by allowing a system to work with smaller subsets of data. This results in faster and more efficient analysis. By applying an effective partitioning strategy in your own data work, you’ll provide your organization a big advantage as it navigates through its data systems!

### Document - [Benefits of data partitioning](https://www.cloudskillsboost.google/course_templates/962/documents/472076)

### Video - [Methods for partitioning tables](https://www.cloudskillsboost.google/course_templates/962/video/472077)

- [YouTube: Methods for partitioning tables](https://www.youtube.com/watch?v=axEK9d6mT9c)

Have you ever searched through a pile of papers trying to find a specific document? It can be very time-consuming! But what if the papers were neatly organized in a folder with labeled sections for personal, banking, medical, and so on? Your search would become so much easier and faster. Well, that’s basically what partitioned tables do in a database system. They neatly categorize data into separate sections, or partitions, streamlining both data management and queries. In this video, you’ll learn what it means to partition a table, when to partition a table, and the types of table partitioning you can perform. You’ll also learn about clustering in concert with partitioning and as another option. Ok, so a partitioned table is a large table that is divided into smaller segments, or partitions. Partitioning reduces the amount of data a query needs to process. This has two benefits: it improves query performance and it controls costs. In a partitioned table, data is stored in separate storage units, each containing a single data partition. Unlike database partitioning which results in managing multiple databases, a partitioned table can still be treated as a single table. Partitioned tables also keep track of metadata about the data stored in each partition, and can use that metadata to optimize queries. Also, thanks to the metadata, BigQuery can provide a more precise estimation of a query’s cost before it’s run. Partitioning is effective for columns with fewer unique values so that each partition still has a relatively sufficient amount of data. In BigQuery, it’s best to have at least 1 GB per partition. For example, if you have a table storing months or years worth of data, you could store each day's data in a different partition. This way, if you are only interested in querying the last 30 day's worth of data, BigQuery knows it does not need to scan any partitions older than 30 days. Now, consider some examples of when to partition a table. First, partitioning is valuable when a data professional needs to improve query performance by only scanning a specific section of a table. Second, if the table operation exceeds the expected volume of data, the data professional needs to limit it to specific partition column values. This is a benefit of higher partitioned table quotas. Third, another time to partition is when the data professional needs to set an expiration schedule to automatically delete entire partitions after a specific time period. Fourth, a data professional should consider partitioning when they need to load data to a specific partition without affecting other partitions in the table. Fifth, and finally, a data professional should partition when they need to delete specific partitions without scanning an entire table. There are three ways to partition a table: Integer-range, time-unit column, and ingestion time partitioning. With integer-range partitioning, users partition a table based on ranges of values in a specific integer column. To create an integer-range partitioned table, provide the partitioning column, the starting value for range partitioning, the ending value for range partitioning, and an interval of each range in the partition. Next up, time-unit column partitioning. This type of partitioning enables data professionals to partition a table based on a date, timestamp, or datetime column. When writing data into the table, BigQuery will automatically place it into the appropriate partition based on the values in the column. When working with timestamp and datetime columns, data professionals can set partitions with hourly, daily, monthly, or yearly divisions. For date columns, they can set partitions with daily, monthly, or yearly divisions. The partition boundaries are based on Coordinated Universal Time or UTC time. Finally, with ingestion time partitioning, BigQuery automatically assigns table rows to partitions based on the time when the data is ingested, or imported, by BigQuery. Data professionals also have the flexibility to choose between different partition divisions based on their specific needs. Ingestion time partition boundaries are also based on UTC time. Great, now let’s move on to clustered tables. A clustered table is a table in which column order is defined by the user using clustered columns. And a clustered column is a user-defined table property that arranges storage blocks based on the values within their columns. The order of the values in a clustered column determines the order in which the rows of the table are stored in memory and on disk. This can improve the performance of queries that access the table by the clustered column, because the rows are already sorted in the order that the query needs them. For example, think of a table with sales data within columns like Product, Date, and Regions. By defining the Date column as the clustered column, the storage blocks within the table will be sorted by date order, so rows with the same date will be grouped together. The size of the storage blocks in a clustered column is flexible and adapts to the size of the table. Clustering is great for columns with tons of unique values so that even more rows can be filtered out by querying. A clustered table keeps the sorting criteria in the context of each operation modifying it. Just like partitioned tables, clustered tables improve query performances. And, if clustering on a table that is also partitioned, each partition's data will be sorted based on the clustering columns. There are many very useful options available for all sorts of data projects! And no matter which you choose, you’ll be able to locate what you need quickly and easily. Just as a well-organized folder helps you locate specific documents, partitioning and clustering in BigQuery facilitates effective data management and faster query performance.

### Document - [Data partitioning reduces cloud costs](https://www.cloudskillsboost.google/course_templates/962/documents/472078)

### Document - [Create a partitioned table](https://www.cloudskillsboost.google/course_templates/962/documents/472079)

### Quiz - [Test your knowledge: Overview of data partitioning](https://www.cloudskillsboost.google/course_templates/962/quizzes/472080)

### Video - [Strategies for querying partitioned tables](https://www.cloudskillsboost.google/course_templates/962/video/472081)

- [YouTube: Strategies for querying partitioned tables](https://www.youtube.com/watch?v=PTQYasvXPk4)

Pruning is the practice of removing unwanted branches from a tree. Gardeners prune trees to improve their overall health to shape the trees and to help the trees produce more fruit. In the data field, we do a kind of pruning, as well. And just like with trees, pruning makes our data work more fruitful! In this video, you’ll explore partition pruning. Partition pruning is the process of eliminating unnecessary or irrelevant data from consideration when running a query. In other words, the data professional runs a query using a qualifying filter on the value of the partitioning column. This tells BigQuery to scan the partitions that match the filter and skip the rest. All right, now, let’s examine the queries that can be run on different types of partitioned tables. First, to remove unnecessary partitions when querying a table partitioned by a time-unit column, use a filter based on the partitioning column. Consider this example of a query-partitioned table. A dataset table is partitioned based on the transaction date column. The example query prunes dates by excluding any before January 1, 2016. Now, in ingestion-time-partitioned tables in BigQuery, there’s a special column called partition time. This column holds the Coordinated Universal Time, or UTC, when each row was imported, rounded to the nearest partition boundary, like hourly or daily. This is represented as a timestamp value. If a data professional were to add data on April 15th, 2021, at 8:15 UTC with a partition granularity of hourly, the partition time column for those rows will store the timestamp value truncated to the hour. And if the table is partitioned by each day, it will also include a fake or pseudo-column called partition date. This is just the partition time shortened to present only the date, and is represented by a date rather than a timestamp value. To prune partitions on ingestion-time-partitioned tables, filter either of these columns. This example query only scans the partitions between the dates January 1, 2016 and January 2, 2016. All right, so, to prune partitions when querying an integer-range-partitioned table, include a filter based on the integer-range-partitioning column. Consider this example of an integer-range-partitioned table. It has partitioning based on a range of numbers, with a focus on customer ID between 0 and 100, in steps of 10. The example query scans the three partitions that start with 30, 40, and 50. Data that is being sent to BigQuery in near real time is written to write-optimized storage before being moved into its assigned partition. The data is temporarily stored in a partition called unpartitioned and you can query data in this temporary unpartitioned partition. Data in the write-optimized storage has null values in the partition time and partition date columns. Ok, now, to query data in the unpartitioned partition, use the partition time pseudo-column with the null value. Great! Now, let’s consider three best practices for designing and implementing partition pruning strategies. The first best practice is to use a constant filter expression to limit the partitions that are scanned in a query. This is helpful because you will always reduce the amount of data processed to only the partitions needed and that will make your query more efficient! The second best practice is to isolate the partition column in the filter. Filters that require data from multiple fields to compute will not prune partitions. And the third best practice is to require a partition filter in queries to ensure queries always eliminate unnecessary partitions. Partition pruning improves queries on partitioned tables by eliminating unnecessary data and enabling analysts to focus on only relevant partitions. This makes data work more efficient, effective, and insightful. Just like a flourishing, beautifully pruned tree, which comes back even more bountiful year after year!

### Document - [Tips for interacting with partitioned tables](https://www.cloudskillsboost.google/course_templates/962/documents/472082)

### Lab - [Manage a partitioned table in BigQuery](https://www.cloudskillsboost.google/course_templates/962/labs/472083)

Manage a partitioned table and use filters to reduce data examined in BigQuery

- [ ] [Manage a partitioned table in BigQuery](../labs/Manage-a-partitioned-table-in-BigQuery.md)

### Quiz - [Test your knowledge: Techniques for managing partitioned tables](https://www.cloudskillsboost.google/course_templates/962/quizzes/472084)

### Video - [Key processes and benefits of Dataproc](https://www.cloudskillsboost.google/course_templates/962/video/472085)

- [YouTube: Key processes and benefits of Dataproc](https://www.youtube.com/watch?v=_3HfCTrcfmE)

Hi there, and thanks for joining me for this video all about Dataproc! Dataproc is a fully managed service for running big data processing jobs on the Google Cloud Platform. You may know a little about Dataproc already, but in this video, you’ll consider how to use Dataproc in your work as a data analyst. To understand Dataproc’s many benefits, let’s check out an example. Maybe a data professional is working at a governmental agency, where data flows like the soundwaves of a never-ending symphony. Day after day, for six whole months, a data analyst diligently combs through and processes the numbers like a true data conductor. It soon becomes clear to the analyst that they’re performing the same tasks and delivering the same reports time and time again. Eventually, the repetition starts to seem like an endless loop. They’re playing the same tune on repeat, and sometimes hitting a few wrong notes along the way. It occurs to them that it must be possible to either automate portions of the work or find a way to reduce errors. So the data analyst decides to use Dataproc to run Hadoop and Spark clusters. Dataproc is a fully managed service that maximizes open-source data tools for batch processing, querying, streaming, and machine learning. With Dataproc, users can create clusters quickly, manage them easily, and save money by turning off clusters when they’re not needed. Using Hadoop and Spark clusters, the data analyst can significantly reduce the manual effort required, process data efficiently, and mitigate potential errors. Before we move into the specifics of Dataproc, let’s review its key processes. Batch processing is a method of collecting large volumes of data over a period of time, then processing it all at once. Querying is requesting data or information from a database. Streaming involves working with data as it is generated. And finally, machine learning is the use and development of algorithms and statistical models to teach computer systems to analyze and discover patterns in data. Data analysts can process data in BigQuery directly from Dataproc, and can manage Dataproc with the Google Cloud Console. Apache Hadoop is a framework that distributes the processing of data across clusters of computers. When working with Dataproc, data analysts may also encounter PySpark, the Python API for Apache Spark. PySpark is a language for creating more automated analyses and pipelines. These can be applied to both structured and semi-structured data. And there are several benefits of integrating Dataproc into a workflow. Once a data analyst integrates Dataproc, they’re able to process data at a lower cost because clusters are turned off when not in use. Also, the process speeds up because the work is distributed across multiple machines in clusters with the integration of Hadoop. The integration with BigQuery means that a data analyst only interacts with a single platform for all of their analysis needs. And Dataproc is fully managed by the Google Cloud platform, so no maintenance is required on analysis systems. Plus, with integrating Dataproc into the workflow, data analysts can ingest data several times per day without fail. Likewise, the clusters scale up and down so the organization only pays for the compute power needed. With very little oversight of the data integration, and a consistent application of the batch processing, the data professional is now able to devote more of their time to analyzing the data and finding valuable business insights. And that’s music to their ears!

### Document - [How to create a Dataproc cluster](https://www.cloudskillsboost.google/course_templates/962/documents/472086)

### Document - [How to manage Dataproc clusters](https://www.cloudskillsboost.google/course_templates/962/documents/472087)

### Quiz - [Test your knowledge: Dataproc for automation and improved data processing](https://www.cloudskillsboost.google/course_templates/962/quizzes/472088)

### Video - [Wrap-up](https://www.cloudskillsboost.google/course_templates/962/video/472089)

- [YouTube: Wrap-up](https://www.youtube.com/watch?v=FrLqgtngTy4)

Congrats! You’ve completed another section of the program. Awesome work. You now know how to interact with BigQuery tables and use optimization techniques to improve queries. Plus, you experienced using a cloud-based data lake or warehouse to connect to data sources and answer business questions with data. You explored data patterns and learned how to create table schemas in BigQuery. Then, you worked on integrating Google Cloud tools into BigQuery. You also discovered the benefits of partitioning and how to manage partitioned tables. Finally, you used Dataproc for data processing and dove into managing Dataproc clusters. Great job. You’re one big step closer to your goal!

### Video - [Vince and George: Interview role play](https://www.cloudskillsboost.google/course_templates/962/video/472090)

- [YouTube: Vince and George: Interview role play](https://www.youtube.com/watch?v=aH8VYB1QkUE)

Hi, I'm Vince. Hi, I'm George. Congratulations on making it through this course. Now, you'll have an opportunity to look in on an interview while it's happening. This interview will include technical topics from the course. We hope that this helps as you prepare for your next interview. What interests you in a career in cloud data analytics? What got me interested about a career in cloud data analytics was in college. Everyone was talking about the cloud and how powerful that it can be and how much it seemed like it was the future of where technology was heading, and it sounded like something I'd be interested in. I started to look more into the cloud and how I can use it in my studies in business and I saw how scalable and how powerful it could be and decided that it was an area that I wanted my career to go into. So can you tell me about a favorite data analytics project that you worked on and tell me what your role was and what you liked about the project? My favorite data analytics project was one where I was a data analyst, where I got to combine all of these data sets about compressors and engines where I was able to put them up in the cloud and perform some statistical analysis to help prescribe if they're gonna have some sort of maintenance event or fail. And I really liked it because it had a real world impact. Suppose you're working for a company that wants to modernize their data lake and their data warehouse. What are some components of a modern cloud data platform they should consider? They should consider their ETL policy, their data lifecycle policy, their privacy policies, and then how they're gonna scale all their data and make it available, and if they can apply any machine learning or AI onto it. So when it comes to ETL, can you talk to me about the differences between batch processing and stream processing? The differences between batch processing and stream processing is for batch processing, you will process all of your data at once in one large workload, but for stream processing you're going to process each little bit of data that comes through your system. Can you tell me some use cases for each? A good use case for batch processing would be data analytics. If you wanna perform data analytics around how a product might be performing, you want to have a complete picture of the performance. And so for batch processing, you would wait until you have a complete data set. A good use case for streaming data would be like stock trading. You want to know where the market is moving at any given minute, and you can't wait until the end of the day and so you would need to process that data in real time. Can you give me an example of a problem that a streaming data source would solve? A problem that a streaming data source would solve might be something like an intrusion detection where a security company could monitor real time data in order to determine if there's any unauthorized personnel in some perimeter. It could also be a virtual perimeter like in the cloud. Alright, so imagine that you've been asked to build a new report for a toy manufacturing company. The report should contain the names of each toy built and the number of units shipped by each month. What data would you look for and where would you look for this data in cloud tools? If I wanted to create a report that showed each toy and how much we shipped each month, the first pieces of data that I'd be looking for is their manufacture date and the names of each of these toys and any variations of these toys because we'd probably wanna track that as well. And I would find all this information in BigQuery. So what language would you use to query the data in BigQuery? I would use structured query language and that's how we talk to databases to retrieve any information that we need from them. If you're looking to essentially query the number of units shipped by month, you don't have to sort of say the query exactly as you might type it into a console, but what might that query look like? In general, that query would look like selecting all of the products and any variations of them that we might be interested in reporting on. And I would sum like a unique count of each product who had a manufacture date within a certain month and I would aggregate those to get a view that shows for each toy that we produce, how many we ship that month. Really appreciate you taking the time to interview with us and I really wish you luck with the rest of the process. Thanks, Vince, I really appreciate your time today and it's been a pleasure. Great to meet you. In this scenario, George demonstrated how to provide examples when answering questions. During an interview, you can differentiate yourself by providing examples of how you've used your skills in relevant situations. Watch for more interview tips later on in this program.

### Document - [Interview tip: Provide examples](https://www.cloudskillsboost.google/course_templates/962/documents/472091)

### Document - [Glossary terms from module 4](https://www.cloudskillsboost.google/course_templates/962/documents/472092)

### Quiz - [Module 4 challenge](https://www.cloudskillsboost.google/course_templates/962/quizzes/472093)

### Video - [Course wrap-up](https://www.cloudskillsboost.google/course_templates/962/video/472094)

- [YouTube: Course wrap-up](https://www.youtube.com/watch?v=FqQsM3LWIbc)

Awesome work! You’ve made it to the end of this course! I hope you’ve enjoyed learning with me all about cloud storage and data management in the cloud. My favorite part about analytics is that it combines the objectivity of data with the creativity of storytelling. At Google, many of my projects end with a colorful dashboard or snazzy slide deck explaining why trends are happening and how folks should respond. At their core, the stories that I craft are grounded in truth. Without proper data management, they’d be impossible to tell. As you continue building your analytics skill set, I’m excited to imagine all of the stories underneath the numbers you’ve gathered. Let’s reflect on what you learned in this course. You started by learning about data storage and connections, types and structures. Then, you moved on to table schemas, and batch and streaming data processing. Next, you explored denormalized data, data governance, metadata, data catalogs, the key components of data lakehouse architecture, and more. Then, you discovered Dataplex and how it can be used to identify data sources in BigQuery. You learned to identify and trace data lineage, and access data libraries. Finally, you explored data reference architectures, how to manage tables in BigQuery, add and export data and query tables, and integrate Dataproc. Then, you explored the benefits of partitioning and managing Dataproc clusters. You now have a solid foundation in organizing and structuring data, which will be important for your role in cloud data analytics! Congratulations on finishing this course! I’m excited for your next steps!

### Document - [Course 2 resources and citations](https://www.cloudskillsboost.google/course_templates/962/documents/472095)

### Document - [Glossary terms from Course 2](https://www.cloudskillsboost.google/course_templates/962/documents/472096)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
