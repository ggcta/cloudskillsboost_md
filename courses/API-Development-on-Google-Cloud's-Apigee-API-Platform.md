---
id: 256
name: 'API Development on Google Cloud's Apigee API Platform'
datePublished: 2022-11-29
topics:
- Cloud APIs
- Caching
- Apigee
type: Course
url: https://www.cloudskillsboost.google/course_templates/256
---

# [API Development on Google Cloud's Apigee API Platform](https://www.cloudskillsboost.google/course_templates/256)

**Description:**

In this course, you learn how to create APIs that utilize multiple services and how you can use custom code on Apigee. You will also learn about fault handling, and how to share logic between proxies. You learn about traffic management and caching. You also create a developer portal, and publish your API to the portal. You learn about logging and analytics, as well as CI/CD and the different deployment models supported by Apigee.

Through a combination of lectures, hands-on labs, and supplemental materials, you will learn how to design, build, secure, deploy, and manage API solutions using Google Cloud's Apigee API Platform.This is the third and final course of the Developing APIs with Google Cloud's Apigee API Platform course series.

**Objectives:**

- Discuss the out-of-the-box platform capabilities for implementing mediation, traffic management, caching, and fault handling.
- Describe the value and use of API analytics.
- Describe the deployment options for the Apigee platform.
- Explore and put into practice API design, development and management concepts.
- Interact with the Apigee API Platform.

## Introduction

Welcome to API Development! This is the third course in the Developing APIs with Google Cloud's Apigee API Platform series.

### Video - [Course Series Introduction](https://www.cloudskillsboost.google/course_templates/256/video/348430)

- [YouTube: Course Series Introduction](https://www.youtube.com/watch?v=sYc68Qum0Hs)

Mike: Hi, I'm Mike Dunker, a course developer at Google. Hansel: And I'm Hansel Miranda, also a course developer at Google. Mike: We'd like to welcome you to Developing APIs with Google Cloud's Apigee API Platform. This is a series of three courses that will teach you API concepts and skills that will help you build great APIs. Hansel: We will teach you how to use Google Cloud's full featured API management platform Apigee to design, build, and secure your APIs. You will also learn about how Apigee's developer portal can allow app developers to discover your APIs and register apps to use them. We will also discuss how Apigee can help you manage and grow your API program. Mike: These courses are not just about how to use Apigee. These courses are also focused on the skills required of an API engineer. You will learn how to design APIs that follow best practices. You will learn about API security concerns and how to mitigate them. You will also learn how to productize your APIs. Hansel: These courses are intended for a technical audience. You will build an API from the ground up, completing a series of labs. Many of the labs build on each other, adding more features and security to your API. It is recommended that you take all three courses in order and complete the labs in order. Mike: These courses cover a wide range of topics, and the information may sometimes feel overwhelming. Take your time to absorb the content and make sure that you understand both the lectures and labs. When Hansel and I joined Apigee, we started with zero knowledge of Apigee and minimal knowledge of APIs. Hansel: We learned how to be API engineers during years of helping companies build their APIs and API programs. Mike: This course teaches you the skills, knowledge and best practices that we've found most important for API engineers.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/256/video/348431)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=mBoKe7TfbDc)

Mike: Welcome to API Development and Operations, the third course in the developing APIs with Google Cloud's Apigee API Platform series. In this course, you will learn about API mediation topics, including payload transformation, calling multiple services, and using custom code. You will learn how to share API proxy logic between proxies and handle errors in your proxies. You will also talk about traffic management and how we can control traffic using rate limiting in caching. You'll learn about publishing your APIs to developer portal where app developers can discover and try out your APIs. We'll show you how you can use analytics to gain insights into your APIs and API traffic. We'll learn about offline development of your APIs and how you can include Apigee in your continuous integration and continuous delivery pipelines. And we'll discuss the different Apigee deployment options and how you choose between them. As with the previous two courses in the series, you will use several labs to explore these topics. You will continue to add functionality to a lab that was built during the API Design and Fundamentals and API Security Courses. If you've not taken these courses yet, we recommend you take them before you take this one. You will add XML support, traffic management, caching, and fault handling to your retail API proxy and use a Google geocoding service to add functionality to your retail API. You will also complete a lab that creates an API proxy that calls services in parallel. Finally, you will create a developer portal and publish your retail API using the open API specification to provide live documentation in the developer portal.

## Mediation

This module introduces mediation concepts, including payload formats and transformation, service callouts, custom code policies, hosted targets, shared flows, and fault handling

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/256/video/348432)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=gV6h871UnFE)

Mike: In this module, you'll learn about API mediation. We will learn about JSON, XML and SOAP, and the related Apigee policies. You'll learn how to call multiple services during an API call, and mash up responses. You will also learn about using custom code, sharing logic between API proxies, and how errors are handled in Apigee. You'll also explore mediation using several labs. You will add XML support to your retail API and call a Google geocoding API to add a feature to your API. You will also add fault handling to your API and use a shared flow to implement proxy code to securely call the backend service. Another lab will explore how you can call services in parallel in your API proxies.

### Video - [JSON, XML, and SOAP](https://www.cloudskillsboost.google/course_templates/256/video/348433)

- [YouTube: JSON, XML, and SOAP](https://www.youtube.com/watch?v=PFZXJcFy-Kk)

The majority of new APIs being created these days are JSON based. However, in many industries, XML and SOAP are still the dominant formats. Apigee can help you support all three formats for your APIs. This lecture will discuss the differences between JSON and XML and introduce some new XML and JSON focused policies. Before we get started, just a reminder, the format of a request or response is specified by the content type header. Even if the payload looks like JSON, it isn't JSON if the content type header isn't application/JSON. If you're trying to use a policy that acts on JSON but the content type header is not application/JSON, the JSON operation in the policy will be skipped. So remember, to validate or set the content type header for your APIs. You will probably need to convert between JSON and XML in your API proxies. For example, you might want to support both formats, or use SOAP service backends for your REST API. Apigee can help you easily convert between the formats, but there are some differences to keep in mind. First, XML has a required root element, but JSON does not. When converting between the two formats, you'll need to think about how to handle the root element. Second, XML element values are not typed unless an XML schema is used Therefore, converting from XML to JSON may require guessing about a data type. As an example, in the United States, a ZIP code is a postal code used for routing mail. A zip code often looks like a five digit number, but the long form of a zip code adds a hyphen and four more digits to the five digit number. A zip code should typically be stored as a string so that either format can be used. An XML element value that looks like a number may need to be a string in JSON depending upon the context. Third, XML has namespaces and attributes, and JSON does not. It may not make sense to strip out attributes when converting from XML to JSON because attributes are often a key part of the data. You'll need to figure out how to map the attributes into JSON. When converting from JSON to XML, if some of the JSON should be represented as attributes in XML, the XML might require some manipulation. For all these reasons, you may need to manipulate your XML or JSON when trying to convert between the formats. The JSON to XML policy can be used to convert a JSON payload to XML, The source must be in the payload of an HTTP message. The policy cannot be used to convert a value stored in a normal flow variable. Like all JSON and XML policies, the content type header for the source must be set correctly, or the policy will be silently skipped. The output variable is another message, not a string. The content type header will be set to an XML type when conversion is successful. If the source and output variable are in the same location, the message payload and content type header will be overwritten. One of the many configurable elements for the policy is the object element root name. Because JSON does not have a root element, the object element root name for the resulting XML should be specified here. The XML to JSON policy converts an XML payload to JSON. This policy operates much like the JSON to XML policy, acting on HTTP messages and using the content type header. The output prefix and output suffix elements can be used to control the root level of the resulting JSON. The stripped levels element can be used to remove the outermost levels of the XML before converting. Stripping the first level off can be used to remove the XML root element. When converting XML to JSON, note that you may need to do further cleanup on the JSON to convert data types or handle the conversion of attributes into JSON. XML to JSON tends to be the more difficult conversion due to lack of data types and because XML tends to be more verbose. Let's look at a couple of examples. When you have a SOAP message, specifically a SOAP envelope, there are many layers of XML that serve no purpose when converting to a restful JSON structure. The strip levels element can be used to remove the SOAP envelope from the message. In this example, the outer four levels are removed from the SOAP envelope root element to the get city weather by zip result element. The resulting JSON is very clean. The second example uses the recognize number and recognize Boolean elements to convert into those types when the data looks like a number or Boolean. By default, the XML to JSON policy will always convert values to strings. In this case, telling the policy to look for fields that look like numbers or Booleans will result in cleaner JSON. You may still need to clean up data types, but this can help set the correct type in most cases. The XSLTransform policy can be used to convert XML documents into any format. XSL stands for eXtensible Stylesheet Language, which describes a set of rules for converting an XML document into another document. The resource URL is a reference to the XSL style sheet stored in the same proxy. The style sheet must be stored as code in the proxy. Here is an example of an XSL style sheet on the left, the source XML document on the upper right, and some text that has been created as the output on the lower right. XSL can be a powerful tool for reformatting complex XML. The message validation policy, sometimes known as the SOAP message validation policy, can validate a message and raise a fault if the message does not match the specified configuration. The policy can be used to validate XML against XML schema definition or XSD files. It can also validate SOAP against a WSDL file or simply confirm that the JSON or XML is well formed. The content type header must be set to the correct type for validation to occur. And, like the SSL policy, XSD and WSDL files must be stored in the proxy code.

### Video - [Lab Intro: Add XML Support](https://www.cloudskillsboost.google/course_templates/256/video/348434)

- [YouTube: Lab Intro: Add XML Support](https://www.youtube.com/watch?v=xb8c6VYvYjQ)

Mike: In this lab, you add XML support to your retail API. You will use the JSON to XML policy to convert the response to XML when the requests accept header is used to request an XML response.

### Document - [Reading: Labs in this course](https://www.cloudskillsboost.google/course_templates/256/documents/348435)

### Document - [Reading: REST clients](https://www.cloudskillsboost.google/course_templates/256/documents/348436)

### Lab - [Apigee Lab 7: Adding XML Support](https://www.cloudskillsboost.google/course_templates/256/labs/348437)

In this lab, you'll add the ability to accept and return XML instead of JSON.

- [ ] [Apigee Lab 7: Adding XML Support](../labs/Apigee-Lab-7-Adding-XML-Support.md)

### Video - [Mediation and Service Callouts (1)](https://www.cloudskillsboost.google/course_templates/256/video/348438)

- [YouTube: Mediation and Service Callouts (1)](https://www.youtube.com/watch?v=KLz_SDLFQuI)

Mike: This lecture will discuss patterns of transformation and mediation used in API proxies. We'll also learn about policies that can help us transform payloads and make calls to external services. API-first development specifies that we should design our APIs with our app developers in mind. We often talk about an Apigee API proxy being an abstraction or facade layer for your backend services. This facade layer allows us to build APIs that allow app developers to leverage backend services in a much simpler way than if they were called directly. The app developer can use the clean API that you've created in your proxy, and allow the proxy to deal with any backend complexity. You can even create multiple APIs for the same backend services if your app developers have different needs. To accomplish this, you'll need to be able to transform requests and responses and often call multiple services during a single API call. We call this ability to mash up APIs and payloads "mediation." One mediation pattern is format conversion. Backend or third party services usually use content types or payload formats that differ from those in your API. Even if your API and your backend services are using JSON, for example, the payload itself often must be rewritten using a different structure. The ExtractVariables policy can extract data for your message, and the AssignMessage policy can be used to build a new message using the extracted variables. The ExtractVariable policy extracts information from messages or variables, and stores the result in variables for use near proxy. Variables can be extracted from headers, query parameters, the URI path, or any variable. In this example, we are extracting information from the authorization header. We look for patterns starting with "Bearer" and a space. The "ignoreCase" attribute is set to false, so "Bearer" must be spelled with a capital B. The curly braces indicate that "oauthToken" is the variable name, and the resulting variable will contain all of the header value that follows "Bearer" and the space. If there is no authorization header, or the value of the header does not start with "Bearer space," the "oauthToken" variable will not be set. When extracting information from a particular location, you can also use multiple patterns to support multiple formats. They are evaluated in order and the first match is used. URI paths are a special case. Variable matching cannot span multiple path segments. In the example, if the proxy path suffix for the request is "/orders/1/lineitems/2." the ID variable will contain 1 and the item ID variable will contain 2. The first pattern wouldn't be a match because the ID variable could not match three paths segments, "1/lineitems/2." You can even match multiple headers or query parameters that have the same name. By default, if you search for a query parameter named "type," you could only find the first occurrence of that query parameter. You can use indexing to select a particular occurrence of the header. Indexes are specified using the "." followed by the one based index number. In the example, searching for a query parameter named "type.1" or just "type" would retrieve the first matching query parameter into the "type1" variable. The second one, using "type.2," would be extracted into the "type2" variable. If any of the variables are optional, you should set "IgnoreResolveVariables" to true. Otherwise, if one or more matches are not found, the ExtractVariables policy will raise an error. The ExtractVariables policy can also extract data from a JSON or XML payload into variables using "JSONPath" or "XPath." The AssignMessage policy creates variables and HTTP messages. Variables can be created by reference or by value. In the example, the name of the variable is "originalVerb." It takes the value of "request.verb" if it exists. "Request.verb" would typically exist, but if it did not, or if the value of the "request.verb" was null, the value "UNKNOWN" would be assigned. The policy can also modify existing messages or create a new one. The "createNew" attribute of the "AssignTo" element specifies whether to update an existing message or create a new one. In this case, "createNew" is false so we are editing in place. The "Set" element allows you to set fields in a message. In the example, we are setting the verb to "POST" and creating a JSON payload that uses the "op" variable as the action. Note that the operations in the policy are executed in order. The "AssignVariable" element saves the value of the "request.verb" and then the "request.verb" was overwritten by the "Set" element. You can also set headers, query and form parameters, status codes, and the request path. The policy will also remove fields from the message. The specified header "Authorization" is the only header that will be removed. The "QueryParams" element has no specified query parameter, so all query parameters are removed.

### Video - [Mediation and Service Callouts (2)](https://www.cloudskillsboost.google/course_templates/256/video/348439)

- [YouTube: Mediation and Service Callouts (2)](https://www.youtube.com/watch?v=ZWM8FtAT9Vs)

Mike: Message templates allow dynamic variable substitution using functions and variables. Let's look at an example of using message templates in a single AssignMessage policy to build a request message. This policy specifies that a new request message should be created, and that it should be called "svcRequest." The first variable created is "isoFormatString," which has the value of the "ISOFormat" query parameter if it exists. If it does not exist, the default format is specified in the value element. The second variable, "currentTimeUTC," uses a message template function that returns a formatted UTC time using a timestamp specified in milliseconds. It takes two parameters: the format of the string to be created and a timestamp in milliseconds. "System.timestamp" is a predefined variable with a timestamp in milliseconds that represents the time the variable is read. The third variable, "authString," uses a message template to combine two variables, "serviceUser" and "servicePassword" separated by a colon. Finally, we set values in the new message. We create the authorization header using the "encodeBase64" message template function, encoding the auth string variable. The verb is set to "POST." The payload has type "text.plain." Setting the content type in the payload element also sets the content type header. The "currentTimeUTC" variable we created earlier is used in the payload. We also use another message template function, "jsonPath," to extract a JSON field from a variable named "calloutResponse.content." Many different template functions are available for use in your proxies. The Apigee documentation contains a complete list. The second mediation pattern we'll discuss is orchestration. Calling multiple services during a single API request and response is often useful. You can create a single call that performs multiple steps of an operation, which reduces latency and simplifies app development. Also, you can use third party services to add functionality to an existing API call. However, you are only allowed a single target for an API call. You can use a ServiceCallout policy to call additional services during the request and response flows of a proxy. The ServiceCallout policy uses separate request and response message variables from those of the proxy request and response. The request can be specified by variable or be built in the policy, as shown here. The response is specified as a variable, which will be populated after the call has been made to the service. You can also specify a timeout in milliseconds. If the timeout is reached, the policy throws an error. The target for a ServiceCallout is configured in a similar way to the TargetEndpoint target. In this case, the host name and path for the call are being populated using variables in message templates. Proxies deployed in the same organization and environment can be called using a process called proxy chaining. Instead of using the "HTTPTargetConnection" element, you use the "LocalTargetConnection" element. The proxy to be called can be specified in two different ways: by specifying the proxy name and the proxy endpoint inside the proxy, or by specifying the path to the proxy without the hostname. One of the benefits to proxy chaining is that the call to the local proxy stays within the gateway. There is no network hop and no additional network connection. A LocalTargetConnection can be used in TargetEndpoints and ServiceCallouts. Proxy chaining is one strategy for avoiding duplicate code in your proxies. In a later lecture, we will discuss a more powerful method of code reuse shared flows.

### Video - [Custom Code](https://www.cloudskillsboost.google/course_templates/256/video/348440)

- [YouTube: Custom Code](https://www.youtube.com/watch?v=zBDZaLgko6k)

So far, we've discussed how we use configuration-based policies to build our proxies. Sometimes, you just want to write code. This lecture will discuss the options for using code in your API proxies. Apigee provides policies that allow you to write your own code within your proxies. JavaScript, Java and Python are the languages supported. Even though these policy types are supported, you should use configuration based policies when possible. The configuration-based policies, like AssignMessage and ExtractVariables have been optimized to run very efficiently on Apigee. In most cases, when you can reasonably implement your functionality using the configuration-based policies, they will perform better and more predictably. So when should you use custom code? Sometimes the configuration-based policies don't provide the functionality required to solve the problem. For example, if you need to iterate through a complex payload and make changes to it, using ExtractVariables and AssignMessage might not be possible. Sometimes you can use the configuration-based policies, but the implementation would be too complex or too confusing to maintain. In general, use the configuration-based policies when you can and use the code policies when you must. The JavaScript policy is the preferred policy for most code use cases. The JavaScript code for these policies is compiled, server-side JavaScript. The code runs within Rhino, an open-source implementation of JavaScript that runs within the gateway's Java virtual machine. JavaScript performance is significantly better than if it were interpreted. The XML policy configuration looks similar to the other policies we've seen. The policy contains references to JavaScript code files that are stored in the resources section of the proxy. The IncludeURL element specifies a JavaScript library file that is a dependency for the main JavaScript file, which is specified with the ResourceURL element. Multiple IncludeURL elements may be specified, and they are evaluated in the order listed in the proxy. Each library file in an IncludeURL element must be stored in the resources section of the proxy or be stored as an organization or environment scoped resource. The Resource URL file is loaded last. JavaScript code is written in a resource file and stored within the proxy. You can get, modify and delete flow variables from within the JavaScript code. contexts.getVariable retrieves a flow variable by name, context.setVariable updates or creates a flow variable and context.removeVariable deletes the flow variable. JavaScript is also useful for looping through and modifying complex payloads. In this case, we are taking the request payload accessible by the variable request.content and parsing it into a JavaScript object. We then manipulate the object and put the results back into the request payload using JSON.stringify. Another task that can be done with JavaScript is calling multiple HTTP services in parallel. The ServiceCallout policy waits for a response from a called service before continuing with the proxy flow. Using the httpclient object in JavaScript, you can call services in parallel and reduce the latency of the API proxy. There's a time limit attribute for JavaScript policies. This sets the timeout for a JavaScript policy in milliseconds. If the policy is still executing when the time out is reached, the policy will stop execution and raise a fault. The print function in JavaScript logs a line to the console in the trace tool. This can help you debug your JavaScript policies or provide useful information while tracing the proxy. For simple use cases, JavaScript code may also be inserted directly into the Source element of the policy configuration file, eliminating the need for a separate resource file. Code specified in the Source element executes identically to code in resource files. For writing code in Java. Apigee provides a JavaCallout policy. The JavaCallout policy allows you to use custom Java code in your proxies. Java code running on a JavaCallout will perform very well. The elements in the JavaCallout policy are similar to those in the JavaScript policy, except that the class to run within the configured Java JAR file must be specified. You can also specify a list of name value properties. The values are plain strings and the properties can be used within the Java code. JavaCallout code is harder to maintain in debug than the JavaScript code used within the JavaScript policy. The Java code is stored in the proxy as a JAR file, which needs to be compiled outside of Apigee. The Java code is not visible in trace and cannot log to the trace tool console. Because this is running within the context of the gateway, most system calls are not allowed. Your code cannot perform network socket i/o, get information about the process or CPU or memory usage or write to the filesystem. You can create, update and remove flow variables and message elements. Apigee also has a policy that can execute Python code. The Python policy runs interpreted Python code inside the Java virtual machine and has functionality similar to the JavaScript policy, including flow variable access. However, since Python is interpreted at runtime, you can run into serious performance problems under load. We recommend that you avoid the Python policy, especially for any high traffic proxies. JavaScript can be used to solve the same problems and is almost always a better choice. So how do you choose which policy to use, if any? First, if you can use the configuration-based policies like ExtractVariables and AssignMessage, you probably should. These policies tend to perform better than custom code. The second choice is the JavaScript policy. JavaScript is pre-compiled, so it performs well. It is easy to write small bits of code to do things like parsing and modifying JSON and XML. Use JavaScript for problems that can't be solved easily with ExtractVariables and AssignMessage. You should use JavaScript for most custom code use cases. Java should be used when performance is the highest priority. If you are doing heavy, compute-intensive cryptography, for example, Java might be the right choice. You also might use the JavaCallout when you have Java libraries that perform the functionality you are adding to your proxy. Finally, you should avoid the Python policy, if at all possible. Python is interpreted and can be slow, performance can especially suffer during heavy load.

### Video - [Lab Intro: Mashup](https://www.cloudskillsboost.google/course_templates/256/video/348441)

- [YouTube: Lab Intro: Mashup](https://www.youtube.com/watch?v=RA2Pm9IuLzU)

Mike: In this lab, you add additional data to your API response by calling a geocoding API and extracting an address from its response. You will add this address to your response payload, thus adding a feature to your API without modifying the backend service.

### Lab - [Apigee Lab 8: Mashing Up Services](https://www.cloudskillsboost.google/course_templates/256/labs/348442)

In this lab, you'll invoke multiple services and combine their responses.

- [ ] [Apigee Lab 8: Mashing Up Services](../labs/Apigee-Lab-8-Mashing-Up-Services.md)

### Video - [Lab Intro: Call Services in Parallel using JavaScript](https://www.cloudskillsboost.google/course_templates/256/video/348443)

- [YouTube: Lab Intro: Call Services in Parallel using JavaScript](https://www.youtube.com/watch?v=kUD9EIAA73k)

person: In this lab, you create a new proxy that uses a JavaScript policy to call services in parallel.

### Lab - [Apigee Lab 8a: Calling Services in Parallel Using JavaScript](https://www.cloudskillsboost.google/course_templates/256/labs/348444)

In this lab, you'll call services in parallel using JavaScript.

- [ ] [Apigee Lab 8a: Calling Services in Parallel Using JavaScript](../labs/Apigee-Lab-8a-Calling-Services-in-Parallel-Using-JavaScript.md)

### Video - [Shared Flows](https://www.cloudskillsboost.google/course_templates/256/video/348445)

- [YouTube: Shared Flows](https://www.youtube.com/watch?v=_RR-r7ajoH0)

Person: In a previous lecture, we talked about proxy chaining, a technique for sharing code by calling into another proxy with the same organization and environment. In this lecture, we'll discuss shared flows, another method for sharing code inside proxies. Proxy chaining is calling another proxy from within a proxy to share code. To call a proxy, you have to build an HTTP request message, and you get back an HTTP response message that you have to parse. A shared flow is a single flow of policies, policy conditions, and resources. The shared flow executes within the context of the hosting proxy's flow. The shared flow has read and write access to the proxy's flow variables, so there is no need to build and parse messages to use a shared flow. When calling a shared flow, the proxy executes as if the policies in the shared flow are in the proxy flow. Shared flows are typically better than proxy chaining when sharing reusable logic between proxies. Shared flows live within the context of a hosting proxy's flow, so there is no way to invoke a shared flow directly. To test a shared flow, you need to call it from a proxy. A shared flow is hosted in an organization and can be deployed to an environment. This is the same way proxies work. A proxy deployed to an organization and environment can only call shared flows deployed to the same organization and environment. Shared flows are invoked by using the FlowCallout policy. The FlowCallout policy can be used in proxies or in other shared flows. A shared flow must be deployed to an environment before a proxy or shared flow using the FlowCallout policy can be deployed. Let's look at an example. Here's the XML configuration for a shared flow named get-backend-token. The steps are at the top level of the XML hierarchy. The first step is a policy named LC-Token. LC stands for lookup cache. We'll discuss more about the cache policies later, but this policy is just looking for a token in the cache. When the token was put into the cache, it was configured with a time to live that expired shortly before the token itself expired. If the token is in the cache, it should be active and should be used. The second step is a policy name SC-GetToken. SC stands for service callout, so this is a callout to get a new backend token from an external service. Note that this step has a condition. The step will only be executed if the condition evaluates to true. The condition is checking a variable called lookupcache. LC-Token.cachehit. This variable was variable was set by the lookup cache policy. If this variable is false, the token was not found in the cache. If the variable is true, this step will be skipped. The third step is a policy called PC-SetToken. PC stands for populate cache, so this policy stores the token we got from the service callout into the cache and makes it available for the next API call. The condition is the same as for the previous step, so this policy is only run when the token wasn't found in the cache originally. The FlowCallout policy is simple. The policy is named FC-GetToken, with FC standing for FlowCallout. The shared flow bundle element contains the name of the shared flow to call. You could also add parameter elements to create variables that are only visible during execution of the shared flow. Within the hosting proxy's flow, the FlowCallout policy can be called as you would call any other policy. The functionality would the same as if you included the three shared flow steps directly in the proxy. When you trace the proxy, you have full access to the trace information for the shared flow steps. Another way to attach shared flows to a proxy is to use flow hooks. Shared flows attached to flow hooks will execute for all proxies deployed in the environment. There are four flow hooks. The pre-proxy flow hook is used for logic that needs to be enforced before the proxy executes. This might be used to execute security policies before letting a call get into the proxy. The pre-target flow hook is used to attach logic after the target endpoint request completes but before the target is called. You might use the pre-target flow hook to strip authorization information from the request message before sending it to the backend. The post-target flow hook executes after the target response is received but before policies in the target endpoint response are executed. This might be used to strip sensitive fields from the backend response. A shared flow attached to the post-proxy flow hook executes after the proxy endpoint response completes and just before the response is sent to the client. This can be used to add response headers or modify the response format in some other way. Flow hooks execute for all proxies deployed to an environment. There is no option to have a proxy execute without running a configured flow hook, but you can use variables and conditions within the shared flow to conditionally skip the shared flow steps. Flow hooks are configured in the environment administration section of the Apigee console. Flow hooks are completely optional. None need to be configured for an environment. An organization admin or environment admin can configure flow hooks, but most Apigee roles cannot. when you configure a flow hook to call a shared flow, that flow will execute for every API proxy deployed in the same environment, so be very careful when configuring flow hooks. A flow hook can only have a single shared flow configured for it. Remember that you can nest shared flows if necessary. So, why should you use shared flows? We use shared flows for code reuse. It is easier and faster to write and debug a series of steps once rather than doing it for every proxy. Each proxy using the shared flows should perform the task in the same way. It is also easier to maintain the shared code in one place rather than in several or even hundreds of proxies. If the flow needs to be changed, it can be changed in a single place rather than changing all of the proxies. For significant changes to a shared flow, we recommend that you test all of the proxies that use the shared flow before making a change to the shared flow in production. Shared flows can be created to allow one to team to own th shared flow and allow other teams to use it. This way, you might have your API security team design, build, and maintain a shared flow that protects against malicious requests. As changes need to be made to security functionality, the team best positioned to make those changes can update the shared flow without updating all of the proxies. Let's look at some common use cases for shared flows. We saw an example of retrieving and populating a cashed value earlier. We used a shared flow to get a cache token for a backend service. If the token was not found in the cache, the service callout would call the backend service to get a token, and then the token would be stored in the cache. Security and traffic management policies that should be run before allowing proxy access can be stored in a shared flow. This can be done in the pre-proxy flow hook if you want to enforce the shared flow for all proxies. Shared flows can also be used for calling external services, especially if there's a significant amount of work to build a request and parse the response. If your process requires several policies, you've greatly reduced the work for your API developers and you make sure that all proxies are accessing the service in the same way. Any repetitive, non-trivial task can generally be implemented using a shared flow. Wherever you see a common pattern in your proxies, you should be thinking about shared flows. Here's an example of steps that may be required to call an external service. This sequence first checks a cache for a token. If the token is expired or missing, a request is built to retrieve and cache a new token. The service is then called with the token and variables are extracted. This process may be deployed as a shared flow. Instead of adding those nine policies to each API proxy, a single FlowCallout to the shared flow can be used.

### Video - [Lab Intro: Shared Flows](https://www.cloudskillsboost.google/course_templates/256/video/348446)

- [YouTube: Lab Intro: Shared Flows](https://www.youtube.com/watch?v=dU9HuqmZz50)

person: In this lab, you use a shared flow for the logic required when calling the back end. You'd create a new shared flow, which contains the policies to access the encrypted key value map and build the basic authentication header. In your retail API, you will remove those policies and instead use a flow call out to call this new shared flow.

### Lab - [Apigee Lab 9: Using Shared Flows](https://www.cloudskillsboost.google/course_templates/256/labs/348447)

In this lab, you'll put a series of conditional policies in a shared flow, allowing the functionality to be shared between proxies.

- [ ] [Apigee Lab 9: Using Shared Flows](../labs/Apigee-Lab-9-Using-Shared-Flows.md)

### Video - [Fault Handling](https://www.cloudskillsboost.google/course_templates/256/video/348448)

- [YouTube: Fault Handling](https://www.youtube.com/watch?v=Zhm6UUieEk0)

Person 1: We've been talking about policies, raising faults, so now let's understand what that means. This lecture, we'll talk about faults, how they are raised, and provide some best practices for handling faults. So what are faults? Faults are similar to exceptions in other programming languages, like Java or JavaScript. When a fault is raised the normal flow processing immediately aborts, proxy execution switches to the FaultRules element in the current endpoint. Processing will never return to the original flow. Note that the FaultRules element is optional, if no FaultRules element exists, an error response will still be sent to the client. We'll talk about the format for the error response shortly. Let's look at an example, we are processing the proxy endpoint request pre-flow, the JSON threat protection step executes successfully, but when we get to the verify key step, a fault is raised because the API key was not valid. No further steps will be processed in this flow. Execution will continue with the FaultRules element in the same endpoint, the proxy endpoint named orders V-1. The FaultRules element is specified in a proxy endpoint or target endpoint. By default, when you create a new proxy, there is no FaultRules element. When you add a new endpoint to a proxy, the FaultRules element also isn't added. Flows are shown visually in the proxy editor, and a flows policies can be edited using drag and drop. FaultRules are not shown visually in the proxy editor, you'll need to add FaultRules manually to the XML configuration of the proxy endpoint or target input. The FaultRules element contains zero or more FaultRules entries. Each FaultRule can have multiple steps and the steps can have conditions. Only the first FaultRule with a condition that evaluates to true is executed. The remaining FaultRules will be skipped. FaultRules without a condition always evaluate to true. Always remember that the FaultRules are evaluated in reverse order from bottom to top in a proxy endpoint. In this example, the invalid API key FaultRule condition would be evaluated first. The condition is true if the fault.name variable is failed to resolve API key. Fault.name contains the name of the fault raised by a policy, in this case, the verify API key policy. Policies will set this variable when raising a fault, so it is a useful variable for FaultRules, where we often build custom error responses. The conditions for the two FaultRules are mutually exclusive. If the first condition matches, the second wouldn't and vice versa, this means that you could evaluate the FaultRules from top to bottom or bottom to top, and you'd get the same result. If you can, it is the best practice to set up your FaultRule conditions this way so that you don't have to worry about the FaultRule's evaluation order. If your FaultRules do depend on this reverse ordering, add an XML comment to the FaultRule section to remind the reader that the evaluation of FaultRules is from bottom to top. The proxy endpoint FaultRules are the only part of a proxy that are evaluated in reverse order. The target endpoint FaultRules are evaluated in a normal top to bottom order. There is an optional FaultRule called the DefaultFaultRule, it is specified outside of the FaultRule section, it does not have a condition, instead, there is an element called AlwaysEnforce, which specifies whether the DefaultFaultRule is treated as a post-processing or DefaultRule. If AlwaysEnforce is set to true, the DefaultFaultRule will always run after the FaultRules. A FaultRule might execute or none might have true conditions. Either way, the DefaultFaultRule would run at the end before responding to the client. If AlwaysEnforce is set to false, the DefaultFaultRule would be executed only if no FaultRules had conditions evaluating to true. You can also create the same functionality as AlwaysEnforce equals false by not giving the last evaluated FaultRule a condition. A FaultRule without a condition is the same as having a FaultRule with a condition that is always true. Looking at this example, we first evaluate the FaultRules list, since this is in the proxy endpoint, we evaluate from bottom to top. The invalid API key FaultRule is evaluated first. If fault.name is failed to resolve API key, the assigned message policy AM-InvalidAPIKeyResponse would be executed and no other FaultRules would be evaluated. If fault.name is not failed to resolve API key, we check the next FaultRule. The unknown error FaultRule has no condition, so it will be evaluated. The assigned message policy, AM-UnknownError is executed. After the FaultRules, we evaluate the DefaultFaultRule element if it exists. In this case, AlwaysEnforce is true, so the DefaultFaultRule steps are executed. Log-LogError would execute regardless of whether any FaultRule was executed. When Faults are raised, processing is transferred to the FaultRules. Faults may be raised in a few ways, when a policy error occurs and the policy attribute continue on error is set to false, a fault is raised. If continue on error is set to true, the fault variables will be set, but processing continues in the current flow. Uncut exceptions from JavaScript and Java code also result in policy errors as well as raised faults if continue on error is false. Faults are also raised when a non-success response is received from a service call out or call to a target. By default, any status code in the 400s or 500s is considered an error. If an error status code is returned, processing will be transferred to the FaultRules in the current end point. For targets, processing would continue in the target endpoint. You can change the status codes that should be treated as successful by using the success.codes property. In this example, 401 and 404 status codes are treated as successful along with the default success status codes in the 100s, 200s and 300s. Error status codes should be treated as success codes when you plan to handle the processing of them in the normal proxy flow. 401, for example might be handled by using a service call-out to request another backend token and then resubmitting the original request using the new token. Faults can also be raised manually by using the RaiseFault policy. In this example, the request is checked for two required parameters, latitude, and longitude, if either is null, we returned an error to the client using a RaiseFault policy. The RaiseFault policy allows you to raise a fault manually exiting from the normal flow into the fault rules. Typically, RaiseFaults are conditionally executed. Within the RaiseFault policy, you can use the fault response element to build the response message as you would in an assigned message policy. This is generally easier than using an assigned message policy in the FaultRules to rewrite the response for your error conditions. The FaultRules you've defined, if any, will still be evaluated. When you create an API that is proxying a backend service, it is a best practice to only allow operations you have specifically approved through to the backend and reject others with a 404 not found error. Even if some of your backend calls are not documented, it is possible for bad actors to attempt to discover calls in your backend that they should not be calling. Also, if you are using your API proxy to protect against malicious data, you want to make sure that you've protected all calls to the backend. New backend calls that haven't been added to the proxy, would not have code checking for data validity. One way to specify the approved operations is to create a conditional flow in your proxy endpoint request for each operation that is allowed. Any data validation required for the operation could be put in the conditional flow request pipeline. The last conditional flow has no condition and will therefore always execute, if none of the other flow conditions are executed. In this flow, you raise a fault with 404 not found. With this pattern, calls that don't match one of the other conditional flows will never get through. Another way to protect your backend service, is to allow only those calls which match in open API specification. The OAS validation policy enables you to validate an incoming JSON request to response message against an open API specification. The OAS resource element is used to specify the open API specification. This specification must be an open API version 3.0 specification, that is stored as a resource in the proxy resources section. The options indicate how strict the validation should be. Validate message body must be set to true, to validate the payload against the operation's schema in the open API specification. If set to false, which is the default, then only the existence of the body is checked. Allow unspecified parameters determines whether headers query parameters or cookies that are not in the specification should be allowed. By default extraneous headers query parameters and cookies are allowed.

### Video - [Lab Intro: Fault Handling](https://www.cloudskillsboost.google/course_templates/256/video/348449)

- [YouTube: Lab Intro: Fault Handling](https://www.youtube.com/watch?v=LyDQZ-nUFPw)

Person: In this lab, you add fault handling to retail API. You use fault rules to rewrite error messages. Check the request for a content type header and raise a fault if it is not included. You will also use the 404 not found pattern to reject operations that are not handled in the proxy.

### Lab - [Apigee Lab 10: Handling Faults](https://www.cloudskillsboost.google/course_templates/256/labs/348450)

In this lab, you'll learn how to handle and raise faults.

- [ ] [Apigee Lab 10: Handling Faults](../labs/Apigee-Lab-10-Handling-Faults.md)

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/256/quizzes/348451)

#### Quiz 1.

> [!important]
> **Which of the following conditions might cause a fault to be raised in an API proxy? Select two.**
>
> - [ ] A policy has invalid XML formatting
> - [ ] A 404 Not Found status code received from a service callout
> - [ ] Executing a RaiseFault policy
> - [ ] Execution of a JSONThreatProtection policy when the Content-Type header is not application/json
> - [ ] A policy failure with continueOnError set to true

#### Quiz 2.

> [!important]
> **Which policy can be used to validate that a request matches an approved pattern?**
>
> - [ ] FlowCallout
> - [ ] AssignMessage
> - [ ] RaiseFault
> - [ ] OASValidation

#### Quiz 3.

> [!important]
> **Which of the following statements about shared flows are true? Select two.**
>
> - [ ] Shared flows cannot be nested.
> - [ ] Multiple shared flows may be attached to a flow hook.
> - [ ] A shared flow cannot be tested without calling it from an API proxy.
> - [ ] A shared flow attached to a flow hook will execute for all proxies in an environment.
> - [ ] A flow callout policy can be used to call any shared flow in an organization.

#### Quiz 4.

> [!important]
> **Which of the following policies can be used to call an external REST API? Select two.**
>
> - [ ] AccessControl policy
> - [ ] ServiceCallout policy
> - [ ] OAuthV2 policy
> - [ ] JavaScript policy
> - [ ] BasicAuthentication policy

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/256/video/348452)

- [YouTube: Module Review](https://www.youtube.com/watch?v=GzePxwbmYOo)

person: In this module, you were introduced to API mediation. You learned about XML, JSON and soap, and how you can manage these different formats in your API proxies. We discussed mediation patterns, using the service call policy to call external services and about writing custom code in our proxies. We learned about sharing proxy logic between API proxies using shared flows. And we learned about fault handling and proxies. You completed labs on mediation, shared flows, and fault handling.

## Traffic Management

This module introduces spike arrests, quotas, and caching within Apigee

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/256/video/348453)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=nmZ-IhfU3mo)

In this module, you will learn about API traffic management. You will learn how to use spike arrests, to control the rate of traffic, and how to use quotas to limit the number of requests over a specified period of time. You'll also learn how to use caching in your API proxies, reducing unnecessary traffic to backend services by caching HTTP responses. You will also complete labs that add a SpikeArrest, Quota, and ResponseCache policy to your retail API proxy.

### Video - [Apigee Components](https://www.cloudskillsboost.google/course_templates/256/video/348454)

- [YouTube: Apigee Components](https://www.youtube.com/watch?v=OXMsfJwse_s)

person: In order to really understand Apigee traffic management, you need to understand the system components of Apigee. In this lecture, we'll discuss these components. This is a simplified logical diagram of the primary Apigee components for Apigee. Although implementations differ, depending on the deployment topology, the logical components remain the same regardless of how Apigee is deployed. Gateways are responsible for routing and processing runtime traffic flowing to the proxies. Logically, we think of the gateway containing a router and a message processor. The router routes the request to the correct proxy. We have seen that environment groups, environments and base parts are used to determine which deployed proxy receives the request. The message processor handles the API traffic running within the proxies. The runtime data store manages persistence of runtime data. Gateways are stateless and all runtime data like tokens and cache entries are persisted in the runtime data store. Only gateways and the runtime data store are required for runtime traffic to be successfully processed. Other components can fail or lose connectivity, and API traffic will still be handled. For multi-region deployments, runtime data propagation between regions is handled using the database replication. Gateways are logically fronted by a load balancer. The load balancer distributes traffic among the gateways. Runtime API capacity can be scaled up and down by scaling up and down the number of gateways. In managed Google Cloud deployments of Apigee, the number of gateways is automatically scaled up and down based on how loaded the gateways are. The runtime data store and other components can also be separately scaled when necessary. You should be aware that increasing the API traffic capacity of Apigee may result in a corresponding increase in traffic being sent to back end services. The analytic services component stores runtime metrics and provides reporting for those metrics. At the end of an API call, the gateway sends an analytics record to analytic services. This record includes standard metrics about the API call and can also contain custom metrics as specified in the proxy. This record is queued and later processed by analytic services, making it available for reporting. The Apigee API allows configuration of organization entities and the API lifecycle. It also allows retrieval of runtime analytics data for the organization. The Apigee API may be used by automation pipelines, like continuous integration, continuous deployment, or CI/CD. Tools can also use the Apigee API to export analytics data or integrate with customer dashboards. The Apigee API stores requested changes to an organization in the management database. The management database stores non-runtime data for an organization. The runtime detects when deployments and other random entities are changed. Apigee users, including organization administrators, use Apigee's console to manage organizations and environments and deploy and trace proxies. The Apigee console uses the Apigee API to integrate with Apigee. Most of the tasks performed in the console can also be done using the Apigee API. A developer portal is a web interface designed to support app developers in the use of APIs implemented on Apigee. App developers can sign up for the portal where they can discover APIs, exploring the API's using live documentation, and register apps to use the API's. Apigee supports two types of developer portals. A flexible Drupal based portal and a lightweight, integrated portal. We will discuss the two types of portals in a later lecture. Developer portals use the Apigee API to integrate with Apigee. A hybrid deployment of Apigee provides a fully-featured API management platform with infrastructure management responsibilities shared between Google and the customer. The management plane for hybrid is hosted in Google Cloud and is managed by Google. The runtime plane is managed by the customer. Runtime plane components are containerized and can be run in private data centers or in a private cloud in Google Cloud or other clouds. From the perspective of an API developer, the hybrid deployment works the same way as the Google Cloud managed deployment.

### Video - [Rate Limiting with Spike Arrests and Quotas (1)](https://www.cloudskillsboost.google/course_templates/256/video/348455)

- [YouTube: Rate Limiting with Spike Arrests and Quotas (1)](https://www.youtube.com/watch?v=MW5DeStm-Gc)

person: This lecture discusses methods for limiting traffic to API proxies. When you increase the usage of your API, or make it available on the internet for the first time, your API traffic may significantly increase. Unless your backends can scale up to handle all the additional traffic, you may need to limit the rate of traffic to your back end services. rate limiting is the process of rejecting excess traffic to your API, and your back end services. What excess means is up to you. Apigee provides two built-in types of rate limiting. The first type protects your back end services from spikes in traffic by limiting the allowed rate of API traffic. The second type sets specific limits on the number of requests allowed over a period of time. Spikes of API traffic can overwhelm backends, thus causing services to become extremely slow, or go completely out of service. Protecting your back end against spikes in traffic is solving a technical problem and is typically an ID concern. We generally use load testing to determine how much sustained traffic can be handled by our back end services. The spike arrest policy blocks traffic spikes by specifying a maximum rate of requests. The most important feature of the spike arrest policy is that it does not use a counter. Instead, it keeps track of the time that the last matching request was allowed through the spike arrest policy. The identifier element specifies the group of traffic that will be rate limited together. For example, if the identifier is the client ID of the application, any traffic for the same application will be considered when rate limiting using the spike arrest policy. The spike arrest policy running in a proxy on a message processor will reject a request. If that same message processor has allowed traffic for the same identifier too recently, we will discuss the rate element in more detail. When traffic is rejected, the policy will return the status code 429 Too Many Requests. Let's take a look at how the identifier is used in the policy. The first policy specifies a rate of 100 per second. There is no identifier configured for this policy. So this applies to all requests sent to this proxy. The spike arrest policy would use all traffic for determining whether to allow a request through. The second policy uses an identifier with a reference to client underscore ID. The client ID variable is set when an API key or auth token is verified and contains the client ID for the application. Therefore, the second policy would only check the last allowed traffic for the same application when determining whether to reject the traffic. It is not unusual for a proxy to have two spike arrest policies like this, with one controlling the overall traffic rate and another controlling the rate allowed for each app. Let's look at the rate configuration for spike arrest. The rate for a spike arrest policy can be specified per minute or per second. The rates are specified as a number followed by PM, indicating per minute, or PS, indicating per second. The spike arrest policy does not use counters. Instead, the rate element indicates how often you are allowed to pass through traffic on a given message processor. Let's look at a couple of examples. And see how rate can be used to calculate the time period between requests. When the rate element is specified per second, the rate is calculated as a number of full requests in intervals of milliseconds. If the rate is specified as 10 per second, this is equivalent to 10 requests per 1000 milliseconds, which is equal to one request per 100 milliseconds. This means that a request will be rejected if the previous allowed request to the same proxy in the same message processor with the same identifier was less than 100 milliseconds ago. When the rate element is specified per minute, the rate is calculated as a number of full requests in intervals of seconds. A rate of 30 per minute is equivalent to 30 requests per 60 seconds. So 30 per minute is one request every two seconds. If a matching request was allowed less than two seconds ago, the traffic would be rejected.

### Video - [Rate Limiting with Spike Arrests and Quotas (2)](https://www.cloudskillsboost.google/course_templates/256/video/348456)

- [YouTube: Rate Limiting with Spike Arrests and Quotas (2)](https://www.youtube.com/watch?v=E9iiiRBMcJM)

person: The spike arrest policy can be confusing. Let's try to get a deeper understanding of how the spike arrest policy works. First, how spike arrest does not work. Spike arrest does not use a counter. If the rate is 10 requests per second, the spike arrest policy does not keep track of the first 10 requests within the second and reject the 11th. Here is how spike arrest works. Remember, a configured rate of 10 per second indicates 100 milliseconds between requests. Each time the spike arrest policy is evaluated, it checks the last time that matching traffic was allowed through the spike arrest policy. In this case, if the previous matching request came in less than 100 milliseconds ago, the traffic will be rejected. Looking at the example, the first three requests are each allowed because no traffic had been received in the previous 100 milliseconds. The fourth request is rejected, because the third request had been allowed less than 100 milliseconds before the fourth. The fifth request is allowed because even though the fourth request came in less than 100 milliseconds before the fifth, that fourth request was rejected, and therefore not counted as allowed traffic. You may be wondering now whether it is possible to send two total requests and have second request be rejected. The answer is yes. You might think that this doesn't constitute much of a spike, and you'd be right. The spike arrest policy works very much like the examples I've been giving, but actually uses a slightly different algorithm. In order to allow for many spikes without rejecting traffic, the spike arrest policy is implemented using the token bucket algorithm. Here's how the token bucket algorithm works. Imagine there is a bucket and it starts out filled with tokens. If the configured rate is 10 per second, a token would be added to the bucket every 100 milliseconds. If the bucket is already full, the token is discarded. When a request is received, the bucket is checked for a token. If there is at least one token in the bucket, the token is taken and the traffic is allowed. If there are no tokens in the bucket, the traffic is rejected. You can see how this would allow bursts of traffic. But the overall rate of traffic would still be one request every 100 milliseconds. For the spike arrest policy, the size of the bucket depends on the number used in the configured rate. The bucket size is equal to the count divided by 10, with the minimum bucket size of one. For 10 per second, 10 divided by 10 is one. So the bucket size is one, which means that this algorithm works very much like the earlier examples. But for a configured rate of 600 per minute, which is equivalent to 10 per second, the bucket size would be 600 divided by 10, or 60. This means that a burst of 60 simultaneous requests would be allowed. But then the bucket would be empty and a token would be added to the bucket once per 100 milliseconds. When creating a spike arrest policy, you can choose the per minute or per second configuration to control this behavior.

### Video - [Rate Limiting with Spike Arrests and Quotas (3)](https://www.cloudskillsboost.google/course_templates/256/video/348457)

- [YouTube: Rate Limiting with Spike Arrests and Quotas (3)](https://www.youtube.com/watch?v=BvmUcFnputY)

Now, let's talk about quotas. The SpikeArrest policy solves a technical problem, protecting your backends from excessive traffic spikes. A Quota policy solves a business problem. How many requests should be allowed for a specific entity over a specified period of time? The policy keeps track of the number of requests that have been accepted. Each time a request is handled by the Quota policy, the counter is checked to see whether the maximum number of requests has been received. If the quota limit has not been reached, the quota counter is incremented and processing continues. If the quota limit has been reached, The quota policy raises a fault to reject the request. Requests will be rejected until the time period has elapsed and the count is reset to zero. A quota does not control the rate of requests, so quotas don't protect against traffic spikes. For example, if a quota allowed 1000 requests per month, it would be legal to use all 1000 requests during the first minute of the month. The Quota and SpikeArrest policies solve two different problems, so it is very common to use both types of policies in the same proxy. Unlike the SpikeArrest policy, the Quota policy uses a counter. The Quota policy keeps track of the number of matching requests received over a given time period. The Quota policy count is typically shared among all the message processes, with the counter values stored in the runtime data store. A quota is scoped to a single proxy and policy. Counters cannot be shared between policies or proxies. The combination of policy, proxy, and Identifier value uniquely specifies a Quota counter. If no Identifier is specified, all traffic through the policy in the proxy is counted. The Quota policy, like the SpikeArrest policy, returns 429 Too Many Requests when rejecting a request. The number of allowed requests per time period is specified using three configuration elements. Allow is the number of allowed requests per time period. TimeUnit is the unit of time to use: minute, hour, day, week or month. Interval is the number of time units in the period. So if Allow is 10, Interval is 1, and TimeUnit is hour, the allowed quota is 10 requests per every 1 hour. Each of these elements can be configured using a reference to a variable, a hardcoded value, or both. If the reference exists, and the variable has a value, that value is used. If there is no configured reference, or if the variable has a value of null, the hardcoded value is used. The Allow, Interval, and TimeUnit variables are often configured using the API product quota settings. When an API key or OAuth token is verified, the value of the quota settings for the API product are automatically populated into variables. These variables can be used in the quota policy to specify a quota for the application associated with the product. The API product quota settings may be used to create different levels of service for applications. For example, an API product named Basic might allow 100 requests per month, and an API product named Premium might allow 100 requests per day. In the Quota policy shown here, variables populated by the VerifyAPIKey policy named VK-VerifyKey are used to set the quota values. The UseQuotaConfigInAPIProduct element is an alternative method for using the API product quota settings without needing to specify each of the very long variable names. The stepName attribute specifies the policy that determines the API product for the calling application. This policy can be a VerifyAPIKey policy or an OAuth policy using the VerifyAccessToken operation. The DefaultConfig element specifies the default values used if the quota values are not set in the API product configured for the application. A quota is specified with a type. The quota type specifies when the quota time period starts and when it resets. There are four different types of quotas. When a type is not specified, the quota period starts at the beginning of the next GMT minute, hour, day or month, depending on the time unit. It then resets every interval number of time units. For example, if the quota is 1000 requests per 1 day, the quota would reset at midnight each day. Calendar has an explicit start time. The StartTime is specified as an element in the policy. Like the default quota type, a calendar type quota is reset each time an interval number of time units elapses. The flexi type is like the calendar type, except that the time period begins when the first request is handled by the Quota policy. The rolling window calendar type works differently from the other calendar types. Requests are counted over the time period ending at the time of the request, and accepted or rejected based on this rolling window. So, for example, if the Quota is checking a request at 10 o'clock, and the quota allows 100 requests every two hours, the request would be rejected if 100 or more requests had been accepted between 8 o'clock and 10 o'clock.

### Video - [Rate Limiting with Spike Arrests and Quotas (4)](https://www.cloudskillsboost.google/course_templates/256/video/348458)

- [YouTube: Rate Limiting with Spike Arrests and Quotas (4)](https://www.youtube.com/watch?v=USfxI6giMIw)

person: By default, code accounts are separately stored for each message processor. This is equivalent to setting the distributed element to false. When distributed is set to true, a shared counter is used for all the message processes. This is typically the setting you will want to use. Scaling up and down message processes should typically not affect the number of allowed requests for a quota. When using a quota with distributed set to true, you can specify whether the update of the counter is synchronous or asynchronous by setting the synchronous element. When synchronous is set to false, you can configure how often the central counter is updated by using the asynchronous configuration settings. Setting SyncIntervalInSeconds specifies the frequency that the message processor synchronizes its count with the central counter. 10 seconds is the minimum interval you can set. SyncMessageCount specifies that the counter be synchronized whenever the configured number of requests has been received. If the sync message count is set but sync interval in seconds is not, the count will also be synchronized at least every 60 seconds. If synchronous is set to false, but the AsynchronousConfiguration settings are not set, the count will be synchronized every 10 seconds. So how do you decide between synchronous and asynchronous for distributed quotas? When a distributed quarter is asynchronous, some extra requests may be accepted, due to the sinking frequency. A synchronous quota is much less likely to allow extra requests. However, a synchronous quota can introduce significant latencies, especially under load. Each quota policy must check the runtime data store before allowing the request through. In most cases, a few extra allowed requests won't make a difference. We strongly recommend that you use a synchronous quotas if your use case will tolerate the extra requests. The MessageWeight element is used to modify the impact of a request against the quota. Using a message rate of two would count that request the same as two requests that have a MessageWeight of one. When MessageWeight is not specified, the default weight is one. MessageWeight can be used when certain messages use more resources than others. For example, an API that allows multiple items in a batch request might count one request per item in the batch. The reset quota policy is used to decrease the number of counted requests in a quota counter. These requests are decreased without regard to the quota time period. The quota allowance will reset to the allow value at the time the current quota period ends. The quota element specifies which quota policy is being referenced. The quota policy name can also be set via a reference. Identifier indicates the identifier value for the quota counter to be updated. This value can also be specified by name or reference. For example, if the quota policy uses an identifier of the client ID, a variable containing the same client ID will generally be used for the reset quota policy. If no identifier is used in the quota policy, use the name underscore default to update the quota counter. Allow specifies the number of counted request to be removed. For example, if the current count is 500, and the maximum count is also 500, specific and allow a value of 100 will reduce the count to 400 and allow 100 more requests come in during the time period.

### Video - [Lab Intro: Traffic Management](https://www.cloudskillsboost.google/course_templates/256/video/348459)

- [YouTube: Lab Intro: Traffic Management](https://www.youtube.com/watch?v=hL9Wq62P8ag)

person: In this lab, you add traffic management to your retail API. You add a spike arrest, intended to protect against bursts of traffic and a quota that uses limits set in the API product.

### Lab - [Apigee Lab 11: Managing Traffic](https://www.cloudskillsboost.google/course_templates/256/labs/348460)

In this lab, you'll learn how to perform traffic management using the SpikeArrest and Quota policies.

- [ ] [Apigee Lab 11: Managing Traffic](../labs/Apigee-Lab-11-Managing-Traffic.md)

### Video - [Caching (1)](https://www.cloudskillsboost.google/course_templates/256/video/348461)

- [YouTube: Caching (1)](https://www.youtube.com/watch?v=sDMex90Pd0c)

person: This lecture will discuss caching within API proxies. We will discuss how caches work in Apigee, how to use general purpose caching to store information between calls, and how to use a response cache to easily eliminate unnecessary calls to backend services. Caches store entries by cache key, which is an index into the data store. The cache key can be used to search for and retrieve an entry from the cache. When an entry is stored, a time to live or TTL is also provided. This provides a method for expiring cache entries. Once the time to live has elapsed, the entry is automatically removed from the cache. In an API proxy, you can persist state between call using a cache. You can also use caching to eliminate unnecessary backend service calls, by caching HTTP responses in the API proxy. Reducing traffic to the back end can improve stability of your back end services. and allow your APIs to handle more traffic. Call latency is improved for calls that can be served from cache. And caching responses can reduce the number of calls to a service, and thus reduce the cost of a metered service. Before we discuss how caching policies can be used in your proxies, let's learn how caches work in Apigee gateways. Apigee caches have two levels L1 and l two. There is a leveL1 or L1 in memory cache in each message processor. The L1 cache is checked first whenever a proxy is looking up an entry in the cache. If the entry is in the L1 cache, it is available very quickly. The L1 cache automatically caches entries for a short period of time, currently one second. Also, if the cache fills up, least recently used entries could be removed from the L1 cache. When a message processor does not find an entry in L1 cache, it next checks L2 cache. The entries for level two or L2 cache are stored in a persistent database. L2 cache entries are removed only when the cache entry's time to live has elapsed, unless the maximum number of cache entries is reached. L2 cache is slower than L1 cache, but much faster than making a network call to retrieve the data. When a message processor successfully retrieves a cache entry from L to cache, the message processor stores the entry in L1 cache for a short time. When an entry is updated or deleted by a message processor, it is updated or deleted in the message processor's L1 cache and the shared L2 cache in the runtime data store. That entry would be unchanged in the L1 cache of other message processes. Any stale data would remain in the L1 cache until the short L1 time to live expired. L2 changes in the runtime data store are propagated to other regions using database replication. Data can be stale for short periods of time in the L1 cache and L2 cache database replication may cause different regions to receive data at different times. You should architect your proxies and your use of caches with this behavior in mind. Apigee automatically caches several types of entities in the L1 cache for 180 seconds after access. OAuth access tokens, developers, developer apps and API products are all cached automatically for 180 seconds after access, as are any custom attributes attached to them.

### Video - [Caching (2)](https://www.cloudskillsboost.google/course_templates/256/video/348462)

- [YouTube: Caching (2)](https://www.youtube.com/watch?v=GGaj_Uyfsxs)

Hansel: The general-purpose caching policies can be used to store and retrieve strings. There are separate policies to populate, look up, and invalidate entries from the cache. The PopulateCache Policy is used to add an entry to the cache. The CacheResource element specifies the logical name of the cache. The CacheResource is implicitly created when the proxy is deployed. Caches are always scoped at the environment level. If the CacheResource element is not specified, a default cache is used. Expiration for the entry is set using the ExpirySettings element. You can expire an entry at a particular date, a particular time of day every day, or in a specified number of seconds. Each of these types of exploration can be hardcoded or specified using a reference to a variable, with the variable reference taking precedence. The Source element specifies the name of the variable that contains the string value to be stored. The Prefix element can be used to specify a prefix for the cache key string. The prefix will scope the entry so that it can only be retrieved using the same prefix. If the Prefix element is not set, the Scope element should contain the cache's scope. The prefix will be automatically generated based on the scope. A Global scope makes the entry available for all proxies in the environment. The Global prefix includes the organization name and the environment name. Application scope makes the entry available anywhere in the existing proxy. The prefix includes the organization, environment, and proxy name. Exclusive scope adds the name of the endpoint in which the policy is attached. The Global and Application scopes tend to be the most useful. Cache keys are generated using the prefix and the key fragments. Each key fragment can be specified as a hardcoded value or a reference. Double underscores separate the concatenated parts of the cache key string. In this example, the application scope specifies the prefix, which includes the organization, environment, and proxy name. The two key fragments come from the user ID variable and the backend service variable. The prefix and fragments are separated by double underscores. The LookupCache policy looks for an entry in a cache. The prefix, scope, and cache key work the same way as in the PopulateCache policy. The AssignedTo element specifies the name of the output variable to be populated with the value of the entry. CacheLookupTimeoutInSeconds specifies the timeout for the cache lookup. The default timeout is 30 seconds, which is a very long timeout, so you should generally set a shorter timeout. When the policy has completed, it sets a cache hit variable to true or false, indicating whether the entry was found and returned. The InvalidateCache policy purges entries from a cache. Entries that match the supplied cache key will be purged. The CacheContext element is used to override the API proxy name, proxy endpoint name, and target endpoint name when the policy builds a prefix using the scope. This may be necessary if cache invalidation is being done in a different proxy from where cache population occurred. Sometimes API calls can be expensive. They can be financially expensive, where you are paying for access to a service, or they can be slow or resource-intensive, where repeatedly calling the service can add significant latency. If the API's data is cacheable, the LookupCache and PopulateCache policies may be used to store the information in a cache and reduce the number of calls to the expensive service. An example might be a third-party mapping API that returns details about a zip code. The data from this API is probably cacheable because there are a relatively small number of zip codes and the mapping data is probably relatively static. The pattern is relatively simple. The proxy first attempts to look up the entry in the cache by zip code. If the entry is not in the cache, the proxy calls out to the service and populates the cache with the result. If the entry is in the cache, the ServiceCallout and PopulateCache policies are skipped.

### Video - [Caching (3)](https://www.cloudskillsboost.google/course_templates/256/video/348463)

- [YouTube: Caching (3)](https://www.youtube.com/watch?v=dAYrY00TbOA)

person: The second type of caching used in Apigee proxies is response caching. Entire HTTP responses are cached, which eliminates calls to the back end, when the response would be the same as the cached response. A ResponseCache policy is used to streamline the process of caching HTTP responses. The ResponseCache policy handles both the lookup and population of the cache. The policy is attached in the request flow, where it checks the cash, and in the response flow, where it populates the cash. Let's look at how the ResponseCache policy works. The ResponseCache policy is attached in the request flow and the response flow. In the request flow, the ResponseCache policy should be attached after the security and input validation policies. If the request is not valid, the ResponseCache policy should not be used. In the response flow, the policy should be attached toward the end of the response handling. Some operations like logging would need to happen after the response cache step in the response flow. When a request comes in, policies are executed up to the ResponseCache policy in the request flow. The ResponseCache policy uses the cache key to check for a matching entry. When there is no match, which is called a cache miss, processing continues in the request flow as usual. The rest of the policies in the request flow are executed and then the back end target is called. In the response flow, when execution reaches the second attachment of the ResponseCache policy, the cache is populated with the response message. The rest of the policies following the ResponseCache are then executed. Later, when a request comes in, policies are again executed up to the ResponseCache policy in the request flow. The ResponseCache policy uses the cache key to check for a matching entry. In this case, there is a match, a cache hit. Execution continues immediately with the ResponseCache policy in the response flow. The response message is loaded from cache, and the remaining policies are executed. When there is a cache hit, all of the processing between the two attachments is skipped. This includes the call to the backend service, and any other services called. This can result in a much faster response to the client and less API traffic to the backend. The ResponseCache policy uses cache keys and scope the same way that they are used in the general purpose caching policies. Setting UseAcceptHeader to true will automatically add the Accept header as a cache key fragment. The Accept header is used by the client to request a certain content type for the response. If your API allows more than one content type format for responses, use accept header can make sure that the client is not served the incorrect format from the cache. Setting ExcludeErrorResponse to true ensures that a response will only be cached if the status code is between 200 and 205. Status codes in the 200s indicate success. SkipCacheLookup can be configured with a conditional expression. If the SkipCacheLookup condition evaluates to true, the cache will not be searched for a a matching entry during the execution of the ResponseCache policy in the request flow. SkipCacheLookup can be used to force an update to the cache. The back end target will be called even if there was already a matching cache entry. The response from the backend could then be stored in the cache. Cache lookups should always be skipped if the request is not safe. Generally, when verbs other than get are used, there are modifications to the state of the backend. Retrieving the response from cache would cause those modifications to be skipped. When the skip cache population condition is true, the cache is not updated during the response flow. Population should be skipped if a response is unlikely to be used again, or the response is an error response. Like cash lookup, cash population should not be done for unsafe requests. When UseResponseCacheHeaders is set to true, any cache related headers in the response will be honored in this order. First is a cache control header specifying s-maxage. s-maxage specifies the number of seconds to cache the response in a shared cache. A cache in an API proxy would be considered a shared cache. Second is a cache control header specifying max-age. This indicates the number of seconds to cache the response in a non-shared cache. Third is an expires header specifying the date and time of expiration. If one of the cache headers is used, the indicated expiration is compared to the expiry settings element. Whichever would result in the lower expiration time will be used. There are several best practices you should consider when adding a response cache to your proxy. First, only GET requests should be cached. Other requests have side effects, and you wouldn't want to skip the side effects by skipping the call to the backend. Second, you should always think carefully about which cache key fragments to use and when to skip cache lookup and population. Any variable that causes the response to change should be used in the cache key. Third, whenever your API returns user data, you must include a unique user identifier as a key fragment. This is extremely important. One of the worst things you can do in an API is return one user's data to a different user. Fourth, it is best to use the proxy.pathsuffix variable and specific query parameters as cache key fragments. If you use the full URL in your cache key, you are likely to get fewer cache hits. Clients can send query parameters in any order, or send ignored query parameters, causing unnecessary cache misses.

### Video - [Lab Intro: Caching](https://www.cloudskillsboost.google/course_templates/256/video/348464)

- [YouTube: Lab Intro: Caching](https://www.youtube.com/watch?v=4WhrpnP-ye8)

Hansel: In this lab, you add a response cache to your retail API. You will reduce repeated requests to the backend by caching the entire response when retrieving products by ID.

### Lab - [Apigee Lab 12: Caching](https://www.cloudskillsboost.google/course_templates/256/labs/348465)

In this lab, you'll learn how to use the ResponseCache policy to cache entire responses, and how to use the LookupCache and PopulateCache policies to cache data.

- [ ] [Apigee Lab 12: Caching](../labs/Apigee-Lab-12-Caching.md)

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/256/quizzes/348466)

#### Quiz 1.

> [!important]
> **Which of the following statements about the Quota policy are true? Select two.**
>
> - [ ] The Quota policy does not limit the rate at which requests can be received.
> - [ ] For most use cases, it is recommended to use synchronous counter updates when using distributed quotas.
> - [ ] The time unit for a Quota policy can be set to per second, minute, hour, day, week, or month.
> - [ ] When Distributed is set to true in a Quota policy, message processors will share counts for a given identifier.
> - [ ] Configuring the quota settings for an API product will automatically limit the number of requests to an API proxy without requiring a Quota policy.

#### Quiz 2.

> [!important]
> **Which of the following statements about the SpikeArrest policy is true?**
>
> - [ ] The SpikeArrest policy uses counters to keep track of traffic.
> - [ ] The SpikeArrest rate can be specified as per second, per minute, or per hour.
> - [ ] When a spike arrest rate is exceeded, the status code 400 Bad Request is returned.
> - [ ] The SpikeArrest policy is primarily designed to protect services against traffic spikes.

#### Quiz 3.

> [!important]
> **Which of the following statements about the ResponseCache policy are true? Select two.**
>
> - [ ] If a response for a particular cache key is currently cached, making a request that results in the same cache key must always return the cached response.
> - [ ] In the ResponseCache policy, cache lookup should be skipped for requests used to update data.
> - [ ] A ResponseCache policy must be attached at two places in the API proxy: the proxy endpoint and the target endpoint.
> - [ ] When the API returns user data, a unique user identifier should always be used as a cache key fragment.
> - [ ] In the ResponseCache policy, setting UseResponseCacheHeaders to true causes the cache headers from the backend to always override the policy expiration configuration.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/256/video/348467)

- [YouTube: Module Review](https://www.youtube.com/watch?v=ffaF2FzdKDE)

Hansel: This module taught you about spike arrest, which is how we control the rate of traffic through our API proxies. You learned about quotas, which are used to control the number of requests that can be made to an API. And you learned about the caching policies that can be used in API proxies to reduce unnecessary calls to backend services. You also completed labs that added a spike arrest, quota and a response cache to your retail API proxy.

## API Publishing

This module introduces API publishing, the developer portal, and API versioning

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/256/video/348468)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=Sjpsuy35ffM)

Mike: In this module, you'll learn about API publishing and developer portals. We use developer portals to publish API products and allow app developers to discover APIs and register apps to use them. During a lab, you will create an integrated developer portal and publish your retail API product to the portal. You will also learn about versioning rest APIs.

### Video - [REST API Design, Part III: Versioning](https://www.cloudskillsboost.google/course_templates/256/video/348469)

- [YouTube: REST API Design, Part III: Versioning](https://www.youtube.com/watch?v=mnbTAoveuE0)

Mike: This lecture is the third of three lectures on REST API design. The first lecture introduced rest APIs and explained how to design restful APIs. In the second lecture, we discussed REST API status codes and responses. In this lecture, you will learn about API versioning. Before we discuss what API versioning is, let's clear up some misconceptions by explaining what API versioning is not. API versioning is not an indicator of your current release version. A release version tracks the software development or release cycle. This typically has no effect on the consumers of an API if care is taken to make changes backward compatible. Therefore, the version of an API is not tied in any way to the proxy revision. API versioning is also not something you should do frequently. Changing an API's version often requires that changes be made in the apps that consume your API. This means extra work for the app developer. The good news is that most changes to an API can be made without changing the API version. Here's what API versioning is. API versioning is a contract. Apps written against a specific API version should continue to work, even if changes are being made to the API. This is especially important for mobile apps because many users do not regularly update their apps. API versioning is used to communicate breaking changes. Breaking changes may be necessary when you add new required inputs to a request or when data that was previously available is removed from responses. If these changes are being made due to a security issue with the previous design, the change is definitely necessary. You can often find a way to add other functionality to an API without breaking the contract. Remember, changing the version has an impact on your app developers, so only do it when necessary. Let's take a look at a URL-based versioning scheme. In this case, the API and version are specified immediately after the host name and resources for the API follow the version number. Versioning is typically done at an API level. If you have multiple APIs, you should generally version them separately. In this example, the campus API is version one, and the fulfillment API is version two. You should avoid minor versions for your APIs. Version changes are supposed to happen infrequently. And they typically have a major impact on applications. You may be used to seeing version numbers that include both major and minor versions. For example, version 1.2 would have a major version of one and a minor version of two. Including minor versions in your version numbers implies that versions change frequently, or that version changes are made for small or incremental updates to the API. APIs may never need to have version changes. For example, Apigee's management API is still version one at the time of this recording. This doesn't mean that the management API hasn't changed. It means that the changes were made without breaking backward compatibility. URL-based versioning is a common scheme. It is easy to see which version of an API you were calling because it is specified in the URL. This is not the only scheme though. Whatever you decide, be consistent across your APIs. So how do we manage multiple versions of an API, and when should APIs be deprecated? It is generally recommended to only have two live versions of your API available at any one time. Having too many active versions can be difficult to maintain and gives a developer less incentive to upgrade to the latest version. You should also try to have a transition period of no more than six months before the old version is deprecated. This should be enough time for the developer to make any necessary changes. Here's how you might handle moving from API version one to version two. The upgrade and deprecation process typically has three phases. The announce phase is when you communicate version two to the app developer community. You specify all of the changes for version two, why the changes are being made, and when the new version will be available. If there are benefits for app developers in the new version, make sure to publicize those. You should also explain the process for upgrading to the new version. Next is the launch and maintain phase. The new API Version can be launched with a blog post. You should continue to market the new version because some app developers may not have made any progress in upgrading to the new version. Any issues that arise when using the new version or upgrading to the new version should be handled quickly. Try to make the upgrade process as easy as possible. Developers should be confident that upgrading to version two is safe. The third phase is deprecate. When you determine the deprecation date for the previous version, it needs to be clearly communicated to the app developer community. Any significant issues should have been resolved during the launch and maintain phase. As the deprecation date gets closer, you should use analytics to see which apps are still sending traffic to the old version, and reach out to developers to help them understand that their apps will stop working after the deprecation date if they are not upgraded. When the deprecation date finally arrives, you can completely turn off version one of the API and change error messages to explain that call should use version two. You may instead choose to capture information about all apps that are still sending to version one, and give the app developers a short grace period to fix their apps. Eventually, you'll only have to maintain version two. Remember, you should avoid changing the version when possible. Most features can be added to an API without breaking backward compatibility.

### Video - [Developer Portals](https://www.cloudskillsboost.google/course_templates/256/video/348470)

- [YouTube: Developer Portals](https://www.youtube.com/watch?v=t0Uu23Ksu18)

Mike: An API is only as useful as the apps that use it. Developer portals help app developers make apps that use your APIs. Remember, we design our APIs with the app developer in mind. App developers are customers of our APIs. Whether developers are internal or external or partners, we want to help app developers use our APIs to build great apps. The first step is to build great API products and publish them. After your APIs are published, you still have more work to do. Next, you have to attract and engage app developers. This happens in the developer portal. The developer portal is where an app developer can learn about the APIs you're providing and any service levels for your APIs. An app developer can use the live documentation in the developer portal to learn how to use your APIs and even try them out. This live documentation is created by linking an open API specification to an API product that is published on the developer portal. App developers can also use a self-service process to register their apps to be able to use your API products. Self Service allows developers to get started with your APIs as quickly as possible. The rich documentation and self signup minimizes the time it takes a developer to build an app using the API. The first type of developer portal is the integrated portal. This is a lightweight self-service portal that can be created and deployed quickly. The second type of portal is the Drupal 8 portal. Drupal is an enterprise level content management system. Apigee has created Drupal modules that integrate with Apigee and support a Drupal-based developer portal. The Drupal 8 portal allows self-service design, creation and management of the portal. Drupal 8 portals are highly flexible, but there is significantly more effort required to develop and maintain them than for the integrated portal. The integrated and Drupal 8 portals integrate with Apigee by using Apigee management API calls. If you need to, you can implement your own custom portal on any platform by using Apigee APIs. However, creating a developer portal from scratch is a large complex project and requires a very high level of effort. How do you decide which type of portal to use? Typically, you won't choose to build your own portal. It requires a huge effort. You should generally only choose the custom portal if you absolutely have to integrate the portal with an existing site that is not based on Drupal. The decision usually comes down to integrated versus Drupal. The integrated portal may be used if speed to launch or costs are most important. The integrated portal is simple to configure, is the fastest to launch, and requires the least effort. Hosting of the integrated portal is included in the cost of Apigee. Use cases that are complex or require features like blogs, forums, or heavy customization typically require a Drupal 8 portal. You must be willing to pay for hosting the portal and more importantly, be willing to absorb the higher total cost of ownership.

### Video - [Lab Intro: Developer Portal](https://www.cloudskillsboost.google/course_templates/256/video/348471)

- [YouTube: Lab Intro: Developer Portal](https://www.youtube.com/watch?v=RPV3oYEhGY0)

Mike: In this lab, you take your retail API and publish it to a new developer portal. You will also use the open API specification to provide live documentation within the portal, which allows you to make API calls directly from the portal into your API.

### Lab - [Apigee Lab 13: Configuring and Using a Developer Portal](https://www.cloudskillsboost.google/course_templates/256/labs/348472)

In this lab, you'll learn how to configure the integrated developer portal and publish your API products to it.

- [ ] [Apigee Lab 13: Configuring and Using a Developer Portal](../labs/Apigee-Lab-13-Configuring-and-Using-a-Developer-Portal.md)

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/256/quizzes/348473)

#### Quiz 1.

> [!important]
> **Which statements about developer portals are true? Select two.**
>
> - [ ] The developer portal is typically where API keys are created.
> - [ ] API developers are the primary users of developer portals.
> - [ ] A do-it-yourself custom portal is typically the best option for use cases that require more customization than allowed by the integrated developer portal.
> - [ ] Of the developer portal options, the integrated developer portal requires the least amount of effort to set up.
> - [ ] The integrated developer portal uses API products to define the paths available when trying the API in the portal.

#### Quiz 2.

> [!important]
> **When should an API's version be changed?**
>
> - [ ] Whenever a new API resource is added
> - [ ] Whenever you save a new proxy revision, because revision is an indicator of the API version
> - [ ] Whenever a new required parameter is added to an existing API call
> - [ ] Whenever your backend service changes its version

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/256/video/348474)

- [YouTube: Module Review](https://www.youtube.com/watch?v=tCfkEiUg_Y0)

Mike: During this module, you learned about considerations when versioning your APIs. You learned why developer portals are important and created and integrated a developer portal during a lab. During this lab, you added cores to your API and published your API product to the developer portal. And finally, you were able to try out your API using live documentation in the developer portal.

## Logging and Analytics

This module introduces message logging, Cloud Logging, and Apigee analytics

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/256/video/348475)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=CwB5YRb9YUM)

Hansel: In this module, you'll learn how to use the message logging policy to log messages in Apigee proxies. You'll also learn about Apigee's analytics, which you can use to gain insights into your API program and track API performance, as well as the data capture policy, which can be used to save custom data for use in analytics reports.

### Video - [Message Logging](https://www.cloudskillsboost.google/course_templates/256/video/348476)

- [YouTube: Message Logging](https://www.youtube.com/watch?v=WjnnNYnpN78)

Hansel: This lecture will discuss how we write to logs from API proxies using the Message Logging policy. Logging messages is one of the best ways to track down issues in your APIs. You can use a Message Logging policy within an API proxy. This policy can be used to log errors that occurred during the handling of an API call. The Message Logging policy can send a custom log entry to a remote Syslog server. Syslog is a standard for message logging. You can set up your own Syslog server or use a third party Syslog server. You can also log messages into Cloud Logging. Cloud Logging provides a REST API, and you can call that API using a Service Callout policy. Here is an example of a Message Logging policy using Syslog. The first thing to notice is that continueOnError is set to true. It generally does not make sense to throw an error if your Message Logging policy fails because you want the API call to work even if you can't log a message. When a policy is added, continueOnError always defaults to false. It is a best practice to set it to true for the Message Logging policy. The Syslog element contains the information necessary for sending the log entry using the Syslog Message Logging standard. The message element contains the custom log message. Variables can be used in the message by using curly brackets around the variable names. The host, port, and protocol specify the destination for the log message. Log messages can use TCP or UDP for the protocol. And TCP messages can send the log message encrypted over TLS if SSLInfo is enabled. There is another flow in the API proxy that we have not talked about yet. It is called the PostClientFlow and is used for logging messages. Message Logging policies in the post client flow are executed after the response is sent to the client. This allows logging to be used in your proxies without adding to the API call latency. It is highly recommended that you put your Message Logging policies in the PostClientFlow. Other types of policies that are placed in the PostClientFlow will be ignored. Google Cloud Logging, previously known as Stackdriver Logging, is a fully managed logging service that allows you to store, search, analyze, monitor, and alert on log data. You can send a log to Cloud Logging by using a REST API. You may find it useful to create a shared flow to incorporate the steps of obtaining and cashing a token and calling the Cloud Logging service.

### Video - [Analytics](https://www.cloudskillsboost.google/course_templates/256/video/348477)

- [YouTube: Analytics](https://www.youtube.com/watch?v=R-C_DmxW-B4)

Hansel: Trends and metrics for APIs, apps, and app developers are important for driving improvements in API programs. Apigee's analytics help give visibility into all aspects of your API program. Apigee's analytics captures a wealth of information flowing through API proxies. The analytics dashboards help answer questions about your APIs, like, "How is my API traffic trending over time?" "When is my API response time the fastest or the slowest?" "Geographically, where are my requests coming from?" "Which of my backends are returning the most errors?" Analytics also captures information about the app developers and apps using your APIs, answering questions like, "Which app developers and apps are driving the most traffic?" "Which of my API products are used the most?" And, "How much traffic is being driven by specific devices, operating systems, and user agents?" There are many out-of-the-box dashboards that can help you analyze your APIs. The proxy performance dashboard breaks down requests by API proxy and includes charts for total traffic, average response time, traffic by proxy, and average response time by proxy. The latency analysis dashboard reports on overall response time, target response time, time taken by the proxy to process the request, and time taken by the proxy for the response. These metrics allow you to determine where most of the time has been taken during API calls and can be analyzed by API proxy and region. The target performance dashboard helps you determine the traffic and performance by backend target. The dashboard can show the total traffic by target, traffic by successful or unsuccessful response, response time for the proxy and target, target error composition by status code, and payload size. The error code analysis dashboard specifies where errors are coming from. The dashboard shows how many errors are coming from the proxy and the target, and the error composition for proxy and target errors by status code range and status code. Dashboards and reports can also help with understanding metrics for app developers, apps, and users of the apps. For example, you can track developer app traffic and errors, as well as the devices and locations for the users of apps using your APIs. You can also use analytics to measure your cache performance. You can analyze count and rate of cache hits, cache hits by app, average times for cache hits and for cache misses, and percentage improvement when there is a cache hit. You can also build your own custom reports when the out-of-the-box reports are not sufficient. Metrics are numeric values that can be used as datapoints in the graph. For example, response time, or error count, can be specified as metrics. String fields can only be specified as dimensions. Dimensions are used to group metrics in meaningful ways. For example, you can view traffic by API proxy name, API product name, or country in which the request originated. Apigee automatically provides many metrics and dimensions for use in your custom reports, but you can also use your own custom metrics and dimensions in your reports by using the DataCapture policy in your API proxy. In addition to using the Apigee console, there are a few methods for integrating your API analytics with your enterprise tools. The Apigee API can be used to retrieve metrics for an organization by hostname over a specific period of time. These metrics can be integrated into external tools and dashboards for easier visibility into important Apigee metrics. You can filter, sort, or group these metrics by dimension. There is also an asynchronous custom reports API for exporting custom report data. Asynchronous reports are useful for when the datasets are large, the time interval is large, or the query is complex due to a variety of dimensions and filters. You can specify your list of metrics, dimensions, and a filter for the asynchronous query. When the report is complete, you can download the data using the Apigee API or visualize the report in the Apigee console. You can also retrieve analytics data by using the data export feature. The raw analytics data can be retrieved by date range into BigQuery. One important feature for Apigee is the ability to capture custom metrics and dimensions for use in your analytics reports or company dashboards. Apigee automatically captures common API metrics and dimensions. Apigee cannot collect metrics and dimensions that are business-specific, though, unless you specify them in your proxies. For example, being able to filter a report based on product category or business unit may be very useful, but product category and business unit do not show up in a common location across APIs. You can use the DataCapture policy to tell analytics to capture these important values. Before using the DataCapture policy to save a value, you must create a data collector to hold the value. A data collector contains a name, type, and optional description. The name is used in the DataCapture policy to reference the data collector. The type can be string, integer, floating point number, Boolean, or a datetime. The DataCapture policy allows you to specify a list of values that should be captured into data collectors for use as custom analytics, metrics, and dimensions. Every captured value must be written into a data collector that already exists. The specified data collector determines the data type of the captured value. Values may be captured from variables or message elements like headers, query parameters, and JSON or XML payloads. If data is captured into a data collector more than once during an API call, only the data from the last capture executed will be saved.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/256/quizzes/348478)

#### Quiz 1.

> [!important]
> **Which of the following statement about Apigee's analytics are correct? Select two.**
>
> - [ ] Metrics captured using the DataCapture policy may be used in custom analytics reports.
> - [ ] Apigee analytics will automatically capture payloads sent to your API proxies for use in reports.
> - [ ] Built-in analytics reports are primarily designed to address operational concerns.
> - [ ] Analytics data can be exported to tools like BigQuery.
> - [ ] The data type can be specified in a DataCapture policy to override a data collector's data type.

#### Quiz 2.

> [!important]
> **Which of the following statements about message logging is correct?**
>
> - [ ] Log messages must be formatted in JSON when the MessageLogging policy is used.
> - [ ] The MessageLogging policy sends log messages to a syslog server or to Google Cloud Logging.
> - [ ] The proxy endpoint PostClientFlow allows message logging after the API response has already been sent to the client.
> - [ ] Use the MessageLogging policy as many times as necessary in your proxy, because only one call to the logging service with all of the logs will be performed at the end of the API call execution.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/256/video/348479)

- [YouTube: Module Review](https://www.youtube.com/watch?v=cdHqjwt-DCM)

Hansel: In this module, you learned about logging messages by using the Message Logging policy. And you learned about analytics and how to use the Data Capture policy to capture custom information for use in analytics reports.

## Advanced Topics

This module introduces Apigee deployment options and CI/CD tools and strategies

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/256/video/348480)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=pWxqDMI5c6o)

Mike: In this module, you will learn how to integrate Apigee build processes into automated build tools, and you will learn about the different deployment options available for Apigee.

### Video - [Apigee Offline Development and CI/CD](https://www.cloudskillsboost.google/course_templates/256/video/348481)

- [YouTube: Apigee Offline Development and CI/CD](https://www.youtube.com/watch?v=U3k2UqVwZWM)

Mike: This lecture will discuss how Apigee supports offline development and continuous integration, continuous delivery, or CI/CD. As you completed the labs for this series of courses, you used the proxy editor to build your proxies. The editor is a great way to learn about proxy development. When building a proxy, an API engineer can seamlessly switch between the editing and testing of the proxy. There are some problems with online editing of API proxies, though. Deployed revisions are immutable and cannot be edited. However, non-deployed revisions can be edited or deleted, and there is no history of changes in a revision. The Apigee console makes it easy to overwrite or delete revisions of an API proxy. It is important to store development artifacts in source control, especially when working as a team. Enterprise teams also generally use CI/CD to provide better control over the release process. Apigee provides utilities that allow you to integrate the deployment of proxies and configuration from build automation tools. When you download an API proxy revision, it is retrieved as a zip file. The downloaded bundle can be unzipped and stored in source control. Apache Maven is an open source build automation tool that can be used as part of a CI/CD pipeline. There are two Maven plugins that can be used for managing Apigee. The Apigee deploy Maven plugin is used to build and deploy Apigee proxies and shared flows. This plugin uses the same directory structure as is extracted from a downloaded zip file bundle. The Apigee config Maven plugin is used to manage and deploy Apigee configuration entities. This plugin can be used as part of a CI/CD process to deploy changes to configuration. The Apigee API can be used if the Maven plugins do not work for you. Nearly everything that can be done using the Apigee console can also be done via the Apigee API. The Apigee API can be called from CI/CD tool chains to manage the API lifecycle. You can call the Apigee API by using IAM credentials. If you do not have access to particular organizations, environments, or entities from the Apigee console, you will not be able to use those credentials to access those entities via the Apigee API.

### Video - [Apigee Deployment Options](https://www.cloudskillsboost.google/course_templates/256/video/348482)

- [YouTube: Apigee Deployment Options](https://www.youtube.com/watch?v=qrDR364lAEk)

Mike: This lecture will discuss the different deployment options available when you use Apigee. Google provides multiple deployment options for Apigee's fully featured API management platform. Apigee can be run in Google Cloud, where the infrastructure is fully managed by Google. Apigee's hybrid deployment model allows customer managed runtimes with infrastructure management shared between Google and the customer. Google also provides the Apigee adapter for Envoy, a lightweight runtime gateway deployment option that lets customers deploy API management functionality in close proximity to back end services. Let's learn about all of these deployment options. Apigee can be deployed as a fully managed full lifecycle API management platform running within Google Cloud. This is a software as a service solution with the entire platform hosted on Google Cloud and managed by Google. The managed model requires the least amount of management effort, allowing you to focus your resources on building your API program. This is the deployment model you have been using for the labs in this course. An organization can be hosted in your choice of Google Cloud regions around the world. Organizations can also be hosted in multiple regions, promoting high availability in case of regional outages and reducing latency by positioning API gateways close to clients and back ends. With managed Apigee, customers can take advantage of Google Cloud's fast worldwide private network, as well as other Google Cloud services and features. Another benefit of the enterprise offerings of managed Apigee is that they include entitlements for hybrid deployments of Apigee, Apigee's full lifecycle API management platform can also be deployed using the hybrid deployment model. The Apigee management plane is hosted in Google Cloud and managed by Google. The Apigee runtime plane is deployed as containerized services on a supported Kubernetes platform running in a Google Cloud project, a customer data center, or a private cloud and managed by the customer. The hybrid deployment model for Apigee allows runtime API traffic to remain within customer control boundaries. There are many benefits of using the hybrid deployment model. Gateways can be deployed to multiple clouds and data centers, thus allowing API proxies to handle API requests as close to back end workloads as possible. API traffic can remain inside specified network boundaries, which can help adhere to security requirements. The hybrid deployment model provides full network customizability, allowing the customer to use chosen TLS ciphers, VPNs, and static IP addresses. Like the managed cloud deployment model, hybrid deployments provide fully featured API management. Envoy is an open source, high performance edge and service proxy that is designed for cloud native applications. Envoy has broad industry support. The Apigee adapter for Envoy turns Envoy into an Apigee managed API gateway that can proxy API traffic. Each instance of the adapter is tied to a specific organization and environment running in a Google Cloud managed or hybrid deployment. The Apigee adapter for Envoy is lightweight and easy to manage, and can be run close to your back end services. Your API traffic does not need to call a central Google Cloud managed or hybrid organization, allowing your traffic to stay within enterprise approved network boundaries for security or compliance purposes. The Envoy adapter communicates with the management plane asynchronously. This allows you to use configuration from the central organization without affecting latency. The adapter can validate API keys and signed JWT tokens, validating them against API products. The adapter asynchronously retrieves API product and API key information from the configured organization and environment. One benefit of the Apigee adapter for Envoy is that it uses configuration based enforcement. It is easy to manage the adapter for many micro services. Analytics data for calls through the adapter is delivered to Apigee asynchronously, allowing full visibility for API traffic running through the adapter. Quotas can also be enforced using the adapter. Spike Arrest rate limiting is a built in feature of Envoy which can also be used.

### Quiz - [Module Quiz](https://www.cloudskillsboost.google/course_templates/256/quizzes/348483)

#### Quiz 1.

> [!important]
> **Which of the following is a reason to select the hybrid cloud deployment option?**
>
> - [ ] The customer does not have its own backend services in Google Cloud, so a hybrid deployment is not recommended.
> - [ ] The customer does not want to manage the runtime.
> - [ ] The customer wants the Apigee deployment option that requires the least amount of management by the customer.
> - [ ] The customer has strict requirements for running API traffic and hosting data within their network boundaries.

#### Quiz 2.

> [!important]
> **Which of the following statements about CI/CD with Apigee are correct? Select two.**
>
> - [ ] Enterprise teams should use source control for proxy development.
> - [ ] The apigee-config-maven-plugin may be used to deploy Apigee proxies.
> - [ ] The Apigee API may be called from CI/CD toolchains to manage the API lifecycle.
> - [ ] Enterprise teams should use the drag-and-drop graphical editor in the Apigee console for all proxy development.
> - [ ] Source control is integrated into the console proxy editor.

#### Quiz 3.

> [!important]
> **Which of the following are features of the Apigee Adapter for Envoy?**
>
> - [ ] Token validation and quota enforcement
> - [ ] OpenAPI spec validation and quota enforcement
> - [ ] Token validation and message transformation
> - [ ] API key validation and message transformation

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/256/video/348484)

- [YouTube: Module Review](https://www.youtube.com/watch?v=lyM_SbtKSGs)

Mike: In this module, you learned about the deployment options for Apigee gateways and how to use Apigee with automated build tools.

### Video - [Course Review](https://www.cloudskillsboost.google/course_templates/256/video/348485)

- [YouTube: Course Review](https://www.youtube.com/watch?v=a-dBd2Qkp6M)

Mike: Thank you for taking the API development and operations course. During this course, you learned about API mediation and traffic management. You learned how APIs can be published using a developer portal. We discussed how analytics can be used to understand the performance of your APIs and API program. And you learned how Apigee proxies and configuration can be deployed using CI/CD tools, as well as how Apigee gateways can be deployed close to back end services.

### Document - [Reading: Apigee X and Apigee Edge differences](https://www.cloudskillsboost.google/course_templates/256/documents/348486)

### Document - [Reading: Next steps](https://www.cloudskillsboost.google/course_templates/256/documents/348487)

## Resources

PDF links to all modules

### Document - [Course Resources](https://www.cloudskillsboost.google/course_templates/256/documents/348488)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
