---
id: 1103
name: 'Building Complex End to End Self-Service Experiences in Dialogflow CX'
datePublished: 2024-07-10
topics: []
type: Course
url: https://www.cloudskillsboost.google/course_templates/1103
---

# [Building Complex End to End Self-Service Experiences in Dialogflow CX](https://www.cloudskillsboost.google/course_templates/1103)

**Description:**

This course will equip you with the tools to develop complex conversational experiences in Dialogflow CX capable of identifying the user intent and routing it to the right self service flow.

**Objectives:**

- How to understand voice agents and how they differ from chat agents.
- Develop the details of advanced settings, tools, and toggles.
- Learn about prebuilt components and their benefits.
- Understand high level multi language events and the testing of multilingual agents.
- Understand how to design your end-to-end flows according to best practices.

## Voice Agents

The module explores what voice virtual agents are and their unique differences from chat agents.

### Video - [What are voice agents (and how do they differ from chat agents?)](https://www.cloudskillsboost.google/course_templates/1103/video/491477)

- [YouTube: What are voice agents (and how do they differ from chat agents?)](https://www.youtube.com/watch?v=oxgyFDIPzI4)

Welcome to Building complex end-to-end self service experiences in Dialogflow CX! In this training, you’ll learn how to develop a voice agent capable of identifying what the user needs and directing them appropriately to the right self service flow. First, we’re going to discuss voice agents and what makes them technically unique from chat. Then we’re going to talk about: the advanced settings and tools you can use within Dialogflow, the pre-built Dialogflow components that can be leveraged, and webhook configurations. After that we’ll talk through multi-language agents and their special considerations. And we’ll finish with a discussion about how to develop end-to-end flows. Let’s get started! Let’s begin with a deep dive on voice agents. In this section, we’ll first explain what voice agents are and outline some key differences between voice and chat. Next, we’ll discuss speech-to-text. Then we’ll explore some speech configurations, and dual tone multi-frequency (or D-T-M-F). Finally, we’ll conclude the section by reviewing text-to-speech. So what are voice agents? And what are some key ways in which they differ from chat agents? Voice agents handle audio conversations with users. While chat agents handle textual conversations, where the users and the bot write to each other to communicate. To help better understand the differences between chat and voice agents let’s review a few diagrams, starting with chat agents. The flow starts with the user submitting text to the messenger interface or chat router. The messenger interface passes this text to Dialogflow. Dialogflow, after processing the input, outputs a response text. The response text is then passed back to the interface and presented to the user. Now let’s look at what this conversational flow looks like for Voice agents. The first step is when the user speaks or inputs audio to the call router. This allows the call router to convert the audio to text using STT or Speech to text. Next, this text is passed back to the call router. Then the call is passed from the call router to Dialogflow. Dialogflow then processes the text and outputs a response. In this diagram we’re assuming it’s a text response to be read to the user. The call router then sends this text to TTS or Text to Speech. Text To Speech then passes the audio back to the Call Router. The call router then plays this audio to the user. As this diagrams shows, there are a large number components involved in voice compared to chat. Because of this, there are many potential failure points and intricacies which must be monitored and designed for.

### Video - [Speech to text (STT) and biasing](https://www.cloudskillsboost.google/course_templates/1103/video/491478)

- [YouTube: Speech to text (STT) and biasing](https://www.youtube.com/watch?v=igimtm7fhd0)

Let’s start reviewing speech to text (STT) and biasing. The need for a speech to text solution originates from the challenge of hearing exactly what a user says. They may be in a noisy area or not articulating clearly. Given this, there are often different ways to transcribe the user’s text. Speech Biasing can help ensure that we transcribe them accurately by using information about the agent’s design to influence how we interpret the user. For example, suppose the user says something that is either “desert questions” or “dessert questions”. If we’re in a use case involving travel to state parks in the southwest, we should interpret them as talking about deserts. If we’re in a use case involving restaurants, we should interpret them as talking about desserts. That’s where Speech adaptation comes in. Speech adaptation is a feature that allows Dialogflow CX to improve the accuracy of its speech recognition models by adapting to the specific characteristics of your audio data. Essentially it works by biasing our speech to text in favor of the text we expect to hear at that point in the conversation. Speech adaptation can improve the accuracy of your agent's speech recognition in noisy environments. It can also improve the accuracy of your agent's speech recognition for users with accents or who speak quickly. Speech adaptation is a simple and easy-to-use feature that can be enabled with just a few clicks in Dialogflow CX, let’s review how. Currently in Dialogflow CX, there are two types of speech adaptation: auto and manual. Auto speech adaptation is a feature that automatically adapts your agent's speech recognition to the specific characteristics of your users' speech. This is the easiest way to improve speech recognition accuracy, and it is recommended for most users. The other type is manual speech adaptation. Manual speech adaptation is a feature that allows you to manually adjust the speech recognition settings for your agent. This is a more advanced feature, and it is only recommended for users who have a specific need to adjust the speech recognition settings. For example, you might want to adjust the speech recognition settings if your users have a specific accent or if you are using a custom vocabulary. You can control where you apply manual speech adaption by overriding or disabling the auto speech adaptation from flow and page level. However, there are limitations: There’s no locale differentiation for manual speech adaptation. So how do you configure manual speech adaptation if it aligns with your use case? If you decide to enable manual speech adaptation, you can configure it at either the flow level or the page level. If configured at the page level, it will override whatever flow level settings you have. Navigate to the settings for the specific flow or page you want to set it to on the left-hand side of the screen. Hover next to the name of the flow or page you want to add manual speech adaptation to. Three dots appear. Click them to open up a menu. Select “Flow settings” or “Page settings” depending on which you want. Here, in this example, we can see the opened flow settings. Once you have navigated to the settings, scroll down to the bottom and select the checkbox next to “Enable manual speech adaptation”. This will let you add in new phrases sets by clicking the “Create new phrase set” icon. You can also choose to edit or delete an existing phrase set by clicking the buttons to the right. Once enabled in settings you need to define a phrase set. A phrase set contains several components. First, you should give it a display name. Next, you should add a phrase. Phrases can be speech class tokens or a string. Then lastly, add an associated boost value. The boost value controls how much we want to bias speech in favor of that phrase. Higher values correspond to more bias. If all else fails, you can use Automatic Speech Recognitions, or A-S-R corrections. A-S-R corrections are a way to identify a mistranscription and force the agent to transcribe correctly. If you go to an intent, you can find these at the bottom. You can try entering a mistranscribed phrase, such as “hey bill” along with an appropriate transcription, such as “pay bill”. ASR corrections should be used to handle “edge cases” on an exception basis as your use cases should typically be handled with ASR biasing. Refer to the additional resources document for more information on ASR error correction.

### Video - [Conversational Agents speech configuration and settings](https://www.cloudskillsboost.google/course_templates/1103/video/491479)

- [YouTube: Conversational Agents speech configuration and settings](https://www.youtube.com/watch?v=OEyHH_A5nS8)

Now, let’s double click on the DialogFlow CX speech configuration settings. In this section, we’ll discuss some key settings related to the way that speech is processed: Speech models, End of speech sensitivity, No speech timeout, And barge in. Speech processing settings can be configured at three levels: the agent, flow, and page level. If you haven’t set your speech configurations at a lower level, then you inherit the speech settings for the next level up. So, if you haven’t set them for page level, you inherit flow level and if you haven’t set flow level either, then you inherit agent level. Let’s walk through an example. We’re trying to determine the settings for a particular page called “Explain bill”, within your bill flow. And we’re trying to identify the no-input timeout for this page. The first question we ask is, is there a page setting present? In this example, the answer is “No”. Since there’s no page setting set, we need to check if there’s a flow setting. In this instance, there is a flow setting: a No-input timeout of 5 seconds. In that case, we use this setting. If there was a page setting, then we could have made use of that. And if there was no flow setting preset, then we could have used the agent setting. There are five speech models to choose from. Which model you choose depends on your use case. For Dialogflow, we recommend “telephony short” for telephony cases and “latest short” for non-telephony cases. Now let’s learn more about end of speech sensitivity. End of speech sensitivity defines how long we wait after the user says something before we process what they say. So, for instance, suppose we ask the user something and they say the word “Yes”. End of speech sensitivity controls how long we will wait to see if they say something else before processing this audio. It’s rated on a 0 to 100 scale. Higher numbers indicate that we will wait for a shorter period of time. A good default value is 80 or 90. Use lower values in cases where you expect longer pauses. For instance in account ID collection use cases users might need to stop for a couple of seconds as they look up the next set of numbers in the ID they’re reading out. Now that you understand end of speech sensitivity, let’s now learn about Barge-In. Barge-in allows the user to interrupt the bot while it is speaking. In certain use cases, enabling barge-in can be very useful. For instance, if you have many users who are familiar with your bot, having navigated through it many times, it is a good idea to enable barge-in. That way they can interrupt the bot rather than having to listen to dialog they’re already familiar with. Another important use case is when the bot asks the user a question and it will take the user some time to generate an answer, for example when the user needs to perform an action like resetting their router. In such a case. It this instance it is common to have the bot play music while waiting for the user’s next prompt. The user should be able to barge-in anytime once they have the answer rather than waiting for the music to finish. There are however, also use cases where it is not a good idea to enable barge-in. If you have menus where it’s important for the user to hear all the options, you should disable barge-in. Likewise, if there’s certain legal language that should be played to the user, remember to have barge-in disabled.

### Video - [Dual-tone multi-frequency (DTMF)](https://www.cloudskillsboost.google/course_templates/1103/video/491480)

- [YouTube: Dual-tone multi-frequency (DTMF)](https://www.youtube.com/watch?v=8xmtoqRf0I4)

Now, let us deep dive on DTMF, or dual-tone multi-frequency. DTMF, or Dual-Tone Multi-Frequency signaling decoding, is a feature in Dialogflow CX that you can enable and configure for a parameter. It allows users to make selections by pressing numbers on a keypad. As a feature, it is reminiscent of legacy IVR flows but can still be useful or required for some use cases. Like for capturing numeric input, such as a PIN or selection from a menu, during a phone call. In this section, we will explore how to create a page to capture the numbers sent via DTMF and set these as a session parameter. Additionally, we’ll discover how to test this feature using the simulator and a Telephony integration to reproduce a real use case. There are several ways to configure DTMF in your agent. One common method is to configure it on a page parameter. Let’s run through the steps. First, create a page and add the parameter you wish to add DTMF to. In this example the parameter is “input_code”. Once the parameter is added to the page, click on the parameter and scroll to the bottom to view the section on DTMF settings. Select the checkbox “Enable DTMF” to enable DTMF collection for this parameter. There are several additional values you can add. With “max digits” you can define the maximum number of digits the end-user can provide for this parameter. With “finish digit” you can set the keypad value that terminates the DTMF input for the parameter. Commonly, the hash key is used for this setting. The finish digit itself is not added to the Dialogflow query. So if the finish digit is the hash key and the input is “1-2-3-hash” the query input will be “1-2-3”. Finally, you can set endpointing timeout duration. If a user starts entering information via DTMF, we will wait this long before proceeding to process their input. So for instance if your value is 3 and the user types in the number 37 and then pauses, we’ll wait 3 seconds before processing just in case they were planning on typing in another number. Now that we have configured the DTMF settings, we can simulate a DTMF input scenario in the “test agent” box on the right of the Dialogflow CX console. If you click the plus icon on the bottom, you can add in a DTMF input. Suppose your agent's parameter is configured to accept a maximum of four digits, and you're using the 'hash' symbol as the finish digit. In this case, you would enter a string of digits in the simulator, like '1234-hash'. It's crucial to understand how the simulator interprets this input. The '#' symbol acts as a signal to the agent, indicating the end of the DTMF input. So, when you enter '1234#', the agent processes '1234' as the input value, excluding the '#' from the final input sent to Dialogflow. Another way to test DTMF is by using a phone gateway. To do so, first you must set up your phone gateway. Go to the 'Manage' tab and select the 'Integrations' tab in the menu. From the Integrations tab, select 'Manage' and then '+Create' to set up a new phone gateway. Now, it's time to configure your phone number. Select the country code for the phone number you want to use. Optionally, select preferred area codes. Click "Request" to proceed. This will generate a few phone numbers to choose from. Choose a telephony number from the list provided. Enter a display name for the conversation profile associated with the phone number. Select the environment and language. Then click "Save" to finalize the setup. Now you have enabled the phone gateway and have a number you can dial into. Using your phone dial the number you've set up through the Phone Gateway. As soon as the call connects, your Dialogflow agent will automatically trigger the Default Welcome Intent. This is your starting point for the VA interaction. During the call, pay special attention to how your agent handles DTMF inputs – that's when you use your phone’s keypad to send digits in response to the agent’s prompts. Also, this is an excellent opportunity to test how well your agent performs under various challenging conditions. Try introducing different variables into your calls, such as varying the length of pauses when you are dialing to test time-outs. Dialogflow recently started also supporting matching intents with DTMF digits or DTMF sequence patterns. This means that DTMF digits can now be used to trigger specific intents within Dialogflow. Along with matching to intents, DTMF can match an entity of custom entity type with DTMF digit or a DTMF sequence pattern, as well as system entity types such as numbers and dates. So how can you configure DTMF for an intent? Go to the edit page of the intent and assign a DTMF digit. Alternatively, if you want to match a more complex DTMF sequence, you can assign a regex pattern in the DTMF pattern box Then at runtime, input '1' via DTMF which triggers the 'Default Welcome Intent'. That’s it! Now let’s learn about matching DTMF to an entity. To configure DTMF to match with an entity, go to the edit page of an entity type. For selected entity, assign a DTMF digit or a regex pattern if you want to match a more complex DTMF sequence. Then at runtime, if the slot being filled is associated with the entity type, you can provide your inputs. In this example the entity type is 'Fruit', so you can input '1' for 'apple', '2' for 'orange' and '3' for 'banana'. Now that you know how to match DTMF with a custom entity type, next we will list out the system entities that can also be configured with DTMF. The following table shows the system entities supported by the DTMF feature. The table contains the type name, the DTMF pattern and an example for each case. For instance, for the system entity “system number”, it’s possible to enter a digit with a period using the asterisk symbol. Congrats on understanding the various configuration options for DTMF in Dialogflow!

### Video - [Text-to-speech (TTS)](https://www.cloudskillsboost.google/course_templates/1103/video/491481)

- [YouTube: Text-to-speech (TTS)](https://www.youtube.com/watch?v=lSYsZBCi02I)

Now let’s take a few moments to explore text to speech, or T-T-S. Text to speech is a function of how the bot uses text output and translates it into speech that is then heard by the end user. T-T-S is a fundamental component of a good conversational experience. The voice of the bot and how it sounds to the user is critical to the end-user’s experience. We will cover two sub-topics here, key to a correct adoption of TTS: selecting a voice and using SSML. Selecting a voice in Dialogflow is straightforward. You select a voice in the “Speech and IVR” tab of agent settings. There are many options to choose from. Also, you can choose to create your own custom voice. This is a more intensive process but allows for even more customization. Further documentation on voices can be found in the additional resources document at the end of the training. You can also use SSML, or speech synthesis markup language to fine-tune how your agent speaks. This gives you a plethora of configuration options. For instance, you can add pauses in the response. Or control how certain parts of the response are pronounced, such as dates and telephone numbers. You can also control speed. The commonly used ssml markers are “speak”, “break” and “prosody” and are used to fine-tune how your bot speaks. Speak is the the root element of SSML response. When you use the speak element, always close it with the forward slash speak element. The Break is an empty element used to control the pausing between words or sentences. The length of the break time can be configured in seconds or milliseconds. Next you have, Prosody, an element that lets you adjust the speed of the responses. You can have a slow rate for prosody, or a fast rate to control the speed of the speech. SSML offers many configuration options. Please refer to the additional resources document at the end of the training for an exhaustive list of SSML tags that can be configured for your responses.

### Quiz - [Voice Agents Quiz](https://www.cloudskillsboost.google/course_templates/1103/quizzes/491482)

## Advanced Settings, Tools and Toggles

The module explores the key advanced settings and functionalities that can be used to further tune and control a virtual agent.

### Video - [Agent settings](https://www.cloudskillsboost.google/course_templates/1103/video/491483)

- [YouTube: Agent settings](https://www.youtube.com/watch?v=D4_Vq0k8gF0)

Now let’s discuss some advanced settings, tools, and toggles. This section focuses on some advanced settings and functionalities you can use to further tune and control your agent. We will start by discussing various agent settings that you can use to more carefully control your agent. Next, we will look at ways of more finely specifying how information is processed by the agent and presented to the user. Then we will look at system functions, which allow you to manipulate values of parameters. Next, we will look at agent responses, which control the structure of how you output information. Finally, we will look at several features related to testing and versioning, like testing the agent with the console and test cases and reviewing versions and environments. Let’s start by deep diving into agent settings. The Agent settings tab in Dialogflow lets you control a range of settings across the agent You can find the agent settings button near the top right of the Dialogflow CX console. When you navigate to agent settings, you’ll see nine tabs along the top. Each of these tabs is a setting. Let’s dive into each. The General tab covers general agent settings. The ML tab covers settings related to machine learning, centered around agent NLU. Generative AI covers settings related to generative AI. The Speech and IVR tab covers settings related to voice agents. The Multimodal tab includes the settings related to multimodal experiences. The Share tab allows you to control the access permissions of your project. The Languages tab concerns what languages your bot supports. It lets you add languages, select locales, and mark your default language. The Security tab lets you select your security settings, which include things like what types of text you want to redact and how long data is stored. And lastly, the Advanced tab covers advanced settings, which includes things like sentiment analysis. Let’s take a few moments to explore some of these tabs in more depth, starting with the General Tab. Here you can adjust the display name of your agent and the time-zone. You can also lock the agent in the General tab. If you do this, no one will be able to edit it until it is unlocked. Near the bottom of the screen is also the option to automatically export the conversational data into BigQuery. This is very useful for production, testing and reporting. If you scroll to the bottom of the agent settings, there you will find a checkbox for “intent suggestions”. This allows Dialogflow CX to automatically make intent suggestions on the basis of conversations people have had with the bot. Basically, this feature looks at cases in which the user said something and the NLU doesn’t match. So a suggestion is made on how existing intents can be adjusted or new ones added to accommodate these utterances. You can also enable user feedback. This allows the user to provide feedback on a conversation they’ve had with your agent. Next, is a section where you can enable custom payload templates. These are templates associated custom payloads, which is one sort of response that your agent can provide. Finally, at the bottom of the General tab, there are two different logging settings. One enables cloud logging, which exports user queries and debugging information to cloud logging. The other enables conversation history. This is an extremely useful feature that records information about conversations users have had with the bot. The next tab is “ML”, which stands for Machine Learning. This tab lets you see when the flows in your agent were last trained. It also lets you retrain them (all together or a subset at a time). From this tab, you can enable Auto-train which will retrain the NLU whenever relevant changes are made instead of having to manually trigger retraining. Within the same table, the classification threshold helps determine whether we will match an intent, a parameter, or no-match. When the NLU returns a result, it indicates whether it matched an intent or parameter, and what is the confidence level of the match. For instance, suppose that we have the settings shown on the screen and the user says “pay”, and the NLU returns the intent “bill pay” with a confidence of 0.7 or 70%. The threshold is 0.3 or 30% and so “pay” would match the intent “bill pay” because .7 is greater than .3. Another very useful tab is “Speech and IVR”. This tab lets you configure various settings related to voice. Here, you can control the text-to-speech model for each language you are using. We can also control certain settings in speech to text. For instance, you can turn on auto speech adaption to automatically improve speech recognition quality using the agent information Alternatively, you can configure advanced speech settings, which we are going to explore soon. The final checkbox is for “Enable DTMF” to allow Dialogflow to process incoming audio as DTMF events. Going back to advanced speech settings. Suppose you click on the first checkbox. Here is what you will see. From here you can select the speech to text model you can use for each language. You can also select the end of speech sensitivity default for the agent. This controls how long the bot waits after the user finishes speaking before processing what the user has said. Then, you can select the agent default no speech timeout. This controls how long we wait for the user to speak before the next virtual agent prompt. In addition, you can control barge-in, which lets the user interrupt the bot before it has finished speaking. Finally, you can specify the audio export bucket for storing audio files captured during the conversation. This completes our tour of agent settings. There are many more settings, but these should give you an overview of some of the most commonly used. For more information on Agent settings, refer to the additional resources document.

### Video - [System functions](https://www.cloudskillsboost.google/course_templates/1103/video/491484)

- [YouTube: System functions](https://www.youtube.com/watch?v=-D7C179kqaI)

Next, let’s review system functions. What are system functions? System functions are used to generate dynamic values during conversations. You can apply them to your agent's: Conditions, static response messages (such as text responses, custom payloads, and conditional responses), parameter presets, and webhook header values. For example, suppose you have a list of team members stored as a parameter named “team members”. Suppose you want to know how many team members are in the list. You could use the system function “COUNT” to count the number of members in this list. You could then store this value on another parameter, such as “team member count” and use it for future routing or dynamic responses. System functions follow the pattern illustrated here. There is a standardized prefix (the syntax of which is “dollar sign system period system function period”), followed by your function name and its arguments in parentheses. The names of the functions are all capitalized. The arguments of the function can be: Inline values (such as number 1, string "abc", boolean true, and list 1, 2, 3) References to parameters (such as dollar session dot params dotcolor) Nested functions (such as dollar sys dot func dot ADD dollar sys dot func fdot MINUS 2 1 3) If you reference parameters, there are two things to note. First, if the parameter referenced by the function expression is not set, it is treated as null. Second, inline objects are not supported. Now that you know more about System function syntax, let’s learn how to check the results of the function. If you want to check the results of a system function, you can look at the results of the evaluations in Query Results. During the conversation, if Dialogflow CX evaluates any inline system function expressions to generate dynamic values, you can find the results of the evaluations in the QueryResult. Specifically, there is a key called “System Function Results” in the Diagnostic Info structure field. This key logs the results (and errors) of system function evaluations. In this example, we’ve used the system function “ADD” to add the values of two session parameters to get the number 7. As you can see, the diagnostic info will tell you which system function was applied and the result. Here are two important tips when it comes to system functions. First, think carefully about the inputs, particularly the difference between strings and numbers. If your input is not of the appropriate type for a system function, there are system functions that can help you convert it. Second, you should be conscious about whether your task may be better handled via a webhook. System functions can greatly cut down on latency. However, you should be very cautious of creating complicated nests of system functions without robustly testing them and where a webhook should handle more complex functions and data manipulation. Refer to the systems functions reference link in the additional resources document at the end of the training to learn more about system functions and their structure. This is a good resource for learning about system functions and understanding their structure. It also contains examples which are helpful to make sure you have the syntax right. It contains descriptions of all supported system functions along with examples. Here, for instance, is the reference for the function “ADD” Note that the list of supported system functions is constantly being updated. So if you don’t currently see a system function, it may well be supported in the future, so bookmark this link!

### Video - [Agent responses](https://www.cloudskillsboost.google/course_templates/1103/video/491485)

- [YouTube: Agent responses](https://www.youtube.com/watch?v=f3h6lcMc0uk)

Next, let’s move on to explore agent responses. Within each fulfillment, you can add agent responses. These let you output text to be displayed or read to the user, but you can define various other types of responses. There are nine in total which we are going to explore next. Remember that you can add more than one agent response to a given fulfillment. If you do, they’re outputted in the order in which you send them. The nine agent response types include: Text, Custom payload, Live agent handoff, Conversation success metadata, And play pre-recorded audio. There’s also… Output audio text, Conditional response, Telephony transfer call, And Data store response options. Many of them, such as “output audio text”, are relatively intuitive. Let’s run through custom payloads and conditional response which require further elaboration. For more information on the different types of Agent responses, refer to the additional resources document. Some integrations support a custom payload response to handle rich responses. These custom payloads are supplied in a JSON format defined in the integration’s documentation available on Google Cloud. You can refer to the additional resources document for more information . For instance, Dialogflow Messenger has a special custom payload format. For more information on this, refer to the additional resources document. Remember that you can include parameter references in your custom payload JSON and that they should be treated as JSON string values, so wrap them in double quotes. You can also send a custom payload to any integrations that you develop, however keep in mind that they won't be processed by Dialogflow, so make sure they follow their own specific business logic. Next we have Conditional Responses. Conditional responses let you control the output of your text depending on various conditions. For example, perhaps you wish to display different text to the user depending on their age, you can achieve this by using a condition response. You can set up an “if then” condition to give the user one message if they’re over 21 and a different one if they’re under.

### Video - [Test agent and test cases](https://www.cloudskillsboost.google/course_templates/1103/video/491486)

- [YouTube: Test agent and test cases](https://www.youtube.com/watch?v=ySN1qKXalZ0)

Next, let’s explore some key tenets on how to test an agent developed following the characteristics just outlined. The Test agent feature allows you to test your agent within the Dialogflow CX console. It can be found on the top right of your console screen. As you’re building your agent, make sure to frequently test it by simulating interactions which confirm the robustness of the conversational experience every step of the way. Let’s explore how. The messages addressed to the virtual agent can be typed at the bottom of the test window. Select the plus icon to input DTMF or set parameters. The microphone lets you say things to the agent and have speech-to-text transcribe them so it’s a great way to test the agent STT capabilities. The phone icon lets you pretend to have a phone conversation. And lastly, the send button lets you send the input you’ve entered. Let us move up. Here, you can see the details of the conversation being tested. The top also shows you the flow and the page the conversation started in. You can see user turns on the right… …and agent turns on the left. There is a pencil icon under the user turn which lets you edit this turn and test what would happen with a different user input. Let’s move up further on the screen. You can use the clipboard icon to view debugging information. The left and right arrows let you move through turns in the conversation, so you can see what state you were in at the end of each turn. The page and flow indicate what turn you were in at the selected turn. The intent indicates what intent (if any) you matched in the current turn. The execution steps let you walk through the steps that occurred during that turn. In other words, it lets you look through each state of the state machine in a turn. Next, let’s review the top icons and buttons. The return arrow lets you go back to the previous turn. The box icon lets you save the test as a test case for further use. Select the play icon to replay the conversation again. Or the reset icon to reset the conversation. Also you can select the menu icon to turn webhooks on and off, and enable or disable partial responses and sentiment analysis. Let’s click on saving a test case to see what happens. When you click to save a test case, a window pops up. You can use this window to fill out various pieces of information, including: the test case name, tags, notes, and what parameter values you wish to track. Once you have completed a test case, you can save it. Remember to have a naming convention for your test cases names and tags. A naming convention can help you organize and group test cases based on different criteria like test scenarios, functionalities, or other categories, and to create a deterministic catalog to limit variability. After filling in these fields, this test case can automatically run in the console. Click the Test cases tab to view test case results and execute a test case on demand. You can also run, export and import new test cases. Dialogflow CX has a test case interface that can be leveraged to assess the testing results and test coverage at a high level. However, if you require more customization and dashboarding capabilities, you have the option to develop your own test running scripts and visuals.

### Video - [Versions and environments](https://www.cloudskillsboost.google/course_templates/1103/video/491487)

- [YouTube: Versions and environments](https://www.youtube.com/watch?v=P4vQPDw2BMg)

Now, let’s cover versions and environments. Versions allow you to save multiple copies of your flows while environments let you deploy different versions of your flows and agent during different stages of development cycle. For example, you might have two separate versions of your bill flow, each with slight differences to accommodate different users or conversational experiences. You can deploy an agent with one version of the flow to environment A, and another agent with a different version of the flow to environment B. All Dialogflow CX agents open to an environment called draft. The draft is your working window. All the changes that you make are automatically saved in the draft. In order to edit specific flow versions, you can load them into your current draft. This configuration greatly allows collaboration like when working with a Google Doc. When you edit a flow, you are editing a draft of that flow. At any point, you can save the draft flow you are working on as a flow version. You can also click on Compare versions to compare the changes between the draft version and any other version of the flow. Just remember that a flow version is an immutable snapshot of your flow data and associated agent data like: Intents, Entities, Webhooks, Pages, Route groups and so on. A version of an agent can be tested in one environment without affecting the production version of your agent. To create a version, you can provide a version display name and a description of the changes this version contains and click save. It will automatically generate a version ID and display the NLU type of this particular flow version, and the time this version was created. You can also train a NLU model for a particular version of the flow. Once the training is finished, it will display the status as ready. Please note that you will not be able to create and save a version into a particular environment until the status says ready. Dialogflow CX provides a managed solution for creating multiple versions of an Agent and deploy them into separate serving environments. A draft flow is updated every time a developer makes a change, so keep the following best practices in mind: First, if multiple people have editing access to the agent, it can be easy to break a draft flow unintentionally or create inconsistencies because a recently edited draft flow may not be in line with the trained model. For example training may have a delay or require manual execution. So, be extremely cautious if you give multiple people editing access. You should always use flow versions for your production traffic. And lastly, draft flows should be tested before promoting them to production versions, and if any problems are discovered, you should rollback to a previous version. Let’s look at this sample diagram. Here we assume that versions are developed in the draft working window. A new version is then cut and saved when needed. The NLU models are retrained based on the changes in that particular version. Every time you save a new version, the retraining happens automatically. A change needs to be made to an existing version, which requires to load it back to the draft. After editing is complete, another version can be saved and a new version ID will be generated. Now that we have a good understanding of how versioning work, let’s double click on environments. In Dialogflow CX, you can create custom environments for development, testing or production purposes. When you work in a multi-flow agent, you can save different versions for each flow. Then point the specific version of the flow to a different environment. For example, you might have versions 1, 2 and 3 for flow A. And you have three environments called Dev, Testing and Production. You can have version 1 serve in the production environment. Version 2 can serve in the testing environment. And you can save version 3 in your development environment. You can create as many environments as you require. You can also consider environments as instances of your agent based on the Flow Versions you choose to publish. Each environment will correspond to a unique url. Please refer to the additional resources document at the end of the training for more information on creating environments. You can also specify an environment for runtime session calls, like “detect Intent”, “streaming Detect Intent”, “match Intent”, and session entity calls. To specify an environment, you can just alter the endpoint URL by inserting environments backslash environment id between the agent and sessions path parameters. Please remember that if an environment is not specified, the default environment will be used. This is extremely helpful because it allows you to test a specific flow version of the agent in a specific, self contained environment. As already mentioned, there’s no limit to the number of environments you can create. So consider having more than one to perform different activities. It is best practice to create environments that mimic the different stages of the development cycle. So have different ones for draft, for development, for staging, and for production. We have already explored the draft environment. So now let’s explore development, staging and production. The Dev environment is where virtual agents are developed. Here, you can also create test cases and run valuation. The staging environment is usually created for testing purposes, as changes are usually not made directly in this environment. Failed test cases in staging should be fixed in the development environment. And lastly, a production environment is reserved for serving the production traffic. You can also use the production environment for conducting live analytics and launch experiments (like AB testing experiments).

### Quiz - [Advanced Settings, Tools, and Toggles Quiz](https://www.cloudskillsboost.google/course_templates/1103/quizzes/491488)

## Prebuilt Components

The module explores the prebuilt components available in Dialogflow CX, their benefits and key configurations to import, customize, and integrate them into a virtual agent.

### Video - [Prebuilt components](https://www.cloudskillsboost.google/course_templates/1103/video/491489)

- [YouTube: Prebuilt components](https://www.youtube.com/watch?v=1V1CniVK1xo)

Now that we have a good understanding of how to build complex end-to-end self-service virtual agents experiences, let’s discuss pre-built components. We’ll start by discussing what prebuilts are and reviewing some of their benefits. Then we’ll talk about how to import, customize, and integrate them. Finally, we’ll explore which prebuilts are currently available in Dialogflow CX. Prebuilt components are templates published for Dialogflow CX that allow you to quickly import, customize, and deploy common conversational tasks. Prebuilt components aim to solve common but challenging scenarios that appear across multiple industries and use cases. Prebuilt components are made of a number of Dialogflow CX resources, such as flows, custom entities, and intents. They also include prebuilt webhooks for cases where Dialogflow-external integration is needed. Prebuilt components come in two different sizes: building blocks and use cases. Building blocks are smaller components that focus on a single conversational task, for example collecting a person’s name, address, or credit card information. Use cases are larger, more complex components that enable a longer user journey. They often cover scenarios that apply to one or more verticals like Financial Services or Telecommunications. They also include prebuilt flexible webhooks that define integration patterns that the bot builder can use to integrate with external services more easily. For example, collecting alphanumeric sequences can often be a challenge over voice, when noise and poor audio quality get in the way of getting accurate transcriptions. The Alphanumeric Collection prebuilt component adds new features like NATO phonetic conversion and several layers of error handling to ensure that a valid alphanumeric sequence is collected from the user. Name collection is also challenging over voice due to the variation in people’s names, creating a large problem space. The Name Collection prebuilt component supports spelling out a user’s name, collecting full names including middle names, and handling of special characters such as hyphens and apostrophes. Booking appointments is a common use case as well, seen in many different industries. For example arranging an appointment at a local bank, finding an appointment at a hair salon, or scheduling an appointment for car maintenance. The Arrange Appointment component collects information from the user and defines prebuilt templates for integration with external scheduling services. There are many benefits that come with the use of prebuilt components. First, by providing templates that are quickly imported and customized, the prebuilt components immediately reduce time needed to design and implement conversational experiences. Second, prebuilt components have also been rigorously tested and implemented according to Google best practices. Next, prebuilt components are standardized, this means that edge cases and error handling are consistently handled. That said, they do allow for customizations to adjust to specific requirements, Finally, pre-packaged integration templates, implemented via flexible webhooks, can greatly simplify the integration process by clearly defining integration patterns and allowing bot builders to adapt the integration to the external service instead of needing to refactor the services themselves. There are templates for integrations with backend services and CRMs, as well as APIs like Google Maps. Now let’s learn how to use prebuilt components. The first step in using prebuilt components is to import them into your new or existing project. Differently from Dialogflow prebuilt agents, prebuilt components are flows that can be imported and integrated into new or existing agents. There are two entry points to import prebuilt components: The first is from the Prebuilt Component Hub, which can be accessed by navigating to Manage > Prebuilt from the Dialogflow CX console. This view shows a detailed list of all available prebuilts, including a summary of features and required settings. Clicking on each card will show you more detail on each component, including sample utterances and a link to public Cloud documentation The second entry point is from the main Build page of the Dialogflow CX console. Clicking the “plus” button next to flows will show a dropdown. From here, you can select “Use Prebuilt flow” and see all available prebuilt components with a short description of each. Scrolling to the bottom of the dropdown and selecting “Prebuilt Component Hub” will take you to the same view mentioned previously. After selecting a component to import from either of the two entry points described, Dialogflow will check to see whether there are conflicts between the resources (intents, entities, or webhooks) already existing in the agent and the resources in the prebuilt component. This happens frequently when multiple prebuilt components are imported into the same agent, as there are many common resources shared across prebuilts. Examples that may create conflicts include: Common intents like “prebuilt_components_confirmation_yes” which are used across all components to guarantee that “yes” or positive responses to questions are handled consistently across all components. Or Common flows like the Authentication flow which are used across many Financial Services use cases (Account Balance, Account Statement and so on) to authenticate users. In general, we recommend selecting “Keep original resources” in order to preserve any customization already done to these resources and ensure that they are applied to the newly imported component. Before or after importing a prebuilt component, remember to read the public Cloud documentation. The link is located in the additional resources documentation. These resources will help you to: Understand the scope of the prebuilt component, See more detailed sample dialogs, And review the specifications for input/output parameters and any required webhooks. After importing a prebuilt component, you can customize it within the Dialogflow CX console. Through customization, you can: Change how it behaves according to prebuilt configurations, Update agent responses to match your brand’s persona, and edit custom entities or intents to support concepts specific to your business. All prebuilt components behave as normal Dialogflow CX resources, so you can immediately go into the console to review and edit the imported resources. Configuration for concepts such as validation and retry logic are defined for each component. You can see all input parameters by reviewing the public Cloud documentation for the prebuilt component referenced in the additional resources documentation at the end of the training. In general, all configurable parameters will be present in the Start Page on the 'true' route as parameter presets. Examples of configurable parameters include: Setting the minimum and maximum valid lengths for alphanumeric and numeric sequences, The number of retries accepted when collecting a user’s telephone number, And whether certain address elements are required for address collection. You can see agent responses by reviewing the flows and pages directly in the console. Alternatively, you can use the simulator or a telephony integration like CX Phone Gateway to test out the component. Important custom entities will also be described in the parameter documentation for each component. Go directly to the “Entities” section in the Dialogflow CX console to review and edit. Let’s now talk about integrating prebuilt components. Imported prebuilt components behave identically to other DF CX flows. To integrate your component into your agent, you can create a transition to the flow from anywhere else in your agent. As already mentioned, please make sure to review each component’s documentation to ensure required input parameters are passed in via session parameters. Some prebuilt components, particularly use cases, require integration with one or more Dialogflow-external services. This is to completely enable the conversational experience. Some examples of where external services are required are: Retrieving and validating user information for authentication Fetching account information and business-specific data like plans and offers. Submit actions on behalf of the user, like booking an appointment or generating an account statement Also, some of the prebuilt components provide templated integrations for Google and external services. These include Google Maps API and CRMs for management of customer information. These are defined by flexible webhooks, which provide examples of expected request, response, and authentication patterns. Each component has a pre-defined request pattern. This specifies the mapping between the external services’ responses and the Dialogflow CX parameters the component expects. The documentation for each component details what parameters and values are expected in order to integrate with the component. For example, the Address Collection prebuilt component provides a flexible webhook that defines the integration between the Google Maps Address Validation API and the Dialogflow agent. To enable this integration, you will need to add a Maps API key to the webhook configuration as described in Webhook setup. Another example is the Credit Card Statement prebuilt component. This requires multiple webhooks to be configured in order to enable the verification and retrieval of user data. The request/response patterns for each webhook are also described in the Webhook setup documentation. Note that a single endpoint can be used by passing tags or other metadata in the webhook request. Here’s a list of the available building blocks for Dialogflow CX, which focus on common conversational tasks. For the latest version of the list, please visit the Prebuilt Component Hub within the Dialogflow CX console. We also have a number of use cases for prebuilt components, which focus on larger user journeys. Again, to view the latest list, please visit the Prebuilt Component Hub within the Dialogflow CX console. A link to the list is provided in the additional resources documentation at the end of the training. Watch the following video for a demo to explore working with prebuilt components.

### Quiz - [Prebuilt Components Quiz](https://www.cloudskillsboost.google/course_templates/1103/quizzes/491490)

## Multi-language events

The module explores the best practices to build multi lingual Voice agents, the key considerations for their implementation and testing.

### Video - [Multi-language events](https://www.cloudskillsboost.google/course_templates/1103/video/491491)

- [YouTube: Multi-language events](https://www.youtube.com/watch?v=UU5wBaq-sRU)

Next, let’s cover the specifics on how to create multi-language agents. We have three objectives for this section. We’ll start with a high level overview of multi-lingual agents. Next, we’ll talk about how to implement them. We’ll then conclude by adding some notes about testing them effectively. Let’s get started! When creating a multilingual agent, it's advisable to design the agent primarily in root languages, and customize for locale-specific languages as necessary. Dialogflow CX ensures that while most agent data is common across all languages, text used for end-user interactions is language-specific. This includes intent training phrases, fulfillment responses, and entity entries. The first step in the process is adding the relevant languages for the virtual agent via the Dialogflow CX Console. It's a best practice to fully develop the agent in the default language before incorporating additional languages. Dialogflow CX distinguishes between root languages, such as English ('en'), which are general and do not specify a region, and locale-specific languages, like English-US ('en-US'), which target specific regional or country nuances. In the agent settings menu, you can configure the root language and locale languages by selecting the Languages tab. This differentiation allows for more precise and culturally relevant interactions within the same language group. It ensures that agents can accurately respond to and understand users from different locations. Language switching is managed by external clients. This requires all parts of the system to seamlessly transition during sessions. This involves both front-end elements like speech-to-text, text-to-speech, and/or GUI's. For example, this could be to assist with special character handling, and back-end aspects to ensure that all information communicated to users aligns with the session's language. A webhook service often bridges the agent and back-end services, facilitating this language adaptability. Note that the language code parameter sent by the client during the API request to the agent, also reaches the webhook calls in those cases where a webhook is called during the fulfillment process. Most agent data is common for all languages of a multilingual agent. However, text used to interact with an end-user is language-specific. In this section, we'll delve into language-specific elements crucial for Dialogflow CX's multilingual capabilities. We'll separate the discussion into language understanding and fulfillments. Understanding involves intent training phrases and entity entries, essential for accurately interpreting user inputs in different languages. Fulfillments, including responses, are tailored to each language, ensuring that the agent's replies are relevant and culturally appropriate. The language-non-specific elements are those that build the taxonomy of Dialogflow CX agents. These core components include Intents, which define the user's intentions. Entities, which capture and categorize user input; And flows, which map the conversation's structure. There’s also pages, which represent a single point in the conversation. And webhooks, which allow for external communications and data processing. All these structural elements and functions are not related with the language in itself but are related to their contents. In crafting multilingual Dialogflow CX agents, the design process should be structured to ensure seamless interaction across diverse languages. The journey begins with the Primary Design phase, where we establish the agent's framework in the main or default language, laying the groundwork. Following this foundational phase, there is a transition to the secondary design, where additional languages are methodically integrated. This phase includes automatic mass translation, manual curation and annotation, and testing among others. Let’s now dive a little deeper into training agents for language understanding. In training Dialogflow CX agents for language understanding, the initial step starts bulk translation of intent training phrases through automation. For example, scripts leveraging translators' APIs, which might utilize LLMs designed for translation. Bulk translation requires knowledge of the Dialogflow API. The subsequent curation process is carried out in the Dialogflow console by developers. These developers are knowledgeable in the target language and context (regionalism, brands, business and so on). They check the translations' relevance for NLU training. This includes adjusting for code-switching or borrowing and annotating entities. This ensures the NLU accurately understands and processes language-specific nuances. The process of adapting Dialogflow CX agents for multilingual support involves understanding unique regional language practices, such as code-borrowing. An example is Spanish speakers in some regions preferring the English word "bill" over the Spanish equivalent "factura". This highlights the importance of including such variations in the training data. It ensures the NLU system can accurately interpret and process these terms, reflecting real-world usage. Translating entity entries is primarily a manual task, heavily influenced by annotating training phrases during the intent curation process. It's important to proactively define new language entries for entities based on these annotations. If a new entity value emerges during curation or annotation, it should be formally added to the entity entries list. Maintain the main entity entry values across languages, supplemented by language-specific synonyms. This is advisable for the sake of business rules consistency. For instance, universal size terms like XS, S should be consistent across languages. Each entry is further defined by additional synonyms relevant to each language context. Static response messages are predefined during the design phase. These responses can be directly translated. For this process one possibility is to extract the static prompts using a specific script. You can then make a bulk translation, followed by the corresponding human curation process. Conversely, dynamic responses are crafted during the run-time. These are generally populated based on internal or external conditions, They’re also internally or provided by external services, And additionally, they’re generally injected using session parameters. For these cases, establish a convention to index the session parameters by language. For example, adding a postfix “es” for Spanish parameters. When the partial or entire prompts are provided by external services, do the translation and curation work in conjunction with the teams in charge of those services. Quality assurance for introducing a new language in Dialogflow CX should follow the same rigorous process applied to other languages. This encompasses: Unit testing to verify individual components, Regression testing to ensure new updates don't negatively impact existing functionalities, And end-to-end testing to validate the overall system performance from start to finish. Scripting by utilizing the Dialogflow agent API for bulk operations, can greatly support the test cases translation process. It starts by exporting test cases in bulk. Followed by bulk translation using an automatic translation service and then importing the new test cases. In this way, you can automate translations on a large scale. Following these automated steps, it's imperative to conduct a human curation and verification process. This ensures that each translated test case aligns with the intended assessment criteria of the agent. This helps it to maintain consistency and accuracy across different languages. Next, let’s look at regression testing. If the agent's state is determined by a specific flow or page combination, then the list of input utterances for those states can be translated in bulk form. Following this mass translation, a meticulous curation process is essential to ensure each accurately translated utterance will help with the NLU assessment. Voice testing introduces another layer of complexity. It requires both automated voice tests to assess the agent's ability to understand and respond correctly in different languages. And it also requires automated pronunciation tests to ensure the agent's responses are intelligible and accurately pronounced according to the norms of the target language. A tool can be designed to streamline the testing and improvement of Dialogflow voice conversations. It has four essential sections: Test Case Importing, Synthetic Audio File Generation, Multiturn Conversation Testing, and Results Analysis. The core component is the Text to Speech module, that generates the audio files. With this tool it’s possible to do a test not only to the NLU but also to the ASR. By comparing the original input text with the transcription returned by the agent, it’s easy to evaluate how well the ASR is recognizing the voice utterances. Another important aspect is the agent’s responses pronunciation. Scripts can be utilized for agent’s text responses extraction. You can then convert those responses in audios using the Text-to-Speech module that the solution uses. Then analyze the transcription result with a speech-to-text module. Any noticeable differences between the original text and its transcription indicates a need for adjustments in pronunciation. This might involve using SSML to fine-tune pronunciations across different languages. Or you might need to employ linguistic mimicry to achieve accurate pronunciations in the target language. This is especially true in cases of code-switching, such as using predominantly Spanish responses with English brand names. Achieving correct pronunciation may require manipulating the text or using SSML tags to guide the TTS engine. This is to ensure comprehensibility and naturalness in multilingual interactions. Language gating is a proposed solution that implements conditional routes based on a request-scoped parameter. The parameter indicates the language code selected by the Dialogflow client during the current session. This is necessary for multilingual agents where certain languages may have different enabled features. Or even different business requirements and rules. This solution is also applicable for scenarios where there may be a temporal delayed implementation of features for each language.

### Quiz - [Multi-language events Quiz](https://www.cloudskillsboost.google/course_templates/1103/quizzes/491492)

## End-to-End Flows

The module explores how to design end to end conversational flows, optimize the occurrence of happy paths and how to handle unhappy ones.

### Video - [Designing your flow](https://www.cloudskillsboost.google/course_templates/1103/video/491493)

- [YouTube: Designing your flow](https://www.youtube.com/watch?v=0x5XJqGKsRc)

By now, you have learned a lot about the principles of conversation design. And you’ve explored advanced tips on how to configure your DFCX agent. Now it’s time to put these into practice and talk about the end-to-end process for designing and building flows. In this section we’ll start from covering how to design your own flows. Then, we’ll talk about building happy paths and handling unhappy ones. We’ll conclude by talking about some best practices for end to end flow design. Let’s start with learning how to design flows. First, it is important to understand the key distinctions between two types of flows. Steering flows help identify a user’s goal, while use case flows deliver on that goal. For instance, in steering we might identify that a user wants to troubleshoot why they’re not receiving calls. Then in a connected use case flow we perform the troubleshooting. As the steering build was covered in previous modules of the Academy, we will now solely focus on use case flows. We can summarize the development process for a use case flow in 5 steps. First, you should identify key user journeys associated with that flow. For example, suppose that you wished to handle bill troubleshooting in your flow. Here you would identify what the key user journeys are related to billing, like explaining a bill, identifying a user’s billing balance, or identifying why a bill has increased. Next, you should identify steps and decision points along these use cases. For instance, one possible subflow part of the billing flow could be identifying why a user’s bill has increased. In this instance the first step is to pull details on the bill. If the bill hasn’t increased, we should clarify this. If it is has, we should pull out the salient reasons. In this stage, the language used by the bot to handle the request is important, so be sure to carefully craft it. For instance, think carefully about how to say that according to records, the bill hasn’t increased. To do so, it is recommended to review human to human conversation transcripts and observe how human agents deliver different pieces of information. The next step is to identify parameters and webhooks to provide the user with what they need. What key pieces of information will we need to track in this flow? How will we gather them? If some pieces will be passed by the backend in webhooks, what will the structure of these webhooks look like? Then in step 4 we identify the page structure. What are the natural ways of dividing up the various paths through the bot into pages? How can you do this in a way that doesn’t overload the amount of things that will have to be done on each page? Keep in mind that a page can represent a conversational state or can be used to route users without the agent responding. With all of this figured out, it is time to build in Dialogflow CX.

### Video - [Building happy paths](https://www.cloudskillsboost.google/course_templates/1103/video/491494)

- [YouTube: Building happy paths](https://www.youtube.com/watch?v=Gnt27nOK4n4)

Now, let’s discuss building happy paths. Let’s first start by distinguishing happy paths from unhappy paths. In happy paths, everything works smoothly. Conversations flow naturally. Users answer questions as expected, and the bot understands them. In unhappy paths, things are not smooth. Users are not understood, and perhaps not even heard. Webhook errors are encountered. The bot misunderstands the user and the user escalates. It is common (though not strictly required) to start by building happy paths through your flow. Add in the pages you’ve identified that you need. Include necessary prompts to the user Add intents and parameters to capture what the user says in response, Establish routes to move the user along. As needed, add in webhooks and parameter presets.

### Video - [Handle unhappy paths](https://www.cloudskillsboost.google/course_templates/1103/video/491495)

- [YouTube: Handle unhappy paths](https://www.youtube.com/watch?v=yMi8IwF_OHI)

Next, let’s talk about how to handle unhappy paths. As a reminder, unhappy paths are cases where things do not go smoothly For instance, include no-match responses in places where you prompt the user. If you are building a voice agent, also include no-input responses. Everywhere you have built a webhook, add events to handle webhook errors. It is important to anticipate and design for unhappy paths to create some level of predictability around their handling from the virtual agent Anticipate non committal answers to bot questions and add routes for them. For instance, if you ask the user “What date will you be traveling”, make sure to add a treatment for answers like “I’m not sure”. You should also Add treatment for users who escalate and Accommodate requests for more time. These are common conversational patterns that should be addressed.

### Video - [Best practices](https://www.cloudskillsboost.google/course_templates/1103/video/491496)

- [YouTube: Best practices](https://www.youtube.com/watch?v=d2rAzgikWwU)

Finally, let’s discuss best practices. Some best practices include the following: Build once and reuse, Keep your build simple and self-document everything you do. Standardize your treatments and your approach. And make sure to minimize the possibility of users encountering unhappy paths. Let’s take a few moments to explore these best practices in more detail. One extremely important best practice is to built once and reuse rather than rebuilding the same thing over and over. Rebuilding things takes time and if you wish to make changes, you’ll have to do so in multiple places. Here are some examples: First, reuse entity types and intents where possible. So, instead of having thirty different “yes” intents, try to use the same yes intent everywhere. This will also help standardize your NLU practices as well. Second, abstract out common patterns (such as handling of authentication) and devote a flow (or subflow) to them, sending everyone with that need to this flow. Similarly, within a flow, if there are multiple places where the same thing needs to be done, build a page for that and send all users there. For instance, in a given flow you might have a single page to handle cases of user escalation. This can also help with reporting and root cause analysis when bugs and optimizations are found. Next, use route groups or routes on the start page rather than copying the same route on multiple pages. This will drastically reduce the time spent on configuring each intent. It is especially helpful in steering flows where 100s of intents could be in scope. Lastly, if you have multiple intents with the same treatment, have them set a parameter and use routes involving that parameter to yield the treatment you want. These are only four possible examples. There are plenty more to think of and you should constantly be looking for ways to reuse components, especially at enterprise scale. Another tip is to keep your design simple and self-document everything you do. . Building a bot is often a highly collaborative effort. Even if you’re the only one building the bot, you may have to return to it after several weeks or months. It’s best if you can easily understand what you’ve built. For instance, you should avoid solutions that are very difficult to understand. You might be able to successfully nested together 10 different system functions. But the next person to stumble upon the place where you did this may find it impossible to understand what you’ve done and accidentally break it. Use names that are easy to understand. For instance, maybe within a subflow, you have a set of pages devoted to helping a user reduce their bill. It might be a good idea to title them all starting with “Bill reduce” Then add in their function, such as “Bill reduce – suggest paperless billing” Take advantage of the ability to add descriptions to intents, pages, and so on. Keep them concise and descriptive. These are also used as meta-data when referencing some of the generative features in Dialogflow CX. Don’t overload a page with too many tasks or routes. If you’re trying to do many different things on a page, it may be a sign that you need to separate it out into separate pages. Another tip is to standardize your treatments and approach. Use consistent naming conventions. For instance, perhaps all the pages in your flow start with the subflow name and then a description of what the page does. Where appropriate, offer consistent treatments. For instance, if you enable DTMF in some places, it’s good to enable it everywhere. It’s frustrating for users if the rules for interacting with the bot change throughout the conversation. And it’s easy to implement and verify that things are implemented correctly if there are standard patterns you can follow. A final tip is to minimize the possibility of unhappy paths. One example is to use clear prompts that minimize the possibility of the bot misunderstanding the answer. For instance, you might ask the user to select between two options. If so, word your prompt so that it’s very clear they have to pick one of the two choices. Where appropriate, offer options to users. For instance, suppose that users are asking to make a payment arrangement. Suppose that the vast majority of users prefer to pay on the last possible day. Instead of asking what day they wish to pay, simply ask them if they wish to pay on this date. Remind them that it is the last possible day. In a chat application, most people might choose one of three options as an answer to a question. So offer these as buttons while leaving a text window for people to make other selections. It is worth emphasizing that you should not always offer options, only do so where appropriate. If there are many different choices a user might make at a given point, then offering options artificially restricts their options. To take an extreme example, when it comes to initial head intent collection, don’t restrict the user’s options. Simply ask them what help they need. You can also minimize the possibility of unhappy paths by trying to simplify things for the user. For instance, if a user can receive texts then send them a text message with a link to a website instead of reading out the name of a long website. There is nothing more frustrating for a user than to get stuck at a page and be unable to proceed no matter what they do. One way to avoid this is to add complimentary paths. So, if you have a yes path, also have a no path. If you have a route for if a parameter is above zero, also add a route for if it’s less than or equal to zero. You might have a page that navigates users to other pages depending upon what sort of account they have. If so, add a route at the end of your list as a catch-all and route them to some sort of fallback behavior. Lastly, to minimize the possibility of unhappy paths, Identify and try to avoid possible loops. For instance, suppose you have a billing disambiguation for users who say vague phrases like “billing”. If they say billing again, they remain on the page. This allows users to say “billing” over and over again and be stuck on the page indefinitely. To handle this, you can use a parameter as a counter. Just add one to its value each time they hit this page. After a while, transfer them away from the page. This concludes our section on best practices. Thank you for joining us on this journey to learn about advanced agent design. Congratulations on completing this training and good luck with creating your very own agents!

### Quiz - [End-to-End Flows Quiz](https://www.cloudskillsboost.google/course_templates/1103/quizzes/491497)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
