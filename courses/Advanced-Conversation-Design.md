---
id: 1106
name: 'Advanced Conversation Design'
type: Course
url: https://www.cloudskillsboost.google/course_templates/1106
date_published: 2025-03-12
topics:
  - Chat
---

# [Advanced Conversation Design](https://www.cloudskillsboost.google/course_templates/1106)

**Description:**

In this course, you will learn the advanced conversational design principles for both the Voice and Caht channels to craft engaging and effective end-to-end experiences that emulate human-like interactions.

**Objectives:**

* Understand how to differentiate between chat and voice conversations.
* Review of the writing process for creating conversation scripts.
* Identify communicative acts and explore the concept of TCUs.
* Learn how to evaluate basic and complex closing sequences.
* Understand how to analyze escalations to live agents and their various types.
* Learn the best repair techniques for voice and chat agents and strategies for holding long pause.
* Understand the different types intonation according to question type.

## Differences between chat and voice conversations

This module explores the differences between chat and voice conversations. 

### Video - [Differences between chat and voice conversations](https://www.cloudskillsboost.google/course_templates/1106/video/526718)

* [YouTube: Differences between chat and voice conversations](https://www.youtube.com/watch?v=KjXJjy2OCAw)

Welcome to this course on Advanced Conversation Design. This training will expand upon the content presented in the Conversation Design Fundamentals course, to delve into specific elements to craft engaging and effective end-to-end experiences for the Voice channel. This advanced course will teach you to design great conversational experiences for both voice and chat using Dialogflow CX. You'll learn the key differences between voice and chat mediums, techniques for writing effective scripts, how to best use questions to unblock a conversation and strategies for crafting strong closings. We'll also cover how to smoothly handle escalations, and some key guardrails to keep the conversation flowing and create a more natural feel through intonation. Let’s jump into it! Do you know what are the differences between voice and chat virtual experiences? Traditional virtual agents relied a lot on the differences between chat and voice. However, when it comes to human-like experiences, there aren’t many differences between customer service conversations occurring via chat or voice. The structure and all the main elements should remain the same regardless of the channel. There are, however, a couple of things to keep in mind: First, participants allow each other more time to respond during chat conversations. For example, taking 60 seconds or even two minutes to send a message is not uncommon. In voice, a delay of more than 1 to 5 seconds can signal communicative challenges. As such, the setting for no speech timeout for voice virtual agents should be much shorter than for chat virtual agents. Second, chat conversations happen on screen and participants can typically scroll up and down the conversation. This affects what makes or doesn’t make sense to say. For example, using “Can you repeat?” as a form of repair in chat doesn't make sense because there’s no need to repeat anything. Participants only need to re-read the previous turn or scroll up to see what was said and then go from there.

### Quiz - [Differences Between Chat and Voice Conversations Quiz](https://www.cloudskillsboost.google/course_templates/1106/quizzes/526719)

## Writing process in creating conversation scripts

This module explores the process of crafting natural, human-like conversation scripts.

### Video - [Writing process in creating conversation scripts](https://www.cloudskillsboost.google/course_templates/1106/video/526720)

* [YouTube: Writing process in creating conversation scripts](https://www.youtube.com/watch?v=JbWABZXst-A)

Next, we will continue expanding on the writing process to write human-like conversation scripts that you learned about in the Conversation Design Fundamentals course. As we saw in the Conversation Design Fundamentals course, when writing a conversation scripts, the best practice is to start with the big picture and add details later on. In a previous module, we started with the first two steps of the conversation script writing development. The first step was to develop the basic template where we laid out the service request, service response, and acceptance of a conversation. This was the core conversation structure. Then, as our step 2, we designed the welcome message. Now, it’s time to move on to further steps of script development to fill in the “in between turns” that make up the conversation, which is called “the happy path”. This is what will enhance the structure of our conversation and it’s specific to our use case. In this diagram, this happy path is part of the “Structure’s enhancement”. Once this use case structure has been designed, you’ll enhance the design further adding guardrails and action to keep the conversation on track. As a last step, it’s important to do a final check of the conversation script to make sure the persona –that you designed at the beginning of this whole process– is well represented in the script.

## The Beginning of the Interrogative Series

This module will improve your understanding of a two way communication between a virtual agent & and a customer by exploring communicative acts, adjacency pairs, and turn constructional units or TCUs.

### Video - [The beginning of the interrogative series](https://www.cloudskillsboost.google/course_templates/1106/video/526721)

* [YouTube: The beginning of the interrogative series](https://www.youtube.com/watch?v=EVvcIXXxflA)

The interrogative series is the back and forth questioning and answering between the agent and the customer. The aim of this series is for the agent to collect necessary information to help the customer. This series contains the bulk of the conversation, and there are some important related concepts that you need to understand. In this section, you’ll learn about communicative acts, adjacency pairs, and turn constructional units or TCUs. When people speak, they perform acts that may not be explicitly stated in their words. For example, a sentence such as “There’s only one slice left” not only states something about the real world, but it also expresses acts that can be interpreted in different ways depending on the communicative context. Such as: As an offer as in “would you like to have it?”. As a warning as in “don’t you dare touching it”. As scolding as in “you didn’t finish your meal”. It’s important, then, to consider the following: First, what the meaning or intention behind each utterance is as opposed to only its literal meaning. Second, what the communicative context is, including speakers, situation, shared knowledge, etc. And third, how the listener interprets the speaker's intended meaning since this can help us to understand more easily what’s going on. Customers and representatives engaging in customer service conversations perform a variety of communicative acts. For example requests for information, or providing informative answers and offers. Other communicative acts could include acceptances or rejections, requests for action, and granting or declining requests. All of these acts need to be correctly recognized by both speakers to avoid communication. breakdowns. The basic building blocks of sequences in conversations are adjacency pairs (or APs). These are action pairs that are One, produced in two different turns, Two, uttered by different speakers, Three, ordered such that the first-pair part (or FPP) precedes the second-pair part (or SPP). And lastly, APs are related such that a particular FPP matches a particular SPP. For example, a typical adjacency pair would be: An agent asking: When and where do you go? And a customer replying: Today… for going to Cheju. In this example, the representative requests for information and the customer provides an informative answer. Note how each action is produced in a different turn, by a different speaker, such that the request precedes the informative answer, and the action of requesting for information naturally matches providing an informative answer. However, the following two consecutive turns would not form an adjacency pair: Say the customer starts by saying: “Hi. Uh… My laptop was sent onto the Toshiba factory from uh… other repairs. How do I check the status of that?” To which the agent responds: “What’s the repair authorization number?” Note how these two turns satisfy the first two conditions for adjacency pairs, which is that they are produced in two different turns, by two different speakers. But they do not meet conditions 3 and 4 since these two turns are both “first turns” and not the first and second parts of the same pair. This example illustrates how actions are organized into adjacency pairs within a naturally occurring customer service conversation. Each adjacency pair is colored differently on the screen for better visualization. For example, the customer’s turn “Can I have the police out here at forty two Kansas.” naturally pairs with the agent’s turn “Ok, I’ll send an officer by there to see if he can help you.” Similarly, the agent’s question “east or west?” naturally pairs with the customer’s answer “west Kansans”; and the agent’s question “What’s the trouble?” naturally pairs with the customer’s answer “Uh well, I’ve had some rabbits stolen”. Note also how actions in conversations are not organized in consecutive turn pairs, but these pairs can be “separated” by other adjacency pairs. That is, the answer to a question doesn’t always come immediately. Or, in this example, we can see how the turns colored in yellow are separated by the turns in green, blue, and red. In this case, then, the fulfillment of a request, doesn’t necessarily come immediately after. Keep also in mind that, while most of the times speakers perform one action per turn, this is not always the case as it can be seen in the second agent’s turn. We just saw that adjacency pairs can be “separated” by other adjacency pairs. Insert-expansions are extra conversational pairs inserted between the first part pair and the second part pair of another adjacency pair. We'll refer to this original pair as the base AP. These additional pairs are related to the base AP and do not change the topic of the conversation. Insert expansions can be directed either to the FPP of the base AP to clarify or address an issue occurring in the previous turn or to the SPP to seek information necessary to successfully produce or deliver the response. The latter is commonly found in customer service conversations where there might be multiple consecutive insert-expansions. In this conversation, four insert-expansions occur (highlighted in green and blue) before the customer is successfully presented with a sandwich. Observe how, once the customer has made their request of having a sandwich, the employee starts gathering information from the customer to deliver the sandwich. First, it asks about the topping –mayonnaise or mustard–, second, whether the customer wants cheese. Third, whether the customer wants any vegetables. And last, whether the customer wants a pickle with their sandwich or not. Once all this information is collected, the employee can deliver the sandwich in their last turn. Note how these insert expansion are used by the employee to collect necessary information from the customer to deliver the sandwich. In other words, these insert expansion are directed towards the resolution of the conversation. This is what we call the happy path in conversation design, this is, the turns aimed to gather information from the end-user to the virtual agent can deliver an appropriate solution. Lastly let’s discuss Turn-constructional units (or TCUs). If an adjacency pair was the smallest sequence in a conversation, a turn constructional unit is the smallest and most basic building block of a turn. It varies in length: it can be a sentence, a phrase, a word, an interjection –such as “uhu”, or “mhm”–, or even silence. It completes a communicative act. They’re self-contained and can be recognizable by the listener as ‘possibly complete’. Note in the exchange above there are different types of TCUs. From phrases such as “west Kansas”, to words like “Yeah”, or complete sentences such as “Can I have the police out here at forty two Kansas?”. Each of these TCUs expresses a different communicative act such as request for action, check understanding, request for information, provide an informative answer, etc. Typically, in human conversations speakers produce only one TCU per turn as can be seen in this example. Note how the customer makes a request using a simple sentence: “I’d like a ham sandwich on wheat”. Then, the employee asks simple questions delivered in very short turns such as in “Mayo and mustard?” or simply “Cheese?”. Similarly, the customer replies with very simple and short turns as in “Just a little bit of mayo. Humans like to be very efficient in conversation and only say what’s necessary to keep the conversation going. . Unfortunately, many virtual agents devote too much time on pleasantries and on providing information that is unnecessary to solve an end-user’s issue. In the example on the left, note how the human agent gets quickly to the point so they can assist the customer as efficiently and quickly as possible. Traditional virtual agents, however, tend to have a large number of TCUs per turn while providing a set of unnecessary and contextually obvious explanations that humans don’t provide. In the example on the right, the virtual agent: Requests the customer to confirm what they want (which they clearly stated). Acknowledges the customer’s input. Informs the customer about their willingness to help. Informs the customer about the virtual agent initiating the process of helping. Explains to the customer that there are necessary steps to continue the conversation and that these steps are about to start. Explains to the customer that the first step for the VA to help them is to know the details about the repair order. Asks for those details, like the repair authorization number. And as you can see from the example on the left, humans do well with just going straight to the point. Now that we’ve gone over the basic elements of the happy path in customer service, it’s time to put the use case together. As mentioned before, when developing a conversation script, step 1 is to lay out the core structure of the conversation with the service request, the service response, and the acceptance of a use case. Step 2 entails designing the welcome message. In this step 3, we can now focus on filling in all the steps to go from the service request to the service response: the happy path. First, it’s important to focus only on the necessary questions needed to gather the required information for completing the transaction. So to start, write down what information you would need to fulfill the end user’s request before writing any question. For example, to fulfill an order status request, you may only need the order number. But to book a flight you may need to collect the city of origin, the destination, the day the end user wants to fly over, Et cetera. Then, write just a simple question (with no explanations and no justifications) to gather that information from the user and move on. If you have human to human data, feel free to borrow verbiage from real human live agents so you’re sure your virtual agent will sound natural. Also, remember that turns typically have only 1 TCU, so keep them short and simple especially at this stage of the conversation script writing process. Lastly, add your question to the basic template between the service request and the service response. In our example for a virtual agent that helps booking flight tickets, we didn’t add the question to collect the city of destination (Where are you departing from?) because the end user provides it in their head intent (Hello, this is Tripworld’s virtual assistant. How can I help you?). But we added the question to collect the city of origin and the departure date.

## Closing Sequence

This module explores the natural process of closing a conversation between a human agent and a customer as well as the difference between basic and complex closing sequences.

### Video - [Closing sequence](https://www.cloudskillsboost.google/course_templates/1106/video/526722)

* [YouTube: Closing sequence](https://www.youtube.com/watch?v=JXme3ieyBqA)

When the customer has been helped, both speakers –the agent and the customer– move together onto the closing sequence. In this next section, we’ll explore what the natural process to close a conversation is. We’ll go over what a basic closing in a conversation is, and how a more complex closing looks like. There isn’t a single prescribed method for closing a conversation in customer service. The conclusion of interactions exhibits considerable diversity in the quantity and nature of conversational turns involved. However, these closures aren't spontaneous occurrences. Speakers actively negotiate the timing and manner of concluding the interaction. Therefore, it's crucial to replicate this negotiation process as closely as possible to ensure a human-like conclusion to the conversation. In this example, we can see a basic closing sequence with two sets of adjacency pairs a pre-closing, and a terminal exchange. Here, both the agent and customer transition from the main sequence to the closing sequence and they organize the latter with just a pre-closing and a terminal exchange. The pre-closing signals that they’re ready to end the conversation, and the terminal exchanges ends it. In this particular case, after the customer accepts the outcome of the conversation, they express appreciation with “Thank you”, and then both speakers move onto the farewell with “bye”. There are different types of pre-closing adjacency pairs at the end of customer service conversations. In this example, we can see some of the most common ones. Here, both the representative and the customer go beyond a basic closing sequence and use four types of pre-closing. First, and after the acceptance, the customer appreciates the agent for their help with “Thanks you so much”. Second, the agent checks on the customer if they have any other issues they need to be resolved as the topic-initial elicitor pre-closing shows. In this case, the agent asks “Was there anything else I could take care of for you today?”. Third, the customer summarizes the reason for their call. Last, the agent shows appreciation to the customer and wishes them a good weekend. All throughout these pre-closings, both speakers are signaling to each other that they’re ready to end the conversation, and they do so with pre-closings that signal the end of the conversation more and more strongly. Eventually, both speakers move onto the terminal exchange and the conversation ends after the last customer’s turn. A common not human-like behavior in virtual agents is to close the conversation too abruptly. Virtual agents often skip the acceptance phase and jump onto the “anything else?” question to quickly. They also tend to use incomplete pre-closings by responding to utterances that didn’t happen. This prevents the customer to properly close pre-closings. And sometimes virtual agents even skip the terminal exchange entirely before disconnecting. For instance, in this example, the virtual agent jumps to “What else can I help you with?” too quickly without allowing the user to accept the outcome of the conversation. Then, once the customer responds to this question, the agent says “glad I could help”; but this is unwarranted since the customer didn’t initiate any pre-closing for appreciation. And right after that and without allowing the customer to say anything else, the agent performs an appreciation pre-closing –without letting the customer react to it–. Then, it moves onto the terminal exchange with “bye” –once again not allowing the customer to react to it– and, finally, disconnects the call. All of this would be considered rude if a human did it. A better approach to the previous conversation is to allow the customer to participate in the closing process together with the virtual agent. In this example, the agent allows the customer to accept the outcome where they say “thanks!”. Then, the agent asks the customer if they can help with anything else just like in the previous conversation. After that, however, the agent doesn’t immediately end the call. First, it wishes the customer a great rest of their day AND allows the customer to react to it. After that, if moves to the terminal exchange with “alright, bye!” and waits for the customer to also say “bye”. Only after the customer says “bye”, the virtual agent ends the call. To write your virtual agent’s closing sequence: First, decide whether you want a basic closing sequence or you want to make it a bit longer. If you want your virtual agent to use an 'Anything else?' question, consider having a longer closing sequence. This will allow you to transition to a pre-closing that clearly indicates the end of the conversation before the final exchange. In this example, a few steps is added to end the conversation, making it a bit longer than the basic closing. The virtual agent performs an appreciation pre-closing (“Thanks for calling us today”) followed by a solicitude pre-closing (“hope you have a good rest of your day”). But you may want to use different pre-closings. Finally, make sure all pre-closing and terminal exchanges are complete and closed before moving onto the next one. If you are concerned that the customer will never responding to these, use no-inputs to move the conversation to the next step.

### Quiz - [Closing Sequence Quiz](https://www.cloudskillsboost.google/course_templates/1106/quizzes/526723)

## Escalations to Live Agents

This module explores how a virtual agent can gracefully transfer a conversation to a live agent once all available options for self service containment are exhausted.

### Video - [Escalations to live agents](https://www.cloudskillsboost.google/course_templates/1106/video/526724)

* [YouTube: Escalations to live agents](https://www.youtube.com/watch?v=dNjUpDHmk7Y)

When the virtual agent has exhausted all the options available to contain a call, prompt escalation to a live agent is required. Next, let’s explore a couple of escalation scenarios related to voice agents. In this section we’ll explore escalation due to no-inputs as well as escalation due to failed data verification and authentication. In the voice channel, the general best practice is to escalate on the third no-input to ensure a smooth interaction for the end-user. The agent should check the availability of the user to resume the conversation twice before escalating to a live agent, unless business rules dictate otherwise. For instance, the customer might say: “I want a different service”. The agent responds: “What service are you interested in?” followed by a silent pause from the customer. The agent makes the first prompt by saying: “Hello?”. Still, no response from the customer. The second prompt follows: “Hello, are you still there?”. Still silence from the customer’s end. Finally the agent initiates the escalation by saying: “Okay, let me connect you with a live agent who can assist you.” Sometimes, though, due to business rules, the third no-input will lead to end the session if the business rules require so. One of the key reasons for escalation is when the data provided by the user couldn't be verified or if the user couldn't complete the required authentication of some form. Another scenario that calls for an escalation to a live agent is when, during authentication, the virtual agent can’t successfully authenticate the customer. In this example, the agent tries to verify the customer’s identity via a PIN code, which the customer provides. However, this PIN code doesn’t seem to be correct and the customer realizes they may have forgotten it. At this point, since the customer’s identity can’t be verified, the virtual agent escalates them to a live agent to continue assisting them.

## Guardrails to Keep Conversation Flowing

This module explores how to optimize the conversation flows by covering the most effective methods for repair and long pauses handling.

### Video - [Repair techniques for voice agents](https://www.cloudskillsboost.google/course_templates/1106/video/526725)

* [YouTube: Repair techniques for voice agents](https://www.youtube.com/watch?v=zC4OUF28bhw)

There are some additional guardrails you can leverage to optimize your conversation flow and that is what the next section will cover. Specifically, we’ll explore repair techniques for voice agents and dive into how to manage silences in conversations. Let’s start by learning some additional repair techniques that humans use in conversation. The following repair techniques expand on what you learned in a previous module. In the Conversation Design Fundamentals course, we learned about conversation repair and some basic techniques that can be used on a steering bot. Specifically, we went over: Open class repair with phrases such as “Excuse me?”, “Pardon”, “Could you say that again?”, etc. This is the weakest form of repair is it doesn’t signal where the issue is. What do you mean plus an element of a prior turn with phrases such as “What do you mean phone number?”, “What do you mean 100GB?”, etc. This type of repair is stronger and can be very useful to disambiguate when customers communicate with words in isolation and it’s hard to know what they mean. (You mean) + understanding check with phrases such as “You mean the data roaming, whether it’s on or off?”. This is the strongest form of repair and can be useful to confirm the correct understanding of the customer’s utterance before continuing the conversation. Building on this knowledge, we will now go over some more advanced techniques that can be utilized in end-to-end experiences in both chat and voice agents. An important type of repair is “interrogatives”. These are prompts that help locate the source of the misunderstanding without explicitly saying the words that caused the misunderstanding. Examples of interrogatives include "Where?", "What number?", "With whom?", and "What do you mean by that?" Here’s an example of how an agent can use an interrogative. The customer says: “I ordered from a store in Kansas last week”. However, before the customer can continue, the agent asks “Where?”. This prompts the customer to supply the information that is causing the misunderstanding. The customer replies “Kansas”. Partial repetition refers to when the Virtual Agent partially repeats the customer's statement to clarify the misunderstanding. This combines with an interrogative at the end to emphasize the specific part of the statement that needs clarification. Examples include: Talked to whom? You have what? You called when? You’re missing how many? Let’s look at an example where the customer says: “I’m calling because I have 13 points and I wanted to use them on gas but I don’t know what’s wrong with my app.” The agent answers: “You were going to use them where?” Then the customer says: “On gas”. Using a stronger repair technique shows more clearly that we’re listening we’re engaged in the conversation. As such, humans prefer stronger forms of repair over weaker ones. The preference for stronger forms of repair over weaker ones is well illustrated through how repair techniques progress in a conversation. Humans move towards stronger forms of repair as the example shows. The agent uses an open class, the weakest form of repair as their first technique “Pardon?”. This neither indicates to the customer what the trouble-source is, nor where it’s located. Then, in their next turn, the agent chooses the strongest form of repair: an understanding check, “Oh, for trade in”, which the customer confirms with “yes, yes”. When it comes to designing prompts for virtual agents, it’s typical to have at least two no-match prompts before escalating to a live agent. In these cases, it’s important to use a stronger technique in the second no-match than in the first one. For example, in this conversation the customer is trying to book a hotel. The virtual agent asks “Which city?” after it didn’t understand the city the customer is interested in booking a hotel in. This is a Wh- interrogative technique. For the second no-match prompt, the virtual agent moves to a stronger form of repair: a partial repetition plus wh- interrogative. “You want a hotel in which city?”

### Video - [Handling long pauses](https://www.cloudskillsboost.google/course_templates/1106/video/526726)

* [YouTube: Handling long pauses](https://www.youtube.com/watch?v=l_NRSbFfS9s)

Next, let’s explore how humans manage unexpected silences in conversations. Long pauses can be the result of any of the speakers having their attention diverted, needing time to complete a task, having technical difficulties, etc. Traditional IVR systems reprompt the same question or turn to the user every 5 to 10 seconds until either the end-user responds or the max-no-input event triggers, which ends the session or escalates to a human agent. Humans, however, typically check on each other’s availability to continue before resuming the conversation. Once they both confirm with each other that they’re “ready”, the conversation resumes. Humans handle long pauses in four ways. We should use these methods to guide us when designing a Voice Virtual Agent. First, if the representative needs time to check on something, then they should announce it with a phrase like “Hold on a second” before the silence occurs. This normally only happens if the representative can predict a long pause. The representative can also check on the customer’s availability before resuming the conversation. For example, “Hello?”. This is to confirm their presence before resuming. Another strategy is to check on the customer with a short utterance, like “Hi”. Then retry with a longer utterance, like. “Sir, hello. Can you hear me?” This strategy shows a readiness to continue. And it reassures the other speaker of your availability. Lastly, you can use an acknowledgement, followed by a prompt. This indicates a reconnection and moves the conversation forward. Once the representative regains the customer’s attention, the conversation can be resumed with a short acknowledgement, like “Okay” or “Alright”. Here is an example: In the conversation the agent says: “Let me check the…” But the customer interrupts saying: “I have to go by and get it.”, probably while having a side conversation, and then a period of silence follows. Then, the agent checks whether the customer is ready to continue or not by saying “Hi.”. Since the customer isn’t ready, the agent tries again with a longer utterance to grab the attention of the customer with “Sir, hello. Can you hear me?”. Finally, the customer confirms that they are still there and ready to continue the conversation with “Yes, ma’am. Yeah.”. And lastly, the agent acknowledges the customer’s confirmation with “Alright”, and resumes the conversation.

### Quiz - [Guardrails to Keep Conversation Flowing Quiz](https://www.cloudskillsboost.google/course_templates/1106/quizzes/526727)

## Intonation

This module explores the intonation patterns normally used by humans during their conversations for a smoother, more connected conversation. 

### Video - [Connecting speech](https://www.cloudskillsboost.google/course_templates/1106/video/526728)

* [YouTube: Connecting speech](https://www.youtube.com/watch?v=z718ELMvkcA)

In the last section, we’ll explore intonation patterns that humans use when having conversations. First, we’ll go over connecting speech within turns and how to adapt our virtual agent’s turns so they sound smoother and more connected. We will also explore how to deploy SSML tagging to dictate intonation. Then we will discuss intonation for questions and go over different types of questions and how to make a virtual agent pronounce them correctly. Let’s start by discussing connecting speech. Typically, humans connect all the TCUs in their turns. They often do this by avoiding drops in intonation to delay signaling to the other speaker that they can “jump in”. Turns are generally made up of only one TCU. To learn more about TCU’s refer to the Additional resources document. These exceptions occur where humans mobilize a set of techniques to connect all TCUs. While in written language we separate all phrases and sentences with commas and periods, in real life humans tend to pronounce their turns connecting multiple phrases and sentences with one single breath. Note the lack of periods on the right column and, when present, they’re followed by latches –indicated by eucual signs– or faster speech –indicated by the greater and smaller than signs– to avoid a pause. This lack of periods indicates lack of falling intonation and connection between all grammatical sentences. Note also that, when a pause is present (as in the second example) it is produced in a grammatically unexpected place so that it’s clear to the other speaker that the turn is not yet finished. You may have written all your longer turns similar to the examples on the left column. Unfortunately, TTS models tend to insert very long pauses after periods and commas, which results in longer turns sounding very choppy and virtual agents sounding very robotic. If you try the turns on the left in the TTS demo, you’ll hear these long pauses. So, you’ll need to modify the writing of these types of longer turns so they sound more like the column on the right. If you’re using TTS models such as the Basic, WaveNet or Neural 2 models, it’s easy to adapt the writing to get an effect in the pronunciation so it’s closer to human-like experiences. The easiest way is to simply remove all punctuation marks except for the last one. For example, “Jack Camera This is Tara speaking May I help you, period.” If you try this modified welcome message in the TTS demo, you’ll hear that everything is now connected. Also, even if you don’t separate all TCUs into different sentences, the TTS model still uses intonation contours to mark each TCU. Intonation contours are the up and down changes in the voice. So, if you pay close attention, you’ll hear that the TTS model lowers the intonation at the end of each TCU which is marked by a period in this example. And adds a very subtle stress at the beginning of each TCU which is marked by the underlined words. Additionally, the model connect all of them with equal signs. Additionally, the TTS model tends to mark certain words with a higher pitch to mark grammatical contrast, indicated with the arrow pointing up in the second example. This results in a pronunciation that, while not exactly human, is much closer to a human-like experience and leads to a much smoother turn. Occasionally, deleting all punctuation marks might confuse the TTS model in some specific places where it can’t correctly predict where the TCUs are. For example, if you try the following welcome message in the TTS demo, you might notice a bit of a rushed speech between “Tara” and “your” . This might make the grammar sound a bit confusing to some listeners. Thanks for calling Jack Camera this is Tara your virtual agent assistant how can I help you? You could add a comma after “Tara”. However, this will insert an unnatural long pause making the turn sound choppy. In these cases, Speech Synthesis Markup Language tagging (or SSML) can help. You can keep the comma after “Tara”, but reduce the pause using a tag for a break time of 1 millisecond. This will create an intonational contour in Tara which will separate the first part of the sentence from the rest while not adding extremely long silences in the middle.

### Video - [Intonation for questions](https://www.cloudskillsboost.google/course_templates/1106/video/526729)

* [YouTube: Intonation for questions](https://www.youtube.com/watch?v=jk3uuHkSgsI)

Now, let’s explore intonation for questions. Each language has very specific intonation rules for questions and statements depending on their syntax and communicative acts. Here are some examples, along with their intonation rules in English: Wh- or open questions are pronounced with falling intonation at the end, which is how statements are pronounced. Yes-no questions, however, are pronounced with rising intonation at the end. And closed-choice alternative questions are pronounced with rising intonation after each choice except for the last one, which is pronounced with falling intonation. Usually, the speech model for a specific language will be trained to follow these rules, but sometimes adjustments to intonation are required. These adjustments can be done with the use of punctuation markers and, or with SSML tagging. “Wh-” [note to actor: pronounce as ‘double you aitch’] or “open” questions are pronounced with falling intonation (like statements). If your agent pronounces them with rising intonation, customers may interpret them as yes-no questions and react to them with answers like “yes”, “no”, or “I don’t know”. If the TTS model doesn’t use falling intonation, try using a period instead of a question mark at the end of the question. For instance, “How can I help you, period.” Or “Which countries are you visiting period.” Or “What’s your account number period.” Yes-No questions are pronounced with rising intonation so we can use punctuation marks as usual. Closed-choice alternative questions have the form of “do you prefer A, B, or C?”. They imply that the only choices are A, B, or C but not D or E. That is, there are a finite number of choices. In this case, all items in the list have rising intonation except for the last one, which has falling intonation. If this is not the case for your virtual agent’s TTS model, there are different ways to get this intonation pattern. Let’s explore some strategies to achieve this. The first solution is to make sure you have a comma separating all elements of the list, even if you only have two elements. For example, “Do you have problems with inbound, comma, or outbound calls?” However, this inserts an unnatural long pause after the comma. This may result in callers thinking that they need to answer a yes or no question as in, “Do you have problems with inbound?”, to which they’ll react by answering “yes”, “no”, or “I don’t know”. Subsequently, you can reduce the pause time with SSML tagging as we saw before: Do you have problems with inbound, break time 1 millisecond, or outbound calls? Another strategy is to replace the question mark with a period with or without a comma after the first element of the list: For example, “Do you want to change it now or later, period.” Or, “Do you want to change it now, comma, or later, period.” If using a comma, use SSML tagging to remove the pause as shown in Solution 1. As a last resource, you can “force” the correct intonational pattern by adding a question mark after each item of the list except for the last one, where you’d need to use a period. Similar to the comma issue, the TTS model inserts unnatural pauses after each punctuation mark, so use SSML tagging to reduce the pause using a break time of 1 millisecond: “Would you like to keep it, question mark, change it, question mark, cancel it, question mark, or pay now, period.” Clarifying questions don’t present new information to the customer. Instead, they repeat or rephrase a question to indicate that something is not clear while soliciting clarification. Generally, we’ll use these types of questions when user utterances are not recognized like with no-matches. These questions are pronounced with rising intonation unless they’re closed-choice alternative questions, which will still follow the intonation patterns explained in the Closed-choice alternative questions slides. However, the voice model may try to pronounce Wh- questions with falling intonation since this is their default pronunciation. This is because voice models don’t know when a Wh- question is actually a clarifying question. To address this issue, try one of the following solutions. Shorten the question. Sometimes, going for Wh- plus a noun phrase (with no verb) will do the trick and you’ll additionally get a more natural, human like no-match. In this example, the clarifying question “Which country?” is short and the TTS might be able to gather what it needs to pronounce it with rising intonation. Sometimes adding words like “sorry”, and “again”, or using indirect questions helps the model understand that it should be pronounced with rising intonation. Using phrases like “Can you repeat”, plus Wh-, plus a noun is another way to achieve this. Examples include: Sorry, which country? Which country again? Can you repeat which country? Can you repeat what country you’re traveling to? As one last piece of advice, it’s recommended and really important to always test your verbiage in the Google Cloud Text-to-Speech demo to make sure the model is using the correct intonation. To access the Cloud TTS, refer to the Additional resources document.

### Quiz - [Intonation Quiz](https://www.cloudskillsboost.google/course_templates/1106/quizzes/526730)

## Additional Resources

This module includes the list of additional resources that complement the course learning.

### Document - [Additional Resources](https://www.cloudskillsboost.google/course_templates/1106/documents/526731)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
