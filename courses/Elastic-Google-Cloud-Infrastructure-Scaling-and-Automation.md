---
id: 178
name: 'Elastic Google Cloud Infrastructure: Scaling and Automation'
type: Course
url: https://www.cloudskillsboost.google/course_templates/178
date: 2025-04-03
datePublished: 2024-04-26
topics:
- Scaling
---

# [Elastic Google Cloud Infrastructure: Scaling and Automation](https://www.cloudskillsboost.google/course_templates/178)

**Description:**

This accelerated on-demand course introduces participants to the comprehensive and flexible infrastructure and platform services provided by Google Cloud. Through a combination of video lectures, demos, and hands-on labs, participants explore and deploy solution elements, including securely interconnecting networks, load balancing, autoscaling, infrastructure automation and managed services.

**Objectives:**

- Connect your infrastructure to Google Cloud.
- Configure load balancers and autoscaling for VM instances.
- Automate the deployment of Google Cloud infrastructure services.
- Leverage managed services in Google Cloud.

## Introduction

In this module we introduce the Architecting with Google Compute Engine course series. This course series is defined for cloud solution architects, DevOps engineers, and anyone who's interested in using Google Cloud, to create new solutions or to integrate existing systems, application environments, and infrastructure with a focus on Compute Engine.

### Video - [Course Introduction](https://www.cloudskillsboost.google/course_templates/178/video/470280)

- [YouTube: Course Introduction](https://www.youtube.com/watch?v=oB8Iro1j8nQ)

Hello. I'm Philipp Maier. I'm Mylene Biddle, we're both Course Developers, at Google Cloud and we want to welcome you to Architecting with Compute Engine, a series of three courses. Before we start using all of the different services that Google Cloud Platform, or GCP offers, let's talk about what GCP is. When you look at Google Cloud, you'll see that it's actually part of a much larger ecosystem. This ecosystem consists of open-source software, providers, partners, developers, third-party software, and other Cloud providers. Google is actually a very strong supporter of open-source software. That's right. Now, Google Cloud consists of Chrome, Google devices, Google Maps, Gmail, Google Analytics, G Suite, Google Search, and the Google Cloud Platform. GCP itself is a computing solution platform that really encompasses three core features: infrastructure, platform, and software. This map represents GCP's global infrastructure. As of this recording, GCP's well-provisioned global network connects over 60 zones to over 130 points of presence through a global network of fiber optic cables. And Google is continuously investing in this network, with new regions, points of presence, and subsea cable investments. On top of this infrastructure, GCP uses state of the art software-defined, networking and distributed systems of technologies to host and deliver your services around the world. These technologies are represented by a suite of Cloud-based products and services that is continuously expanding. Now, it's important to understand that there is usually more than one solution for a task or application in GCP. To better understand this, let's look at a solution continuum. Google Cloud Platform spans from infrastructure as a service, or IaaS, to software as a service, or SaaS. You really can build applications on GCP for the web or mobile that are global, auto-scaling, and assistive, and that provide services where the infrastructure is completely invisible to the user. It is not just that Google has opened the infrastructure that powers applications like Search, Gmail, Google Maps, and G Suite. Google has opened all of the services that make these products possible and packaged them for your use. Alternative solutions are possible. For example, you could start up your own VM in Google Compute Engine, install open-source MySQL on it and run it just like a MySQL database on your own computer in a data center. Or you could use the Cloud SQL service, which provides a MySQL instance and handles operational work like backups and security patching for you using the same services Google does to automate backups and patches. You could even move to a NoSQL database that is auto-scaling and serverless so that growth no longer requires adding server instances or possibly changing the design to handle the new capacity. This series of courses focuses on the infrastructure. An IT infrastructure is like a city infrastructure. The infrastructure is the basic underlying framework of fundamental facilities and systems, such as transport, communications, power, water, fuel, and other essential services. The people in the city are like users, and the cars and bikes, and buildings in the city are like applications. Everything that goes into creating and supporting those applications for the users is the infrastructure. The purpose of this course is to explore as efficiently and clearly as possible the infrastructure services provided by GCP. You should become familiar enough with the infrastructure services that you will know what services do and how to use them. We won't go into very deep dive case studies on specific vertical applications. But you'll know enough to put all the building blocks together to build your own solution. Now, GCP offers a range of compute services. The service that might be most familiar to newcomers is Compute Engine, which lets you run virtual machines on-demand in the Cloud. It's Google Cloud's infrastructure as a service solution. It provides maximum flexibility for people who prefer to managed server instances themselves. Google Kubernetes Engine lets you run containerized applications on a cloud environment that Google manages for you under your administrative control. Think of containerization as a way to package code that's designed to be highly portable and to use resources very efficiently. And think of Kubernetes as a way to orchestrate code in containers. App Engine is GCP's fully managed platform as a service framework. That means it's a way to run code in the cloud without having to worry about infrastructure. You just focus on your code and let Google deal with all the provisioning and resource management. You can learn a lot more about App Engine in the "Developing Applications with Google Cloud Platform" course series. Cloud Functions is a completely serverless execution environment or functions as a service. It executes your code in response to events, whether those events occur once a day or many times per second. Google scales resources as required, but you only pay for the service while your code runs. The "Developing Applications with Google Cloud" course series also discusses Cloud Functions. Cloud Run, a managed compute platform that lets you run stateless containers via web requests or Pub/Sub events. Cloud Run is serverless. That means it removes all infrastructure management tasks so you can focus on developing applications. It is built on Knative, an open API and runtime environment built on Kubernetes that gives you freedom to move your workloads across different environments and platforms. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere Knative runs. Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges you only for the resources you use calculated down to the nearest 100 milliseconds, so you'll never pay for your over-provisioned resources. In this series of courses, In this series of courses, Compute Engine will be our main focus. The Architecting with Google Compute Engine courses are part of the Cloud Infrastructure learning path. This path is designed for IT professionals who are responsible for implementing, deploying, migrating, and maintaining applications in the cloud. The prerequisite for these courses is the Google Cloud Platform Fundamentals: Core Infrastructure course, which you can find in the link section for this video. The Architecting with Google Compute Engine series consists of three courses. Essential Cloud Infrastructure: Foundation is the first course of the Architecting with Compute Engine series. In that course, we start by introducing you to GCP and how to interact with the GCP Console and Cloud Shell. Next, we'll get into virtual networks and you will create VPC networks and other networking objects. Then we'll take a deep dive into virtual machines, and you will create virtual machines using Compute Engine. Essential Cloud Infrastructure: Core Services is the second course of this series. In that course, we start by talking about Cloud IAM and you will administer Identity and Access Management for resources. Next, we'll cover the different data storage services in GCP, and you will implement some of those services. Then we'll go over resource management, where you will manage and examine billing of GCP resources. Lastly, we'll talk about resource monitoring and you will monitor GCP resources using Stackdriver services. Elastic Cloud Infrastructure: Scaling, and Automation, is the last course of the series. In that course, we start by going over the different options to interconnect networks to enable you to connect your infrastructure to GCP. Next, we'll go over GCP is load balancing and auto-scaling services. Would you will get to explore directly. Then we'll cover infrastructure automation services like Terraform so that you can automate the development of GCP infrastructure services. Lastly, we'll talk about other managed services that you might want to leverage in GCP. Now, our goal for you is to remember and understand the different GCP services and features, and also be able to apply your knowledge, analyze requirements, evaluate different options, and create your own services. That's why these courses include interactive hands-on maps through the Qwiklabs platform. Qwiklabs provisions you with a Google account and credentials, so you can access the GCP console for each lab at no cost.

### Document - [Welcome to Elastic Cloud Infrastructure: Scaling and Automation](https://www.cloudskillsboost.google/course_templates/178/documents/470281)

## Interconnecting Networks

Connect your infrastructure to Google Cloud

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/178/video/470282)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=Q0u6bAOOrck)

Priyanka: Hi, I'm Priyanka Vergadia, a developer advocate for Google Cloud. In this module, we focus on interconnecting networks. Different applications and workloads require different network connectivity solutions. That is why Google supports multiple ways to connect your infrastructure to GCP. In this module, we'll focus on GCP's hybrid connectivity products, which Cloud VPN, Cloud Interconnect, and Peering. We'll also look at options for sharing VCP networks within GCP. Let's start by talking about Cloud VPN so that you can explore in a lab.

### Video - [Cloud VPN](https://www.cloudskillsboost.google/course_templates/178/video/470283)

- [YouTube: Cloud VPN](https://www.youtube.com/watch?v=oVRAkKd-JXA)

Google Cloud offers two types of Cloud VPN gateways: HA VPN and Classic VPN. All Cloud VPN gateways created before the introduction of HA VPN are considered Classic VPN gateways. Classic VPN securely connects your on-premises network to your Google Cloud VPC network through an IPsec VPN tunnel. Traffic traveling between the two networks is encrypted by one VPN gateway, then decrypted by the other VPN gateway. This protects your data as it travels over the public internet, and that's why Classic VPN is useful for low-volume data connections. As a managed service, Classic VPN provides an SLA of 99.9% service availability and supports site-to-site VPN, static and dynamic routes, and IKEv1 and IKEv2 ciphers. Classic VPN doesn't support use cases where client computers need to "dial in" to a VPN using client VPN software. Also, dynamic routes are configured with Cloud Router, which we will cover briefly. For more information about the SLA and these features, please refer to the documentation link in the course resources for this module. Let me walk through an example of Cloud VPN. This diagram shows a Classic VPN connection between your VPC and on-premises network. Your VPC network has subnets in us-east1 and us-west1, with Google Cloud resources in each of those regions. These resources are able to communicate using their internal IP addresses because routing within a network is automatically configured (assuming that firewall rules allow the communication). Now, in order to connect to your on-premises network and its resources, you need to configure your Cloud VPN gateway, on-premises VPN gateway, and two VPN tunnels. The Cloud VPN gateway is a regional resource that uses a regional external IP address. Your on-premises VPN gateway can be a physical device in your data center or a physical or software-based VPN offering in another cloud provider's network. This VPN gateway also has an external IP address. A VPN tunnel then connects your VPN gateways and serves as the virtual medium through which encrypted traffic is passed. In order to create a connection between two VPN gateways, you must establish two VPN tunnels. Each tunnel defines the connection from the perspective of its gateway, and traffic can only pass when the pair of tunnels is established. Now, one thing to remember when using Cloud VPN is that the maximum transmission unit, or MTU, for your on-premises VPN gateway cannot be greater than 1460 bytes. This is because of the encryption and encapsulation of packets. For more information about this MTU consideration, please refer to the documentation link in the course resources. In addition to Classic VPN, Google Cloud also offers a second type of Cloud VPN gateway, HA VPN. HA VPN is a high availability Cloud VPN solution that lets you securely connect your on-premises network to your Virtual Private Cloud (VPC) network through an IPsec VPN connection in a single region. HA VPN provides an SLA of 99.99% service availability. To guarantee a 99.99% availability SLA for HA VPN connections, you must properly configure two or four tunnels from your HA VPN gateway to your peer VPN gateway or to another HA VPN gateway. When you create an HA VPN gateway, Google Cloud automatically chooses two external IP addresses, one for each of its fixed number of two interfaces. Each IP address is automatically chosen from a unique address pool to support high availability. Each of the HA VPN gateway interfaces supports multiple tunnels. You can also create multiple HA VPN gateways. When you delete the HA VPN gateway, Google Cloud releases the IP addresses for reuse. You can configure an HA VPN gateway with only one active interface and one external IP address; however, this configuration does not provide a 99.99% service availability SLA. VPN tunnels connected to HA VPN gateways must use dynamic (BGP) routing. Depending on the way that you configure route priorities for HA VPN tunnels, you can create an active/active or active/passive routing configuration. HA VPN supports site-to-site VPN in one of the following recommended topologies or configuration scenarios: An HA VPN gateway to peer VPN devices An HA VPN gateway to an Amazon Web Services (AWS) virtual private gateway Two HA VPN gateways connected to each other Let's explore these configurations in a bit more detail. There are three typical peer gateway configurations for HA VPN. An HA VPN gateway to two separate peer VPN devices, each with its own IP address, an HA VPN gateway to one peer VPN device that uses two separate IP addresses and an HA VPN gateway to one peer VPN device that uses one IP address. Let's walk through an example. In this topology, one HA VPN gateway connects to two peer devices. Each peer device has one interface and one external IP address. The HA VPN gateway uses two tunnels, one tunnel to each peer device. If your peer-side gateway is hardware-based, having a second peer-side gateway provides redundancy and failover on that side of the connection. A second physical gateway lets you take one of the gateways offline for software upgrades or other scheduled maintenance. It also protects you if there is a failure in one of the devices. In Google Cloud, the REDUNDANCY_TYPE for this configuration takes the value TWO_IPS_REDUNDANCY. The example shown here provides 99.99% availability. When configuring an HA VPN external VPN gateway to Amazon Web Services (AWS), you can use either a transit gateway or a virtual private gateway. Only the transit gateway supports equal-cost multipath (ECMP) routing. When enabled, ECMP equally distributes traffic across active tunnels. Let's walk through an example. In this topology, there are three major gateway components to set up for this configuration. An HA VPN gateway in Google Cloud with two interfaces, two AWS virtual private gateways, which connect to your HA VPN gateway, and an external VPN gateway resource in Google Cloud that represents your AWS virtual private gateway. This resource provides information to Google Cloud about your AWS gateway. The supported AWS configuration uses a total of four tunnels. Two tunnels from one AWS virtual private gateway to one interface of the HA VPN gateway, and two tunnels from the other AWS virtual private gateway to the other interface of the HA VPN gateway. You can connect two Google Cloud VPC networks together by using an HA VPN gateway in each network. The configuration shown provides 99.99% availability. From the perspective of each HA VPN gateway you create two tunnels. You connect interface 0 on one HA VPN gateway to interface 0 on the other HA VPN, and interface 1 on one HA VPN gateway to interface 1 on the other HA VPN. For more information on HA VPN and moving to HA VPN, refer to the documentation links in the course resources. We mentioned earlier that Cloud VPN supports both static and dynamic routes. In order to use dynamic routes, you need to configure Cloud Routers. Cloud Router can manage routes for a Cloud VPN tunnel using Border Gateway Protocol, or BGP. This routing method allows for routes to be updated and exchanged without changing the tunnel configuration. For example, this diagram shows two different regional subnets in a VPC network, namely Test and Prod. The on-premises network has 29 subnets, and the two networks are connected through Cloud VPN tunnels. Now, how would you handle adding new subnets? For example, how would you add a new "Staging" subnet in the Google Cloud network and a new on-premises 10.0.30.0/24 subnet to handle growing traffic in your data center? To automatically propagate network configuration changes, the VPN tunnel uses Cloud Router to establish a BGP session between the VPC and the on-premises VPN gateway, which must support BGP. The new subnets are then seamlessly advertised between networks. This means that instances in the new subnets can start sending and receiving traffic immediately, as you will explore in the upcoming lab. To set up BGP, an additional IP address has to be assigned to each end of the VPN tunnel. These two IP addresses must be link-local IP addresses, belonging to the IP address range 169.254.0.0/16. These addresses are not part of IP address space of either network and are used exclusively for establishing a BGP session.

### Document - [HA VPN](https://www.cloudskillsboost.google/course_templates/178/documents/470284)

### Video - [Lab Intro: Configuring Google Cloud HA VPN](https://www.cloudskillsboost.google/course_templates/178/video/470285)

- [YouTube: Lab Intro: Configuring Google Cloud HA VPN](https://www.youtube.com/watch?v=pou3FC-2Vc8)

Let's apply what we just covered. In this lab you create a global VPC called vpc-demo, with two custom subnets in us-east1 and us-central1. In this VPC, you add a Compute Engine instance in each region. You then create a second VPC called on-prem to simulate a customer's on-premises data center. In this second VPC, you add a subnet in region us-central1 and a Compute Engine instance running in this region. Finally, you add an HA VPN and a cloud router in each VPC and run two tunnels from each HA VPN gateway before testing the configuration to verify the 99.99% SLA.

### Lab - [Configuring Google Cloud HA VPN](https://www.cloudskillsboost.google/course_templates/178/labs/470286)

Set up two VPCs and add a cloud HA-VPN gateway in each, then run two tunnels from each VPN gateway to demonstrate the HA-VPN gateway configuration for 99.99% SLA.

- [ ] [Configuring Google Cloud HA VPN](../labs/Configuring-Google-Cloud-HA-VPN.md)

### Video - [Cloud Interconnect and Peering](https://www.cloudskillsboost.google/course_templates/178/video/470287)

- [YouTube: Cloud Interconnect and Peering](https://www.youtube.com/watch?v=IlIEUeSfyfY)

Next, let's talk about the Cloud Interconnect and Peering services. There are different Cloud Interconnect and Peering services available to connect your infrastructure to Google's network. These services can be split into Dedicated versus Shared connections and Layer 2 versus Layer 3 connections. The services are Direct Peering, Carrier Peering, Dedicated Interconnect, and Partner Interconnect, which supports both layer 2 and layer 3. Dedicated connections provide a direct connection to Google's network, but shared connections provide a connection to Google's network through a partner. Layer 2 connections use a VLAN that pipes directly into your GCP environment, providing connectivity to internal IP addresses in the RFC 1918 address space. Layer 3 connections provide access to Google Workspace services, YouTube, and Google Cloud APIs using public IP addresses. Now, as explained earlier, Google also offers its own Virtual Private Network service, called Cloud VPN. This service uses the public internet, but traffic is encrypted and provides access to internal IP addresses. That's why Cloud VPN is a useful addition to Direct Peering and Carrier Peering. Let's explain the Cloud Interconnect and Peering services separately first before discussing guidance on choosing the right connection.

### Video - [Cloud Interconnect](https://www.cloudskillsboost.google/course_templates/178/video/470288)

- [YouTube: Cloud Interconnect](https://www.youtube.com/watch?v=vWtRYP8PMgk)

Dedicated Interconnect provides direct physical connections between your on-premises network and Google's network. This enables you to transfer large amounts of data between networks, which can be more cost-effective than purchasing additional bandwidth over the public internet. In order to use Dedicated Interconnect, you need to provision a cross connect between the Google network and your own router in a common colocation facility, as shown in this diagram. To exchange routes between the networks, you configure a BGP session over the interconnect between the Cloud Router and the on-premises router. This will allow user traffic from the on-premises network to reach GCP resources on the VPC network, and vice versa. Dedicated Interconnect can be configured to offer a 99.9% or a 99.99% uptime SLA. See the Dedicated Interconnect documentation for details on how to achieve these SLAs [https://cloud.google.com/interconnect/docs/concepts/dedicated-overview#redundancy]. In order to use Dedicated Interconnect, your network must physically meet Google's network in a supported colocation facility. This map shows the locations where you can create dedicated connections. For a full list of these locations, see the links section of this video: [https://cloud.google.com/interconnect/docs/concepts/colocation-facilities]. Now, you might look at this map and say, "well I am nowhere near one of those locations." That's when you want to consider Partner Interconnect. Partner Interconnect provides connectivity between your on-premises network and your VPC network through a supported service provider. This is useful if your data center is in a physical location that cannot reach a Dedicated Interconnect colocation facility or if your data needs don't warrant a Dedicated Interconnect. In order to use Partner Interconnect, you work with a supported service provider to connect your VPC and on-premises networks. For a full list of providers, see the links section of this video: [https://cloud.google.com/interconnect/docs/concepts/service-providers] These service providers have existing physical connections to Google's network that they make available for their customers to use. After you establish connectivity with a service provider, you can request a Partner Interconnect connection from your service provider. Then, you establish a BGP session between your Cloud Router and on-premises router to start passing traffic between your networks via the service provider's network. Partner Interconnect can be configured to offer a 99.9% or a 99.99% uptime SLA between Google and the service provider. See the Partner Interconnect documentation for details on how to achieve these SLAs [https://cloud.google.com/interconnect/docs/concepts/partner-overview#redundancy]. Cross-Cloud Interconnect helps you to establish high-bandwidth dedicated connectivity between Google Cloud and another cloud service provider. When you buy Cross-Cloud Interconnect, Google provisions a dedicated physical connection between the Google network and that of another cloud service provider. You can use this connection to peer your Google Virtual Private Cloud (VPC) network with your network that's hosted by a supported cloud service provider. Google currently supports Amazon Web Services, Microsoft Azure, Oracle Cloud Infrastructure, and Alibaba Cloud for use with Cross-Cloud Interconnect. Cross-Cloud Interconnect supports the adoption of an integrated multi cloud strategy and offers reduced complexity, site-to-site data transfer, and encryption. Cross-Cloud Interconnect connections are available in two sizes: 10 Gbps or 100 Gbps. First, you identify supported locations where you want Google to place your connections. Then you purchase primary and redundant Cross-Cloud Interconnect ports. You also buy primary and redundant ports from your cloud service provider. After provisioning the connection, Google supports the connection up to the point where it reaches the network of your other cloud service provider. Google does not guarantee uptime from the other cloud service provider and cannot create a support ticket on your behalf. Let me compare the interconnect options that we just discussed. All of these options provide internal IP address access between resources in your on-premises network and in your VPC network. The main differences are the connection capacity and the requirements for using a service. The IPsec VPN tunnels that Cloud VPN offers have a capacity of 1.5 to 3 Gbps per tunnel and require a VPN device on your on-premises network. The 1.5-Gbps capacity applies to traffic that traverses the public internet, and the 3-Gbps capacity applies to traffic that is traversing a direct peering link. You can configure multiple tunnels if you want to scale this capacity. Dedicated Interconnect has a capacity of 10 Gbps or 100 Gbps per link and requires you to have a connection in a Google-supported colocation facility. You can have up to 8 links to achieve multiples of 10 Gbps, or up to 2 links to achieve multiples of 100 Gbps, but 10 Gbps is the minimum capacity. Partner Interconnect has a capacity of 50 Mbps to 50 Gbps per connection, and requirements depend on the service provider. Cross-Cloud Interconnect enables you to establish high-bandwidth dedicated connectivity between Google Cloud and another cloud service provider. Cross-Cloud Interconnect connections are available in two sizes: 10 Gbps or 100 Gbps. If you need a lower cost solution, have lower bandwidth needs, or you are experimenting with migrating your workloads to Google Cloud, you can choose Cloud VPN. If you need an enterprise-grade connection to Google Cloud that has higher throughput, you can choose Dedicated Interconnect or Partner Interconnect. If you need to connect to another cloud service provider, choose Cross-Cloud Interconnect. Google recommends using Cloud Interconnect instead of Direct Peering and Carrier Peering, which you would only use in certain circumstances.

### Video - [Peering](https://www.cloudskillsboost.google/course_templates/178/video/470289)

- [YouTube: Peering](https://www.youtube.com/watch?v=dN1vbXinNaY)

Person: Let's talk about the Cloud Peering services, which are direct peering and carrier peering. These services are useful when you require access to Google and Google Cloud properties. Google allows you to establish a direct peering connection between your business network and Google's. With this connection, you will be able to exchange Internet traffic between your network and Google's at one of the Google's broad-reaching edge network locations. Direct peering with Google is done by exchanging BGP routes between Google and the peering entity. After a direct peering connection is in place, you can use it to reach all the Google services, including the full suite of Google Cloud platform products. Unlike dedicated interconnect, direct peering does not have an SLA. In order to use direct peering you need to satisfy the peering requirements in the link section of this video. GCP's Edge Points of Presence, or PoPs, are where Google's network connects to the rest of the Internet via peering. PoPs are present at over 90 Internet exchanges and at over 100 interconnection facilities around the world. For more information about these exchange points and facilities I recommend looking at Google's peering DB entries which are linked below this video. If you look at this map and say, "Hey, I am nowhere near one of these locations," you will want to consider carrier peering. If you require access to Google public infrastructure and cannot satisfy Goggle's peering requirements, you can connect via a carrier peering partner. Work directly with your service provider to get the connection you need and to understand the partner's requirements. For a full list of available service providers, see the link section of this video. Now, just like direct peering, carrier peering also does not have an SLA. Let me compare the peering options that we just discussed. All of these options provide public IP address access to all of Google's services. The main differences are capacity and the requirements for using a service. Direct peering has a capacity of 10 Gbps per link and requires you to have a connection in a GCP Edge Point of Presence. Carrier peering's capacity and requirements vary depending on the service provider that you work with.

### Video - [Choosing a connection](https://www.cloudskillsboost.google/course_templates/178/video/470290)

- [YouTube: Choosing a connection](https://www.youtube.com/watch?v=3-vAztO2XFU)

Now that we've discussed all the different connection services, let me help you determine which service best meets your hybrid connectivity needs. We started this lesson by introducing the 5 different ways to connect your infrastructure to Google Cloud. We split these services into Dedicated versus Shared connections and Layer 2 versus Layer 3 connections. Another way to organize these services is by Interconnect services and by Peering services. Interconnect services provide direct access to RFC1918 IP addresses in your VPC, with an SLA. Peering services, in contrast, offer access to Google public IP addresses only, without an SLA. Another way to choose the right service that meets your needs is with a flow diagram. Let me walk you through three diagrams, using the assumption that you want to extend your infrastructure to the cloud. Ask yourself whether you need to extend your network for Workspace services, YouTube, or Google Cloud APIs. If you do, choose one of the Peering services. If you can meet Google's Direct Peering requirements, choose Direct Peering; otherwise, choose Carrier Peering. If you don't need to extend your network for Workspace services or Google Cloud APIs but want to connect your Google Cloud network with other cloud services, Cross Cloud Interconnect is the option for you. Cross Cloud Connect is a Google-managed routing solution that is an ideal choice for workloads that need high bandwidth and need a Google-managed routing solution. If you're looking to simply connect Google Cloud VPC with another cloud service and have no high bandwidth requirements and want to keep your encryption managed by Google, choose Cloud VPN. If you want to extend the reach of your network to Google Cloud, you want to pick one of the Interconnect services. Start with colocation facilities. If you cannot meet Google at one of its colocation facilities, choose Cloud VPN or Partner Interconnect. This choice will depend on your bandwidth and encryption requirements, along with the purpose of the connection. Specifically, if you have modest bandwidth needs, will use the connection for short durations and trials, and require an encrypted channel, choose Cloud VPN. Otherwise, choose Partner Interconnect. Within Partner Interconnect, you can choose between L2 Partner Interconnect and L3 Partner Interconnect. Choose L2 Partner Interconnect, if you need BGP peering and L3 Partner Interconnect if you don't. If you can meet Google at one of its colocation facilities, you might jump to Dedicated Interconnect. However, if you cannot provide your own encryption mechanisms for sensitive traffic, feel that a 10 Gbps connection is too large, or want access to multiple cloud services, you want to consider Cloud VPN or Partner Interconnect instead. Google now supports VPN over Interconnect options. After you have made your decision on the Interconnect options, you can choose whether or not you want Google-managed encryption. If you want Google-managed encryption, choose the Cloud VPN over Interconnect option.

### Video - [Shared VPC and VPC Peering](https://www.cloudskillsboost.google/course_templates/178/video/470291)

- [YouTube: Shared VPC and VPC Peering](https://www.youtube.com/watch?v=pKBtuvt7fiY)

Let's move our attention from hybrid connectivity to sharing VPC networks. In the simplest cloud environment, a single project might have one VPC network, spanning many regions, with VM instances hosting very large and complicated applications. However, many organizations commonly deploy multiple, isolated projects with multiple VPC networks and subnets. In this lesson, we are going to cover two configurations for sharing VPC networks across GCP projects. First, we will go over shared VPC, which allows you to share a network across several projects in your GCP organization. Then, we will go over VPC Network Peering, which allows you to configure private communication across projects in the same or different organizations. Shared VPC allows an organization to connect resources from multiple projects to a common Virtual Private Cloud (VPC) network so that they can communicate with each other securely and efficiently by using internal IP addresses from that network. When you use Shared VPC, you designate a project as a host project and attach one or more other service projects to it. The VPC networks in the host project are called Shared VPC networks. Eligible resources from service projects can use subnets in the Shared VPC network. Shared VPC lets organization administrators delegate administrative responsibilities, such as creating and managing instances, to Service Project Admins while maintaining centralized control over network resources like subnets, routes, and firewalls. A project that participates in Shared VPC is either a host project or a service project. A project that does not participate in Shared VPC is called a standalone project. This emphasizes that it is neither a host project nor a service project. A standalone VPC network is an unshared VPC network that exists in either a standalone project or a service project. VPC Network Peering, in contrast, allows private RFC 1918 connectivity across two VPC networks, regardless of whether they belong to the same project or the same organization. Now, remember that each VPC network will have firewall rules that define what traffic is allowed or denied between the networks. For example, in this diagram there are two organizations that represent a consumer and a producer, respectively. Each organization has its own organization node, VPC network, VM instances, Network Admin, and Instance Admin. In order for VPC Network Peering to be established successfully, the Producer Network Admin needs to peer the Producer Network with the Consumer Network, and the Consumer Network Admin needs to peer the Consumer Network with the Producer Network. When both peering connections are created, the VPC Network Peering session becomes Active and routes are exchanged. This allows the virtual machine instances to communicate privately using their internal IP addresses. VPC Network Peering is a decentralized or distributed approach to multi-project networking, because each VPC network may remain under the control of separate administrator groups and maintains its own global firewall and routing tables. Historically, such projects would consider external IP addresses or VPNs to facilitate private communication between VPC networks. However, VPC Network Peering does not incur the network latency, security, and cost drawbacks that are present when using external IP addresses or VPNs. Now that we've talked about Shared VPC and VPC Network Peering, let me compare both of these configurations to help you decide which is appropriate for a given situation. If you want to configure private communication between VPC networks in different organizations, you have to use VPC Network Peering. Shared VPC only works within the same organization. Somewhat similarly, if you want to configure private communication between VPC networks in the same project, you have to use VPC Network Peering. This doesn't mean that the networks need to be in the same project, but they can be. Shared VPC only works across projects. In my opinion, the biggest difference between the two configurations is the network administration models. Shared VPC is a centralized approach to multi-project networking, because security and network policy occurs in a single designated VPC network. In contrast, VPC Network Peering is a decentralized approach, because each VPC network can remain under the control of separate administrator groups and maintains its own global firewall and routing tables.

### Quiz - [Quiz: Interconnecting Networks](https://www.cloudskillsboost.google/course_templates/178/quizzes/470292)

#### Quiz 1.

> [!important]
> **If you cannot meet Google's peering requirements, which network connection service should you choose to connect to Google Workspace and YouTube?**
>
> - [ ] Direct Peering
> - [ ] Partner Interconnect
> - [ ] Dedicated Interconnect
> - [ ] Carrier Peering

#### Quiz 2.

> [!important]
> **What is the purpose of Virtual Private Networking (VPN)?**
>
> - [ ] It is a method to detect intruders at the edge of a network boundary.
> - [ ] The main purpose is to encrypt data so that it can be stored in an encrypted format.
> - [ ] To enable a secure communication method (a tunnel) to connect two trusted environments through an untrusted environment, such as the Internet.
> - [ ] VPNs are also called access control lists, or ACLs, and they limit network access.

#### Quiz 3.

> [!important]
> **Which Google Cloud Interconnect service requires a connection in a Google Cloud colocation facility and provides 10 Gbps per link?**
>
> - [ ] Carrier Peering
> - [ ] Dedicated Interconnect
> - [ ] Cloud VPN
> - [ ] Partner Interconnect
> - [ ] Direct Peering

#### Quiz 4.

> [!important]
> **Which of the following approaches to multi-project networking, uses a centralized network administration model?**
>
> - [ ] Shared VPC
> - [ ] VPC Network Peering
> - [ ] Cloud VPN

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/178/video/470293)

- [YouTube: Module Review](https://www.youtube.com/watch?v=T2WFJmoWSV0)

person: In this module, we looked at five different ways of connecting your infrastructure to GCP, which are dedicated interconnect, partner interconnect, Cloud VPN, direct peering, and carrier peering. I also gave you some guidance on how to choose between different services. Remember, you might start out using one service, and as your requirements change, or new core location facilities open, you switch to a different service. I also gave you a brief overview of shared VPC and VPC network peering, which are two configurations for sharing VPC networks across GCP projects.

## Load Balancing and Autoscaling

Configure load balancers and autoscaling for VM instances

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/178/video/470294)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=K57H2ESAZ1s)

In this module, we focus on Load Balancing and Autoscaling. Cloud Load Balancing gives you the ability to distribute load-balanced compute resources in single or multiple regions to meet your high availability requirements, to put your resources behind a single anycast IP address, and to scale your resources up or down with intelligent autoscaling. Using Cloud Load Balancing, you can serve content as close as possible to your users on a system that can respond to over 1 million queries per second. Cloud Load Balancing is a fully distributed, software-defined, managed service. It isn't instance- or device-based, so you don't need to manage a physical load balancing infrastructure. Google Cloud offers different types of load balancers that can be divided into two categories: global and regional. The global load balancers are the HTTP(S), SSL proxy, and TCP proxy load balancers. These load balancers leverage the Google frontends, which are software-defined, distributed systems that sit in Google's points of presence and are distributed globally. Therefore, you want to use a global load balancer when your users and instances are distributed globally, your users need access to the same applications and content, and you want to provide access using a single anycast IP address. The regional load balancers are external and internal HTTP(S), and TCP Proxy. There are also internal TCP/UDP, and external TCP/UDP Network load balancers. The internal and network load balancers distribute traffic to instances that are in a single Google Cloud region. The internal load balancer uses Andromeda, which is Google Cloud's software-defined network virtualization stack, and the network load balancer uses Maglev, which is a large, distributed software system. The internal load balancer for HTTP(S) traffic is a proxy-based, regional Layer 7 load balancer that enables you to run and scale your services behind a private load balancing IP address that is accessible only in the load balancer's region in your VPC network. In this module, we cover the different types of load balancers that are available in Google Cloud. We also go over managed instance groups and their autoscaling configurations, which can be used by these load balancing configurations. You explore many of the covered features and services throughout the two labs of this module. The module wraps things up by helping you determine which Google Cloud load balancer best meets your needs. Let's start by talking about managed instance groups.

### Video - [Managed instance groups](https://www.cloudskillsboost.google/course_templates/178/video/470295)

- [YouTube: Managed instance groups](https://www.youtube.com/watch?v=3Xs3_us5mNU)

Person: A managed instance group is a collection of identical VM instances that you control as a single entity using an instance template. You can easily update all the instances in a group by specifying a new template in a rolling update. Also when your applications require additional compute resources, managed instance groups can scale automatically to the number of instances in the group. Managed instance groups can work with load balancing services to distributor network traffic to all of the instances in the group. If an instance in the group stops, crashes or is deleted by an action other than the instance group commands, the managed instance group automatically recreates the instance so it can resume its processing tasks. The recreated instance uses the same name and the same instance template as the previous instance. Managed instance groups can automatically identify and recreate unhealthy instances in a group to ensure that all instances are running optimally. Regional managed instance groups are generally recommended over zonal managed instance groups because they allow you to spread the application load across multiple zones instead of confining your application to a single zone or having you manage multiple instance groups across different zones. This replication protects against zonal failures and unforeseen scenarios where an entire group of instances in a single zone malfunctions. If that happens, your application can continue serving traffic from instances running in another zone in the same region. In order to create a managed instance group, you first need to create a instance template. Next, you're going to create a managed instance group of N specified instances. The instance group manager then automatically populates the instance group based on the instance template. You can easily create instance templates using the cloud console. The instance template dialogue looks and works exactly like creating an instance, except that the choices are recorded so they can be repeated. When you create an instance group, you define the specific rules for that instance group. First, you decide what type of managed instance group you want to create. You can use managed instance groups for stateless serving or batch workloads. such as website front end or image processing from a queue, or for stateful applications. such as databases or legacy applications. Second, provide a name for the instance group. Third, decide whether the instance group is going to be single or multizoned and where those locations will be. You can optionally provide port name mapping details. Fourth, select the instance template that you want to use. Fifth, decide whether you want to autoscale and under what circumstances. Finally, consider creating a health check to determine which instances are healthy and should receive traffic. Essentially, you're creating virtual machines, but you're applying more rules to that instance group.

### Video - [Autoscaling and health checks](https://www.cloudskillsboost.google/course_templates/178/video/470296)

- [YouTube: Autoscaling and health checks](https://www.youtube.com/watch?v=Y6qPLCO2hFM)

Let me provide more details on the autoscaling and health checks of a managed instance group. As I mentioned earlier, managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. You just define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load. Applicable autoscaling policies include scaling based on CPU utilization, load balancing capacity, or monitoring metrics, or by a queue-based workload like Pub/Sub or schedule such as start-time, duration and recurrence. For example, let's assume you have 2 instances that are at 100% and 85% CPU utilization as shown on this slide. If your target CPU utilization is 75%, the autoscaler will add another instance to spread out the CPU load and stay below the 75% target CPU utilization. Similarly, if the overall load is much lower than the target, the autoscaler will remove instances as long as that keeps the overall utilization below the target. Now, you might ask yourself how do I monitor the utilization of my instance group. When you click on an instance group (or even an individual VM), you can choose to view different metrics. By default you'll see the CPU utilization over the past hour, but you can change the time frame and visualize other metrics like disk and network usage. These graphs are very useful for monitoring your instances' utilization and for determining how best to configure your Autoscaling policy to meet changing demand. If you monitor the utilization of your VM instances in Cloud Monitoring, you can even set up alerts through several notification channels. A link to more information on autoscaling can be found in the Course Resources for this module. Another important configuration for a managed instance group and load balancer is a health check. A health check is very similar to an uptime check in Cloud Monitoring. You just define a protocol, port, and health criteria, as shown in this screenshot. Based on this configuration, Google Cloud computes a health state for each instance. The health criteria define how often to check whether an instance is healthy (that's the check interval); how long to wait for a response (that's the timeout); how many successful attempts are decisive (that's the healthy threshold); and how many failed attempts are decisive (that's the unhealthy threshold). In the example on this slide, the health check would have to fail twice over a total of 15 seconds before an instance is considered unhealthy. Configuring stateful IP addresses in a managed instance group ensures that applications continue to function seamlessly during autohealing, update, and recreation events. Both internal and external IPv4 addresses can be preserved. You can configure IP addresses to be assigned automatically or assign specific IP addresses to each VM instance in a managed instance group. Preserving an instance's IP addresses is useful in many different scenarios. Your application requires an IP address to remain static after it has been assigned. Your application's configuration depends on specific IP addresses. Users, including other applications, access a server through a dedicated static IP address. You need to migrate existing workloads without changing network configuration. You can do the following operations by configuring stateful policy on an existing managed instance group: Configure IP addresses as stateful for all existing and future instances in the group. This will promote the corresponding ephemeral IP addresses of all existing instances to static IP addresses. And update the existing stateful configuration for IP addresses.

### Video - [Overview of HTTP(S) load balancing](https://www.cloudskillsboost.google/course_templates/178/video/470297)

- [YouTube: Overview of HTTP(S) load balancing](https://www.youtube.com/watch?v=ai6vf9gdhCs)

Person: Now let's talk about HTTP(S) load balancing, which acts at Layer 7 of the OSI model. This is the application layer, which deals with the actual content of each message, allowing for routing decisions based on the URL. GCP's HTTP(S) load balancing provides global load balancing for HTTP(S) requests destined for your instances. This means that your applications are available to your customers at a single anycast IP address, which simplifies your DNS setup. HTTP(S) load balancing balances HTTP and HTTPS traffic across multiple back-end instances and across multiple regions. HTTP requests are load balanced on port 80 or 8080, and HTTPS requests are load balanced on port 443. This load balancer supports both IPv4 and IPv6 clients, is scalable, requires no pre-warming, and enables content-based and cross-regional load balancing. You can configure URL maps that route some URLs to one set of instances and route other URLs to other instances. Requests are generally routed to the instance group that is closest to the user. If the closest instance group does not have sufficient capacity, the request is sent to the next closest instance group that does have the capacity. You will get to explore most of these benefits in the first lab of the module. Let me walk through the complete architecture of an HTTPS load balancer by using this diagram. A global forwarding rule directs incoming requests from the Internet to a target HTTP proxy. The target HTTP proxy checks each request against a URL map to determine the appropriate back-end service for the request. For example, you can send requests for www.example.com/audio to one back-end service, which contains instances configured to deliver audio files, and the requests for www.example.com/video to another back-end service which contains instances configured to deliver video files. The back-end service directs each request to an appropriate back-end based on serving capacity, zone, and instance health of its attached backends. The back-end services contain a health check, session affinity, a timeout setting and one or more backends. A health check polls instances attached to the back-end service at configured intervals. Instances that pass the health check are allowed to receive new requests. Unhealthy instances are not sent requests until they are healthy again. Normally, HTTP(S) load balancing uses a round-robin algorithm to distribute requests among available instances. This can be overridden with session affinity. Session affinity attempts to send all requests from the same client to same virtual machine instance. Back-end services also have a timeout setting, which is set to 30 seconds by default. This is the amount of time the back-end service will wait on the backend before considering the request a failure. This is a fixed timeout, not an idle timeout. If you require longer-lived connections, set this value appropriately. The backends themselves contain an instance group, a balancing mode and a capacity scaler. An instance group contains virtual machine instances. The instance group may be a managed instance group with or without autoscaling or an unmanaged instance group. A balancing mode tells the load balancing system how to determine when the back-end is at full usage. If all the backends for the back-end service in a region are at the full usage, new requests are automatically routed to the nearest region that can still handle requests. The balancing mode can be based on CPU utilization or requests per second (RPS). A capacity setting is an additional control that interacts with the balancing mode setting. For example, if you normally want your instances to operate at a maximum of 80 percent CPU utilization, you would set your balancing mode to 80 percent CPU utilization and your capacity to 100 percent. If you want to cut instance utilization in half, you could leave the balancing mode at 80 percent CPU utilization and set capacity to 50 percent. Now any changes to your back-end services are not instantaneous, so don't be surprised if it takes several minutes for your changes to propagate throughout the network.

### Video - [Example: HTTP load balancer'](https://www.cloudskillsboost.google/course_templates/178/video/470298)

- [YouTube: Example: HTTP load balancer'](https://www.youtube.com/watch?v=vTSJ1UpNN6A)

Let me walk through an HTTP load balancer in action. The project on this slide has a single global IP address, but users enter the Google Cloud network from two different locations: one in North America and one in EMEA. First, the global forwarding rule directs incoming requests to the target HTTP proxy. The proxy checks the URL map to determine the appropriate backend service for the request. In this case, we are serving a guestbook application with only one backend service. The backend service has two backends: one in us-central1-a and one in europe-west1-d. Each of those backends consists of a managed instance group. Now, when a user request comes in, the load balancing service determines the approximate origin of the request from the source IP address. The load balancing service also knows the locations of the instances owned by the backend service, their overall capacity, and their overall current usage. Therefore, if the instances closest to the user have available capacity, the request is forwarded to that closest set of instances. In our example, traffic from the user in North America would be forwarded to the managed instance group in us-central1-a, and the traffic from the user in EMEA would be forwarded to the managed instance group in europe-west1-d. If there are several users in each region, the incoming requests to the given region are distributed evenly across all available backend services and instances in that region. If there are no healthy instances with available capacity in a given region, the load balancer instead sends the request to the next closest region with available capacity. Therefore, traffic from the EMEA user could be forwarded to the us-central1-a backend if the europe-west1-d backend does not have capacity or has no healthy instances as determined by the health checker. This is referred to as cross-region load balancing. Another example of an HTTP load balancer is a content-based load balancer. In this case, there are two separate backend services that handle either web or video traffic. The traffic is split by the load balancer based on the URL header as specified in the URL map. If the user is navigating to /video, the traffic is sent to the backend video service, and if the user is navigating anywhere else, the traffic is sent to the web-service backend. All of that is achieved with a single global IP address.

### Video - [HTTP(S) load balancing](https://www.cloudskillsboost.google/course_templates/178/video/470299)

- [YouTube: HTTP(S) load balancing](https://www.youtube.com/watch?v=F-BiJWsm5a8)

An HTTP(S) load balancer has the same basic structure as an HTTP load balancer, but it differs in the following ways: An HTTP(S) load balancer uses a target HTTPS proxy instead of a target HTTP proxy. An HTTP(S) load balancer requires at least once signed SSL certificate installed on the target HTTPS proxy for the load balancer. The client SSL sessions terminate at the load balancer. HTTP(S) load balancers support the QUIC transport layer protocol. QUIC is a transport layer protocol that allows for faster client connection initiation, eliminates head-of-line blocking in multiplexed streams, and supports connection migration when a client's IP address changes. For more information on the QUIC protocol, see the link in the course resources. To use HTTPS, you must create at least one SSL certificate that can be used by the target proxy for the load balancer. You can configure the target proxy with up to 15 SSL certificates. For each SSL certificate, you first create an SSL certificate resource, which contains the SSL certificate information. SSL certificate resources are used only with the load balancing proxies such as a target HTTPS proxy or target SSL proxy, which we'll discuss later in this module. Backend buckets allow you to use Google Cloud Storage buckets with HTTP(S) Load Balancing. An external HTTP(S) load balancer uses a URL map to direct traffic from specified URLs to either a backend service or a backend bucket. One common use case is: send requests for dynamic content, such as data, to a backend service; and send requests for static content, such as images, to a backend bucket. In this diagram, the load balancer sends traffic with a path of /love-to-fetch/ to a Cloud storage bucket in the europe-north region. All the other requests go to a Cloud Storage bucket in the us-east region. After you configure a load balancer with the backend buckets, requests to your URL paths to begin with /love-to-fetch/ are sent to the europe-north Cloud Storage bucket, and all other requests are sent to the us-east Cloud Storage bucket. A network endpoint group, or NEG, is a configuration object that specifies a group of backend endpoints or services. A common use case for this configuration is deploying services in containers. You can also distribute traffic in a granular fashion to applications running on your backend instances. You can use NEGs as backends for some load balancers and with Traffic Director. Zonal and internet NEGs define how endpoints should be reached, whether they are reachable, and where they are located. Unlike these NEG types, serverless NEGs don't contain endpoints. A zonal NEG contains one or more endpoints that can be Compute Engine VMs or services running on the VMs. Each endpoint is specified by either an IP address or an IP:port combination. An Internet NEG contains a single endpoint that is hosted outside of Google Cloud. This endpoint is specified by hostname FQDN:port or IP:port. A hybrid connectivity NEG points to Traffic Director services running outside of Google Cloud. A serverless NEG points to Cloud Run, App Engine, Cloud Functions services residing in the same region as the NEG. For more information on using NEGs, please see the link in course resources.

### Video - [Lab Intro: Configuring an HTTP Load Balancer with Autoscaling](https://www.cloudskillsboost.google/course_templates/178/video/470300)

- [YouTube: Lab Intro: Configuring an HTTP Load Balancer with Autoscaling](https://www.youtube.com/watch?v=hdNuX4tHrO0)

Philipp: Let's apply what we just covered. In this lab, you can configure an HTTP load balancer with autoscaling. Specifically, you create two managed instance groups that serve as backends in US-central1 and Europe-west1. Then you create and stress-test a load balancer to demonstrate global load balancing and autoscaling.

### Lab - [Configure an Application Load Balancer with Autoscaling](https://www.cloudskillsboost.google/course_templates/178/labs/470301)

In this lab, you configure an Application Load Balancer (HTTP). Then, you stress test the Load Balancer to demonstrate global load balancing and autoscaling.

- [ ] [Configure an Application Load Balancer with Autoscaling](../labs/Configure-an-Application-Load-Balancer-with-Autoscaling.md)

### Video - [Lab Review: Configuring an HTTP Load Balancer with Autoscaling](https://www.cloudskillsboost.google/course_templates/178/video/470302)

- [YouTube: Lab Review: Configuring an HTTP Load Balancer with Autoscaling](https://www.youtube.com/watch?v=WojfW6KZUpw)

In this lab, you configured an HTTP load balancer with backends in us-central1 and europe-west1. Then you stress-tested the load balancer with a VM to demonstrate global load balancing and autoscaling. You can stay for a lab walk-through, but remember, that the GCP's user interface can change, so your environment might look slightly different. All right. So, here we are in the GCP Console and the first thing I'm going to do is configure the HTTP and health check firewall rules. So let me go ahead and do that by navigating to VPC network and specifically firewall rules. So you'll notice that there are already some firewall rules here for ICMP internal RDP and SSH traffic. These are the ones that always come with the default network, and we're now going to create a firewall rule to allow HTTP. So let me click, create firewall rule, and I'll provide it a name. It's going to be for the default network. I'm going to specify the target tags by using HTTP server, and then we'll have to define that target tag on our instances later. The source, I'm going to set to IP ranges and just set from anywhere, and then I can specify the TCP port to 80. That's for HTTP, and then we can click create, and I'm going to create a similar firewall rule for our health checkers. So I can do that while this rule is being created, and again, on the same network, I'm going to use the same target tags. So it just applies to instances that have that tag, and now for the IP ranges, I'm going to be a lot more specific, and these are provided in the lab instructions for you, but these are the IP ranges of the health checker. Now when you enter those, make sure you enter one first. You can click space and you can see that, sort of, has acknowledged that. Then you can copy and paste the other IP range and then click away and you can see that it's got that as well. Now for the protocol supports, in this case, we're just going to specify all of TCP, but you could narrow that down a little bit depending on what kind of health check you're doing. So let me click create on that, and while these are being created, I can now create my custom image. So I'm going to go to Compute Engine, and we're going to create a VM. We're going to call that Web Server. So let me go ahead and do that. I can leave the region as us-central1, zone us-central1-a, and I'm going to now expand this option down here, management security disk network and sole tenancy. A couple things I want to do, first, under Disks, I want to make sure that this disk is not deleted when the instance is deleted, and that works because these are just persistent disks, they're just network attached, and on our networking, I'm going to define the network tag, HTTP sever, and this is going to be for our default network. So that way the firewalls that we just created are going to be plied to this instance. So let me go ahead and click Create, and once this instance is up and running, we're going to customize it by installing some software. So I'm going to just wait for the instance to be created. There it is. I can click on SSH, and I'm going to just run the commands that are in the lab instructions. So first, I'm just going to install Apache 2, and then I'm going to start the Apache server after that. We're going to double check that server by navigating to the external IP address that we have here, and that is why we attached that firewall rule for the external IP. We don't really need the firewall rule for the health checker yet, that is going to be later for our backend instances, and we haven't really configured that health check yet anyway. So here we are. It's still connecting, so let's just give it a couple of seconds. There we are. I'm going to paste those two commands in there, and let that run, and then I'm going to start the service. So let me now go back to the console and click on external IP. Here, we can see the Apache 2 default page. So we see that this has worked. Now I want to set that service to start on boot. So there's a command for that. So let me go back to my SSH terminal and paste in that command, and now I'm going to go back to Compute Engine, and for the web sever, I'm going to select Reset, and, yep, I want to make sure I do that. So I'm going to click Reset on that confirmation. So it's now going to stop and reboot the machine. I'll keep the same IP addresses and the same persistent boot disk, but the memory is essentially wiped, so therefore, the Apache service should be available after the reset and the update RC command should have been successful. So we can wait for that. We have two options of checking that status. We could navigate to the external IP address, or once it's back up, we could SSH back to the instance and just run a command to check the status, and it's telling me that the Apache service is actually running. So let's now prepare the disk and we'll create a custom image from that disk. So first, let's get out of the SSH session, and let's verify one more time that the instance that we have here has a disk associated, that that disk is not deleted when I delete the instance. I can verify that by just clicking on the name of the instance, and then I'm going to scroll down to where it talks about my boot disk. Here it is. And under 'When deleting instance', it says, keep disk, and if that was not the case, I could click edit and I could change that behavior. In our case, it's all good, so I'm going to go ahead and delete the instance. Here, it's asking me would you also want to delete that disk? Which, in our case, we're not going to do. So we are going to delete the instance, and if I go over here to disks, we can see that here, we have the disk itself. Now, we can go back to instances. We could wait for this to be deleted but the disk will remain. So really, what we can do now is get on and create an image. So I'm going to click on the Images section, and here we have the images that are available. I'm going to create my custom image, give it a name, mywebserver we're going to use as a source disk, but you can see there are lots of other options, like a snapshot, you could even do it from another disk or a Cloud Storage file. So we're going to do that from the disk. We only have one disk available, so let's choose that. We can keep all the other settings, like the encryption, the location, and I can click Create. So this is now going to create an image from that disk. And at this point, we could even delete the disk itself once that image has been created, because we're actually being charged for the disk while it's there, but for the purposes of the lab, we can leave that, as all your resources are being cleaned up in every quick labs project that you're using. So let's go ahead and configure the instance tablet and create the instance groups. So I'm going to go to Compute Engine, and I'm going to go to Instance templates, and we're going to create new instance template, and I'm going to give it a name, mywebserver-template. We're going to change the machine type to a micro. We're just doing some very small prototyping here, and now the important thing is I need to change the boot disk to select my custom image. So I'm going to change that, go to Custom images, and here, I have my web server image from this project. If you have access to other projects, you could also grab an image from there. It's all set. I can choose a size as well as the type of disk. We're just going to leave those and click Select. Now, I also need to make sure that I have the right network tags. So let me go and expand the management security and disk options. By the way, you can see that this whole instance template UI is very similar to the VM instance template, because all you're doing is you're just defining rules for the VM instances, and once you can group some of those, it will just use all those settings. So under here, I'm going to go to networking, and pick the network, and I can then make sure that I have the default network and then I want to make sure that I have my network tags so that the firewall rules that we created at the beginning are going to be applied to all the instances created from this template. So let's go click Create. That really shouldn't take long. It's just going to create a template, not create any instances yet, and sometimes if I'm a little impatient I'll just click Refresh, and we see we have everything here. So now I can click on Instance group, and create my instance group. So I'm going to start by creating an instance group in us-central1, and this is going to be a multi-zone or a regional, across the region us-central1. I could look into the zones and, maybe, un-select certain zones or select more zones if I wanted to. This is going to be based on the template that we just created, and now the important piece is we're going to have some autoscaling. So we're going to have autoscaling on, and we're going to do that on the HTTP load balance usage. This is going to be in port 80. We want a minimum of one instance and maximum of five, and we can leave the cool-down period, and you can hover over here to see that it just waits that much time before collecting information. So we have some initialization in this instance, so you want to make sure that it at least waits those 60 seconds before it starts looking into that, and then we can also go to Health check. We don't have one yet, so we can go Create a health check, and we can just call it the http-health-check, protocol, we could use HTTP or leave it as TCP 80, and this is now -- What it's going to do is it's going check every 10 seconds. It's going to wait 5 seconds in between and if there are two consecutive successes, it's successful. Three consecutive failures means it's a failure and it means it's unhealthy instance. So let me click Save and continue on that. There's this initial delay here. This is for the boot, so we're going to set that to 60 seconds. That's for the health check, and them I'm going to click Create. Now, it's telling me that, well, the autoscaling isn't really complete yet, because we haven't set up the HTTP load balancing. That's okay. We're about to do that. So let's just click okay, and we're going to repeat the same now for our instance group in europe-west1. So let me grab that name. It's also going to be a multi-zone, obviously, in this case, the region is europe-west1, same instance template, autoscaling, also based on HTTP, port 80, minimum one, maximum five, cool down, and now we can just select the health check. It seems like it doesn't have that health check yet. That could actually happen if you just go into those too fast. So let's actually click Cancel. Let's go back. Let's see if this instance group has been created. Let's try that one more time and see if we can get that health check, and there it is, okay? So we're just a little bit too fast, so that could certainly happen. So let me back track, put my information back in here, multiple zones, europe-west1, my template. I don't need to create one, I want to just select that HTTP, maximum of five, and set that initial delay, again, to 60. We don't want to wait this long for the lab, and then we're going to click Create, and it's again giving us the same warning that we just saw. So we can just click Okay. So here, we can see the creation of this instance group. We can also go to VM instances, and we'll see that one of the instance groups has already created an instance, so you can see it starts off with that name of the instance group, and MIG, by the way, it's what I put in here. That's short for Maddish Instance Group. We can see the scaling happening here. This one already has one instance. This is scaling from zero to one, and you can actually click in here and get a ton more information. If I go to monitoring, you'll see CPU usage, details, members, it will show us that it's scaling and how many it has. So you can get a lot of information by either going into the instance group page or the VM instances. So either way, we have at least once instance in each of the groups. So we are ready to now configure the backend. So let's just verify these actually. We can go to the navigation menu in VM instance. We're already here, and we could look into these IP addresses. I can click on both of these and we'll see that both of them have the default page up, so that proves that the custom image that we created earlier is actually being leveraged here. So we installed all that custom software and our backend now has that. So let's configure the HTTP load balancer. We're going to go to the navigation menu, network services, load balancing, create load balancer. This is going to be an HTTP load balancer, so we'll start that. I can choose if it's Internet facing or internal only, so from Internet to my VMs, yep. Click Continue. I can give it a name, HTTP load balancer, and I'll start by configuring the backend. I want to create a backend service. I'm going to give it a name, and I'm going to select the instance groups. So let's start first with us-central1, port number 80. The balancing mode is going to be rate, maximum of 50 requests per second, capacity 100, so just following the lab instructions here. So it just means that that load balancer tends to keep each of the instances that will happen there at or below 50 requests per second. So I can click Done and add another backend, which is just the only other one left, and let's here, for example, utilization at a CPU utilization rate of 80 and a capacity of 100. So that's just going to mean that this configuration means that a load balance attempts to keep each instance of europe-west1, at or below 80 percent CPU utilization, and I can also attach the same health check here, and then click Create. I could configure host and path rules that could define that certain traffic is being sent to other backends depending on the URL of the traffic, so video service could be sent to, maybe, a video backend versus static content to a static backend. We're not leveraging that here, so let's move on to the frontend configuration. I could give it a name, but really, I just need to specify the protocol, the IP version, let's just keep it Ephemeral, port 80, click Done, and we can review and finalize. So here we have our backend, our instances, I should say our instance groups, as well as our frontend. I could also add, if I go back here, another frontend. I have HTTP. We could also also add IPv6. So let's do that, and then we finalize, so now we have two frontends, and we'll get two IP address. So let's go ahead and create that, and once that is up and running, we should be seeing two addresses, and the one in hexadecimal format is going to be our IPv6 address and you're only going to be able to navigate to that if your connection actually allows it from where you are. Cell phones, for example, very often use IPv6. So you could maybe try to plug in the address on your cell phone and see if your able to access those backends. So let's wait for that to load up. So I click on my load balancer. It's just a frontend -- It just isn't ready yet. I went into here a bit fast, so refresh and just wait for that service to be ready, and then we can go in and get some more information about it. So here I am. The load balancer is now set up. It only took, actually, a couple more seconds. So here, we can see the IP addresses, again, this is the IPv4. This is the IPv6. So the first thing I could do is I could actually just navigate to those using my browser because I did allow HTTP traffic from anywhere. So let me just plug that into my browser, and first navigate to the IPv4, and I'm actually getting a 404 error, and the manual does talk about that. So let me also open another tab and type in the IPv6 address, and run that, and it says it hasn't found the service yet, and so the LAN manual does talk about the fact that you could be getting a 404 or a 502 for a while. So what you want to do here is just refresh for a while, and what you're really just doing is you're waiting for this configuration to be applied to all of the Google frontends, and so this is, again, a global load balance. This has to be applied everywhere, so the actual implementation, even though the console looks like everything is ready, the service can sometimes take some time to actually be reachable, and this can take a couple minutes to be set up. So just refresh a couple times, and let's wait for that to come up. All right. So here we are. I'm looking at the IPv4 address, so I just refreshed that a couple times and I can see the backend, which, as we know, should be the Apache 2 Debian default page, and I'm also navigating to the IPv6 address. I actually have access to that here, so that is working as well as expected. So now that we know the backend is working, it's time to stress test it. So what we're going to do is we're going to create another instance now, and just generate a ton of traffic to the load balancer, and then we're going to monitor that traffic. So let me open up another tab here because I want to be able to come back to the load balancer, and I'm going to create another instance now, by going back to Compute Engine, and Create instance. I'm going to define a name, just stress-test. I'm going to put this in a whole different region now. I'm going to select us-west1. Now, in terms of my backends, I have a backend us-central1 and a backend in europe-west1. The closest backend from this new instance I'm creating is going to be us-central1. So we would imagine that the traffic should be forwarded from us-west to us-central. That's going to be unless the load is too high, and let's see if we can actually break that and create a really high load so that we also have traffic that, sort of, spills over into the Europe region that we created. So I want to change the boot disk here. Let's actually select the custom image that we already have, that way we get a bunch of software preinstalled, and then I'm just going to go create that, and once that is up, I'm going to take the IP address of our load balancer, I'm going to store that in an environment variable. We'll verify to make sure that we have that, and then we're going to place a load on it. So let's wait for that instance to come up. With any new project, you always have a lot of information here on the right-hand side. It's useful to check out if you're new to GCP. The instance is up. Let me go SSH, and let's store the IP address. Now I need to grab that. So let me go back here. We're going to use the IPv4 address, and let me get my stress test back up and store that. Let's also verify, make sure this is stored, and here, we can see it returning, and it matches that IP address. That's great. And then let's run a command to place a load on our load balancer, okay? So this uses ApacheBench, and it's now going to benchmark this and this is now going to run the background. So now, what I can do is I can go back to my load balancer, which I'm looking at right now, and if I'm looking at it this way, I can actually directly look at the backend and click on HTTP backend, and we don't really have any traffic yet. This takes a little while to update here. We can see the two backends we have. We can see that one is scaling on rate. One is scaling on CPU utilization, and once we have a lot of traffic, we'll start showing here where that traffic is coming from and which instance it is going to. So what we want to do is just hang on here and refresh this page for a couple minutes until we can actually see some traffic being generated. So I'm actually just going to go back and go back in here, and no traffic yet, so let's just wait a minute or two and see what we can visualize here. All right. So this only took a couple seconds. So here we are. We can see that there's a lot of traffic coming from North America, that's from our stress test, and we can see it's going both to us-central1, which is the closest and that's where most of our traffic is going, most of requests, but we also have some traffic that is actually spilling over to our europe-west1 instance. So we can see that we have global load balancing here, and what we could do now is also we can monitor the backends to see if they're actually scaling. So if I go to Compute Engine, and refresh, we can already see that we have a bunch more backends now that are trying to handle all of this sudden increase in traffic and I really am stress testing this quite a lot. If I go to instance groups, we can get more information here. It's saying that it's already having issue with the amount of instances I've selected, the maximum that is five. If we go in here, into the europe-west1, we can get more details and monitoring. So it's showing us how it scaled up, how it's managing the load that's being placed on it, and we could also look in us-central1 and see that we now have up to five instances already across different zones, and I can also go into the monitoring here, and get more information. See that, you know, when we scaled up, and as we refresh this a little bit, we'll see more instances in here and I'll talk more about the capacity it has. I can come back here. Now we can see that we have -- because I really provided very minimal traffic to us-central1, just 50 requests per second, but I'm making, you know, almost 281 here. So now we have a lot of the traffic is spilling over. So this is a really good view to come back to, to always monitor your load balancer. Visually you can also use Stackdrive or locking and monitoring. You set up alerts, you could set up rules, so maybe you need to, you know, increase that max limit of five now. That's really a cost limit. You set that so that you don't exceed your cost too much, but if you're saying, "Oh my God, I need to work on this traffic," you could have more instances. Maybe you're getting an attack, actually, that at that point you could use a product called Cloud Armor to, maybe, allow and deny certain IP addresses, but this is really all we wanted to achieve for the lab.

### Video - [Cloud CDN](https://www.cloudskillsboost.google/course_templates/178/video/470303)

- [YouTube: Cloud CDN](https://www.youtube.com/watch?v=scJsKfGmspA)

Cloud CDN, or Content Delivery Network, uses Google's globally distributed edge points of presence to cache HTTP(S) load-balanced content close to your users. Specifically, content can be cached at CDN nodes as shown on this map. There are over 90 of these cache sites spread across metropolitan areas in Asia Pacific, Americas, and EMEA. For an up-to-date list, please refer to the Cloud CDN documentation. Now, why should you consider using Cloud CDN? Well, Cloud CDN caches content at the edge of Google's network providing faster delivery of content to your users while reducing serving costs. You can enable Cloud CDN with a simple checkbox when setting up the backend service of your HTTP(S) load balancer. So it's easy to enable and benefits you and your users but how does Cloud CDN do all of this? Let's walk through the Cloud CDN response flow with this diagram. In this example, the HTTP(S) load balancer has two types of backends. There are managed VM instance groups in the us-central1 and asia-east1 regions, and there is a Cloud Storage bucket in us-east1. A URL map will decide which backend to send the content to: the Cloud Storage bucket could be used to serve static content and the instance groups could handle PHP traffic. Now, when a user in San Francisco is the first to access a piece of content, the cache site in San Francisco sees that it can't fulfill the request. This is called a cache miss. The cache might attempt to get the content from a nearby cache, for example if a user in Los Angeles has already accessed the content. Otherwise, the request is forwarded to the HTTP(S) load balancer, which in turn forwards the request to one of your backends. Depending on what content is being served, the request will be forwarded to the us-central1 instance group or the us-east1 storage bucket. If the content from the backend is cacheable, the cache site in San Francisco can store it for future requests. In other words, if another user requests the same content in San Francisco, the cache site might now be able to serve that content. This shortens the round trip time and saves the origin server from having to process the request. This is called a cache hit. For more information on what content can be cached, please refer to the documentation. Now, each Cloud CDN request is automatically logged within Google Cloud. These logs will indicate a "Cache Hit" or "Cache Miss" status for each HTTP request of the load balancer. You will explore such logs in the next lab. But how do you know how Cloud CDN will cache your content? How do you control this? This is where cache modes are useful. Using cache modes, you can control the factors that determine whether or not Cloud CDN caches your content by using cache modes. Cloud CDN offers three cache modes, which define how responses are cached, whether or not Cloud CDN respects cache directives sent by the origin, and how cache TTLs are applied. The available cache modes are USE_ORIGIN_HEADERS, CACHE_ALL_STATIC and FORCE_CACHE_ALL. USE_ORIGIN_HEADERS mode requires origin responses to set valid cache directives and valid caching headers. CACHE_ALL_STATIC mode automatically caches static content that doesn't have the no-store, private, or no-cache directive. Origin responses that set valid caching directives are also cached. FORCE_CACHE_ALL mode unconditionally caches responses, overriding any cache directives set by the origin. You should make sure not to cache private, per-user content (such as dynamic HTML or API responses) if using a shared backend with this mode configured.

### Video - [SSL proxy load balancing](https://www.cloudskillsboost.google/course_templates/178/video/470304)

- [YouTube: SSL proxy load balancing](https://www.youtube.com/watch?v=HTkMB0A7gXY)

Person: Let's talk about SSL proxy and TCP proxy load balancing. SSL proxy is a global load balancing service for encrypted non-HTTP traffic. This load balancer terminates user SSL connections at the load balancing layer, then balances the connections across your instances using the SSL or TCP protocols. These instances can be in multiple regions and the load balancer automatically directs traffic to the closest region that has capacity. SSL proxy load balancing supports both IPv4 and IPv6 addresses for client traffic and provides intelligent routing, certificate management, security patching and SSL policies. Intelligent routing means that this load balancer can route requests to back-end locations where there is capacity. From a certificate management perspective, you only need to update your customer-facing certificate in one place when you need to switch those certificates. Also you can reduce the management overhead for your virtual machine instances by using self-signed certificates on your instances. In addition, if vulnerabilities arise in the SSL or TCP stack, GCP will apply patches at the load balancer automatically in order to keep your instances safe. For the full list of ports supported by SSL proxy load balancing and other benefits, please refer to the links section of this video. This network diagram illustrates SSL proxy load balancing. In this example, traffic from users in Iowa and Boston is terminated at the global load balancing layer. From there a separate connection established to the closest back-end instance. In other words the user in Boston would reach the US east region, and the user in Iowa would reach the US central region if there's enough capacity. Now the traffic between the proxy and the backend can use SSL and TCP. I recommended using SSL.

### Video - [TCP proxy load balancing](https://www.cloudskillsboost.google/course_templates/178/video/470305)

- [YouTube: TCP proxy load balancing](https://www.youtube.com/watch?v=ZgTf-WdGYwM)

Person: TCP proxy is a global load balancing service for unencrypted, non-HTTP traffic. This load balancer terminates your customer's TCP sessions at the load balancing layer then forwards the traffic to your virtual machine instances using TCP or SSL. These instances can be in multiple regions and the load balancer automatically directs traffic to the closest region that has capacity. TCP proxy load balancing supports both IPv4 and IPv6 addresses for client traffic. Similar to SSL proxy load balancer, the TCP proxy load balancer provides intelligent routing and security patching. For the full list of ports supported by TCP proxy load balancing and other benefits please refer to the links section of this video. This network diagram illustrates TCP proxy load balancing. In this example, traffic from users in Iowa and Boston is terminated at the global load balancing layer. From there a separate connection established to the closest back-end instance. As in the SSL proxy load balancing example, the users in Boston would reach the US east region and the user in Iowa would reach the US central region if there's enough capacity. Now the traffic between the proxy and the backend can use SSL or TCP, and I also recommend using SSL here.

### Video - [Network load balancing](https://www.cloudskillsboost.google/course_templates/178/video/470306)

- [YouTube: Network load balancing](https://www.youtube.com/watch?v=Te_E_gEz14g)

Person: Now let's talk about network load balancing, which is a regional load balancing service. Network load balancing is a regional, non-proxied load balancing service. In other words, all traffic is passed through the load balancer instead of being proxied, and the traffic can only be balanced between VM instances that are in the same region, unlike a global load balancer. This load balancing service uses forwarding rules to balance the load of your systems based on the incoming IP protocol data, such as address, port and protocol type. You can use it to load balance UDP traffic and to load balance TCP and SSL traffic on ports that are not supported with the TCP proxy and SSL proxy load balancers. The architecture of a network load balancer depends on whether you use a back-end service-based network load balancer or a target-pool-based network load balancer. Let's explore these in more detail. New network load balancers can be created with a regional back-end service that defines the behavior of the load balancer and how it distributes traffic across its back-end instance groups. Back-end services enable new features that are not supported with legacy target pools, such as support for non-legacy health checks, TCP, SSL, HTTP, HTTPS and HTTP/2 auto-scaling with managed instance groups, connection draining and a configurable failover policy. You can also transition an existing target-pool-based network load balancer to use a back-end service instead, but what is a target pool resource? A target pool resource defines a group of instances that receive incoming traffic from forwarding rules. When a forwarding rule directs traffic to a target pool, the load balancer picks an instance from these target pools based on a hash of the source IP and port and the destination IP and port. These target pools can only be used with forwarding rules that can handle TCP and UDP traffic. Now, each project can have up to 50 target pools, and each target pool can only have one health check. Also, all the instances of a target pool must be in the same region, which is the same limitation as for the network load balancer.

### Video - [Internal load balancing](https://www.cloudskillsboost.google/course_templates/178/video/470307)

- [YouTube: Internal load balancing](https://www.youtube.com/watch?v=AEMOqeOboMQ)

Person: Next let's talk about internal load balancing. The internal TCP/UDP load balancer is a regional private load balancing service for TCP and UDP-based traffic. In other words this load balancer enables you to run and scale your services behind a private load balancing IP address. This means that it's only accessible through internal IP addresses or virtual machine instances in the same region. Therefore you configure an internal TCP/UDP load balancing IP address to act as the front end to your private back-end instances. Because you don't need a public IP address for you load balanced service your internal client requests stay internal to your VPC network and region. This often results in lowered latency because all of your load balanced traffic stays within Google's network making your configuration much simpler. Let's talk more about the benefits of using a software-defined internal TCP/UDP load balancing service. Google Cloud internal load balancing is not based on a device or on a VM instance. Instead it is software-defined fully distributed load balancing solution. In the traditional proxy model of internal load balancing, as shown on the left, you configure an internal IP address on an load balancing device or instances, and your client instance connects to this IP address. Traffic coming to the IP address is terminated at the load balancer, and the load balancer selects a back end to establish a new connection to. Essentially you have two connections, one between the client and the load balancer and one between the load balancer and the back end. Google Cloud internal load balancing distributes client instance requests to back ends using a different approach, as shown on the right. It uses lightweight load balancing built on top of Andromeda, Google's network virtualization stack, to provide software-defined load balancing that directly delivers the traffic from the client instance to a back-end instance. For more information on Andromeda see the link section of this video. Now, Google Cloud's internal HTTPS load balancing is a proxy-based regional layer seven load balancer that also enables you to run and scale your services behind an internal load balancing IP address. Back-end services support HTTP, HTTPS and HTTP/2 protocols. Internal HTTPS load balancing is a managed service based on the open source Envoy proxy. This enables rich traffic control capabilities based on HTTPS parameters. After the load balancer has been configured it automatically allocates Envoy proxies to meet your traffic's needs. Now, internal load balancing enables you to support use cases such as the traditional three-tier web services. In this example, the web tier uses an external HTTPS load balancer that provides a single global IP address for users in San Francisco, Iowa, Singapore and so on. The back ends of this load balancer are located in US West One, US Central One and Asia East One regions because this is a global load balancer. These back ends then access an internal load balancer in each region as the application or internal tier. The back ends of this internal tier are located in US West One A, US Central One B and Asia East One B. The last tier is the data-based tier in each of these zones. The benefit of this three-tier model is that neither the data-based tier nor the application tier is exposed externally. This simplifies security and network pricing.

### Video - [Lab Intro: Configuring an Internal Load Balancer](https://www.cloudskillsboost.google/course_templates/178/video/470308)

- [YouTube: Lab Intro: Configuring an Internal Load Balancer](https://www.youtube.com/watch?v=ShUy-M6osWE)

Philipp: Let's apply some of the internal load balancer concepts that we just discussed in a lab. In this lab, you create two managed instance groups in the same region. Then you configure and test an internal load balancer within the instance groups as the backends as shown in this network diagram.

### Lab - [Configure an Internal Network Load Balancer](https://www.cloudskillsboost.google/course_templates/178/labs/470309)

In this lab, you create two managed instance groups in the same region. Then, you configure an Internal Network Load Balancer with the instances groups as the backends.

- [ ] [Configure an Internal Network Load Balancer](../labs/Configure-an-Internal-Network-Load-Balancer.md)

### Video - [Lab Review: Configuring an Internal Load Balancer](https://www.cloudskillsboost.google/course_templates/178/video/470310)

- [YouTube: Lab Review: Configuring an Internal Load Balancer](https://www.youtube.com/watch?v=6YHAd4YakL0)

Philipp: In this lab, you created two managed instance groups in the US central one region, along with firewall rules to allow HTTP traffic to those instances, and TCP traffic from the GCP health checker. Then you configured and tested an internal load balancer for those instance groups. You can stay for our lab walkthrough, but remember, that GCPs user interface can change. So your environment might look slightly different. So here I'm in the GCP console and in this lab, similar to other labs, we've actually pre-created some resources for you. You can explore those, again, if you go to a navigation menu and then go to deployment manager, you'll see a deployment here. We created a network with two subnets and some firewall rules. We can also just explore those by navigating to VPC network, that's what the lab instructions actually mention. So I can click there. I already have the default network and here is that extra network I've with the two subnets. And I also have some firewall rules for those right here, to allow ICMP and SSH, and RDP. All right. So what we're going to do now, is we're going to create some more firewall rules, we're going to create one for HTTP, and then we're also going to create some for the health check. So let me just click create firewall rule, and this is going to be fairly similar to what we already did for the HTTP load balancer lab, the big difference is we now have our own network that we're going to apply this to. We're also going to have target tags, load balancer backend, IP ranges, we want to HTTP from anywhere and HTTP would be TCP-80, so we can click create, and then we're just going to repeat the same thing for the health checker. So let me copy the name of the firewall rule, apply it to the right network, have the load balancer backend as the target tag. Now the IP ranges, we're going to copy them one by one in here, so let me paste one, headspace, let me grab the other one, and paste that as well. And for now, I'm just going to do all ports under GCP, but you could be a little bit more specific depending on what you want your health checker to look for. So let's click create, and now we're going to configure instance templates and instance groups. So let me navigate to compute engine and then instance templates. And we're going to create a template in there and just call it instance template one then we click create. It's actually the name that's already in there. Then I can expand management security disnetworking. Now, a couple things, first of all, in the HTTP load balancer, we had a custom image, in this case, we're actually going to set up a startup script. So under the metadata, I'm going to provide as a key, the startup script URL and in a Cloud storage bucket, that's publicly accessible. We've placed a startup file and you could go in there and you could actually review that and the link to that is in the lab. Then I'm going to go to networking. I've created all these firewall rules, they apply to specific network tags, and they're also for a specific network. So let me make sure I have the right network selected and select denetwork tag, and this is going to be for subnet A. So now I can click create, and then we're going to create another instance template for subnet B. So let me just wait for this to be created, and then I'm just going to create another one from there by selecting it, and then clicking copy. It's going to change the name automatically, and the main difference is I now need to make sure I select the different subnet. This is going to be for subnet B, and then I click create as well. So once we have these up, we can now create the managed instance groups, so let me navigate to instance groups and start up by creating our first one, just call it instance group one. This is going to be a single zone. It's going to be in US central one A. We're going to use instance template one, and we're going to select the, this will be based on CPU usage, let's set 80 as a usage minimum of one maximum of five, and I could change the cool on period for example, to 45 seconds, and now I can click create. I could also attach a health check here or just attach that later to the load balancer. So let me click create, and we're going to repeat the same for the instant script two, and this is going to be now another one. It's going to be based on the other instance template also in US central one. Let's do that in B, for example, change the target CPU usage to match what we had earlier of 80 maximum of five, cool line of 45. And then we can go ahead and create that as well. So if I click on VM instances now, I should already have an instance from the first instance group and if I come back here, I had to refresh to see that other one, we can see that the other instances now being created for the instance group two. So we can, you know, verify, again, that they're being created here. So here we see, we have now one instance group each. So now what we can do is, we're going to create a utility VM to navigate to these instances. And so we can see also, by the way, if we look at their internal IP addresses, that they're both part of a different siter range, and if I click on nick zero here, we can see which network interface this is part of. You can see it's part of subnet A, that's correct. And if I click on the other one, we can see this is part of subnet B. Okay. So each subnet now has an instance group in it. So let me create another instance, this is going to be our utility VM. Now, an internal load balancer is regional, so I want to use the same region. We could use a different zone, let's say US central 1F, I need a very small machine only for this, and for, I want to make sure this is in the right network, so let me expand this option down here, networking and make sure that this is in my right network. I have the choice of the two different subnets I have in there, let's see if it's subnet A and if I want to match this to the network diagram that we have, I can specify the actual internal IP. So instead of Ephemeral automatic, I can choose Ephemeral custom and then just type in that IP address. And again, this is just to match that network diagram that we have. I can click down on that, and then I can go ahead and create that. All right. Now the lab instructions say, you know, make sure that the IP addresses that you have match the lab instructions, this is because these are the first available IP addresses. Again, the first IP, our first, and second is reserved as well as the last and second to last. So that's why we start with the .2 here and here we have a .50 because we defined that. So now I can go SSH to the utility VM and all the curl commands are based on these two IPS. So if yours are different, you maybe want to see if you have some other instances that you need to delete first. And I'm going to curl first to this first IP here. So let me just copy that directly from the lab instructions. And this is, what's displayed here is just the page that we've set up for these instances, this comes directly from the startup script, and it's just telling me the IP address where I'm coming from, while I'm coming from this utility VM, it has the name that's telling me that it's coming to this instance and it tells me the region and zone. And I can repeat the same for the other instance. And it's now telling me, again, from the same address, but different instance and a different zone. And this is going to be really useful for when we have the internal load balancers set up and we curl the load balancer [INAUDIBLE] itself, we should be able to see that if we curl several times, that we're kind of hopping between the different backends that we have established. So we can actually exit out of here for now, and what we're going to do now, is configure the internal load balancer. So to do that, I'm going to go to the navigation menu, go to network services, load balancing, we're going to create a load balancer, this is going to be a TCP load balancing. So we start that, it going to be only between VMs, this is an internal load balancer. When I do that, it restricts me to be regional. We covered that in the slides that the internal load balancer is regional. So we click continue, and then I'll give it a name, just call it my internal load balancer. We are going to configure the backend. Specifically, this is in a specific region, which is US central one. The network is my internal app, and then the instance group, we are going to click first instance group one, click done, and then add another backend, which is going to be instance group two, and click done for that. Now, we didn't create a health check earlier, we can do that here now. So let me just go create a health check, just call it my internal health check TCP 80. That's great. And here we again, have the health, the criteria, how often it's going to check what the internal is, the timeout and how it's going to define if a backend is healthy or unhealthy. So let's save and continue that. And we can see that we have a blue check mark, this is all set up. So now I can click on the frontend configuration, the subnetwork, let's, for example, put this in subnet B. For internal IP, we could actually reserve a static internal IP address. Let's give that a name, it's just called my internal advanced IP. And rather than assigning automatically, we could choose our own because it's just an internal IP and we could match this, again, to the network diagram. So it's going to be 10.10.30.5, let's reserve that. And then we're going to finish the configuration for the load balancer by setting the port here to 80, and then we're going to click done, and now we can review and finalize this. We have our two backends, we see the auto scaling on that, we see the zones, we see, and we have the frontend itself. So we have the exact IP address, the way we can then access this internal load balancer. So let me click create, and then let's wait for the load balancer to be created before we move on to the next step. So here we are, actually clicked refresh, and we can see that the load balancer is all set up. Now we specified the IP address, so I don't have to grab it from here, instead, I'm going to go back to my compute engine instances and use the utility VM to navigate to our load balancer IP. So I'm just going to curl that and since I had that startup script on the backends that defines which instance I'm looking at, this is now going to give me some more information. So I'm going to curl the IP address, and the first time I did it, you can see it targeted instant group two. Let's run that one more time, instance group two again, group two again. Let's maybe run the command a couple more times and let's see if we can get a couple different backends. So here, run it a couple times, we can see that's 2, 2, 2, 2, 2, then it's got 1, 2, 2, 2, 1. So we can certainly see that it is load balancing between the different backends that we have. And that's the end of the lab.

### Video - [Choosing a load balancer](https://www.cloudskillsboost.google/course_templates/178/video/470311)

- [YouTube: Choosing a load balancer](https://www.youtube.com/watch?v=tZ3Kjh5Zr28)

Now that we've discussed all the different load balancing services within Google Cloud, let me help you determine which load balancer best meets your needs. One differentiator between the different Google Cloud load balancers is the support for IPv6 clients. Only the HTTP(S), SSL proxy, and TCP proxy load balancing services support IPv6 clients. IPv6 termination for these load balancers enables you to handle IPv6 requests from your users and proxy them over IPv4 to your backends. For example, in this diagram there is a website www.example.com that is translated by Cloud DNS to both an IPv4 and IPv6 address. This allows a desktop user in New York and a mobile user in Iowa to access the load balancer through the IPv4 and IPv6 addresses, respectively. But how does the traffic get to the backends and their IPv4 addresses? Well, the load balancer acts as a reverse proxy, terminates the IPv6 client connection, and places the request into an IPv4 connection to a backend. On the reverse path, the load balancer receives the IPv4 response from the backend and places it into the IPv6 connection back to the original client. In other words, configuring IPv6 termination for your load balancers lets your backend instances appear as IPv6 applications to your IPv6 clients. To determine which Cloud Load Balancing product to use, you must first determine what traffic type your load balancers must handle. As a general rule, you'd choose an Application Load Balancer when you need a flexible feature set for your applications with HTTP(S) traffic. You'd choose a proxy Network Load Balancer to implement TLS offload, TCP proxy, or support for external load balancing to backends in multiple regions. You'd choose a passthrough Network Load Balancer to preserve client source IP addresses, avoid the overhead of proxies, and to support additional protocols like UDP, ESP, and ICMP. UDP, or if you need to expose client IP addresses to your applications. You can further narrow down your choices depending on your application's requirements: whether your application is external (internet-facing), or internally, and whether you need backends deployed globally, or regionally. If you prefer a table over a flow chart, we recommend this summary table. The load-balancing scheme is an attribute on the forwarding rule and the backend service of a load balancer and indicates whether the load balancer can be used for internal or external traffic. The term MANAGED in the load-balancing scheme indicates that the load balancer is implemented as a managed service either on Google Front Ends or on the open source Envoy proxy. In a load-balancing scheme that is MANAGED, requests are routed either to the Google Front End or to the Envoy proxy. For more information on Network Service Tiers, refer to the documentation.

### Quiz - [Quiz: Load Balancing and Autoscaling](https://www.cloudskillsboost.google/course_templates/178/quizzes/470312)

#### Quiz 1.

> [!important]
> **Which of the following is not a Google Cloud load balancing service?**
>
> - [ ] SSL proxy load balancing
> - [ ] Network load balancing
> - [ ] Internal load balancing
> - [ ] Hardware-defined load balancing
> - [ ] TCP proxy load balancing
> - [ ] HTTP(S) load balancing

#### Quiz 2.

> [!important]
> **Which three Google Cloud load balancing services support IPv6 clients?**
>
> - [ ] Internal load balancing
> - [ ] HTTP(S) load balancing
> - [ ] TCP proxy load balancing
> - [ ] Network load balancing
> - [ ] SSL proxy load balancing

#### Quiz 3.

> [!important]
> **Which of the following are applicable autoscaling policies for managed instance groups?**
>
> - [ ] Load balancing capacity
> - [ ] Queue-based workload
> - [ ] CPU utilization
> - [ ] Monitoring metrics

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/178/video/470313)

- [YouTube: Module Review](https://www.youtube.com/watch?v=C1TE7ybhgUY)

person: In this module, we looked at the different types of load balancers that are available in GCP along with the managed instance groups and autoscaling. You were able to apply most of the covered services and concepts by working through the two labs of this module. We also discussed the criteria for choosing between the different load balancers and looked at a flowchart and summary table to help you pick the right load balancers. Remember, sometimes it's useful to combine an internal and an external load balancer to support three-tier web services.

## Infrastructure Automation

Automate the deployment of Google Cloud infrastructure services

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/178/video/470314)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=eGvJtazQ3fo)

Person: Now that we've covered several of Google Cloud's services and features, it makes sense to talk about how to automate the deployment of Google Cloud Infrastructure. Calling the Cloud API from code is a powerful way to generate infrastructure. But writing code to create infrastructure also has some challenges. One issue is that the maintainability of the infrastructure depends directly on the quality of the software. For example, a program could have dozens of locations that call the Cloud API to create VMs. Fixing the problem with the definition of one VM would require you first identifying which of the dozen calls actually created it. Standard software development best practices will apply and it's important to note that applications undergo changes rapidly requiring maintenance on your code. Clearly another level of organization is needed, and that's the purpose of Terraform. Terraform uses a system of highly structured templates and configuration files to document the infrastructure in an easily readable and understandable format. Terraform conceals the actual Cloud API calls so you don't need to write the code and can focus on the definition of the infrastructure. In this module we will cover how to use Terraform to automate the deployment of infrastructure and how to use Google Cloud Marketplace to launch infrastructure solutions. You will use Terraform to deploy a VPC network, a firewall rule and VM instances in the lab for this module. Let's start by talking about Terraform.

### Video - [Terraform](https://www.cloudskillsboost.google/course_templates/178/video/470315)

- [YouTube: Terraform](https://www.youtube.com/watch?v=fi50QyizrVU)

So far, you have been creating Google Cloud resources using the Google Cloud console and Cloud Shell. We recommend the console when you are new to using a service or if you prefer a UI. Cloud Shell works best when you are comfortable using a specific service and you want to quickly create resources using the command line. Terraform takes this one step further. Terraform is one of the tools used for Infrastructure as Code or IaC. Before we dive into understanding Terraform, let's look at what Infrastructure as Code is. In essence, infrastructure as code allows for the quick provisioning and removing of infrastructures. The on-demand provisioning of a deployment is extremely powerful. This can be integrated into a continuous integration pipeline that smoothens the path to continuous deployment. Automated infrastructure provisioning means that the infrastructure can be provisioned on demand, and the deployment complexity is managed in code. This provides the flexibility to change infrastructure as requirements change. And all the changes are in one place. Infrastructure for environments such as development and test can now easily replicate production and can be deleted immediately when not in use. All because of infrastructure as code. Several tools can be used for IaC. Google Cloud supports Terraform, where deployments are described in a file known as a configuration. This details all the resources that should be provisioned. Configurations can be modularized using templates which allow the abstraction of resources into reusable components across deployments. In addition to Terraform, Google Cloud also provides support for other IaC tools, including: Chef Puppet Ansible Packer In this course we will focus on Terraform. Terraform lets you provision Google Cloud resourcessuch as virtual machines, containers, storage, and networkingwith declarative configuration files. You just specify all the resources needed for your application in a declarative format and deploy your configuration. HashiCorp Configuration Language (HCL) allows for concise descriptions of resources using blocks, arguments, and expressions. This deployment can be repeated over and over with consistent results, and you can delete a whole deployment with one command or click. The benefit of a declarative approach is that it allows you to specify what the configuration should be and let the system figure out the steps to take. Instead of deploying each resource separately, you specify the set of resources which compose the application or service, allowing you to focus on the application. Unlike Cloud Shell, Terraform will deploy resources in parallel. Terraform uses the underlying APIs of each Google Cloud service to deploy your resources. This enables you to deploy almost everything we have seen so far, from instances, instance templates, and groups, to VPC networks, firewall rules, VPN tunnels, Cloud Routers, and load balancers. For a full list of supported resource types, a link to the Using Terraform with Google Cloud documentation page is included in the Course Resources. The Terraform language is the user interface to declare resources. Resources are infrastructure objects such as Compute Engine virtual machines, storage buckets, containers, or networks. ATerraform configurationis a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories. The syntax of the Terraform language includes: Blocks that represent objects and can have zero or more labels. A block has a body that enables you to declare arguments and nested blocks. Arguments are used to assign a value to a name. An expression represents a value that can be assigned to an identifier. Terraform can be used on multiple public and private clouds. Terraform is already installed in Cloud Shell. The example Terraform configuration file shown starts with a provider block that indicates that Google Cloud is the provider. The region for the deployment is specified inside the provider block. The resource block specifies a Google Cloud Compute Engine instance, or virtual machine. The details of the instance to be created are specified inside the resource block. The output block specifies an output variable for the Terraform module. In this case, a value will be assigned to the output variable "instance_ip." Let's look at a simple example in Terraform. Before you get into the lab, let me walk you through how Terraform can be used to set up an auto mode network with an HTTP firewall rule. For this example we are going to define our infrastructure in a single file, main.tf. As our infrastructure becomes more complex we can build each element in a separate file to make the management easier. Let's start with the main.tf file. The main.tf file is where we specify the infrastructure we wish to create. It is like a blueprint for our desired state. First we define the provider. Next we define our network, setting the auto create subnetworks flag to true which will automatically create a subnetwork in each region. We also set the mtu to 1460. Next, we define our firewall. Here we are allowing TCP access to port 80 and 8080. Terraform takes this main.tf file and uses it as the specification for what to create. Once we have completed the main.tf file, we can deploy the defined infrastructure in Cloud Shell. We use the command terraform init to initialize the new Terraform configuration. We run this command in the same folder as the main.tf file. The terraform init command makes sure that the Google provider plugin is downloaded and installed in a subdirectory of the current working directory, along with various other bookkeeping files. You will see an "Initializing provider plugins" message. Terraform knows that you're running from a Google project, and it is getting Google resources. The terraform plan command performs a refresh, unless explicitly disabled, and then determines what actions are necessary to achieve the desired state specified in the configuration files. This command is a convenient way to check whether the execution plan for a set of changes matches your expectations without making any changes to real resources or to the state. The terraform apply command creates the infrastructure defined in the main.tf file. Once this command has completed you will be able to access the defined infrastructure.

### Video - [Lab Intro: Automating the Infrastructure of Networks Using Terraform](https://www.cloudskillsboost.google/course_templates/178/video/470316)

- [YouTube: Lab Intro: Automating the Infrastructure of Networks Using Terraform](https://www.youtube.com/watch?v=nJOQ15vW8i0)

Person: Let's apply what you've just learned in a hands-on lab where you'll automate the deployment of VPC networks, firewall rules and VM instances. You deploy an auto mode network called My Network with a firewall rule to allow HTTP, SSH or DP and ICMP traffic. You also deploy the VM instances shown in this network diagram.

### Lab - [Automating the Deployment of Infrastructure Using Terraform](https://www.cloudskillsboost.google/course_templates/178/labs/470317)

In this lab, you create a Terraform configuration with a module to automate the deployment of GCP infrastructure.

- [ ] [Automating the Deployment of Infrastructure Using Terraform](../labs/Automating-the-Deployment-of-Infrastructure-Using-Terraform.md)

### Video - [Lab Review: Automating the Infrastructure of networks using Terraform](https://www.cloudskillsboost.google/course_templates/178/video/470318)

- [YouTube: Lab Review: Automating the Infrastructure of networks using Terraform](https://www.youtube.com/watch?v=rvl5RC9JOHo)

Philipp: In this lab, you created a Terraform configuration with a module to automate the deployment of GCP infrastructure. As your configuration changes, Terraform can create incremental execution plans, which allocates you to build your overall configuration step-by-step. The Instance module allowed you to reuse the same resource configuration for multiple resources while providing properties as input variables. You can leverage the configuration and module that you created as a starting point for future deployments. You can stay for a lab walk through, but remember that GCP's user interface can change. So your environment might look slightly different. So here I am in the GCP console, and the first thing we want to do is configure our Cloud Shell environment to use Terraform. Terraform is actually now integrated into Cloud Shell, so let's just start by verifying which version is installed. So I'm going to click and activate Cloud Shell, and then start Cloud Shell. And then we're going to run the Terraform version command to verify the version. Let me run that here, and then we'll see that this is the current version that is configured. You see that there's an even newer version here. That's fine. You could go, download that and their instructions in the lab on how to do that, but the lab instructions will work with the 12.2 or anything later. So we're ready to go. I'm going to set up a folder for us, and then we're going to launch the code editor, which is this little pencil icon up here. And we're going to use the code editor now to work in that folder that we just created and place all of our files in there. And that's going to be a much more interactive experience rather than using a command-line editor like Nano, so let's just wait for that to come up. In the meantime, I can clear this, and the first thing we're going to do once we're in here is, we're going to create a file called Provider TF, and this is going to help us initialize Terraform because Terraform uses a plug-in-based architecture to support many different infrastructure and service providers, so the provider file will specify that we're using Google as the provider. So let me right-click on TF Infra, create a new file. Plug in Provider TF, and then we're just going to copy in that the provider is Google. And I can save that, and autosave is actually enabled, so I won't have to click save all the time. And then within Cloud Shell, I'm going to navigate to that folder, and then I'm going to run the Terraform init command. And this is going to now initialize the provider, so we can see here this is the provider version. It's been initialized, so now we're ready to work with Terraform and Cloud Shell. So let's start off by configuring my network. I'm just going to now create a new file in this folder, call it My Network TF, and I'm going to copy the base code that we have in the lab instructions. So in here we have a comment. We have the resource, the type along with the name, and then we will also have resource properties. And this is a base template that's great for starting any resources in GCP, and we'll use the name and the field -- the name field as well as the type field and properties to really define what each of these resources do. So first things first, I want to replace the type with Google_compute_network, and what's important here is to also include these quotes for all of the resources that we're going to define. And this is just ABPC network. You can find more out about this in the two documentation links that are in the lab. One link is to the Google's hot platform documentation, and the other link is to the Terraform documentation. Now I also want to replace the name, so we're going to replace the resource name with My Network, again, quotes are important. And then we're going to create some properties. This is going to be an auto mode network, which means that all of these subnets are automatically created. I need to define that. Properties are optional for some resources, but in this case it's required for us to say that auto-create subnetworks is true, right? Now, I can verify that my file looks exactly like what's provided in the lab, and that seems to be true. It has moved around and spaced out some of these properties. There's a command we'll run later that will actually do that for us as well, so that's not really critical right now. I can go ahead and save this. Now next I want to configure the firewall rule. I have again some base code for that, so let me just paste that below my network resource. And we're going to create a file rule that will allow HTTP, SSH, RDP and ICMP, so I want to obviously find the right type. Now, you could look that up in the Terraform documentation or use what's in the lab instructions, and that is Google Compute Firewall. And again, we need to place the quotes around that and have a space between these two. I'm also going to have a name that's going to be the name of the firewall rule, the one that we'll actually see within GCP when we create this. And now a couple of different resource properties that I need to provide. If you think of a firewall rule, there are a couple of key things. There is the network to which the firewall rule applies. There are the source IP ranges and the protocols and ports. If you don't define the source IP ranges, it's just going to take 0.0.0/0, so in our case we're going to define the network. So let me paste that in here, and because this firewall rule depends on its network, we're using this self link reference here. And this instructs Terraform to resolve these resources in a dependent order, so in this case the network needs to be created before the firewall rule is created. We're going to do the same when we create the VM instances, so let me also now add the properties to allow and -- to allow a certain combination of protocols and ports. Specifically, I'm going to allow TCP22 for SSH, 80 for HTTP, 33 for RDP, and then the whole ICMP protocol. And then I can verify that this looks just like the instructions that are given to me, and that is the case. So I can go ahead and save that, but it's really being autosaved, so no need to hit save all the time in here. So now we're going to configure DVM instances, and what we're going to do is, we're going to create an instance module. And a module is just something that's a reusable configuration inside a folder, so we'll create one module. And we'll use it for both of the VM instances that we're going to create. To do that, we need to create a folder for the module, so let me create a new folder here, call it Instance Within the TF Infra Folder. And you see it created it outside, so I'm going to drag it in this folder. Alternatively, I could have right-clicked and created it, and the lab does show the hierarchy of these folders. And now within this folder, I'm going to create a file and call it Main TF. All right. And now within this file, we're going to again copy some base code to get us started. We have the resource type, the resource name and the type it's going to be a Google Compute instance, so let me replace that with the quotes. Now, rather than giving it a name and kind of hard-coding that, I'm going to now use a variable because I want to be able to create multiple instances with multiple different names, so I'm going to replace TF name with this construct. And then we'll later have to define from the parent configuration how to affect this module. We're also going to add some properties, which are the zone and machine type, and here again we are using variables that we'll have to define. We will also add a boot disk, and now the boot disk will just sort of hard code. We'll give it an image, and they'll be used for all of the instances that we create. And then we're also going to add a network interface, and in there we have to define a subnetwork. So where does this instance live? And if I just provide this construct here, it's going to allocate an external IP address or a public IP address to my instance. So now I need to define some input variables, right, so I'm using an input variable for name, zone, the type and the subnetwork. So let me add some stuff on top of my resource, and specifically I'm going to add a variable for the name and zone. I'm also going to define the instance type, and if I provide a value in these brackets, then that's going to be the default value. So if I don't provide another value from my configuration, it will just use this type, and that's kind of the default anyway. So that may be a good thing to do. We could have done something similar with the image, and that way we could control the image through an input variable. So now I'm just going to verify that my configuration, or I should say this module, looks exactly like the lab instructions, and that is true. So now I can go on and save this. And the next thing we need to do is we've defined the module, but now we need to use the module within my configuration. So in here I have a network and a file rule, but I also now need to say, "I want to create VM instances. I'm going to provide these input variables, and this is the module that I want you to use." So I'm just going to copy the lab instructions. Here I'm defining the module. I'm giving it the name, and then I'm defining the source. This lives in the instance folder, and then I'm just providing three of the four input variables because I already have a default value for one of them. Now, important again is I'm going to use the self link reference here because I cannot create these instances, nor the firewall rule, until the network is created. After that, all of these resources can and will be created in parallel , and we'll see that in a second, so let's go ahead and set this all up. I'm going to now just work from Cloud Shell. Let me clear this up here. We're going to run the Terraform FMT command, and this just rewrites the files into a canonical format and style. And if I do that, you might have just seen that everything got indented a little bit here and there. That's not really that critical, it's just telling us it did that, and specifically it touched the My Network TF file. And if you get an error here, you want to, you know, make sure that your configuration looks similar to the ones that we have so far. We also link the configuration to all of the three TS files the provider might not recommend in the lab instructions, so you can always refer to them and make sure that they align with what you have, and if not, you know, fix what's different. Now I'm going to need to run the Terraform init command again, and I need to mainly do that because I now have a module. It's going to say, "Oh, there's some modules that need to be used. Let me initialize those." So we've done that, and now we can go ahead and plan our configuration. We can say, "Okay, we're ready to go. Tell me what you would create when I run this command." So Terraform plan is going to run through this. It's going to tell me it's going to create these resources here. It's telling me that a lot of the values are provided, but some of the values won't be known until after it's created. And specifically it's going to add four different things, the VPC network, the firewall rule and the two instance, so if we're all good with that, we can run the Terraform apply command. It's actually going to walk us through those resources one more time, but now it's going to ask us if we're ready. So we just type yes in here. And it's going to start creating the resources, and you can see the network is the first resource that is being created here. And once the network is created, it's going to start creating all the other resources in parallel. It also gives us an update every 10 seconds saying it's still working on this, and that's pretty interesting. That way you can see that at least it's still working on this and it didn't get stuck on something. So let's wait for this to complete, and then we'll check back in. So here we can see that all of the resources were created. As I mentioned, the network gets created first, and once that's completed you can see one of the instances, the firewall rule and the other instances are starting to be created. Instances were created really quickly, and then we're just waiting for the firewall rule to be created. Now let's actually verify that all of these resources were created by navigating back to the GCP console, so is going to switch tabs here and go to the navigation menu. And first go to VPC network, and every network comes by default with a default network that is here. And here we can see the My Network that we created, which is an auto mode network. I can also go to the firewall rules, and I'll see that my custom firewall rule with the non-default firewall rule has been created. And that should allow me to ping between the two instances that I have in a network. I have ICMP traffic allowed, so I should be able to ping both in the external IP address, but even the internal IP address because both of these instances are on the same network. So let's try that out. I'm going to go back to the navigation menu, go to Compute Engine, and I'm going to grab the IP address of this first VM, and then SSH to this other VM. And then we'll try to ping that instance. So here I am. Let me run ping three times on that IP address, and we can see that all the packets were transmitted. So this should work again because both VM instances are in the same network, and the firewall rule that we created allows ICMP traffic. And that's the end of the lab.

### Video - [Google Cloud Marketplace](https://www.cloudskillsboost.google/course_templates/178/video/470319)

- [YouTube: Google Cloud Marketplace](https://www.youtube.com/watch?v=sAm9koV3mHA)

Person: Let's learn a little more about Google Cloud Marketplace. Google Cloud Marketplace let's you quickly deploy functional software packages that run on Google Cloud. Essentially Cloud Marketplace offers production grade solutions from third-party vendors who have already created their own deployment configurations based on Terraform. These solutions are billed together with all of your project's Google Cloud Services. If you already have a license for a third-party service, you might be able to use a bring-your-own-license solution. You can deploy a software package now and scale that deployment when your application requires additional capacity. Google Cloud even updates the images of these software packages to fix critical issues and vulnerabilities, but doesn't update software that you've already deployed. You even can get direct access to partner support.

### Video - [Demo: Launch Infrastructure Solutions on Google Cloud Marketplace](https://www.cloudskillsboost.google/course_templates/178/video/470320)

- [YouTube: Demo: Launch Infrastructure Solutions on Google Cloud Marketplace](https://www.youtube.com/watch?v=eYELOYvQad8)

Philipp: Let me show you how to launch infrastructure solutions on GCP Marketplace. My goal is to deploy a LAMP stack to a single compute engine instance. A LAMP stack consists of Linux, Apache HTTP server, MySQL, and PHP. So here I am in the GCP Console. And let's go ahead and navigate to the GCP Marketplace. I'm going to go to the navigation menu and just go to Marketplace. Now I have lots of different options available. There's some filters on the left that could search directly. There's some featured solutions that are in here. So there's really a lot of stuff to choose from. In my case, I'm going to search for LAMP stack, because that's what I want to create. And here, I actually have different options. They're different providers, that's really what that means. The different providers will offer these services. I'm going to click the first one that's in here. Now I have the configuration page. I see the package contents. Tells me what LAMP is again. I can see that also here, the operating system is Linux. It has Apache installed. I have PHP, and I have MySQL. And we should also obviously have HTTP enabled and we'll see that in a second. There's no usage fee for the service. If there was, it would all be billed together. We have an instance billing. This is just an N1 standard one instance along with its persistent disk and there's a sustained use discount. So if I click on launch and compute engine, I get the actual VM configuration page. I could now change the instance type if I wanted. I could create a larger instance, a small instance, I could customize an instance. And now because this is an Apache HTTP, we can see that the HTTP firewall role is also set up. I also have some networking options if I want to, you know, place this somewhere else. I even have some extra options if I want to install phpMyAdmin. All that is available to me here. And I have logging options for Stackdriver to enable Stackdriver logging and monitoring and I can do that directly in here. So it's just like a regular VM instance page. So I'm going to go and click deploy. And when I do that, it's going to navigate us to Deployment Manager. And you can see all of the configuration, as well as all the imported files that were just displayed there that are used throughout this deployment. So we can see again that the solutions on Marketplace are just Deployment Manager configurations that are already set up for you to use so that you don't have to recreate them. I also see that a password is being generated, a VM is being generated. I can click on that and get some more information about it. We can see the recent software and we have that HTTP firewall role, so just TCP80 that's being enabled here. So we can just wait for that. The instance is up. It's just configuring some more software. And then once it's up and running, we can get some more information about that LAMP stack that we just have generated. So I can click back on LAMP. It's still pending, but once it's up and running, we'll have some more information here. Let's see. It doesn't have the address yet, it's still pending. And there we go. So we have an address, we have a user, we have a password, the instance. So all the type of information. We can visit the site, we can SSH to this. We have some next steps. We could open also HTTPS traffic, change the password, assign a static external IP address rather then the current default ephemeral IP address. We can learn more about the software that's being installed. But we can also look at this from a Compute Engine perspective. So if I navigate to Compute Engine, I'll also see the instance right here. That's how easy it is to launch infrastructure solutions on GCP Marketplace.

### Quiz - [Quiz: Infrastructure Automation](https://www.cloudskillsboost.google/course_templates/178/quizzes/470321)

#### Quiz 1.

> [!important]
> **What does Google Cloud Marketplace offer?**
>
> - [ ] A centralized billing platform for all Google Cloud services and applications
> - [ ] Production-grade solutions from third-party vendors who have already created their own deployment configurations based on Terraform
> - [ ] A platform for trading VM instances

#### Quiz 2.

> [!important]
> **What's the benefit of writing templates for your Terraform configuration?**
>
> - [ ] Allows you to hardcode properties for your resources
> - [ ] Allows you to abstract part of your configuration into individual building blocks that you can reuse
> - [ ] Allows you to run configuration management software.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/178/video/470322)

- [YouTube: Module Review](https://www.youtube.com/watch?v=7VBHokOQUNo)

Person: In this module we automated the deployment of infrastructure using Terraform, and we looked at infrastructure solutions in Cloud Marketplace. Now, you might say that going through all the effort to deploy a network, a firewall rule and two VM instances doesn't convince you to use Terraform. That's true if you only need to create those resources once and don't foresee ever needing to create them again. However, for those of us who manage several resources and need to deploy, update and destroy them in a repeatable way, an infrastructure automation tool like Terraform becomes essential.

## Managed Services

Leverage managed services in Google Cloud

### Video - [Module Overview](https://www.cloudskillsboost.google/course_templates/178/video/470323)

- [YouTube: Module Overview](https://www.youtube.com/watch?v=hc7Cpb0rgOY)

Person: In the last module, we discussed how to automate the creation of infrastructure. As an alternative to infrastructure automation, you can eliminate the need to create infrastructure by leveraging a managed service. Managed services are partial or complete solutions offered as a service. They exist on a continuum between platform as a service and software as a service, depending on how much of the internal methods and controls are exposed. Using a managed service allows you to outsource a lot of the administrative and maintenance overhead to Google if your application requirements fits within the service offering. In this module, we give you an overview of BigQuery, Cloud Dataflow, Cloud Dataprep by Trifacta and Cloud Dataproc. Now, all of these services are for data analytics purposes, and since that's not the focus on this course series, there won't be any labs on this module. Instead, we'll have a quick demo to illustrate how easy it is to use managed services. Let's start by talking about BigQuery.

### Video - [BigQuery](https://www.cloudskillsboost.google/course_templates/178/video/470324)

- [YouTube: BigQuery](https://www.youtube.com/watch?v=ImSrmrC0aFI)

person: BigQuery is Google Cloud's serverless, highly-scalable, and cost-effective cloud data warehouse. It is a petabyte scale data warehouse that allows for super-fast queries using the processing power of Google's infrastructure. Because there is no infrastructure for you to manage, you can focus on uncovering meaningful insights using familiar SQL without the need for a database administrator. BigQuery is used by all types of organizations. You can access BigQuery by using the cloud console, by using a command line tool, or by making calls to the BigQuery rest API using a variety of client libraries such as Java, . net, or Python. There are also several third-party tools that you can use to interact with BigQuery, such as visualizing the data, or loading the data. Here's an example of a standard SQL query on a table called Groceries. This query produces one output client for each client in the table Groceries aliased as "G."

### Video - [Dataflow](https://www.cloudskillsboost.google/course_templates/178/video/470325)

- [YouTube: Dataflow](https://www.youtube.com/watch?v=sfBxpHzPrIg)

Let's learn a little bit about Cloud Dataflow. Cloud Dataflow is a managed service for executing a wide variety of data processing patterns. It's essentially a fully managed service for transforming and enriching data in stream and batch modes with equal reliability and expressiveness. With Cloud Dataflow, a lot of the complexity of infrastructure setup and maintenance is handled for you. It's built on Google Cloud infrastructure and auto-scaled to meet the demands of your data pipeline, allowing it to intelligently scale to millions of queries per second. Cloud Dataflow supports fast, simplified pipeline development via expressive SQL, Java and Python APIs in the Apache Beam SDK, which provides a rich set of windowing and session analysis primitives, as well as an ecosystem of source and sync connectors. Cloud Dataflow is also tightly coupled with other GCP services, like Stackdriver, so you can set up priority alerts and notifications to monitor your pipeline and the quality of data coming in and out. This diagram shows some example use cases of Cloud Dataflow. As I just mentioned, Cloud Dataflow processes stream and batch data. This data could come from other GCP services like Cloud Datastore or Cloud Pub/Sub, which is Google's messaging and publishing service. The data could also be ingested from third-party services like Apache Avro and Apache Kafka. After you transform the data with Cloud Dataflow, you can analyze it in BigQuery, AI Platform or even Cloud Bigtable. Using Data Studio, you can even build real time dashboards for IoT devices.

### Video - [Dataprep](https://www.cloudskillsboost.google/course_templates/178/video/470326)

- [YouTube: Dataprep](https://www.youtube.com/watch?v=uhnNswM3cDE)

Person: Let's learn a little bit about Cloud Dataprep. Cloud Dataprep is an intelligent data service for visually exploring, cleaning and preparing structured and unstructured data for analysis reporting and machine learning. Because Cloud Dataprep is serverless and works at any scale, there's no infrastructure to deploy or manage. Your next ideal data transformation is suggested and predicted with each UI input, so you don't have to write code. With automatic schema, data types, possible joins and anomaly detection, you can skip the time-consuming data profiling and focus on data analysis. Cloud Dataprep is an integrated partner service operated by Trifacta and based on their industry-leading data-preparation solution, Trifacta Wrangler. Google works closely with Trifacta to provide a seamless user experience that removes the need for up-front software installation, separate license and cost or ongoing operational overhead. Cloud Dataprep is fully managed and scaled on-demand to meet your growing data-preparation needs, so you can stay focused on analysis. Here's an example of a Cloud Dataprep architecture. As you can see, Cloud Dataprep can be leveraged to prepare raw data from BigQuery, Cloud Storage or a file upload before ingesting it into a transformational pipeline like Cloud Dataflow. The refined data can then be exported to BigQuery or Cloud Storage for analysis and machine learning.

### Video - [Dataproc](https://www.cloudskillsboost.google/course_templates/178/video/470327)

- [YouTube: Dataproc](https://www.youtube.com/watch?v=lHYHXzFCF10)

Person: Let's learn a little bit about Cloud Dataproc. Cloud Dataproc is a fast, easy to use, fully managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler way. You only pay for the resources you use with per-second billing. If you leverage preemptible instances in your cluster, you can reduce your cost even further. Without using Cloud Dataproc, it can take from 5 to 30 minutes to create Spark and Hadoop clusters on-premise or through other infrastructure as a service providers. Cloud Dataproc clusters are quick to start, scale and shut down, with each of these operations taking 90 seconds or less on average. This means you can spend less time waiting for clusters and more hands-on time working with your data. Cloud Dataproc has built-in integration with other GCP services such as BigQuery, Cloud Storage, Cloud Bigtable, Stackdriver Logging and Stackdriver Monitoring. This provides you with a complete data platform rather than just a Spark or a Hadoop cluster. As a managed service, you can create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spend on administration, you can focus on your jobs and your data. If you're already using Spark, Hadoop, Pig or Hive, you don't even need to learn new tools or APIs to use Cloud Dataproc. This makes it easy to move existing projects into Cloud Dataproc without redevelopment. Now, Cloud Dataproc and Cloud Dataflow can both be used for data processing, and there is overlap in their batch and streaming capabilities. So how do you decide which product is a better fit for your environment? Well, first, ask yourself whether you have dependencies on specific tools or packages in the Apache Hadoop or Spark ecosystem. If that's the case, you'll obviously want to use Cloud Dataproc. If not, ask yourself whether you prefer a hands-on or dev ops approach to operations, or a hands-off or serverless approach. If you opt for the dev ops approach, you want to use Cloud Dataproc. Otherwise, use Cloud Dataflow.

### Video - [Demo: Dataproc](https://www.cloudskillsboost.google/course_templates/178/video/470328)

- [YouTube: Demo: Dataproc](https://www.youtube.com/watch?v=HEQvXxTBuH4)

Philipp: Let me show you how to create a Cloud Dataproc cluster, modify the number of workers in the cluster and submit a simple Apache Spark job. So here I am in the GCP console. And the first thing I want to do is navigate to Cloud Dataproc. It's pretty far down, so let's navigate down to big data. We have Dataproc. And it's going to check if there is already a cluster, which we don't have, so we can go ahead and now create a cluster. And we can start off by defining the name. Let's just call it our example cluster. And then I'm not going to change any of the other settings. I'll just kind of highlight them. We can define where this is stored, what regions and zones, what kind of mode, which defines the relationship between nodes and workers. We want to have one master and workers. You can also have a high availability setting where you have three masters and then define the name of the workers. You have the machine types available for the master node, so four virtual CPUs. And then we also have the workers. There also can be four virtual CPUs, and they're going to be two of them. So in total, this itself is going to create 12 virtual CPUs. If we go to the advanced options, we could make some of these nodes preemptible. We can define the network [Indistinct] network tags in terms of firewall rules, make this internal IP only, Cloud storage bucket for staging, image. You can see there are lots of other options, all the way down to the specific encryption. So let me go ahead and just create this with the default configuration. And click create. And again, this is going to create a bunch of different machines for us now. If I open another tab and actually navigate to Compute Engine, we'll see all those instances being generated for us. So I can go to Compute Engine. So even though this is a managed service, we can see all the instances. They're all ready. So we have the master and we have our two worker nodes, and they just take the name that I specified. That attaches M for master, W for worker, and starts with a zero index. So if I come back here, I can refresh. The cluster itself is still being initialized. That's the software that's being installed and all the setup is happening in the back end. And once the cluster is ready, we can go ahead and we could maybe resize that. We see that we currently have two worker nodes. We could change that to something else that can be three worker nodes. And then after that, we're actually going to go ahead and submit a job for this. So here we are. Just took another minute or two. We have the cluster up and running. I can go click on the cluster itself and I can get more information about it. So here we have all sorts of monitoring set up. If I go to the VM instances, I'll see those. [Indistinct] master, any jobs I have, which currently we don't have any yet. And if I click on the configuration, we'll see that we currently have two worker nodes. And if I click on edit, I can change that. So let's say we want three worker node. We can change that to three and hit save, and it's now going to go ahead and request that update for us. So it's going to create another worker and it's also going to update the master and let the master know that there is another worker out there so when we submit jobs, all the workers are being leveraged. So if I change back to Compute Engine, here we see the new worker is already up and running. And if come back here and refresh, you can see that the cluster itself is still being updated. And this again should just take a minute or two. Pretty fast. Again, this is a managed service, but we can see the actual back-end instances that are being leveraged. So here we can see the cluster update is complete. I can click on it again and go to the configuration. We can see that we now have three worker nodes. So time to submit a job. Let's go to the job section and click on submit a job. I can leave the job ID, leave the region. Obviously you want to select the cluster, especially if I had multiple clusters. The job type in this case is going to be Spark. I'm going to define a main class. This is just from the example class. And what we're going to do actually is we're going to provide an example to calculate the value of pi. So arguments, I'm just going to give it 1,000. And jar file, I'm going to provide that as well. And then I can review that. There's lots of other things. I have properties, labels. So I'm all set, so I'm going to click submit on this job. So it's going to go ahead and submit that. And that job is now running. That's the status symbol that's on here right now. I can go click on that job itself. And here I can see the job actually running. I can also review the configuration one more time, so here you see all the different settings that I just specified. And we can go back to output. And again, this is now going to do a rough calculation for us to estimate the value of pi. So we'll just wait for that. And here we go. It says pi is roughly this. So the job is now complete. And if this is all that we wanted to do, we could go ahead and delete the cluster. Otherwise, we could submit more jobs. In our case, we're done. So let's go back to the cluster, select it and click delete. It's going to delete also all the data. Can't undo this. Okay. Click that. You can go to Compute Engine, refresh here. We can already see that all of these are now being stopped and will then be deleted. And that way you can easily spin up clusters and delete them so that you're only being charged for the uses of the cluster while you need it. So we can wait around for this to be deleted. So that just took another minute or two. We can see that the cluster itself is deleted. And if I go to the instances, we can also see that all the instances are gone. That's how easy it is to create a Cloud Dataproc cluster and submit a job to that cluster.

### Quiz - [Quiz: Managed Services](https://www.cloudskillsboost.google/course_templates/178/quizzes/470329)

#### Quiz 1.

> [!important]
> **How are Managed Services useful?**
>
> - [ ] If you have an existing infrastructure service, Google will manage it for you if you purchase a Managed Services contract.
> - [ ] Managed Services are pay services offered by 3rd party vendors.
> - [ ] Managed Services may be an alternative to creating and managing infrastructure solutions.
> - [ ] Managed Services are more customizable than infrastructure solutions.

#### Quiz 2.

> [!important]
> **Which of the following is a feature of Dataproc?**
>
> - [ ] Dataproc billing occurs in 10-hour intervals.
> - [ ] It typically takes less than 90 seconds to start a cluster.
> - [ ] It doesn't integrate with Cloud Monitoring, but it has its own monitoring system.
> - [ ] Dataproc allows full control over HDFS advanced settings.

### Video - [Module Review](https://www.cloudskillsboost.google/course_templates/178/video/470330)

- [YouTube: Module Review](https://www.youtube.com/watch?v=RgKsNJG6Rso)

In this module, we provided you with an overview of managed services for data processing in GCP, namely BigQuery, Cloud DataFlow, Cloud DataPrep, and Cloud DataProc. Managed services allow you to outsource a lot of the administrative and maintenance overhead to Google, so you can focus on your workloads instead of infrastructure. Speaking of infrastructure, most of the services that we covered are serverless. Now, this doesn't mean that there aren't any actual servers processing your data. Serverless means that servers or Compute Engine instances are obfuscated so that you don't have to worry about the infrastructure. Cloud DataProc isn't a serverless service, because you were able to view and manage the underlying master and worker instances.

### Video - [Course Series Review](https://www.cloudskillsboost.google/course_templates/178/video/470331)

- [YouTube: Course Series Review](https://www.youtube.com/watch?v=JuKqF8Z98Xs)

Philipp: Thank you for taking the Architecting with Google Compute Engine course series. I hope you have a better understanding of the comprehensive and flexible infrastructure and platform services provided by GCP. I also hope that the demos and labs made you feel more comfortable with using the different GCP services that we covered. Now it's your turn. Go ahead and apply what you've learned by architecting you own infrastructure in GCP. See you next time.

### Document - [Whats Next? Get Certified](https://www.cloudskillsboost.google/course_templates/178/documents/470332)

## Course Resources

PDF links to all modules

### Document - [Course Resources](https://www.cloudskillsboost.google/course_templates/178/documents/470333)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
