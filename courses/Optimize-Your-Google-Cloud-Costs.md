---
id: 767
name: 'Optimize Your Google Cloud Costs'
type: Course
url: https://www.cloudskillsboost.google/course_templates/767
date_published: 2023-09-18
topics:
  - Alerts
  - Google Cloud Platform
  - Cloud Quotas
---

# [Optimize Your Google Cloud Costs](https://www.cloudskillsboost.google/course_templates/767)

**Description:**

This is the second Quest in a two-part series on Google Cloud billing and cost management essentials.

This Quest is most suitable for those in a Finance and/or IT related role responsible for optimizing their organization's cloud infrastructure. Here you'll learn several ways to control and optimize your Google Cloud costs, including setting up budgets and alerts, managing quota limits, and taking advantage of committed use discounts. In the hands-on labs, you'll practice using various tools to control and optimize your Google Cloud costs or to influence your technology teams to apply the cost optimization best practices.

**Objectives:**

* Explain the operational prerequisites to achieve cloud cost optimization
* Differentiate between budgets and quota
* Set up budgets and alerts for actual and forecasted thresholds
* Set up cost controls with quota
* Analyze your committed use discounts
* Use advanced methods for cost control and optimization

## Optimizing Your Google Cloud Costs

### Video - [Introduction: Optimizing Your Google Cloud Costs](https://www.cloudskillsboost.google/course_templates/767/video/408401)

* [YouTube: Introduction: Optimizing Your Google Cloud Costs](https://www.youtube.com/watch?v=tlRJHCZW6yw)

hi and welcome i'm saman giovan lead business curriculum developer at google cloud once you've stepped through the building section of the google cloud console you should have a better understanding of your gcp costs the next step is to take advantage of the available tools to control and optimize how much you're spending on your cloud resources so you can maximize your investment before we get into the details of how to use the tools though let's cover some important points about roles and responsibilities moving to the cloud introduces new complexities when it comes to managing costs budgeting is no longer a one-time operational process completed annually instead spending must be monitored and controlled on an ongoing basis due to the dynamic nature of the cloud if you're working in a small organization chances are you're responsible for managing all aspects of gcp finances from budgeting to procurement tracking optimization and payment in larger organizations multiple people might be involved the key to successfully managing and optimizing costs is establishing a partnership between finance and technology teams and we've designed this quest to account for this partnership first you'll learn about what tools are available to control costs and how to set up cost controls using the google cloud console then for those looking for a technical challenge you can use the hands-on labs to fine-tune your organization's resource consumption so you can ensure your teams are not spending money on resources that aren't being used or not being used efficiently and if other teams within your organization manage these technical tasks we'd still encourage you to learn about what's possible so you can share these cost optimization opportunities with your technology teams so here's what you can expect to learn about creating budgets and alerts at billing account project and product levels setting up quota limits to control your costs understanding committed use discounts and how you can save money on your compute engine resources advanced tips for setting up cost controls best practices for managing your costs and a walkthrough of the hands-on labs let's get started

### Video - [Setting up budgets and alerts for cost management](https://www.cloudskillsboost.google/course_templates/767/video/408402)

* [YouTube: Setting up budgets and alerts for cost management](https://www.youtube.com/watch?v=iN1RmaP9uBI)

a surprise party may be fun but a surprise on your cloud bill probably not thankfully you can set up budgets and alerts using google cloud platform to closely monitor your cost and work with the appropriate teams so that they can take action when needed in this video we'll talk about setting up budgets and how you can get notified when there's an unexpected event setting a budget in gcp lets you trigger a notification when your costs hit or are forecasted to hit a specified amount you can set up a budget for an entire billing account or multiple budgets for different combinations of projects and gcp products so that you can stay informed on the areas that matter the most to you let's look at the interface where you can manage your budgets set one up and see what the options are for notifications we'll start by going to the budgets and alerts page in the gcp console you can use the top left navigation to choose billing and then select a billing account from there click on budgets and alerts only billing account administrators are able to create or update budgets and alerts for a billing account so make sure you have the right permissions for the billing account you've chosen on this page you can see all the budgets that belong to this billing account each budget or line item contains key information on the left you'll find the budget name which is helpful for you to identify what the budget is meant to track next is the budget type which can either be set as specified amount or last month spent specified amount is used to set a fixed amount to compare your monthly span to and last month spent is used to set a target amount based on the previous month spent we'll come back to budget types when we walk through creating a budget after that is the scope of the budget which refers to the combination of projects and gcp products that the budget covers for example you might have a single budget for your entire building account another for just your production projects one for just your bigquery spend or you might have a budget for any combination of these on the right side of the page are the threshold details for when an alert notification should be triggered each budget can have multiple thresholds for example 50 and 90 the progress bar lets you know how you're currently tracking against your budgets depending on the type of budgets you select the thresholds become percentages of either a specified amount or of last month's spend let's go through and create a budget to see all of the available options we'll do that by clicking the create budget button at the top the first screen that appears is the scope of the budget which includes the name projects and products covered if you don't choose any projects or products from the drop downs this budget will default to the entire billing account since you can set up as many budgets as you want feel free to create multiple budgets to track different groups of projects and products based on your needs on the next screen you can choose the budget type and amount if you choose specified amount you can choose a fixed target for this budget to track towards depending on the cost you're trying to measure that could be a hundred dollars a hundred thousand dollars or more if you choose last month's spend the amount will be pre-populated based on the scope you just chose and will be updated each month for example if you choose a product that you haven't used before or you've only used within the free tier the amount may show up as zero this can be a good way to get notified as soon as you start spending money on something you didn't expect to spend money on in addition you can choose to include credits in your budget amount including credits means you'll take the total cost then subtract any credits that were applied for that billing period credits may include things like usage discounts promotions or grants on the next step you'll be able to set up the actions you can set up different thresholds based on the amount from the previous step for when the alert notifications should trigger for example you may want to get an alert when you've reached the 50 percent 90 percent and 100 of your budget for the current month you can also add additional thresholds in case you want more granularity and each threshold can be based on actual or forecasted cost actual cost thresholds trigger when the spend from the scope you selected exceeds the specified amount for example you could set up a fifty percent actual threshold on a thousand dollar budget and you'll get an alert notification when your costs exceed 500 forecasted cost thresholds trigger when your cost is predicted to exceed the threshold amount by the end of the month if you add a forecasted cost threshold for 110 on it thousand dollar budget it'll trigger an alert notification as soon as your cost for the total billing period is expected to hit hundred dollars even if you're only a few days into the month there's also an option for connecting a budget to pub sub for programmatic budget notifications which lets you add more advanced cost controls to your budgets we'll talk about that in more detail in a later video now keep in mind that costs associated with gcp products or services are updated at different times most services will update costs within an hour but some may take up to a day to update each service has costs based on different metrics as well we recommend setting multiple budgets so you can get notified as soon as possible and so you can take action if necessary once you click finish you've saved your budget and you're good to go but be sure to allow a few hours for the budget to actually kick in here's an email alert notification that was sent off for when 100 of a 10 budget was spent we can see the display name of the budget the budget amount and period the triggering threshold and the associated billing account id in the email it's also easy to get straight to the budget page in case you want to see more information or edit your budgets only billing account administrators and billing account users will get email notifications for a budget remember budget alerts are sent out based on rules you set but don't actually stop any usage from happening if you want to go beyond emails and automate actions you should consider using quotas or programmatic budget notifications both of which we'll talk about in later videos

### Video - [Setting up cost controls with quota](https://www.cloudskillsboost.google/course_templates/767/video/408403)

* [YouTube: Setting up cost controls with quota](https://www.youtube.com/watch?v=WSBT2uZAt-k)

if you have a large team using google cloud platform resources you may want to make sure you set some limits on how much they can spend on certain types of resources in this video we'll talk about what quotas are and how to set them up budgets allow you to trigger notifications so that you can know how your actual and forecasted costs are trending over time and prompt you to take action quotas on the other hand provide an extra advantage they allow you to set a limit on the number of concurrent resources in a project or the number of api requests quotas are important to protect the community of gcp users and safeguard against unforeseen spikes in usage that could lead to budget overruns there are two types of quotas allocation and rate allocation quotas measure and limit a resource like the number of virtual machines you can have spun up at once and rate quotas reset after a certain amount of time like api requests per minute most resources and apis in each project have quotas set by default and some can even be unlimited but you can also configure specific quotas to help manage your cloud resources and establish cost controls for example let's say you have a test environment where a lot of users have access to bigquery since it's a test environment people may be a little less careful about optimizing their queries which may result in some costly mistakes so you may want to set up a quota limit to ensure each user can only query a certain amount of data each day let's look at how to do this by viewing and modifying an existing quota limit in the gcp console you can use the top left navigation to select quotas under iam and admin since quotas are set per project you may need to choose a project if you haven't selected one already you'll also need to have the right permissions for the project such as project owner or editor in order to change any quota limits this page contains information about all the quotas for gcp products and services available to you there's a lot so you can use the filters to narrow down the quotas you're interested in filtering by type lets you choose between unlimited quotas or quotas with usage service metric and location filter down to just quotas matching what you select for our example we'll start by filtering to just bitquery to keep the list manageable you can click on the service drop-down click on none to deselect everything and then click bigquery api now we're looking at just the bigquery related quotas each row is a different quota and you can click on a quota to go to the api specific quota page which may show you graphs or other details on our current project wide quota page each row starts with a service quota description then the location or regions that the quota affects after that the current usage and peak usage over the past seven days is shown for allocation quotas the current usage is how much of that resource is currently in use while for rate quotas current usage refers to how much has been used so far in the current day the last column is the actual limit enforced by the quota the first item on the list is query usage per day which shows us how much data we've created today and that there's currently no limit set up if we edited this quota we could set a limit on the amount of data that could be queried per day for the entire project for our example we want to limit each user on the project so let's select the query usage per day per user quota instead we'll select the check box and then hit edit quotas at the top of the page it's also possible to edit multiple quotas at once on the right side of the console a form will open up where we can input our limit some edits may require you to fill out contact information and a form to request the limit update rather than modifying the limits directly quota requests are typically handled within two business days and an email confirmation would be sent out once the requested update has been reviewed for this quota we can simply lower it from unlimited to any number we want without needing to make a request let's limit each user to half a terabyte of data queried with the quota limit set there will now be a limit for each user when they run queries one important note is that the quota page doesn't show you the current usage for every quota such as the per user quota we just edited if you want to learn more about monitoring quotas check out the technical documentation at cloud.google.com docs then search for monitoring quota metrics to make sure we've set up the limit correctly we can run a query in bigquery that is projected to process much more data than what we've allowed if it's set up correctly bigquery will display this error that shows us that the quota has exceeded and the query won't run we could also run smaller queries up until our quota limit at which point we can't run anymore until the next day now you should be able to view your existing quotas and update their limits to protect against any surprises we recommend setting up quota limits that best match your desired usage across many gcp services it's also possible to set up monitoring for quota metrics in stackdriver so you could create custom dashboards or alerts find out more in upcoming videos

### Video - [Analyzing your committed use discounts](https://www.cloudskillsboost.google/course_templates/767/video/408404)

* [YouTube: Analyzing your committed use discounts](https://www.youtube.com/watch?v=_wBBymdjKKM)

one of the greatest advantages of the cloud is how easy it can be to scale your usage up or down however if you're working with a predictable compute workload you can save some money with committed use discounts in this video we'll talk about what committed use discounts are and how you can use them to optimize your costs committed use discounts or cuds for short are basically contracts saying you'll commit to a certain amount of compute engine resources for one or a three year period in exchange for your commitment you'll get a significant price discount even as high as 70 percent in some cases you can make these commitments on your typical compute resources such as vcpus and memory but also on gpus and local ssds it's worth noting that committed use discounts are different from sustained use discounts committed use discounts can be purchased before consuming compute engine resources based on what you intend to use sustained use discounts are automatic discounts credited to your account after running specific compute engine resources for a significant portion of the billing month so remember the two discounts are different and they can't be combined let's look at how to set up commitments then we'll explore the committed use discount analysis report to help you gain some insights from your data the committed use discount analysis report is meant to help you answer questions like how much of my eligible compute engine usage is covered by my commitments how much are my commitments actually saving me and am i fully utilizing my existing commitments to purchase commitments refer to the google cloud console using the top left navigation click on compute engine and select committed use discounts from here click on purchase commitment to enter the specifics such as the time frame and which resources you want to purchase each commitment is purchased for a specific project and region commitments for vcpus and memory are applied simultaneously but you have separate commitments for gpus and local ssds as well you'll also get an estimated cost to let you see how much you'll pay each month and how much the commitment would save you if you run into any issues purchasing a commitment you may need to make sure you have the right quota for it once you've made the commitment you'll start to pay for that commitment over its lifetime with each billing cycle even if you don't have any resources active in this example a certain amount of vcpus and ram have been used by compute engine instance if you have a lot of resources and commitments it can be confusing to see what you're spending and which commitments are active especially since they don't auto renew that's where the committed use discount analysis report comes in it helps you visualize all of your commitments alongside your compute resources using the google cloud console choose a billing account and click on the commitments option to get the report unlike billing reports you won't be able to get access to the cud analysis report by being a project owner though you can filter by project you'll need to be a billing account administrator or billing account viewer to see this report let's step through the different areas to see how this report works the resources are split up by type this lets you see vcpu ram ssd and each gpu type separately along with your usage and relevant commitments you can toggle between them by clicking the tabs here we'll start with vcpus above the graph you can see a summary of your commitments in one row the left side lets you see if you're looking at all regions or if you're looking at specific regions which i'll show you how to do in just a second the middle shows the current number of active commitments which may take a day or two to update when a new commitment is purchased the right side shows you the utilization of your commitments by comparing the amount of eligible usage being covered by your commitments to the total commitments you have you can also hover over the question mark for the specific resource counts the stacked bar chart itself shows you the resource usage for each day the amount that was covered by a commitment and a line that shows you the current quantity of commitments you can hover over each column to see the exact usage numbers for each day there's also a summary table below the chart when looking at the cud report for an entire billing account the cud utilization is grouped by projects and regions that's why for certain time frames you might notice the difference between eligible resource usage compared to cud coverage to understand the delta you'll need to break down the data by region and then project to see why there was a dip like that let's look at a specific subset of days as an example here the usage view is narrowed down to a few days on the first date all commitments were fully utilized and there was additional eligible resource usage on top of the purchase commitments for the remaining days the graph is revealing different information the dotted line that shows the active commitment hasn't changed but the actual resource usage has since we're talking about the cloud it makes sense for the resource usage to fluctuate but how can we find out why the resource usage wasn't covered by the commitments let's drill down using the filters you can also change the time range here if you want to see more dates we'll change the view to region and see the specific split up by each region this is the us central 1 region but the graph is still showing the same usage pattern remember that committed use discounts are specific to a project and region so let's use the filters to select a specific project with this filtered view we're now able to answer our question fewer vcpus were used for a few days in this project which led to commitment under use understanding more details about your commitment utilization can give you a much better idea of how to plan for future usage of your resources and how to ensure purchase commitments are fully utilized or even where you may need to purchase additional commitments committed use discounts can help you save money on your compute engine resources especially if you have a lot of consistent usage it's also important to use the committed use discount analysis report to see how your existing commitments are sizing up and if it makes sense to purchase additional commitments use the gcp console and try the report for yourself

### Video - [Setting up advanced cost controls](https://www.cloudskillsboost.google/course_templates/767/video/408405)

* [YouTube: Setting up advanced cost controls](https://www.youtube.com/watch?v=UeWGrJ8f0pY)

getting a budget alert notification when your gcp costs exceed your budget is a great way to stay on top of your cloud costs but controlling costs is more than just avoiding billing surprises it's also important to provide your teams with the flexibility they need while also minimizing or restricting unnecessary costs in this video we'll talk about programmatic budget notifications and go over some examples to help you set up advanced cost controls in an earlier video we covered how to trigger email alert notifications when you've exceeded your budgets and these budget alerts are sent to billing account administrators and billing account users so you can easily control who gets those notifications however this also means if there's a need to investigate or follow up with other teams about a charge you'd have to depend on potentially time-consuming offline conversations to determine if any changes need to be made if you want to put more proactive controls in place programmatic budget notifications then allow you to run code based on your budgets for this we'll use cloud pubs up if you're not familiar with pub sub think of it as a simple message handler which passes on any events such as budget updates to any services that are subscribed to it cloud functions is a lightweight and serverless way to run code and it's an ideal solution for quickly handling budget alerts any budgets you create with a pub sub topic will publish the current status of a budget multiple times per day rather than waiting for a threshold to be crossed with each budget you set up you can have both email notifications and programmatic notifications when setting up advanced cost controls like this you'll want to have your finance and it teams work together since this process involves both sides when setting up a budget after choosing the thresholds you can connect cloud pops up to the budget where you'll choose a project and pub sub topic to receive notifications once the budget is publishing to pub sub you can have any number of subscribers that will handle the notifications such as cloud functions once you've got everything connected with pubsub the cloud function will receive notifications with some information such as the billing account id the display name of the budget the most recent threshold exceeded and the amount of your cost for the scope of your budget now that you're writing code with this information you can pretty much do anything the gcp api lets you do one very common example is to post an update to slack or other platforms connecting any tools your teams use to stay on top of your costs again this keeps people up to date but we can also automate direct cost controls to respond to these notifications one of the more drastic actions you can take is to disable billing on a project entirely for example you may have a development or test environment and a dev team that constantly spins up resources in that environment if someone on the team spins up an expensive resource like a large compute instance and forgets to turn it off it can rack up costs pretty quickly one way to avoid this may be to set up a budget and programmatic budget notification that removes the billing account from the project after a specified cost is reached it's important to note that when you disable billing on a project all the billing resources for that project are shut down and then deleted this is probably not something you want to do in a production environment but can be a quick way to make sure you set a very hard limit on your cost for that project sometimes a more nuanced approach is better such as limiting the number of resources that can be spun up with quota or simply stopping all compute instances without deleting them that way you're not paying for them and the appropriate team can investigate why the usage was higher than planned when testing your code you can manually send pub sub messages so you don't have to actually wait for the budget amount to be triggered another driver of unnecessary or accidental cost can be from resources that aren't being used optimally over-provisioned resources are larger than they need to be to serve their current workloads and since this is the cloud it can be easier to scale them down as needed idle or part-time idle resources can also be an easy target for saving costs since they can be stopped or scaled down when they're not at peak usage all of these can also be automated on a scheduled basis such as regularly looking for resources that aren't being used and removing them you'll find examples of these in the labs covered later in this quest using programmatic budget notifications is another step towards setting up cost controls that match your business needs each business is different so how much you'd like to control and to what extent are decisions to be made collaboratively between your finance and technology teams coming up next are the hands-on labs we've organized the labs into two groups along with walkthrough videos to ease you into the more technical concepts if you're billing account administrator or billing account viewer we recommend paying close attention to the lab walk through videos and if you're also responsible for carrying out the technical tasks for cost optimization we encourage you to complete the labs as well

### Quiz - [Knowledge Check: Controlling Your Costs](https://www.cloudskillsboost.google/course_templates/767/quizzes/408406)

#### Quiz 1.

> [!important]
> **Suppose one of your engineers spun up an expensive compute instance in a Test environment and left it running overnight. You receive an alert notification the next day with a significant spike in your costs. Which of the following could you implement using cost management tools to help prevent this from happening in the future? Select the two correct answers.**
>
> * [ ] Set up a programmatic budget notification to disable billing for the project specific to the test environment to stop usage when exceeding the budget threshold.
> * [ ] Change the quota limit for that resource type to unlimited.
> * [ ] Use the shared responsibility model to encourage project owners to make cost-effective decisions when consuming Google Cloud resources.
> * [ ] Disable that individual's access to all Google Cloud resources until the next billing cycle.
> * [ ] Set up a quota limit to cap service usage when developing and testing applications.

#### Quiz 2.

> [!important]
> **Which of the following is not a usage-based credit type that you can filter your billing data by? Select the correct answer.**
>
> * [ ] Committed use discounts on your compute resources.
> * [ ] Sustained use discounts on your compute resources.
> * [ ] Tax reductions issued by Google Cloud for using cloud resources.
> * [ ] Promotions, such as Google Cloud free trial credits.

#### Quiz 3.

> [!important]
> **What is meant by the scope of a budget? Select the correct answer.**
>
> * [ ] Refers only to the number of projects the budget is intended to cover.
> * [ ] Refers to the combination of Google Cloud projects and products that the budget covers.
> * [ ] Refers to the source payment profile associated with the budget.
> * [ ] Refers to the source billing account associated with the budget.

#### Quiz 4.

> [!important]
> **When should you set up quotas? Select the correct answer.**
>
> * [ ] When you want to receive an email notification for when your costs hit, or are forecasted to hit, a specific amount.
> * [ ] Only when you first start using a resource type you haven't used before.
> * [ ] Only when you need to decrease the limit of your cloud storage space.
> * [ ] When you want to set a limit on the number of concurrent resources in a project or the number of API requests.

#### Quiz 5.

> [!important]
> **Which of the following best describes the ideal use cases for setting up a budget? Select the two correct options.**
>
> * [ ] To monitor resource consumption at specific intervals before the end of the billing cycle (e.g., at 25%, 50%, etc.).
> * [ ] To prevent project owners from incurring costs beyond last month's spend.
> * [ ] To establish a benchmark for how much to allocate per resource type each month.
> * [ ] To provide a weekly update to project owners on how the organization is tracking against overall GCP costs.
> * [ ] To receive an email alert notification for when your costs hit, or are forecasted to hit, a specified amount.

#### Quiz 6.

> [!important]
> **A budget can only be set for  _______________ and configured to cover the scope of _____________ and/or ________________. Fill in the blanks by selecting the correct answer.**
>
> * [ ] A single billing account, projects, products
> * [ ] Multiple projects, a single billing account, multiple projects
> * [ ] Multiple products, a single project, and multiple products
> * [ ] Multiple billing accounts, projects, products

#### Quiz 7.

> [!important]
> **You run a query for the fifth time in BigQuery, and you receive the following error message - 'Custom quota exceeded: Your usage exceeded the custom quota for QueryUsagePerUserPerDay, which is set by your administrator.' What does this message mean? Select all that apply.**
>
> * [ ] Your access to BigQuery has been revoked indefinitely.
> * [ ] You've exceeded the limit on the amount of queries that you can run for the day.
> * [ ] Only your project administrator can adjust the quota limit.
> * [ ] The limit will be reset the following day.

### Video - [Lab walk-through - set 1](https://www.cloudskillsboost.google/course_templates/767/video/408407)

* [YouTube: Lab walk-through - set 1](https://www.youtube.com/watch?v=uZ0tXZXLjks)

hi i'm evan jones a technical curriculum developer for google cloud i'm going to be walking you through three labs that'll cover the basics for how you can understand and analyze your costs on google cloud platform with these labs you'll be more prepared to work with the appropriate teams on setting up advanced cost controls based on your business needs first i'll go through a lab that walks you through setting a quota to control bigquery usage and costs the next lab focuses on stackdriver stackdriver is an important tool that provides monitoring and management for services containers applications and infrastructure it's also a great resource for collecting information related to your gcp costs in the third lab we'll dive into cloud functions a lightweight compute solution for developers to create single purpose standalone functions functions can respond to cloud events such as programmatic budget notification without the need to manage your own server now for the first lab i'll query a public data set and explore the associated bigquery costs then i'll modify the per day quota allowed for my lab user and try to rerun that same query with that newer reduced quota let's see what happens here's the first quick lab that you'll be walking through it's all about setting up cost control with a quota inside of bigquery the lab prompt is going to look like this some of the things that i like to do inside of quick laps when i first start are take a brief look at the learning objectives here's what we're going to do we're going to query that bigquery public data set it'll be a large one we'll be processing terabytes of data and explore the costs of processing that much data then we'll exercise a really cool option as a billing administrator and you can modify the quota which is the cap that folks are allowed to process inside of bigquery and then we're going to make sure that quota actually works by rerunning one of those really large queries after we've actually done the quota the lab provides you information on bigquery pricing and a few tips on setting up your lab inside of the qwiklabs console remember inside of quiklabs you can start and stop these labs but as soon as you click end lab that project is deleted you can restart and create new projects if you want to do the lab multiple times so the main tool that you'll be using for this lab is bigquery that's really processing large amounts of data so the first thing we want to do is query that public data set i'm going to copy over the query here inside of google cloud platform as you'll see in the lab when you walk through it you'll be taken to bigquery in the navigation that's under big data if you scroll all the way down you'll see big data my tip and trick of the day is if you hover over and you click the pin icon you can actually pin it to the top you can see that it gets removed and then added to the kind of like your favorites it's starring a particular product and i'm going to paste that query in and you can see that it takes 1.4 terabytes it's going to process 1.4 terabytes of data to run which depending upon your budgets maybe you want a little bit more data that you want to process based on how much bigquery is going to charge you for the bytes that are processed and as a quick review bigquery will process um one terabyte of data per month uh per project for free and it will charge you i think it's upwards of like it's like five dollars a terabyte or something like that after that so about half a terabyte is like two bucks so one of the questions in the lab is gonna ask you the the total cost of processing this entire query so if i were to run this query which is what you're going to do as part of the lab 1.4 terabytes actually processed super super quickly and that's because i've actually run this one before and this lab actually takes advantage of cash so if you're working on a project with a bunch of other folks and everyone's running the same query at the same time bigquery is going to go hey i just gave the same answer for the exact same query to person a and then person b is asking me the same thing cache is actually a really really great feature that's built into bigquery to prevent you from being charged twice for the exact same query but say i wanted to for the purposes of just experimenting with the quota i'm going to go into more settings and then disable the automatic caching of a query so i really wanted to process 1.4 terabytes of data let's see as i start to run this i get this pop-up that says your custom quota has exceeded your usage uh based on this quota policy query usage per day which was set by me about 10 minutes before you saw this video uh is breached by this particular query now you won't get that when you're running this lab for the first time because you haven't set up that quota so the last part of the lab is going to get you inside the navigation menu under identity access management and admin there's a great spot called quotas and within there though i will walk you through how to set up a custom quota for bigquery if you want to set a ceiling and how many bytes of data that people can process and you can see that i've set a really really arbitrarily low quota i think it's only like a couple hundred gigabytes uh for the bigquery api and the lab will walk you through you can set up that quota then you can edit the quota that exists and uh let me just provide and you can see where you can actually set the the quota inside of here so we did .25 terabytes instead of unlimited and that's why it triggered that so it's a great way and this is just one of the quotas that you can set across many of their products in there but it's a great way to prevent people from using more than they should most of the other labs inside of this course will provide you monitoring on the usage quotas is one of those hard ways where you can basically say you can use this much and no more in this lab i'll specifically cover how to monitor a google compute engine virtual machine or vm instance with stackdriver you'll also learn how to install monitoring and logging agents for your own vms which collect information from your instance and can include things like useful metrics or logs from third-party applications in subsequent labs we'll be using stackdriver a lot to monitor events that can help you identify cost optimization opportunities in gcp let's take a look as we mentioned in the introduction stackdriver is an extremely useful tool for monitoring the usage of your google cloud platform project this is going to get you familiar with the basics of stackdriver and here we're actually just going to monitor just a very simple virtual machine that we've converted into an apache web server so as part of the lab i'm not going to walk you through all of it but i'm going to hit the highlights on the right hand side you can clearly see the lab involves quite a few steps but they're all step by step it'll walk you through even if you've never worked with stackdriver gcp before it's pretty just hitting one different item at a time and moving through and collecting these activity tracking points which is great it's another nice feature about quick labs that i like is as you complete a task you automatically earn these points afterwards so you know that you did this step correctly so i'm a huge fan of the activity tracking so the first thing that's going to ask you to do inside of the lab is you need to create something that's going to consume resources so you can monitor it later just for this use case stackdriver can be used to monitor lots of resources within gcp but this use case we're just going to create a simple web server off of a compute engine vm i'll show you i'll show you i've already done that here so in gcp under compute you can get to the virtual machine instances page as you're going to you can do all this inside of your lab the cool thing is i have other virtual machines here that's basically saying hey that work you're doing with tensorflow you're not using it at all you should switch it to a different custom machine type that's super cool so that's just intelligence that's built into the google cloud platform but ours it's going to ask you to create this lamp 1vm lampstack is for linux apache mysql php it's just a particular stack of software technologies and this one is just going to be a web server so what we're actually going to do is let's see if we can't find the external ip address boom so this is just the blank default page if you install install apache on your particular bare bones virtual machine and what you're going to be doing is you're going to be looking at metrics like usage metrics network traffic computational power or cpu power by monitoring it with stackdriver so inside of the lab you'll see you first need to create a stackdriver account and then once you've created accounts and it involves installing little agent scripts on the actual virtual machine itself which you'll be doing via ssh you'll be creating checks and alerting policies i'll show you what the finished product of that looks like to get distract driver by the way as you'll see in the lab navigation menu all the way down for monitoring that'll kick you over into stackdriver and you'll be building a dashboard based on your based on your usage of that virtual machine so let's see i'm in my particular workspace i've already as part of the lab installed these agents created these uptime checks i have an alerting policy that actually kicks out an email if my web server is getting way too much traffic so let's see i want to check in on my uptime check hopefully my server is up and running you can monitor the latency let's take a look at a dashboard that we built so you can see the cpu load on my cpu so what i do to test this by the way back on this page i frantically refresh this or you can arbitrarily send like lots of network traffic to your virtual machine because you're essentially mocking what a production user would do and then you can see the packets that are received over time i think the one that i'm working on here is the blue one i have multiple virtual machines in this quick labs account but the blue one you can see there is the receive per second and then the load on that particular cpu as well and it's great because the cool thing that you can do is all right well dashboards are pretty but i need to take action off of them that's where you can set up an alert so i can set up a policy to basically say hey i've breached 500 bytes or some other threshold of traffic uh you know because it's a holiday sale and i want to make sure that my uh n1 standard one web server has enough serving capacity because i didn't build it with containers or something like that and i want to kick out an email so this will actually send an email out and i just added in some little custom text here that just says hey this was actually triggered so it gets your feet wet with a lot of the things that you can do inside of stackdriver as far as monitoring goes and then beyond just visualizing those in a dashboard you can trigger out those alerts via an email or another different medium as well so that'll get you started with stackdriver in this next lab you'll create a simple cloud function then you'll deploy and test that function and view the output logs are we exploring deploying cloud functions via the command line as that's how we'll deploy functions in all of our subsequent labs if you're interested in exploring the cloud function ui feel free to check out the cloud functions quick start console version of the lab inside of the qwiklabs catalog let's take a look in this third lab it's all about cloud functions if you want some single use generally event triggered code to run inside the cloud you want to use cloud functions you don't have to worry about maintaining a server you can just write in node js code python code javascript code go lang code as well if you want just to execute and do something based on a response to like an http request or a gcs bucket alteration happening that's a perfect use case for cloud functions so this just gets you very just loosely familiar with the basics of creating invoking and then monitoring one of those functions it's the simplest of all functions it's just basically that literally the hello world function and monitoring the output so we're going to be using the command line here which means you can be using the the terminal instead of clicking around inside of the ui if you haven't used the command line before it's actually not too bad inside of gcp i'm going to just open up the cloud shell which is a free terminal that you get inside of there it's a micro virtual machine that automatically opens up and some of the different functions that you'll invoke on your on your virtual machine is deploying the hello world function i'll show you what that function actually looks like so this is the function that you're going to create you make a directory called hello world you'll navigate to that directory and it's just a javascript function so the language we're gonna write for this function is javascript that just says hey log something and then into the logs and just say hey hello world it's my cloud function and it actually invoked so you're just it's literally just saying you called the function here is the output that it was called it's not doing anything special but in subsequent labs after this you can do a ton of things inside of your cloud function code but this is just getting you familiar with actually creating testing and deploying and monitoring the output of a cloud function so let's see if we can't get that cloud function to run i created it a little bit earlier so i'm going to say call this cloud function that was created earlier and i should hopefully get a result that says it was executed i got an executed id which is great and then i can be able i then can find the logs let's find view the logs i can then view out all the times i executed this hello world function just by copying that i think i've done it more than once at this point so hopefully there'll be more than one that shows up so it's reading from the logs the function that was invoked i just gave the custom name hello world and then it'll return us back the results you can see that it was invoked at all these different times throughout the day today how long it took and the status of the function a really cool thing about cloud functions is as you're going to see in subsequent labs you're kind of going to set up automated cron jobs via cloud scheduler and you want it to invoke like a function like clean up persistent disks clean up ip addresses clean up other resources that aren't used and the cool thing about cloud functions is for whatever reason if that function failed then it's not like a programming error it's going to fail every single time if it was like a a teeny issue that'll resolve itself you can actually automatically specify a retry time for that cloud function to be invoked so it's designed to be what they call serverless or hands off and you don't have to monitor the hardware it's going to make sure that whatever code that you want to run as part of that function works fine and google will handle the rest of the maintenance

### Lab - [Setting Up Cost Control with Quota](https://www.cloudskillsboost.google/course_templates/767/labs/408408)

In this lab you will query a large dataset, update the BigQuery API quota, and then optimize your query to run within quota.

* [ ] [Setting Up Cost Control with Quota](../labs/Setting-Up-Cost-Control-with-Quota.md)

### Lab - [Cloud Monitoring: Qwik Start](https://www.cloudskillsboost.google/course_templates/767/labs/408409)

This lab shows you how to monitor a Compute Engine virtual machine (VM) instance with Cloud Monitoring.

* [ ] [Cloud Monitoring: Qwik Start](../labs/Cloud-Monitoring-Qwik-Start.md)

### Lab - [Cloud Run Functions: Qwik Start - Command Line](https://www.cloudskillsboost.google/course_templates/767/labs/408410)

This hands-on lab shows you how to create and deploy a Cloud Run function using Cloud Shell, the Google Cloud command line.

* [ ] [Cloud Run Functions: Qwik Start - Command Line](../labs/Cloud-Run-Functions-Qwik-Start-Command-Line.md)

### Video - [Lab walk-through - set 2](https://www.cloudskillsboost.google/course_templates/767/video/408411)

* [YouTube: Lab walk-through - set 2](https://www.youtube.com/watch?v=-tfQDx76tNU)

hi again evan here in this video i'm going to walk you through three labs that provide solutions to help your engineering teams drive cost optimization one quick way to optimize costs is to get rid of resources that you're not using in our first lab you learn how to identify and then delete unused ip addresses automatically in the next lab we'll walk you through identifying unused and orphaned persistent disks another easy way to reduce your cost the third lab will show you how to use stackdriver cloud functions and then cloud scheduler to identify opportunities to migrate storage buckets to less expensive storage classes okay so in this lab we'll use cloud functions and cloud scheduler to identify and clean up wasted cloud resources so in google cloud platform when a static ip address is reserved but not used it accumulates a higher hourly charge than if it's actually attached to a machine in apps that heavily depend on static ip addresses and large-scale dynamic provisioning this waste can become very significant over time so what are you going to do you create a compute engine vm with the static external ip address and a separate unused static external ip address then you'll deploy a cloud function to sniff out and identify any unused ip addresses and then you'll create a cloud scheduler job that's going to run every night at 2 am to call that function and delete those ip addresses once again just remember that gcp's user interface can change so your environment may look slightly different though i'm going to show you in this walkthrough let's take a look so here we find ourselves back in another quick lab now this one is all about using cloud functions to do magical things in this realm of world we're just going to be creating cloud functions to clean up resources aren't being used this particular case we're going to be creating a function that's going to clean up unused ip addresses so the actual function code as you're going to see way down there is just a little bit of i think it's written in javascript it's just a function java functional javascript code the great news is you don't have to write the function yourself google cloud engineers have provided a lot of these functions on their external github repository which is kind of cool so if you wanted to just use literally the things that you're using inside of this lab right now you can copy and paste into your own project at work as well so highlighting the things that you're going to be doing you first need to create a virtual machine and like we said in the introduction you're creating a couple external ip addresses one that you're going to use and one that's going to be unused and then you can be building that code that's actually going to go through sniff out any that aren't in use and then bring them down now that's only if you manually triggered it so the second part of this lab is actually to schedule that cloud function to run in this particular case nightly at 2am i believe and that'll automatically invoke that function and then do that cleanup automatically so once you set it up once which is great it'll just run in perpetuity so a couple of different things that i want to highlight so the first thing you need to do is inside of the lab you'll be working off of code that already exists inside of this github repository in fact this is for the last three labs anything that has to do with cleaning up things is gonna be based on this uh repository here so i'll show you just very briefly the first lab is gonna be on the unused ip addresses the second cleanup lab is the unattached persistent disk or pd and the third lab is gonna be a migrating storage to a cheaper storage class if that bucket isn't being used uh that actively sounds kind of cool huh so the code for the unused ip address is just this javascript code here again you don't have to write it you just have to invoke it deploy it for your own functional code but you can kind of see what it's doing it says all right you've you can actually view the watch for this the function is called there are how many ip addresses that are out there for each of them uh if it's not used or if it's not reserved then you know you could potentially delete it if you can't it'll say could not delete the address and then boom just 60 or so lines of javascript code that basically says all right there are statuses associated with these ip addresses i want you to have some logic around them to basically say i'm going to iterate through all the ip addresses that people on my team have created throughout my project and then remove those ones that aren't used so this is the actual nuts and bolts of the the lab is this public um javascript code here in the in the github library let's take a look at how you actually deploy and invoke that so after you've cloned that project code in what you need to do then is you need to kind of simulate a production environment you're going to create the unused ip address and the used ip address then you're going to associate them with a particular project and then you want to confirm that they're actually created so i'll actually show you just this one command right here so this says hey gcloud compute how these commands are structured by the way is google cloud uh what service or product you want to use this is compute engine and then for ip addresses it's just called addresses and i just want a list and i only want to filter those for my particular region this is just a flag filter that you can have i've actually already run through this lab so you can see that there is no ip address that doesn't say it's uh not in use because i actually already ran the function and it deleted it but as you work your way through your lab you'll have a long list of unused ip addresses that it'll trim down to just the ones that are in use so it's pretty cool so most of the magic again since this is using the command line to deploy the cloud function is going to happen here but once you actually validate that it works and it cleaned up those ip addresses that weren't in use what you can then do at the end of the lab is basically say hey i don't want to come into the command line every time and invoke this function i'll just show you what the function invokes looks like right now deploy it trigger it uh here we go so after you deploy it and you get ready to ready to work the last part of the lab is actually to schedule it so it uses a gcloud scheduler cloud scheduler is a relatively new product it's essentially just like a glorified cron job that google will manage all the maintenance and the hardware behind the scenes for you you can use the vm i use the um the ssh command line terminal to create it but then i also like to go into and see where it actually is i think it's under like the admin tools here uh tools we want cloud scheduler with a little clock here and one of the things that you can do is this one was the unused ip addresses the next lab you'll be creating one for the unattached persistent disk jobs you can instead of invoking it via the terminal you could do the run now as well which is kind of cool so it goes lightning fast because just running that javascript code and then just you know killing out all the ip addresses that aren't used which is great so i'm a big fan of seeing after you've created all your work inside of the terminal you can view all of your jobs either via the terminal or within the ui as well and then boom it automatically runs at a frequency and again much like a cron job this denotes 2 am every night and there are website utilities out there that help you convert time into a cron job syntax right here so don't worry about that too much all right so that's the first deep dive that you've had into using a cloud function to do something a little bit more sophisticated than hello world and our first cleanup use case was removing those unused ip address so go ahead and try that lab and then all that knowledge that you're going to be learning there will make the next two labs very very easy because you can be doing the same things operating off of the same repository good luck in this lab you'll use cloud functions and cloud scheduler to identify and clean up wasted cloud resources in this case you'll schedule your cloud function to identify and clean up unattached and orphaned persistent disks you'll start by creating two persistent disks and then create a vm that only uses one of those disks then you'll deploy and test a cloud function like we did before that can identify those orphan disks and then clean them up so you're not paying for them anymore let's take a look here we are into the quick lab for cleaning up those unused and orphaned persistent discs again one of my favorite things about those quick labs is as you're working your way through the lab you'll get those points as you complete all those lab objectives automatically quick labs is smart and knows whether or not you did the work or not but it's also really really fun to get those perfect scores all the way at the end so as you scroll down and look at this lab you're already starting to get familiar with cloud functions and again those are those magical serverless triggers that can look for things to happen be triggered and then do other things so the other lab that you worked on just before this was cleaning up those unused ip addresses and you set that up as running as a cron job via the cloud scheduler at 2 am same general kind of concept for this lab except you don't care about ip addresses here you care about persistent disks those are those hard drives that are attached to the virtual machines because again inside of google you have the separation of compute and storage just because you get a virtual machine doesn't mean that you need to have that virtual machine running 24 7 just to keep that data alive so if you need compute power for an hour and you need persistent storage for in perpetuity you can actually separate those which is kind of cool but say you didn't want that data just around when you had no virtual machine associated with it you can identify those orphaned persistent disks so as we mentioned in the introduction you'll be creating two of those persistent disks the vm is only going to use one of them we're going to detach that disk and then we're going to create some code or copy some code from the repository that it's going to be able to look through and find any of those disks that were never attached never used and basically say hey while you're paying for stuff that you're not using then you deploy that cloud function that'll remove all this persistent disks and then lastly so you don't have to constantly wake up every morning and press that button to say remove persistent disks that'd a really boring job you're going to create a cron job via the cloud scheduler to automatically do that for you so again if you already did the the last lab or you've seen the demo video for the last lab you'll be working off of the code that's in this public google repository for gcf automated resource resource cleanup gcf is just google cloud function here we have the unattached persistent disks instead of javascript this time it's actually written in python which is pretty cool it's a little bit more involved so it basically says all right well i want to find and look at and delete the unattached persistent disks so much like you iterated through the ip addresses before in the prior lab here you're getting the list of all the disks and iterating through them and then hey if the disk was never attached and that's some metadata associated with the disks that it was never attached there's a timestamp associated with it in fact it's actually just last attached timestamp is not present then you're basically going to say all right well this disk was never attached to anything it was never used we're going to go ahead and delete that so this code will run and handle all of that code automatically for you you'll not be writing python don't worry about it this is just code that you can lift and shift and use on your own public applications the main argument that you want to be considering here is deploying this code as a repeatable cloud function and then having it invoke at a regular nightly interval say every night at 2 a.m as the cloud scheduler will help you so back inside of the lab the orphan persistent disks let's take a look at some of the things that we can do we'll run some of this too so we just look through the the repository after that you're going to actually create those persistent disks here is where you give just an orphan disk great unused disk great you're actually going to create those two disks so i'll go ahead and just run these now inside of cloud shell let's see what directory i'm in i'm in the root directory i need to go into the wherever the code for the unattached persistent disk is now i'm in there as you saw we were just looking at that python function before main.pi by the way if you're not familiar with unix commands couple useful ones are ls which just lists the contents of a given working directory cd says change directory it's kind of like double clicking on a particular thing into the directory so it's double double clicking on unattached pd and then cat shows the contents of the file doesn't do anything with it but it shows the contents so that same python code that you saw before is now just visible on the screen here so what do we want to do we want to oh i didn't want to copy that we want to create some persistent disks have some that aren't going to be used and then delete them so we're in that directory we're going to create some names and this is literally what you're going to be doing inside of the lab is working your way through copying and pasting i'm hovering over these boxes clicking on the clipboard to copy creating all of them i need to make sure that my project id is set so let's see it's probably because i skipped an earlier step inside of the lab but the great news is if your project id is not set there is a command for that as well so we'll set the project id it's updated properly now we'll try again export is just basically saying define this as a variable create those two disks ah no it's because i didn't run the export project id up here boom done this is why it's super helpful to go through the lab in order and then let's create the disk which should not work make this a little bit larger it's creating all these disks automatically this is exactly what you could be doing through the ui here we go we've got some disks that are ready let's validate these these disks were created before we blow away the ones that are unattached what disks do we have we've got a lot great we've got an orphan disk and an unused disk and i have other stuff that i really don't want to be deleted so hopefully this code works as intended so orphan disk and unused disk keep your eyes on those and of course as you're working my way through the lab you click on check your progress in your real lab instances as well i've created the vm already and then let's uh i'll give it just a different name this time let's see so here we're going to create that virtual machine instance and then look we're giving it the disk name orphan disk which i bet you can tell exactly what we're going to do to it so you now right now we have a virtual machine that's using this disk so the next thing in order to get it orphaned we've got to detach it so let's see inspect to make sure that it was actually attached to the disk boom and it has last attachment time and everything in there now let's orphan it detach the disk marked orphan just a command to detach it so now it's off in the world uh let's see detach disk disk instance my name for this demo i just have a dot one boom it's gonna detach it and now it detached it and we're going to view the detached disk it is orphaned it is detached great now the last part of this code is actually deploying that cloud function that'll sniff through and look through all the uh disks that are out there and then detach them it's having you inspect that python code just kind of be familiar with it again you don't have to write any of that python code yourself but getting with a familiarity with it can't hurt okay so now we i've already deployed the cloud function before recording this video i've scheduled it now what i want to do is list all this will be the magic that you're going to be doing inside of your labs i'll list all the disks that are there save an orphan disk and an unused disk and then now if i got everything set up correctly i'm going to go into my uh cloud scheduler i'm going to show you just using the ui here you can use the command line as you see uh as you wish unattached persistent disk job boom run now it took like less than a second to run right and let's see if they're still there are they gone all right so as you see here we're just about to run that clean up of the attached unattached persistent disks we've got an orphan disk and then one that was just never used let's see if and hopefully that code runs uh gcloud compute disk we've already run the function it takes up to a minute for that actually to run it'll submit the function but sometimes the code will take a little bit longer so i've gone ahead and ran that gcloud just compute disks list shows the disks that are out there and if you notice there are two disks that are no longer in here the one that was unused and the one that was orphaned so i can say with certainty that the code works at least when i recorded this video so go ahead inside of your labs experiment with that and then maybe create three unused ones or a couple orphan ones and just get familiar with how to create and deploy those cloud functions and then get the ability to invoke them manually via cloud scheduler or automatically via a cloud schedule or a cron job frequency give it a try gcp provides storage object life cycle rules they can use to automatically move objects to different storage classes these rules can be based on a set of attributes such as their creation date or their live state however they can't take into account whether or not the objects have been accessed one way you might want to control your costs is to move newer objects to near-line storage if they haven't been accessed for a certain period of time so in this lab you'll create two storage buckets and generate loads of traffic against just one of them and then you'll create a stackdriver monitoring dashboard to visualize that bucket's utilization or usage after that like what we did before you'll create a cloud function to migrate the idle bucket to a less expensive storage class and then we can test this by using a mock stackdriver notification to trigger that function let's take a look now one of the last optimization strategies that you're going to see here is saying all right well i've got objects that i'm storing instead of a google cloud storage bucket or a gcs bucket what happens if i have them in a storage class like regional or near line or there's a more efficient way to store those assets depending upon their usage and how can i migrate them move them between those storage classes automatically so one of the first things that i want to show you is just what all the different storage classes are and you experiment with these inside of your lab so this is just the url for google cloud storage and the different storage classes that are out there this all shows just the storage classes that are available so generally for standard if you just create a google cloud storage bucket it'll be just standard storage that's accessible uh and you don't need to have any um you don't need to specify any particular class when you're first creating it it'll default to standard but if you don't use your data that often say it's not a public bucket that gets a lot of traffic and you want to enable some cost savings like for maybe archival data or you want to automatically say well if you're not using it let's put it on something that costs a little bit less and is accessed a little bit more infrequently so that's when you can actually shift data that's stored in a gcs bucket for standard storage and then reclass it or reclassified into something like near line storage or even cold line storage if it's maybe accessed like once a year or once a quarter instead of once a day for something like standard storage so now that you're familiar with the fact that different buckets can have different storage classes let's get back to the lab so the lab here it's going to walk you through the different types of storage and then you're going to be creating different storage buckets so i've already created these buckets a little bit before but you're going to be running through just the the same repository before where you're going to be migrating the storage you're going to creating a a public bucket you'll be uploading a text file that just says this is a test and then you'd be creating a second bucket that just doesn't have any data in it and then spoiler alert we're going to call that the idle bucket or the bucket's not going to do anything so you've got those two buckets and one of the really cool things that you can do is you'll set up a stackdriver workspace and a monitoring dashboard that's going to see the usage of each of those different buckets similar to how in a previous lab you monitored the cpu usage instead of this lab you'd be monitoring the usage of the bucket again stackdriver is very flexible in terms of finding a resource on google cloud platform and monitoring how well it's used after that one of my favorite things to do is to be using an apache library i think this is apache bench to serve traffic fake traffic to that particular text file so let's do that right now this is kind of fun so hopefully this will work let's see i don't want to be an unattached persistent disk i will actually want to be in migrate storage let's see ls into the my grade storage this is where the code the python code that actually handles the storage actually lives which is cool so let's see if we can't just generate the um uh traffic now the bench command is not found so one of the things that you'll have to do is you'll have to install apache bench serving library so we'll go ahead and install that and then once that's available we're going to serve 10 000 requests uh to our one of the public buckets so as you can see here i'm in the google cloud storage page how you can get here is just in the navigation menu under not compute but storage this time i'm just going into the browser i have a couple of buckets the two that you'll be creating as part of this lab were the serving bucket which has the text file and you can see it's marked as public which means anyone on the internet can access it and the idle bucket which is doing nothing it's already been reclassified to near line storage as opposed to something like standard original but that's because i've ran the cloud function to make sure that this demo worked before we recorded it okay so now let's serve a ton of traffic uh we've done that command and then boom benchmarking be patient a thousand requests like a thousand different people wanting to hit that text file four thousand five thousand so you can see if you're on your stackdriver dashboard it's like spiking up through the roof so what you're going to be doing later is going to be saying all right well this one particular file this bucket is getting a ton of traffic so regional storage is perfectly fine for it but this other one has got nothing nothing's being accessed and there's nothing in there to be accessed uh let's move it from say regional to near line and that's exactly what the that python function is going to do that you're going to be creating a cloud function for and then wrapping that inside of a cloud scheduler as well so back inside the lab after you've done that artificial traffic which is really fun uh i like being it's kind of kind of like ddosing your yourself right uh you see the actual uh code that's going to be my doing the migration is basically says all right well let's up update it to nearline storage and set if it's not used that much same thing as your previous labs you deploy that function you'll give it an http endpoint that cloud scheduler can then invoke and then you'll make sure that it actually gets set up with a a logging feature where you can see them actually being deployed via the json file and then let's see for us i've already invoked the function and let's just confirm that it is in nearline storage boom so it was moved from a more frequently accessed storage class likely regional or standard and it has been reclassed into something that's a little bit cheaper because the thought is you're going to be accessing that data more infrequently as evidenced by the fact that it wasn't given 10 000 units of traffic and it reclassified it automatically to nearline alright that's it for this lab good luck with your attempt of it and keep in mind for quick labs you can execute them more than one time so don't worry if the timer runs out or if you didn't complete all of your activity tracking objectives you can always click end lab and started it again for another fresh hour at the lab good luck

### Lab - [Clean Up Unused IP Addresses](https://www.cloudskillsboost.google/course_templates/767/labs/408412)

In this lab you will schedule a Cloud Function to identify and clean up unused IP addresses.

* [ ] [Clean Up Unused IP Addresses](../labs/Clean-Up-Unused-IP-Addresses.md)

### Lab - [Clean Up Unused and Orphaned Persistent Disks](https://www.cloudskillsboost.google/course_templates/767/labs/408413)

In this lab you will schedule a Cloud Function to identify and clean up unused and orphaned persistent disks.

* [ ] [Clean Up Unused and Orphaned Persistent Disks](../labs/Clean-Up-Unused-and-Orphaned-Persistent-Disks.md)

### Lab - [Optimizing Cost with Google Cloud Storage](https://www.cloudskillsboost.google/course_templates/767/labs/408414)

In this lab you will configure Cloud Monitoring to observe bucket object access and deploy a Cloud Function to migrate objects from a Regional bucket to a Nearline bucket.

* [ ] [Optimizing Cost with Google Cloud Storage](../labs/Optimizing-Cost-with-Google-Cloud-Storage.md)

### Lab - [Network Tiers - Optimizing Network Spend](https://www.cloudskillsboost.google/course_templates/767/labs/408415)

In this lab, you create one VM in the premium network service tier (default) and one VM in the standard network service tier. Then you compare the latency and routes for each VM instance.

* [ ] [Network Tiers - Optimizing Network Spend](../labs/Network-Tiers-Optimizing-Network-Spend.md)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.google)
