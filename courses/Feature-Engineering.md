---
id: 11
name: 'Feature Engineering'
datePublished: 2023-12-14
topics:
- Machine Learning Models
- Machine Learning
- BigQuery ML
type: Course
url: https://www.cloudskillsboost.google/course_templates/11
---

# [Feature Engineering](https://www.cloudskillsboost.google/course_templates/11)

**Description:**

This course explores the benefits of using Vertex AI Feature Store, how to improve the accuracy of ML models, and how to find which data columns make the most useful features. This course also includes content and labs on feature engineering using BigQuery ML, Keras, and TensorFlow.

**Objectives:**

- Describe Vertex AI Feature Store and compare the key required aspects of a good feature.
- Perform feature engineering using BigQuery ML, Keras, and TensorFlow.
- Discuss how to preprocess and explore features with Dataflow and Dataprep.
- Use tf.Transform.

## Introduction

This module provides an overview of the course and its objectives.

### Video - [Course introduction](https://www.cloudskillsboost.google/course_templates/11/video/437459)

- [YouTube: Course introduction](https://www.youtube.com/watch?v=T7-3kaYY_c4)

person: Welcome to Feature Engineering. This is the fourth course of the Machine Learning on Google Cloud series. Want to know how you can improve the accuracy of your ML models? Well, this course covers Vertex AI Feature Store and feature engineering, using BigQuery machine learning, Keras, and tf. Transform. In this course, you learn to use the Vertex AI Feature Store, describe how to move from raw data to features, perform feature engineering in BigQuery ML and Keras, preprocess features using Apache Beam and Cloud Dataflow, and describe use cases for tf. Transform.

## Introduction to Vertex AI Feature Store

This module introduces Vertex AI Feature Store.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/11/video/437460)

- [YouTube: Introduction](https://www.youtube.com/watch?v=QIjP792cNOo)

person: Welcome to an Introduction to Vertex AI Feature Store, part of the feature engineering course. After completing this module, you will be able to understand the benefits of Feature Store, define Feature Store terminology and concepts, summarize the Feature Store data model, add or import your features into Feature Store, and describe how to serve batch and online requests using the imported features.

### Video - [Feature Store benefits](https://www.cloudskillsboost.google/course_templates/11/video/437461)

- [YouTube: Feature Store benefits](https://www.youtube.com/watch?v=pMkUoEGVaWo)

Person: In this lesson, we will describe the benefits of Feature Store by examining a typical use case. Since deploying their first ML models using Auto ML and BigQuery ML, the XYZ team has added feature engineering to their skill set. However, the team is now divided into multiple teams recreating the same features, which leads to redundant work. And each time the team needs to add a new feature to a model, they are dependent on the ops team. The team is also spending too much time trying to manage the life cycle of a feature from training to deployment and serving. The XYZ team's software developer needs to understand how to add ML to their apps via online prediction endpoints without having to understand how to procure or generate the features required by the models at prediction time. XYZ's DevOps resource is new to the XYZ team and needs to understand how to monitor and maintain feature serving and management infrastructure. Three key challenges come up often for people working in ML Features. Features are hard to share and reuse, and reliably serving in production with low latency is a challenge. In addition, inadvertent skew in feature values between training and serving is common. These three key challenges are all considered feature management pain points. How can the expanding machine learning team of XYZ solve these challenges? Vertex AI Feature Store solves these feature management problems for them. Before using Feature Store, the team computed feature values and saved them in various locations such as tables in BigQuery or hedge files in Cloud Storage. They also built and managed separate solutions for storage and consumption of feature values. Vertex AI Feature Store provides a centralized repository for organizing, storing and serving machine learning features. By using a central feature store, the XYZ team can efficiently share, discover and reuse ML features at scale, letting the team increase the speed at which they can develop and deploy new management learning applications. Feature Store is a fully managed solution. It manages and scales the underlying infrastructure for you, which means that your data scientist can focus on the feature computation logic instead of worrying about the challenges of deploying features into production. For example, the team can share and reuse ML features across use cases because Feature Store has a centralized feature repository with easy APIs to search and discover features, fetch them for training and serving and manage permissions. The team can also serve ML features at scale with low latency because the operational overhead is handled by Feature Store. A common problem in machine learning is termed training-serving skew, which means the data you train with may be skewed or different from the data that is served in production. Feature Store alleviates training-serving skew because it let's you compute feature values once, and reuse them for both training and serving. You can also track and monitor for drift and other quality issues. Feature Store also allows for feature ingestion in large batches or in real time as data streams in. With Vertex AI Feature Store, the team can store features with batch and stream import APIs and register the feature to its feature registry. This allows other team members such as data analysts, ML engineers, software developers and the data scientists to easily find the feature with Discovery API and retrieve the feature values for fast online serving with continuation feature monitoring. They can also retrieve feature batches for training jobs and point-in-time lookup to prevent data leakage.

### Video - [Feature Store terminology and concepts](https://www.cloudskillsboost.google/course_templates/11/video/437462)

- [YouTube: Feature Store terminology and concepts](https://www.youtube.com/watch?v=OrfqmJhq6JA)

Person: In this lesson, we will define feature store terminology and concepts. A feature store is a top-level container for your features and their values. When you set up a feature store, permitted users can add and share their features without additional engineering support. Users can define features and then ingest or import feature values from various data sources. An entity type is a collection of semantically related features. You define your own entity types based on the concepts that are relevant to your use case. For example, a movie service might have the entity types Movies and Users that group related features that correspond to movies or customers. An entity is an instance of an entity type. For example, movie_01 and movie_02 are entities of the movies entity type. In a feature store, each entity must have a unique ID and must be of type STRING. When you retrieve values from a feature store, their service returns an entity view that contains the feature values that you requested. You can think of an entity view as a projection of the features and values that feature store returns from an online or a batch serving request. For online serving requests, you can get all or a subset of features for a particular entity type. For batch serving requests, you can get all or a subset of features for one or more entity types. For example, if features are distributed across multiple entity types, you can retrieve them together in a single request that you can feed to a machine learning or batch prediction request. Simply stated, a feature is a value that is passed as an input to a model. Know also that feature store can store scalars and arrays and/or tensors. Essentially a feature describes some entity, for example, age of user price of product or category of web page. Feature store captures feature values for a feature at specific point in time. In other words, you can have multiple values for a given entity and feature. For example, the movie_01 entity can have multiple feature values for the average_rating feature. The value can be 4.4 at one time and 4.8 at some later time. feature store associates a tuple identifier with each feature value, entity_ID, feature_ID and timestamp, and which it then uses to look up values at surfing time. The timestamp column indicates when the feature values were generated. In the feature store, the time stamps are an attribute of the feature values, not a separate resource type. If all feature values were generated at the same time, you are not required to have a time stamp column. You can specify the timestamp as part of your ingestion request. Feature store keeps feature values up to the data retention limit. This limit is based on a time stamp associated with the feature values, not when the values were imported. Let's examine an entity view that shows a list of five features for the entity type budget and the feature store Hello World. When you create a feature, you specify its value type, such as Boolean array, double array or STRING. This value determines what value types you can ingest for a particular feature. This entity view also shows the count per feature, missing values, the mean, standard deviation, the number of zeros present for each feature and the min, medium, and max values for each feature. These two columns are associated with the entity_ID field, which is a STRING. And this column shows the graphical distribution of values per feature. There is a one-to-many relationship between an entity and a feature. In this Vertex AI example, there is one entity type called budget, but here many values are associated with the entity type. Feature ingestion is the process of importing feature values computed by your feature engineering jobs into a feature store. Before you can ingest data, the corresponding entity type and features must be defined in the feature store. Feature store offers batch ingestion so that you can do a bulk ingestion of values into a feature store. For example, your computed source data might live in locations such as BigQuery or cloud storage. You can then ingest data from those sources into a feature store, so that feature values can be served in a unified format. Feature serving is the process of exporting stored future values for training or inference. Feature store offers two methods for serving features, batch and online. Batch serving is for high-throughput and serving large volumes of data for offline processing, like for model training or batch predictions. Online serving is for low-latency data retrieval of small batches of data for real-time processing, like for online predictions.

### Video - [The Feature Store data model](https://www.cloudskillsboost.google/course_templates/11/video/437463)

- [YouTube: The Feature Store data model](https://www.youtube.com/watch?v=GykHfZo2DJI)

WOMAN: In this lesson, we will describe the Feature Store data model. The Feature Store data model includes an entity type, entities, features, and feature values. Feature Store uses a time series data model to store a series of values for features. This model enables Feature Store to maintain feature values as they change over time. Let's review the data model with an example where you have a sample source data from a BigQuery table and your source data is about movies and their features. In this example, the movie_id column header can map to an entity type called movies. The average_rating, title, and genres are features of the movies entity type. The values in each column map to specific instances of an entity type or features, which are called entities and feature values. The timestamp column indicates when the feature values were generated. In the Feature Store, the timestamps are an attribute of the feature values, not a separate resource type. If all feature values were generated at the same time, you are not required to have a timestamp column. You can specify the timestamp as part of your ingestion request.

### Video - [Creating a Feature Store](https://www.cloudskillsboost.google/course_templates/11/video/437464)

- [YouTube: Creating a Feature Store](https://www.youtube.com/watch?v=QpqB4mnFQp0)

Person: In this lesson, we will describe how to create a feature store, create an entity type and add features and also describe the ingestion process. Recall the three key challenges with ML features that come up often. Features are hard to share and reuse. Reliably serving a feature in production with low latency is a challenge, and inadvertent skew in feature values between training and serving is common. So let's explore how creating a feature store can help address these challenges. Before creating a feature store, you'll need to preprocess your data. Ensure that your features are clean and tidy, which means that there are no missing values, data types are correct, and any one-hot encoding of categorical values has already been done. There are some requirements for your source data. Vertex AI Feature Store can ingest data from tables in BigQuery or files in Cloud Storage, but for files in Cloud Storage, they must be in the Avro or CSV format. You must have a column for entity IDs, and the values must be of type STRING. This column contains the entity IDs that the feature values are for. Your source data values or your source data value types must match the value types of the destination feature in the feature store. For example, boolean values must be ingested into a feature that is of type bool or boolean. All columns must have a header that is of type STRING. There are no restrictions on the names of the headers. For BigQuery tables, the column header is the column name. For Avro, the column header is defined by the Avro schema that is associated with the binary data. And for CSV files, the column header is the first row. If you provide a column for feature generation timestamps, use one of the following timestamp formats. For BigQuery tables, timestamps must be in the TIMESTAMP column. For Avro, timestamps must be of type long and logical time or logical type timestamp-micros. For CSV files, timestamps must be in the RFC 3339 format. CSV files cannot include array data types. Use Avro or BigQuery instead. For array types, you cannot include a null value in the array although you can include an empty array. After you preproc data, you're ready to begin. You can create a feature store in the Vertex AI console or the Vertex AI Workbench using the API. On a dashboard, click Features and then select the region. Click Create Feature Store, and in this field, name the feature store. Keep the default region. Enter the number of nodes. You can optionally use a customer-managed encryption key, then click Create. Here is the feature store we created. A couple of notes here. One, you cannot delete a feature store from the console at this time. You must do it from the API. Similarly, if you need to add another feature store, you must add it using the API. When you select the name of the feature store, you will see the properties of the new hello_world feature store. Now that the feature store is created, you need to create an entity type. Click Create Entity Type, and when the window changes, click Create Entity Type again. We recall the entity types group and contain-related features. For example, a movie's entity type might contain features like title and genre. Note that this feature store is hello_world. We're naming our entity type budget_id because our data set has media budgets for radio, television and online newspapers. Here entity type description is optional as is feature monitoring. Click Create to create the entity type. After the entity type is created, the entity is presented in the feature-store window. The hello_world feature store is also presented. Select entity type budget_id, which takes you to the hello_world entity properties. Basic information presented includes the name, region, feature-store name, entity created and updated dates and any description. Here we added a description to this entity type simply called budget. Step three is to add features. Recall that a feature is a measurable attribute of an entity type. After you add features in your entity type, you can then associate your features with values stored in BigQuery or Cloud Storage. The Add Feature window displays input fields for a feature name, value type, description, override monitoring values, feature monitoring and interval. Of the six input fields, only three are required, feature name, value type and interval. Before adding features, let's look at XYZ's team's data set. Recall the source requirements for feature store. You must have a column for entity IDs, and the values must be of type STRING. Team XYZ made sure to add an entity-type field on the budget table. This column contains the entity IDs that the feature values are for. They've also ensured that the data type is a STRING. Note that the five edit features map to the five fields in the data set. Selecting the budget_id feature shows the feature properties. Note that the data type is STRING. To confirm that the feature set contains four integers and one STRING, select entity type and feature-value distribution. To add feature monitoring to any entity type, click Edit Info while you are in the entity window. Click Enabled and then Update. After updating, check the entity properties again and note that feature monitoring is enabled with a time interval of 1 day. Note that monitoring can be enabled at any time. Feature owners, such as data scientists, might monitor feature values to detect data drift over time. In feature store, you can monitor and set alerts on feature stores and features. For example, the XYZ operations team is monitoring a feature store to track its CPU utilization. Note the metrics provide both the date and time at any point along the timeline. The team also set up an ingestion job. Ingestion jobs import features data from BigQuery or Cloud Storage, so it can be used in a feature store. Before you import data, you need to define the corresponding entity type and features. Feature store offers batch ingestion so that you can do a bulk ingestion of values into a feature store. For example, your computed source data might live in locations, such as BigQuery or Cloud Storage, and you can then ingest data from those sources into a feature store so that feature values can be served at a uniform format from the central feature store. Selecting the ingestion job takes you to the ingestion properties, which identify when the job was created, how long it took to process, the region, the number of workers and a link to the data source. The properties also identify the entity type and the name of the feature store. Note the number of ingested entities of 1,200, which barely meets the minimum number of 1,000 rows required for a data set to be uploaded into Vertex AI. As a reminder, data for ingestion should have the following columns, entity_id, the ID of the ingested entity, timestamp, the timestamp at which the feature was generated or computed, and feature columns that match the destination feature name.

### Video - [Serving features: Batch and online](https://www.cloudskillsboost.google/course_templates/11/video/437465)

- [YouTube: Serving features: Batch and online](https://www.youtube.com/watch?v=o0YMcTgHhYA)

Person: In this lesson, we describe feature serving using batch and online methods. Feature serving is the process of exporting stored feature values for training or inference. Feature Store offers two methods for serving features: batch and online. As illustrated in the overview diagram, Vertex Feature Store uses a combination of storage systems and back-end components. The key APIs are batch ingestion API. When a user ingests feature values via the batch ingestion API, the data is reliably written both to an offline store and to an online store. The offline store will retain feature values for a long time so that they can later be retrieved for training. The online store will contain the latest feature values for online predictions. The online serving API will be used by client applications to fetch feature values to perform online predictions. And batch serving API. The batch serving API is used to fetch data from the offline store for training a model or for performing batch predictions. To fetch the appropriate feature values for training, the batch serving API performs point in time lookups. Online serving is for low latency data retrieval of small batches of data for real-time processing, like for online predictions. Batch serving is for high throughput and serving large volumes of data for offline processing, like for model training or for batch predictions. Let's take a look at an example of batch serving. What if want to build an application to predict a baby's weight? Low and high birth weight can be indicative of problems that require urgent and specialized care. The doctors who provide such care are scarce. In a perfect world, we'd know where and when those doctors are needed. But we don't currently live in that world. Using an open dataset of births available in BigQuery, we could create an application to predict a baby's weight. The dataset includes details about the pregnancy. Other factors might have an impact. But for our simple application, we are using the date of birth, location of the birth, baby's birth weight, mother's age at birth and the duration of the pregnancy. These features are considered historical data because the babies have already been born. These features will be added to Vertex AI's Feature Store and then batch ingested. They can then be used to serve a prediction to a mobile app that let's the user select the mother's age, gestational age, plurality and the baby's gender. Clicking predict will predict the baby's weight based on these variables. Batch ingestion let's you ingest feature values in bulk from a valid data source. For each request, you can import values for up to 100 features for a single entity type. Note that you can run only one batch ingestion job per entity type to avoid any collisions. In a batch ingestion request, specify the location of your source data and how it maps to feature in your feature store. Because each batch ingestion request is for a single entity type, your source data must also be for a single entity type. After the import has successfully been completed, feature values are available to subsequent read operations such as for model training. In our baby weight example, our historical features are ingested in batch and made available or served to a mobile app. Another way to add feature values is by streaming near real-time data with the online serving API and calling the API with the list of required features. As part of a batch serving request, the following information is required. A list of existing features to get values for, a read-instance list that contains information for each training example, the destination URI and format where the output is written. In the output, Feature Store essentially joins the table from the read instances list and the feature values from the feature store. Specify one of the following formats and locations for the output: CSV file in a regional or multi-regional Cloud Storage bucket, or TFRecord file in a Cloud Storage bucket. As a final note, batch serving jobs must be created in the Feature Store API.

### Quiz - [Quiz: Introduction to Vertex AI Feature Store](https://www.cloudskillsboost.google/course_templates/11/quizzes/437466)

#### Quiz 1.

> [!important]
> **Which of the following is an instance of an entity type?**
>
> - [ ] Feature
> - [ ] Featurestore
> - [ ] Online Store
> - [ ] Entity

#### Quiz 2.

> [!important]
> **Vertex AI Feature Store provides a centralized repository for organizing, storing, and serving ML features. Using a central featurestore, enables an organization to efficiently share, discover, and re-use ML features at scale, which can increase the velocity of developing and deploying new ML applications. What are the key challenges that Vertex AI Feature Store solves?**
>
> - [ ] Mitigate data storage silos, which occurs when you might have built and managed separate solutions for storage and the consumption of feature values.
> - [ ] All of the options are correct.
> - [ ] Detect drift, as a result of significant changes to your feature data distribution over time.
> - [ ] Mitigate training-serving skew, which occurs when the feature data distribution that you use in production differs from the feature data distribution that was used to train your model.

#### Quiz 3.

> [!important]
> **Where are the features registered?**
>
> - [ ] Offline Store
> - [ ] Feature Monitoring
> - [ ] Feature registry
> - [ ] Online Store

#### Quiz 4.

> [!important]
> **What is one definition of a feature in machine learning?**
>
> - [ ] A value that is passed as input to a model
> - [ ] A value that you receive from a model as an output
> - [ ] A place to store any data
> - [ ] A method of feature store

#### Quiz 5.

> [!important]
> **Which of the following is the process of importing feature values computed by your feature engineering jobs into a featurestore?**
>
> - [ ] Feature store
> - [ ] Feature serving
> - [ ] Feature Monitoring
> - [ ] Feature ingestion

#### Quiz 6.

> [!important]
> **What are the two methods feature store offers for serving features?**
>
> - [ ] Online serving and Offline serving
> - [ ] Offline serving and Stream serving
> - [ ] Batch serving and Online serving
> - [ ] Batch serving and Stream serving

### Document - [Resources: Introduction to Vertex AI Feature Store](https://www.cloudskillsboost.google/course_templates/11/documents/437467)

## Raw Data to Features

Feature engineering is often the longest and most difficult phase of building your ML project. In the feature engineering process, you start with your raw data and use your own domain knowledge to create features that will make your machine learning algorithms work. In this module we explore what makes a good feature and how to represent them in your ML model.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/11/video/437468)

- [YouTube: Introduction](https://www.youtube.com/watch?v=6NKeFbTwdtM)

INSTRUCTOR: Welcome to the Raw Data to Features module. In this module, you'll learn to turn raw data into features, compare good features versus bad features, and represent features.

### Video - [Overview of feature engineering](https://www.cloudskillsboost.google/course_templates/11/video/437469)

- [YouTube: Overview of feature engineering](https://www.youtube.com/watch?v=1xinMNo4P7w)

Person: Predictive models are constructed using supervised learning algorithms where classification or regression models are trained on historical data to predict future outcomes. Feature engineering is a crucial step in the process of predictive modeling. It involves the transformation of a given feature with the objective of reducing the modeling error for a given target. The underlying representation of the data is crucial for the learning algorithm to work effectively. The training data used in machine learning can often be enhanced by extraction of features from the raw data collected. In most cases, appropriate transformation of data is an essential prerequisite step before model construction. Feature engineering can be defined as a process that attempts to create additional relevant features from the existing raw features in the data and to increase the predictive power of the learning algorithm. It can also be defined as the process of combining domain knowledge, intuition and data science skill sets to create features that make the models train faster and provide more accurate predictions. Machine learning models such as neural networks accept a feature vector and provide a prediction. These models learn in a supervised fashion where a set of feature vectors with expected output is provided. From a practical perspective, many machine learning models must represent the feature as real numbered vectors because the feature values must be multiplied by the model weights. In some cases, the data is raw and must be transformed into feature vectors. Features, the columns of your data frame, are key in assisting machine learning models to learn. Better features result in faster training and more accurate predictions. As the diagram shows, feature columns are input into the model not as raw data but as feature columns. Engineering new features from a provided feature set is a common practice. Such engineered features will either augment or replace portions of the existing feature vector. These engineered features are essentially calculated fields based on the values of the other features. As you will see later in the labs, feature vectors can be numerical, categorical, bucketized, crossed and hashed. Engineering features is primarily a manual time-consuming task. The process involves brainstorming where you delve into the problem. So what does it mean to delve into the problem? Well, if your goal is to solve a problem where you need to predict an outcome and get the best possible results from a predictive model, you need to determine if your source data can aid in this goal. In other words, how can you get the most out of your data for predictive modeling? This is the problem that the process and practice of feature engineering solves. So you start by looking at a lot of data. If you work as an academic researcher, you may have access to a plethora of data sources, from government data to industry data. If you work in a corporation, your data sources are the data your organization uses to manage the business. For example, you may be looking at customer data, sales data, product data, inventory data, operational data and people-management data, and this data could be in different formats. The customer data could be in a CSV file. The sales data could come from a JSON file. The operational data could be in an XML format. I think you get the picture. Depending on your problem and your domain, you then need to leverage your domain-specific engineered features. For example, if your machine learning problem is to predict seasonal sales based on past customer purchases in a geographic location, then you must either have domain-specific knowledge or find domain-specific features to solve the problem. In this case, you may be looking for data that allows you to create features for customer purchases at a specific date and time and in a certain geolocation or spatial region. Studying other feature-engineering problems is highly recommended. The example of seasonal sales, for example, lends itself to time-series features and longitude and latitude features. You then must devise your features. This can be done in several ways. For example, in manual feature extraction, features are constructed manually, and in this lesson, we use manual feature extraction where we manually devise features using Python and TensorFlow code libraries. Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. For example, principal component analysis, or PCA, is a way to reduce the number of variables or features in your data set. As the name states, you are simply analyzing the principal components, your independent variables or features to determine how well they predict your dependent variable or target, which is the final output you're trying to predict. In technical terms, you want to reduce the dimension of your feature space. By reducing the dimension of your feature space, you have fewer relationships between variables to consider, and you are less likely to overfit your model. In automatic feature extraction, the idea is to automatically learn a set of features from potentially noisy raw data that can be useful in supervised learning tasks, such as in computer vision. You can also devise features using a combination of both automatic and manual methods. In this example here, the feature columns derived from this process can include numerical, categorical, bucketized, crossed, embedding and hashed feature columns. There's no well defined basis for performing effective feature engineering. It involves domain knowledge, intuition and most of all, a lengthy process of trial and error. The key learning is that different problems in the same domain may need different features. It depends on you and your subject-matter expertise to determine which fields you want to start with for your hypothesis or your problem. Recall that feature engineering can be defined as the process of transforming raw data into features that are input into the final model. However, feature engineering in reality is an iterative process. For example, you can improve the accuracy of models by increasing the predictive power of learning algorithms through iteration. In other words, you create a baseline model with little to no feature engineering as determined by your data types and then add feature engineering to see how your model improves. Feature engineering types include using indicator variables to isolate key information, for example geolocation features for a New York City taxi service where you isolate a certain area for your training data set. It can also include highlighting interactions between two or more features, meaning that you can actually include the sum of two features, the difference between two features, the product of two features and the quotient of two features. Other types include representing the same feature in a different way. For example, in numeric to categorical mappings where you have grade, you can create categories and create a new feature, grade, with elementary school, middle school and high school as classes. Another example is to group sparse classes where you group similar classes and then group the remaining ones into a single other class. And another example is to transform categorical features into dummy variables.

### Video - [Raw data to features](https://www.cloudskillsboost.google/course_templates/11/video/437470)

- [YouTube: Raw data to features](https://www.youtube.com/watch?v=_ZwfubRCV-c)

Person: Let's look at how you can turn your raw data to useful feature vectors that can then be used properly in your ML models. So let's take a look at a problem. So your end objective is to build a model to predict the price of a house for a given set of inputs. What types of data points would you want to know about this house to begin with? So you might set things like the square footage of the house, maybe the size of the lot. What about the number of the rooms? Or, if it was sold in the past, how much was it sold for? You've probably already guessed that location, location, location could be a prime influencer of housing prices. You must first choose your features, that is, the data you will be basing your predictions on. Why not try to build a model that predicts the price based on the area of a house or apartment? Your features will be the square footage, number of bedrooms, garage, driveway and the category, house or apartment. Wouldn't it be great if your raw data was already clean and just the key fields you needed as well as in a format you could just pass off to your ML model for training? Well, this is almost never the case. Good feature engineering can take, on average, 50 to 75 percent of your time working with machine learning. It's critical you get it right. What you ultimately want to do is shown in this example. There's raw data on houses on the left in the vector, which is to be mapped to one or more fields in the photo on the right, which you can then use in your ML model training. This might look like an easy mapping exercise to you, but how do you even know what a good feature is in the first place?

### Video - [Good features versus bad features](https://www.cloudskillsboost.google/course_templates/11/video/437471)

- [YouTube: Good features versus bad features](https://www.youtube.com/watch?v=R49VnuxMa1E)

Person: What makes a good feature? Basically you want to take your raw data, and you want to represent it in a form that's amenable to machine learning. So it has be related to the objective. You don't just want to throw random data in there. That just makes the ML problem harder, and the idea is to make the ML problem easier. Make it easier for you to find the solution. If something is not related, throw it away. You have to make sure that it's known at prediction time. This can be surprisingly tricky. You need to make sure it is numeric. It has to have enough examples. And you need to have some human insights. So a good feature needs to be related to what you're predicting. You need to have a reasonable hypothesis of why a particular feature might matter for this particular problem. Don't just throw arbitrary data in there in the hope that there is some relationship somewhere. You don't want to do what is called data dredging, that's dredging the large data set and finding whatever spurious correlations exist, because the larger the data set is, the more likely it is that there are lots of these spurious correlations. In this example, just because we have a data point on whether chairs exist in the porch in a house photo or how many concrete blocks make up the driveway, doesn't mean they should be included in your housing model. You should have some reasonable idea of why these things could affect the outcome. The outcome is what's represented by the label. You have to have some reasonable idea of why they could be related. Now, you might be thinking, what if you could tell if the driveway had cracks in it from the photo? That would be a good feature. Keep that in mind for later. We'll come back to it. A good feature must be related to what you're predicting. You need to have a reasonable hypothesis about why a particular feature might matter for this particular problem. In this example, of all the raw features on the left, we choose only six features for our machine learning problem. The label or the value we are predicting is the fair amount. Test your knowledge with a quick quiz. You'll be shown some features and given an objective. You determine whether or not the features are related to that objective. Assume that you want to predict the total number of customers who will use a discount coupon. Which of these are related. The font of the text in which the discount is advertised, yes or no? Absolutely. The bigger the font, the more likely it is to be seen, perhaps. There is probably a difference between Comic Sans and Times New Roman. some fonts are more trustworthy thank others. Yes, it's probably a good feature. What about the price of an item the coupon applies to? You can imagine that people use a coupon more if the item costs less, so this could be a good feature. But notice what you're doing. You're not just saying yes or no for a particular feature. You're saying yes because people may use a coupon more if the item is not highly priced. You're saying yes. People might use a coupon more if they get to see it, and you get to see it better if the font is bigger. You need to have a reasonable hypothesis for why a particular feature might be good. Number of items in stock, no, how would the user even know that? Out of stock versus in stock could be a feature but not 800 items versus 1,000 items. Now, consider this example. Predict whether the credit card transaction is fraudulent or not. Whether the cardholder has purchased these items at the store before, is that a good feature or not? It could be a feature. Is this a common purchase for this user, or is it a completely unfamiliar, unlikely thing? Whether a card holder has purchased items at the store before is probably a good feature. Would you consider the category of the item being purchased to be a good feature? There probably is some fraud committed with an item like a television and not so much fraud committed with a clothing item. The category of the item could be a feature that you want to use in your model. The expiry date of the credit card would probably not be considered. Perhaps, however, the issue date would be considered, as new credit cards experience more fraud.

### Video - [Features should be known at prediction-time](https://www.cloudskillsboost.google/course_templates/11/video/437472)

- [YouTube: Features should be known at prediction-time](https://www.youtube.com/watch?v=k2BULtIsb1M)

Person: The second aspect of the good feature. You would need to know the value at the time you're predicting. Remember, the whole reason to build the machine-learning model is so you can predict with it. If you can't predict with it, there is no point in building the machine learning model. A common mistake people make is to look into their data warehouse and take all the data in there. Take all the related fields, and throw it into the model. If you take all these fields and you use it in the machine-learning model, what happens when you're going to go predict with it? When you predict with it, maybe you'll discover that your data warehouse has sales data. So that's an input into your model. "How many things were sold the previous day?" would be an input into your model. But then it turns out that daily sales data actually comes in a month later. It takes some time for information to come out from your store. There is a delay in collecting this data. Your data warehouse has the information because somebody went through the trouble of taking all the data or joining the tables, putting them in there, but at prediction time, in real time, you don't have it. Some of the information in a data warehouse is known immediately, and some other information is not known in real time. If you have used the data that is not known at prediction time as an input to your model, you don't have a numeric value for that input that your model needs. You need to ensure that you will have every input and every feature at prediction time. You want to ensure that those input variables are available. You're collecting it in a timely manner. In many cases, you also have to worry about whether it's legal or ethical to collect that data at the time that you're doing the prediction. Sometimes, that's information that you will have available to you in your data warehouse, but you cannot collect it from the user at the time that you're trying to do the prediction. If that's the case, you cannot use it in your ML model. Earlier, you used the housing price prediction model. If you had today's sale price of the house and the model in the dataset, the model could just output what the price was and be perfectly accurate during training because it had access to this magic data field. But come prediction time, your new houses for sale won't have already been sold, so your model is useless because you cannot feed it what you do not know at prediction. In this example, consider why the second is a bad feature. What could go wrong? What would happen if the cluster ID was taken from another model? What if that model updates without telling you? Will you still be able to learn anything from your training data? Feature definitions should not change over time. Time for another quiz. Are the features we're going to show you knowable at prediction time or not? Taking the discount coupon case you looked at earlier and the total number of discountable items that have been sold, how long a period are you looking at this total number for? How long does it take for you to know that value? This is not a yes or no answer, but it's a question that you need to ask before you take it as an input. The number of discountable items sold the previous month. This is getting closer. This seems like something that you should have available to you, so notice that there is a way to define this. If it's something as vague as the total number of discountable items sold, it's too vague. You don't have a time period. You don't know how long it takes to collect. If you make it more practical, for example, the total number of discountable items sold the previous month, at this point you've defined it in a way that you can have it, and of course, this time frame is going to depend on the latency of your system. So this is, again, a prompt for you to go find these kinds of things. How long does it take for you to actually get this data before you can use this data in real time? Number of customers who viewed ads about an item? Again, this is a question of timing. How long does it take for you to get the ad analysis back so you can use it? You cannot train with current data and predict with stale data. If you go to a data warehouse for training, you cannot use all of the values for a customer's credit card usage because not all of those values are going to be available at the same time. What you would have to do is modify your training data to be as of 3 days ago. The key point is that you have to train with stale data if stale data is what you have in real time. So let's think about this. Say you're doing a prediction on May 15th. The data in your database is only going to be current as of May 12th. Which means, when you're training, and you're training on some data on February 12th, that will use the input variable. You can only train with the number of times this card has been used as of February 9th. You have to correspondingly correct for the staleness of your data. If you've trained your model assuming that you know exactly the data, up to the minute, but then, at prediction time, you'll only know the data as of three days ago, then you'll have a very poorly performing machine learning model. You have to think about the temporal nature of all the input variables that you're using. Okay, next one. Is the item new at the store? And so it cannot have been purchased before. Yes, this is something you should know from your catalog, perfectly valid input. Category of the item being purchased. No problem, again. You'll know it. You'll know if this thing is a grocery item or if this thing is an apparel item or if this thing is an electronic item. No problem, we can look at the time and we know what it is. Online purchase, or in-person purchase? Yes, you know this thing, too, in real time. It's not a problem.

### Video - [Features should be numeric](https://www.cloudskillsboost.google/course_templates/11/video/437473)

- [YouTube: Features should be numeric](https://www.youtube.com/watch?v=Okfgli3fNd8)

Person: The third key aspect of a good feature is that all your features have to be numeric, and they have to have meaningful magnitude. A neural network is simply an adding, multiplying, weighing machine. It's just carrying out arithmetic operations, computing trigonometric functions, and algebraic functions on your input variables. So your inputs better be numbers, and those magnitudes better have some meaning. Let's take a quiz again. Which of these are numeric? Note, non-numeric features can be used. It's just that we need to find a way to represent them in numeric form. You're trying to predict the number of coupons that are going to be used, and you're looking at different features of that discount coupon. Would the percent value of the discount-- for example, 10% off, 20% off, et cetera-- be numeric? It would. Meaningful magnitude is a 20% coupon worth twice a 10% off coupon. The percent value of the discount is a numeric input. Consider the size of the coupon. If you define it as 4 square centimeters, or 24 square centimeters, or 48 square centimeters, then this is numeric. You could imagine that this is numeric. However, it's also unclear whether the magnitudes are actually meaningful. If this were an ad the size of a banner ad, you could argue the magnitude is meaningful because larger ads are better since they are more visible. However, if it's a physical coupon that's just a piece of paper, then you have to wonder whether a 48 square centimeter coupon is twice as good as a 24 square centimeter coupon. Let's change this problem a little bit and define the size of a coupon as "small," "medium," and "large." Are "small," "medium," or "large" numeric values now? Not at all. It is not suggested that you cannot have categorical variables as inputs to neural networks. You can; it's just that you can't just use "small," "medium," or "large" directly. You have to do something smart to them. You would have to find a way to represent them in numeric form. You will look at how to do this shortly. Let's look at the font of an advertisement. For example, Arial 18, Times New Roman 24, et cetera. Would you say this is numeric? It isn't. But how do you convert Times New Roman to numeric? You could say Arial is number one, Times New Roman is number two, Roboto is number three, and Comic Sans is number four. That's a number code, but they don't have meaningful magnitudes. If you set Arial as one and Times New Roman as two, Times New Roman is not twice as good as Arial. So the meaningful magnitude part is important. How about the color of the coupon? For example, red, black, blue, et cetera. But again, no, these are not numeric values. They also don't have meaningful magnitudes. Again, we can come up with a number, like RGB values, to make colors numbers, but they're not going to be meaningful numerically. If I subtract two colors and the difference between them is three, does that mean if I subtract two other colors and the difference between them is also three, are these two commensurated? No, and that's a problem. How about item category? For example, one for dairy, two for deli, three for canned goods, et cetera. This doesn't work because these are categorical, not numeric. It's not that you can't use non-numerical values, but you need to do something to them and you need to look at what things need to be done to them. So as an example, suppose you have two words in a natural language processing system. To make the words numeric, you can run something called Word2vec. Word2vec is a very standard technique. You take your words and apply this technique to make those words vectors, so each word becomes a vector. When you look at these vectors at the end of Word2vec, these vectors are such that, if you take the vector from "man" and you take the vector from "woman" and you subtract them, the difference is similar to the difference of the vector for "king" and the vector for "queen." Changing an input variable that's not numeric to be numeric is not a simple matter. It requires a lot of work. You could just go ahead and throw random encoding in there, but your model is not going to be as good as if you had started with a vector encoding that's nice enough that it understands the context of male and female, "man" and "woman," "king" and "queen." Your features need to be numeric, and they need to have meaningful magnitudes. They need to be useful. You need to be able to do arithmetic on them, and you need to find vector representations in such a way that these kinds of qualities exist.

### Video - [Features should have enough examples](https://www.cloudskillsboost.google/course_templates/11/video/437474)

- [YouTube: Features should have enough examples](https://www.youtube.com/watch?v=DGQDbDTQ0qg)

PERSON: Point number four. You need to have enough examples of feature value in the dataset, so it's the value of the feature. You need to have enough examples of this. Rule of thumb, and this is just a rule of thumb, is that you need to have at least five examples of any value before using it in your model. So what is meant by this? If you're purchasing category equals auto, then you must have enough transactions, fraud or no fraud, of auto purchases. So you should have enough fraudulent auto transactions, enough not fraudulent auto transactions. If you have only three auto transactions in your dataset and all three of them are not fraudulent, then essentially a model is going to learn that nobody can ever commit fraud on auto transactions. And that's going to be a problem. You want to avoid having values of which you don't have enough examples. For every value of a particular column, you need to have at least five examples. Think back to the cracked driveway example. If you believe the photo showing a cracked driveway could be a good indicator of housing price, be sure to have enough examples for your model to train off of. Which of these features will it be difficult to get enough examples for? Consider, again, that you are trying to predict the number of customers who will use a discount coupon. And you have, as a feature, the percent discount of the coupon. So consider, in this example, that you have a coupon that has a 10% discount. You'll probably have at least five times that a 10% off coupon has been used. If you have a 10% off or a 5% off or a 15% off, you'd probably have at least five examples of these. But what if you have this one special customer to whom you give an 85% discount? Can you use that in your dataset? No. You don't have enough samples. That 85% is now way too specific. You don't have enough examples of an 85% discount, so you have to throw it out. Or you have to find five examples of when you did give somebody an 85% discount. So that's cool if you have discrete values. But what if you have continuous numbers? Then you may have to group them up. This is called "discretization." And then see if, when discrete bands, you have at least five examples of each band. Take the second one, the data that a promotional offer starts. Assuming that you may have to group things, all promotional offers that start in January. Do you have at least five promotional offers that you started in January? Do you have at least five promotional offers that you started in February? If you don't, you may have to group things again. You may not be able to use date. You may not be able to use month. You may have to use quarter. Do you have at least five examples of things that started in Q1, Q2, Q3, and Q4, for example? You may have to group up your values so that you have enough examples of each value. Number of customers who opened advertising emails. Hopefully, you have enough examples of that, irrespective of the number you picked. You have different types of advertising emails. And you have some that were opened by 1,000 people and some that were opened by 1,200 people and some that were opened by 8,000 people. Maybe you'll have enough until you get to the very tail end of your distribution, and then you have only one email that actually got opened by 15 million customers. And you know that's an outlier. You can't use 15 million in your dataset. Whether a cardholder has purchased these items at this store before. This is to predict whether a credit card transaction is fraudulent. Will you have enough examples of cardholders who've purchased and cardholders who haven't purchased? It doesn't matter which item or which store. Because the way you're defining this, you should have enough customers who've purchased the item and enough customers who haven't. Suppose you define this as whether the cardholder or customer has purchased a bag of diapers between 8:00 PM and 9:00 PM at this specific store. That is way too specific, so it really depends on how you define it. But if you define it general enough such that you'd have enough examples of a value, then you're good. Distance between a cardholder address and the store. Will you have enough examples of customers who live 10 miles away? You probably will. What about customers living 50 miles away? If maybe, then this starts becoming a problem. This is basically where you may have to start grouping them together. You can't use a value as is. You say, I'm going to take all the customers who live more than 50 miles away, and I'll treat them as one lump. So you're not going to actually take 3,243 miles and then use that in your training dataset. Because now your neural network will happily go and train that, any time that somebody comes in from 3,243 miles away, whatever that person does is fine. Because that one time that this person came and used their card, they didn't commit a fraud, so that's basically what you want to avoid. So we're talking about the values of the features, not the values of the labels. If you're going to use rainfall amount as a feature, you're going to have to make sure that you have enough days that it rained one centimeter, two centimeters, three centimeters, et cetera. How do you make sure that you have enough examples of a particular value? Plot histograms of your input features. As a rule of thumb, make sure you have at least five, so try to look for at least five. You want to have enough examples. Consider this the category of the item being purchased. Sure, you'll have enough examples of that for any category that you choose. Online purchase, in-person purchase. Again, you'll have enough examples of those, so those shouldn't be a problem.

### Video - [Bringing human insight](https://www.cloudskillsboost.google/course_templates/11/video/437475)

- [YouTube: Bringing human insight](https://www.youtube.com/watch?v=KC7XaoffZKM)

Person: Bring your human insight into the problem. You need to have subject matter expertise and a curious mind to think of all of the ways a data field could be used as a feature. Remember that feature engineering is not done in a vacuum. After you train your first model, you can always come back and add or remove features for model two.

### Video - [Representing features](https://www.cloudskillsboost.google/course_templates/11/video/437476)

- [YouTube: Representing features](https://www.youtube.com/watch?v=4nR8oaxUn6o)

PROFESSOR: Next, you're going to look at representing features with some examples. In this example, this is your raw data. You're in an ice cream store. You're trying to figure out if your ice cream is served by some employee, or if the customer waited 1.4 seconds or 1.4 minutes. You want to predict the rating. How satisfied is your customer going to be based on who served them, how long they waited, what is it that they bought, what the store location was, et cetera. This is your training data. You would take this training data and make them all numbers because neural networks deal with numbers. You'll take your data, make them all numbers, and those are your features. So in TensorFlow, you're going to take this thing, which is a JSON input, comes out of your web application, and ultimately goes into a data warehouse-- you pull it out. You create numeric values. In TensorFlow, each of these columns is a feature column. So how do you take some data like this and make them feature columns? Make them numeric. There are values like price or wait time. These are already numeric. That's easy to encode. You take them and use them as is. They're numeric. They have meaningful magnitude-- so 2.5, 1.4 in TensorFlow in TFLearn. This is what is called a real-valued column. So you'll just say layers are real-valued column price-- layers that real-valued column wait time. So these numbers that you used as is, they'll just be real-valued columns. How about this input? Transaction ID is 42. No, that's way too specific. Throw it out. It can't be used as a feature. Note, categorical column with vocabulary list is a type of feature column. How about employee ID? Employee ID is 72365. Is that numeric? It's a number. But does it have meaningful magnitude? Is somebody with an employee ID of 72365 twice as good as somebody with an employee ID of 36182? No. So you can't use the employee ID as it is. You have to do something with it. Suppose your ice cream shop has five employees-- employee number 8345, employee number 72365, et cetera. What you can do is say if this is employee number 72365, I'll represent this employee ID by this vector, the vector 01000, because I've defined the second column as corresponding to employee 72365. So essentially, I make it like a bit mask. You make that employee's column 1 and all other columns 0. This is called one-hot in coding. There's one column that's hot. And all the other columns are cold. If you have five employees in an ice cream store, you essentially have five columns. In TensorFlow, this is called a sparse column. You basically say that I want to create a sparse column with the keys. The column name is Employee ID. And the keys are 8345, 72365, et cetera. You just pass the strings for each of those keys. TensorFlow will take the string that you provide a direct training time-- our prediction time-- and represent them, one-hot encode them, make them all numeric. What you do is say that Employee ID is a sparse column. This is if you know the keys beforehand. What if you don't know the keys beforehand? You would take your input data. You would preprocess this data to find out all the keys that occur in your training data set, and create a vocabulary of keys. So that's your first step. That's the preprocessing. You'd have to do this before you do your new training. And then you would create a new data set where these preprocessed values can then be used. So before you get to training the model, you need to create a vocabulary. And this vocabulary needs to be available at prediction time because at prediction time, the user is going to come back and say employee ID 72365. And the model needs to know that at training time, it decided that 72365 was the second column. The vocabulary needs to be identical. And the mapping of the vocabulary needs to be identical at prediction time. Otherwise, it's not going to work. So consider what would happen if you hire a new employee. Would the model still be the same? At this point, you don't have a place to put this new employee. So what this means is that you are unable to predict for this new employee. This is the kind of thing that you need to think about beforehand. And you might add something around what do I do with an employee that I don't know about, an employee that isn't found? And then you decide that perhaps you're going to find the average of all your current employees and use them. Meanwhile, you collect data about the times that this employee is on duty and the customer satisfaction associated with this employee with different wait times and different things that they're serving. Once you've collected that, you use that in your prediction. So if you know the keys beforehand, you essentially create sparse columns with keys. And you pass in the keys. You just hardcode the keys. These are all different ways of creating a sparse column. Sometimes, your data might already be indexed. Maybe, for example, you have employee ID. And they just happen to be numbers 1 to 1,000. At that point, they're already indexed. They're not arbitrarily big numbers all over the place. They're just 1 to n. If that's the case, you say, I want to create a sparse column with the integerized feature, which is employee ID. And there are five employees. So where this is useful in your taxi example is that you'll use it for the hour of the day because that's automatically integerized from 0 to 23. It's perfect as an integerized feature because the hour of the day is not completely numeric because the number, like 23, is very close to the number 01. It's only two hours away. Take the third possibility. Suppose you don't have a vocabulary and it's not integerized. You don't want to go out, build a vocabulary. And you don't really care. So what you do is that you say, I'm going to take my employee ID, hash it, compute the hash of the employee ID, and just break that hash up into 500 buckets. Why would you want to do this? Suppose your company has 100 employees and you hash it to 500 buckets. So on average, each bucket will either have zero employees or one employee in it. It's almost like one-hot encoding, but a 500-hot encoding. That kind of gets the same thing, but without having to build a vocabulary. Thinking back to the customer rating example, what do you do with it? If you are trying to predict the customer rating, then it's a label, and you're not even worried. But say you're trying to use this as an input because you're trying to predict something else. So if you have something like a rating, and you want to use it as an input feature, you could do one of two things. You could treat it like a continuous number, 1 through 5. It's numeric. It sort of has meaningful magnitude for us, more than 3, or you can say that 4 stars is very different from 5 stars is very different from 2 stars, in which case I'm going to one-hot encode it. So in some cases, you have choices to the customer rating. You can one-hot encode it or you can treat it as a number-- up to you. Choose how to deal with the rating. One thing to watch out for is what you do if the customer didn't actually provide a rating-- that you might be doing a survey and the customer didn't answer that survey. What do you do with that missing data? One option is to use two columns, one for the rating and one for whether or not you got a rating. In this case, the number 4 is a rating that a customer gave you. And 1 means that they gave you a rating. A 0 is they didn't rate you. You could also do it in the other way if you're doing one-hot encoding. You would say I got a rating 4-- 0, 0, 0, 1-- or I didn't get a rating-- 0, 0, 0, 0. But don't make the mistake of not having the second column. You don't want to mix magic numbers with real values. You have to add an extra column to state whether or not you observed the value or not. If you have missing data, you need to have another column.

### Quiz - [Quiz: Raw Data to Features](https://www.cloudskillsboost.google/course_templates/11/quizzes/437477)

#### Quiz 1.

> [!important]
> **In what form can raw data be used inside ML models?**
>
> - [ ] After turning your raw data into a useful feature vectors
> - [ ] After turning your raw data into a useful feature matrix
> - [ ] None of the options are correct.
> - [ ] After turning your raw data into multidimensional vectors

#### Quiz 2.

> [!important]
> **A good feature has which of the following characteristics?**
>
> - [ ] It should be numeric with meaningful magnitude.
> - [ ] It should be related to the objective.
> - [ ] It should be known at prediction time.
> - [ ] All of the options are correct.

#### Quiz 3.

> [!important]
> **Which of the following are the requirements to build an effective machine learning model?**
>
> - [ ] It should scale to a large dataset.
> - [ ] It should find good features.
> - [ ] All of the options are correct.
> - [ ] It should be able to preprocess with Vertex AI Platform.

#### Quiz 4.

> [!important]
> **Which of the following statements is true about preprocessing?**
>
> - [ ] None of the options are correct.
> - [ ] Both options are correct.
> - [ ] Preprocessing within the context of Cloud ML allows you to do it at scale.
> - [ ] Preprocessing without the context of Cloud ML allows you to do it at scale.

#### Quiz 5.

> [!important]
> **Which of the following statements is true?**
>
> - [ ] Different problems in different domains may need the same features.
> - [ ] None of the options are correct.
> - [ ] Same problems in the same domain may need different features.
> - [ ] Different problems in the same domain may need different features.

### Document - [Resources: Raw Data to Features](https://www.cloudskillsboost.google/course_templates/11/documents/437478)

## Feature Engineering

This module reviews the differences between machine learning and statistics, and how to perform feature engineering in both BigQuery ML and Keras. We'll also cover some advanced feature engineering practices.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/11/video/437479)

- [YouTube: Introduction](https://www.youtube.com/watch?v=3xhB9DVvAws)

person: Welcome to the Feature Engineering module. In this module, you'll learn to: Distinguish machine learning from statistics, perform feature engineering using BigQuery ML, and perform feature engineering using Keras.

### Video - [Machine learning versus statistics](https://www.cloudskillsboost.google/course_templates/11/video/437480)

- [YouTube: Machine learning versus statistics](https://www.youtube.com/watch?v=9l_hJowNS60)

Person: But if you had taken statistics, you might see if those missing values -- You would normally impute a value like the average for that column, so that's where philosophically, ML and statistics start to diverge. In ML, the idea is that you build a separate model for this situation where you have the data versus when you don't. We can afford to do this in ML where we actually have the data and where we don't have the data because, in ML, we have enough data where we want to build something as fine-grained as we can. Statistics, on the other hand, is about keeping the data that you have and getting the best results out of the data that you have. The difference in philosophy affects how you treat outliers. In ML, you go out and find enough outliers that it becomes something that you can actually train with. You remember that five-sample rule that we had? With statistics, you say, I've got all the data I'll ever be able to collect, so you throw out outliers. It's a philosophical difference because of the scenarios where ML and statistics are used. Statistics is often used in a limited data regime where ML operates with lots of data, so having an extra column to flag with all of your missing data is what you would normally do in ML. When you don't have enough data, you impute it or replace it by an average. Now, this example here is of predicting house value. The data set includes latitude and two peaks that you see here, one for SFO and the other for LAX. That's San Francisco and Los Angeles. It doesn't make sense to represent latitude as a floating point feature in our model. It's because there's no linear relationship exists between latitude and housing values. For example, houses in latitude 35 are not 35, 34 times more expensive than houses at latitude 34. And yet individual latitudes are probably a good indicator of housing values. So what do we do with that magnitude piece? Well, what if we did this? Instead of having one floating point feature, let's take a look and have 11 distinct boolean features, yes, no, LatitudeBin1, LatitudeBin2 all the way to LatitudeBin11 with yes, no boolean values. And here, we've just used fixed-bin boundaries. Another option that you see commonly used between data scientists is to have quantile boundaries so that the number of values in each bin is constant. You'll see this a lot in regression problems. Quite a few training cycles will be spent trying to get the unusual instances correct, so you're collapsing a long tail in ML versus removing them from those set and normal statistics. If the house has 50 rooms, we set it to have four rooms, which is the top of our range. The idea is that the price of a home in the hundreds of thousands, while things like the number of rooms are small numbers, and optimizers have traditionally a hard time dealing with this. The price ends up dominating your gradient. Now, modern architectures for ML end up taking variable magnitudes into account because of what's called batch normalization. Although you might run into issues if a batch of examples happens to have all unusual values, so this is not as important as it used to be.

### Video - [Basic feature engineering](https://www.cloudskillsboost.google/course_templates/11/video/437481)

- [YouTube: Basic feature engineering](https://www.youtube.com/watch?v=blOxswF8eRg)

Person: BigQuery preprocessing involves two aspects, representation transformation and feature construction. Feature representation is converting a numeric feature to a categorical feature through bucketization. And converting categorical features to a numeric representation through one-hot encoding, learning with counts, sparse feature embeddings and so on. Some models work only with numeric or categorical features, while others can handle mixed type features. Even when models handle both types, they can benefit from different representation, numeric and categorical of the same feature. Feature construction is creating new features, either by using typical techniques such as polynomial expansion by using univariate mathematical functions or feature crossing to capture feature interactions. Features can also be constructed by using business logic from the domain of the ML use case. BigQuery ML supports two types of feature preprocessing, automatic and manual. Automatic preprocessing occurs during training. BigQuery ML provides the transform clause for you to define custom preprocessing using the manual preprocessing functions. You can also use these functions outside the transform clause. You can learn more about feature preprocessing in the BigQuery ML reference documentation. BigQuery can help with feature engineering because it lets you use SQL to implement common preprocessing tasks. For example, if you are preprocessing a dataset with records of taxi rides in New York City, you can specify SQL filtering operations to exclude bogus data from your training examples datasets, like rides with a distance of zero miles. Built-in SQL math and data processing functions are also valuable for simple calculation additions over source data. Data processing functions are also valuable for parsing common data formats, like timestamps, to extract details about the time of day. Filter out bogus data. For example, trip distances must be above zero. Extract hourly data. Perform calculations to get a new field, fare amount. For example, here are three types of preprocessing for dates using SQL in BigQuery ML. Extracting the parts of the date into different columns: year, month, day, et cetera. Extracting the time period between the current date and columns in terms of years, months, days, et cetera. Extracting some specific features of the date, name of the week day, weekend or not, holiday or not, et cetera. Here's an example of the days of week and hours of day queries extracted using SQL and visualized as a table in Data Studio. Note, for all non-numeric columns other than timestamp, BigQuery ML performs a one-hot encoding transformation. This transformation generates a separate feature for each unique value in the column. So let's go ahead and learn how to incorporate feature engineering into BigQuery machine learning model. We will use the New York taxi driver dataset to examine the effect of feature engineering on model prediction. Our machine learning problem is a regression problem. The measure we will use is the root mean square error, or RMSE. There's additional content on RMSE in the labs. Our machine learning problem is to predict taxi fare price in New York City using BigQuery ML. We will begin with a baseline model to help us set a goal for a good value for the error metric. Baselines are important to know because they help us determine a reasonable or good metric for the problem. Here's the code you will use to evaluate the model. Note that BigQuery automatically splits the data we gave it and trained on only a part of the data, using the rest for evaluation. After creating your model, you evaluate the performance of the regressor using the ML.EVALUATE function. The ML.EVALUATE function evaluates the predicted values against the actual data ... root mean squared error, RMSE. The primary evaluation metric for this ML problem is the RMSE. RMSE measures the difference between the predictions of a model and the observed values. A large RMSE is equivalent to a large average error, so smaller values of RMSE are better. One nice property of RMSE is that the error is given in the units being measured, so you can tell very directly how incorrect the model might be on the unseen data. As you evaluate the model in the labs, notice that you begin with a baseline model for evaluation and your benchmark model to evaluate RMSE. The baseline model has no feature engineering. The final model has feature engineering and will show a lower RMSE of $4.65 for a taxi fare. Here's a table of all the models you will see in the lab.

### Video - [Lab intro: Performing Basic Feature Engineering in BigQuery ML](https://www.cloudskillsboost.google/course_templates/11/video/437482)

- [YouTube: Lab intro: Performing Basic Feature Engineering in BigQuery ML](https://www.youtube.com/watch?v=KHXiExVtie4)

person: In this lab, we perform basic feature engineering in BigQuery Machine Learning or BQML. You will first create SQL statements to evaluate a model train without any feature engineering. You will then extract temporal features, for example, those dealing with date and time. Then you will perform a feature cross on those temporal features. Throughout the lab, you will evaluate the model using root mean squared error. Root mean squared error is explained in more detail in the lab.

### Lab - [Performing Basic Feature Engineering in BQML](https://www.cloudskillsboost.google/course_templates/11/labs/437483)

In this lab, you will utilize feature engineering to improve a model which predicts the fare amount for a taxi ride in New York City.

- [ ] [Performing Basic Feature Engineering in BQML](../labs/Performing-Basic-Feature-Engineering-in-BQML.md)

### Video - [Advanced feature engineering: Feature crosses](https://www.cloudskillsboost.google/course_templates/11/video/437484)

- [YouTube: Advanced feature engineering: Feature crosses](https://www.youtube.com/watch?v=-7MExvrK0oI)

Person: Here are some of the advanced feature engineering preprocessing functions in BigQuery ML. ML.FEATURE_CROSS(STRUCT(features)) does a feature cross of all the combinations. The TRANSFORM clause which allows you to specify all preprocessing during model creation. The preprocessing is automatically applied during the prediction and evaluation phases of machine learning. And ML.BUCKETIZE(f, split_points) where split_points is an array. Feature crosses are about memorization. Memorization is the opposite of generalization, which is what machine learning aims to do. So should you do this? In a real-world ML system, there's a place for both. Memorization works when you have so much data that for any single grid cell within your input space the distribution of data is statistically significant. When that is the case, you can memorize. You are essentially just learning the mean for every grid cell. Deep learning also needs a lot of data. Whether you want to feature cross or you want to use many layers, you need a lot of data. If you're familiar with traditional machine learning, you may not have heard much about feature crosses because they memorize and only work on large data sets. You will find feature crosses extremely useful in real-world data sets. Larger data allows you to make your boxes smaller and you can memorize more finely. Feature crosses are a powerful feature preprocessing technique on large data sets. Our ML lab model would be greatly improved if instead of treating the hour of day and day of week as independent inputs, we essentially concatenated them to create a feature cross. Here's an example. For any particular row of your input data set, how many nodes in x3 are lit up? Just one. Do you see why? Every label, every observation of the table, is taken at a specific time. That corresponds to a specific hour on a specific day of the week, so 3 p.m. in the hour of day input and Wednesday in the day of week input, feature cross these, and what do you have? You have one input node. The input node that corresponds to 3 p.m. on Wednesday will be one. All other input nodes for x3 will be zero. The input, therefore, will consist of 167 zeroes and one one. That is the definition of sparsity, a feature with mostly missing values in our case, zero. When you do a feature cross, the input is very very sparse. TensorFlow will give us easy tools to deal with this. Note some observations about sparsity. Sparse models contain fewer features and therefore are easier to train on limited data. Fewer features also means less chance of overfitting. Fewer features also mean it is easier to explain to users because only the most meaningful features remain. This is what a sparse matrix looks like, very, very wide with lots and lots of features. You want to use linear models to minimize the number of free parameters. And if the columns are independent, linear models may suffice. In this lab, you'll see examples of both spatial and temporal functions used in preprocessing. Note that the geography or spatial functions operate or generate BigQuery geography values. The signature of any geography function starts with ST_. In this example, ST_Distance returns the shortest distance in meters between two non-empty geographies. Taxi fare pickup longitude and latitude. For example, if you go to the BigQuery console, you would see that SQL statements from the previous slide executed here. On the left is the schema showing the new features, euclidean, day_hr, and day_hr.dayofweek_hourofday. And on the right side is the JSON file showing the new features. Again, euclidean, day_hr, and day_hr.dayofweek_hourofday. Note that BigQuery ML, by default, assumes that numbers are numeric features and strings are categorical features. We need to convert both the day of week and hour of day features to strings because the model neural network will automatically treat any integer as a numerical value rather than a categorical value. Thus, if not cast as string, the day of week feature will be interpreted as numerical values, for example, one, two, three, four, five, six, seven. And hour of day will also be interpreted as numeric values. For example, the day begins at midnight, or 0:00, and the last minute of the day begins at 23:59 and ends at 24:00. As such, there is no way to distinguish the feature cross of hour of day and day of week numerically. Casting day of week and hour of day as strings ensures that each element will be treated like a label and will have its own associated coefficient.

### Video - [Bucketize and Transform Functions](https://www.cloudskillsboost.google/course_templates/11/video/437485)

- [YouTube: Bucketize and Transform Functions](https://www.youtube.com/watch?v=ZU6XwoUsv-Y)

BUCKETIZE is a preprocessing function that creates buckets or bins. That is, it bucketizes a continuous numerical feature into a string feature with bucket names as the value. Here we are concatenating all of the pickup and dropoff longitude and latitude and putting them into buckets. The function outputs a string for each row. Which is the bucket name. Bucket name is in the format of bin. Currently our model uses the ST_geogpoint function to derive the pickup and dropoff feature. Before we perform our prediction, we should encapsulate the entire feature set in a TRANSFORM clause. BigQuery ML now supports defining data transformations during model creation. Which will be automatically applied during prediction and evaluation. This is done through the TRANSFORM clause in the existing create model statement. When the TRANSFORM clause is used, user specify transforms during training will be automatically applied during model serving, prediction, evaluation, et cetera. In our case, we are using the TRANSFORM clause to separate the raw input data from the transformed features. The input columns of the TRANSFORM clause are the query expression, as select part. The output columns of TRANSFORM from select list are used in training. These transformed columns are post-process, with standardization for numerics and one-hot encoding for categorical variables by default. The advantage of encapsulating features in the TRANSFORM clause is that the client code doing the predict doesn't change. That is, our model improvement is transparent to client code. Note that the TRANSFORM clause must be placed after the CREATE statement. In summary, the TRANSFORM clause ensures that transformations are automatically applied during prediction. This covers operations like excluding some data points from the training data set. Notice that to compute the rescaling formula shown on the screen, you need to know the computing statistics and vocabularies over the entire input data set. Keep in mind that for some features, you will need statistics over a limited time window like the number of products sold over the past hour. For these types of time-windowed features, you will use Beam, batch and streaming data pipelines. Other features that could be preprocessed one data point at a time can be implemented either in TensorFlow directly or using Beam. Apache Beam and complementary Google Cloud technology called Dataflow will be important to this part of the module.

### Video - [Predict housing prices](https://www.cloudskillsboost.google/course_templates/11/video/437486)

- [YouTube: Predict housing prices](https://www.youtube.com/watch?v=AgwXRTT5oMM)

Person: Our machine learning problem is to predict housing prices in California. One caveat, the data set is from the 1990 Census, so prices have increased dramatically since then. We will wrap the dataframe with tf.data. This will enable us to use feature columns as the bridge to map the columns in the Pandas dataframe to features used to train the model. The tf.data API enables you to build complex input pipelines from simple, reusable pieces. For example the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations. Most machine learning performance is heavily dependent on the representation of the feature vector. As a result, much of the actual effort in deploying machine learning algorithms goes into the design of preprocessing pipelines and data transformations. Our first step is to build an input data pipeline. We start with numeric columns. The categorical column, ocean proximity is not included. The output of a feature column becomes the input to the module. A numeric is the simplest type of column. It is used to represent real valued features. When using this column, your model will receive the column value from the data frame unchanged. In this dataset, ocean proximity is represented as a string. We cannot feed strings directly to a model. Instead, we must first map them to numeric values. The categorical vocabulary columns provide a way to represent strings as a one-hot vector unchanged. Use this when your inputs are in string or integer format and you have an in-memory vocabulary mapping each value to an integer ID. Often you don't want to feed a number directly into the model, but instead split its value into different categories based on numerical ranges. Consider our raw data that represents a home's age. Instead of representing the house age as a numeric column, we could split the home age into several buckets using the bucketized column. Notice the one-hot values below describe which age range each row matches. Combining features into a single feature, better known as feature crosses, enables a model to learn separate weights for each combination of features. Here's our feature-engineered input data pipeline. Here, use the Keras Sequential Model API to build a deep neural network. The sequential model API is a way of creating deep learning models where an instance of the sequential class is created and model layers are created and added to it. Some common and useful layer types you can choose from are dense, fully connected layer, and the most common type of layer used on multilayer perceptron models. Dropout. Apply dropout to the model, setting a fraction of inputs to 0 in an effort to reduce overfitting, and merge. Combine the inputs from multiple models into a single model. In this model, we build a linear stack of dense layers. The first layer in your model must specify the shape of the input. This is the number of input attributes and is defined by the input_dim argument. This argument expects an integer. Keras also supports a range of standard neuron activation functions such as ReLU, softmax, rectifier, 10-H, and sigmoid. You can typically specify the type of activation function used by layer in the activation argument, which takes a string value. The last layer is the output layer and we use linear as the activation function. The name of our label or what we are predicting is the median house value. After you define your model, it needs to be compiled. This creates the efficient structures used by the underlying back end, TensorFlow. In order to efficiently execute your model during training. You compile your model using the compile function and it accepts three important attributes, model optimizer, lost function, and metrics. Recall that the optimizer is a search technique used to update weights in your model. Adaptive moment estimation or adam is a popular gradient descent optimizer that uses adaptive learning rates, so we use that. Recall that the loss function is the evaluation of the model used by the optimizer to navigate the weight space. We choose the mean square error. Metrics are evaluated by the model during training. We choose MSC. Note that we declared batch size -- that is, the number of training instances shown to the model before a weight update is performed. When we initialize the training data set. Finally, the model is trained using the model.fit function. An epoch is a number of times that the model is exposed to the training data set. Fitting the model which returns a history object with details and metrics calculated for the model each epoch. We use this for graphing model performance. Next, we use Matplotlib to draw the model's loss curves for training and validation. A line plot is also created showing the mean squared error loss over the training epochs for both the train, or blue, and test, orange, sets. Once we have incorporated our features and fit our model, we will perform a prediction. Here, we're interested in seeing how well our model performs against the test data for a house with the median value of $249,000. With the longitude and latitudes listed here as well as the feature values shown. Our prediction shows $234,000. Yours may vary due to the random shuffling of the data.

### Video - [Estimate taxi fare](https://www.cloudskillsboost.google/course_templates/11/video/437487)

- [YouTube: Estimate taxi fare](https://www.youtube.com/watch?v=jrSsZxkYD6w)

Person: Our next machine-learning problem is to predict taxi fare price in New York City using Keras. These next slides will highlight the key concepts you will see when completing the labs. Here is step one of a two-step process to build the pipeline. Step one is to define our columns of data, which column we're predicting for and the default values. In our lab, we'll first define our columns of data. Here we define three variables called CSV columns, label column and defaults. Step two is to define two functions, a function to define the features and label we want to use and a function to load to the training data. Here we define a function to build a deep neural network model. Here, use the Keras Functional Model API to build a deep neural network. The Sequential API allows you to create models layer by layer for most problems. It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs. The functional API in Keras is an alternative way of creating models that offers a lot more flexibility, including creating more complex models. You'll notice that we're still using the same activation functions. For example, ReLU for our hidden layers and Linear for the final output layer. Similarly, our model optimizer, loss and metrics are the same. These five lines of code are the same in the previous example. Because the functional API in Keras requires you to specify the layer constructor inputs, the inputs into the layer are the five numeric features: dropoff_latitude, dropoff_longitude, passenger_count, pickup_latitude and pickup_longitude. The two hidden layers are h1 and h2. The final output layer outputs the taxi fare. After defining our features, default values and label, we visualize the model. Here we train the model, initializing variables such as TRAIN_BATCH_SIZE and NUM_TRAIN_EXAMPLES. Next, we use Matplotlib to draw the model's loss curves for training and validation. A line plot is also created, showing the mean squared error loss over the training epochs for both the train, blue and test, orange sets. After we incorporate our features and fit our model, we perform a prediction. Here we are interested in seeing how well our model performs against test data for a taxi ride in New York City. Our prediction shows $12.29. Yours may vary due to the random shuffling of the data.

### Video - [Temporal and geolocation features](https://www.cloudskillsboost.google/course_templates/11/video/437488)

- [YouTube: Temporal and geolocation features](https://www.youtube.com/watch?v=wtUQq-rAfy0)

Person: Here, we define three functions. The first is the parse the datetime feature column, where we use the numpy.decode UTF-8 to decode the numpy array of string elements. The datetime STRP time Python class method creates a datetime object from a string, representing a date and time and a corresponding format string, in our case, year, month, day, hour, minute, second and time zone. The second is the get day of week function, where we get the days of the week from the parse datetime function and return days, a variable we initialized earlier. Next, we have the day of week function. We pass in the get day of week TF.string parameter to a TF.py function which then maps a string to a tensor so that every element in day of week is passed to the model as a tensor. Computing Euclidean distance, the dataset contains information regarding the pickup and drop-off coordinates. However, there is no information regarding the distance between the pickup and drop-off points. Therefore, we create a new feature that calculates the distance between each pair of pickup and drop-off points. We can do this using the Euclidean distance, which is the straight line distance between any two coordinate points. Scaling latitude and longitude, it is very important for numeric variables to get scaled before they are fed into the neural network. Here, we use min/max scaling, also called normalization, on the geolocation features. Later in our model, you will see that these values are shifted and rescaled so that they end up ranging from zero to one. First, we create a function named scale longitude, where we pass in all longitudinal values and add 78 to each value. Note that our scaling longitude ranges from negative 70 to negative 78, thus the value 78 is the maximum longitudinal value. The delta or difference between negative 70 and negative 78 is eight. We add 78 to each longitudinal value and then divide by eight to return a scaled value. Next, we create a function called scale latitude, where we pass in all the latitudinal values and subtract 37 from each value. Note that our scaling longitude ranges from negative 37 to negative 45, thus the value 37 is the minimal latitudinal value. The delta or difference between negative 37 and negative 45 is eight. We subtract 37 from each latitudinal value and then divide by eight to return a scaled value. The pickup and drop-off longitude and latitude data are crucial to predicting the fare amount because fare amounts in New York City taxis are largely determined by the distance traveled. As such, we need to teach the model the Euclidean distance between the pickup and drop-off points. Recall that latitude and longitude allows us to specify any location on Earth using a set of coordinates. In our training dataset, we restricted our datapoints to only pickups and drop-offs between New York City. New York City has an approximate longitude range of negative 74.05 to negative 73.75 at a latitude range of 40.63 to 40.85. Here are the transformations we implement on the geolocation features. Here's an example where we created new features using Lambda layers. Normal functions are defined using the def keyword. In Python, anonymous functions are defined using the Lambda keyword. Keras employs a similar naming scheme to define anonymous custom layers. Lambda layers in Keras help you to implement layers or functionality that are not pre-built, and which do not require trainable weights. Here are the continued transformations we implement on the geolocation features. Here's the example where we replaced feature columns by scaled values. Here are the continued transformations we implement on the temporal features. Here's a model visualization showing all engineered features. Next, we use Matplotlib to draw the model's loss curves for training and validation. A line plot is also created showing the MeanSquaredError loss over the training epochs for both the train, blue, and test, orange sets. Here's a common loss curve shape. The loss drops off rapidly with your big steps down the gradient and then smooths out over time with smaller steps as it reaches a minima on the loss surface. After we incorporate our features and fit our model, we perform a prediction. Here, we are interested in seeing how well our model performs against test data for a taxi ride in New York City. Our prediction shows $7.28. Yours may vary due to the random shuffling of the data. Here's a summary of the RMSE values. This slide summarizes the different feature engineering types that can be applied to feature columns. We'll explore them in the next two labs.

### Video - [Lab intro: Basic Feature Engineering in Keras](https://www.cloudskillsboost.google/course_templates/11/video/437489)

- [YouTube: Lab intro: Basic Feature Engineering in Keras](https://www.youtube.com/watch?v=-KLy5ZQ8vo8)

PERSON: In this lab, we utilize feature engineering to improve the prediction of housing prices using a Keras sequential model. You will create an input pipeline using tf.data. You will then engineer features to create categorical, crossed, and numerical feature columns. Throughout the lab, you will evaluate the model using root mean squared error. Root mean squared error is explained in more detail in the lab.

### Lab - [Performing Basic Feature Engineering in Keras](https://www.cloudskillsboost.google/course_templates/11/labs/437490)

In this lab, you will utilize feature engineering to improve the prediction of housing prices using a Keras Sequential Model.

- [ ] [Performing Basic Feature Engineering in Keras](../labs/Performing-Basic-Feature-Engineering-in-Keras.md)

### Video - [Lab intro: Advanced Feature Engineering in Keras](https://www.cloudskillsboost.google/course_templates/11/video/437491)

- [YouTube: Lab intro: Advanced Feature Engineering in Keras](https://www.youtube.com/watch?v=yG7EM_8f-Fk)

person: In this lab you continue to use Keras to build a taxi fare prediction model and utilize feature engineering to improve the fare amount prediction for a New York City taxicab ride. You will first pre-process temporal feature columns in Keras, then use Lambda layers to perform feature engineering on geo-location features, then create bucketized and crossed feature columns. You will then evaluate model performance.

### Lab - [Performing Advanced Feature Engineering in Keras](https://www.cloudskillsboost.google/course_templates/11/labs/437492)

In this lab, you will use Keras to build a taxifare price prediction model and utilize advanced feature engineering to improve the fare amount prediction for NYC taxi cab rides.

- [ ] [Performing Advanced Feature Engineering in Keras](../labs/Performing-Advanced-Feature-Engineering-in-Keras.md)

### Quiz - [Quiz: Feature Engineering](https://www.cloudskillsboost.google/course_templates/11/quizzes/437493)

#### Quiz 1.

> [!important]
> **What is one-hot encoding?**
>
> - [ ] One-hot encoding is a process by which numeric variables are converted into a categorical form that could be provided to neural networks to do a better job in prediction.
> - [ ] One-hot encoding is a process by which categorical variables are converted into a form that could be provided to neural networks to do a better job in prediction.
> - [ ] One-hot encoding is a process by which only the hottest numeric variable is retained for use by the neural network.
> - [ ] One-hot encoding is a process by which numeric variables are converted into a form that could be provided to neural networks to do a better job in prediction.

#### Quiz 2.

> [!important]
> **What is a feature cross?**
>
> - [ ] A feature cross is a synthetic feature formed by multiplying (crossing) two or more features. Crossing combinations of features can provide predictive abilities beyond what those features can provide individually.
> - [ ] None of the options are correct.
> - [ ] A feature cross is a synthetic feature formed by adding (crossing) two or more features. Crossing combinations of features can provide predictive abilities beyond what those features can provide individually.
> - [ ] A feature cross is a synthetic feature formed by dividing (crossing) two or more features. Crossing combinations of features can provide predictive abilities beyond what those features can provide individually.

#### Quiz 3.

> [!important]
> **Which of the following statements are true regarding the ML.EVALUATE function?**
>
> - [ ] The ML.EVALUATE function can be used with linear regression, logistic regression, k-means, matrix factorization, and ARIMA-based time series models.
> - [ ] The ML.EVALUATE function evaluates the predicted values against the actual data.
> - [ ] You can use the ML.EVALUATE function to evaluate model metrics.
> - [ ] All of the options are correct.

#### Quiz 4.

> [!important]
> **Which of the following statements are true regarding the ML.BUCKETIZE function?**
>
> - [ ] Both options are correct.
> - [ ] ML.BUCKETIZE is a pre-processing function that creates buckets by returning a STRING as the bucket name after numerical_expression is split into buckets by array_split_points..
> - [ ] None of the options are correct.
> - [ ] It bucketizes a continuous numerical feature into a string feature with bucket names as the value.

#### Quiz 5.

> [!important]
> **What do you use the tf.feature_column.bucketized_column function for?**
>
> - [ ] To compute the hash buckets needed to one-hot encode categorical values
> - [ ] To count the number of unique buckets the input values falls into
> - [ ] To discretize floating point values into a smaller number of categorical bins
> - [ ] None of the options are correct.

#### Quiz 6.

> [!important]
> **What is the significance of ML.FEATURE_CROSS?**
>
> - [ ] None of the options are correct.
> - [ ] ML.FEATURE_CROSS generates a STRUCT feature with all combinations of crossed categorical features including 1-degree items.
> - [ ] ML.FEATURE_CROSS generates a STRUCT feature with few combinations of crossed categorical features except for 1-degree items.
> - [ ] ML.FEATURE_CROSS generates a STRUCT feature with all combinations of crossed categorical features except for 1-degree items.

#### Quiz 7.

> [!important]
> **Which of the following is true about Feature Cross?**
>
> - [ ] None of the options are correct.
> - [ ] Both options are correct.
> - [ ] Feature Cross enables a model to learn separate weights for each combination of features.
> - [ ] It is a process of combining features into a single feature.

#### Quiz 8.

> [!important]
> **True or False:  <br> Feature Engineering is often one of the most valuable tasks a data scientist can do to improve model performance, for three main reasons:
<br> 1. You can isolate and highlight key information, which helps your algorithms "focus" on what's important.
<br> 2. You can bring in your own <strong>domain expertise</strong>.
<br> 3. Once you understand the "vocabulary" of feature engineering, you can bring in other people's domain expertise. <br>**
>
> - [ ] True
> - [ ] False

### Document - [Resources: Feature Engineering](https://www.cloudskillsboost.google/course_templates/11/documents/437494)

## Preprocessing and Feature Creation

In this module you will learn more about Dataflow, which is a complementary technology to Apache Beam and both of them can help you build and run preprocessing and feature engineering.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/11/video/437495)

- [YouTube: Introduction](https://www.youtube.com/watch?v=L3CUZhg8T6s)

person: Welcome to the Preprocessing and Feature Creation: Apache Beam and Dataflow module. In this module, you will learn more about Dataflow, which is a complementary technology to Apache Beam, and both of them can help you build and run preprocessing and feature engineering. In this module, you learn to explain Apache Beam and describe Dataflow.

### Video - [Apache Beam and Dataflow](https://www.cloudskillsboost.google/course_templates/11/video/437496)

- [YouTube: Apache Beam and Dataflow](https://www.youtube.com/watch?v=E-JGLWa-Yjs)

Person: In the next part of this section, you will learn more about Google Cloud Dataflow, which is a complementary technology to Apache Beam, and both of them can help you build and run preprocessing and feature engineering. So first of all, what is Cloud Dataflow? One of the ways to think about feature preprocessing or even any data transformation is to think in terms of pipelines. Here when I say pipeline, I mean a sequence of steps that change data from one format into another. So suppose you have some data in a data warehouse like BigQuery. Then you can use BigQuery as an input to your pipeline, do a sequence of steps to transform the data, maybe introduce some new features as part of the transformation. Finally, you can save the result to an output, like Google Cloud Storage. Now, Cloud Dataflow is a platform that allows you to run these kinds of data processing pipelines. Dataflow can run pipelines written in Python and Java programming languages. Dataflow sets itself apart as a platform for data transformations because it is a serverless, fully managed offering from Google that allows you to execute data processing pipelines at scale. As a developer, you don't have to worry about managing the size of the cluster that runs your pipeline. Dataflow can change the amount of computer resources, the number of servers that will run your pipeline and do that elastically depending on the amount of data that your pipeline needs to process. The way that you write code for Dataflow is by using an open-source library called Apache Beam. So to implement a data processing pipeline, you write your code using the Apache Beam APIs and then deploy the code to Cloud Dataflow. One thing that makes Apache Beam easy to use is that the code written for Beam is similar to how people think of data processing pipelines. Take a look at the pipeline in the center of the slide. This sample Python code analyzes the number of words in lines of text in documents. So as an input to the pipeline, you may want to read text files from Google Cloud Storage. Then you transform the data, figure out the number of words in each line of text. As I will explain shortly, this kind of a transformation can be automatically scaled by Dataflow to run in parallel. Next in your pipeline, you can group lines by the number of words using grouping and other aggregation operations. You can also filter out values, for example, to ignore lines with fewer than 10 words. Once all the transformation, grouping and filtering operations are done, the pipeline writes the result to Google Cloud Storage. Notice that this implementation separates the pipeline definition from the pipeline execution. All the steps that you see before call to the p.run method are just defining what the pipeline should do. The pipeline actually gets executed only when you call the run method. One of the coolest things about Apache Beam is that it supports both batch and streaming data processing using the same pipeline code. In fact, the library's name, Beam, comes from a contraction of batch and stream. So why should you care? Well, it means that regardless of whether your data is coming from a batch data source, like Google Cloud Storage, or even from a streaming data source, like PubSub, you can reuse the same pipeline logic. You can also output data to both batch and streaming data destinations. You can also easily change these data sources in the pipeline without having to change the logic of your pipeline implementation. Here's how. Notice in the code on the screen that the read and write operations are done using the beam.io methods. These methods use different connectors. For example, the PubSub connector can read the content of the messages that are streamed into the pipeline. Other connectors can read raw text from Google Cloud Storage or file system. The Apache Beam has a variety of connectors to help you use services on Google Cloud like BigQuery. Also, since Apache Beam is an open-source project, companies can implement their own connectors.

### Video - [Dataflow terms and concepts](https://www.cloudskillsboost.google/course_templates/11/video/437497)

- [YouTube: Dataflow terms and concepts](https://www.youtube.com/watch?v=neVSgzWtTv8)

Person: Before going too much further, let's cover some terminology that I will be using over and over again in this module. You already know about the data processing pipelines that can run a dataflow. On the right-hand side of the slide, you can see the graphic for the pipeline. Let's explore the Apache Beam pipelines in more detail. The pipeline must have a source, which is where the pipeline gets the input data. The pipeline has a series of steps. Each of the steps in Beam is called a transform. Each transform works on a structure called PCollection. I'll return to a detailed explanation of PCollections shortly. For now, just remember that every transform gets a PCollection as input and outputs the result to another PCollection. The result of the last transform in a pipeline is important. It goes to sync, which is the output of the pipeline. To run a pipeline, you need something called a runner. A runner takes the pipeline code and executes. Runners are platform-specific, meaning that there's a dataflow runner for executing a pipeline on Cloud dataflow. There's another runner if you want to use Apache Spark to run your pipeline. There's also a direct runner that will execute a pipeline on your local computer. If you'd like, you can even implement your own custom runner for your own distributed computing platform. So how do you implement these pipelines? If you take a look at the code on the slide, you will notice that the pipeline operation in the main method is the beam. Pipeline, which creates a pipeline instance. Once it is created, every transform is implemented as an argument to the apply method of the pipeline. In the Python version of the Apache Beam library, the pipe operator is overloaded to call the apply method. That's why you have this funky syntax with pipe operators on top of each other. I like it. It's much easier to read this way. The strings like Read, CountWords and Write are just the human readable names that you can specify for each transform in the pipeline. Notice that this pipeline is reading from and writing to Google Cloud Storage. And as I pointed out earlier, none of the pipeline operators actually run the pipeline. When you need your pipeline to process some data, you need to call the run method on the pipeline instance to execute it. As I mentioned earlier, every time you use the pipe operator you provide a PCollection data structure as input and return a PCollection as output. An important thing to know about PCollections is that unlike many data structures, PCollection does not store all of its data memory. Remember, the dataflow is elastic and can use a cluster of servers for your pipeline. So PCollection is like a data structure with pointers to where the dataflow cluster stores your data. That's how dataflow can provide elastic scaling of the pipeline. Let's say we have a PCollection of lines. For example, the lines could come from a file in Google Cloud Storage. One way to implement the transformation is to take a PCollection of strings, which are called lines in the code, and return a PCollection of integers. This specific transform step in the code computes the length of each line. As you already know, Apache Beam SDK comes with a variety of connectors that enable dataflow to read from many data sources, including text files in Google Cloud Storage or file systems. With different connectors, it's possible to read even from real time streaming data sources, like Google Cloud Pub/Sub or Kafka. One of the connectors is for BigQuery Data Warehouse on GCP. When using the BigQuery connector, you need to specify the SQL statement that BigQuery will evaluate to return back at table with rows of results. The table rows are then passed to the pipeline in a PCollection. To export out the result of a pipeline, there are connectors for Cloud Storage, Pub/Sub, BigQuery and more. Of course, you can also just write the results to the file system. An important thing to keep in mind when writing to a file system is that dataflow can distribute execution of your pipeline across a cluster of servers. This means that there can be multiplier servers trying to write results to the file system. In order to avoid contention issues where multiple servers are trying to get a file locked to the same file concurrently, by default the TextIO connector will shard the output, writing the results across multiple files on the file system. For example, here the pipeline is writing the result to a file with the prefix output in the data connector. Let's say there's a total of 10 files that will be written, so dataflow will write files like output zero of 10 txt, output one of 10 txt and so forth, so on. Keep in mind that if you do that, you will have the file lock contention issue that I mentioned earlier. So it only makes sense to use the withoutSharding writes rights when working with smaller datasets that can be processed on a single node. With a pipeline implemented in Python, you can run the code directly in the shell using the Python command. To submit the pipeline as a job, to execute in dataflow on GCP, you need to provide some additional information. You need to include arguments with the name of the GCP project, location in Google Cloud Storage Bucket, where dataflow will keep some staging and temporary data, and you also need to specify the name of the runner, which in this case is that dataflow runner.

### Quiz - [Quiz: Preprocessing and Feature Creation](https://www.cloudskillsboost.google/course_templates/11/quizzes/437498)

#### Quiz 1.

> [!important]
> **Your development team is about to execute this code block. What is your team about to do?
<img src="https://cdn.qwiklabs.com/JO8HBQJzlpCeB94ge9iMBDFsqRqVSCJIRc5kHNHd4zk%3D">**
>
> - [ ] We are compiling our Cloud Dataflow pipeline written in Python and are loading the outputs of the executed pipeline inside of Google Cloud Storage (gs://)
> - [ ] We are preparing a staging area in Google Cloud Storage for the output of our Cloud Dataflow pipeline and will be submitting our BigQuery job with a later command.
> - [ ] We are compiling our Cloud Dataflow pipeline written in Java and are submitting it to the cloud for execution.Notice that we are calling mvn compile and passing in --runner=DataflowRunner.

#### Quiz 2.

> [!important]
> **True or False: A ParDo acts on all items at once (like a Map in MapReduce).**
>
> - [ ] False
> - [ ] True

#### Quiz 3.

> [!important]
> **What is the purpose of a Cloud Dataflow connector?.apply(TextIO.write().to("gs://…"));**
>
> - [ ] Connectors allow you to output the results of a pipeline to a specific data sink like Bigtable, Google Cloud Storage, flat file, BigQuery, and more.
> - [ ] Connectors allow you to authenticate your pipeline as specific users who may have greater access to datasets.
> - [ ] Connectors allow you to chain multiple data-processing steps together automatically so they process in parallel.

#### Quiz 4.

> [!important]
> **To run a pipeline you need something called a ________.**
>
> - [ ] executor
> - [ ] runner
> - [ ] Apache Beam
> - [ ] pipeline

#### Quiz 5.

> [!important]
> **What is one key advantage of preprocessing your features using Apache Beam?**
>
> - [ ] The same code you use to preprocess features in training and evaluation can also be used in serving.
> - [ ] Apache Beam code is often harder to maintain and run at scale than BigQuery preprocessing pipelines.
> - [ ] Apache Beam transformations are written in Standard SQL which is scalable and easy to author.

#### Quiz 6.

> [!important]
> **True or False: The Filter method can be carried out in parallel and autoscaled by the execution framework:
<img src="https://cdn.qwiklabs.com/tdbwGpro8kEkyktN5xcKaRAuBLvobRVzfXMbD2Eurrc%3D">**
>
> - [ ] False: Anything in Map or FlatMap can be parallelized by the Beam execution framework.
> - [ ] True: Anything in Map or FlatMap can be parallelized by the Beam execution framework.

#### Quiz 7.

> [!important]
> **Which of these accurately describes the relationship between Apache Beam and Dataflow?**
>
> - [ ] They are the same.
> - [ ] Dataflow is the proprietary version of the Apache Beam API and the two are not compatible.
> - [ ] Apache Beam is the API for data pipeline building in Java or Python and Dataflow is the implementation and execution framework.

### Document - [Resources: Preprocessing and Feature Creation](https://www.cloudskillsboost.google/course_templates/11/documents/437499)

## Feature Crosses - TensorFlow Playground

In traditional machine learning, feature crosses don't play much of a role, but in modern day ML methods, feature crosses are an invaluable part of your toolkit. In this module, you will learn how to recognize the kinds of problems where feature crosses are a powerful way to help machines learn.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/11/video/437500)

- [YouTube: Introduction](https://www.youtube.com/watch?v=naHF-p81B44)

person: Welcome to the Feature Crosses module. In this module, you'll learn to describe Feature Crosses and use Feature Crosses to create a good classifier.

### Video - [What is a feature cross](https://www.cloudskillsboost.google/course_templates/11/video/437501)

- [YouTube: What is a feature cross](https://www.youtube.com/watch?v=8-TH62zU5c0)

Person: Remember these diagrams that we used to explain what neural networks were? You could think of the blue dots as maybe customers who buy a particular phone and the yellow dots as customers who don't buy the phone. Perhaps the X axis is the time since this customer last bought a phone, and perhaps the Y axis is the income level of the customer. Essentially people who buy the product, if it has been a long time since they bought the phone, and they are relatively wealthy. So look at this data. Can you come up with a line that more or less separates these two classes? Sure, we can. It might have a little bit of error. It's not perfectly separable, but a linear model is probably pretty good here. So this is a linear problem. The blue dots and the yellow dots are linearly separable by the green line. Great. But what if our data looks like this? Can we still use a linear model? Well, it seems that I cannot draw a line that manages to separate the blue dots from the yellow dots. Nope. Wherever I draw my line there are blue points on either side of the line. The data are not linearly separable. So I cannot use a linear model. Can we be a bit more specific about what we mean by linear model? So define a new feature X3 as a product of X1 and X2. So how does this help? So take X3, the product of X1 and X2, where is it positive? Exactly. When X1 and X2 are both positive or when X1 and X2 are both negative. And where is it negative? Where is X3 negative? Exactly. When X1 or X2 is negative and the other one is positive. So now we have X3. Can you see how the addition of X3 makes this solvable via a linear model?

### Video - [Discretization](https://www.cloudskillsboost.google/course_templates/11/video/437502)

- [YouTube: Discretization](https://www.youtube.com/watch?v=j6j_czJ_ECo)

person: Well, here is a more complex problem. Obviously, a linear model wouldn't help. Or can it? What if I discretize the x1 axis by drawing not just one white line, but lots of these black lines? And we do the same thing for the x2 axis, by drawing a whole bunch of black lines. Now we have discretized the x1 axis and the x2 axis. When we drew two white lines, we ended up with four quadrants. So what about now? If I have M vertical lines and N horizontal lines, we will end up with M+1 x N+1 grid cells, right? Now let's consider what this looks like when we discretize x1 and x2 and then multiply.

### Video - [Lab intro: TensorFlow Playground: Use feature crosses to create a good classifier](https://www.cloudskillsboost.google/course_templates/11/video/437503)

- [YouTube: Lab intro: TensorFlow Playground: Use feature crosses to create a good classifier](https://www.youtube.com/watch?v=2NURSaraYv8)

person: In this lab, you will use the TensorFlow Playground to develop an intuitive understanding of the limits of Feature Crosses. So click on this link and launch the training of the job. Is the model behavior surprising? Be specific. What's strange? What's the issue? Now try removing the cross-product features. What happens? Does performance improve? Can you explain what's going on?

### Video - [Lab intro: TensorFlow Playground: Too much of a good thing](https://www.cloudskillsboost.google/course_templates/11/video/437504)

- [YouTube: Lab intro: TensorFlow Playground: Too much of a good thing](https://www.youtube.com/watch?v=IsOsSbMHBqc)

person: In this lab you will use the TensorFlow Playground to develop an intuitive understanding of feature crosses. Click on these links and try to add new features while continuing to use a linear model. What is the best performance you can get? Which feature crosses help the most? Does the model output surface look like a linear model?

### Quiz - [Quiz: Feature Crosses - TensorFlow Playground](https://www.cloudskillsboost.google/course_templates/11/quizzes/437505)

#### Quiz 1.

> [!important]
> **Why might you create an embedding of a feature cross?**
>
> - [ ] To create a lower-dimensional representation of the input space
> - [ ] All of the options are correct.
> - [ ] To identify similar sets of inputs for clustering
> - [ ] To reuse weights learned in one problem in another problem

#### Quiz 2.

> [!important]
> **True or False: <br> We can create many different kinds of feature crosses. <br> For example: <br>
• [A X B]: a feature cross formed by multiplying the values of two features. <br> • [A x B x C x D x E]: a feature cross formed by multiplying the values of five features.<br> • [A x A]: a feature cross formed by squaring a single feature. <br>**
>
> - [ ] False
> - [ ] True

#### Quiz 3.

> [!important]
> **True or False: <br> In TensorFlow Playground, orange and blue are used throughout the visualization in slightly different ways, but in general orange shows negative values while blue shows positive values.**
>
> - [ ] False
> - [ ] True

#### Quiz 4.

> [!important]
> **True or False: <br> In TensorFlow Playground, in the output layer, the dots are colored orange or blue depending on their original values. The background color shows what the network is predicting for a particular area. The intensity of the color shows how confident that prediction is. <br>**
>
> - [ ] False
> - [ ] True

#### Quiz 5.

> [!important]
> **True or False: <br> In TensorFlow Playground, the data points (represented by small circles) are initially colored orange or blue, which correspond to zero and negative one.**
>
> - [ ] False
> - [ ] True

#### Quiz 6.

> [!important]
> **Fill in the blanks: <br> In the __________ layers, the lines are colored by the __________ of the connections between neurons. Blue shows a _________ weight, which means the network is using that _________ of the neuron as given. An orange line shows that the network is assigning a __________  weight. <br>**
>
> - [ ] Hidden, weights, positive, output, negative
> - [ ] Hidden, weights, negative, output, positive
> - [ ] Weights, hidden, negative, output, positive
> - [ ] Output, weights, negative, hidden, positive

### Document - [Resources: Feature Crosses - TensorFlow Playground](https://www.cloudskillsboost.google/course_templates/11/documents/437506)

## Introduction to TensorFlow Transform

TensorFlow Transform (tf.Transform) is a library for preprocessing data with TensorFlow. tf.Transform is useful for preprocessing that requires a full pass the data, such as: - normalizing an input value by mean and stdev - integerizing a vocabulary by looking at all input examples for values - bucketizing inputs based on the observed data distribution In this module we will explore use cases for tf.Transform.

### Video - [Introduction](https://www.cloudskillsboost.google/course_templates/11/video/437507)

- [YouTube: Introduction](https://www.youtube.com/watch?v=ZtxdA9ngdmI)

PERSON: Welcome to the Introduction to TensorFlow Transform module. In this module, you'll learn how to implement feature preprocessing and feature creation using tf.transform. Using tf.transform will allow us to carry out feature processing efficiently at scale and on streaming data.

### Video - [TensorFlow Transform](https://www.cloudskillsboost.google/course_templates/11/video/437508)

- [YouTube: TensorFlow Transform](https://www.youtube.com/watch?v=K5Bn0ncvDiU)

Person: Recall that we were talking about three possible places to do feature engineering. We said that you could do feature engineering within TensorFlow itself using feature columns or by wrapping the feature dictionary and adding arbitrary TensorFlow code. This is great because it's efficient. TensorFlow code on a GPU or TPU. But why do I say arbitrary TensorFlow code? Because this needs to be code that is executed as part of the model function, as part of the TensorFlow graph. So you can't do a query on your corporate database and stick a value in there. Well, you could write a custom TensorFlow op in C++ and call it, but let's ignore that for now. Also, you can only do things that rely on this input value and this input value alone. So what if you want to compute a rolling average? Well, that's a bit hard to do. Later, we'll look at sequence models where it appears we are processing a time series, so multiple input values. But the input there is the entire sequence. So the limit here is that we can do pre-processing on a single input only. TensorFlow models -- Sequence models are an exception. TensorFlow models tend to be stateless. We have looked at how to do preprocessing or feature creation in Apache Beam on Dataflow. Dataflow lets us execute arbitrary Python or Java code and allows us to handle multiple input values in a stateful way. For example, you can compute a time-windowed average like the average number of bicycles at a traffic intersection over the past hour. However, you will have to run your prediction code also within a pipeline. This is good for examples like time-windowed averages where you need a pipeline in any case. But what if all you want is the min-max to scale the values or the vocabulary to convert categorical values into numbers? Running a Dataflow pipeline in prediction seems a bit like overkill. Enter tf. Transform, which is a hybrid of the first two approaches. With TensorFlow Transform, you are limited to TF methods, but then you get the efficiency of TensorFlow. You can also use the aggregate of your entire training data set because tf. Transform uses Dataflow during training but only TensorFlow during prediction. Let's look at how TensorFlow Transform works. TensorFlow Transform is a hybrid of Apache Beam and TensorFlow. It's in between the two. Dataflow preprocessing only works in the context of a pipeline. Think in terms of incoming streaming data, such as IoT data or flights data. The Dataflow pipeline might invoke the predictions and save the predictions to Bigtable. These predictions are then served to anyone who visits the web page in the next 60 seconds at which point a new prediction is available in Bigtable. In other words, think of back-end preprocessing for ML models in Dataflow. You use Dataflow for preprocessing that needs to maintain state, such as time windows. Think on the fly preprocessing for ML models in TensorFlow You use TensorFlow for preprocessing that is based on the provided input only. If you put all the stuff in the dotted box into the TensorFlow graph, then it is quite easy for clients to just invoke a web application and get all the processing handled for them. tf. Transform is the component used to analyze and transform training data. Artifacts produced by tf. Transform are consumed at both training and serving time to avoid skew. Here is a typical ML pipeline. Problems with the typical ML pipeline, you need to keep batch and live processing in sync, and all other tooling, such as evaluation, must also be kept in sync with batch processing. Problems with do everything in the training graph, this loses the benefits of materialization, and it doesn't allow for reduces, more on this in Beam versus TensorFlow. Problems with do everything in the training graph plus using statistics and vocabs generated from raw data, it only allows for stats or vocabularies on raw data, and it still doesn't address materialization. Transform does batch process but also emits a tf. Graph that can be used to repeat these transformations in serving. More on what repeat means later. By combining this graph with the trained model graph into a single serving graph, you can guarantee that the same operations that were done to the training data will be done to the request during serving before passing the transformed result to the trained model graph. tf.transform performs a full pass over the data before the user starts their trainer job. But what about the in-between things? For example, you want to scale your inputs based on the min or max value in the data set. If you want to do this, you need to analyze your data in Dataflow so you can do the entire data set, find the min and max, and then do the transformation and Dataflow so that you can scale each individual input value. So that's what tf. Transform is about. It's a hybrid of Apache Beam and TensorFlow. To understand how this works, consider that in general preprocessing has two stages. Consider for example that you want to scale your input raw data, so that gradient descent works better. In order to do that, you will first have to find the minimum and the maximum of the numeric feature over the entire training data set. And then you will scale every input value by the min and max that were computed on the training data set. Or consider that you want to find the vocabulary of keys for a categorical variable. Let's say you have a categorical feature that is a manufacturer of a vehicle. You will go through the entire training data set to find all the possible values of a particular feature. Essentially, get the list of all the manufacturers. Then, if you find 20 different manufacturers in your training data set, you will one-hot encode the manufacturer column, into a vector of length 20. The first step involves traversing the entire data set once. We call this the analysis phase. The second step involves on the fly transformation of the input data. We call this the transformation phase. Which technology, Beam or TensorFlow, is better suited to do analysis of the training data set? Which technology, Beam or TensorFlow, is better suited to doing on the flight transformation of the input data? Analysis in Beam, transform in TensorFlow. There are two PTransforms in tf. Transform, AnalyzeAndTransformDataset, which is executed in Beam to create a preprocessed training data set, and TransformDataset, which is executed in Beam to create the evaluation data set. Remember that computing the min and max, et cetera, the analysis is done only on the training data set. We cannot use the evaluation data set for that. So the evaluation data set is scaled using the min and max found in the training data. But what if the max in the evaluation is bigger? Well, this simulates a situation where you deploy your model, and then you find that a bigger value comes in a prediction time. It's no different. You cannot use evaluation data set to compute min and max of vocabulary, et cetera. You have to deal with it. However, the transformation code that's invoked is executed in TensorFlow at prediction time. Another way to think about it is that there are two phases. The analysis phase is executed in Beam while creating a training data set. The transformation phase is executed in TensorFlow during prediction so executed in Beam to create your training and evaluation data sets.

### Video - [Analyze phase](https://www.cloudskillsboost.google/course_templates/11/video/437509)

- [YouTube: Analyze phase](https://www.youtube.com/watch?v=u9LnpPWWTaQ)

Person: Let's look at the analyzed face. Remember that you analyze that training data set. You first have to tell Beam what kind of data to expect. You do that by setting up a schema. So in the first line, I set up a dictionary called raw-data-schema, and I add entries for all the string columns. The string here is a TensorFlow data type. I then update the raw data schema by adding all the TF.float32-type columns. After this, I have a raw data schema that has all the columns in the data set that will be processed by Beam on dataflow. The raw data schema is used to create a metadata template. Next, run the analyze-and-transform PTransform on the training data set to get back preprocessed training data and the transform function. First, do beam.io.read to read in the training data. This is similar to all the Beam pipelines that you saw in the previous module on Beam. Here, I'm reading from BigQuery. Next, filter out the data that you don't want to train with. I'm doing that with the function is valid that I'm not showing you on this slide. I will show you this method later. Third, take the raw data that you get from reading and filtering and the raw data metadata that you got from the previous slide, and pass it to the analyze-and-transform data set PTransform. Beam will execute this transform in a distributed way and do all the analysis that you told it to do in the method preprocess. I'll show you this method also later. For now, the is-valid method and the preprocess method are executed by Beam on the training data set to filter it and to preprocess it. The preprocess data comes back in a PCollection, in a parallel collection that I'm calling transformed_dataset. But notice that the transformations that you carried out in preprocess are saved in the second return value, transform function. This is important. Take the transform data, and write it out. Here, I'm writing it out as TFRecords, which is the most efficient format for TensorFlow. I can do that by using the WriteToTFRecord PTransform that comes with TensorFlow transform. The files will be sharded automatically, but notice what schema is being used: not the raw data schema, the transformed schema. Why? Because, of course, what we are writing out is the transformed data, the preprocessed data, not the raw data.

### Video - [Transform phase](https://www.cloudskillsboost.google/course_templates/11/video/437510)

- [YouTube: Transform phase](https://www.youtube.com/watch?v=r6tn-X7qWFg)

Person: The preprocessing function is a function where we transform the input data. In Beam, it's called as part of the analyze-and-transform dataset. In TensorFlow, the things you do in preprocess will get called essentially as part of the serving input function in TensorFlow. You cannot call regular Python functions since the preprocess is part of the TensorFlow graph during serving. Let's look at an example. In this example, I'm taking a set of inputs and preprocessing them. What is a data type of inputs? It's a dictionary who's values are tensors. Remember, this is what is returned from the serving input function and represents the raw data as it's read. Input functions return (features, labels) and this is a features, and features is a dictionary. TFTransform will take care of converting the data that comes in via PTransform into tensors during the analysis phase. We take the tensors and we used them to create new features, and we put these features into the dictionary. The first result, fair amount in my example, is passed through unchanged. We take the input tensor and add it to the result. No changes. The next result we want is a day of the week. We want this to be an integer. However, in the input, it is the string like THU for Thursday. So what we are doing is that we are asking TensorFlow Transform to convert the string that is read, such as THU, into an integer such as three or five or whatever the number is. What TFTransform will do is to compute the vocabulary of all the possible days of the week in the training dataset. It'll do this during the analyze phase and use that information to do the string-to-int mapping in the prediction phase. Next, we want to scale off the droplat into a number that lies between zero and one. In the analysis phase, TFTransform will compute the min and the max of the column and use those values to scale the inputs. We can also invoke other TensorFlow functions. In this case, I'm taking the input number of passengers, which happens to be an integer in JSON and casting it to be a real valued number. Once all the features have been created and added, we can return the result. The analyze-and-transform PTransform happens on the training dataset. What should happen on the evaluation dataset? For the evaluation dataset, we carry out pretty much the same Beam pipeline that we did in the same training dataset. There's one big exception though: We don't analyze the evaluation dataset. If we are scaling the values, the values in the evaluation dataset will be scaled based on the min and max found in the training dataset. So on the evaluation dataset, we don't call analyze-and-transform. We just call transform dataset. This will take care of calling all the things that we didn't preprocess. Pretty cool, huh? Notice, however, that the transform dataset needs as input the transform function that was computed on the training data. That's what makes the magic possible. Once we have the transform dataset, we can write it out, just like we wrote out the training dataset.

### Video - [Supporting serving](https://www.cloudskillsboost.google/course_templates/11/video/437511)

- [YouTube: Supporting serving](https://www.youtube.com/watch?v=9JqM3YAfWkc)

Person: We use the Transform function to transform the evaluation data set, and we wrote the transformed evaluation data. For what type of data did we use AnalyzeAndTransformDataset? Right, the training data, and we used TransformDataset for the evaluation data. Even though we created the preprocessed features using Beam, the preprocessed method couldn't have arbitrary Python code. It had to consist solely of TensorFlow functions. The reason these functions needed to be in TensorFlow was that they're part of the prediction graph, and why are they part of the prediction graph? So that the end user can give the model raw data, and the model can do the necessary preprocessing. But how will the model know what functions to call? In order for the model to know what functions to call, we need to save the Transform function, and that's what I'm doing here. I'm saving the transform function itself into a directory called metadata alongside a trained model. Then we tell the input function to pick up the metadata. Which input function? All three. First, let's look at the training and evaluation input functions. They read the preprocessed features. So, notice that I specify that the schema corresponds to the transformed metadata. Change the training and evaluation input functions to read the preprocessed features. The serving input function accepts the raw data, so here, I'm passing in the raw data metadata, not the transformed metadata. While the raw data alone isn't enough, we could also have arbitrary TensorFlow functions in the preprocessing code. Those operations are stored in savedmodel.pb, but again, notice a nice TensorFlow Transform helper function, build_parsing_transforming_serving_input_fn Parse the JSON according to the raw data schema. Transform the raw data based on the TensorFlow operations and savemodel.pb. Then set it along the model. This happens because the model reads the metadata and includes the preprocessing code, so that's how TensorFlow Transform works. Let's now use it on a taxifare prediction problem.

### Quiz - [Quiz: Introduction to TensorFlow Transform](https://www.cloudskillsboost.google/course_templates/11/quizzes/437512)

#### Quiz 1.

> [!important]
> **As an approach to feature engineering, which of the following most accurately describes what TensorFlow Transform is a hybrid of?**
>
> - [ ] Apache Beam on Dataflow and TensorFlow
> - [ ] Dataflow and TensorFlow
> - [ ] AI Platform and TensorFlow
> - [ ] Apache Beam and TensorFlow

#### Quiz 2.

> [!important]
> **What does tf.Transform do during the training and serving phase?**
>
> - [ ] Provides computation over the entire dataset, including on both internal and external data sources
> - [ ] Provides a transformation polynomial to train the data
> - [ ] None of the options are correct.
> - [ ] Provides a TensorFlow graph for preprocessing

#### Quiz 3.

> [!important]
> **True or False: One of the goals of tf.Transform is to provide a TensorFlow graph for preprocessing that can be incorporated into the serving graph (and, optionally, the training graph).**
>
> - [ ] True
> - [ ] False

#### Quiz 4.

> [!important]
> **Fill in the blank:
The ______________   _______________  is the most important concept of tf.Transform. The ______________   _______________ is a logical description of a transformation of the dataset. The ______________   _______________ accepts and returns a dictionary of tensors, where a tensor means Tensor or 2D SparseTensor.**
>
> - [ ] Preprocessing function
> - [ ] Preprocessing method
> - [ ] Preprocessing variable

### Document - [Resources: Introduction to TensorFlow Transform](https://www.cloudskillsboost.google/course_templates/11/documents/437513)

## Summary

This module is a summary of the Feature Engineering course.

### Document - [Summary](https://www.cloudskillsboost.google/course_templates/11/documents/437514)

### Document - [Resource: All quiz questions](https://www.cloudskillsboost.google/course_templates/11/documents/437515)

### Document - [Resource: All readings](https://www.cloudskillsboost.google/course_templates/11/documents/437516)

### Document - [Resource: All slides](https://www.cloudskillsboost.google/course_templates/11/documents/437517)

## Your Next Steps

### Badge - [Course Badge](https://www.cloudskillsboost.googleNone)
